
R version 3.4.1 (2017-06-30) -- "Single Candle"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> options(repos=structure(c(CRAN="https://rweb.crmda.ku.edu/cran/")))
> ## capture messages and errors to a file.https://rweb.crmda.ku.edu/cran/
> #zz <- file("all.Rout", open="wt")https://cran.cnr.berkeley.edu
> #sink(zz, type="message") edit for rebaseless
> #chek for R package updates
> #try(log("a")) ## test --no-edit
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> which.computer<-Sys.info()[['nodename']]
> task.subject<-"14th20hp3cv"
> out.file<-paste("out",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,".csv",sep="")
> importance.file<-paste("importance",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,sep="")
> 
> base.folder<-getwd()
> cpout.folder<-paste(base.folder,"/",which.computer,sep = "")
> setwd(cpout.folder)
> 
> if(length(which(list.files() == out.file))<1) write.table( "0.01,0.01,100,100,100,Wed Aug 02 16:37:25 2017,dummy,8,1,basic latent features,ignore,none,asis,1.12784979099243,random,333,53,adaptive_cv,16,5,2,2,19,0.0107744822639878,FALSE,,,,,,,,,," ,file =,out.file,  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,".csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,".csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,"mlr.csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,"mlr.csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> 
> cv.iters=3
> tuneLength=20
> tuneLength2=8
> normings=c("YeoJohnson","ICA", "centernscale","expoTrans","range01","asis","quantile")#,"centernscale"
> 
> gensTTesto<-c(56,53,4,12,13,14,15,20,45,54,55, 44,52,1,3)#,  51,c(4)#c(1:40)#c(5,10,11,13,14,15,16,17,18,19,20,21,24,28,38,39,40)
> gensTTest<-vector()
> write.table( t(gensTTesto),file = "initial tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
> try({
+   gensTTest<-t(read.csv("tasks to test.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   gensTTest<-as.vector(gensTTest)
+ })
> if(!exists("gensTTest")) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> if(length(gensTTest)<1) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> 
> 
> ########packages install check######
> 
> #list.of.packages <- c("caret","caretEnsemble","mlr","MLmetrics","tgp")
> #list.of.packages <- c("gower","dimRed","DEoptimR","caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> #new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> #if(length(new.packages)) install.packages(new.packages, dep = TRUE)
> 
> 
> #install.packages("mlr", dependencies = c("Depends", "Suggests"))
> #install.packages("caret", dependencies = c("Depends", "Suggests"))
> #install.packages("caret",repos = "http://cran.r-project.org",dependencies = c("Depends", "Imports", "Suggests"))
> #install.packages("SuperLearner", dependencies = c("Depends", "Suggests"))
> #install.packages("rattle", dependencies = c("Depends", "Suggests"))
> 
> # Load libraries
> #library(mlbench)
> 
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> #library(caretEnsemble)
> library(MLmetrics)

Attaching package: 'MLmetrics'

The following object is masked from 'package:caret':

    RMSE

The following object is masked from 'package:base':

    Recall

> 
> ########error no repeat#########
> 
> 
> try({
+   before.last.alg<-as.matrix(read.csv("beforelast algorithm.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.alg<-as.matrix(read.csv("last algorithm tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   #write.table(paste(date(), last.alg,.Platform$OS.type,.Platform$r_arch,which.computer,sep=" "),file = "algos after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.alg==before.last.alg){print("algorithm may be broken")}
+   write.table(last.alg,file = "beforelast algorithm.csv",  quote = F, row.names = F,col.names = F)
+ })
[1] "algorithm may be broken"
> try({
+   before.last.tsk<-as.matrix(read.csv("beforelast task.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.tsk<-as.matrix(read.csv("last task tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   write.table(paste(date(),last.alg, last.tsk,cv.iters,tuneLength,.Platform$OS.type,.Platform$r_arch,which.computer,sep=","),file = "test after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.tsk==before.last.tsk){print("task may be broken")}
+   write.table(last.tsk,file = "beforelast task.csv",  quote = F, row.names = F,col.names = F)
+ })
[1] "task may be broken"
> bad.models=c("spaccceeee")
> previous.fails<-(read.csv("test after which reset.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
> previous.fails<-previous.fails[previous.fails[,8]==which.computer,]
> lgf<-length(previous.fails[,2])
> for(lt in 2:lgf)  {
+   if(previous.fails[lt,2]==previous.fails[lt-1,2])  {
+     bad.models=union(bad.models,c(paste(previous.fails[lt,2])))  }}
> 
> #######not to redo a test function#####
> check.redundant<-function(df=df.previous.calcs,norming="asis",trans.y=1,withextra="missing",missingdata="leaveempty",datasource="mean" ,column.to.predict=200,allmodel="ctree")
+ {
+   for(intern in 1:length(df[,1])){
+     if((any(df[intern,] == norming, na.rm=T))&&
+        (any(df[intern,] == withextra, na.rm=T))&&
+        (any(df[intern,] == missingdata, na.rm=T))&&
+        (any(df[intern,] == datasource, na.rm=T))&&
+        (any(df[intern,] == column.to.predict, na.rm=T))&&
+        (any(df[intern,] == allmodel, na.rm=T))&&
+        (  (df[intern,9] == trans.y)))
+     {return(TRUE)}
+   }
+   return(FALSE)
+ }
> #####caret init#####
> best.ranged <- c("avNNet", "nnet", "pcaNNet", "glm.nb")
> best.asis <- c("svmLinear3", "relaxo", "superpc", "xgbTree")
> best.cns <- c("gam", "bam", "svmLinear2", "msaenet", "BstLm", "gbm") 
> 
> cv6hp5 <- c( "BstLm", "qrnn")#earth
> cv3hp32 <- c("Rborist", "pcaNNet", "SBC")
> cv7x5hp32 <- c("gbm", "krlsPoly", "kknn", "xgbLinear","RRF", "cubist", "rlm" )
> cv6hp5.avoid <- c("pcaNNet")
> cv3hp32.avoid <- c("glm.nb", "gamboost", "ctree2","glmboost", "leapSeq","ctree","svmLinear2")
> cv7x5hp32.avoid <- c("SBC","bagearthgcv","gcvearth","lmStepAIC","glmStepAIC","bridge","lm","glm","bayesglm","blassoAveraged","treebag","rpart1SE")
> 
> allmodels <- c("avNNet", "bagEarth", "bagEarthGCV",
+                "bayesglm", "bdk", "blackboost", "Boruta", "brnn", "BstLm" ,
+                "bstTree", "cforest", "ctree", "ctree2", "cubist", "DENFIS",
+                "dnn", "earth", "elm", "enet",   "evtree",
+                "extraTrees",  "gamLoess",  "gaussprLinear", "gaussprPoly", "gaussprRadial",
+                "gcvEarth","glm", "glmboost",  "icr", "kernelpls",
+                "kknn", "knn",  "krlsRadial", "lars" , "lasso",
+                "leapBackward", "leapForward", "leapSeq", "lm", "M5", "M5Rules",
+                "mlpWeightDecay", "neuralnet" , "partDSA",
+                "pcaNNet", "pcr", "penalized", "pls", "plsRglm", "ppr",
+                "qrf" , "ranger",  "rf")
> allmodels <- c("rlm", "rpart", "rpart2",
+                "RRF", "RRFglobal",  "simpls",
+                "svmLinear", "svmPoly", "svmRadial", "svmRadialCost",
+                "widekernelpls",  "xgbLinear",
+                "xgbTree")
> allmodels <- c("avNNet","BstLm","bstTree","cforest","ctree","ctree2",
+                "cubist","earth","enet","evtree","glmboost",
+                "icr","kernelpls","kknn","lasso","pcaNNet",
+                "pcr","pls","qrf","ranger","rf")
> 
> allmodels <- c("kknn", "cubist", "avNNet", "xgbLinear", "RRF", "pcaNNet","earth","nnet","gbm","enet","lasso","BstLm",
+                "foba", "leapBackward", "gcvEarth", "SBC","glm.nb","gamboost","ctree2","relaxo", 
+                "bartMachine","extraTrees","bam","gam","randomGLM")
> #allmodels <- c("bam")
> #allmodels <- c("rf")"rqlasso",, "xyf" "rvmPoly", "rvmRadial",    "spls", "superpc" ,   "treebag",  "svmLinear2",  "SBC",
> #allmodels <- c("bartMachine", "xgbLinear", "pcaNNet","svmLinear","glmnet","cforest","cubist","rf","ranger")"glmnet",
> #wow rfRules is really slow "rfRules","WM", takes 50min
> # brak everythig "rbfDDA","ridge","rqnc",
> # use "rf" to test all
> library(caret)
> allmodels <- unique(modelLookup()[modelLookup()$forReg,c(1)])
> #allmodels <-c("avNNet", "nnet", "pcaNNet",  "glm.nb", "gam" ,
> #              "bam","msaenet", "svmLinear2","svmLinear3",
> #              "relaxo",  "superpc", "xgbTree", "BstLm")
> #allmodels<- c("svmLinear","svmPoly","svmRadial")
> #library(doParallel); cl <- makeCluster(detectCores()); registerDoParallel(cl)
> #allmodels<-c("bartMachine","extraTrees")#,"randomGLM"
> 
> 
> adaptControl <- trainControl(method = "adaptive_cv",
+                              number = 7, repeats = 5,
+                              adaptive = list(min = 4, alpha = 0.05,
+                                              method = "gls", complete = FALSE),
+                              search = "random")
> adaptControl <-trainControl(method = "cv", number = cv.iters,  search = "random")
> simpleControl <- trainControl(method = "cv",
+                               number = cv.iters,
+                               search = "random")
> 
> 
> #########MLR init######
> #R.utils::gcDLLs()
> #list.of.packages <- c("ParamHelpers","devtools","mlrMBO","RJSONIO","plot3D","plotly")
> #install.packages("mlrMBO", dependencies = c("Depends", "Suggests"))
> list.of.packages <- c("caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> if(length(new.packages)) install.packages(new.packages, dep = TRUE)
Installing packages into 'C:/Users/irina grishina/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Warning message:
packages ' RWeka', 'rPython', 'rrf' are not available (for R version 3.4.1) 
> 
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> tuneLengthMLR<-tuneLength
> mlr.iters<-cv.iters
> #######data read process start#####
> seed.var =222+round(runif(1,min=0,max=100))
> column.to.predict=1
> print(date());
[1] "Sun Feb 18 15:31:09 2018"
> 
> setwd(base.folder)
> if(!exists("gen.count")){gen.count=56}
> gens.names<-as.matrix(read.table("gens names.csv", sep = ",",header = FALSE,row.names=1,fill=TRUE, quote="",dec="."))
> count.toy.data.passed<-1
> for(gend.data in gensTTest){
+   count.toy.data.passed<-count.toy.data.passed+1
+   setwd(base.folder)
+   data.source<-as.matrix(read.csv(paste("Generats/",gens.names[gend.data],".csv", sep = ""), sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   datasource<-gens.names[gend.data,1]
+   setwd(cpout.folder)
+   missingdatas=c("ignore")
+   for(missingdata in missingdatas){
+     withextras=c("none")
+     for(withextra in withextras){
+       ################data wrestling###############
+       
+       dependant.selection=complete.cases(data.source[,column.to.predict])
+       df.previous.calcs=as.data.frame(read.csv(file=out.file, header = FALSE, sep = ",", quote = "",
+                                                dec = ".", fill = TRUE, comment.char = ""))
+       unimportant.computations<-vector(mode = "logical",length=length(df.previous.calcs[,1])  )
+       for(intern in 1:length(df.previous.calcs[,1])){
+         if((any(df.previous.calcs[intern,] == withextra, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == missingdata, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == datasource, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == column.to.predict, na.rm=T)))
+         {unimportant.computations[intern]<-T}}
+       
+       df.previous.calcs<-df.previous.calcs[unimportant.computations,]
+       
+       #data.source=data.frame( data.source[,column.to.predict],data.source[,1:2], data.source[,4:(column.to.predict-1)], data.source[,(column.to.predict+1):length( data.source[1,])])
+       
+         for(norming in normings) {
+         for(trans.y in 1:2) {
+           df.toprocess=data.source
+           y.untransformed<-df.toprocess[,1]
+           
+           if(norming=="centernscale"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("center", "scale"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="range01"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="expoTrans"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("expoTrans"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="YeoJohnson"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("YeoJohnson"))#"center", "scale",
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           if((norming=="asis")&&(trans.y==2)){next}
+           
+           
+           ################preprocess###########
+           df.toprocess=data.frame(df.toprocess[dependant.selection,])
+           y.untransformed=y.untransformed[dependant.selection]
+           if(norming=="quantile"){
+             for(Clol in trans.y:length(data.source[1,])){
+               df.toprocess[,Clol]<- (rank(df.toprocess[,Clol],na.last = "keep",ties.method = "average")-1) }
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           loess.model<-loess(y.untransformed~ df.toprocess[,1],span = 0.21, degree = 1)
+   
+           #df.toprocess = data.frame(df.toprocess,)
+           nzv <- nearZeroVar(df.toprocess[,])#, saveMetrics= TRUE
+           #nzv[nzv$nzv,][1:10,]
+           if(length(nzv)>1){
+             df.toprocess = (df.toprocess[, -nzv])}
+           df.toprocess = signif(df.toprocess,digits = 3)
+           
+           seed.var =222+round(runif(1,min=0,max=100))
+           set.seed(seed.var)
+           inTrain <- createDataPartition(y = df.toprocess[,1],
+                                          p = .75,
+                                          list = FALSE)
+           training <- df.toprocess[ inTrain,]
+           testing  <- df.toprocess[-inTrain,]
+           write.table(df.toprocess,file = "sanity check 1.csv",  quote = F, row.names = F,col.names = F)
+           
+           ###########for all models#################
+           setwd(base.folder)
+           if(max(which.computer==c("ALTA","HOPPER"))>0)
+             source("MLR part.R")
+           else
+             source("Caret part.R")
+           
+          setwd(cpout.folder)
+           if(norming == normings[length(normings)]){
+             write.table( t(gensTTest[count.toy.data.passed:length(gensTTest)]),file = "tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)}
+           
+         }
+       }
+     }
+   }
+   
+ }
Loading required package: gam
Loading required package: splines
Loading required package: foreach
Loaded gam 1.14-4

Generalized Additive Model using Splines 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  df         RMSE        Rsquared   Selected
  0.1608893  0.01117330  0.9999413          
  0.2015712  0.01117330  0.9999413          
  0.8245236  0.01117330  0.9999413          
  1.1034637  0.01104390  0.9999426          
  1.5570296  0.01063275  0.9999467          
  1.8729282  0.01047747  0.9999482          
  2.2311692  0.01039159  0.9999490          
  2.7079948  0.01035938  0.9999492  *       
  2.9318981  0.01036043  0.9999492          
  2.9693419  0.01036120  0.9999492          
  3.3552108  0.01037573  0.9999491          
  3.4158200  0.01037882  0.9999490          
  3.7886214  0.01040073  0.9999488          
  4.0277786  0.01041644  0.9999487          
  4.0748469  0.01041963  0.9999486          
  4.4485045  0.01044540  0.9999484          
  4.5238792  0.01045066  0.9999483          
  4.6850602  0.01046185  0.9999482          
  4.6873264  0.01046201  0.9999482          
  4.9769099  0.01048203  0.9999481          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was df = 2.707995.
[1] "Sun Feb 18 15:31:19 2018"
Loading required package: kernlab

Attaching package: 'kernlab'

The following object is masked from 'package:ggplot2':

    alpha

Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
[1] "failed"                   "failed"                  
[3] "Sun Feb 18 15:31:49 2018" "basic sum C1 + C2"       
[5] "ignore"                   "none"                    
[7] "YeoJohnson"               "gaussprLinear"           
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
[1] "failed"                   "failed"                  
[3] "Sun Feb 18 15:35:44 2018" "basic sum C1 + C2"       
[5] "ignore"                   "none"                    
[7] "YeoJohnson"               "gaussprPoly"             
Loading required package: h2o

----------------------------------------------------------------------

Your next step is to start H2O:
    > h2o.init()

For H2O package documentation, ask for help:
    > ??h2o

After starting H2O, you can use the Web UI at http://localhost:54321
For more information visit http://docs.h2o.ai

----------------------------------------------------------------------


Attaching package: 'h2o'

The following objects are masked from 'package:stats':

    cor, sd, var

The following objects are masked from 'package:base':

    %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,
    colnames<-, ifelse, is.character, is.factor, is.numeric, log,
    log10, log1p, log2, round, signif, trunc

Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
[1] "failed"                   "failed"                  
[3] "Sun Feb 18 15:36:55 2018" "basic sum C1 + C2"       
[5] "ignore"                   "none"                    
[7] "YeoJohnson"               "gbm_h2o"                 
Loading required package: earth
Loading required package: plotmo
Loading required package: plotrix
Loading required package: TeachingDemos
Multivariate Adaptive Regression Splines 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results:

  RMSE        Rsquared 
  0.01114897  0.9999414

Tuning parameter 'degree' was held constant at a value of 1
[1] "Sun Feb 18 15:36:59 2018"
Generalized Linear Model 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results:

  RMSE        Rsquared 
  0.01117335  0.9999413

[1] "Sun Feb 18 15:37:00 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

2: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

3: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

4: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-1.76,  : 
  could not find function "glm.nb"

5: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-1.76,  : 
  could not find function "glm.nb"

6: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-1.76,  : 
  could not find function "glm.nb"

7: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

8: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

9: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-1.76,  : 
  could not find function "glm.nb"

2: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-1.76,  : 
  could not find function "glm.nb"

3: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-1.76,  : 
  could not find function "glm.nb"

4: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

5: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

6: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

7: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

8: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

9: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
[1] "failed"                   "failed"                  
[3] "Sun Feb 18 15:37:05 2018" "basic sum C1 + C2"       
[5] "ignore"                   "none"                    
[7] "YeoJohnson"               "glm.nb"                  
Loading required package: plyr
Loading required package: mboost
Loading required package: parallel
Loading required package: stabs
This is mboost 2.8-1. See 'package?mboost' and 'news(package  = "mboost")'
for a complete list of changes.


Attaching package: 'mboost'

The following object is masked from 'package:MLmetrics':

    AUC

The following object is masked from 'package:ggplot2':

    %+%

Boosted Generalized Linear Model 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  mstop  prune  RMSE        Rsquared   Selected
   33    yes    0.23693800  0.9998263          
   41    yes    0.15306895  0.9998976          
  165    yes    0.01118192  0.9999413          
  221    no     0.01117381  0.9999413          
  312    no     0.01117335  0.9999413          
  375    no     0.01117335  0.9999413          
  447    yes    0.01117331  0.9999413  *       
  542    no     0.01117335  0.9999413          
  587    no     0.01117335  0.9999413          
  594    no     0.01117335  0.9999413          
  672    yes    0.01117331  0.9999413          
  684    yes    0.01117331  0.9999413          
  758    no     0.01117335  0.9999413          
  806    yes    0.01117331  0.9999413          
  815    yes    0.01117331  0.9999413          
  890    yes    0.01117331  0.9999413          
  905    no     0.01117335  0.9999413          
  938    no     0.01117335  0.9999413          
  996    yes    0.01117331  0.9999413          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mstop = 447 and prune = yes.
[1] "Sun Feb 18 15:37:14 2018"
Loading required package: glmnet
Loading required package: Matrix
Loaded glmnet 2.0-10

glmnet 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  alpha       lambda       RMSE        Rsquared   Selected
  0.03217786  0.023439592  0.04342052  0.9999413          
  0.04031423  0.021795177  0.04323610  0.9999413          
  0.16490472  0.040210673  0.04382183  0.9999413          
  0.22069274  2.167095650  1.07439934  0.9997267          
  0.31140591  0.634182698  0.53701312  0.9999252          
  0.37458565  0.367730775  0.35742966  0.9999341          
  0.44623383  0.065829138  0.07483852  0.9999411          
  0.54159895  3.300859488  1.43967217        NaN          
  0.58637961  5.191355286  1.43967217        NaN          
  0.59386839  0.764675803  0.76916019  0.9997217          
  0.67104215  0.003015359  0.04430436  0.9999411          
  0.68316399  0.035219941  0.04519232  0.9999411          
  0.75772429  0.201693923  0.25120416  0.9999298          
  0.80555573  0.002431224  0.04263623  0.9999411          
  0.81496939  0.003621458  0.04230810  0.9999411  *       
  0.88970091  0.003785842  0.04354998  0.9999410          
  0.90477584  1.019840151  1.29052292  0.9796299          
  0.93701204  0.561655360  0.74716577  0.9995221          
  0.93746527  7.110151657  1.43967217        NaN          
  0.99538198  0.026882137  0.04421579  0.9999409          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were alpha = 0.8149694 and lambda
 = 0.003621458.
[1] "Sun Feb 18 15:37:17 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
[1] "failed"                   "failed"                  
[3] "Sun Feb 18 15:38:36 2018" "basic sum C1 + C2"       
[5] "ignore"                   "none"                    
[7] "YeoJohnson"               "glmnet_h2o"              
Loading required package: MASS
Start:  AIC=-3106.7
.outcome ~ V2 + V3

       Df Deviance     AIC
<none>        0.06 -3106.7
- V2    1   457.48  1380.5
- V3    1   522.22  1446.7
Start:  AIC=-3086.59
.outcome ~ V2 + V3

       Df Deviance     AIC
<none>        0.06 -3086.6
- V3    1   491.47  1418.2
- V2    1   504.46  1431.2
Start:  AIC=-3069.33
.outcome ~ V2 + V3

       Df Deviance     AIC
<none>        0.06 -3069.3
- V2    1   490.85  1421.2
- V3    1   516.41  1446.7
Start:  AIC=-4631.35
.outcome ~ V2 + V3

       Df Deviance     AIC
<none>        0.09 -4631.4
- V2    1   727.23  2114.9
- V3    1   765.84  2153.8
Generalized Linear Model with Stepwise Feature Selection 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results:

  RMSE        Rsquared 
  0.01117335  0.9999413

[1] "Sun Feb 18 15:38:37 2018"
Loading required package: fastICA
Independent Component Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  n.comp  RMSE        Rsquared   Selected
  1       0.02750485  0.9995146          
  2       0.01117335  0.9999413  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was n.comp = 2.
[1] "Sun Feb 18 15:38:40 2018"
Loading required package: pls

Attaching package: 'pls'

The following object is masked from 'package:caret':

    R2

The following object is masked from 'package:stats':

    loadings

Partial Least Squares 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  ncomp  RMSE        Rsquared   Selected
  1      0.04834657  0.9984507          
  2      0.01117335  0.9999413  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 2.
[1] "Sun Feb 18 15:38:42 2018"
k-Nearest Neighbors 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  k    RMSE       Rsquared   Selected
    9  0.1740466  0.9894785  *       
   11  0.1871856  0.9884472          
   42  0.3331993  0.9776930          
   56  0.3816344  0.9741072          
   78  0.4546076  0.9692372          
   94  0.5024519  0.9655902          
  112  0.5508231  0.9620666          
  136  0.6143185  0.9570790          
  147  0.6421974  0.9545560          
  149  0.6465309  0.9541857          
  168  0.6902802  0.9501759          
  171  0.6971994  0.9495677          
  190  0.7376278  0.9460647          
  202  0.7629811  0.9441013          
  204  0.7670997  0.9437323          
  223  0.8063533  0.9398828          
  227  0.8147146  0.9394123          
  235  0.8305084  0.9379239          
  249  0.8582182  0.9344485          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was k = 9.
[1] "Sun Feb 18 15:38:46 2018"
Loading required package: KRLS
## KRLS Package for Kernel-based Regularized Least Squares.

## See Hainmueller and Hazlett (2014) for details.

Polynomial Kernel Regularized Least Squares 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  lambda        degree  RMSE         Rsquared    Selected
  1.448403e-05  2       1635.826342  0.10222410          
  1.590637e-05  2       1489.529712  0.10222546          
  6.676112e-05  2        354.713042  0.10227414          
  1.269006e-04  3        158.006169  0.01959890          
  3.606032e-04  3         55.685812  0.01942267          
  7.463254e-04  2         31.541631  0.10292794          
  1.702822e-03  2         13.744335  0.10385856          
  5.104989e-03  3          4.253911  0.01633926          
  8.548660e-03  3          2.801397  0.01470305          
  9.318413e-03  3          2.633565  0.01440778          
  2.265744e-02  1         16.687565  0.02162562          
  2.605073e-02  2          1.499541  0.13114522          
  6.146409e-02  2          1.348730  0.17971848  *       
  1.066053e-01  1          3.814582  0.01797421          
  1.188083e-01  1          3.476558  0.01771586          
  2.808695e-01  1          1.909788  0.02091490          
  3.341021e-01  3          1.358780  0.52048110          
  4.842395e-01  3          1.356638  0.60433084          
  4.867729e-01  3          1.356615  0.60523779          
  9.482218e-01  2          1.359464  0.75029667          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 0.06146409 and degree = 2.
[1] "Sun Feb 18 15:39:59 2018"

 Average Marginal Effects:
 
       V2        V3 
0.9996662 1.0002572 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9931986 0.9934175
50% 1.0001662 1.0017802
75% 1.0072889 1.0085342

 Average Marginal Effects:
 
       V2        V3 
0.9441442 0.9348782 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9653129 0.9709783
50% 0.9964782 0.9985968
75% 1.0149709 1.0131764

 Average Marginal Effects:
 
       V2        V3 
0.9988033 0.9986974 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9907203 0.9913686
50% 0.9988907 0.9995518
75% 1.0082241 1.0072773

 Average Marginal Effects:
 
       V2        V3 
0.9897280 0.9881053 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9885895 0.9880603
50% 0.9994940 0.9991051
75% 1.0106361 1.0096737

 Average Marginal Effects:
 
       V2        V3 
0.7650971 0.7637411 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.7615871 0.7607285
50% 0.7692680 0.7680753
75% 0.7725761 0.7711707

 Average Marginal Effects:
 
       V2        V3 
0.7178890 0.7161583 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.7176090 0.7159216
50% 0.7182293 0.7165117
75% 0.7184940 0.7167574

 Average Marginal Effects:
 
       V2        V3 
0.9986017 0.9985681 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9915860 0.9914492
50% 0.9993696 1.0002643
75% 1.0076783 1.0087529

 Average Marginal Effects:
 
       V2        V3 
0.6025979 0.6005561 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.6022279 0.6002452
50% 0.6030431 0.6010221
75% 0.6033979 0.6013476

 Average Marginal Effects:
 
       V2        V3 
0.9904203 0.9935309 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9606325 0.9663727
50% 1.0151851 1.0191214
75% 1.0428722 1.0482363

 Average Marginal Effects:
 
       V2        V3 
0.9980099 0.9992696 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9874484 0.9894029
50% 1.0035678 1.0068448
75% 1.0157123 1.0169969

 Average Marginal Effects:
 
       V2        V3 
0.9957310 0.9948673 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9861166 0.9860631
50% 0.9975113 0.9973033
75% 1.0148300 1.0150604

 Average Marginal Effects:
 
       V2        V3 
0.9996322 1.0002408 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9931705 0.993458
50% 1.0003467 1.002037
75% 1.0073156 1.008371

 Average Marginal Effects:
 
       V2        V3 
0.9770897 0.9774079 

 Quartiles of Marginal Effects:
 
          V2       V3
25% 0.995059 1.001042
50% 1.008635 1.013596
75% 1.024487 1.028565

 Average Marginal Effects:
 
       V2        V3 
0.9710693 0.9764344 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9167241 0.9264253
50% 1.0238220 1.0303009
75% 1.0691350 1.0797712

 Average Marginal Effects:
 
       V2        V3 
0.9995208 1.0001617 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9931423 0.993516
50% 1.0006718 1.002297
75% 1.0076676 1.009083

 Average Marginal Effects:
 
       V2        V3 
0.9991386 0.9992379 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9925954 0.9920418
50% 0.9991329 0.9990289
75% 1.0074528 1.0077594

 Average Marginal Effects:
 
       V2        V3 
0.9940221 0.9931308 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9901385 0.9910751
50% 0.9989823 1.0007842
75% 1.0116004 1.0092027

 Average Marginal Effects:
 
       V2        V3 
0.7741873 0.7727106 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.7737363 0.7723214
50% 0.7747358 0.7732800
75% 0.7751668 0.7736828

 Average Marginal Effects:
 
       V2        V3 
0.9373834 0.9369772 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9369234 0.9365246
50% 0.9379213 0.9375457
75% 0.9383699 0.9379656

 Average Marginal Effects:
 
       V2        V3 
0.9925032 0.9933056 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9850004 0.9867076
50% 0.9983486 0.9996935
75% 1.0066078 1.0071665

 Average Marginal Effects:
 
       V2        V3 
0.9999721 1.0000766 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9934395 0.9929205
50% 1.0003910 1.0006616
75% 1.0074348 1.0085168

 Average Marginal Effects:
 
       V2        V3 
0.9492302 0.9449879 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9748796 0.9810118
50% 0.9954989 0.9965822
75% 1.0072620 1.0049895

 Average Marginal Effects:
 
       V2        V3 
0.9998624 0.9992254 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9934098 0.9913600
50% 1.0004740 0.9985847
75% 1.0072965 1.0083759

 Average Marginal Effects:
 
       V2        V3 
0.9932133 0.9893330 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9860696 0.9891742
50% 1.0037544 1.0002331
75% 1.0127237 1.0116624

 Average Marginal Effects:
 
       V2        V3 
0.7531032 0.7538215 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.7496519 0.7512616
50% 0.7568512 0.7580016
75% 0.7600914 0.7609549

 Average Marginal Effects:
 
       V2        V3 
0.7042546 0.7043889 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.7039847 0.7041831
50% 0.7045588 0.7047259
75% 0.7048159 0.7049660

 Average Marginal Effects:
 
       V2        V3 
0.9990824 0.9984244 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9925247 0.9919546
50% 1.0013863 1.0001069
75% 1.0089132 1.0095718

 Average Marginal Effects:
 
       V2        V3 
0.5868019 0.5869068 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.5864512 0.5866332
50% 0.5872020 0.5873470
75% 0.5875357 0.5876642

 Average Marginal Effects:
 
       V2        V3 
0.9913411 0.9981943 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9685577 0.9819857
50% 1.0174594 1.0214450
75% 1.0414054 1.0399823

 Average Marginal Effects:
 
      V2       V3 
0.998603 1.000562 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9886959 0.9930706
50% 1.0053119 1.0083372
75% 1.0151870 1.0159542

 Average Marginal Effects:
 
       V2        V3 
0.9984709 0.9972876 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9877013 0.9872935
50% 1.0019311 0.9993361
75% 1.0165925 1.0126975

 Average Marginal Effects:
 
       V2        V3 
0.9999447 1.0000964 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9933496 0.9930149
50% 1.0004419 1.0009694
75% 1.0076866 1.0087577

 Average Marginal Effects:
 
       V2        V3 
0.9794021 0.9835547 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9954407 0.9860588
50% 1.0107279 1.0016274
75% 1.0194930 1.0248072

 Average Marginal Effects:
 
       V2        V3 
0.9709084 0.9830658 

 Quartiles of Marginal Effects:
 
          V2        V3
25% 0.919085 0.9582768
50% 1.026207 1.0233250
75% 1.069641 1.0581130

 Average Marginal Effects:
 
       V2        V3 
0.9998399 1.0001069 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9928458 0.9929373
50% 1.0006588 1.0012424
75% 1.0079299 1.0089888

 Average Marginal Effects:
 
       V2        V3 
0.9999792 0.9994195 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9938526 0.9908768
50% 1.0007772 0.9995826
75% 1.0078429 1.0080533

 Average Marginal Effects:
 
       V2        V3 
0.9966292 0.9941441 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9909846 0.989053
50% 1.0023731 1.002261
75% 1.0093530 1.011605

 Average Marginal Effects:
 
       V2        V3 
0.7622680 0.7624517 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.7618252 0.7621209
50% 0.7627607 0.7630004
75% 0.7631827 0.7633876

 Average Marginal Effects:
 
       V2        V3 
0.9328432 0.9330938 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9323855 0.9327231
50% 0.9333204 0.9336520
75% 0.9337798 0.9340483

 Average Marginal Effects:
 
       V2        V3 
0.9922675 0.9941956 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9854362 0.9889524
50% 0.9980709 1.0021211
75% 1.0055568 1.0076537

 Average Marginal Effects:
 
      V2       V3 
1.000076 1.000534 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9934420 0.9936615
50% 0.9997437 1.0010250
75% 1.0068631 1.0091547

 Average Marginal Effects:
 
       V2        V3 
0.9503557 0.9474791 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9798459 0.9778641
50% 0.9953843 0.9948331
75% 1.0053730 1.0059836

 Average Marginal Effects:
 
       V2        V3 
1.0000025 0.9994779 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9919945 0.9905918
50% 1.0001979 0.9990530
75% 1.0081916 1.0075263

 Average Marginal Effects:
 
       V2        V3 
0.9910196 0.9898142 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9899833 0.9905009
50% 1.0012444 0.9999677
75% 1.0074010 1.0092166

 Average Marginal Effects:
 
       V2        V3 
0.7646161 0.7654040 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.7614163 0.7623945
50% 0.7684334 0.7698828
75% 0.7721115 0.7729395

 Average Marginal Effects:
 
       V2        V3 
0.7172134 0.7174053 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.7169557 0.7171694
50% 0.7175263 0.7177669
75% 0.7178194 0.7180149

 Average Marginal Effects:
 
       V2        V3 
0.9986024 0.9986306 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9913523 0.9918953
50% 0.9999928 0.9995073
75% 1.0085085 1.0097598

 Average Marginal Effects:
 
       V2        V3 
0.6018853 0.6018286 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.6015379 0.6015222
50% 0.6023024 0.6023042
75% 0.6026869 0.6026337

 Average Marginal Effects:
 
       V2        V3 
0.9924234 0.9978805 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9701365 0.9750064
50% 1.0139560 1.0236089
75% 1.0404097 1.0430310

 Average Marginal Effects:
 
       V2        V3 
0.9989649 1.0007139 

 Quartiles of Marginal Effects:
 
          V2        V3
25% 0.991278 0.9926773
50% 1.003995 1.0086348
75% 1.014194 1.0165263

 Average Marginal Effects:
 
       V2        V3 
0.9990365 0.9976201 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9874220 0.9861625
50% 0.9994771 0.9998979
75% 1.0150947 1.0116171

 Average Marginal Effects:
 
      V2       V3 
1.000054 1.000548 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9932496 0.9936384
50% 1.0000072 1.0011183
75% 1.0069980 1.0094011

 Average Marginal Effects:
 
       V2        V3 
0.9808745 0.9816731 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9925211 0.9869365
50% 1.0064451 1.0004841
75% 1.0194422 1.0192399

 Average Marginal Effects:
 
       V2        V3 
0.9734628 0.9831332 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9346243 0.9531244
50% 1.0214894 1.0281200
75% 1.0643658 1.0643215

 Average Marginal Effects:
 
       V2        V3 
0.9999911 1.0005324 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9932418 0.9936165
50% 1.0002401 1.0014714
75% 1.0068729 1.0093463

 Average Marginal Effects:
 
      V2       V3 
1.000118 1.000221 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9924842 0.9919873
50% 1.0008301 0.9997089
75% 1.0080354 1.0089125

 Average Marginal Effects:
 
       V2        V3 
0.9948680 0.9946021 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9886079 0.9896933
50% 1.0002372 0.9985052
75% 1.0096998 1.0105493

 Average Marginal Effects:
 
       V2        V3 
0.7735639 0.7739378 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.7731498 0.7735479
50% 0.7740664 0.7745293
75% 0.7745452 0.7749243

 Average Marginal Effects:
 
       V2        V3 
0.9369833 0.9379451 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9365501 0.9375520
50% 0.9374738 0.9385361
75% 0.9379643 0.9389330

 Average Marginal Effects:
 
       V2        V3 
0.9929647 0.9951314 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9871939 0.9885275
50% 0.9980299 1.0032430
75% 1.0067178 1.0092018

 Average Marginal Effects:
 
      V2       V3 
1.000004 1.000301 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9935887 0.9931543
50% 1.0002214 1.0007923
75% 1.0069896 1.0083973
Radial Basis Function Kernel Regularized Least Squares 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  lambda        sigma        RMSE         Rsquared   Selected
  1.448403e-05   388.331553  0.004157236  0.9999917  *       
  1.590637e-05   418.303465  0.004361936  0.9999908          
  6.676112e-05   223.678331  0.005056148  0.9999875          
  1.269006e-04     3.799840  0.027890086  0.9994530          
  3.606032e-04    13.342582  0.010582584  0.9999355          
  7.463254e-04    23.289626  0.012425419  0.9999144          
  1.702822e-03   135.147868  0.017101548  0.9998522          
  5.104989e-03     2.471564  0.070643137  0.9964319          
  8.548660e-03     1.555843  0.109430961  0.9911671          
  9.318413e-03    11.019920  0.032552334  0.9993521          
  2.265744e-02  3158.816987  0.093417227  0.9999404          
  2.605073e-02   256.124052  0.018628989  0.9998733          
  6.146409e-02    43.030110  0.056685629  0.9983916          
  1.066053e-01  3936.481247  0.413975494  0.9999393          
  1.188083e-01  2619.507114  0.331996864  0.9999387          
  2.808695e-01  2503.305581  0.581096502  0.9999374          
  3.341021e-01     8.210243  0.090915064  0.9954527          
  4.842395e-01    15.106076  0.114002181  0.9936064          
  4.867729e-01     1.128092  0.228134646  0.9694729          
  9.482218e-01   337.576186  0.344924161  0.9998694          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 1.448403e-05 and sigma
 = 388.3316.
[1] "Sun Feb 18 15:42:07 2018"
Loading required package: lars
Loaded lars 1.2

Least Angle Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  fraction    RMSE        Rsquared   Selected
  0.03217786  1.39511285  0.7729957          
  0.04031423  1.38343530  0.8276088          
  0.16490472  1.20384503  0.9889576          
  0.22069274  1.12343038  0.9945734          
  0.31140591  0.99267474  0.9978351          
  0.37458565  0.90160753  0.9987415          
  0.44623383  0.79833562  0.9992793          
  0.54159895  0.66088297  0.9996339          
  0.58637961  0.59634177  0.9997280          
  0.59386839  0.58554861  0.9997408          
  0.67104215  0.47432731  0.9998385          
  0.68316399  0.45685863  0.9998493          
  0.75772429  0.34942132  0.9998977          
  0.80555573  0.28051543  0.9999165          
  0.81496939  0.26695674  0.9999194          
  0.88970091  0.15938637  0.9999348          
  0.90477584  0.13771653  0.9999366          
  0.93701204  0.09147449  0.9999394          
  0.93746527  0.09082599  0.9999395          
  0.99538198  0.01298916  0.9999413  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was fraction = 0.995382.
[1] "Sun Feb 18 15:42:09 2018"
Least Angle Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  step  RMSE      Rsquared   Selected
  1     1.439672        NaN          
  2     1.416607  0.5023048  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was step = 2.
[1] "Sun Feb 18 15:42:11 2018"
Loading required package: elasticnet
The lasso 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  fraction    RMSE        Rsquared   Selected
  0.03217786  1.39511285  0.7729957          
  0.04031423  1.38343530  0.8276088          
  0.16490472  1.20384503  0.9889576          
  0.22069274  1.12343038  0.9945734          
  0.31140591  0.99267474  0.9978351          
  0.37458565  0.90160753  0.9987415          
  0.44623383  0.79833562  0.9992793          
  0.54159895  0.66088297  0.9996339          
  0.58637961  0.59634177  0.9997280          
  0.59386839  0.58554861  0.9997408          
  0.67104215  0.47432731  0.9998385          
  0.68316399  0.45685863  0.9998493          
  0.75772429  0.34942132  0.9998977          
  0.80555573  0.28051543  0.9999165          
  0.81496939  0.26695674  0.9999194          
  0.88970091  0.15938637  0.9999348          
  0.90477584  0.13771653  0.9999366          
  0.93701204  0.09147449  0.9999394          
  0.93746527  0.09082599  0.9999395          
  0.99538198  0.01298916  0.9999413  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was fraction = 0.995382.
[1] "Sun Feb 18 15:42:13 2018"
Loading required package: leaps
Linear Regression with Backwards Selection 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  nvmax  RMSE        Rsquared   Selected
  1      1.01957510  0.5023048          
  2      0.01117335  0.9999413  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sun Feb 18 15:42:15 2018"
Linear Regression with Forward Selection 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  nvmax  RMSE        Rsquared   Selected
  1      1.01957510  0.5023048          
  2      0.01117335  0.9999413  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sun Feb 18 15:42:16 2018"
Linear Regression with Stepwise Selection 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  nvmax  RMSE        Rsquared   Selected
  1      1.01957510  0.5023048          
  2      0.01117335  0.9999413  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sun Feb 18 15:42:18 2018"
Linear Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results:

  RMSE        Rsquared 
  0.01117335  0.9999413

Tuning parameter 'intercept' was held constant at a value of TRUE
[1] "Sun Feb 18 15:42:19 2018"
Start:  AIC=-4527.64
.outcome ~ V2 + V3

       Df Sum of Sq    RSS     AIC
<none>                0.06 -4527.6
- V2    1    457.42 457.48   -40.4
- V3    1    522.17 522.22    25.7
Start:  AIC=-4510.37
.outcome ~ V2 + V3

       Df Sum of Sq    RSS     AIC
<none>                0.06 -4510.4
- V3    1    491.41 491.47    -5.6
- V2    1    504.40 504.46     7.4
Start:  AIC=-4498.78
.outcome ~ V2 + V3

       Df Sum of Sq    RSS     AIC
<none>                0.06 -4498.8
- V2    1    490.78 490.85    -8.3
- V3    1    516.35 516.41    17.2
Start:  AIC=-6767.44
.outcome ~ V2 + V3

       Df Sum of Sq    RSS     AIC
<none>                0.09 -6767.4
- V2    1    727.14 727.23   -21.2
- V3    1    765.74 765.84    17.7
Linear Regression with Stepwise Selection 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results:

  RMSE        Rsquared 
  0.01117335  0.9999413

[1] "Sun Feb 18 15:42:21 2018"
Loading required package: logicFS
Loading required package: LogicReg
Loading required package: survival

Attaching package: 'survival'

The following object is masked from 'package:caret':

    cluster

Loading required package: mcbiopi
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :15    NA's   :15   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
[1] "failed"                   "failed"                  
[3] "Sun Feb 18 15:42:25 2018" "basic sum C1 + C2"       
[5] "ignore"                   "none"                    
[7] "YeoJohnson"               "logicBag"                
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
[1] "failed"                   "failed"                  
[3] "Sun Feb 18 15:42:30 2018" "basic sum C1 + C2"       
[5] "ignore"                   "none"                    
[7] "YeoJohnson"               "logreg"                  
Loading required package: RWeka
Error: package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Loading required package: RWeka
Error: package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Loading required package: RWeka
Error: package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
[1] "failed"                   "failed"                  
[3] "Sun Feb 18 15:42:31 2018" "basic sum C1 + C2"       
[5] "ignore"                   "none"                    
[7] "YeoJohnson"               "M5"                      
Loading required package: RWeka
Error: package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Loading required package: RWeka
Error: package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Loading required package: RWeka
Error: package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
[1] "failed"                   "failed"                  
[3] "Sun Feb 18 15:42:32 2018" "basic sum C1 + C2"       
[5] "ignore"                   "none"                    
[7] "YeoJohnson"               "M5Rules"                 
Loading required package: msaenet
Multi-Step Adaptive MCP-Net 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  alphas      nsteps  scale      RMSE        Rsquared   Selected
  0.07896008   5      0.6456895  0.02013549  0.9999413          
  0.08628281   5      2.5509855  0.01887886  0.9999413          
  0.19841425   5      2.8184765  0.01260025  0.9999413          
  0.24862346   9      3.7835582  0.01199424  0.9999413          
  0.33026532   8      2.0005769  0.01155164  0.9999413          
  0.38712708   7      2.8960486  0.01140613  0.9999413          
  0.45161045   6      2.8801457  0.01131149  0.9999413          
  0.53743906  10      1.1018308  0.01124350  0.9999413          
  0.57774165  10      0.3318914  0.01122422  0.9999413          
  0.58448155   8      3.0335864  0.01122150  0.9999413          
  0.65393794   3      3.6253757  0.01120033  0.9999413          
  0.66484759   5      1.4442506  0.01119790  0.9999413          
  0.73195186   7      0.6708837  0.01118652  0.9999413          
  0.77500015   2      2.4060556  0.01118175  0.9999413          
  0.78347245   3      0.9395519  0.01118099  0.9999413          
  0.85073082   3      1.5031007  0.01117656  0.9999413          
  0.86429826   8      0.7989267  0.01117595  0.9999413          
  0.89331084   8      3.3178529  0.01117491  0.9999413          
  0.89371875  10      2.1710388  0.01117490  0.9999413          
  0.94584379   5      1.4801753  0.01117377  0.9999413  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were alphas = 0.9458438, nsteps = 5
 and scale = 1.480175.
[1] "Sun Feb 18 15:42:41 2018"
Loading required package: nnet
# weights:  5
initial  value 1141.980017 
iter  10 value 1050.051475
iter  20 value 1048.973017
iter  30 value 1048.911464
iter  40 value 1048.384435
iter  50 value 692.106358
iter  60 value 690.053463
iter  70 value 687.877473
iter  80 value 684.085002
final  value 683.967093 
converged
# weights:  77
initial  value 1196.804107 
iter  10 value 924.602158
iter  20 value 893.129907
iter  30 value 891.753709
iter  40 value 891.509982
iter  50 value 891.471654
iter  60 value 891.463358
final  value 891.463272 
converged
# weights:  33
initial  value 1296.643763 
iter  10 value 767.458186
iter  20 value 693.671452
iter  30 value 690.043562
iter  40 value 689.971380
iter  50 value 689.952739
iter  60 value 689.943198
iter  70 value 689.942706
final  value 689.942627 
converged
# weights:  49
initial  value 1376.689120 
iter  10 value 895.627274
iter  20 value 843.937662
iter  30 value 840.228435
iter  40 value 840.130568
iter  50 value 840.026735
iter  60 value 840.015178
iter  60 value 840.015176
iter  60 value 840.015175
final  value 840.015175 
converged
# weights:  81
initial  value 1160.405581 
iter  10 value 1049.341591
iter  20 value 685.390596
iter  30 value 683.390506
iter  40 value 683.120988
iter  50 value 682.908181
iter  60 value 682.839074
iter  70 value 682.810514
iter  80 value 682.783465
iter  90 value 682.772732
iter 100 value 682.763610
final  value 682.763610 
stopped after 100 iterations
# weights:  69
initial  value 1161.306365 
iter  10 value 1048.983976
final  value 1048.832252 
converged
# weights:  21
initial  value 1237.238001 
iter  10 value 925.733474
iter  20 value 753.559195
iter  30 value 753.072624
final  value 753.072505 
converged
# weights:  73
initial  value 1196.712056 
iter  10 value 1049.298949
iter  20 value 1048.835579
iter  30 value 1048.832951
iter  40 value 1048.832782
iter  50 value 1048.832569
iter  60 value 1048.832291
iter  70 value 1048.831909
iter  80 value 1048.831350
iter  90 value 1048.830446
iter 100 value 1048.828729
final  value 1048.828729 
stopped after 100 iterations
# weights:  65
initial  value 1284.417954 
iter  10 value 1044.485130
iter  20 value 691.253277
iter  30 value 686.439333
iter  40 value 686.149313
iter  50 value 685.999077
iter  60 value 685.951670
iter  70 value 685.929836
iter  80 value 685.918999
iter  90 value 685.909986
iter 100 value 685.906624
final  value 685.906624 
stopped after 100 iterations
# weights:  37
initial  value 1077.938688 
iter  10 value 1047.275395
iter  20 value 684.197420
iter  30 value 683.917764
iter  40 value 683.873614
iter  50 value 683.858124
iter  60 value 683.836357
iter  70 value 683.829293
iter  80 value 683.827261
iter  90 value 683.825610
iter 100 value 683.824502
final  value 683.824502 
stopped after 100 iterations
# weights:  49
initial  value 1296.768136 
iter  10 value 970.729087
iter  20 value 719.540152
iter  30 value 704.863269
iter  40 value 702.044907
iter  50 value 700.614797
iter  60 value 700.255584
iter  70 value 700.148041
iter  80 value 700.121572
iter  90 value 700.101090
iter 100 value 700.094921
final  value 700.094921 
stopped after 100 iterations
# weights:  5
initial  value 1233.097085 
iter  10 value 1050.522274
iter  20 value 1049.038856
iter  30 value 1048.963475
iter  40 value 1048.896118
iter  50 value 1048.823106
iter  60 value 1042.546185
iter  70 value 684.465026
iter  80 value 683.918289
iter  90 value 683.903358
final  value 683.903066 
converged
# weights:  77
initial  value 1353.436278 
iter  10 value 743.394233
iter  20 value 708.474076
iter  30 value 706.878579
iter  40 value 706.836714
iter  50 value 706.833202
final  value 706.832501 
converged
# weights:  77
initial  value 1267.471819 
iter  10 value 1026.044493
iter  20 value 703.346092
iter  30 value 695.299626
iter  40 value 694.264881
iter  50 value 694.119776
iter  60 value 694.055476
iter  70 value 694.052626
iter  80 value 694.052085
iter  90 value 694.051890
iter 100 value 694.051395
final  value 694.051395 
stopped after 100 iterations
# weights:  17
initial  value 1097.676117 
iter  10 value 1049.106148
iter  20 value 702.781110
iter  30 value 690.047468
iter  40 value 684.505212
iter  50 value 683.598781
iter  60 value 683.241796
iter  70 value 683.179601
iter  80 value 683.168446
iter  90 value 683.156031
iter 100 value 683.154319
final  value 683.154319 
stopped after 100 iterations
# weights:  29
initial  value 1375.674030 
iter  10 value 751.099038
iter  20 value 701.146863
iter  30 value 698.093680
iter  40 value 697.699502
iter  50 value 697.518393
iter  60 value 697.368984
final  value 697.368413 
converged
# weights:  45
initial  value 1364.260577 
iter  10 value 844.750972
iter  20 value 786.701591
iter  30 value 782.561236
iter  40 value 781.767068
iter  50 value 781.324227
iter  60 value 781.028021
final  value 781.025106 
converged
# weights:  69
initial  value 1083.805293 
iter  10 value 1048.953484
iter  20 value 1048.838761
iter  30 value 1048.835065
iter  40 value 1048.789022
iter  50 value 683.256958
iter  60 value 682.899836
iter  70 value 682.327595
iter  80 value 682.216088
iter  90 value 682.205559
iter 100 value 682.196054
final  value 682.196054 
stopped after 100 iterations
# weights:  57
initial  value 1110.390999 
iter  10 value 1048.973245
iter  20 value 1048.701663
iter  30 value 690.662600
iter  40 value 688.810079
iter  50 value 683.268181
iter  60 value 682.317425
iter  70 value 682.241719
iter  80 value 682.232074
iter  90 value 682.218342
iter 100 value 682.210062
final  value 682.210062 
stopped after 100 iterations
# weights:  57
initial  value 1174.510142 
iter  10 value 1049.415270
iter  20 value 693.204216
iter  30 value 688.283693
iter  40 value 684.676857
iter  50 value 683.338559
iter  60 value 683.156726
iter  70 value 683.100739
iter  80 value 683.070556
iter  90 value 683.033228
iter 100 value 683.005030
final  value 683.005030 
stopped after 100 iterations
# weights:  5
initial  value 1061.634270 
iter  10 value 998.898303
iter  20 value 655.722006
iter  30 value 653.108310
iter  40 value 647.837033
iter  50 value 645.296730
iter  60 value 643.781962
iter  70 value 635.270358
iter  80 value 635.010650
iter  90 value 634.989567
final  value 634.984672 
converged
# weights:  77
initial  value 1356.632800 
iter  10 value 946.447409
iter  20 value 844.885345
iter  30 value 843.628952
iter  40 value 843.568741
final  value 843.568195 
converged
# weights:  33
initial  value 1307.634393 
iter  10 value 994.263983
iter  20 value 649.509506
iter  30 value 641.546022
iter  40 value 641.031515
iter  50 value 640.969368
iter  60 value 640.964222
final  value 640.963774 
converged
# weights:  49
initial  value 1046.749826 
iter  10 value 812.892643
iter  20 value 792.814463
iter  30 value 791.073096
iter  40 value 791.045925
iter  50 value 791.044136
final  value 791.043738 
converged
# weights:  81
initial  value 1152.741076 
iter  10 value 999.285542
iter  20 value 638.856950
iter  30 value 634.389536
iter  40 value 634.285950
iter  50 value 634.175446
iter  60 value 633.995153
iter  70 value 633.893880
iter  80 value 633.866227
iter  90 value 633.856108
iter 100 value 633.848663
final  value 633.848663 
stopped after 100 iterations
# weights:  69
initial  value 1264.636562 
iter  10 value 998.708621
iter  20 value 998.461180
final  value 998.460326 
converged
# weights:  21
initial  value 1113.720413 
iter  10 value 714.597667
iter  20 value 702.076144
iter  30 value 701.181277
final  value 701.172874 
converged
# weights:  73
initial  value 1013.538923 
iter  10 value 992.179054
iter  20 value 643.327098
iter  30 value 642.235807
iter  40 value 634.783056
iter  50 value 634.474925
iter  60 value 634.204306
iter  70 value 634.107697
iter  80 value 634.014969
iter  90 value 633.859169
iter 100 value 633.732823
final  value 633.732823 
stopped after 100 iterations
# weights:  65
initial  value 1012.953821 
iter  10 value 1000.699003
iter  20 value 688.147073
iter  30 value 637.960568
iter  40 value 637.060765
iter  50 value 636.892382
iter  60 value 636.861261
iter  70 value 636.844396
iter  80 value 636.837692
iter  90 value 636.836946
final  value 636.836922 
converged
# weights:  37
initial  value 1061.787175 
iter  10 value 999.261655
iter  20 value 676.121236
iter  30 value 644.234719
iter  40 value 636.824424
iter  50 value 635.433572
iter  60 value 635.078976
iter  70 value 634.938435
iter  80 value 634.865104
iter  90 value 634.821574
final  value 634.811528 
converged
# weights:  49
initial  value 1105.813260 
iter  10 value 729.936570
iter  20 value 654.157471
iter  30 value 651.415747
iter  40 value 651.214806
iter  50 value 651.161845
iter  60 value 651.124034
final  value 651.123802 
converged
# weights:  5
initial  value 1119.223064 
iter  10 value 999.624239
iter  20 value 998.539876
iter  30 value 645.841615
iter  40 value 635.472308
final  value 634.885142 
converged
# weights:  77
initial  value 1089.285702 
iter  10 value 932.024924
iter  20 value 686.586497
iter  30 value 660.774309
iter  40 value 658.199818
iter  50 value 658.064117
iter  60 value 658.060364
iter  60 value 658.060359
iter  60 value 658.060359
final  value 658.060359 
converged
# weights:  77
initial  value 1270.725944 
iter  10 value 819.808128
iter  20 value 648.381777
iter  30 value 645.473771
iter  40 value 645.207385
iter  50 value 645.133023
iter  60 value 645.111788
iter  70 value 645.093204
iter  80 value 645.076168
final  value 645.074892 
converged
# weights:  17
initial  value 1153.971348 
iter  10 value 998.977271
iter  20 value 892.630557
iter  30 value 637.850490
iter  40 value 636.813263
iter  50 value 635.452688
iter  60 value 634.589510
iter  70 value 634.264206
iter  80 value 634.210298
iter  90 value 634.201262
iter 100 value 634.194458
final  value 634.194458 
stopped after 100 iterations
# weights:  29
initial  value 1263.505016 
iter  10 value 821.946740
iter  20 value 652.767258
iter  30 value 649.013951
iter  40 value 648.750897
final  value 648.749905 
converged
# weights:  45
initial  value 1207.770437 
iter  10 value 907.593318
iter  20 value 755.737525
iter  30 value 734.498595
iter  40 value 732.867785
iter  50 value 732.501142
iter  60 value 732.119046
iter  70 value 732.111756
iter  70 value 732.111753
final  value 732.111649 
converged
# weights:  69
initial  value 1009.364028 
iter  10 value 998.509096
iter  20 value 640.391433
iter  30 value 634.423520
iter  40 value 633.370284
iter  50 value 633.257980
iter  60 value 633.252683
iter  70 value 633.250316
iter  80 value 633.243004
iter  90 value 633.238006
iter 100 value 633.234556
final  value 633.234556 
stopped after 100 iterations
# weights:  57
initial  value 1300.875180 
iter  10 value 998.676692
iter  20 value 998.462446
final  value 998.462430 
converged
# weights:  57
initial  value 1045.415358 
iter  10 value 999.157851
iter  20 value 636.354015
iter  30 value 634.481583
iter  40 value 634.334854
iter  50 value 634.187416
iter  60 value 634.085193
iter  70 value 634.049906
iter  80 value 634.036030
iter  90 value 634.031524
iter 100 value 634.028736
final  value 634.028736 
stopped after 100 iterations
# weights:  5
initial  value 1205.246953 
iter  10 value 1073.874477
iter  20 value 1072.483771
iter  30 value 1072.387704
iter  40 value 1072.326441
iter  50 value 1070.740248
iter  60 value 721.874471
iter  70 value 717.977371
iter  80 value 709.939852
iter  90 value 709.498444
iter 100 value 709.389715
final  value 709.389715 
stopped after 100 iterations
# weights:  77
initial  value 1162.000237 
iter  10 value 919.708788
iter  20 value 917.297109
iter  30 value 916.747202
iter  40 value 916.731241
final  value 916.731220 
converged
# weights:  33
initial  value 1176.865353 
iter  10 value 808.285031
iter  20 value 718.146405
iter  30 value 716.130038
iter  40 value 715.583472
iter  50 value 715.370008
iter  60 value 715.353882
iter  70 value 715.350685
iter  80 value 715.347681
final  value 715.347606 
converged
# weights:  49
initial  value 1391.943579 
iter  10 value 895.041359
iter  20 value 868.592857
iter  30 value 865.778138
iter  40 value 865.505148
iter  50 value 865.500538
final  value 865.500455 
converged
# weights:  81
initial  value 1289.619324 
iter  10 value 1072.492995
iter  20 value 709.215178
iter  30 value 708.662592
iter  40 value 708.399593
iter  50 value 708.243697
iter  60 value 708.184157
iter  70 value 708.161629
iter  80 value 708.148764
iter  90 value 708.141354
iter 100 value 708.139650
final  value 708.139650 
stopped after 100 iterations
# weights:  69
initial  value 1242.051838 
iter  10 value 1072.512088
iter  20 value 1072.312888
iter  30 value 795.102129
iter  40 value 718.955741
iter  50 value 717.740416
iter  60 value 717.243504
iter  70 value 716.130438
iter  80 value 713.744045
iter  90 value 710.389753
iter 100 value 709.268292
final  value 709.268292 
stopped after 100 iterations
# weights:  21
initial  value 1317.777486 
iter  10 value 910.543799
iter  20 value 784.412031
iter  30 value 776.848409
iter  40 value 775.562461
iter  50 value 775.536910
iter  50 value 775.536907
iter  50 value 775.536905
final  value 775.536905 
converged
# weights:  73
initial  value 1193.130388 
iter  10 value 1072.653658
iter  20 value 1072.315161
iter  30 value 1062.863563
iter  40 value 708.500133
iter  50 value 708.279745
iter  60 value 707.722077
iter  70 value 707.644845
iter  80 value 707.618707
iter  90 value 707.608847
iter 100 value 707.605244
final  value 707.605244 
stopped after 100 iterations
# weights:  65
initial  value 1362.896037 
iter  10 value 764.290602
iter  20 value 711.879616
iter  30 value 711.334136
iter  40 value 711.271271
iter  50 value 711.246687
iter  60 value 711.234173
iter  70 value 711.228224
iter  80 value 711.219855
iter  90 value 711.215685
iter 100 value 711.215104
final  value 711.215104 
stopped after 100 iterations
# weights:  37
initial  value 1138.060822 
iter  10 value 1073.411865
iter  20 value 732.516443
iter  30 value 713.829016
iter  40 value 710.753996
iter  50 value 709.926118
iter  60 value 709.501870
iter  70 value 709.312146
iter  80 value 709.250078
iter  90 value 709.215430
iter 100 value 709.189915
final  value 709.189915 
stopped after 100 iterations
# weights:  49
initial  value 1255.579441 
iter  10 value 766.802605
iter  20 value 726.700806
iter  30 value 725.568365
iter  40 value 725.542832
iter  50 value 725.534283
final  value 725.534254 
converged
# weights:  5
initial  value 1314.471730 
iter  10 value 1074.056337
iter  20 value 1072.469619
iter  30 value 1071.991731
iter  40 value 718.665030
iter  50 value 717.218936
iter  60 value 710.432029
iter  70 value 709.591123
iter  80 value 709.449317
iter  90 value 709.325015
iter 100 value 709.279815
final  value 709.279815 
stopped after 100 iterations
# weights:  77
initial  value 1120.770044 
iter  10 value 853.041416
iter  20 value 752.005537
iter  30 value 733.472090
iter  40 value 732.488579
iter  50 value 732.370908
iter  60 value 732.351660
iter  70 value 732.350987
iter  70 value 732.350983
iter  70 value 732.350982
final  value 732.350982 
converged
# weights:  77
initial  value 1347.158671 
iter  10 value 997.586341
iter  20 value 726.607017
iter  30 value 720.036924
iter  40 value 719.565357
iter  50 value 719.487649
iter  60 value 719.482654
iter  70 value 719.477937
iter  80 value 719.477415
iter  80 value 719.477409
iter  80 value 719.477409
final  value 719.477409 
converged
# weights:  17
initial  value 1194.712849 
iter  10 value 1077.043321
iter  20 value 1069.356752
iter  30 value 734.560280
iter  40 value 723.205245
iter  50 value 714.015182
iter  60 value 709.235294
iter  70 value 708.683088
iter  80 value 708.585436
iter  90 value 708.576414
iter 100 value 708.570652
final  value 708.570652 
stopped after 100 iterations
# weights:  29
initial  value 1189.515970 
iter  10 value 769.697893
iter  20 value 724.399241
iter  30 value 722.873040
iter  40 value 722.798217
iter  50 value 722.795728
final  value 722.795705 
converged
# weights:  45
initial  value 1457.543289 
iter  10 value 869.874370
iter  20 value 814.329406
iter  30 value 806.948046
iter  40 value 806.556761
iter  50 value 806.540021
final  value 806.538865 
converged
# weights:  69
initial  value 1172.886149 
iter  10 value 1072.748254
iter  20 value 1072.315643
iter  30 value 1072.302756
iter  40 value 1043.796337
iter  50 value 716.013519
iter  60 value 709.760321
iter  70 value 707.893738
iter  80 value 707.704018
iter  90 value 707.687013
iter 100 value 707.664291
final  value 707.664291 
stopped after 100 iterations
# weights:  57
initial  value 1156.944899 
iter  10 value 1072.410443
iter  20 value 1072.313788
iter  30 value 1072.313586
iter  40 value 1072.313327
iter  50 value 1072.312974
iter  60 value 1072.312471
iter  70 value 1072.311662
iter  80 value 1072.310168
iter  90 value 1072.306478
iter 100 value 1072.284993
final  value 1072.284993 
stopped after 100 iterations
# weights:  57
initial  value 1346.882652 
iter  10 value 1073.258654
iter  20 value 759.781000
iter  30 value 711.417496
iter  40 value 709.027028
iter  50 value 708.696724
iter  60 value 708.506649
iter  70 value 708.427625
iter  80 value 708.389294
iter  90 value 708.377440
iter 100 value 708.372668
final  value 708.372668 
stopped after 100 iterations
# weights:  69
initial  value 1755.602208 
iter  10 value 1560.850104
iter  20 value 1559.811645
final  value 1559.804789 
converged
Neural Network 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  size  decay         RMSE      Rsquared   Selected
   1    1.168861e-03  1.159514  0.7044121          
   1    1.306760e-03  1.159507  0.7039015          
   4    2.989263e-03  1.158687  0.7175111          
   5    1.350057e+00  1.174912  0.7956699          
   7    2.051824e-01  1.161294  0.7373659          
   8    8.897259e-02  1.160046  0.7230363          
   9    6.364821e-03  1.159303  0.7083234          
  11    2.573608e+00  1.183625  0.8307696          
  12    2.733584e-01  1.161656  0.7464665          
  12    5.152896e+00  1.204897  0.8694169          
  14    5.632657e-05  1.345917  0.7549913          
  14    2.439633e-03  1.158665  0.7173397          
  16    3.542757e-02  1.159480  0.7147487          
  17    4.048906e-05  1.341854  0.8212747          
  17    7.458829e-05  1.158474  0.7177195  *       
  18    7.984159e-05  1.252111  0.7576435          
  19    1.703227e-01  1.160523  0.7369069          
  19    4.250754e-01  1.162999  0.7593896          
  19    8.346093e+00  1.226356  0.8991885          
  20    1.612287e-03  1.158592  0.7177051          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 17 and decay = 7.458829e-05.
[1] "Sun Feb 18 15:42:52 2018"
Loading required package: nnls
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  predictions failed for Fold1: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments

2: In eval(xpr, envir = envir) :
  predictions failed for Fold2: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments

3: In eval(xpr, envir = envir) :
  predictions failed for Fold3: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments

4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  predictions failed for Fold1: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments

2: In eval(xpr, envir = envir) :
  predictions failed for Fold2: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments

3: In eval(xpr, envir = envir) :
  predictions failed for Fold3: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments

4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
[1] "failed"                   "failed"                  
[3] "Sun Feb 18 15:42:55 2018" "basic sum C1 + C2"       
[5] "ignore"                   "none"                    
[7] "YeoJohnson"               "nnls"                    
Loading required package: ordinalNet
Error : The tuning parameter grid should have columns alpha, criteria, link
Error : The tuning parameter grid should have columns alpha, criteria, link
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :12    NA's   :12   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
[1] "failed"                   "failed"                  
[3] "Sun Feb 18 15:42:58 2018" "basic sum C1 + C2"       
[5] "ignore"                   "none"                    
[7] "YeoJohnson"               "ordinalNet"              
Loading required package: e1071
Loading required package: randomForest
randomForest 4.6-12
Type rfNews() to see new features/changes/bug fixes.

Attaching package: 'randomForest'

The following object is masked from 'package:ggplot2':

    margin

Parallel Random Forest 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared  Selected
  1     0.1992017  0.985997          
  2     0.1719175  0.988262  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 2.
[1] "Sun Feb 18 15:43:06 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: Warning message:
executing %dopar% sequentially: no parallel backend registered 
# weights:  5
initial  value 1158.820136 
iter  10 value 1049.877352
iter  20 value 1048.936648
iter  30 value 1048.858107
iter  40 value 1045.566605
iter  50 value 702.079665
iter  60 value 693.940512
iter  70 value 690.334698
iter  80 value 685.471103
iter  90 value 684.731591
iter 100 value 684.411062
final  value 684.411062 
stopped after 100 iterations
# weights:  77
initial  value 1191.832206 
iter  10 value 921.837570
iter  20 value 893.288858
iter  30 value 891.004501
iter  40 value 890.810109
iter  50 value 890.744063
final  value 890.742615 
converged
# weights:  33
initial  value 1305.864878 
iter  10 value 704.724335
iter  20 value 691.966985
iter  30 value 690.365862
iter  40 value 690.210079
iter  50 value 690.182271
iter  60 value 690.178984
final  value 690.178933 
converged
# weights:  49
initial  value 1344.535101 
iter  10 value 865.327101
iter  20 value 841.377724
iter  30 value 839.180988
iter  40 value 838.716837
iter  50 value 838.686026
final  value 838.685721 
converged
# weights:  81
initial  value 1154.096488 
iter  10 value 1049.486448
iter  20 value 720.429530
iter  30 value 704.132776
iter  40 value 694.090033
iter  50 value 689.397678
iter  60 value 685.883778
iter  70 value 684.514981
iter  80 value 683.579882
iter  90 value 683.072241
iter 100 value 682.929118
final  value 682.929118 
stopped after 100 iterations
# weights:  69
initial  value 1248.202659 
iter  10 value 1049.014730
iter  20 value 1048.832410
iter  30 value 722.509488
iter  40 value 692.921184
iter  50 value 691.962319
iter  60 value 691.770078
iter  70 value 691.615546
iter  80 value 684.097575
iter  90 value 682.519785
iter 100 value 682.240493
final  value 682.240493 
stopped after 100 iterations
# weights:  21
initial  value 1226.323839 
iter  10 value 873.616774
iter  20 value 766.049842
iter  30 value 754.172919
iter  40 value 752.837533
final  value 752.828258 
converged
# weights:  73
initial  value 1119.133010 
iter  10 value 1049.249020
iter  20 value 1048.834735
iter  20 value 1048.834730
final  value 1048.834730 
converged
# weights:  65
initial  value 1305.605874 
iter  10 value 705.248998
iter  20 value 686.722024
iter  30 value 685.911456
iter  40 value 685.813521
iter  50 value 685.796148
iter  60 value 685.789265
iter  70 value 685.785168
iter  80 value 685.784594
iter  80 value 685.784588
iter  80 value 685.784588
final  value 685.784588 
converged
# weights:  37
initial  value 1077.368721 
iter  10 value 1049.629447
iter  20 value 697.759112
iter  30 value 685.927470
iter  40 value 684.363903
iter  50 value 683.951039
iter  60 value 683.814724
iter  70 value 683.793778
iter  80 value 683.787221
iter  90 value 683.783915
iter 100 value 683.779247
final  value 683.779247 
stopped after 100 iterations
# weights:  49
initial  value 1341.353801 
iter  10 value 882.106213
iter  20 value 702.218698
iter  30 value 700.043481
iter  40 value 700.020767
iter  50 value 700.020621
final  value 700.020582 
converged
# weights:  5
initial  value 1211.841476 
iter  10 value 1050.345430
iter  20 value 1049.014777
iter  30 value 1040.339556
iter  40 value 698.182617
iter  50 value 695.927186
iter  60 value 689.243438
iter  70 value 685.045273
iter  80 value 683.947315
final  value 683.903043 
converged
# weights:  77
initial  value 1273.188673 
iter  10 value 773.782743
iter  20 value 709.782616
iter  30 value 706.904588
iter  40 value 706.752790
iter  50 value 706.737160
final  value 706.730238 
converged
# weights:  77
initial  value 1325.384745 
iter  10 value 882.621507
iter  20 value 698.145330
iter  30 value 694.961992
iter  40 value 694.389876
iter  50 value 694.186650
iter  60 value 694.115285
iter  70 value 694.058226
iter  80 value 694.003290
iter  90 value 693.998929
iter 100 value 693.997023
final  value 693.997023 
stopped after 100 iterations
# weights:  17
initial  value 1085.466635 
iter  10 value 1049.047395
iter  20 value 691.774010
iter  30 value 686.691869
iter  40 value 684.470173
iter  50 value 683.568312
iter  60 value 683.295688
iter  70 value 683.257194
iter  80 value 683.242700
iter  90 value 683.233532
iter 100 value 683.225223
final  value 683.225223 
stopped after 100 iterations
# weights:  29
initial  value 1365.693475 
iter  10 value 724.699566
iter  20 value 698.496542
iter  30 value 697.399898
iter  40 value 697.319518
final  value 697.318200 
converged
# weights:  45
initial  value 1282.345753 
iter  10 value 846.645890
iter  20 value 782.733106
iter  30 value 780.545404
iter  40 value 780.446539
iter  50 value 780.431088
iter  60 value 780.423492
final  value 780.423428 
converged
# weights:  69
initial  value 1059.217870 
iter  10 value 1048.969142
iter  20 value 1048.829972
iter  30 value 684.565947
iter  40 value 683.108066
iter  50 value 683.015368
iter  60 value 682.564361
iter  70 value 682.245622
iter  80 value 682.228199
iter  90 value 682.205489
iter 100 value 682.198039
final  value 682.198039 
stopped after 100 iterations
# weights:  57
initial  value 1154.642744 
iter  10 value 1048.989749
final  value 1048.834419 
converged
# weights:  57
initial  value 1144.905945 
iter  10 value 1049.385895
iter  20 value 688.218808
iter  30 value 683.930036
iter  40 value 683.268890
iter  50 value 683.129485
iter  60 value 683.081530
iter  70 value 683.047283
iter  80 value 683.027849
iter  90 value 683.011713
iter 100 value 682.995230
final  value 682.995230 
stopped after 100 iterations
# weights:  5
initial  value 1066.100189 
iter  10 value 998.934132
iter  20 value 998.602226
iter  30 value 998.406925
iter  40 value 637.956810
iter  50 value 635.289665
iter  60 value 634.948405
final  value 634.947656 
converged
# weights:  77
initial  value 1423.067376 
iter  10 value 849.395960
iter  20 value 844.201402
iter  30 value 843.427539
iter  40 value 843.358732
final  value 843.357460 
converged
# weights:  33
initial  value 1291.756221 
iter  10 value 718.237363
iter  20 value 648.772781
iter  30 value 641.864271
iter  40 value 641.287779
iter  50 value 641.014218
iter  60 value 640.943301
iter  70 value 640.938652
final  value 640.938564 
converged
# weights:  49
initial  value 1050.114378 
iter  10 value 808.792062
iter  20 value 793.481773
iter  30 value 791.616044
iter  40 value 791.556654
iter  50 value 791.542841
final  value 791.542555 
converged
# weights:  81
initial  value 1177.606234 
iter  10 value 999.310520
iter  20 value 929.895203
iter  30 value 647.487645
iter  40 value 639.934303
iter  50 value 637.348923
iter  60 value 635.587412
iter  70 value 634.489869
iter  80 value 634.162897
iter  90 value 633.966637
iter 100 value 633.863275
final  value 633.863275 
stopped after 100 iterations
# weights:  69
initial  value 1251.121350 
iter  10 value 998.748750
iter  20 value 998.461643
iter  30 value 956.360426
iter  40 value 634.308008
iter  50 value 634.062972
iter  60 value 634.021958
iter  70 value 633.902415
iter  80 value 633.344389
iter  90 value 633.284828
iter 100 value 633.268220
final  value 633.268220 
stopped after 100 iterations
# weights:  21
initial  value 1124.005337 
iter  10 value 720.395887
iter  20 value 702.004890
iter  30 value 701.112544
final  value 701.096721 
converged
# weights:  73
initial  value 1034.684491 
iter  10 value 998.509808
iter  20 value 635.074623
iter  30 value 633.682029
iter  40 value 633.383153
iter  50 value 633.293123
iter  60 value 633.279407
iter  70 value 633.271621
iter  80 value 633.262551
iter  90 value 633.252440
iter 100 value 633.246399
final  value 633.246399 
stopped after 100 iterations
# weights:  65
initial  value 1038.345084 
iter  10 value 1001.989782
iter  20 value 669.887188
iter  30 value 639.630382
iter  40 value 637.395633
iter  50 value 637.172892
iter  60 value 637.028421
iter  70 value 636.885847
iter  80 value 636.825028
iter  90 value 636.803107
iter 100 value 636.794659
final  value 636.794659 
stopped after 100 iterations
# weights:  37
initial  value 1061.704988 
iter  10 value 999.204520
iter  20 value 660.626710
iter  30 value 646.347000
iter  40 value 636.890793
iter  50 value 635.202611
iter  60 value 634.862734
iter  70 value 634.794462
iter  80 value 634.778880
iter  90 value 634.772249
iter 100 value 634.768182
final  value 634.768182 
stopped after 100 iterations
# weights:  49
initial  value 1080.780190 
iter  10 value 804.570574
iter  20 value 651.994977
iter  30 value 651.279213
iter  40 value 651.127851
iter  50 value 651.098954
final  value 651.098813 
converged
# weights:  5
initial  value 1116.856106 
iter  10 value 999.447152
iter  20 value 998.571854
iter  30 value 820.905448
iter  40 value 635.803454
iter  50 value 634.892416
final  value 634.883919 
converged
# weights:  77
initial  value 1117.008615 
iter  10 value 795.032364
iter  20 value 669.014961
iter  30 value 658.174577
iter  40 value 657.891758
iter  50 value 657.872393
final  value 657.872311 
converged
# weights:  77
initial  value 1250.926154 
iter  10 value 792.543200
iter  20 value 646.279822
iter  30 value 645.189274
iter  40 value 645.080133
iter  50 value 645.063969
iter  60 value 645.057948
final  value 645.057679 
converged
# weights:  17
initial  value 1154.870118 
iter  10 value 998.977765
iter  20 value 955.587449
iter  30 value 643.222376
iter  40 value 635.891180
iter  50 value 634.552122
iter  60 value 634.296854
iter  70 value 634.221627
iter  80 value 634.211394
iter  90 value 634.209511
iter 100 value 634.209101
final  value 634.209101 
stopped after 100 iterations
# weights:  29
initial  value 1231.276999 
iter  10 value 783.719577
iter  20 value 658.645572
iter  30 value 649.299489
iter  40 value 648.559932
iter  50 value 648.420447
iter  60 value 648.391898
iter  70 value 648.390002
final  value 648.389432 
converged
# weights:  45
initial  value 1182.513488 
iter  10 value 803.184681
iter  20 value 738.260502
iter  30 value 732.369513
iter  40 value 732.000451
iter  50 value 731.995982
iter  60 value 731.993239
final  value 731.992545 
converged
# weights:  69
initial  value 1021.315786 
iter  10 value 998.513146
iter  20 value 635.622711
iter  30 value 634.047196
iter  40 value 633.814266
iter  50 value 633.313079
iter  60 value 633.279959
iter  70 value 633.268562
iter  80 value 633.259172
iter  90 value 633.251942
iter 100 value 633.242177
final  value 633.242177 
stopped after 100 iterations
# weights:  57
initial  value 1292.584732 
iter  10 value 998.684677
iter  20 value 998.462378
iter  20 value 998.462375
iter  20 value 998.462373
final  value 998.462373 
converged
# weights:  57
initial  value 1041.118467 
iter  10 value 999.132636
iter  20 value 998.542183
iter  30 value 635.677019
iter  40 value 634.384857
iter  50 value 634.314950
iter  60 value 634.220484
iter  70 value 634.113301
iter  80 value 634.072807
iter  90 value 634.049893
iter 100 value 634.040372
final  value 634.040372 
stopped after 100 iterations
# weights:  5
initial  value 1189.847060 
iter  10 value 1073.529229
iter  20 value 1072.506610
iter  30 value 1072.326808
iter  40 value 1069.152970
iter  50 value 715.377410
iter  60 value 710.725454
iter  70 value 709.945843
iter  80 value 709.602679
iter  90 value 709.339556
final  value 709.339298 
converged
# weights:  77
initial  value 1265.396555 
iter  10 value 955.061335
iter  20 value 921.918855
iter  30 value 917.420810
iter  40 value 917.125880
iter  50 value 917.113989
iter  60 value 917.109895
final  value 917.109332 
converged
# weights:  33
initial  value 1187.837754 
iter  10 value 799.497932
iter  20 value 720.677519
iter  30 value 715.815144
iter  40 value 715.645254
iter  50 value 715.642178
iter  60 value 715.639293
final  value 715.639033 
converged
# weights:  49
initial  value 1385.426634 
iter  10 value 904.431403
iter  20 value 870.550946
iter  30 value 867.016520
iter  40 value 866.497208
iter  50 value 865.859414
iter  60 value 865.832629
final  value 865.832579 
converged
# weights:  81
initial  value 1266.904165 
iter  10 value 850.010578
iter  20 value 728.642298
iter  30 value 717.845185
iter  40 value 713.076826
iter  50 value 710.298540
iter  60 value 709.187203
iter  70 value 708.596052
iter  80 value 708.339277
iter  90 value 708.248657
iter 100 value 708.204307
final  value 708.204307 
stopped after 100 iterations
# weights:  69
initial  value 1208.106372 
iter  10 value 1072.491662
iter  20 value 1072.311713
iter  30 value 1072.311220
iter  40 value 1072.310457
iter  50 value 1072.309059
iter  60 value 1072.305592
iter  70 value 1072.285266
iter  80 value 717.020503
iter  90 value 710.202340
iter 100 value 707.601629
final  value 707.601629 
stopped after 100 iterations
# weights:  21
initial  value 1310.435359 
iter  10 value 905.431633
iter  20 value 791.186455
iter  30 value 780.387412
iter  40 value 780.198440
iter  50 value 776.819557
iter  60 value 775.758532
final  value 775.757840 
converged
# weights:  73
initial  value 1187.824762 
iter  10 value 1072.649531
iter  20 value 1072.315537
iter  30 value 711.733833
iter  40 value 707.997183
iter  50 value 707.657826
iter  60 value 707.636948
final  value 707.627680 
converged
# weights:  65
initial  value 1445.691010 
iter  10 value 1080.178588
iter  20 value 729.777752
iter  30 value 712.447643
iter  40 value 711.520218
iter  50 value 711.353933
iter  60 value 711.277945
iter  70 value 711.238220
iter  80 value 711.217080
iter  90 value 711.212914
iter 100 value 711.211986
final  value 711.211986 
stopped after 100 iterations
# weights:  37
initial  value 1081.941883 
iter  10 value 742.604228
iter  20 value 714.208675
iter  30 value 710.510678
iter  40 value 709.519284
iter  50 value 709.307896
iter  60 value 709.237085
iter  70 value 709.201226
iter  80 value 709.191771
iter  90 value 709.169032
iter 100 value 709.162913
final  value 709.162913 
stopped after 100 iterations
# weights:  49
initial  value 1253.159503 
iter  10 value 818.537345
iter  20 value 728.203118
iter  30 value 725.761876
iter  40 value 725.609378
iter  50 value 725.605216
final  value 725.605141 
converged
# weights:  5
initial  value 1311.281125 
iter  10 value 1073.940853
iter  20 value 1072.464285
iter  30 value 718.711074
iter  40 value 711.825832
iter  50 value 710.707311
iter  60 value 710.014325
iter  70 value 709.284931
final  value 709.275739 
converged
# weights:  77
initial  value 1120.895533 
iter  10 value 958.359695
iter  20 value 755.238140
iter  30 value 735.493526
iter  40 value 733.351604
iter  50 value 732.735340
iter  60 value 732.553828
iter  70 value 732.430539
iter  80 value 732.389829
iter  90 value 732.382714
iter 100 value 732.377964
final  value 732.377964 
stopped after 100 iterations
# weights:  77
initial  value 1358.803474 
iter  10 value 950.180627
iter  20 value 721.127838
iter  30 value 719.737497
iter  40 value 719.582081
iter  50 value 719.544403
iter  60 value 719.538160
iter  70 value 719.529879
final  value 719.529752 
converged
# weights:  17
initial  value 1189.316506 
iter  10 value 1072.804456
iter  20 value 809.759624
iter  30 value 709.959821
iter  40 value 709.417419
iter  50 value 708.924305
iter  60 value 708.821430
iter  70 value 708.808295
iter  80 value 708.805617
iter  90 value 708.799982
iter 100 value 708.793948
final  value 708.793948 
stopped after 100 iterations
# weights:  29
initial  value 1252.155550 
iter  10 value 836.173250
iter  20 value 736.074689
iter  30 value 726.042989
iter  40 value 723.673243
iter  50 value 723.235446
iter  60 value 723.206622
iter  70 value 723.194478
final  value 723.190697 
converged
# weights:  45
initial  value 1470.008583 
iter  10 value 879.521959
iter  20 value 816.944993
iter  30 value 809.805631
iter  40 value 808.404340
iter  50 value 808.151292
iter  60 value 808.143273
iter  60 value 808.143268
iter  60 value 808.143268
final  value 808.143268 
converged
# weights:  69
initial  value 1188.442117 
iter  10 value 1072.795005
iter  20 value 1072.316182
iter  30 value 719.414733
iter  40 value 708.484452
iter  50 value 708.412568
iter  60 value 708.229978
iter  70 value 707.769843
iter  80 value 707.678598
iter  90 value 707.654994
iter 100 value 707.640975
final  value 707.640975 
stopped after 100 iterations
# weights:  57
initial  value 1180.610216 
iter  10 value 1072.436153
iter  20 value 1072.313077
iter  30 value 1072.312824
iter  40 value 1072.312483
iter  50 value 1072.311995
iter  60 value 1072.311236
iter  70 value 1072.309885
iter  80 value 1072.306827
iter  90 value 1072.294192
iter 100 value 845.999312
final  value 845.999312 
stopped after 100 iterations
# weights:  57
initial  value 1416.208063 
iter  10 value 1072.443598
iter  20 value 801.180081
iter  30 value 739.004850
iter  40 value 724.563748
iter  50 value 711.294058
iter  60 value 709.657794
iter  70 value 709.003572
iter  80 value 708.706526
iter  90 value 708.595748
iter 100 value 708.548745
final  value 708.548745 
stopped after 100 iterations
# weights:  69
initial  value 1770.997509 
iter  10 value 1560.880874
iter  20 value 1559.812000
iter  30 value 1559.802068
iter  40 value 1559.732939
iter  50 value 1014.453704
iter  60 value 1012.894939
iter  70 value 1012.774124
iter  80 value 1012.696699
iter  90 value 1011.775215
iter 100 value 1011.513007
final  value 1011.513007 
stopped after 100 iterations
Neural Networks with Feature Extraction 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  size  decay         RMSE      Rsquared   Selected
   1    1.168861e-03  1.159509  0.7040592          
   1    1.306760e-03  1.159497  0.7045587          
   4    2.989263e-03  1.158718  0.7173535          
   5    1.350057e+00  1.174918  0.7958263          
   7    2.051824e-01  1.161300  0.7374035          
   8    8.897259e-02  1.160095  0.7229427          
   9    6.364821e-03  1.159147  0.7112907          
  11    2.573608e+00  1.183775  0.8309271          
  12    2.733584e-01  1.161658  0.7465238          
  12    5.152896e+00  1.204836  0.8695211          
  14    5.632657e-05  1.381184  0.8212836          
  14    2.439633e-03  1.158688  0.7169840          
  16    3.542757e-02  1.159463  0.7148900          
  17    4.048906e-05  1.158531  0.7166401          
  17    7.458829e-05  1.158483  0.7171273  *       
  18    7.984159e-05  1.252098  0.6395185          
  19    1.703227e-01  1.160526  0.7369790          
  19    4.250754e-01  1.162985  0.7595471          
  19    8.346093e+00  1.226237  0.8990779          
  20    1.612287e-03  1.158608  0.7180873          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 17 and decay = 7.458829e-05.
[1] "Sun Feb 18 15:43:18 2018"
Principal Component Analysis 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results:

  RMSE       Rsquared 
  0.5836749  0.8111451

Tuning parameter 'ncomp' was held constant at a value of 1
[1] "Sun Feb 18 15:43:19 2018"
Installing package into 'C:/Users/irina grishina/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Loading required package: plsRglm
____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.197382774343714) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.190401711175218) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0735814098734409) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0202445906586945) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0300743066240102) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.118311377242208) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.14789107888937) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.154282200662419) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.141042416123673) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.180351411178708) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0290890208911151) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0250237767584622) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0795774818398058) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

Partial Least Squares Generalized Linear Models  

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  nt  alpha.pvals.expli  RMSE        Rsquared   Selected
  1   0.06892543         0.01171269  0.9999347          
  1   0.07053987         0.01171269  0.9999347          
  1   0.08251880         0.01171269  0.9999347          
  1   0.09345954         0.01171269  0.9999347          
  1   0.13164187         0.01171269  0.9999347          
  1   0.14373801         0.01171269  0.9999347          
  1   0.17101174         0.01171269  0.9999347          
  2   0.02024459         0.01117231  0.9999413          
  2   0.02502378         0.01117231  0.9999413          
  2   0.02908902         0.01117231  0.9999413          
  2   0.03007431         0.01117231  0.9999413          
  2   0.07358141         0.01117231  0.9999413          
  2   0.07957748         0.01117231  0.9999413          
  2   0.11831138         0.01117231  0.9999413          
  2   0.14104242         0.01117231  0.9999413          
  2   0.14789108         0.01117231  0.9999413          
  2   0.15428220         0.01117231  0.9999413          
  2   0.18035141         0.01117231  0.9999413          
  2   0.19040171         0.01117231  0.9999413          
  2   0.19738277         0.01117231  0.9999413  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nt = 2 and alpha.pvals.expli
 = 0.1973828.
[1] "Sun Feb 18 15:43:33 2018"
Projection Pursuit Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 501, 503 
Resampling results across tuning parameters:

  nterms  RMSE        Rsquared   Selected
   1      0.03985155  0.9981955          
   2      0.03835532  0.9982158  *       
   3      0.03839638  0.9982128          
   4      0.03839171  0.9982130          
   5      0.03838949  0.9982125          
   6      0.03842206  0.9982129          
   7      0.03842069  0.9982129          
   8      0.03841160  0.9982129          
   9      0.03841495  0.9982129          
  10      0.03841407  0.9982129          
  11      0.03841493  0.9982129          
  12      0.03842547  0.9982129          
  13      0.03842547  0.9982129          
  14      0.03842547  0.9982129          
  15      0.03842547  0.9982129          
  16      0.03842547  0.9982129          
  17      0.03842547  0.9982129          
  18      0.03842547  0.9982129          
  19      0.03842547  0.9982129          
  20      0.03842547  0.9982129          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nterms = 2.
[1] "Sun Feb 18 15:43:36 2018"
Installing package into 'C:/Users/irina grishina/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Loading required package: quantregForest
Loading required package: RColorBrewer


R version 3.4.3 (2017-11-30) -- "Kite-Eating Tree"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> options(repos=structure(c(CRAN="https://rweb.crmda.ku.edu/cran/")))
> ## capture messages and errors to a file.https://rweb.crmda.ku.edu/cran/
> #zz <- file("all.Rout", open="wt")https://cran.cnr.berkeley.edu
> #sink(zz, type="message") edit for rebaseless
> #chek for R package updates
> #try(log("a")) ## test --no-edit
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> task.subject<-"14th20hp3cv"
> pc.mlr<-c("ACE")#"ALTA","HOPPER"
> which.computer<-Sys.info()[['nodename']]
> out.file<-paste("out",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,".csv",sep="")
> importance.file<-paste("importance",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,sep="")
> 
> base.folder<-getwd()
> cpout.folder<-paste(base.folder,"/",which.computer,sep = "")
> setwd(cpout.folder)
> 
> if(length(which(list.files() == out.file))<1) write.table( "0.01,0.01,100,100,100,Wed Aug 02 16:37:25 2017,dummy,8,1,basic latent features,ignore,none,asis,1.12784979099243,random,333,53,adaptive_cv,16,5,2,2,19,0.0107744822639878,FALSE,,,,,,,,,," ,file =,out.file,  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,".csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,".csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,"mlr.csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,"mlr.csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> 
> cv.iters=3
> tuneLength=20
> tuneLength2=8
> normings=c("YeoJohnson","ICA", "centernscale","expoTrans","range01","asis","quantile")#,"centernscale"
> 
> gensTTesto<-c(56,53,4,12,13,14,15,20,45,54,55, 44,3,1,52)#,  51,c(4)#c(1:40)#c(5,10,11,13,14,15,16,17,18,19,20,21,24,28,38,39,40)
> gensTTest<-vector()
> write.table( t(gensTTesto),file = "initial tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
> try({
+   gensTTest<-t(read.csv("tasks to test.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   gensTTest<-as.vector(gensTTest)
+ })
> if(!exists("gensTTest")) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> gensTTesto<-c(gensTTesto[length(gensTTesto):1])
> if(length(gensTTest)<1) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> 
> 
> ########packages install check######
> 
> #list.of.packages <- c("caret","caretEnsemble","mlr","MLmetrics","tgp")
> #list.of.packages <- c("gower","dimRed","DEoptimR","caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> #new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> #if(length(new.packages)) install.packages(new.packages, dep = TRUE)
> 
> 
> #install.packages("mlr", dependencies = c("Depends", "Suggests"))
> #install.packages("caret", dependencies = c("Depends", "Suggests"))
> #install.packages("caret",repos = "http://cran.r-project.org",dependencies = c("Depends", "Imports", "Suggests"))
> #install.packages("SuperLearner", dependencies = c("Depends", "Suggests"))
> #install.packages("rattle", dependencies = c("Depends", "Suggests"))
> 
> # Load libraries
> #library(mlbench)
> 
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> #library(caretEnsemble)
> library(MLmetrics)

Attaching package: 'MLmetrics'

The following objects are masked from 'package:caret':

    MAE, RMSE

The following object is masked from 'package:base':

    Recall

> 
> ########error no repeat#########
> 
> 
> try({
+   before.last.alg<-as.matrix(read.csv("beforelast algorithm.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.alg<-as.matrix(read.csv("last algorithm tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   #write.table(paste(date(), last.alg,.Platform$OS.type,.Platform$r_arch,which.computer,sep=" "),file = "algos after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.alg==before.last.alg){print("algorithm may be broken")}
+   write.table(last.alg,file = "beforelast algorithm.csv",  quote = F, row.names = F,col.names = F)
+ })
[1] "algorithm may be broken"
> try({
+   before.last.tsk<-as.matrix(read.csv("beforelast task.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.tsk<-as.matrix(read.csv("last task tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   write.table(paste(date(),last.alg, last.tsk,cv.iters,tuneLength,.Platform$OS.type,.Platform$r_arch,which.computer,sep=","),file = "test after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.tsk==before.last.tsk){print("task may be broken")}
+   write.table(last.tsk,file = "beforelast task.csv",  quote = F, row.names = F,col.names = F)
+ })
> bad.models=c("spaccceeee")
> previous.fails<-(read.csv("test after which reset.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
> previous.fails<-previous.fails[previous.fails[,8]==which.computer,]
> lgf<-length(previous.fails[,2])
> for(lt in 2:lgf)  {
+   if(previous.fails[lt,2]==previous.fails[lt-1,2])  {
+     bad.models=union(bad.models,c(paste(previous.fails[lt,2])))  }}
> 
> #######not to redo a test function#####
> check.redundant<-function(df=df.previous.calcs,norming="asis",trans.y=1,withextra="missing",missingdata="leaveempty",datasource="mean" ,column.to.predict=200,allmodel="ctree")
+ {
+   for(intern in 1:length(df[,1])){
+     if((any(df[intern,] == norming, na.rm=T))&&
+        (any(df[intern,] == withextra, na.rm=T))&&
+        (any(df[intern,] == missingdata, na.rm=T))&&
+        (any(df[intern,] == datasource, na.rm=T))&&
+        (any(df[intern,] == column.to.predict, na.rm=T))&&
+        (any(df[intern,] == allmodel, na.rm=T))&&
+        (  (df[intern,9] == trans.y)))
+     {return(TRUE)}
+   }
+   return(FALSE)
+ }
> #####caret init#####
> best.ranged <- c("avNNet", "nnet", "pcaNNet", "glm.nb")
> best.asis <- c("svmLinear3", "relaxo", "superpc", "xgbTree")
> best.cns <- c("gam", "bam", "svmLinear2", "msaenet", "BstLm", "gbm") 
> 
> cv6hp5 <- c( "BstLm", "qrnn")#earth
> cv3hp32 <- c("Rborist", "pcaNNet", "SBC")
> cv7x5hp32 <- c("gbm", "krlsPoly", "kknn", "xgbLinear","RRF", "cubist", "rlm" )
> cv6hp5.avoid <- c("pcaNNet")
> cv3hp32.avoid <- c("glm.nb", "gamboost", "ctree2","glmboost", "leapSeq","ctree","svmLinear2")
> cv7x5hp32.avoid <- c("SBC","bagearthgcv","gcvearth","lmStepAIC","glmStepAIC","bridge","lm","glm","bayesglm","blassoAveraged","treebag","rpart1SE")
> 
> allmodels <- c("avNNet", "bagEarth", "bagEarthGCV",
+                "bayesglm", "bdk", "blackboost", "Boruta", "brnn", "BstLm" ,
+                "bstTree", "cforest", "ctree", "ctree2", "cubist", "DENFIS",
+                "dnn", "earth", "elm", "enet",   "evtree",
+                "extraTrees",  "gamLoess",  "gaussprLinear", "gaussprPoly", "gaussprRadial",
+                "gcvEarth","glm", "glmboost",  "icr", "kernelpls",
+                "kknn", "knn",  "krlsRadial", "lars" , "lasso",
+                "leapBackward", "leapForward", "leapSeq", "lm", "M5", "M5Rules",
+                "mlpWeightDecay", "neuralnet" , "partDSA",
+                "pcaNNet", "pcr", "penalized", "pls", "plsRglm", "ppr",
+                "qrf" , "ranger",  "rf")
> allmodels <- c("rlm", "rpart", "rpart2",
+                "RRF", "RRFglobal",  "simpls",
+                "svmLinear", "svmPoly", "svmRadial", "svmRadialCost",
+                "widekernelpls",  "xgbLinear",
+                "xgbTree")
> allmodels <- c("avNNet","BstLm","bstTree","cforest","ctree","ctree2",
+                "cubist","earth","enet","evtree","glmboost",
+                "icr","kernelpls","kknn","lasso","pcaNNet",
+                "pcr","pls","qrf","ranger","rf")
> 
> allmodels <- c("kknn", "cubist", "avNNet", "xgbLinear", "RRF", "pcaNNet","earth","nnet","gbm","enet","lasso","BstLm",
+                "foba", "leapBackward", "gcvEarth", "SBC","glm.nb","gamboost","ctree2","relaxo", 
+                "bartMachine","extraTrees","bam","gam","randomGLM")
> #allmodels <- c("bam")
> #allmodels <- c("rf")"rqlasso",, "xyf" "rvmPoly", "rvmRadial",    "spls", "superpc" ,   "treebag",  "svmLinear2",  "SBC",
> #allmodels <- c("bartMachine", "xgbLinear", "pcaNNet","svmLinear","glmnet","cforest","cubist","rf","ranger")"glmnet",
> #wow rfRules is really slow "rfRules","WM", takes 50min
> # brak everythig "rbfDDA","ridge","rqnc",
> # use "rf" to test all
> library(caret)
> allmodels <- unique(modelLookup()[modelLookup()$forReg,c(1)])
> #allmodels <-c("avNNet", "nnet", "pcaNNet",  "glm.nb", "gam" ,
> #              "bam","msaenet", "svmLinear2","svmLinear3",
> #              "relaxo",  "superpc", "xgbTree", "BstLm")
> #allmodels<- c("svmLinear","svmPoly","svmRadial")
> #library(doParallel); cl <- makeCluster(detectCores()); registerDoParallel(cl)
> #allmodels<-c("bartMachine","extraTrees")#,"randomGLM"
> 
> 
> adaptControl <- trainControl(method = "adaptive_cv",
+                              number = 7, repeats = 5,
+                              adaptive = list(min = 4, alpha = 0.05,
+                                              method = "gls", complete = FALSE),
+                              search = "random")
> adaptControl <-trainControl(method = "cv", number = cv.iters,  search = "random")
> simpleControl <- trainControl(method = "cv",
+                               number = cv.iters,
+                               search = "random")
> 
> 
> #########MLR init######
> #R.utils::gcDLLs()
> #list.of.packages <- c("ParamHelpers","devtools","mlrMBO","RJSONIO","plot3D","plotly")
> #install.packages("mlrMBO", dependencies = c("Depends", "Suggests"))
> list.of.packages <- c("caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> if(length(new.packages)) install.packages(new.packages, dep = TRUE)
Warning message:
packages 'logicFS', ' RWeka', 'rPython', 'rrf' are not available (for R version 3.4.3) 
> 
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> tuneLengthMLR<-tuneLength
> mlr.iters<-cv.iters
> #######data read process start#####
> seed.var =222+round(runif(1,min=0,max=100))
> column.to.predict=1
> print(date());
[1] "Sat Mar 10 16:39:51 2018"
> 
> setwd(base.folder)
> if(!exists("gen.count")){gen.count=56}
> gens.names<-as.matrix(read.table("gens names.csv", sep = ",",header = FALSE,row.names=1,fill=TRUE, quote="",dec="."))
> count.toy.data.passed<-1
> for(gend.data in gensTTest){
+   count.toy.data.passed<-count.toy.data.passed+1
+   setwd(base.folder)
+   data.source<-as.matrix(read.csv(paste("Generats/",gens.names[gend.data],".csv", sep = ""), sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   datasource<-gens.names[gend.data,1]
+   setwd(cpout.folder)
+   missingdatas=c("ignore")
+   for(missingdata in missingdatas){
+     withextras=c("none")
+     for(withextra in withextras){
+       ################data wrestling###############
+       
+       dependant.selection=complete.cases(data.source[,column.to.predict])
+       df.previous.calcs=as.data.frame(read.csv(file=out.file, header = FALSE, sep = ",", quote = "",
+                                                dec = ".", fill = TRUE, comment.char = ""))
+       unimportant.computations<-vector(mode = "logical",length=length(df.previous.calcs[,1])  )
+       for(intern in 1:length(df.previous.calcs[,1])){
+         if((any(df.previous.calcs[intern,] == withextra, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == missingdata, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == datasource, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == column.to.predict, na.rm=T)))
+         {unimportant.computations[intern]<-T}}
+       
+       df.previous.calcs<-df.previous.calcs[unimportant.computations,]
+       
+       #data.source=data.frame( data.source[,column.to.predict],data.source[,1:2], data.source[,4:(column.to.predict-1)], data.source[,(column.to.predict+1):length( data.source[1,])])
+       
+         for(norming in normings) {
+         for(trans.y in 1:2) {
+           df.toprocess=data.source
+           y.untransformed<-df.toprocess[,1]
+           
+           if(norming=="centernscale"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("center", "scale"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="range01"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="expoTrans"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("expoTrans"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="YeoJohnson"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("YeoJohnson"))#"center", "scale",
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           if((norming=="asis")&&(trans.y==2)){next}
+           
+           
+           ################preprocess###########
+           df.toprocess=data.frame(df.toprocess[dependant.selection,])
+           y.untransformed=y.untransformed[dependant.selection]
+           if(norming=="quantile"){
+             for(Clol in trans.y:length(data.source[1,])){
+               df.toprocess[,Clol]<- (rank(df.toprocess[,Clol],na.last = "keep",ties.method = "average")-1) }
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           loess.model<-loess(y.untransformed~ df.toprocess[,1],span = 0.21, degree = 1)
+   
+           #df.toprocess = data.frame(df.toprocess,)
+           nzv <- nearZeroVar(df.toprocess[,])#, saveMetrics= TRUE
+           #nzv[nzv$nzv,][1:10,]
+           if(length(nzv)>1){
+             df.toprocess = (df.toprocess[, -nzv])}
+           df.toprocess = signif(df.toprocess,digits = 3)
+           
+           seed.var =222+round(runif(1,min=0,max=100))
+           set.seed(seed.var)
+           inTrain <- createDataPartition(y = df.toprocess[,1],
+                                          p = .75,
+                                          list = FALSE)
+           training <- df.toprocess[ inTrain,]
+           testing  <- df.toprocess[-inTrain,]
+           write.table(df.toprocess,file = "sanity check 1.csv",  quote = F, row.names = F,col.names = F)
+           
+           ###########for all models#################
+           setwd(base.folder)
+           if(max(which.computer==pc.mlr)>0)
+             source("MLR part.R")
+           else
+             source("Caret part.R")
+           
+          setwd(cpout.folder)
+           if(norming == normings[length(normings)]){
+             if(count.toy.data.passed>length(gensTTest)){gensTTest<-c(gensTTesto)}
+             write.table( t(gensTTest[count.toy.data.passed:length(gensTTest)]),file = "tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
+             
+             }
+           
+         }
+       }
+     }
+   }
+   
+ }
Error : package arm is required
Error : package arm is required
Error : package arm is required
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 16:58:34 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "bayesglm"                       
t=100, m=1
t=200, m=1
t=300, m=3
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=3
t=800, m=3
t=900, m=1
t=100, m=3
t=200, m=4
t=300, m=3
t=400, m=1
t=500, m=7
t=600, m=2
t=700, m=2
t=800, m=3
t=900, m=1
t=100, m=3
t=200, m=3
t=300, m=6
t=400, m=4
t=500, m=5
t=600, m=1
t=700, m=2
t=800, m=1
t=900, m=2
t=100, m=2
t=200, m=3
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=6
t=800, m=2
t=900, m=2
The Bayesian lasso 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  sparsity    RMSE       Rsquared   MAE        Selected
  0.03726011  0.2536973  0.9828499  0.1957312          
  0.09538976  0.2536973  0.9828499  0.1957312          
  0.13628871  0.2536973  0.9828499  0.1957312          
  0.17376387  0.2545863  0.9827302  0.1973923          
  0.25579013  0.2525761  0.9830209  0.1960695          
  0.27824472  0.2522217  0.9830548  0.1948564          
  0.30463149  0.2502917  0.9833261  0.1940615          
  0.40082870  0.2502917  0.9833261  0.1940615          
  0.45832706  0.2502917  0.9833261  0.1940615          
  0.48257268  0.2502917  0.9833261  0.1940615          
  0.53837072  0.2450634  0.9841261  0.1882032          
  0.57844659  0.2450634  0.9841261  0.1882032          
  0.57993361  0.2450634  0.9841261  0.1882032          
  0.59947350  0.2450634  0.9841261  0.1882032          
  0.66151637  0.2450634  0.9841261  0.1882032          
  0.67110902  0.2450634  0.9841261  0.1882032          
  0.76452146  0.2450634  0.9841261  0.1882032          
  0.76630054  0.2450634  0.9841261  0.1882032          
  0.83536128  0.2450634  0.9841261  0.1882032          
  0.96014406  0.2450634  0.9841261  0.1882032  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was sparsity = 0.9601441.
[1] "Sat Mar 10 16:58:49 2018"
t=100, m=2
t=200, m=1
t=300, m=2
t=400, m=3
t=500, m=3
t=600, m=2
t=700, m=6
t=800, m=5
t=900, m=3
t=100, m=4
t=200, m=3
t=300, m=2
t=400, m=2
t=500, m=3
t=600, m=3
t=700, m=3
t=800, m=4
t=900, m=1
t=100, m=2
t=200, m=2
t=300, m=4
t=400, m=2
t=500, m=3
t=600, m=4
t=700, m=2
t=800, m=3
t=900, m=3
t=100, m=3
t=200, m=5
t=300, m=2
t=400, m=3
t=500, m=1
t=600, m=4
t=700, m=2
t=800, m=2
t=900, m=3
Bayesian Ridge Regression (Model Averaged) 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results:

  RMSE       Rsquared   MAE      
  0.2519794  0.9830109  0.1958058

[1] "Sat Mar 10 16:59:03 2018"
t=100, m=1
t=200, m=1
t=300, m=1
t=400, m=2
t=500, m=1
t=600, m=1
t=700, m=1
t=800, m=1
t=900, m=1
t=100, m=1
t=200, m=1
t=300, m=1
t=400, m=1
t=500, m=2
t=600, m=1
t=700, m=1
t=800, m=1
t=900, m=2
t=100, m=1
t=200, m=1
t=300, m=1
t=400, m=1
t=500, m=1
t=600, m=1
t=700, m=1
t=800, m=1
t=900, m=1
t=100, m=1
t=200, m=1
t=300, m=2
t=400, m=2
t=500, m=1
t=600, m=1
t=700, m=2
t=800, m=1
t=900, m=2
Bayesian Ridge Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results:

  RMSE       Rsquared   MAE      
  0.2471908  0.9837833  0.1924806

[1] "Sat Mar 10 16:59:15 2018"
Boosted Linear Model 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  nu          mstop  RMSE      Rsquared   MAE       Selected
  0.03169925  417    1.595533  0.5436289  1.356304          
  0.03260088  241    1.551590  0.6563868  1.345760  *       
  0.06137542  139    1.557713  0.6366958  1.346401          
  0.06739926  382    1.648542  0.4567105  1.371999          
  0.15172267  331    1.693617  0.4177426  1.381806          
  0.23347517  300    1.704741  0.4112945  1.386988          
  0.31328101   48    1.616565  0.5022484  1.359664          
  0.38096497  480    1.709765  0.4086071  1.389219          
  0.39176202  201    1.707902  0.4096119  1.388341          
  0.42911422  290    1.709672  0.4086510  1.389177          
  0.48492951  128    1.706004  0.4102570  1.387308          
  0.49217495   19    1.597359  0.5444267  1.360438          
  0.51102236  335    1.709766  0.4086067  1.389220          
  0.51453484   69    1.689710  0.4187986  1.380050          
  0.52568398  153    1.708707  0.4090362  1.388767          
  0.52781103   87    1.699180  0.4138388  1.384304          
  0.53392217  289    1.709762  0.4086090  1.389218          
  0.53720375  269    1.709756  0.4086113  1.389215          
  0.55683453  383    1.709769  0.4086055  1.389221          
  0.56384772  229    1.709737  0.4086183  1.389207          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 241 and nu = 0.03260088.
[1] "Sat Mar 10 17:00:31 2018"
Conditional Inference Random Forest 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  mtry  RMSE      Rsquared   MAE        Selected
  1     1.869540  0.4143932  1.3607185          
  2     1.779836  0.5808913  1.2514096          
  3     1.709148  0.5491304  1.1593577          
  4     1.556029  0.6638367  1.0109594          
  5     1.474827  0.6845626  0.9260044          
  6     1.352064  0.6975864  0.8195505          
  7     1.238492  0.7099327  0.7149214          
  8     1.107288  0.7426773  0.6300357          
  9     1.013262  0.7476276  0.6142044  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 17:00:47 2018"
Error in varimp(object, ...) : could not find function "varimp"
Conditional Inference Tree 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  mincriterion  RMSE      Rsquared   MAE        Selected
  0.03726011    1.080218  0.6907101  0.8085064          
  0.09538976    1.080218  0.6907101  0.8085064          
  0.13628871    1.080218  0.6907101  0.8085064          
  0.17376387    1.080218  0.6907101  0.8085064          
  0.25579013    1.080218  0.6907101  0.8085064          
  0.27824472    1.080218  0.6907101  0.8085064          
  0.30463149    1.080218  0.6907101  0.8085064          
  0.40082870    1.080218  0.6907101  0.8085064          
  0.45832706    1.080218  0.6907101  0.8085064          
  0.48257268    1.080218  0.6907101  0.8085064          
  0.53837072    1.080218  0.6907101  0.8085064          
  0.57844659    1.080218  0.6907101  0.8085064          
  0.57993361    1.080218  0.6907101  0.8085064          
  0.59947350    1.080218  0.6907101  0.8085064          
  0.66151637    1.080218  0.6907101  0.8085064          
  0.67110902    1.080218  0.6907101  0.8085064          
  0.76452146    1.080218  0.6907101  0.8085064          
  0.76630054    1.080218  0.6907101  0.8085064          
  0.83536128    1.080218  0.6907101  0.8085064          
  0.96014406    1.080218  0.6907101  0.8085064  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mincriterion = 0.9601441.
[1] "Sat Mar 10 17:01:01 2018"
Conditional Inference Tree 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  maxdepth  mincriterion  RMSE      Rsquared   MAE        Selected
   1        0.81999157    1.144757  0.6558455  0.8574538          
   2        0.52133724    1.080218  0.6907101  0.8085064  *       
   3        0.85732026    1.080218  0.6907101  0.8085064          
   3        0.87948419    1.080218  0.6907101  0.8085064          
   4        0.80789568    1.080218  0.6907101  0.8085064          
   5        0.10079370    1.080218  0.6907101  0.8085064          
   5        0.87593318    1.080218  0.6907101  0.8085064          
   7        0.65235730    1.080218  0.6907101  0.8085064          
   7        0.93964561    1.080218  0.6907101  0.8085064          
   8        0.05275606    1.080218  0.6907101  0.8085064          
   9        0.38810546    1.080218  0.6907101  0.8085064          
   9        0.71471489    1.080218  0.6907101  0.8085064          
   9        0.88968643    1.080218  0.6907101  0.8085064          
   9        0.89516486    1.080218  0.6907101  0.8085064          
  10        0.25162383    1.080218  0.6907101  0.8085064          
  11        0.85145636    1.080218  0.6907101  0.8085064          
  12        0.11085019    1.080218  0.6907101  0.8085064          
  12        0.92793745    1.080218  0.6907101  0.8085064          
  13        0.05125084    1.080218  0.6907101  0.8085064          
  15        0.63433217    1.080218  0.6907101  0.8085064          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were maxdepth = 2 and mincriterion
 = 0.5213372.
[1] "Sat Mar 10 17:01:14 2018"
Cubist 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  committees  neighbors  RMSE       Rsquared   MAE         Selected
   6          4          0.1265375  0.9972281  0.07473216          
   6          8          0.1251603  0.9972152  0.07222124          
  11          2          0.1217895  0.9972927  0.07257070          
  12          7          0.1194254  0.9972802  0.06962289          
  26          6          0.1166163  0.9973270  0.06825137          
  39          5          0.1178651  0.9972787  0.06964611          
  53          0          0.1155212  0.9972958  0.06720750          
  64          9          0.1153841  0.9972783  0.06731288          
  66          4          0.1164143  0.9973046  0.06976866          
  72          5          0.1170483  0.9972806  0.06921187          
  81          2          0.1165625  0.9973132  0.07000691          
  82          0          0.1150355  0.9972969  0.06696212          
  86          1          0.1128857  0.9974119  0.06886331          
  86          6          0.1170423  0.9972665  0.06834329          
  88          1          0.1128680  0.9974120  0.06885427  *       
  88          3          0.1152408  0.9973482  0.06861511          
  89          5          0.1170429  0.9972542  0.06932831          
  90          5          0.1168616  0.9972818  0.06911692          
  93          7          0.1148008  0.9973088  0.06721720          
  94          4          0.1161128  0.9973066  0.06961439          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were committees = 88 and neighbors = 1.
[1] "Sat Mar 10 17:01:44 2018"
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
Stacked AutoEncoder Deep Neural Network 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  layer1  layer2  layer3  hidden_dropout  visible_dropout  RMSE      Rsquared  
   2      17      10      0.28325637      0.02896527       2.157285  0.15367753
   3      11      17      0.32751024      0.29017929       2.347786  0.01200091
   4      18       4      0.55781049      0.38914078       1.994396  0.09942745
   5      18       3      0.69543093      0.28887812       2.034924  0.22233242
   6      17       9      0.20171537      0.59563682       2.017603  0.27674305
   7       3      10      0.13372035      0.45381646       2.322774  0.28242302
   7      18       3      0.02876899      0.14947118       1.930850  0.34157695
   9      14       4      0.44398854      0.16637709       1.922523  0.06599922
  10      19       7      0.61687125      0.29817935       1.994118  0.02324471
  11       3       7      0.45397617      0.17241954       2.080792  0.37542834
  12      18       5      0.42156775      0.68430579       2.184409  0.07031123
  12      19      16      0.09843334      0.23920020       2.422900  0.64771301
  13       9      13      0.53557815      0.55539028       2.110433  0.13912183
  13      15      13      0.28667970      0.52511685       2.005764  0.15533061
  14       6      19      0.34599802      0.05818871       4.279997  0.16879702
  14      18       5      0.62378946      0.01427877       1.951140  0.10570685
  16       4       9      0.48064272      0.01884833       2.001884  0.17469842
  16      19       6      0.63714381      0.18306499       2.129664  0.26362923
  17       2       6      0.36788321      0.06226381       2.108894  0.32269679
  20      14       9      0.63551385      0.37498960       2.036824  0.36241096
  MAE       Selected
  1.655110          
  1.677936          
  1.596890          
  1.675632          
  1.512848          
  1.542509          
  1.400515          
  1.426615  *       
  1.608501          
  1.463607          
  1.724066          
  1.797007          
  1.558409          
  1.370133          
  4.025349          
  1.530731          
  1.552177          
  1.827969          
  1.474028          
  1.500099          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were layer1 = 9, layer2 = 14, layer3 =
 4, hidden_dropout = 0.4439885 and visible_dropout = 0.1663771.
[1] "Sat Mar 10 17:01:58 2018"
Loading required package: earth
Loading required package: plotmo
Loading required package: plotrix
Loading required package: TeachingDemos
Multivariate Adaptive Regression Spline 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  degree  nprune  RMSE       Rsquared   MAE        Selected
  1       2       0.3695573  0.9632925  0.2994954          
  1       3       0.1621745  0.9928806  0.1422369  *       
  1       4       0.1739604  0.9922047  0.1464373          
  2       2       0.3695573  0.9632925  0.2994954          
  2       3       0.1621745  0.9928806  0.1422369          
  2       4       0.1952463  0.9913554  0.1339007          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nprune = 3 and degree = 1.
[1] "Sat Mar 10 17:02:13 2018"
Loading required package: MASS
Extreme Learning Machine 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  nhid  actfun   RMSE       Rsquared     MAE        Selected
   1    tansig   2.6677206  0.071445152  1.9869878          
   2    purelin  2.0819211  0.232458728  1.6559246          
   3    tansig   2.7958612  0.109202250  2.1425046          
   4    tansig   2.7942460  0.077553587  1.9850320          
   5    tansig   2.2204349  0.256108555  1.5574631          
   6    sin      2.4441986  0.121423759  1.8250803          
   6    tansig   1.9514057  0.200817194  1.4540593          
   8    purelin  1.2490248  0.567407386  1.0584648          
   9    tansig   1.8755046  0.258146976  1.3955481          
  10    sin      2.1927184  0.243621709  1.5845128          
  11    tansig   2.2541033  0.132987203  1.6205985          
  12    purelin  0.2818752  0.978637138  0.2231349  *       
  12    radbas   2.6667757  0.046622364  2.0674196          
  13    radbas   2.8378159  0.053630731  2.3433859          
  13    tansig   1.9135870  0.475264753  1.6450812          
  15    sin      3.1137451  0.084035477  2.5563208          
  15    tansig   1.8313551  0.386586067  1.4735896          
  16    sin      3.6596133  0.004843133  2.9631384          
  19    purelin  0.2818752  0.978637138  0.2231349          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nhid = 12 and actfun = purelin.
[1] "Sat Mar 10 17:02:26 2018"
Elasticnet 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  lambda        fraction    RMSE       Rsquared   MAE        Selected
  1.673249e-05  0.81999157  0.2570058  0.9839816  0.1818048          
  3.735413e-05  0.52133724  0.7783570  0.9841261  0.5184341          
  6.572525e-05  0.85732026  0.2538193  0.9831162  0.1928675  *       
  1.103020e-04  0.87948419  0.2565793  0.9824674  0.2000664          
  3.425633e-04  0.80789568  0.2633355  0.9841261  0.1783026          
  4.671629e-04  0.10079370  1.6997339  0.9841261  1.2533327          
  6.726497e-04  0.87593318  0.2559570  0.9825931  0.1986294          
  2.540810e-03  0.65235730  0.5132026  0.9841261  0.3227785          
  5.622926e-03  0.93964561  0.2674831  0.9806295  0.2112551          
  7.860245e-03  0.05275606  1.8077272  0.9841261  1.3405462          
  1.699121e-02  0.89516486  0.2565999  0.9825013  0.1984249          
  2.955830e-02  0.88968643  0.2552645  0.9828966  0.1935526          
  3.017183e-02  0.71471489  0.4378427  0.9841261  0.2624213          
  3.952219e-02  0.38810546  1.0997054  0.9841261  0.7662922          
  9.313185e-02  0.25162383  1.3784730  0.9841261  0.9930579          
  1.063296e-01  0.85145636  0.2667272  0.9824377  0.1893323          
  3.864816e-01  0.11085019  1.6477333  0.9841261  1.2113083          
  3.960985e-01  0.92793745  0.4480353  0.9520134  0.3191043          
  1.028413e+00  0.05125084  1.7739975  0.9841261  1.3133193          
  5.765863e+00  0.63433217  0.7101382  0.8849627  0.4856090          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were fraction = 0.8573203 and lambda
 = 6.572525e-05.
[1] "Sat Mar 10 17:02:39 2018"
Error : package evtree is required
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Error : package evtree is required
Error : package evtree is required
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 17:02:52 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "evtree"                         
Random Forest by Randomization 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  mtry  numRandomCuts  RMSE       Rsquared   MAE        Selected
  1     14             1.5625394  0.6038017  1.1242113          
  1     21             1.5740079  0.5838826  1.1200993          
  2     22             1.2573274  0.7781255  0.8530220          
  3      3             0.9633612  0.9119633  0.6266754          
  3     21             1.0152796  0.8642346  0.6823125          
  3     22             1.0383043  0.8607802  0.6838617          
  4     17             0.8159401  0.9080046  0.5197460          
  5      2             0.5876317  0.9676013  0.3658709          
  5     23             0.6916445  0.9215992  0.4464113          
  5     24             0.6800773  0.9219166  0.4377788          
  6      7             0.5231095  0.9564695  0.3284186          
  6     10             0.5320344  0.9510354  0.3406328          
  6     18             0.5702348  0.9363965  0.3600653          
  6     23             0.5918725  0.9286119  0.3717363          
  7      3             0.3269757  0.9840107  0.2137043          
  7     22             0.5248957  0.9344447  0.3205555          
  7     24             0.5191062  0.9305521  0.3017800          
  8      2             0.2659155  0.9873148  0.1664681  *       
  9     16             0.4005127  0.9449519  0.2052291          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 8 and numRandomCuts = 2.
[1] "Sat Mar 10 17:03:21 2018"
Ridge Regression with Variable Selection 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  lambda        k  RMSE       Rsquared   MAE        Selected
  1.673249e-05  8  0.2835523  0.9783981  0.2240741          
  3.735413e-05  5  0.2864141  0.9777366  0.2294561          
  6.572525e-05  8  0.2835208  0.9784009  0.2240288          
  1.103020e-04  8  0.2834923  0.9784034  0.2239876          
  3.425633e-04  8  0.2833455  0.9784165  0.2237731          
  4.671629e-04  1  0.2442379  0.9841261  0.1939643  *       
  6.726497e-04  8  0.2831427  0.9784347  0.2234689          
  2.540810e-03  6  0.2795576  0.9788305  0.2240262          
  5.622926e-03  9  0.2840794  0.9780676  0.2219243          
  7.860245e-03  1  0.2446384  0.9841261  0.1901251          
  1.699121e-02  9  0.2751177  0.9794928  0.2069707          
  2.955830e-02  9  0.2821727  0.9789378  0.2081604          
  3.017183e-02  7  0.2824567  0.9789323  0.2079890          
  3.952219e-02  4  0.2821790  0.9780476  0.2045452          
  9.313185e-02  3        NaN        NaN        NaN          
  1.063296e-01  8        NaN        NaN        NaN          
  3.864816e-01  1        NaN        NaN        NaN          
  3.960985e-01  9        NaN        NaN        NaN          
  1.028413e+00  1        NaN        NaN        NaN          
  5.765863e+00  6        NaN        NaN        NaN          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were k = 1 and lambda = 0.0004671629.
[1] "Sat Mar 10 17:03:34 2018"
Loading required package: mgcv
Loading required package: nlme
This is mgcv 1.8-22. For overview type 'help("mgcv-package")'.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 37 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 13 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 17:03:55 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "gam"                            
Boosted Generalized Additive Model 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  mstop  prune  RMSE        Rsquared   MAE         Selected
   38    no     0.04245350  0.9998550  0.02367159          
   96    no     0.02277379  0.9998745  0.01619659          
  137    no     0.02267266  0.9998726  0.01650233  *       
  174    no     0.02271728  0.9998735  0.01658047          
  256    no     0.02290697  0.9998742  0.01673356          
  279    yes    0.02274568  0.9998748  0.01615654          
  305    no     0.02303725  0.9998742  0.01678626          
  401    no     0.02340452  0.9998723  0.01700442          
  459    no     0.02357958  0.9998714  0.01716161          
  483    yes    0.02274568  0.9998748  0.01615654          
  539    no     0.02377027  0.9998704  0.01738533          
  579    no     0.02384273  0.9998701  0.01747637          
  580    no     0.02384244  0.9998701  0.01747746          
  600    yes    0.02274568  0.9998748  0.01615654          
  662    yes    0.02274568  0.9998748  0.01615654          
  672    no     0.02396140  0.9998698  0.01765705          
  765    yes    0.02274568  0.9998748  0.01615654          
  767    no     0.02406532  0.9998696  0.01780249          
  836    yes    0.02274568  0.9998748  0.01615654          
  961    no     0.02422789  0.9998692  0.01802418          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 137 and prune = no.
[1] "Sat Mar 10 17:04:29 2018"
Loading required package: gam
Loading required package: splines
Loading required package: foreach
Loaded gam 1.14-4


Attaching package: 'gam'

The following objects are masked from 'package:mgcv':

    gam, gam.control, gam.fit, plot.gam, predict.gam, s, summary.gam

Generalized Additive Model using Splines 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  df         RMSE        Rsquared   MAE         Selected
  0.1863005  0.28187198  0.9786376  0.22313244          
  0.4769488  0.28187198  0.9786376  0.22313244          
  0.6814435  0.28187198  0.9786376  0.22313244          
  0.8688193  0.28187198  0.9786376  0.22313244          
  1.2789507  0.24446893  0.9838967  0.19022307          
  1.3912236  0.23167065  0.9855553  0.17785038          
  1.5231574  0.21736855  0.9873049  0.16377566          
  2.0041435  0.16866027  0.9923696  0.12415971          
  2.2916353  0.14354755  0.9944617  0.10347677          
  2.4128634  0.13448378  0.9951387  0.09738740          
  2.6918536  0.11720074  0.9963233  0.08631718          
  2.8922329  0.10755888  0.9969261  0.08069795          
  2.8996680  0.10724126  0.9969452  0.08050070          
  2.9973675  0.10327429  0.9971805  0.07797066          
  3.3075819  0.09299073  0.9977563  0.07169547          
  3.3555451  0.09163437  0.9978283  0.07087058          
  3.8226073  0.08131445  0.9983417  0.06399288          
  3.8315027  0.08115574  0.9983491  0.06387397          
  4.1768064  0.07582374  0.9985874  0.05966422          
  4.8007203  0.06947921  0.9988442  0.05433643  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was df = 4.80072.
[1] "Sat Mar 10 17:04:49 2018"
Error in if (any(extras)) { : missing value where TRUE/FALSE needed
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 17:05:06 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "gaussprLinear"                  
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 17:05:28 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "gaussprPoly"                    
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 17:05:46 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "gaussprRadial"                  
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        3.3923             nan     0.0608    0.2307
     2        3.1048             nan     0.0608    0.1616
     3        2.9609             nan     0.0608    0.1540
     4        2.8232             nan     0.0608    0.1353
     5        2.6075             nan     0.0608    0.1418
     6        2.4108             nan     0.0608    0.1012
     7        2.3500             nan     0.0608    0.0802
     8        2.2445             nan     0.0608    0.1176
     9        2.1451             nan     0.0608    0.0119
    10        2.0426             nan     0.0608    0.0938
    20        1.4643             nan     0.0608   -0.0076
    40        1.1797             nan     0.0608   -0.0016
    60        1.1155             nan     0.0608    0.0121
    80        0.9077             nan     0.0608   -0.0089
   100        0.8157             nan     0.0608   -0.0162
   120        0.6763             nan     0.0608   -0.0025
   140        0.6027             nan     0.0608   -0.0056
   160        0.5418             nan     0.0608   -0.0119
   180        0.4958             nan     0.0608   -0.0094
   200        0.4650             nan     0.0608   -0.0187
   220        0.4376             nan     0.0608   -0.0214
   240        0.4168             nan     0.0608   -0.0156
   260        0.4020             nan     0.0608   -0.0040
   280        0.3819             nan     0.0608   -0.0039
   300        0.3478             nan     0.0608   -0.0060
   320        0.3276             nan     0.0608   -0.0106
   340        0.3003             nan     0.0608   -0.0059
   360        0.2859             nan     0.0608   -0.0063
   380        0.2567             nan     0.0608   -0.0094
   400        0.2390             nan     0.0608   -0.0039
   420        0.2271             nan     0.0608   -0.0036
   440        0.2170             nan     0.0608   -0.0074
   460        0.1963             nan     0.0608   -0.0054
   480        0.1868             nan     0.0608   -0.0033
   500        0.1711             nan     0.0608   -0.0038
   520        0.1672             nan     0.0608   -0.0044
   540        0.1583             nan     0.0608   -0.0031
   560        0.1446             nan     0.0608   -0.0011
   580        0.1321             nan     0.0608    0.0002
   600        0.1270             nan     0.0608   -0.0038
   620        0.1207             nan     0.0608   -0.0034
   640        0.1208             nan     0.0608   -0.0029
   660        0.1086             nan     0.0608   -0.0007
   680        0.1012             nan     0.0608    0.0003
   700        0.0975             nan     0.0608   -0.0021
   720        0.0932             nan     0.0608   -0.0007
   740        0.0853             nan     0.0608   -0.0020
   760        0.0835             nan     0.0608   -0.0020
   780        0.0710             nan     0.0608   -0.0014
   800        0.0673             nan     0.0608   -0.0024
   820        0.0644             nan     0.0608   -0.0024
   840        0.0611             nan     0.0608   -0.0019
   860        0.0573             nan     0.0608   -0.0023
   880        0.0550             nan     0.0608   -0.0002
   900        0.0523             nan     0.0608   -0.0031
   920        0.0493             nan     0.0608   -0.0009
   940        0.0458             nan     0.0608   -0.0003
   960        0.0434             nan     0.0608    0.0000
   980        0.0408             nan     0.0608   -0.0011
  1000        0.0394             nan     0.0608   -0.0006
  1020        0.0371             nan     0.0608   -0.0002
  1040        0.0350             nan     0.0608   -0.0002
  1060        0.0340             nan     0.0608   -0.0005
  1080        0.0328             nan     0.0608   -0.0010
  1100        0.0311             nan     0.0608   -0.0003
  1120        0.0289             nan     0.0608   -0.0007
  1140        0.0269             nan     0.0608   -0.0009
  1160        0.0256             nan     0.0608   -0.0001
  1180        0.0239             nan     0.0608   -0.0008
  1200        0.0223             nan     0.0608   -0.0003
  1220        0.0210             nan     0.0608   -0.0003
  1240        0.0192             nan     0.0608   -0.0006
  1260        0.0183             nan     0.0608   -0.0004
  1280        0.0177             nan     0.0608   -0.0004
  1300        0.0167             nan     0.0608   -0.0003
  1320        0.0162             nan     0.0608   -0.0003
  1340        0.0153             nan     0.0608   -0.0004
  1360        0.0149             nan     0.0608   -0.0002
  1380        0.0135             nan     0.0608   -0.0003
  1400        0.0130             nan     0.0608   -0.0002
  1420        0.0130             nan     0.0608   -0.0003
  1440        0.0126             nan     0.0608   -0.0004
  1460        0.0125             nan     0.0608   -0.0001
  1480        0.0116             nan     0.0608   -0.0003
  1500        0.0114             nan     0.0608   -0.0002
  1520        0.0108             nan     0.0608   -0.0004
  1523        0.0105             nan     0.0608   -0.0001

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        3.6640             nan     0.0608    0.2459
     2        3.5282             nan     0.0608    0.1469
     3        3.4074             nan     0.0608    0.1147
     4        3.1448             nan     0.0608    0.1380
     5        3.0377             nan     0.0608    0.1319
     6        2.8170             nan     0.0608    0.1546
     7        2.6236             nan     0.0608    0.1196
     8        2.5085             nan     0.0608    0.0998
     9        2.4169             nan     0.0608    0.1162
    10        2.2676             nan     0.0608    0.0973
    20        1.6478             nan     0.0608    0.0224
    40        1.0624             nan     0.0608   -0.0024
    60        0.8832             nan     0.0608   -0.0157
    80        0.7037             nan     0.0608   -0.0125
   100        0.5569             nan     0.0608   -0.0047
   120        0.4795             nan     0.0608   -0.0108
   140        0.4320             nan     0.0608   -0.0034
   160        0.3823             nan     0.0608    0.0020
   180        0.3378             nan     0.0608   -0.0025
   200        0.2956             nan     0.0608   -0.0056
   220        0.2729             nan     0.0608   -0.0040
   240        0.2472             nan     0.0608    0.0001
   260        0.2196             nan     0.0608   -0.0036
   280        0.2013             nan     0.0608   -0.0030
   300        0.1856             nan     0.0608   -0.0022
   320        0.1699             nan     0.0608   -0.0023
   340        0.1571             nan     0.0608   -0.0008
   360        0.1511             nan     0.0608   -0.0068
   380        0.1353             nan     0.0608   -0.0016
   400        0.1255             nan     0.0608   -0.0060
   420        0.1166             nan     0.0608   -0.0031
   440        0.1039             nan     0.0608   -0.0017
   460        0.0974             nan     0.0608   -0.0030
   480        0.0886             nan     0.0608   -0.0032
   500        0.0829             nan     0.0608   -0.0018
   520        0.0762             nan     0.0608   -0.0008
   540        0.0720             nan     0.0608   -0.0032
   560        0.0676             nan     0.0608   -0.0010
   580        0.0628             nan     0.0608   -0.0013
   600        0.0585             nan     0.0608   -0.0001
   620        0.0534             nan     0.0608   -0.0026
   640        0.0511             nan     0.0608   -0.0011
   660        0.0462             nan     0.0608   -0.0015
   680        0.0426             nan     0.0608   -0.0004
   700        0.0395             nan     0.0608   -0.0009
   720        0.0366             nan     0.0608   -0.0004
   740        0.0347             nan     0.0608   -0.0005
   760        0.0317             nan     0.0608   -0.0020
   780        0.0293             nan     0.0608   -0.0009
   800        0.0269             nan     0.0608   -0.0014
   820        0.0260             nan     0.0608   -0.0013
   840        0.0245             nan     0.0608   -0.0005
   860        0.0222             nan     0.0608   -0.0002
   880        0.0204             nan     0.0608   -0.0002
   900        0.0191             nan     0.0608   -0.0006
   920        0.0179             nan     0.0608   -0.0006
   940        0.0165             nan     0.0608   -0.0004
   960        0.0151             nan     0.0608   -0.0004
   980        0.0147             nan     0.0608   -0.0003
  1000        0.0139             nan     0.0608   -0.0004
  1020        0.0125             nan     0.0608   -0.0006
  1040        0.0117             nan     0.0608   -0.0002
  1060        0.0112             nan     0.0608   -0.0002
  1080        0.0103             nan     0.0608   -0.0002
  1100        0.0095             nan     0.0608   -0.0004
  1120        0.0088             nan     0.0608   -0.0003
  1140        0.0082             nan     0.0608   -0.0003
  1160        0.0081             nan     0.0608   -0.0003
  1180        0.0076             nan     0.0608   -0.0004
  1200        0.0072             nan     0.0608   -0.0002
  1220        0.0068             nan     0.0608   -0.0001
  1240        0.0063             nan     0.0608   -0.0000
  1260        0.0059             nan     0.0608   -0.0001
  1280        0.0057             nan     0.0608   -0.0003
  1300        0.0053             nan     0.0608   -0.0001
  1320        0.0048             nan     0.0608   -0.0002
  1340        0.0044             nan     0.0608   -0.0001
  1360        0.0042             nan     0.0608   -0.0001
  1380        0.0040             nan     0.0608   -0.0001
  1400        0.0036             nan     0.0608   -0.0001
  1420        0.0034             nan     0.0608   -0.0001
  1440        0.0031             nan     0.0608   -0.0000
  1460        0.0029             nan     0.0608   -0.0001
  1480        0.0027             nan     0.0608   -0.0001
  1500        0.0024             nan     0.0608   -0.0000
  1520        0.0023             nan     0.0608   -0.0000
  1523        0.0023             nan     0.0608   -0.0001

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        3.3566             nan     0.0608    0.1873
     2        3.1548             nan     0.0608    0.1854
     3        2.9737             nan     0.0608    0.2067
     4        2.7839             nan     0.0608    0.1426
     5        2.6606             nan     0.0608    0.1496
     6        2.4900             nan     0.0608    0.1549
     7        2.3019             nan     0.0608    0.0820
     8        2.1414             nan     0.0608    0.0536
     9        2.0500             nan     0.0608    0.0912
    10        1.9748             nan     0.0608    0.0885
    20        1.3899             nan     0.0608   -0.0049
    40        0.9591             nan     0.0608   -0.0076
    60        0.7933             nan     0.0608   -0.0329
    80        0.6606             nan     0.0608    0.0014
   100        0.5765             nan     0.0608   -0.0051
   120        0.4788             nan     0.0608   -0.0101
   140        0.4215             nan     0.0608   -0.0117
   160        0.3893             nan     0.0608   -0.0044
   180        0.3447             nan     0.0608   -0.0076
   200        0.3001             nan     0.0608   -0.0068
   220        0.2800             nan     0.0608   -0.0061
   240        0.2692             nan     0.0608   -0.0070
   260        0.2507             nan     0.0608   -0.0018
   280        0.2312             nan     0.0608   -0.0046
   300        0.2237             nan     0.0608   -0.0033
   320        0.2087             nan     0.0608   -0.0032
   340        0.1900             nan     0.0608   -0.0053
   360        0.1782             nan     0.0608   -0.0042
   380        0.1706             nan     0.0608   -0.0031
   400        0.1654             nan     0.0608   -0.0039
   420        0.1524             nan     0.0608   -0.0017
   440        0.1466             nan     0.0608    0.0004
   460        0.1429             nan     0.0608   -0.0031
   480        0.1361             nan     0.0608   -0.0021
   500        0.1311             nan     0.0608   -0.0016
   520        0.1252             nan     0.0608   -0.0025
   540        0.1189             nan     0.0608   -0.0038
   560        0.1101             nan     0.0608   -0.0031
   580        0.1050             nan     0.0608   -0.0007
   600        0.1017             nan     0.0608   -0.0012
   620        0.0993             nan     0.0608   -0.0024
   640        0.0962             nan     0.0608   -0.0012
   660        0.0958             nan     0.0608   -0.0013
   680        0.0886             nan     0.0608   -0.0027
   700        0.0832             nan     0.0608   -0.0020
   720        0.0811             nan     0.0608   -0.0015
   740        0.0807             nan     0.0608   -0.0007
   760        0.0742             nan     0.0608   -0.0006
   780        0.0716             nan     0.0608   -0.0024
   800        0.0702             nan     0.0608   -0.0022
   820        0.0680             nan     0.0608   -0.0015
   840        0.0642             nan     0.0608   -0.0004
   860        0.0608             nan     0.0608   -0.0020
   880        0.0585             nan     0.0608   -0.0007
   900        0.0560             nan     0.0608   -0.0001
   920        0.0550             nan     0.0608   -0.0003
   940        0.0524             nan     0.0608    0.0001
   960        0.0529             nan     0.0608   -0.0012
   980        0.0517             nan     0.0608   -0.0011
  1000        0.0489             nan     0.0608   -0.0019
  1020        0.0464             nan     0.0608   -0.0010
  1040        0.0452             nan     0.0608   -0.0013
  1060        0.0426             nan     0.0608   -0.0008
  1080        0.0408             nan     0.0608   -0.0010
  1100        0.0392             nan     0.0608   -0.0010
  1120        0.0375             nan     0.0608   -0.0005
  1140        0.0353             nan     0.0608   -0.0002
  1160        0.0341             nan     0.0608   -0.0012
  1180        0.0333             nan     0.0608   -0.0012
  1200        0.0319             nan     0.0608   -0.0004
  1220        0.0306             nan     0.0608   -0.0005
  1240        0.0299             nan     0.0608   -0.0006
  1260        0.0282             nan     0.0608   -0.0002
  1280        0.0272             nan     0.0608   -0.0004
  1300        0.0259             nan     0.0608   -0.0002
  1320        0.0250             nan     0.0608   -0.0002
  1340        0.0236             nan     0.0608   -0.0010
  1360        0.0226             nan     0.0608   -0.0008
  1380        0.0219             nan     0.0608   -0.0007
  1400        0.0206             nan     0.0608    0.0001
  1420        0.0198             nan     0.0608   -0.0007
  1440        0.0187             nan     0.0608   -0.0005
  1460        0.0180             nan     0.0608   -0.0006
  1480        0.0173             nan     0.0608   -0.0005
  1500        0.0169             nan     0.0608   -0.0002
  1520        0.0163             nan     0.0608   -0.0004
  1523        0.0162             nan     0.0608   -0.0005

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        3.3473             nan     0.0608    0.2754
     2        2.9935             nan     0.0608    0.1544
     3        2.7489             nan     0.0608    0.2187
     4        2.5901             nan     0.0608    0.1744
     5        2.4557             nan     0.0608    0.1553
     6        2.2797             nan     0.0608    0.1736
     7        2.1098             nan     0.0608    0.1525
     8        2.0327             nan     0.0608    0.0837
     9        1.9536             nan     0.0608    0.0912
    10        1.8995             nan     0.0608    0.0568
    20        1.0118             nan     0.0608   -0.0142
    40        0.5970             nan     0.0608    0.0119
    60        0.4424             nan     0.0608   -0.0089
    80        0.3538             nan     0.0608   -0.0051
   100        0.3054             nan     0.0608   -0.0057
   120        0.2540             nan     0.0608   -0.0033
   140        0.1943             nan     0.0608   -0.0021
   160        0.1722             nan     0.0608   -0.0024
   180        0.1356             nan     0.0608   -0.0067
   200        0.1192             nan     0.0608   -0.0012
   220        0.0978             nan     0.0608   -0.0025
   240        0.0829             nan     0.0608   -0.0030
   260        0.0706             nan     0.0608   -0.0025
   280        0.0574             nan     0.0608   -0.0010
   300        0.0494             nan     0.0608   -0.0017
   320        0.0433             nan     0.0608   -0.0007
   340        0.0376             nan     0.0608   -0.0005
   360        0.0324             nan     0.0608   -0.0005
   380        0.0280             nan     0.0608   -0.0003
   400        0.0228             nan     0.0608   -0.0001
   420        0.0197             nan     0.0608   -0.0002
   440        0.0170             nan     0.0608   -0.0006
   460        0.0143             nan     0.0608   -0.0001
   480        0.0122             nan     0.0608   -0.0003
   500        0.0103             nan     0.0608   -0.0002
   520        0.0087             nan     0.0608   -0.0001
   540        0.0076             nan     0.0608   -0.0002
   560        0.0064             nan     0.0608   -0.0002
   580        0.0056             nan     0.0608   -0.0002
   600        0.0049             nan     0.0608   -0.0002
   620        0.0043             nan     0.0608   -0.0001
   640        0.0038             nan     0.0608   -0.0000
   660        0.0032             nan     0.0608   -0.0000
   680        0.0028             nan     0.0608   -0.0000
   700        0.0024             nan     0.0608   -0.0001
   720        0.0020             nan     0.0608   -0.0000
   740        0.0017             nan     0.0608   -0.0000
   760        0.0014             nan     0.0608   -0.0000
   780        0.0012             nan     0.0608   -0.0000
   800        0.0011             nan     0.0608   -0.0000
   820        0.0010             nan     0.0608   -0.0000
   840        0.0008             nan     0.0608   -0.0000
   860        0.0007             nan     0.0608   -0.0000
   880        0.0006             nan     0.0608   -0.0000
   900        0.0005             nan     0.0608   -0.0000
   920        0.0004             nan     0.0608   -0.0000
   940        0.0004             nan     0.0608   -0.0000
   960        0.0003             nan     0.0608   -0.0000
   980        0.0003             nan     0.0608   -0.0000
  1000        0.0002             nan     0.0608   -0.0000
  1020        0.0002             nan     0.0608   -0.0000
  1040        0.0002             nan     0.0608   -0.0000
  1060        0.0002             nan     0.0608   -0.0000
  1080        0.0001             nan     0.0608   -0.0000
  1100        0.0001             nan     0.0608   -0.0000
  1120        0.0001             nan     0.0608   -0.0000
  1140        0.0001             nan     0.0608   -0.0000
  1160        0.0001             nan     0.0608   -0.0000
  1180        0.0001             nan     0.0608   -0.0000
  1200        0.0001             nan     0.0608   -0.0000
  1220        0.0001             nan     0.0608   -0.0000
  1240        0.0000             nan     0.0608   -0.0000
  1260        0.0000             nan     0.0608   -0.0000
  1280        0.0000             nan     0.0608   -0.0000
  1300        0.0000             nan     0.0608   -0.0000
  1320        0.0000             nan     0.0608   -0.0000
  1340        0.0000             nan     0.0608   -0.0000
  1360        0.0000             nan     0.0608   -0.0000
  1380        0.0000             nan     0.0608   -0.0000
  1400        0.0000             nan     0.0608   -0.0000
  1420        0.0000             nan     0.0608   -0.0000
  1440        0.0000             nan     0.0608   -0.0000
  1460        0.0000             nan     0.0608   -0.0000
  1480        0.0000             nan     0.0608   -0.0000
  1500        0.0000             nan     0.0608   -0.0000
  1520        0.0000             nan     0.0608   -0.0000
  1523        0.0000             nan     0.0608   -0.0000

Stochastic Gradient Boosting 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  shrinkage   interaction.depth  n.minobsinnode  n.trees  RMSE      Rsquared
  0.04461684   9                 25               869          NaN       NaN
  0.06079205   9                  5              1523     1.462288  0.561631
  0.08061999   9                 21               682          NaN       NaN
  0.08064168   7                 18              2004          NaN       NaN
  0.11244275   9                 17              2892          NaN       NaN
  0.11664595   9                 23              3355          NaN       NaN
  0.14103291  10                 24              3831          NaN       NaN
  0.14937311   1                 16              4176          NaN       NaN
  0.16422688   1                 18              2413          NaN       NaN
  0.18413356  10                 23              2292          NaN       NaN
  0.22606395   9                 11              1279          NaN       NaN
  0.23811497   7                 24              4800          NaN       NaN
  0.24867791   2                 19              3822          NaN       NaN
  0.25728317   9                 13               187          NaN       NaN
  0.26584378   2                  9              1391          NaN       NaN
  0.35324582   4                 21              2997          NaN       NaN
  0.36903779   8                 13              2900          NaN       NaN
  0.46678650   9                  7              2692          NaN       NaN
  0.48850354   6                 14               477          NaN       NaN
  0.56739156   3                 15              3307          NaN       NaN
  MAE      Selected
      NaN          
  1.20339  *       
      NaN          
      NaN          
      NaN          
      NaN          
      NaN          
      NaN          
      NaN          
      NaN          
      NaN          
      NaN          
      NaN          
      NaN          
      NaN          
      NaN          
      NaN          
      NaN          
      NaN          
      NaN          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were n.trees = 1523, interaction.depth =
 9, shrinkage = 0.06079205 and n.minobsinnode = 5.
[1] "Sat Mar 10 17:06:00 2018"
Error in relative.influence(object, n.trees = numTrees) : 
  could not find function "relative.influence"
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 17:08:47 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "gbm_h2o"                        
Multivariate Adaptive Regression Splines 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results:

  RMSE       Rsquared   MAE      
  0.1739604  0.9922047  0.1464373

Tuning parameter 'degree' was held constant at a value of 1
[1] "Sat Mar 10 17:09:00 2018"
Generalized Linear Model 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results:

  RMSE       Rsquared   MAE      
  0.2818752  0.9786371  0.2231349

[1] "Sat Mar 10 17:09:13 2018"
Negative Binomial Generalized Linear Model 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  link      RMSE       Rsquared   MAE        Selected
  identity  0.3019928  0.9809833  0.1906362  *       
  log       0.8416333  0.9263119  0.4772374          
  sqrt      0.3436944  0.9863779  0.2350095          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was link = identity.
[1] "Sat Mar 10 17:09:25 2018"
Boosted Generalized Linear Model 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  mstop  prune  RMSE       Rsquared   MAE        Selected
   38    no     0.2562198  0.9830938  0.1904743  *       
   96    no     0.2675769  0.9805359  0.2098013          
  137    no     0.2694164  0.9801712  0.2129664          
  174    no     0.2709363  0.9799341  0.2147857          
  256    no     0.2736252  0.9795627  0.2174925          
  279    yes    0.2627592  0.9818506  0.1991913          
  305    no     0.2748899  0.9794019  0.2185286          
  401    no     0.2768159  0.9791733  0.2199776          
  459    no     0.2776843  0.9790772  0.2205492          
  483    yes    0.2627592  0.9818506  0.1991913          
  539    no     0.2786196  0.9789781  0.2211525          
  579    no     0.2790468  0.9789305  0.2214241          
  580    no     0.2790565  0.9789306  0.2214229          
  600    yes    0.2627592  0.9818506  0.1991913          
  662    yes    0.2627592  0.9818506  0.1991913          
  672    no     0.2798192  0.9788490  0.2218902          
  765    yes    0.2627592  0.9818506  0.1991913          
  767    no     0.2803859  0.9787894  0.2222313          
  836    yes    0.2627592  0.9818506  0.1991913          
  961    no     0.2810986  0.9787157  0.2226563          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 38 and prune = no.
[1] "Sat Mar 10 17:09:41 2018"
glmnet 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  alpha       lambda       RMSE       Rsquared   MAE        Selected
  0.03726011  1.579962552  1.0334197  0.9339836  0.6859365          
  0.09538976  0.107126264  0.3004024  0.9801034  0.1937560          
  0.13628871  2.211711812  1.1380769  0.9801859  0.7929622          
  0.17376387  2.700627196  1.2657269  0.9841261  0.9012729          
  0.25579013  1.416807530  0.9418056  0.9841261  0.6405164          
  0.27824472  0.002421838  0.2769259  0.9792496  0.2182960          
  0.30463149  2.615580882  1.3587704  0.9841261  0.9769988          
  0.40082870  0.348841822  0.4004356  0.9836812  0.2331931          
  0.45832706  4.644080302  1.9234550        NaN  1.4339021          
  0.48257268  0.001570924  0.2765546  0.9793021  0.2188670          
  0.53837072  3.110492958  1.7910319  0.9841261  1.3269962          
  0.57844659  2.960669682  1.8065051  0.9841261  1.3394809          
  0.57993361  0.611868513  0.5906731  0.9841261  0.3805765          
  0.59947350  0.032248392  0.2666679  0.9810454  0.2025710  *       
  0.66151637  0.009427623  0.2711876  0.9799422  0.2138199          
  0.67110902  2.097880755  1.5596051  0.9841261  1.1398925          
  0.76452146  0.002651551  0.2761468  0.9793481  0.2192428          
  0.76630054  4.179084352  1.9234550        NaN  1.4339021          
  0.83536128  0.001549760  0.2761142  0.9793490  0.2193315          
  0.96014406  0.296544038  0.3823422  0.9841261  0.2212276          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alpha = 0.5994735 and lambda
 = 0.03224839.
[1] "Sat Mar 10 17:09:56 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 17:12:44 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "glmnet_h2o"                     
Start:  AIC=18.44
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V8    1    1.270  16.447
- V3    1    1.271  16.476
- V4    1    1.272  16.480
- V6    1    1.277  16.585
- V10   1    1.289  16.830
- V9    1    1.327  17.549
- V5    1    1.335  17.694
- V7    1    1.370  18.345
<none>       1.270  18.442
- V2    1   54.702 110.522

Step:  AIC=16.45
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9 + V10

       Df Deviance     AIC
- V3    1    1.271  14.477
- V4    1    1.272  14.480
- V6    1    1.278  14.610
- V10   1    1.295  14.938
- V9    1    1.327  15.550
- V5    1    1.346  15.905
- V7    1    1.370  16.345
<none>       1.270  16.447
- V2    1   54.803 108.569

Step:  AIC=14.48
.outcome ~ V2 + V4 + V5 + V6 + V7 + V9 + V10

       Df Deviance     AIC
- V4    1    1.273  12.508
- V6    1    1.280  12.649
- V10   1    1.299  13.015
- V9    1    1.341  13.811
- V5    1    1.363  14.215
- V7    1    1.370  14.345
<none>       1.271  14.477
- V2    1   57.174 107.627

Step:  AIC=12.51
.outcome ~ V2 + V5 + V6 + V7 + V9 + V10

       Df Deviance     AIC
- V6    1    1.282  10.679
- V10   1    1.305  11.129
- V9    1    1.342  11.821
- V5    1    1.363  12.215
- V7    1    1.371  12.361
<none>       1.273  12.508
- V2    1   57.658 105.838

Step:  AIC=10.68
.outcome ~ V2 + V5 + V7 + V9 + V10

       Df Deviance     AIC
- V10   1    1.308   9.188
- V9    1    1.345   9.882
- V5    1    1.363  10.216
- V7    1    1.378  10.488
<none>       1.282  10.679
- V2    1   67.220 107.674

Step:  AIC=9.19
.outcome ~ V2 + V5 + V7 + V9

       Df Deviance     AIC
- V5    1    1.363   8.216
- V9    1    1.368   8.308
- V7    1    1.383   8.580
<none>       1.308   9.188
- V2    1   84.739 111.464

Step:  AIC=8.22
.outcome ~ V2 + V7 + V9

       Df Deviance     AIC
- V9    1    1.387   6.661
- V7    1    1.416   7.164
<none>       1.363   8.216
- V2    1   86.537 109.989

Step:  AIC=6.66
.outcome ~ V2 + V7

       Df Deviance     AIC
- V7    1    1.469   6.083
<none>       1.387   6.661
- V2    1   90.225 109.033

Step:  AIC=6.08
.outcome ~ V2

       Df Deviance     AIC
<none>       1.469   6.083
- V2    1   90.229 107.034
Start:  AIC=13.42
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V4    1    1.151  11.425
- V9    1    1.152  11.452
- V5    1    1.153  11.474
- V7    1    1.157  11.572
- V8    1    1.186  12.245
- V6    1    1.189  12.302
- V10   1    1.195  12.434
<none>       1.151  13.424
- V3    1    1.448  17.630
- V2    1   85.236 127.661

Step:  AIC=11.42
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V9    1    1.152   9.453
- V5    1    1.153   9.475
- V7    1    1.157   9.579
- V8    1    1.189  10.309
- V6    1    1.189  10.318
- V10   1    1.197  10.496
<none>       1.151  11.425
- V3    1    1.453  15.721
- V2    1   85.570 125.767

Step:  AIC=9.45
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V10

       Df Deviance     AIC
- V5    1    1.153   7.486
- V7    1    1.160   7.638
- V6    1    1.190   8.323
- V8    1    1.190   8.323
- V10   1    1.200   8.553
<none>       1.152   9.453
- V3    1    1.459  13.829
- V2    1   87.131 124.255

Step:  AIC=7.49
.outcome ~ V2 + V3 + V6 + V7 + V8 + V10

       Df Deviance     AIC
- V7    1    1.162   5.682
- V6    1    1.194   6.414
- V10   1    1.203   6.626
- V8    1    1.216   6.909
<none>       1.153   7.486
- V3    1    1.463  11.913
- V2    1   91.957 123.711

Step:  AIC=5.68
.outcome ~ V2 + V3 + V6 + V8 + V10

       Df Deviance     AIC
- V6    1    1.199   4.533
- V10   1    1.211   4.800
<none>       1.162   5.682
- V8    1    1.259   5.854
- V3    1    1.463   9.917
- V2    1   95.197 122.646

Step:  AIC=4.53
.outcome ~ V2 + V3 + V8 + V10

       Df Deviance     AIC
- V10   1    1.257   3.810
<none>       1.199   4.533
- V8    1    1.302   4.759
- V3    1    1.543   9.339
- V2    1   95.351 120.689

Step:  AIC=3.81
.outcome ~ V2 + V3 + V8

       Df Deviance     AIC
<none>       1.257   3.810
- V8    1    1.396   4.647
- V3    1    1.545   7.384
- V2    1   96.965 119.143
Start:  AIC=16.08
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V7    1    1.329  14.114
- V9    1    1.331  14.161
- V10   1    1.354  14.653
- V8    1    1.358  14.723
- V5    1    1.364  14.860
- V3    1    1.373  15.038
- V6    1    1.396  15.492
- V4    1    1.403  15.648
<none>       1.327  16.079
- V2    1   82.978 129.879

Step:  AIC=14.11
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V9 + V10

       Df Deviance     AIC
- V9    1    1.334  12.219
- V10   1    1.356  12.688
- V8    1    1.361  12.798
- V5    1    1.368  12.934
- V3    1    1.373  13.039
- V6    1    1.397  13.513
<none>       1.329  14.114
- V4    1    1.447  14.502
- V2    1   84.462 128.375

Step:  AIC=12.22
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V10

       Df Deviance     AIC
- V10   1    1.366  10.898
- V5    1    1.371  10.991
- V8    1    1.371  10.994
- V3    1    1.380  11.171
- V6    1    1.397  11.519
<none>       1.334  12.219
- V4    1    1.452  12.597
- V2    1   84.467 126.377

Step:  AIC=10.9
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8

       Df Deviance     AIC
- V8    1    1.385   9.286
- V5    1    1.393   9.437
- V3    1    1.395   9.485
- V6    1    1.427  10.121
- V4    1    1.465  10.858
<none>       1.366  10.898
- V2    1   84.495 124.386

Step:  AIC=9.29
.outcome ~ V2 + V3 + V4 + V5 + V6

       Df Deviance     AIC
- V5    1    1.400   7.570
- V3    1    1.410   7.776
- V6    1    1.427   8.121
- V4    1    1.469   8.931
<none>       1.385   9.286
- V2    1   89.758 124.078

Step:  AIC=7.57
.outcome ~ V2 + V3 + V4 + V6

       Df Deviance     AIC
- V3    1    1.430   6.168
- V6    1    1.443   6.436
- V4    1    1.488   7.291
<none>       1.400   7.570
- V2    1   98.810 124.768

Step:  AIC=6.17
.outcome ~ V2 + V4 + V6

       Df Deviance     AIC
- V6    1    1.483   5.197
- V4    1    1.514   5.776
<none>       1.430   6.168
- V2    1   99.438 122.946

Step:  AIC=5.2
.outcome ~ V2 + V4

       Df Deviance     AIC
- V4    1    1.562   4.653
<none>       1.483   5.197
- V2    1   99.516 120.968

Step:  AIC=4.65
.outcome ~ V2

       Df Deviance     AIC
<none>       1.562   4.653
- V2    1  101.299 119.465
Start:  AIC=16.68
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V8    1    2.051  14.685
- V4    1    2.055  14.773
- V9    1    2.058  14.825
- V5    1    2.076  15.177
- V10   1    2.080  15.252
- V6    1    2.080  15.259
- V7    1    2.115  15.919
<none>       2.050  16.677
- V3    1    2.189  17.304
- V2    1  132.143 181.315

Step:  AIC=14.69
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9 + V10

       Df Deviance     AIC
- V4    1    2.057  12.813
- V9    1    2.058  12.827
- V5    1    2.084  13.332
- V6    1    2.085  13.348
- V10   1    2.088  13.414
- V7    1    2.115  13.921
<none>       2.051  14.685
- V3    1    2.190  15.321
- V2    1  133.729 179.792

Step:  AIC=12.81
.outcome ~ V2 + V3 + V5 + V6 + V7 + V9 + V10

       Df Deviance     AIC
- V9    1    2.065  10.965
- V6    1    2.091  11.468
- V10   1    2.092  11.480
- V5    1    2.097  11.575
- V7    1    2.152  12.610
<none>       2.057  12.813
- V3    1    2.196  13.422
- V2    1  133.747 177.798

Step:  AIC=10.97
.outcome ~ V2 + V3 + V5 + V6 + V7 + V10

       Df Deviance     AIC
- V6    1    2.094   9.515
- V5    1    2.097   9.584
- V10   1    2.103   9.701
<none>       2.065  10.965
- V7    1    2.172  10.977
- V3    1    2.215  11.764
- V2    1  136.721 176.678

Step:  AIC=9.51
.outcome ~ V2 + V3 + V5 + V7 + V10

       Df Deviance     AIC
- V5    1    2.130   8.204
- V10   1    2.141   8.416
- V7    1    2.181   9.153
<none>       2.094   9.515
- V3    1    2.255  10.481
- V2    1  137.721 174.969

Step:  AIC=8.2
.outcome ~ V2 + V3 + V7 + V10

       Df Deviance     AIC
- V10   1    2.160   6.770
- V7    1    2.213   7.727
<none>       2.130   8.204
- V3    1    2.313   9.508
- V2    1  138.981 173.333

Step:  AIC=6.77
.outcome ~ V2 + V3 + V7

       Df Deviance     AIC
- V7    1    2.239   6.202
<none>       2.160   6.770
- V3    1    2.321   7.645
- V2    1  144.683 172.942

Step:  AIC=6.2
.outcome ~ V2 + V3

       Df Deviance     AIC
<none>       2.239   6.202
- V3    1    2.361   6.327
- V2    1  144.711 170.949
Generalized Linear Model with Stepwise Feature Selection 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results:

  RMSE       Rsquared   MAE      
  0.2654115  0.9808536  0.2082253

[1] "Sat Mar 10 17:12:57 2018"
Independent Component Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  n.comp  RMSE       Rsquared    MAE        Selected
  1       2.0590835  0.02929083  1.4837607          
  2       2.0168814  0.03576316  1.4345155          
  3       1.7970238  0.16500638  1.2750998          
  4       1.5852162  0.34992210  1.1447595          
  5       1.4012753  0.48311965  1.0473157          
  6       0.7351734  0.86479842  0.5693680          
  7       0.6356517  0.88542199  0.4935038          
  8       0.4609981  0.92999703  0.3517163          
  9       0.2818752  0.97863714  0.2231349  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was n.comp = 9.
[1] "Sat Mar 10 17:13:14 2018"
Partial Least Squares 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      1.7261569  0.2155810  1.1981579          
  2      0.9637946  0.7522673  0.7077900          
  3      0.6132242  0.8939517  0.4459301          
  4      0.3999216  0.9505565  0.2998980          
  5      0.3326390  0.9696011  0.2656812          
  6      0.2745232  0.9798189  0.2230613  *       
  7      0.2829081  0.9784785  0.2245543          
  8      0.2820827  0.9786068  0.2233790          
  9      0.2818752  0.9786371  0.2231349          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 6.
[1] "Sat Mar 10 17:13:27 2018"
Error in MSEP(object) : could not find function "MSEP"
k-Nearest Neighbors 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  kmax  distance   kernel        RMSE      Rsquared    MAE        Selected
   1    2.4599747  biweight      1.489859  0.50117987  1.0458000          
   2    1.5640117  inv           1.376720  0.53063319  0.9554363          
   2    2.5719608  triangular    1.389588  0.54342375  1.0176922          
   3    2.6384526  rectangular   1.186496  0.77857601  0.8884190          
   4    0.3023811  biweight      1.896862  0.13978323  1.1693556          
   4    2.4236870  biweight      1.249380  0.57723223  0.9347736          
   4    2.6277996  rectangular   1.186496  0.77857601  0.8884190  *       
   6    1.9570719  triangular    1.292164  0.69450218  0.8791144          
   6    2.8189368  epanechnikov  1.270864  0.69843121  0.8935024          
   7    0.1582682  epanechnikov  1.915093  0.09732874  1.2437451          
   7    2.6854946  inv           1.357733  0.74743114  0.9884055          
   8    1.1643164  triweight     1.432924  0.46684488  0.9769804          
   8    2.1441447  triweight     1.194945  0.65103755  0.8399249          
   8    2.6690593  triangular    1.306274  0.70269619  0.9261648          
   9    0.7548715  gaussian      1.653368  0.32586025  1.0700038          
   9    2.5543691  triangular    1.311941  0.70030608  0.9273257          
  10    0.3325506  biweight      1.842056  0.14925581  1.1275720          
  10    2.7838124  triangular    1.265145  0.70723741  0.8916550          
  11    0.1537525  triangular    1.941144  0.08743500  1.2684310          
  13    1.9029965  biweight      1.254107  0.70763127  0.8783324          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were kmax = 4, distance = 2.6278 and
 kernel = rectangular.
[1] "Sat Mar 10 17:13:42 2018"
k-Nearest Neighbors 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  k   RMSE      Rsquared    MAE       Selected
   1  2.252374  0.03997083  1.558131          
   2  1.828819  0.20000695  1.250052          
   3  1.650638  0.32044915  1.138092  *       
   4  1.725646  0.24188971  1.115340          
   6  1.694564  0.39456938  1.162518          
   7  1.670133  0.41857707  1.136020          
   8  1.683024  0.42744595  1.146638          
   9  1.671753  0.42668092  1.152825          
  10  1.731076  0.32807239  1.204317          
  11  1.766080  0.30587621  1.227553          
  13  1.763781  0.39484778  1.238595          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 3.
[1] "Sat Mar 10 17:13:55 2018"
Polynomial Kernel Regularized Least Squares 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  lambda        degree  RMSE        Rsquared    MAE         Selected
  1.535679e-05  3         1.921999  0.06212723    1.433470          
  2.998809e-05  2         1.907686  0.44287823    1.416565  *       
  4.802236e-05  3         1.921999  0.06212742    1.433470          
  7.392977e-05  3         1.921999  0.06212757    1.433470          
  1.900862e-04  3         1.921999  0.06212824    1.433470          
  2.461635e-04  1       699.271876  0.07746051  572.044398          
  3.335473e-04  3         1.921999  0.06212908    1.433470          
  1.009586e-03  2         1.907687  0.44292687    1.416565          
  1.957200e-03  3         1.921999  0.06213853    1.433469          
  2.587399e-03  1        66.540121  0.07763328   54.518043          
  4.918737e-03  3         1.921998  0.06215581    1.433468          
  7.802485e-03  3         1.921997  0.06217269    1.433466          
  7.937213e-03  3         1.921997  0.06217348    1.433466          
  9.939568e-03  2         1.907694  0.44336626    1.416565          
  2.030401e-02  1         8.667278  0.07899560    7.154437          
  2.267489e-02  3         1.921994  0.06226058    1.433459          
  6.646719e-02  1         3.180517  0.08298451    2.580255          
  6.784264e-02  3         1.921985  0.06253607    1.433436          
  1.502472e-01  1         2.182952  0.09169815    1.631907          
  6.320047e-01  2         1.908238  0.46198498    1.416760          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 2.998809e-05 and degree = 2.
[1] "Sat Mar 10 17:14:09 2018"

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 2.9812554426  0.0245098888 -0.0004544292  0.0403902667  0.0092865979 
           V7            V8            V9           V10 
-0.0600071329 -0.0118438276 -0.0350468393 -0.0013846950 

 Quartiles of Marginal Effects:
 
          V2         V3            V4         V5          V6          V7
25% 2.975743 0.01973859 -0.0032744046 0.03642039 0.005850323 -0.06271988
50% 2.984182 0.02427715 -0.0003869159 0.04136901 0.010520656 -0.06020141
75% 2.989860 0.02805569  0.0028478673 0.04596538 0.013230524 -0.05658248
              V8          V9           V10
25% -0.015970477 -0.03815310 -0.0064042815
50% -0.011768234 -0.03582929 -0.0009957284
75% -0.008590871 -0.03193401  0.0027046563

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.827185309 -0.046922366  0.002749339 -0.154548678  0.021748372 -0.007267677 
          V8           V9          V10 
 0.015644768  0.068388415  0.142816843 

 Quartiles of Marginal Effects:
 
           V2           V3          V4          V5           V6           V7
25% 0.4542006 -0.261413978 -0.05468888 -0.21764468 -0.143969770 -0.071020013
50% 0.7557171  0.001653658 -0.01512253 -0.09243079  0.003956207 -0.006332978
75% 1.2122720  0.095439253  0.06464488 -0.04968951  0.123252616  0.056462485
             V8           V9        V10
25% -0.04327795 -0.001853514 0.02033952
50%  0.01775258  0.036608088 0.15619314
75%  0.11140415  0.081864899 0.20997702

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 1.456209e-01  8.268658e-03  1.196902e-02 -5.123100e-03  2.503071e-02 
           V7            V8            V9           V10 
 5.136887e-05  1.345234e-02  1.766484e-02  4.154422e-02 

 Quartiles of Marginal Effects:
 
           V2          V3         V4           V5         V6           V7
25% 0.1455541 0.008235204 0.01195656 -0.005161131 0.02500370 2.476166e-05
50% 0.1456757 0.008257086 0.01198027 -0.005128655 0.02504778 4.654305e-05
75% 0.1457314 0.008298700 0.01198875 -0.005087586 0.02506431 8.123609e-05
            V8         V9        V10
25% 0.01341252 0.01763987 0.04151699
50% 0.01346167 0.01766655 0.04156414
75% 0.01348609 0.01768766 0.04158943

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 2.926457e-01 -3.747433e-02 -9.311955e-03 -6.634801e-02  6.462992e-05 
           V7            V8            V9           V10 
-1.789440e-02  7.764620e-03  3.876370e-02  7.894202e-02 

 Quartiles of Marginal Effects:
 
            V2          V3           V4          V5          V6           V7
25% 0.05241503 -0.14630003 -0.047509792 -0.10053798 -0.08268659 -0.073054835
50% 0.22555983 -0.01513128 -0.002477493 -0.03618555 -0.01914949 -0.003410438
75% 0.44031895  0.06510593  0.022812687  0.01491727  0.05136199  0.040106181
              V8          V9        V10
25% -0.051006051 -0.02679175 0.02460875
50%  0.004508775  0.01823317 0.06193879
75%  0.108400957  0.03528633 0.09768752

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.223379374 -0.032292242 -0.009201420 -0.052037177 -0.001740300 -0.018350296 
          V8           V9          V10 
 0.006747676  0.031408025  0.063488030 

 Quartiles of Marginal Effects:
 
            V2          V3           V4          V5          V6          V7
25% 0.00757118 -0.15040240 -0.038245254 -0.08614870 -0.08633538 -0.05794689
50% 0.16618807 -0.01100872 -0.001005533 -0.02801765 -0.01832750 -0.00410816
75% 0.33960356  0.08108777  0.013753161  0.02161281  0.03914117  0.03272454
              V8          V9         V10
25% -0.048846659 -0.02204284 0.008369428
50%  0.001289442  0.01064958 0.040606306
75%  0.087233664  0.03054723 0.078897488

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.96006115  0.07491764  0.04400312 -0.12490694  0.14398107  0.03648105 
         V8          V9         V10 
 0.03315266  0.09862287  0.22297907 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5        V6         V7         V8
25% 1.953560 0.07112286 0.04317512 -0.1281874 0.1423379 0.03423337 0.03010969
50% 1.963345 0.07419977 0.04436363 -0.1259201 0.1446295 0.03584057 0.03366604
75% 1.968770 0.07843398 0.04496810 -0.1219044 0.1455464 0.03921035 0.03611332
            V9       V10
25% 0.09677421 0.2207996
50% 0.09832524 0.2236119
75% 0.10001974 0.2254637

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.31343315  0.03278052  0.04464721 -0.10958499  0.12374279  0.03977238 
         V8          V9         V10 
 0.02676643  0.10478527  0.19237504 

 Quartiles of Marginal Effects:
 
          V2          V3         V4         V5         V6           V7
25% 1.202678 -0.03235327 0.02776393 -0.1414219 0.09607039 -0.005138595
50% 1.357667  0.02735000 0.04996557 -0.1259683 0.12841812  0.044183601
75% 1.506570  0.07867348 0.06218513 -0.0563961 0.16674316  0.093628816
             V8         V9       V10
25% -0.02922606 0.07917719 0.1323057
50%  0.03757286 0.10298219 0.2066191
75%  0.08674042 0.13637612 0.2488103

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.2739759078 -0.0362515160 -0.0093568729 -0.0625749368 -0.0005252389 
           V7            V8            V9           V10 
-0.0181501365  0.0074893107  0.0369048074  0.0750223836 

 Quartiles of Marginal Effects:
 
           V2          V3           V4          V5          V6           V7
25% 0.0381834 -0.14896142 -0.045085978 -0.09369689 -0.08018042 -0.070045428
50% 0.2092595 -0.01394981 -0.001561912 -0.03416163 -0.01928989 -0.003754298
75% 0.4021655  0.07009020  0.019772839  0.01546185  0.04809274  0.038219221
              V8          V9        V10
25% -0.050639626 -0.02574294 0.02846191
50%  0.003515198  0.01837960 0.05078469
75%  0.103000441  0.03417894 0.09283675

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.073526164 -0.015158481 -0.006142871 -0.018470818 -0.002702233 -0.013102313 
          V8           V9          V10 
 0.003718784  0.010485143  0.021409172 

 Quartiles of Marginal Effects:
 
              V2            V3            V4           V5           V6
25% -0.002193404 -0.0818763234 -2.090009e-02 -0.025587837 -0.037479488
50%  0.032612094 -0.0000636483 -1.432626e-05 -0.005444966 -0.001067401
75%  0.101336264  0.0336640018  5.031412e-03  0.009250955  0.016260086
              V7            V8           V9           V10
25% -0.033195174 -0.0322366339 -0.035119509 -0.0009808344
50% -0.003454118  0.0007896628 -0.002569215  0.0130818921
75%  0.011622602  0.0253878727  0.017029421  0.0391494274

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 2.195975e+00 -1.271188e-02  3.294542e-02 -1.286397e-01  4.705408e-02 
           V7            V8            V9           V10 
-2.200024e-02  7.226123e-05  4.555367e-02  1.035532e-01 

 Quartiles of Marginal Effects:
 
          V2          V3          V4         V5          V6          V7
25% 1.820681 -0.14885375 -0.01840530 -0.2020931 -0.05363729 -0.08029821
50% 2.212068 -0.03269236  0.03989386 -0.1237824  0.03638227  0.01602510
75% 2.589128  0.11437956  0.07635761 -0.0461012  0.13312186  0.04753710
             V8          V9         V10
25% -0.10457404 -0.01255859 -0.05421334
50%  0.03062137  0.05911097  0.08630635
75%  0.09436619  0.10982130  0.22051857

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.200744815 -0.030341332 -0.009047875 -0.047254166 -0.002298170 -0.018348474 
          V8           V9          V10 
 0.006382758  0.028697877  0.057951529 

 Quartiles of Marginal Effects:
 
             V2           V3            V4          V5          V6           V7
25% 0.003806919 -0.148920802 -0.0355103648 -0.07235543 -0.08560108 -0.052058756
50% 0.146844912 -0.009666309 -0.0006307264 -0.02499317 -0.01726071 -0.004013009
75% 0.312629614  0.077797156  0.0143404699  0.02529828  0.03467311  0.029544476
               V8           V9          V10
25% -0.0480330485 -0.019880898 -0.003580874
50%  0.0007242892  0.007259155  0.037971816
75%  0.0792423570  0.028299302  0.072226492

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.7106483120 -0.0476375881 -0.0008725358 -0.1388439227  0.0169430937 
           V7            V8            V9           V10 
-0.0089096850  0.0140799960  0.0646258331  0.1349378711 

 Quartiles of Marginal Effects:
 
           V2           V3          V4         V5           V6          V7
25% 0.3734939 -0.264172028 -0.05207222 -0.2035293 -0.110333436 -0.05833972
50% 0.6458732 -0.007025406 -0.01619954 -0.0772084 -0.005826691 -0.02004670
75% 1.0639857  0.099923956  0.07060539 -0.0313840  0.110698667  0.04992695
             V8          V9        V10
25% -0.04804441 -0.01316001 0.01697629
50%  0.01502762  0.03751938 0.14210276
75%  0.12914969  0.08120339 0.19624039

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.73558932 -0.02412122  0.02755278 -0.18474730  0.04893450 -0.01162505 
         V8          V9         V10 
 0.01353159  0.06786373  0.14033898 

 Quartiles of Marginal Effects:
 
          V2          V3          V4          V5          V6          V7
25% 1.317906 -0.17157950 -0.03475173 -0.27072787 -0.08039971 -0.07679917
50% 1.719448 -0.08671915  0.01714607 -0.16251744  0.03172603  0.03378176
75% 2.182142  0.15200033  0.08739156 -0.07880647  0.15707687  0.09917105
             V8         V9          V10
25% -0.07179391 0.02081543 -0.008161463
50%  0.03557171 0.08831291  0.150277222
75%  0.13958541 0.12815896  0.246179156

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.12106720  0.07643131  0.03828892 -0.12357067  0.13659339  0.03238721 
         V8          V9         V10 
 0.02623504  0.08885694  0.21066429 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5        V6         V7         V8
25% 2.119800 0.07574741 0.03821346 -0.1241294 0.1362633 0.03196057 0.02567385
50% 2.121645 0.07642367 0.03835666 -0.1237007 0.1366897 0.03227782 0.02646867
75% 2.122550 0.07711452 0.03843051 -0.1230287 0.1368910 0.03289906 0.02676688
            V9       V10
25% 0.08853811 0.2102090
50% 0.08878999 0.2107520
75% 0.08909855 0.2110629

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.091999031 -0.017593418 -0.006702945 -0.022812856 -0.002562426 -0.014199927 
          V8           V9          V10 
 0.004184779  0.013498780  0.027378174 

 Quartiles of Marginal Effects:
 
             V2            V3            V4          V5           V6
25% -0.00258126 -0.1028778087 -2.043209e-02 -0.04099392 -0.047020693
50%  0.05055771 -0.0001114359 -4.226205e-05 -0.01097185 -0.006211146
75%  0.13433263  0.0431688326  7.851401e-03  0.01230563  0.022900170
              V7            V8          V9          V10
25% -0.038789452 -0.0364548531 -0.04187528 -0.001505047
50% -0.002446789  0.0002843864 -0.00282548  0.016137738
75%  0.014860866  0.0350997141  0.01970296  0.044793510

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.437972772 -0.043414111 -0.007432027 -0.093541733  0.006398463 -0.014164346 
          V8           V9          V10 
 0.010144022  0.050737571  0.103897215 

 Quartiles of Marginal Effects:
 
           V2          V3           V4           V5           V6          V7
25% 0.1732222 -0.18059485 -0.047230399 -0.145742420 -0.084189841 -0.06920920
50% 0.3918244 -0.02614016 -0.005857492 -0.047210270 -0.009587289 -0.01016369
75% 0.6776705  0.07685310  0.049390314  0.004249243  0.075984118  0.05297588
             V8          V9         V10
25% -0.06378002 -0.01860683 0.003395366
50%  0.01146061  0.01256374 0.091567928
75%  0.12419081  0.04806888 0.134743202

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.406309584 -0.043085605 -0.008227737 -0.088358870  0.004183832 -0.015603600 
          V8           V9          V10 
 0.009449212  0.048401820  0.099644936 

 Quartiles of Marginal Effects:
 
           V2          V3           V4           V5          V6          V7
25% 0.1429087 -0.16657847 -0.050328777 -0.137784209 -0.08481580 -0.06878810
50% 0.3454007 -0.02306604 -0.005601933 -0.046246449 -0.01324033 -0.01257677
75% 0.6299348  0.07717710  0.043290079  0.007689688  0.06982443  0.05055406
              V8          V9         V10
25% -0.054915339 -0.02313840 0.001947176
50%  0.008330524  0.01361093 0.086068041
75%  0.129939679  0.04139862 0.124602421

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.68939877  0.03585174  0.01970400 -0.04514147  0.06112369 -0.01483362 
         V8          V9         V10 
-0.02343164  0.01915448  0.08667439 

 Quartiles of Marginal Effects:
 
          V2         V3         V4          V5         V6           V7
25% 2.656966 0.01255293 0.01139058 -0.06779620 0.05266518 -0.031614480
50% 2.712385 0.03039716 0.01929807 -0.04376782 0.06283104 -0.018499563
75% 2.730275 0.06155694 0.03105989 -0.02355668 0.07720801  0.005869455
              V8         V9        V10
25% -0.043858401 0.00440370 0.06528685
50% -0.018662041 0.01608311 0.09014558
75% -0.004782002 0.02843155 0.10785967

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.48414878  0.02640542  0.03335721 -0.02363237  0.07424545  0.00451509 
         V8          V9         V10 
 0.03812263  0.05352112  0.12170067 

 Quartiles of Marginal Effects:
 
           V2         V3         V4          V5         V6          V7
25% 0.4837507 0.02619554 0.03329342 -0.02386210 0.07412094 0.004368752
50% 0.4844559 0.02633069 0.03341380 -0.02368253 0.07432027 0.004489650
75% 0.4847761 0.02657890 0.03346120 -0.02344813 0.07442250 0.004696153
            V8         V9       V10
25% 0.03789987 0.05338203 0.1215497
50% 0.03816812 0.05352272 0.1217864
75% 0.03831368 0.05364359 0.1219454

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.676706200 -0.005627086  0.039265393 -0.021717324  0.037218884 -0.029191789 
          V8           V9          V10 
-0.029698104  0.006285735  0.044728133 

 Quartiles of Marginal Effects:
 
          V2           V3           V4          V5          V6           V7
25% 2.439819 -0.103454337 -0.006462895 -0.07928013 -0.02340415 -0.092641732
50% 2.741315  0.004181705  0.042620398 -0.03189249  0.04492329 -0.006913078
75% 2.908337  0.041772722  0.087256421  0.03735529  0.08820713  0.045198340
              V8          V9         V10
25% -0.076059552 -0.02501304 -0.09061534
50% -0.005894129  0.01624574  0.02260591
75%  0.031975397  0.05164305  0.13540723

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 2.9600643746  0.1214582169  0.0017984566  0.0225808780 -0.0362100958 
           V7            V8            V9           V10 
-0.0150977349 -0.0455925441 -0.0007843483 -0.0538082538 

 Quartiles of Marginal Effects:
 
          V2        V3          V4         V5          V6          V7
25% 2.953424 0.1195339 0.001150555 0.01991902 -0.03899469 -0.01717500
50% 2.959718 0.1214769 0.001826847 0.02328758 -0.03582542 -0.01566168
75% 2.967465 0.1242868 0.002566251 0.02574412 -0.03297420 -0.01371791
             V8            V9         V10
25% -0.04815257 -0.0025795364 -0.05683750
50% -0.04553857 -0.0006771520 -0.05275300
75% -0.04284189  0.0004757908 -0.05074973

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.11827118 -0.03611238 -0.05154392  0.03538091  0.05329623  0.01494359 
         V8          V9         V10 
 0.01876306  0.05207856  0.08871852 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6          V7
25% 0.5543127 -0.12127148 -0.15244006 -0.02791354 -0.05349007 -0.11491198
50% 1.0426150 -0.01770883 -0.05360922  0.05202208  0.02774573  0.06362088
75% 1.4092535  0.04915170  0.04889648  0.13384332  0.15416821  0.17856084
             V8           V9         V10
25% -0.06699844 -0.003065805 0.004003349
50%  0.03000436  0.087146478 0.062112932
75%  0.16221694  0.174041942 0.185883469

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
0.1575269853 0.0255734767 0.0004005104 0.0254851886 0.0027575055 0.0238873010 
          V8           V9          V10 
0.0030831226 0.0265368567 0.0230275224 

 Quartiles of Marginal Effects:
 
           V2         V3           V4         V5          V6         V7
25% 0.1574215 0.02555996 0.0003879073 0.02546453 0.002747187 0.02386527
50% 0.1575643 0.02557720 0.0003975567 0.02549686 0.002759638 0.02390294
75% 0.1576401 0.02560992 0.0004125132 0.02552054 0.002771985 0.02391795
             V8         V9        V10
25% 0.003066994 0.02651272 0.02299947
50% 0.003085390 0.02655400 0.02303903
75% 0.003102253 0.02657124 0.02306445

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.48356501 -0.04018259 -0.03289776 -0.01950315  0.03231754  0.01709771 
         V8          V9         V10 
 0.01495566  0.03874545  0.05919951 

 Quartiles of Marginal Effects:
 
            V2          V3          V4           V5          V6          V7
25% 0.07047428 -0.09326138 -0.08462789 -0.093557448 -0.03510054 -0.04054989
50% 0.29430555  0.00432615 -0.02876107 -0.005047084  0.02919383  0.01499404
75% 0.60775901  0.02981313  0.03801840  0.059141613  0.08587422  0.11232629
             V8          V9         V10
25% -0.06357682 -0.01458391 -0.07253543
50%  0.01674708  0.04171801  0.04268754
75%  0.09711266  0.11451749  0.19755181

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.38131738 -0.03793164 -0.02675378 -0.02497245  0.02542028  0.01282279 
         V8          V9         V10 
 0.01216645  0.03189958  0.05123778 

 Quartiles of Marginal Effects:
 
            V2           V3          V4           V5          V6          V7
25% 0.02777634 -0.086747926 -0.06727300 -0.105179311 -0.03000822 -0.03765815
50% 0.19462312  0.003425638 -0.02474098 -0.003028481  0.02487173  0.01374892
75% 0.47865978  0.028917158  0.02893532  0.055029870  0.06525612  0.09234531
              V8          V9         V10
25% -0.051795105 -0.01486168 -0.05759899
50%  0.007610941  0.03666913  0.02739313
75%  0.074120125  0.09338390  0.16401584

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.079413735  0.135840118  0.015411236  0.101492721  0.014384629  0.081867224 
          V8           V9          V10 
-0.004457014  0.078678562  0.009497490 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5         V6         V7           V8
25% 2.073253 0.1341163 0.01474351 0.09945765 0.01308552 0.08098125 -0.005857250
50% 2.081016 0.1364488 0.01540428 0.10260551 0.01477839 0.08263016 -0.004767302
75% 2.088233 0.1377552 0.01604855 0.10332898 0.01585658 0.08332311 -0.002973469
            V9         V10
25% 0.07735640 0.007720974
50% 0.08002237 0.009747170
75% 0.08061503 0.011605318

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 1.497118626  0.104693554  0.007027689  0.089391549  0.019938679  0.074685103 
          V8           V9          V10 
-0.003026019  0.086148184  0.038427581 

 Quartiles of Marginal Effects:
 
          V2         V3           V4        V5           V6         V7
25% 1.284370 0.05916402 -0.009281348 0.0432963 -0.004392346 0.04781216
50% 1.494675 0.10518563 -0.001797887 0.1006840  0.032633086 0.09925092
75% 1.675821 0.14852450  0.020185321 0.1295788  0.053068681 0.11807839
              V8         V9         V10
25% -0.028376133 0.05438872 0.000993978
50% -0.004257279 0.09667727 0.055677592
75%  0.041794008 0.13142026 0.075342727

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.45665220 -0.03981591 -0.03136877 -0.02119702  0.03061787  0.01604642 
         V8          V9         V10 
 0.01427240  0.03707984  0.05727373 

 Quartiles of Marginal Effects:
 
           V2           V3          V4           V5          V6          V7
25% 0.0568725 -0.093206197 -0.08029075 -0.099704895 -0.03394879 -0.03958921
50% 0.2662665  0.003080733 -0.02780646 -0.004456697  0.02831871  0.01492098
75% 0.5737058  0.028506922  0.03564936  0.058451636  0.08147155  0.10573852
             V8          V9         V10
25% -0.06062241 -0.01453584 -0.07176004
50%  0.01416653  0.04071059  0.03835973
75%  0.09311916  0.10655897  0.18703184

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 1.414975e-01 -2.244786e-02 -8.066920e-03 -2.643780e-02  5.651523e-03 
           V7            V8            V9           V10 
 8.696122e-05  4.428897e-03  1.126860e-02  2.602764e-02 

 Quartiles of Marginal Effects:
 
              V2            V3            V4            V5            V6
25% 2.344105e-05 -0.0550097854 -0.0183890473 -0.0585859230 -0.0164425982
50% 4.135497e-02  0.0003005907  0.0008524053 -0.0003219746 -0.0004707971
75% 1.369428e-01  0.0226230995  0.0208054711  0.0210617700  0.0258811087
              V7            V8           V9          V10
25% -0.007374038 -0.0241410294 -0.009022185 -0.015468299
50%  0.002837919 -0.0001045303  0.005879856  0.005072899
75%  0.027256863  0.0208275349  0.029602443  0.063721324

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.233980103  0.003593988 -0.012842292  0.115135093  0.031099015 -0.026370502 
          V8           V9          V10 
 0.009062273  0.009048300  0.087385680 

 Quartiles of Marginal Effects:
 
          V2          V3          V4         V5          V6          V7
25% 1.935542 -0.07441159 -0.07147542 0.03491907 -0.03541087 -0.10573410
50% 2.250283  0.01281265 -0.01743297 0.13792573  0.04395772 -0.02524674
75% 2.617296  0.05965703  0.05227183 0.22874875  0.08721005  0.12621405
              V8           V9        V10
25% -0.083785983 -0.031949541 0.02204793
50% -0.007648713  0.004710826 0.11697377
75%  0.094955207  0.083953431 0.15456358

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.34692742 -0.03695242 -0.02441916 -0.02659717  0.02287058  0.01103943 
         V8          V9         V10 
 0.01109134  0.02930570  0.04837655 

 Quartiles of Marginal Effects:
 
            V2           V3          V4           V5          V6          V7
25% 0.01752523 -0.083431192 -0.06079182 -0.103120894 -0.02789605 -0.03785624
50% 0.16304690  0.002742216 -0.02317027 -0.002449423  0.02077254  0.01271507
75% 0.43000482  0.031688789  0.02574312  0.052983066  0.05707720  0.08161650
             V8          V9         V10
25% -0.04722043 -0.01463906 -0.04984891
50%  0.00454634  0.03433155  0.02263246
75%  0.06892508  0.08629337  0.15175910

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.99785551 -0.03790552 -0.05032289  0.02371100  0.05171634  0.01891523 
         V8          V9         V10 
 0.01944813  0.05300749  0.08497922 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5         V6          V7
25% 0.4106693 -0.11093840 -0.15773222 -0.04185189 -0.0482966 -0.10320982
50% 0.9028982 -0.01811954 -0.06157780  0.03000880  0.0184324  0.04733185
75% 1.2660915  0.05713655  0.05515421  0.13913481  0.1450062  0.18768036
             V8          V9         V10
25% -0.07284912 0.002585305 -0.02519878
50%  0.03867707 0.075968342  0.05982969
75%  0.15596759 0.176769462  0.19220370

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 1.904469378 -0.007336731 -0.029724667  0.099690891  0.044143448 -0.019137684 
          V8           V9          V10 
 0.009040476  0.025463488  0.090250877 

 Quartiles of Marginal Effects:
 
          V2           V3          V4          V5          V6           V7
25% 1.541112 -0.089944891 -0.10302748 0.006328725 -0.05454875 -0.117548875
50% 1.883354 -0.007935422 -0.03434807 0.101298945  0.04992895  0.006981486
75% 2.188230  0.053757792  0.03686750 0.209190683  0.12292060  0.118231411
             V8          V9        V10
25% -0.06978909 -0.03426300 0.02187633
50%  0.03385942  0.04870046 0.12500811
75%  0.13826179  0.11538757 0.15870768

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.236143374  0.135794911  0.014665099  0.093784894  0.009132607  0.071958444 
          V8           V9          V10 
-0.008117981  0.068097219 -0.003150188 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5          V6         V7
25% 2.235326 0.1354643 0.01453582 0.09341275 0.008923486 0.07180147
50% 2.236417 0.1358815 0.01466623 0.09396653 0.009192195 0.07210138
75% 2.237677 0.1361267 0.01477055 0.09410177 0.009427041 0.07218610
              V8         V9          V10
25% -0.008369100 0.06783665 -0.003450947
50% -0.008155249 0.06831235 -0.003151921
75% -0.007906086 0.06841479 -0.002775866

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.171176814 -0.025313791 -0.011069460 -0.026623389  0.008556756  0.002101530 
          V8           V9          V10 
 0.005392229  0.014179135  0.029398941 

 Quartiles of Marginal Effects:
 
              V2            V3            V4            V5            V6
25% 0.0004273646 -0.0629974269 -0.0258006522 -0.0715774569 -0.0195616773
50% 0.0534771236  0.0005362768 -0.0002804904 -0.0005440287  0.0002925257
75% 0.1799926317  0.0255177597  0.0200595671  0.0305422658  0.0299820584
              V7            V8           V9          V10
25% -0.015589884 -0.0239323983 -0.007589840 -0.019929732
50%  0.003808234 -0.0002985632  0.008331172  0.006652288
75%  0.036336957  0.0291852901  0.038379710  0.076366990

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.678684865 -0.039157945 -0.041846149 -0.003920498  0.041837590  0.022499413 
          V8           V9          V10 
 0.018417100  0.047841592  0.070255462 

 Quartiles of Marginal Effects:
 
           V2           V3          V4          V5          V6          V7
25% 0.1836329 -0.079440490 -0.12222883 -0.09052840 -0.04397513 -0.03091961
50% 0.5283954  0.008433467 -0.03320084  0.01928322  0.02897422  0.02256981
75% 0.8847577  0.041604723  0.05239082  0.06039017  0.11018819  0.15710153
             V8           V9         V10
25% -0.08180189 -0.005616479 -0.04889384
50%  0.02784381  0.057274899  0.04007865
75%  0.12153021  0.147568845  0.21485245

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.639215327 -0.040946042 -0.040424726 -0.008200788  0.040691182  0.021295540 
          V8           V9          V10 
 0.017980380  0.046509724  0.069007826 

 Quartiles of Marginal Effects:
 
           V2          V3          V4           V5          V6          V7
25% 0.1577799 -0.08481070 -0.11425941 -0.091294717 -0.04044827 -0.03014773
50% 0.4707575  0.01016310 -0.03269619 -0.006326725  0.03118490  0.02021993
75% 0.8336885  0.03980546  0.05101537  0.059697371  0.10572761  0.14915489
             V8           V9         V10
25% -0.07870648 -0.008720693 -0.05513437
50%  0.03162286  0.054418175  0.04197187
75%  0.11537686  0.140148679  0.22611758

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.759683069  0.120734243  0.008450912  0.040099892 -0.021992156 -0.005817986 
          V8           V9          V10 
-0.038080740  0.017335033 -0.036465221 

 Quartiles of Marginal Effects:
 
          V2        V3          V4         V5           V6           V7
25% 2.722284 0.1099625 0.005046572 0.03282075 -0.031166467 -0.007654893
50% 2.764610 0.1220706 0.006756776 0.03918420 -0.019704603 -0.004505909
75% 2.814058 0.1311823 0.012464248 0.04993594 -0.006959088  0.002183077
             V8          V9         V10
25% -0.04814607 0.009565956 -0.04837966
50% -0.04047974 0.018975900 -0.03578606
75% -0.02817545 0.025142651 -0.02281201

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
0.520937556 0.071702401 0.002711561 0.070828751 0.010791831 0.066636144 
         V8          V9         V10 
0.008034999 0.071764036 0.057270007 

 Quartiles of Marginal Effects:
 
           V2         V3          V4         V5         V6         V7
25% 0.5203556 0.07160924 0.002637529 0.07070804 0.01073523 0.06652120
50% 0.5211376 0.07173650 0.002700345 0.07091403 0.01079719 0.06671378
75% 0.5215646 0.07187522 0.002776471 0.07100938 0.01088498 0.06679136
             V8         V9        V10
25% 0.007942463 0.07163591 0.05714221
50% 0.008044959 0.07185646 0.05736118
75% 0.008148643 0.07196315 0.05743928

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.570929620  0.027214097  0.004909862  0.101881978  0.010406478 -0.022532857 
          V8           V9          V10 
-0.003929025  0.003669113  0.076903394 

 Quartiles of Marginal Effects:
 
          V2          V3            V4         V5           V6            V7
25% 2.336224 -0.03869705 -0.0387468149 0.02362906 -0.021591128 -0.1056590093
50% 2.470050  0.04943407 -0.0001939127 0.10051849  0.005246153 -0.0008062444
75% 2.853990  0.09832340  0.0400037043 0.16524245  0.047718816  0.0682188097
             V8          V9        V10
25% -0.07424042 -0.03864923 0.03803687
50% -0.01908159 -0.00128035 0.07627451
75%  0.02884698  0.04404712 0.13719099

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.957519198  0.056956083  0.064660438  0.045311133 -0.059303880 -0.014779368 
          V8           V9          V10 
 0.049751026 -0.007072298 -0.043525361 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5          V6          V7
25% 2.949692 0.05451048 0.05970034 0.04032393 -0.06243207 -0.01924744
50% 2.957062 0.05784120 0.06380246 0.04752060 -0.05979323 -0.01349650
75% 2.967200 0.06040330 0.06771958 0.05233449 -0.05749784 -0.00848660
            V8           V9         V10
25% 0.04456691 -0.009404161 -0.04580318
50% 0.04919930 -0.006507128 -0.04292821
75% 0.05334189 -0.003966525 -0.04060766

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.98602805  0.01016782 -0.02698556 -0.05233215  0.01078293  0.03685155 
         V8          V9         V10 
 0.04796689  0.05947361 -0.01739346 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6           V7
25% 0.4480157 -0.09481442 -0.14611943 -0.15819962 -0.06004040 -0.050356829
50% 0.8491384  0.01987776 -0.03887271 -0.05186509  0.01998025  0.002563151
75% 1.4760434  0.10754468  0.10442615  0.03800659  0.11171065  0.116976620
             V8          V9         V10
25% -0.06182620 -0.04706961 -0.08893558
50%  0.05274585  0.04727635 -0.01498631
75%  0.15834378  0.17923338  0.04701523

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.1619659803  0.0094789075 -0.0146747206 -0.0372015647  0.0009420406 
           V7            V8            V9           V10 
-0.0065516920  0.0269531209 -0.0075427715  0.0124235277 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5           V6           V7
25% 0.1618876 0.009461526 -0.01469351 -0.03725325 0.0009248415 -0.006575686
50% 0.1619847 0.009487368 -0.01467806 -0.03720836 0.0009480396 -0.006559731
75% 0.1620707 0.009505122 -0.01466761 -0.03716434 0.0009678056 -0.006528590
            V8           V9        V10
25% 0.02691797 -0.007567951 0.01241243
50% 0.02696881 -0.007544331 0.01242439
75% 0.02699472 -0.007528250 0.01243654

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.371933148 -0.007576019 -0.026634823 -0.045352773  0.038351934  0.046656367 
          V8           V9          V10 
 0.046802376  0.017379749  0.037646500 

 Quartiles of Marginal Effects:
 
            V2           V3          V4          V5          V6           V7
25% 0.01837757 -0.055160174 -0.09622525 -0.09122389 -0.02630615 -0.029112777
50% 0.21654182 -0.007571763 -0.01711462 -0.03263306  0.02065036  0.007189928
75% 0.51180850  0.040523156  0.07683233  0.02937511  0.11656246  0.107483549
             V8           V9         V10
25% -0.03343570 -0.027810283 -0.02299834
50%  0.01985860  0.004992754  0.02005262
75%  0.08500079  0.106368729  0.07989285

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.290758220 -0.008551494 -0.021750991 -0.040022037  0.035670120  0.041201568 
          V8           V9          V10 
 0.042265436  0.013706569  0.039219388 

 Quartiles of Marginal Effects:
 
             V2          V3          V4          V5          V6           V7
25% 0.003316874 -0.04498271 -0.09176080 -0.07580369 -0.01576629 -0.016198977
50% 0.155211059 -0.01001815 -0.01190030 -0.02505264  0.01325388  0.004715525
75% 0.398455474  0.03163004  0.07688262  0.03066928  0.10019121  0.086522468
             V8          V9         V10
25% -0.03249353 -0.01986326 -0.01208506
50%  0.01069098  0.01384335  0.01802027
75%  0.07559031  0.08614129  0.06894912

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.098893017  0.079737883 -0.022493381 -0.105484912 -0.058724870 -0.054425134 
          V8           V9          V10 
 0.102329765 -0.001122017  0.010208486 

 Quartiles of Marginal Effects:
 
          V2         V3          V4         V5          V6          V7
25% 2.090268 0.07829593 -0.02365166 -0.1090583 -0.06051627 -0.05569638
50% 2.100928 0.08001300 -0.02306940 -0.1054189 -0.05876607 -0.05481556
75% 2.106283 0.08203266 -0.02154440 -0.1021582 -0.05726921 -0.05261481
            V8            V9        V10
25% 0.09954629 -0.0022154018 0.00911882
50% 0.10275534 -0.0008054850 0.01032281
75% 0.10427106  0.0002990635 0.01109060

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.49861053  0.07687128 -0.03390213 -0.09660780 -0.04678465 -0.03002381 
         V8          V9         V10 
 0.07651837  0.03015979  0.01218756 

 Quartiles of Marginal Effects:
 
          V2         V3          V4          V5          V6          V7
25% 1.294938 0.04620894 -0.05954678 -0.14674617 -0.08794128 -0.06301971
50% 1.498141 0.09284271 -0.04486142 -0.09407609 -0.04182883 -0.02459103
75% 1.665308 0.12118639 -0.01886489 -0.02884075 -0.01305162  0.01266736
             V8          V9         V10
25% 0.005127746 0.006783446 -0.01071976
50% 0.094522812 0.041631490  0.01456696
75% 0.129159136 0.074332767  0.03920595

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.350238913 -0.007959013 -0.025460348 -0.044090821  0.037956135  0.045487570 
          V8           V9          V10 
 0.045812870  0.016283706  0.038377156 

 Quartiles of Marginal Effects:
 
            V2          V3          V4          V5          V6           V7
25% 0.01109406 -0.05167320 -0.09555841 -0.08712878 -0.02336228 -0.026079324
50% 0.19881533 -0.00855676 -0.01580969 -0.03235360  0.01858601  0.006029053
75% 0.48252105  0.03840366  0.07964939  0.03025400  0.11351196  0.102442251
             V8           V9         V10
25% -0.03329386 -0.024410130 -0.01887347
50%  0.01726692  0.007126915  0.02055976
75%  0.08368174  0.101343741  0.07748683

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.108296509 -0.007840509 -0.005026358 -0.021536842  0.019724027  0.019083872 
          V8           V9          V10 
 0.023701757  0.010229939  0.025469855 

 Quartiles of Marginal Effects:
 
              V2           V3            V4            V5           V6
25% -0.001013683 -0.024693481 -0.0212810185 -0.0334274334 -0.003545134
50%  0.031459561 -0.001327111 -0.0005758079 -0.0009228372  0.001423016
75%  0.145569704  0.009703663  0.0260423946  0.0183901702  0.026231598
              V7            V8           V9          V10
25% -0.007420656 -0.0136252500 -0.001834425 -0.005275001
50%  0.001471538 -0.0002307531  0.008025938  0.001569276
75%  0.033869229  0.0251735093  0.039937865  0.028909867

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.42381341  0.05421952  0.02077950  0.02346367 -0.09489184 -0.07298297 
         V8          V9         V10 
 0.02127020  0.10456570 -0.10803549 

 Quartiles of Marginal Effects:
 
          V2          V3           V4          V5          V6          V7
25% 1.969858 -0.02006085 -0.004485816 -0.10587715 -0.16095043 -0.12602159
50% 2.386384  0.05720540  0.015098784  0.06537315 -0.06838669 -0.07039138
75% 2.719502  0.15571172  0.059681620  0.13132761 -0.03478573  0.02853373
             V8         V9         V10
25% -0.09510872 0.05096521 -0.16784900
50%  0.06700614 0.12516006 -0.10785038
75%  0.10899168 0.17279657 -0.05564604

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.264132564 -0.008886724 -0.019671693 -0.037955847  0.034421635  0.038990248 
          V8           V9          V10 
 0.040425656  0.012777883  0.038950034 

 Quartiles of Marginal Effects:
 
             V2          V3           V4          V5          V6           V7
25% 0.002358009 -0.04047913 -0.089189601 -0.07001097 -0.01268726 -0.011701270
50% 0.136338119 -0.01102350 -0.007468315 -0.02118661  0.01096835  0.004766502
75% 0.368533576  0.02631431  0.073453354  0.03010210  0.09107365  0.080407548
              V8          V9         V10
25% -0.031192593 -0.01951134 -0.01040755
50%  0.007665924  0.01387092  0.01286600
75%  0.069633189  0.07828597  0.06414555

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.852335331  0.005739351 -0.030371021 -0.054133026  0.020553523  0.043853756 
          V8           V9          V10 
 0.050007213  0.050172926 -0.003454470 

 Quartiles of Marginal Effects:
 
          V2          V3          V4          V5          V6           V7
25% 0.338093 -0.09230271 -0.14045710 -0.15895152 -0.06131343 -0.046034182
50% 0.713961  0.01481966 -0.03639515 -0.04895661  0.02019752  0.007004653
75% 1.283142  0.09480271  0.09354224  0.04244803  0.11028459  0.109023789
             V8          V9         V10
25% -0.05261463 -0.05567919 -0.07938836
50%  0.04194043  0.03740782 -0.00861420
75%  0.15262816  0.15027479  0.05364277

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 1.999995629  0.044620735  0.011619414 -0.008639805 -0.070951292 -0.042851861 
          V8           V9          V10 
 0.028090807  0.103094575 -0.096749030 

 Quartiles of Marginal Effects:
 
          V2          V3           V4          V5          V6          V7
25% 1.459304 -0.07138738 -0.060431406 -0.17123531 -0.16105265 -0.09636117
50% 1.941089  0.06419033 -0.003005751  0.03118384 -0.05927814 -0.05191404
75% 2.421713  0.19699278  0.066934193  0.13506775  0.02485398  0.07055878
             V8          V9         V10
25% -0.09781604 0.002765346 -0.18440351
50%  0.05831513 0.113204167 -0.09800228
75%  0.16283184 0.220002633 -0.02924256

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.252340346  0.077745364 -0.012533719 -0.088297469 -0.061078436 -0.052548508 
          V8           V9          V10 
 0.097443205 -0.002564325  0.003736378 

 Quartiles of Marginal Effects:
 
          V2         V3          V4          V5          V6          V7
25% 2.250820 0.07744703 -0.01276297 -0.08891930 -0.06140219 -0.05278571
50% 2.252749 0.07777803 -0.01262522 -0.08825055 -0.06111767 -0.05258510
75% 2.253580 0.07814731 -0.01234718 -0.08769093 -0.06081241 -0.05221393
            V8           V9         V10
25% 0.09694366 -0.002743486 0.003539661
50% 0.09746013 -0.002500198 0.003752741
75% 0.09777011 -0.002263613 0.003894516

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.130666741 -0.007608680 -0.007916207 -0.024074043  0.021886460  0.022298765 
          V8           V9          V10 
 0.026299445  0.009894805  0.029110858 

 Quartiles of Marginal Effects:
 
              V2           V3           V4           V5           V6
25% -0.001590145 -0.030485282 -0.035752079 -0.040321222 -0.002391088
50%  0.047454194 -0.003771896 -0.001151993 -0.001429807  0.003111348
75%  0.187246275  0.012735391  0.033239929  0.017155553  0.033346274
              V7            V8           V9          V10
25% -0.010188973 -0.0173418706 -0.006875250 -0.004774123
50%  0.002006216  0.0001815698  0.008105149  0.002441342
75%  0.048720990  0.0332940024  0.044436982  0.035822040

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.538585473 -0.002518929 -0.032467417 -0.051949990  0.035248132  0.049965200 
          V8           V9          V10 
 0.050479639  0.027839510  0.027316587 

 Quartiles of Marginal Effects:
 
            V2           V3          V4          V5          V6         V7
25% 0.09904657 -0.075221017 -0.12098057 -0.12681059 -0.05406087 -0.0358535
50% 0.37991958  0.006384395 -0.02762708 -0.04285151  0.02859103  0.0110402
75% 0.78401803  0.055038095  0.05832784  0.02013806  0.12033188  0.1280259
             V8          V9          V10
25% -0.03366928 -0.04957152 -0.042357072
50%  0.04112784  0.01000237  0.008678989
75%  0.11431620  0.10960086  0.083903821

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.503151512 -0.004691715 -0.031341696 -0.050991756  0.037791412  0.050715540 
          V8           V9          V10 
 0.050575869  0.025279900  0.029865949 

 Quartiles of Marginal Effects:
 
            V2           V3          V4          V5          V6          V7
25% 0.07567161 -0.072918028 -0.11554482 -0.11939725 -0.04720565 -0.03567565
50% 0.33707008  0.001788972 -0.02403829 -0.04349980  0.02912856  0.01101923
75% 0.72294635  0.048841112  0.05812598  0.01859927  0.11881367  0.12790205
             V8           V9         V10
25% -0.03336633 -0.045064507 -0.03997505
50%  0.03548939  0.006062859  0.01223545
75%  0.10611951  0.113311341  0.09447680

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.77146440  0.07291504  0.04577136  0.02173826 -0.06247670 -0.02661963 
         V8          V9         V10 
 0.05147176  0.01381207 -0.04582046 

 Quartiles of Marginal Effects:
 
          V2         V3         V4           V5          V6           V7
25% 2.711680 0.06104767 0.03182385 -0.002079588 -0.07460007 -0.043731763
50% 2.771136 0.07945211 0.04270892  0.031055308 -0.06923770 -0.022965717
75% 2.823670 0.09074490 0.06032575  0.048148885 -0.05342867 -0.007993696
            V8          V9         V10
25% 0.03169757 0.009346537 -0.05224741
50% 0.04877224 0.015876540 -0.04585946
75% 0.06494158 0.023972437 -0.03400405

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.53428556  0.03107633 -0.03925410 -0.10233058 -0.00472671 -0.02107438 
         V8          V9         V10 
 0.07302648 -0.01607928  0.03193941 

 Quartiles of Marginal Effects:
 
           V2         V3          V4         V5           V6          V7
25% 0.5338326 0.03095744 -0.03934885 -0.1025838 -0.004829126 -0.02119962
50% 0.5343954 0.03112374 -0.03927017 -0.1023502 -0.004713695 -0.02111580
75% 0.5348776 0.03122250 -0.03921062 -0.1021160 -0.004584410 -0.02095233
            V8          V9        V10
25% 0.07283556 -0.01620151 0.03187699
50% 0.07311496 -0.01607128 0.03192739
75% 0.07325785 -0.01599564 0.03201194

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.74050940  0.06275213  0.01863268  0.05240408 -0.09283114 -0.06506452 
         V8          V9         V10 
 0.02298924  0.08128308 -0.08671277 

 Quartiles of Marginal Effects:
 
          V2          V3          V4          V5          V6            V7
25% 2.549592 0.002490436 0.001282064 -0.01407625 -0.15276581 -0.1281191833
50% 2.698358 0.057770447 0.026205172  0.07558816 -0.09152130 -0.0601255108
75% 2.915718 0.153543217 0.054476073  0.13247876 -0.04980535 -0.0007335966
             V8         V9         V10
25% -0.04390133 0.03458253 -0.13515515
50%  0.02515272 0.06988297 -0.08025238
75%  0.07163595 0.14218633 -0.03914476

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.993626371  0.071629647  0.013761976  0.028683067 -0.026675197 -0.044702862 
          V8           V9          V10 
-0.008427939 -0.008710657 -0.030992480 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5          V6          V7
25% 2.980550 0.06696039 0.01156666 0.02273127 -0.03087227 -0.04940756
50% 2.995261 0.07224009 0.01355507 0.02999548 -0.02608069 -0.04365212
75% 3.001708 0.07518885 0.01653744 0.03419933 -0.02357686 -0.03876279
              V8           V9         V10
25% -0.013792072 -0.011798715 -0.03587609
50% -0.009237383 -0.008665215 -0.03001760
75% -0.004499149 -0.006722228 -0.02554353
Radial Basis Function Kernel Regularized Least Squares 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  lambda        sigma        RMSE       Rsquared   MAE        Selected
  1.535679e-05     5.248482  1.5418370  0.5712570  0.9684532          
  2.998809e-05    82.158223  0.3937666  0.9745810  0.2487747          
  4.802236e-05     3.721508  1.6940998  0.4733672  1.1122895          
  7.392977e-05     3.034333  1.7650957  0.3845294  1.2038618          
  1.900862e-04     5.867016  1.4864220  0.5953816  0.9233152          
  2.461635e-04  3952.075314  0.2787423  0.9795198  0.2044130  *       
  3.335473e-04     3.135215  1.7548616  0.4000994  1.1901352          
  1.009586e-03    24.579373  0.7539081  0.8952453  0.4469930          
  1.957200e-03     1.743482  1.8790857  0.1600521  1.3664798          
  2.587399e-03  6151.425435  0.6857574  0.9495465  0.4266396          
  4.918737e-03     2.626277  1.8057519  0.3142269  1.2595082          
  7.802485e-03     2.762195  1.7929276  0.3384469  1.2418989          
  7.937213e-03    13.840139  1.0399263  0.7851980  0.6116337          
  9.939568e-03   280.270988  0.3669286  0.9750366  0.2084152          
  2.030401e-02   985.155248  0.7841027  0.9384787  0.4947183          
  2.267489e-02     3.928028  1.6764505  0.4917593  1.0937840          
  6.646719e-02  3602.460640  1.6566988  0.7473614  1.1997684          
  6.784264e-02     1.942004  1.8676589  0.1913436  1.3502186          
  1.502472e-01  6237.299987  1.8438460  0.6723259  1.3646418          
  6.320047e-01    29.018256  1.1924093  0.8286645  0.7485179          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 0.0002461635 and sigma
 = 3952.075.
[1] "Sat Mar 10 17:14:23 2018"
Least Angle Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  fraction    RMSE       Rsquared   MAE        Selected
  0.03726011  1.8406281  0.9841261  1.3670981          
  0.09538976  1.7115453  0.9841261  1.2628769          
  0.13628871  1.6208428  0.9841261  1.1895487          
  0.17376387  1.5378356  0.9841261  1.1223591          
  0.25579013  1.3565787  0.9841261  0.9752934          
  0.27824472  1.3070876  0.9841261  0.9350343          
  0.30463149  1.2490163  0.9841261  0.8877252          
  0.40082870  1.0383423  0.9841261  0.7170759          
  0.45832706  0.9135299  0.9841261  0.6185681          
  0.48257268  0.8612607  0.9841261  0.5793988          
  0.53837072  0.7421108  0.9841261  0.4917708          
  0.57844659  0.6578988  0.9841261  0.4307246          
  0.57993361  0.6548030  0.9841261  0.4284715          
  0.59947350  0.6143553  0.9841261  0.3988653          
  0.66151637  0.4897918  0.9841261  0.3048598          
  0.67110902  0.4712637  0.9841261  0.2903253          
  0.76452146  0.3117899  0.9841261  0.1808111          
  0.76630054  0.3093381  0.9841261  0.1801759          
  0.83536128  0.2543104  0.9835928  0.1876772  *       
  0.96014406  0.2739069  0.9796918  0.2179923          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.8353613.
[1] "Sat Mar 10 17:14:37 2018"
Least Angle Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  step  RMSE       Rsquared   MAE        Selected
  1     1.9234550        NaN  1.4339021          
  2     0.2553738  0.9841261  0.1813550          
  3     0.2549908  0.9832112  0.1911037  *       
  4     0.2588313  0.9824786  0.1960653          
  5     0.2611518  0.9817518  0.2017864          
  6     0.2689172  0.9803758  0.2102506          
  7     0.2703904  0.9801310  0.2141033          
  8     0.2707332  0.9799806  0.2153994          
  9     0.2712843  0.9798875  0.2165080          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was step = 3.
[1] "Sat Mar 10 17:14:50 2018"
The lasso 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  fraction    RMSE       Rsquared   MAE        Selected
  0.03726011  1.8406281  0.9841261  1.3670981          
  0.09538976  1.7115453  0.9841261  1.2628769          
  0.13628871  1.6208428  0.9841261  1.1895487          
  0.17376387  1.5378356  0.9841261  1.1223591          
  0.25579013  1.3565787  0.9841261  0.9752934          
  0.27824472  1.3070876  0.9841261  0.9350343          
  0.30463149  1.2490163  0.9841261  0.8877252          
  0.40082870  1.0383423  0.9841261  0.7170759          
  0.45832706  0.9135299  0.9841261  0.6185681          
  0.48257268  0.8612607  0.9841261  0.5793988          
  0.53837072  0.7421108  0.9841261  0.4917708          
  0.57844659  0.6578988  0.9841261  0.4307246          
  0.57993361  0.6548030  0.9841261  0.4284715          
  0.59947350  0.6143553  0.9841261  0.3988653          
  0.66151637  0.4897918  0.9841261  0.3048598          
  0.67110902  0.4712637  0.9841261  0.2903253          
  0.76452146  0.3117899  0.9841261  0.1808111          
  0.76630054  0.3093381  0.9841261  0.1801759          
  0.83536128  0.2543104  0.9835928  0.1876772  *       
  0.96014406  0.2739069  0.9796918  0.2179923          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.8353613.
[1] "Sat Mar 10 17:15:03 2018"
Linear Regression with Backwards Selection 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  nvmax  RMSE       Rsquared   MAE        Selected
  2      0.2645188  0.9811605  0.2147104  *       
  3      0.2767896  0.9793112  0.2203898          
  4      0.2788938  0.9788054  0.2228105          
  5      0.2864340  0.9777348  0.2294849          
  6      0.2866645  0.9778815  0.2312991          
  7      0.2861152  0.9780417  0.2270828          
  8      0.2835631  0.9783971  0.2240896          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sat Mar 10 17:15:17 2018"
Linear Regression with Forward Selection 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  nvmax  RMSE       Rsquared   MAE        Selected
  2      0.2645188  0.9811605  0.2147104  *       
  3      0.2699778  0.9802157  0.2133445          
  4      0.2724947  0.9796833  0.2152071          
  5      0.2808875  0.9784125  0.2246141          
  6      0.2836937  0.9781811  0.2282493          
  7      0.2835189  0.9783614  0.2244675          
  8      0.2835631  0.9783971  0.2240896          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sat Mar 10 17:15:29 2018"
Linear Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results:

  RMSE       Rsquared   MAE      
  0.2818752  0.9786371  0.2231349

Tuning parameter 'intercept' was held constant at a value of TRUE
[1] "Sat Mar 10 17:15:42 2018"
Start:  AIC=-54.5
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V8    1     0.000  1.270 -56.500
- V3    1     0.002  1.271 -56.471
- V4    1     0.002  1.272 -56.467
- V6    1     0.007  1.277 -56.362
- V10   1     0.020  1.289 -56.117
- V9    1     0.057  1.327 -55.398
- V5    1     0.065  1.335 -55.253
- V7    1     0.100  1.370 -54.602
<none>               1.270 -54.505
- V2    1    53.432 54.702  37.575

Step:  AIC=-56.5
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V3    1     0.002  1.271 -58.470
- V4    1     0.002  1.272 -58.467
- V6    1     0.008  1.278 -58.337
- V10   1     0.025  1.295 -58.009
- V9    1     0.057  1.327 -57.397
- V5    1     0.076  1.346 -57.042
- V7    1     0.100  1.370 -56.602
<none>               1.270 -56.500
- V2    1    53.533 54.803  35.622

Step:  AIC=-58.47
.outcome ~ V2 + V4 + V5 + V6 + V7 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V4    1     0.002  1.273 -60.439
- V6    1     0.009  1.280 -60.298
- V10   1     0.028  1.299 -59.932
- V9    1     0.070  1.341 -59.136
- V5    1     0.092  1.363 -58.732
- V7    1     0.099  1.370 -58.602
<none>               1.271 -58.470
- V2    1    55.902 57.174  34.680

Step:  AIC=-60.44
.outcome ~ V2 + V5 + V6 + V7 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V6    1     0.009  1.282 -62.268
- V10   1     0.032  1.305 -61.818
- V9    1     0.069  1.342 -61.126
- V5    1     0.090  1.363 -60.732
- V7    1     0.098  1.371 -60.586
<none>               1.273 -60.439
- V2    1    56.385 57.658  32.891

Step:  AIC=-62.27
.outcome ~ V2 + V5 + V7 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V10   1     0.026  1.308 -63.759
- V9    1     0.063  1.345 -63.065
- V5    1     0.081  1.363 -62.731
- V7    1     0.096  1.378 -62.459
<none>               1.282 -62.268
- V2    1    65.939 67.220  34.728

Step:  AIC=-63.76
.outcome ~ V2 + V5 + V7 + V9

       Df Sum of Sq    RSS     AIC
- V5    1     0.055  1.363 -64.731
- V9    1     0.060  1.368 -64.638
- V7    1     0.075  1.383 -64.367
<none>               1.308 -63.759
- V2    1    83.431 84.739  38.518

Step:  AIC=-64.73
.outcome ~ V2 + V7 + V9

       Df Sum of Sq    RSS     AIC
- V9    1     0.024  1.387 -66.286
- V7    1     0.053  1.416 -65.783
<none>               1.363 -64.731
- V2    1    85.174 86.537  37.042

Step:  AIC=-66.29
.outcome ~ V2 + V7

       Df Sum of Sq    RSS     AIC
- V7    1     0.081  1.469 -66.864
<none>               1.387 -66.286
- V2    1    88.838 90.225  36.086

Step:  AIC=-66.86
.outcome ~ V2

       Df Sum of Sq    RSS     AIC
<none>               1.469 -66.864
- V2    1    88.761 90.229  34.087
Start:  AIC=-65.2
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V4    1     0.000  1.151 -67.198
- V9    1     0.001  1.152 -67.171
- V5    1     0.002  1.153 -67.148
- V7    1     0.006  1.157 -67.050
- V8    1     0.036  1.186 -66.378
- V6    1     0.038  1.189 -66.321
- V10   1     0.044  1.195 -66.188
<none>               1.151 -65.199
- V3    1     0.297  1.448 -60.992
- V2    1    84.085 85.236  49.039

Step:  AIC=-67.2
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V9    1     0.001  1.152 -69.170
- V5    1     0.002  1.153 -69.147
- V7    1     0.007  1.157 -69.044
- V8    1     0.038  1.189 -68.313
- V6    1     0.039  1.189 -68.304
- V10   1     0.047  1.197 -68.127
<none>               1.151 -67.198
- V3    1     0.302  1.453 -62.902
- V2    1    84.420 85.570  47.145

Step:  AIC=-69.17
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V10

       Df Sum of Sq    RSS     AIC
- V5    1     0.001  1.153 -71.136
- V7    1     0.008  1.160 -70.984
- V6    1     0.038  1.190 -70.300
- V8    1     0.038  1.190 -70.299
- V10   1     0.048  1.200 -70.070
<none>               1.152 -69.170
- V3    1     0.307  1.459 -64.793
- V2    1    85.979 87.131  45.632

Step:  AIC=-71.14
.outcome ~ V2 + V3 + V6 + V7 + V8 + V10

       Df Sum of Sq    RSS     AIC
- V7    1     0.008  1.162 -72.941
- V6    1     0.040  1.194 -72.209
- V10   1     0.050  1.203 -71.996
- V8    1     0.062  1.216 -71.714
<none>               1.153 -71.136
- V3    1     0.310  1.463 -66.710
- V2    1    90.803 91.957  45.088

Step:  AIC=-72.94
.outcome ~ V2 + V3 + V6 + V8 + V10

       Df Sum of Sq    RSS     AIC
- V6    1     0.037  1.199 -74.090
- V10   1     0.049  1.211 -73.823
<none>               1.162 -72.941
- V8    1     0.097  1.259 -72.769
- V3    1     0.302  1.463 -68.706
- V2    1    94.035 95.197  44.023

Step:  AIC=-74.09
.outcome ~ V2 + V3 + V8 + V10

       Df Sum of Sq    RSS     AIC
- V10   1     0.058  1.257 -74.813
<none>               1.199 -74.090
- V8    1     0.103  1.302 -73.863
- V3    1     0.344  1.543 -69.284
- V2    1    94.153 95.351  42.067

Step:  AIC=-74.81
.outcome ~ V2 + V3 + V8

       Df Sum of Sq    RSS     AIC
<none>               1.257 -74.813
- V8    1     0.139  1.396 -73.975
- V3    1     0.288  1.545 -71.239
- V2    1    95.708 96.965  40.520
Start:  AIC=-65.38
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V7    1     0.002  1.329 -67.346
- V9    1     0.004  1.331 -67.300
- V10   1     0.028  1.354 -66.807
- V8    1     0.031  1.358 -66.738
- V5    1     0.038  1.364 -66.601
- V3    1     0.046  1.373 -66.423
- V6    1     0.069  1.396 -65.969
- V4    1     0.077  1.403 -65.812
<none>               1.327 -65.382
- V2    1    81.651 82.978  48.418

Step:  AIC=-67.35
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V9    1     0.005  1.334 -69.241
- V10   1     0.028  1.356 -68.772
- V8    1     0.033  1.361 -68.662
- V5    1     0.039  1.368 -68.526
- V3    1     0.045  1.373 -68.422
- V6    1     0.068  1.397 -67.947
<none>               1.329 -67.346
- V4    1     0.118  1.447 -66.958
- V2    1    83.133 84.462  46.915

Step:  AIC=-69.24
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V10

       Df Sum of Sq    RSS     AIC
- V10   1     0.033  1.366 -70.563
- V5    1     0.037  1.371 -70.470
- V8    1     0.037  1.371 -70.466
- V3    1     0.046  1.380 -70.290
- V6    1     0.063  1.397 -69.942
<none>               1.334 -69.241
- V4    1     0.118  1.452 -68.863
- V2    1    83.134 84.467  44.917

Step:  AIC=-70.56
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8

       Df Sum of Sq    RSS     AIC
- V8    1     0.019  1.385 -72.174
- V5    1     0.027  1.393 -72.023
- V3    1     0.029  1.395 -71.975
- V6    1     0.061  1.427 -71.340
- V4    1     0.099  1.465 -70.603
<none>               1.366 -70.563
- V2    1    83.129 84.495  42.926

Step:  AIC=-72.17
.outcome ~ V2 + V3 + V4 + V5 + V6

       Df Sum of Sq    RSS     AIC
- V5    1     0.014  1.400 -73.890
- V3    1     0.024  1.410 -73.685
- V6    1     0.042  1.427 -73.339
- V4    1     0.084  1.469 -72.530
<none>               1.385 -72.174
- V2    1    88.373 89.758  42.618

Step:  AIC=-73.89
.outcome ~ V2 + V3 + V4 + V6

       Df Sum of Sq    RSS     AIC
- V3    1     0.030  1.430 -75.293
- V6    1     0.044  1.443 -75.025
- V4    1     0.089  1.488 -74.170
<none>               1.400 -73.890
- V2    1    97.410 98.810  43.308

Step:  AIC=-75.29
.outcome ~ V2 + V4 + V6

       Df Sum of Sq    RSS     AIC
- V6    1     0.054  1.483 -76.264
- V4    1     0.085  1.514 -75.684
<none>               1.430 -75.293
- V2    1    98.008 99.438  41.485

Step:  AIC=-76.26
.outcome ~ V2 + V4

       Df Sum of Sq    RSS     AIC
- V4    1     0.079  1.562 -76.807
<none>               1.483 -76.264
- V2    1    98.033 99.516  39.507

Step:  AIC=-76.81
.outcome ~ V2

       Df Sum of Sq     RSS     AIC
<none>                1.562 -76.807
- V2    1    99.737 101.299  38.004
Start:  AIC=-98.84
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq     RSS      AIC
- V8    1     0.000   2.051 -100.830
- V4    1     0.005   2.055 -100.742
- V9    1     0.008   2.058 -100.690
- V5    1     0.026   2.076 -100.338
- V10   1     0.030   2.080 -100.263
- V6    1     0.030   2.080 -100.256
- V7    1     0.065   2.115  -99.596
<none>                2.050  -98.838
- V3    1     0.139   2.189  -98.211
- V2    1   130.093 132.143   65.800

Step:  AIC=-100.83
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9 + V10

       Df Sum of Sq     RSS      AIC
- V4    1     0.007   2.057 -102.702
- V9    1     0.007   2.058 -102.688
- V5    1     0.033   2.084 -102.183
- V6    1     0.034   2.085 -102.167
- V10   1     0.038   2.088 -102.101
- V7    1     0.064   2.115 -101.594
<none>                2.051 -100.830
- V3    1     0.140   2.190 -100.194
- V2    1   131.678 133.729   64.277

Step:  AIC=-102.7
.outcome ~ V2 + V3 + V5 + V6 + V7 + V9 + V10

       Df Sum of Sq     RSS      AIC
- V9    1     0.008   2.065 -104.550
- V6    1     0.034   2.091 -104.047
- V10   1     0.035   2.092 -104.035
- V5    1     0.040   2.097 -103.940
- V7    1     0.095   2.152 -102.905
<none>                2.057 -102.702
- V3    1     0.139   2.196 -102.093
- V2    1   131.690 133.747   62.283

Step:  AIC=-104.55
.outcome ~ V2 + V3 + V5 + V6 + V7 + V10

       Df Sum of Sq     RSS      AIC
- V6    1     0.029   2.094 -106.000
- V5    1     0.032   2.097 -105.931
- V10   1     0.038   2.103 -105.814
<none>                2.065 -104.550
- V7    1     0.106   2.172 -104.538
- V3    1     0.150   2.215 -103.751
- V2    1   134.656 136.721   61.163

Step:  AIC=-106
.outcome ~ V2 + V3 + V5 + V7 + V10

       Df Sum of Sq     RSS      AIC
- V5    1     0.036   2.130 -107.311
- V10   1     0.048   2.141 -107.099
- V7    1     0.088   2.181 -106.362
<none>                2.094 -106.000
- V3    1     0.161   2.255 -105.034
- V2    1   135.627 137.721   59.454

Step:  AIC=-107.31
.outcome ~ V2 + V3 + V7 + V10

       Df Sum of Sq     RSS      AIC
- V10   1     0.030   2.160 -108.746
- V7    1     0.083   2.213 -107.788
<none>                2.130 -107.311
- V3    1     0.183   2.313 -106.007
- V2    1   136.851 138.981   57.818

Step:  AIC=-108.75
.outcome ~ V2 + V3 + V7

       Df Sum of Sq     RSS      AIC
- V7    1     0.079   2.239 -109.314
<none>                2.160 -108.746
- V3    1     0.161   2.321 -107.870
- V2    1   142.522 144.683   57.427

Step:  AIC=-109.31
.outcome ~ V2 + V3

       Df Sum of Sq     RSS      AIC
<none>                2.239 -109.314
- V3    1     0.122   2.361 -109.189
- V2    1   142.472 144.711   55.434
Linear Regression with Stepwise Selection 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results:

  RMSE       Rsquared   MAE      
  0.2654115  0.9808536  0.2082253

[1] "Sat Mar 10 17:15:55 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 17:16:37 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "logreg"                         
Error : package RWeka is required
Error : package RWeka is required
Error : package RWeka is required
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 17:16:50 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "M5"                             
Error : package RWeka is required
Error : package RWeka is required
Error : package RWeka is required
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 17:17:03 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "M5Rules"                        
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 17:17:17 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "mlpKerasDecay"                  
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 17:17:30 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "mlpKerasDropout"                
Multi-Step Adaptive MCP-Net 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  alphas     nsteps  scale      RMSE       Rsquared   MAE        Selected
  0.0835341   9      1.8544439  0.2471620  0.9841261  0.1859124          
  0.1358508   6      3.3019838  0.2452004  0.9841261  0.1882157          
  0.1726598   9      0.7484557  0.2447697  0.9841261  0.1894764          
  0.2063875   9      0.5230604  0.2445701  0.9841261  0.1904044          
  0.2802111   9      1.6589980  0.2443742  0.9841261  0.1916611          
  0.3004202   2      1.9080370  0.2443468  0.9841261  0.1918981          
  0.3241683   9      0.6243242  0.2443221  0.9841261  0.1921391          
  0.4107458   7      0.7485915  0.2442715  0.9841261  0.1927828          
  0.4624943  10      1.3964956  0.2442569  0.9841261  0.1930529          
  0.4843154   2      1.2718711  0.2442527  0.9841261  0.1931496          
  0.5345336  10      3.1660257  0.2442458  0.9841261  0.1933422          
  0.5706019  10      0.9476800  0.2442425  0.9841261  0.1934597          
  0.5719402   8      2.5540763  0.2442424  0.9841261  0.1934638          
  0.5895261   5      2.4552117  0.2442412  0.9841261  0.1935156          
  0.6453647   4      3.7958570  0.2442387  0.9841261  0.1936615          
  0.6539981   9      0.9739938  0.2442384  0.9841261  0.1936819          
  0.7380693   2      1.8005712  0.2442371  0.9841261  0.1938552          
  0.7396705  10      1.1266668  0.2442371  0.9841261  0.1938581          
  0.8018251   2      1.1788800  0.2442371  0.9841261  0.1939624  *       
  0.9141297   7      1.7344426  0.2442381  0.9841261  0.1941150          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alphas = 0.8018251, nsteps = 2
 and scale = 1.17888.
[1] "Sat Mar 10 17:17:50 2018"
# weights:  67
initial  value 171.142481 
iter  10 value 123.975264
iter  20 value 123.972531
iter  30 value 123.935643
iter  40 value 123.098300
iter  50 value 122.866749
iter  60 value 122.242052
iter  70 value 122.164887
iter  80 value 122.162478
iter  90 value 122.140655
iter 100 value 122.125547
final  value 122.125547 
stopped after 100 iterations
# weights:  67
initial  value 158.451492 
iter  10 value 128.418716
final  value 128.408621 
converged
# weights:  188
initial  value 151.721647 
iter  10 value 123.973364
final  value 123.972074 
converged
# weights:  78
initial  value 182.000583 
iter  10 value 131.791151
iter  20 value 131.771195
final  value 131.771171 
converged
# weights:  133
initial  value 215.072266 
iter  10 value 131.210121
iter  20 value 131.192429
final  value 131.192415 
converged
# weights:  155
initial  value 159.202248 
iter  10 value 124.004094
iter  20 value 123.979668
iter  30 value 123.975946
iter  40 value 123.974821
iter  50 value 123.973137
iter  60 value 123.969870
iter  70 value 123.960771
iter  80 value 123.906237
iter  90 value 122.992799
iter 100 value 122.245802
final  value 122.245802 
stopped after 100 iterations
# weights:  221
initial  value 128.304276 
iter  10 value 124.426860
final  value 124.426067 
converged
# weights:  45
initial  value 173.975996 
iter  10 value 133.447341
iter  20 value 133.431724
final  value 133.431713 
converged
# weights:  111
initial  value 224.780067 
iter  10 value 135.567438
iter  20 value 135.526851
final  value 135.526840 
converged
# weights:  100
initial  value 160.941083 
iter  10 value 124.817263
iter  20 value 124.750070
final  value 124.750049 
converged
# weights:  122
initial  value 207.086434 
iter  10 value 131.805026
iter  20 value 131.795412
final  value 131.795407 
converged
# weights:  12
initial  value 182.224424 
iter  10 value 131.631066
final  value 131.630174 
converged
# weights:  133
initial  value 159.355456 
iter  10 value 125.405924
final  value 125.323897 
converged
# weights:  111
initial  value 165.214302 
iter  10 value 123.972773
final  value 123.972391 
converged
# weights:  177
initial  value 273.233721 
iter  10 value 132.875530
iter  20 value 132.783134
final  value 132.783114 
converged
# weights:  155
initial  value 172.004866 
iter  10 value 128.810215
final  value 128.807405 
converged
# weights:  34
initial  value 163.259045 
iter  10 value 132.412690
iter  20 value 132.405064
iter  20 value 132.405064
iter  20 value 132.405063
final  value 132.405063 
converged
# weights:  133
initial  value 152.654331 
iter  10 value 124.098631
iter  20 value 124.007262
iter  30 value 122.398443
iter  40 value 122.262102
iter  50 value 122.256592
iter  60 value 122.255617
iter  70 value 122.255490
final  value 122.255488 
converged
# weights:  177
initial  value 166.729220 
iter  10 value 123.975311
final  value 123.973315 
converged
# weights:  23
initial  value 153.430441 
iter  10 value 124.488771
iter  20 value 124.212110
iter  30 value 123.052496
iter  40 value 123.044783
iter  50 value 123.044535
iter  50 value 123.044535
iter  50 value 123.044535
final  value 123.044535 
converged
# weights:  67
initial  value 184.377007 
iter  10 value 138.184073
final  value 138.184045 
converged
# weights:  67
initial  value 181.178134 
iter  10 value 142.630130
final  value 142.627268 
converged
# weights:  188
initial  value 196.298476 
iter  10 value 138.183624
final  value 138.182295 
converged
# weights:  78
initial  value 197.149919 
iter  10 value 146.066095
iter  20 value 146.029216
final  value 146.029180 
converged
# weights:  133
initial  value 248.347989 
iter  10 value 145.518216
iter  20 value 145.459679
final  value 145.459637 
converged
# weights:  155
initial  value 180.467822 
iter  10 value 138.215316
iter  20 value 138.190005
iter  30 value 138.173559
iter  40 value 138.140764
iter  50 value 137.668622
iter  60 value 136.093709
iter  70 value 135.980564
iter  80 value 135.967903
iter  90 value 135.965739
iter 100 value 135.963149
final  value 135.963149 
stopped after 100 iterations
# weights:  221
initial  value 178.985571 
iter  10 value 138.889365
iter  20 value 138.635376
final  value 138.635323 
converged
# weights:  45
initial  value 168.416131 
iter  10 value 147.713986
iter  20 value 147.709355
final  value 147.709348 
converged
# weights:  111
initial  value 288.316701 
iter  10 value 149.931805
iter  20 value 149.883634
final  value 149.883631 
converged
# weights:  100
initial  value 167.198974 
iter  10 value 138.966851
iter  20 value 138.960625
iter  20 value 138.960624
iter  20 value 138.960624
final  value 138.960624 
converged
# weights:  122
initial  value 193.384172 
iter  10 value 146.078525
iter  20 value 146.070018
final  value 146.069989 
converged
# weights:  12
initial  value 179.375107 
iter  10 value 145.921286
final  value 145.919511 
converged
# weights:  133
initial  value 159.231611 
iter  10 value 139.583581
final  value 139.532462 
converged
# weights:  111
initial  value 169.304933 
iter  10 value 138.182783
final  value 138.182331 
converged
# weights:  177
initial  value 271.617801 
iter  10 value 147.205840
iter  20 value 147.096696
final  value 147.096643 
converged
# weights:  155
initial  value 206.791159 
iter  10 value 143.045369
final  value 143.043462 
converged
# weights:  34
initial  value 178.594630 
iter  10 value 146.679009
final  value 146.669496 
converged
# weights:  133
initial  value 150.583209 
iter  10 value 138.241372
iter  20 value 138.214940
iter  30 value 137.938826
iter  40 value 136.383541
iter  50 value 136.209189
iter  60 value 136.182370
iter  70 value 136.169667
iter  80 value 136.156197
iter  90 value 136.134277
iter 100 value 136.124121
final  value 136.124121 
stopped after 100 iterations
# weights:  177
initial  value 181.897705 
iter  10 value 138.188172
final  value 138.183303 
converged
# weights:  23
initial  value 167.626552 
iter  10 value 138.743522
iter  20 value 138.383872
iter  30 value 137.067094
iter  40 value 136.982990
iter  50 value 136.981721
final  value 136.981570 
converged
# weights:  67
initial  value 191.973061 
iter  10 value 133.742307
iter  20 value 133.740411
iter  30 value 133.740320
iter  40 value 133.738277
iter  50 value 133.737819
iter  60 value 133.737207
iter  70 value 133.736362
iter  80 value 133.735140
iter  90 value 133.733278
iter 100 value 133.730230
final  value 133.730230 
stopped after 100 iterations
# weights:  67
initial  value 166.279488 
iter  10 value 138.279283
final  value 138.276167 
converged
# weights:  188
initial  value 192.073387 
iter  10 value 133.740568
final  value 133.739237 
converged
# weights:  78
initial  value 204.061445 
iter  10 value 141.720688
iter  20 value 141.710844
final  value 141.710835 
converged
# weights:  133
initial  value 201.572076 
iter  10 value 141.106396
iter  20 value 141.104527
final  value 141.104515 
converged
# weights:  155
initial  value 196.484218 
iter  10 value 133.762526
iter  20 value 133.744917
iter  30 value 133.742747
iter  40 value 133.742035
iter  50 value 133.740977
iter  60 value 133.739175
iter  70 value 133.735320
iter  80 value 133.722177
iter  90 value 133.584529
iter 100 value 132.716016
final  value 132.716016 
stopped after 100 iterations
# weights:  221
initial  value 151.188792 
iter  10 value 134.256630
iter  20 value 134.201949
final  value 134.201941 
converged
# weights:  45
initial  value 177.610500 
iter  10 value 143.428146
iter  20 value 143.422843
iter  20 value 143.422842
iter  20 value 143.422841
final  value 143.422841 
converged
# weights:  111
initial  value 230.711328 
iter  10 value 145.597778
iter  20 value 145.548877
final  value 145.548875 
converged
# weights:  100
initial  value 173.712706 
iter  10 value 134.575428
iter  20 value 134.540067
final  value 134.540062 
converged
# weights:  122
initial  value 212.091776 
iter  10 value 141.726477
iter  20 value 141.723233
final  value 141.723225 
converged
# weights:  12
initial  value 176.586373 
iter  10 value 141.569009
final  value 141.563370 
converged
# weights:  133
initial  value 182.826955 
iter  10 value 135.289801
final  value 135.119824 
converged
# weights:  111
initial  value 180.804754 
iter  10 value 133.739924
final  value 133.739904 
converged
# weights:  177
initial  value 259.388827 
iter  10 value 142.771618
iter  20 value 142.723155
final  value 142.723148 
converged
# weights:  155
initial  value 216.145395 
iter  10 value 138.667019
final  value 138.666608 
converged
# weights:  34
initial  value 155.320871 
iter  10 value 142.381270
iter  20 value 142.372158
iter  20 value 142.372157
iter  20 value 142.372157
final  value 142.372157 
converged
# weights:  133
initial  value 154.480493 
iter  10 value 133.835297
iter  20 value 133.774425
iter  30 value 133.711991
iter  40 value 132.590794
iter  50 value 132.560098
iter  60 value 132.553472
iter  70 value 132.552155
iter  80 value 132.447788
iter  90 value 132.281949
iter 100 value 132.274330
final  value 132.274330 
stopped after 100 iterations
# weights:  177
initial  value 171.082976 
iter  10 value 133.743821
final  value 133.740000 
converged
# weights:  23
initial  value 179.212953 
iter  10 value 134.331147
iter  20 value 134.064672
iter  30 value 132.949988
iter  40 value 132.486874
iter  50 value 132.410155
iter  60 value 132.403516
iter  70 value 132.402860
final  value 132.402840 
converged
# weights:  188
initial  value 233.668157 
iter  10 value 197.948771
final  value 197.946308 
converged
Neural Network 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  size  decay         RMSE      Rsquared    MAE       Selected
   1    8.316669e-01  2.252448  0.52719437  1.429642          
   2    1.342837e-02  2.235024  0.08907484  1.407263          
   3    1.392905e+00  2.256053  0.52426331  1.433023          
   4    1.891930e+00  2.261032  0.51717224  1.437671          
   6    4.024966e-05  2.229967  0.14846200  1.418666          
   6    7.036781e-01  2.239686  0.51871556  1.417173          
   7    1.801354e+00  2.253563  0.50892814  1.430479          
   9    8.206232e-02  2.227865  0.46215887  1.405429          
  10    2.072700e-05  2.225994  0.30905087  1.403348          
  10    4.343838e+00  2.272289  0.48976587  1.448101          
  11    2.349574e+00  2.253815  0.49715411  1.430632          
  12    2.131242e-03  2.233894  0.05950718  1.421818          
  12    1.942179e-01  2.229447  0.51053755  1.406929          
  12    2.178304e+00  2.251149  0.49665165  1.428068          
  14    3.234022e-04  2.238969  0.05811002  1.421139          
  14    1.284512e+00  2.241343  0.50101152  1.418583          
  16    4.624887e-05  2.225994  0.41226197  1.403348          
  16    3.695087e+00  2.258489  0.48560401  1.435043          
  17    2.030042e-05  2.225993  0.19399884  1.403347  *       
  20    6.397246e-02  2.227009  0.50700626  1.404409          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 17 and decay = 2.030042e-05.
[1] "Sat Mar 10 17:18:38 2018"
Non-Negative Least Squares 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results:

  RMSE       Rsquared   MAE      
  0.2783208  0.9810847  0.2036209

[1] "Sat Mar 10 17:18:51 2018"
Non-Informative Model 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results:

  RMSE      Rsquared  MAE     
  1.923455  NaN       1.433902

[1] "Sat Mar 10 17:19:03 2018"
Parallel Random Forest 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE        Selected
  1     1.6416164  0.5196016  1.1985575          
  2     1.3721775  0.7724162  0.9891374          
  3     1.2117203  0.8271703  0.8601440          
  4     1.0054769  0.9013362  0.6950194          
  5     0.8493211  0.9342582  0.5516611          
  6     0.7453542  0.9470061  0.4724938          
  7     0.6135094  0.9582979  0.3605703          
  8     0.5298664  0.9626611  0.3031037          
  9     0.4586992  0.9615072  0.2494266  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 17:19:19 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: Warning messages:
1: package 'mxnet' is not available (for R version 3.4.3) 
2: package 'mxnet' is not available (for R version 3.4.3) 
3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
4: executing %dopar% sequentially: no parallel backend registered 
# weights:  67
initial  value 170.313664 
iter  10 value 123.975099
iter  20 value 123.971124
iter  30 value 123.970583
iter  40 value 123.969863
iter  50 value 123.968861
iter  60 value 123.967386
iter  70 value 123.965057
iter  80 value 123.961021
iter  90 value 123.953065
iter 100 value 123.933744
final  value 123.933744 
stopped after 100 iterations
# weights:  67
initial  value 159.992474 
iter  10 value 128.537267
final  value 128.535575 
converged
# weights:  188
initial  value 151.704272 
iter  10 value 123.972886
final  value 123.972111 
converged
# weights:  78
initial  value 184.074973 
iter  10 value 131.943402
iter  20 value 131.919829
final  value 131.919766 
converged
# weights:  133
initial  value 219.627301 
iter  10 value 131.288682
iter  20 value 131.285371
final  value 131.285365 
converged
# weights:  155
initial  value 165.310011 
iter  10 value 124.000180
iter  20 value 123.979406
iter  30 value 123.969868
iter  40 value 123.960047
iter  50 value 123.900944
iter  60 value 122.627047
iter  70 value 122.120345
iter  80 value 122.113947
iter  90 value 122.113080
final  value 122.112855 
converged
# weights:  221
initial  value 129.394690 
iter  10 value 124.437949
final  value 124.437044 
converged
# weights:  45
initial  value 177.539506 
iter  10 value 133.656914
iter  20 value 133.638187
final  value 133.638004 
converged
# weights:  111
initial  value 223.492401 
iter  10 value 135.700941
iter  20 value 135.636996
final  value 135.636989 
converged
# weights:  100
initial  value 165.094890 
iter  10 value 124.854485
iter  20 value 124.508119
iter  30 value 124.499535
final  value 124.499528 
converged
# weights:  122
initial  value 206.482144 
iter  10 value 131.903991
final  value 131.898155 
converged
# weights:  12
initial  value 181.764131 
iter  10 value 131.801004
final  value 131.799959 
converged
# weights:  133
initial  value 158.006882 
iter  10 value 125.410067
final  value 125.361429 
converged
# weights:  111
initial  value 169.192165 
iter  10 value 123.972770
final  value 123.972383 
converged
# weights:  177
initial  value 271.445098 
iter  10 value 132.924235
iter  20 value 132.851382
final  value 132.851339 
converged
# weights:  155
initial  value 169.660768 
iter  10 value 128.878648
final  value 128.878620 
converged
# weights:  34
initial  value 164.262169 
iter  10 value 132.654039
iter  20 value 132.620101
final  value 132.620099 
converged
# weights:  133
initial  value 154.623527 
iter  10 value 124.102746
iter  20 value 123.985864
iter  30 value 122.397576
iter  40 value 122.261724
iter  50 value 122.250523
iter  60 value 122.247661
iter  70 value 122.247060
iter  80 value 122.231752
iter  90 value 122.224685
iter 100 value 122.221862
final  value 122.221862 
stopped after 100 iterations
# weights:  177
initial  value 161.723491 
iter  10 value 123.975999
final  value 123.973052 
converged
# weights:  23
initial  value 150.898410 
iter  10 value 124.461822
iter  20 value 123.977798
iter  30 value 122.999850
iter  40 value 122.995023
final  value 122.994922 
converged
# weights:  67
initial  value 189.653309 
iter  10 value 138.184222
final  value 138.184203 
converged
# weights:  67
initial  value 185.670322 
iter  10 value 142.761552
final  value 142.759426 
converged
# weights:  188
initial  value 199.673178 
iter  10 value 138.182876
final  value 138.182342 
converged
# weights:  78
initial  value 198.662322 
iter  10 value 146.214789
iter  20 value 146.204112
final  value 146.204052 
converged
# weights:  133
initial  value 245.506512 
iter  10 value 145.580333
iter  20 value 145.574808
iter  20 value 145.574807
iter  20 value 145.574807
final  value 145.574807 
converged
# weights:  155
initial  value 175.256148 
iter  10 value 138.214505
iter  20 value 138.189339
iter  30 value 138.064216
iter  40 value 136.078019
iter  50 value 135.410424
iter  60 value 135.395022
iter  70 value 135.376165
iter  80 value 135.371903
iter  90 value 135.370408
iter 100 value 135.369269
final  value 135.369269 
stopped after 100 iterations
# weights:  221
initial  value 173.604982 
iter  10 value 139.034489
iter  20 value 138.645303
final  value 138.645230 
converged
# weights:  45
initial  value 169.578126 
iter  10 value 147.966746
iter  20 value 147.945664
final  value 147.945660 
converged
# weights:  111
initial  value 291.635885 
iter  10 value 150.193579
iter  20 value 150.026358
final  value 150.026110 
converged
# weights:  100
initial  value 160.881367 
iter  10 value 139.015743
iter  20 value 138.669156
iter  30 value 138.533248
final  value 138.533245 
converged
# weights:  122
initial  value 192.818240 
iter  10 value 146.235010
iter  20 value 146.197143
final  value 146.197108 
converged
# weights:  12
initial  value 179.039704 
iter  10 value 146.082232
final  value 146.081729 
converged
# weights:  133
initial  value 158.487448 
iter  10 value 139.573000
final  value 139.567843 
converged
# weights:  111
initial  value 168.572070 
iter  10 value 138.182552
final  value 138.182365 
converged
# weights:  177
initial  value 281.436291 
iter  10 value 147.198552
iter  20 value 147.185474
iter  20 value 147.185473
iter  20 value 147.185473
final  value 147.185473 
converged
# weights:  155
initial  value 207.623663 
iter  10 value 143.129424
final  value 143.129187 
converged
# weights:  34
initial  value 182.246711 
iter  10 value 146.925847
iter  20 value 146.902824
iter  20 value 146.902823
iter  20 value 146.902823
final  value 146.902823 
converged
# weights:  133
initial  value 147.274086 
iter  10 value 138.227590
iter  20 value 136.805739
iter  30 value 135.597841
iter  40 value 135.573830
iter  50 value 135.569945
iter  60 value 135.567054
iter  70 value 135.553092
iter  80 value 135.538280
iter  90 value 135.529380
iter 100 value 135.525398
final  value 135.525398 
stopped after 100 iterations
# weights:  177
initial  value 179.730615 
iter  10 value 138.186196
final  value 138.183330 
converged
# weights:  23
initial  value 166.782889 
iter  10 value 138.717777
iter  20 value 137.504302
iter  30 value 136.424831
iter  40 value 136.417390
iter  50 value 136.416536
final  value 136.416477 
converged
# weights:  67
initial  value 189.187636 
iter  10 value 133.742019
final  value 133.740365 
converged
# weights:  67
initial  value 165.489067 
iter  10 value 138.320177
final  value 138.317953 
converged
# weights:  188
initial  value 192.315756 
iter  10 value 133.740427
final  value 133.739179 
converged
# weights:  78
initial  value 202.935770 
iter  10 value 141.810966
iter  20 value 141.765977
final  value 141.765915 
converged
# weights:  133
initial  value 205.558572 
iter  10 value 141.138862
iter  20 value 141.136138
final  value 141.136125 
converged
# weights:  155
initial  value 199.114880 
iter  10 value 133.756847
iter  20 value 133.609786
iter  30 value 130.851785
iter  40 value 130.737059
iter  50 value 130.734171
iter  60 value 130.731665
iter  70 value 130.730747
iter  80 value 130.730159
iter  90 value 130.729768
iter 100 value 130.729293
final  value 130.729293 
stopped after 100 iterations
# weights:  221
initial  value 152.316400 
iter  10 value 134.254815
iter  20 value 134.203541
final  value 134.203531 
converged
# weights:  45
initial  value 178.890702 
iter  10 value 143.518727
iter  20 value 143.507958
final  value 143.507951 
converged
# weights:  111
initial  value 233.261407 
iter  10 value 145.664902
iter  20 value 145.592152
iter  20 value 145.592151
iter  20 value 145.592150
final  value 145.592150 
converged
# weights:  100
initial  value 177.209927 
iter  10 value 134.567523
iter  20 value 134.511155
iter  30 value 134.317738
final  value 134.316737 
converged
# weights:  122
initial  value 214.588791 
iter  10 value 141.827598
iter  20 value 141.759262
final  value 141.759129 
converged
# weights:  12
initial  value 177.583427 
iter  10 value 141.637997
final  value 141.636509 
converged
# weights:  133
initial  value 181.402586 
iter  10 value 135.301713
final  value 135.127459 
converged
# weights:  111
initial  value 180.847985 
iter  10 value 133.739942
final  value 133.739854 
converged
# weights:  177
initial  value 262.218633 
iter  10 value 142.762700
final  value 142.746789 
converged
# weights:  155
initial  value 214.453997 
iter  10 value 138.688683
final  value 138.688470 
converged
# weights:  34
initial  value 157.529759 
iter  10 value 142.475362
iter  20 value 142.461411
final  value 142.461396 
converged
# weights:  133
initial  value 153.239518 
iter  10 value 133.823476
iter  20 value 133.757377
iter  30 value 133.715785
iter  40 value 132.471530
iter  50 value 130.972013
iter  60 value 130.953606
iter  70 value 130.951641
iter  80 value 130.950744
iter  90 value 130.949174
iter 100 value 130.948244
final  value 130.948244 
stopped after 100 iterations
# weights:  177
initial  value 168.064891 
iter  10 value 133.743199
final  value 133.740034 
converged
# weights:  23
initial  value 179.249956 
iter  10 value 134.327679
iter  20 value 133.649162
iter  30 value 131.901826
iter  40 value 131.892048
iter  50 value 131.891894
final  value 131.891888 
converged
# weights:  133
initial  value 280.672155 
iter  10 value 198.271981
iter  20 value 197.987122
iter  30 value 195.459910
iter  40 value 194.447019
iter  50 value 194.317592
iter  60 value 194.291987
iter  70 value 194.274028
iter  80 value 194.260268
iter  90 value 194.248566
iter 100 value 194.238989
final  value 194.238989 
stopped after 100 iterations
Neural Networks with Feature Extraction 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  size  decay         RMSE      Rsquared    MAE       Selected
   1    8.316669e-01  2.252696  0.66274728  1.431231          
   2    1.342837e-02  2.221734  0.17178862  1.381824          
   3    1.392905e+00  2.256621  0.67428585  1.435035          
   4    1.891930e+00  2.261740  0.67386272  1.439665          
   6    4.024966e-05  2.225862  0.23932492  1.402961          
   6    7.036781e-01  2.239794  0.65488350  1.418384          
   7    1.801354e+00  2.254013  0.66765558  1.431940          
   9    8.206232e-02  2.227463  0.20907493  1.404186          
  10    2.072700e-05  2.225993  0.17241385  1.403347          
  10    4.343838e+00  2.272867  0.65556342  1.449305          
  11    2.349574e+00  2.254175  0.66184162  1.431682          
  12    2.131242e-03  2.220962  0.13958854  1.372008  *       
  12    1.942179e-01  2.229413  0.61541603  1.407364          
  12    2.178304e+00  2.251451  0.66065102  1.429020          
  14    3.234022e-04  2.221792  0.10991838  1.373158          
  14    1.284512e+00  2.241500  0.66134389  1.419298          
  16    4.624887e-05  2.225994  0.07680313  1.403348          
  16    3.695087e+00  2.258772  0.65298903  1.435774          
  17    2.030042e-05  2.225993  0.09518479  1.403347          
  20    6.397246e-02  2.226994  0.59700867  1.404556          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 12 and decay = 0.002131242.
[1] "Sat Mar 10 17:19:34 2018"
Principal Component Analysis 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared    MAE        Selected
  1      1.9581865  0.08826203  1.4419872          
  2      2.1151528  0.05968939  1.5599025          
  3      1.9868160  0.02053879  1.3874057          
  4      2.0985656  0.02470050  1.5087629          
  5      2.1632428  0.04002835  1.5655837          
  6      2.1932081  0.09600905  1.6179008          
  7      2.1378933  0.09011123  1.4677631          
  8      0.9545516  0.73645892  0.7594258  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 8.
[1] "Sat Mar 10 17:19:47 2018"
Loading required package: plsRglm
____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 4 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0503247662447393) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 4 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 4 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 4 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 4 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 4 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 4 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 4 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 4 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.177937285229564) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
No more significant predictors (<0.0503247662447393) found
Warning only 4 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.126866434933618) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.187929121823981) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.130471460241824) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.179032972920686) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.142942977929488) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.18558749044314) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.170291271945462) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0776210927870125) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0221700377762318) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0102501683868468) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.177937285229564) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0503247662447393) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.126866434933618) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.187929121823981) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.130471460241824) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.179032972920686) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.142942977929488) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.010551211796701) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.18558749044314) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.170291271945462) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0776210927870125) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0221700377762318) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

Partial Least Squares Generalized Linear Models  

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  nt  alpha.pvals.expli  RMSE       Rsquared   MAE        Selected
  1   0.10426745         1.1156805  0.6893467  0.8022734          
  1   0.16399831         1.0855180  0.7045677  0.7571737          
  2   0.17146405         0.3049519  0.9760777  0.2184469          
  2   0.17589684         0.3049519  0.9760777  0.2184469          
  3   0.02015874         0.2678190  0.9809397  0.2052482          
  3   0.16157914         0.2601099  0.9818437  0.1988239          
  3   0.17518664         0.2601099  0.9818437  0.1988239          
  4   0.13047146         0.2601099  0.9818437  0.1988239          
  5   0.01055121         0.2611605  0.9817511  0.2023711          
  5   0.17903297         0.2601099  0.9818437  0.1988239  *       
  5   0.18792912         0.2623945  0.9810810  0.2100693          
  6   0.05032477         0.2614716  0.9815249  0.2121661          
  6   0.07762109         0.2637275  0.9813582  0.2060138          
  6   0.14294298         0.2601099  0.9818437  0.1988239          
  6   0.17793729         0.2601099  0.9818437  0.1988239          
  7   0.02217004         0.2678190  0.9809397  0.2052482          
  7   0.17029127         0.2601099  0.9818437  0.1988239          
  7   0.18558749         0.2623945  0.9810810  0.2100693          
  8   0.01025017         0.2611605  0.9817511  0.2023711          
  9   0.12686643         0.2601099  0.9818437  0.1988239          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nt = 5 and alpha.pvals.expli
 = 0.179033.
[1] "Sat Mar 10 17:20:38 2018"
Projection Pursuit Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  nterms  RMSE        Rsquared   MAE         Selected
   1      0.09216457  0.9978691  0.06373102  *       
   2      0.10329499  0.9967776  0.06887968          
   3      0.09576735  0.9971037  0.06823313          
   4      0.09227786  0.9974047  0.06579898          
   5      0.09432158  0.9973237  0.06722859          
   6      0.09538047  0.9972625  0.06717666          
   7      0.09416562  0.9973049  0.06707174          
   8      0.09412267  0.9973067  0.06678637          
   9      0.09393489  0.9973152  0.06669612          
  10      0.09415249  0.9973043  0.06674070          
  11      0.09415712  0.9973053  0.06674084          
  12      0.09419308  0.9973030  0.06678191          
  13      0.09420077  0.9973027  0.06677812          
  14      0.09419769  0.9973030  0.06677541          
  15      0.09419655  0.9973031  0.06677504          
  16      0.09419657  0.9973031  0.06677535          
  17      0.09419421  0.9973032  0.06677382          
  18      0.09419323  0.9973032  0.06677364          
  19      0.09419304  0.9973032  0.06677450          
  20      0.09419386  0.9973032  0.06677410          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nterms = 1.
[1] "Sat Mar 10 17:20:52 2018"
Quantile Random Forest 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  mtry  RMSE      Rsquared    MAE        Selected
  1     2.515761  0.08735854  1.7204017          
  2     2.393169  0.34915160  1.6407675          
  3     2.257212  0.49615497  1.5310171          
  4     2.078643  0.66601711  1.4204444          
  5     1.821471  0.76537410  1.2267284          
  6     1.672583  0.77729478  1.0772395          
  7     1.610200  0.74843001  0.9791244          
  8     1.333512  0.81364848  0.7846957          
  9     1.246843  0.83849628  0.7080668  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 17:21:28 2018"
Random Forest 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  min.node.size  mtry  splitrule   RMSE       Rsquared   MAE        Selected
   1             8     extratrees  0.5294063  0.9722001  0.3178694          
   2             5     maxstat     1.2288316  0.7174661  0.6934100          
   3             8     variance    0.4957691  0.9632977  0.2734494  *       
   4             8     variance    0.5285777  0.9599579  0.3018911          
   6             1     extratrees  1.6189925  0.6721519  1.1672576          
   6             8     extratrees  0.5773379  0.9695972  0.3394949          
   7             8     variance    0.5577352  0.9547528  0.3032116          
   9             6     variance    0.7879954  0.9327910  0.4963112          
  10             1     variance    1.6776129  0.4690520  1.2301522          
  10             9     variance    0.5034747  0.9606880  0.2738473          
  11             9     maxstat     0.9170411  0.8006187  0.5022987          
  12             4     extratrees  1.1298321  0.9125260  0.7763245          
  12             7     extratrees  0.8136352  0.9470783  0.5258516          
  12             9     variance    0.5317587  0.9544301  0.2987627          
  14             3     maxstat     1.5674461  0.5011070  1.0390357          
  14             8     variance    0.5861699  0.9587524  0.3289274          
  16             1     extratrees  1.7457760  0.6409523  1.2646969          
  16             9     variance    0.5567306  0.9512428  0.3315988          
  17             1     variance    1.7390025  0.4286680  1.2911061          
  20             6     extratrees  1.0995141  0.9520907  0.7638292          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 8, splitrule = variance
 and min.node.size = 3.
[1] "Sat Mar 10 17:21:48 2018"
Error in code$varImp(object$finalModel, ...) : 
  No importance values available
In addition: Warning messages:
1: package 'gpls' is not available (for R version 3.4.3) 
2: package 'rPython' is not available (for R version 3.4.3) 
Random Forest 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  predFixed  minNode  RMSE       Rsquared   MAE        Selected
  1          11       1.7253513  0.4163131  1.2753388          
  1          17       1.8186426  0.3183228  1.3468881          
  2          18       1.6546542  0.5753194  1.2318870          
  3           3       1.0948181  0.8610596  0.7534140          
  3          17       1.4498783  0.7732222  1.0899001          
  3          18       1.5352766  0.6958080  1.1529128          
  4          14       1.2097913  0.8451230  0.8760793          
  5           2       0.6902711  0.9518161  0.4615697          
  5          18       1.2122470  0.8685659  0.9130000          
  5          19       1.2608086  0.8332094  0.9705160          
  6           6       0.7286118  0.9283236  0.4564957          
  6           8       0.8107939  0.9170522  0.5271289          
  6          15       0.9975885  0.9037789  0.7042901          
  6          18       1.1267781  0.8712872  0.8559157          
  7           3       0.4220762  0.9747587  0.2685418          
  7          18       0.9268426  0.8996231  0.7128590          
  7          19       0.9886220  0.8978994  0.7622966          
  8           2       0.2735434  0.9861701  0.1732505  *       
  9          13       0.5843283  0.9352886  0.3824363          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were predFixed = 8 and minNode = 2.
[1] "Sat Mar 10 17:24:03 2018"
Loading required package: lars
Loaded lars 1.2

Relaxed Lasso 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  phi         lambda     RMSE       Rsquared   MAE        Selected
  0.05125084  51.456944  2.8638250        NaN  2.1233547          
  0.05275606   9.817258  0.4055258  0.9558274  0.3033016          
  0.10079370   3.760947  0.3948871  0.9584664  0.2967438          
  0.11085019  36.895886  0.3927021  0.9589990  0.2954195          
  0.25162383  22.746582  0.3638350  0.9657329  0.2779639          
  0.38810546  16.997694  0.3395077  0.9709809  0.2642172          
  0.52133724   1.593657  0.2741601  0.9828833  0.2179545          
  0.63433217  92.452311  2.8638250        NaN  2.1233547          
  0.65235730   6.687856  0.3069369  0.9776500  0.2524571          
  0.71471489  15.507495  0.3029374  0.9785769  0.2520332          
  0.80789568   3.384580  0.3001400  0.9795225  0.2523253          
  0.81999157   1.212967  0.2738680  0.9828738  0.2184046          
  0.85145636  23.794620  0.3002030  0.9797886  0.2531082          
  0.85732026   1.931085  0.2664394  0.9840494  0.2060233  *       
  0.87593318   4.257049  0.3006272  0.9798900  0.2537666          
  0.87948419   2.302639  0.2875004  0.9812322  0.2394034          
  0.88968643  15.399588  0.3009878  0.9799320  0.2541365          
  0.89516486  12.757954  0.3011559  0.9799457  0.2542838          
  0.92793745  37.205409  1.1483831  0.9783151  0.8548672          
  0.93964561   8.760830  0.3030295  0.9799950  0.2554803          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 1.931085 and phi = 0.8573203.
[1] "Sat Mar 10 17:24:18 2018"
Random Forest 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE        Selected
  1     1.6300394  0.5338670  1.1868151          
  2     1.4032644  0.7484906  1.0193900          
  3     1.2356569  0.8081543  0.8883560          
  4     1.0225591  0.8929879  0.7022738          
  5     0.8453111  0.9301384  0.5516684          
  6     0.7556793  0.9448158  0.4815150          
  7     0.6392809  0.9543569  0.3776966          
  8     0.5499077  0.9584853  0.3102641          
  9     0.4630676  0.9636321  0.2570954  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 17:24:32 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Ridge Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  lambda        RMSE       Rsquared   MAE        Selected
  1.673249e-05  0.2818650  0.9786383  0.2231256          
  3.735413e-05  0.2818524  0.9786396  0.2231140          
  6.572525e-05  0.2818351  0.9786415  0.2230982          
  1.103020e-04  0.2818080  0.9786445  0.2230733          
  3.425633e-04  0.2816679  0.9786598  0.2229437          
  4.671629e-04  0.2815936  0.9786680  0.2228743          
  6.726497e-04  0.2814722  0.9786812  0.2227601          
  2.540810e-03  0.2804377  0.9787941  0.2217320          
  5.622926e-03  0.2789956  0.9789510  0.2200764          
  7.860245e-03  0.2781473  0.9790433  0.2189051          
  1.699121e-02  0.2762720  0.9792512  0.2149699  *       
  2.955830e-02  0.2772029  0.9791673  0.2110577          
  3.017183e-02  0.2773345  0.9791540  0.2109032          
  3.952219e-02  0.2801364  0.9788608  0.2102562          
  9.313185e-02  0.3115063  0.9750377  0.2254829          
  1.063296e-01  0.3207228  0.9737672  0.2312121          
  3.864816e-01  0.4892077  0.9421894  0.3579910          
  3.960985e-01  0.4937988  0.9411301  0.3612939          
  1.028413e+00  0.7014720  0.8821716  0.5294203          
  5.765863e+00  1.0843878  0.7308491  0.8605400          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was lambda = 0.01699121.
[1] "Sat Mar 10 17:24:46 2018"
Robust Linear Model 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  intercept  psi           RMSE       Rsquared   MAE        Selected
  FALSE      psi.huber     0.3085006  0.9811170  0.1898832          
  FALSE      psi.hampel    0.3189973  0.9803322  0.1953012          
  FALSE      psi.bisquare  0.3827364  0.9823047  0.1773130          
   TRUE      psi.huber     0.2938103  0.9798095  0.2000275          
   TRUE      psi.hampel    0.2852213  0.9785940  0.2188363  *       
   TRUE      psi.bisquare  0.4478805  0.9836418  0.1885186          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were intercept = TRUE and psi = psi.hampel.
[1] "Sat Mar 10 17:24:59 2018"
CART 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  cp          RMSE      Rsquared   MAE        Selected
  0.00000000  1.075255  0.6934668  0.8001889          
  0.02840137  1.075255  0.6934668  0.8001889  *       
  0.09517241  1.144757  0.6558455  0.8574538          
  0.71152434  1.923455        NaN  1.4339021          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.02840137.
[1] "Sat Mar 10 17:25:12 2018"
CART 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results:

  RMSE      Rsquared   MAE      
  1.075255  0.6934668  0.8001889

[1] "Sat Mar 10 17:25:26 2018"
CART 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  maxdepth  RMSE      Rsquared   MAE        Selected
  1         1.144757  0.6558455  0.8574538          
  2         1.075255  0.6934668  0.8001889  *       
  3         1.075255  0.6934668  0.8001889          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was maxdepth = 2.
[1] "Sat Mar 10 17:25:39 2018"
Quantile Regression with LASSO penalty 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 25, 27, 28 
Resampling results across tuning parameters:

  lambda        RMSE       Rsquared   MAE        Selected
  1.673249e-05  0.3241823  0.9834171  0.1737316          
  3.735413e-05  0.3241823  0.9834171  0.1737316          
  6.572525e-05  0.3241823  0.9834171  0.1737316          
  1.103020e-04  0.3241823  0.9834171  0.1737316  *       
  3.425633e-04  0.3241823  0.9834171  0.1737316          
  4.671629e-04  0.3244665  0.9834268  0.1735061          
  6.726497e-04  0.3369138  0.9834781  0.1718991          
  2.540810e-03  0.3520924  0.9833884  0.1735280          
  5.622926e-03  0.3554926  0.9834793  0.1719919          
  7.860245e-03  0.3609916  0.9833161  0.1731912          
  1.699121e-02  0.3707193  0.9830425  0.1684570          
  2.955830e-02  0.3772332  0.9825496  0.1693932          
  3.017183e-02  0.3772332  0.9825496  0.1693932          
  3.952219e-02  0.3862715  0.9825220  0.1705886          
  9.313185e-02  0.4602228  0.9839469  0.1774688          
  1.063296e-01  0.4621998  0.9839449  0.1781183          
  3.864816e-01  2.0715589        NaN  1.3003291          
  3.960985e-01  2.0715589        NaN  1.3003291          
  1.028413e+00  2.0715589        NaN  1.3003291          
  5.765863e+00  2.0715589        NaN  1.3003291          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was lambda = 0.000110302.
[1] "Sat Mar 10 17:25:54 2018"

WARNING: --max-mem-size= 0.0M: too small and ignored


R version 3.4.2 (2017-09-28) -- "Short Summer"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> options(repos=structure(c(CRAN="https://rweb.crmda.ku.edu/cran/")))
> ## capture messages and errors to a file.https://rweb.crmda.ku.edu/cran/
> #zz <- file("all.Rout", open="wt")https://cran.cnr.berkeley.edu
> #sink(zz, type="message") edit for rebaseless
> #chek for R package updates
> #try(log("a")) ## test --no-edit
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> memory.limit()
[1] 7626
> task.subject<-"14th20hp3cv"
> pc.mlr<-c("ACE")#"ALTA","HOPPER"
> which.computer<-Sys.info()[['nodename']]
> out.file<-paste("out",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,".csv",sep="")
> importance.file<-paste("importance",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,sep="")
> 
> if(exists("base.folder")){setwd(base.folder)}
> base.folder<-getwd()
> cpout.folder<-paste(base.folder,"/",which.computer,sep = "")
> setwd(cpout.folder)
> 
> if(length(which(list.files() == out.file))<1) write.table( "0.01,0.01,100,100,100,Wed Aug 02 16:37:25 2017,dummy,8,1,basic latent features,ignore,none,asis,1.12784979099243,random,333,53,adaptive_cv,16,5,2,2,19,0.0107744822639878,FALSE,,,,,,,,,," ,file =,out.file,  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,".csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,".csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,"mlr.csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,"mlr.csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> 
> cv.iters=3
> tuneLength=20
> tuneLength2=8
> normings=c("YeoJohnson","ICA", "centernscale","expoTrans","range01","asis","quantile")#,"centernscale"
> 
> gensTTesto<-c(56,53,4,12,13,14,15,20,45,54,55, 44,3,1,52)#,  51,c(4)#c(1:40)#c(5,10,11,13,14,15,16,17,18,19,20,21,24,28,38,39,40)
> gensTTest<-vector()
> write.table( t(gensTTesto),file = "initial tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
> try({
+   gensTTest<-t(read.csv("tasks to test.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   gensTTest<-as.vector(gensTTest)
+ })
> if(!exists("gensTTest")) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> gensTTesto<-c(gensTTesto[length(gensTTesto):1])
> if(length(gensTTest)<1) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> 
> 
> ########packages install check######
> 
> #list.of.packages <- c("caret","caretEnsemble","mlr","MLmetrics","tgp")
> #list.of.packages <- c("gower","dimRed","DEoptimR","caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> #new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> #if(length(new.packages)) install.packages(new.packages, dep = TRUE)
> 
> 
> #install.packages("mlr", dependencies = c("Depends", "Suggests"))
> #install.packages("caret", dependencies = c("Depends", "Suggests"))
> #install.packages("caret",repos = "http://cran.r-project.org",dependencies = c("Depends", "Imports", "Suggests"))
> #install.packages("SuperLearner", dependencies = c("Depends", "Suggests"))
> #install.packages("rattle", dependencies = c("Depends", "Suggests"))
> 
> # Load libraries
> #library(mlbench)
> 
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> #library(caretEnsemble)
> library(MLmetrics)

Attaching package: 'MLmetrics'

The following objects are masked from 'package:caret':

    MAE, RMSE

The following object is masked from 'package:base':

    Recall

> 
> ########error no repeat#########
> 
> 
> try({
+   before.last.alg<-as.matrix(read.csv("beforelast algorithm.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.alg<-as.matrix(read.csv("last algorithm tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   #write.table(paste(date(), last.alg,.Platform$OS.type,.Platform$r_arch,which.computer,sep=" "),file = "algos after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.alg==before.last.alg){print("algorithm may be broken")}
+   write.table(last.alg,file = "beforelast algorithm.csv",  quote = F, row.names = F,col.names = F)
+ })
> try({
+   before.last.tsk<-as.matrix(read.csv("beforelast task.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.tsk<-as.matrix(read.csv("last task tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   write.table(paste(date(),last.alg, last.tsk,cv.iters,tuneLength,.Platform$OS.type,.Platform$r_arch,which.computer,sep=","),file = "test after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.tsk==before.last.tsk){print("task may be broken")}
+   write.table(last.tsk,file = "beforelast task.csv",  quote = F, row.names = F,col.names = F)
+ })
[1] "task may be broken"
> bad.models=c("spaccceeee")
> previous.fails<-(read.csv("test after which reset.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
> previous.fails<-previous.fails[previous.fails[,8]==which.computer,]
> lgf<-length(previous.fails[,2])
> for(lt in 2:lgf)  {
+   if(previous.fails[lt,2]==previous.fails[lt-1,2])  {
+     bad.models=union(bad.models,c(paste(previous.fails[lt,2])))  }}
> 
> #######not to redo a test function#####
> check.redundant<-function(df=df.previous.calcs,norming="asis",trans.y=1,withextra="missing",missingdata="leaveempty",datasource="mean" ,column.to.predict=200,allmodel="ctree")
+ {
+   for(intern in 1:length(df[,1])){
+     if((any(df[intern,] == norming, na.rm=T))&&
+        (any(df[intern,] == withextra, na.rm=T))&&
+        (any(df[intern,] == missingdata, na.rm=T))&&
+        (any(df[intern,] == datasource, na.rm=T))&&
+        (any(df[intern,] == column.to.predict, na.rm=T))&&
+        (any(df[intern,] == allmodel, na.rm=T))&&
+        (  (df[intern,9] == trans.y)))
+     {return(TRUE)}
+   }
+   return(FALSE)
+ }
> #####caret init#####
> best.ranged <- c("avNNet", "nnet", "pcaNNet", "glm.nb")
> best.asis <- c("svmLinear3", "relaxo", "superpc", "xgbTree")
> best.cns <- c("gam", "bam", "svmLinear2", "msaenet", "BstLm", "gbm")
> 
> cv6hp5 <- c( "BstLm", "qrnn")#earth
> cv3hp32 <- c("Rborist", "pcaNNet", "SBC")
> cv7x5hp32 <- c("gbm", "krlsPoly", "kknn", "xgbLinear","RRF", "cubist", "rlm" )
> cv6hp5.avoid <- c("pcaNNet")
> cv3hp32.avoid <- c("glm.nb", "gamboost", "ctree2","glmboost", "leapSeq","ctree","svmLinear2")
> cv7x5hp32.avoid <- c("SBC","bagearthgcv","gcvearth","lmStepAIC","glmStepAIC","bridge","lm","glm","bayesglm","blassoAveraged","treebag","rpart1SE")
> 
> allmodels <- c("avNNet", "bagEarth", "bagEarthGCV",
+                "bayesglm", "bdk", "blackboost", "Boruta", "brnn", "BstLm" ,
+                "bstTree", "cforest", "ctree", "ctree2", "cubist", "DENFIS",
+                "dnn", "earth", "elm", "enet",   "evtree",
+                "extraTrees",  "gamLoess",  "gaussprLinear", "gaussprPoly", "gaussprRadial",
+                "gcvEarth","glm", "glmboost",  "icr", "kernelpls",
+                "kknn", "knn",  "krlsRadial", "lars" , "lasso",
+                "leapBackward", "leapForward", "leapSeq", "lm", "M5", "M5Rules",
+                "mlpWeightDecay", "neuralnet" , "partDSA",
+                "pcaNNet", "pcr", "penalized", "pls", "plsRglm", "ppr",
+                "qrf" , "ranger",  "rf")
> allmodels <- c("rlm", "rpart", "rpart2",
+                "RRF", "RRFglobal",  "simpls",
+                "svmLinear", "svmPoly", "svmRadial", "svmRadialCost",
+                "widekernelpls",  "xgbLinear",
+                "xgbTree")
> allmodels <- c("avNNet","BstLm","bstTree","cforest","ctree","ctree2",
+                "cubist","earth","enet","evtree","glmboost",
+                "icr","kernelpls","kknn","lasso","pcaNNet",
+                "pcr","pls","qrf","ranger","rf")
> 
> allmodels <- c("kknn", "cubist", "avNNet", "xgbLinear", "RRF", "pcaNNet","earth","nnet","gbm","enet","lasso","BstLm",
+                "foba", "leapBackward", "gcvEarth", "SBC","glm.nb","gamboost","ctree2","relaxo",
+                "bartMachine","extraTrees","bam","gam","randomGLM")
> #allmodels <- c("bam")
> #allmodels <- c("rf")"rqlasso",, "xyf" "rvmPoly", "rvmRadial",    "spls", "superpc" ,   "treebag",  "svmLinear2",  "SBC",
> #allmodels <- c("bartMachine", "xgbLinear", "pcaNNet","svmLinear","glmnet","cforest","cubist","rf","ranger")"glmnet",
> #wow rfRules is really slow "rfRules","WM", takes 50min
> # brak everythig "rbfDDA","ridge","rqnc",
> # use "rf" to test all
> library(caret)
> allmodels <- unique(modelLookup()[modelLookup()$forReg,c(1)])
> #allmodels <-c("avNNet", "nnet", "pcaNNet",  "glm.nb", "gam" ,
> #              "bam","msaenet", "svmLinear2","svmLinear3",
> #              "relaxo",  "superpc", "xgbTree", "BstLm")
> #allmodels<- c("svmLinear","svmPoly","svmRadial")
> #library(doParallel); cl <- makeCluster(detectCores()); registerDoParallel(cl)
> #allmodels<-c("bartMachine","extraTrees")#,"randomGLM"
> 
> 
> adaptControl <- trainControl(method = "adaptive_cv",
+                              number = 7, repeats = 5,
+                              adaptive = list(min = 4, alpha = 0.05,
+                                              method = "gls", complete = FALSE),
+                              search = "random")
> adaptControl <-trainControl(method = "cv", number = cv.iters,  search = "random")
> simpleControl <- trainControl(method = "cv",
+                               number = cv.iters,
+                               search = "random")
> 
> 
> #########MLR init######
> #R.utils::gcDLLs()
> #list.of.packages <- c("ParamHelpers","devtools","mlrMBO","RJSONIO","plot3D","plotly")
> #install.packages("mlrMBO", dependencies = c("Depends", "Suggests"))
> list.of.packages <- c("caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> if(length(new.packages)) install.packages(new.packages, dep = TRUE)
Installing packages into 'C:/Users/John/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Warning message:
packages ' RWeka', 'rPython', 'rrf' are not available (for R version 3.4.2) 
> 
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> tuneLengthMLR<-tuneLength
> mlr.iters<-cv.iters
> #######data read process start#####
> seed.var =222+round(runif(1,min=0,max=100))
> column.to.predict=1
> print(date());
[1] "Sun Mar 11 18:44:50 2018"
> 
> setwd(base.folder)
> if(!exists("gen.count")){gen.count=56}
> gens.names<-as.matrix(read.table("gens names.csv", sep = ",",header = FALSE,row.names=1,fill=TRUE, quote="",dec="."))
> count.toy.data.passed<-1
> for(gend.data in gensTTest){
+   count.toy.data.passed<-count.toy.data.passed+1
+   setwd(base.folder)
+   data.source<-as.matrix(read.csv(paste("Generats/",gens.names[gend.data],".csv", sep = ""), sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   datasource<-gens.names[gend.data,1]
+   setwd(cpout.folder)
+   missingdatas=c("ignore")
+   for(missingdata in missingdatas){
+     withextras=c("none")
+     for(withextra in withextras){
+       ################data wrestling###############
+ 
+       dependant.selection=complete.cases(data.source[,column.to.predict])
+       df.previous.calcs=as.data.frame(read.csv(file=out.file, header = FALSE, sep = ",", quote = "",
+                                                dec = ".", fill = TRUE, comment.char = ""))
+       unimportant.computations<-vector(mode = "logical",length=length(df.previous.calcs[,1])  )
+       for(intern in 1:length(df.previous.calcs[,1])){
+         if((any(df.previous.calcs[intern,] == withextra, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == missingdata, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == datasource, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == column.to.predict, na.rm=T)))
+         {unimportant.computations[intern]<-T}}
+ 
+       df.previous.calcs<-df.previous.calcs[unimportant.computations,]
+ 
+       #data.source=data.frame( data.source[,column.to.predict],data.source[,1:2], data.source[,4:(column.to.predict-1)], data.source[,(column.to.predict+1):length( data.source[1,])])
+ 
+         for(norming in normings) {
+         for(trans.y in 1:2) {
+           df.toprocess=data.source
+           y.untransformed<-df.toprocess[,1]
+ 
+           if(norming=="centernscale"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("center", "scale"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="range01"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="expoTrans"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("expoTrans"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="YeoJohnson"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("YeoJohnson"))#"center", "scale",
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+ 
+           if((norming=="asis")&&(trans.y==2)){next}
+ 
+ 
+           ################preprocess###########
+           df.toprocess=data.frame(df.toprocess[dependant.selection,])
+           y.untransformed=y.untransformed[dependant.selection]
+           if(norming=="quantile"){
+             for(Clol in trans.y:length(data.source[1,])){
+               df.toprocess[,Clol]<- (rank(df.toprocess[,Clol],na.last = "keep",ties.method = "average")-1) }
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+ 
+           loess.model<-loess(y.untransformed~ df.toprocess[,1],span = 0.21, degree = 1)
+ 
+           #df.toprocess = data.frame(df.toprocess,)
+           nzv <- nearZeroVar(df.toprocess[,])#, saveMetrics= TRUE
+           #nzv[nzv$nzv,][1:10,]
+           if(length(nzv)>1){
+             df.toprocess = (df.toprocess[, -nzv])}
+           df.toprocess = signif(df.toprocess,digits = 3)
+ 
+           seed.var =222+round(runif(1,min=0,max=100))
+           set.seed(seed.var)
+           inTrain <- createDataPartition(y = df.toprocess[,1],
+                                          p = .75,
+                                          list = FALSE)
+           training <- df.toprocess[ inTrain,]
+           testing  <- df.toprocess[-inTrain,]
+           write.table(df.toprocess,file = "sanity check 1.csv",  quote = F, row.names = F,col.names = F)
+ 
+           ###########for all models#################
+           setwd(base.folder)
+           if(max(which.computer==pc.mlr)>0)
+             source("MLR part.R")
+           else
+             source("Caret part.R")
+ 
+          setwd(cpout.folder)
+           if(norming == normings[length(normings)]){
+             if(count.toy.data.passed>length(gensTTest)){gensTTest<-c(gensTTesto)}
+             write.table( t(gensTTest[count.toy.data.passed:length(gensTTest)]),file = "tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
+ 
+             }
+ 
+         }
+       }
+     }
+   }
+ 
+ }
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  These variables have zero variances: V11, V12
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  These variables have zero variances: V11, V12

Attaching package: 'kknn'

The following object is masked from 'package:caret':

    contr.dummy

k-Nearest Neighbors 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  kmax  distance   kernel        RMSE        Rsquared   MAE         Selected
    6   0.7995372  biweight      0.04625399  0.9950186  0.03450304          
   17   2.5570063  epanechnikov  0.04697947  0.9951247  0.03545510          
   29   0.1084052  epanechnikov  0.08868811  0.9828459  0.06486354          
   30   2.1089618  cos           0.04581371  0.9953428  0.03456188          
   46   1.6581629  epanechnikov  0.04545528  0.9954362  0.03424813          
   47   2.5869983  epanechnikov  0.04703566  0.9951127  0.03550322          
   54   0.4874255  epanechnikov  0.04756390  0.9948553  0.03528480          
   62   1.7470282  rectangular   0.05393494  0.9934601  0.04159031          
   75   2.9100063  triweight     0.04662992  0.9951895  0.03509341          
  145   2.4251911  rectangular   0.05476214  0.9933950  0.04084079          
  147   2.1057207  epanechnikov  0.04623432  0.9952705  0.03493581          
  148   2.4274699  triweight     0.04590000  0.9953753  0.03426857          
  157   1.5220583  cos           0.04469123  0.9955423  0.03363511          
  181   0.5656866  biweight      0.04560350  0.9953380  0.03359367          
  184   1.7771909  triangular    0.04451115  0.9955893  0.03345338  *       
  185   0.1341173  biweight      0.07101239  0.9886834  0.05201093          
  187   1.9097487  triangular    0.04475872  0.9955400  0.03365825          
  195   2.3724075  inv           0.04716564  0.9951235  0.03503808          
  205   2.8084700  cos           0.04709082  0.9950772  0.03562251          
  242   2.4105018  triweight     0.04586842  0.9953818  0.03424269          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were kmax = 184, distance = 1.777191
 and kernel = triangular.
[1] "Sun Mar 11 19:28:52 2018"
k-Nearest Neighbors 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  k    RMSE        Rsquared   MAE         Selected
    6  0.06654303  0.9901733  0.05116790  *       
   17  0.08599267  0.9845192  0.06523125          
   29  0.10797885  0.9768560  0.08269252          
   30  0.10952311  0.9762616  0.08367559          
   46  0.13306117  0.9668880  0.10116054          
   47  0.13473852  0.9661368  0.10240223          
   54  0.14448240  0.9616437  0.10949091          
   62  0.15410315  0.9574092  0.11693091          
   75  0.16998719  0.9503657  0.12857903          
  145  0.24744294  0.9145432  0.18414345          
  147  0.24953781  0.9134883  0.18562225          
  148  0.25055992  0.9128845  0.18638456          
  157  0.25980950  0.9080733  0.19309254          
  181  0.28390344  0.8956970  0.21171278          
  184  0.28684839  0.8943923  0.21401597          
  185  0.28785702  0.8940363  0.21487154          
  187  0.28980188  0.8935644  0.21630886          
  195  0.29730359  0.8912289  0.22246547          
  205  0.30696163  0.8868041  0.23021683          
  242  0.34315620  0.8701409  0.25919859          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was k = 6.
[1] "Sun Mar 11 19:29:03 2018"
## KRLS Package for Kernel-based Regularized Least Squares.

## See Hainmueller and Hazlett (2014) for details.

Polynomial Kernel Regularized Least Squares 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  lambda        degree  RMSE          Rsquared     MAE           Selected
  1.273966e-05  1       2.080984e+04  0.004061066  1.802013e+04          
  2.140217e-05  3       2.390732e+01  0.028526851  1.930238e+01          
  3.800331e-05  1       6.976430e+03  0.004061724  6.041132e+03          
  3.864499e-05  3       1.390416e+01  0.030820659  1.127294e+01          
  8.086287e-05  2       1.840983e+01  0.254170549  1.490193e+01          
  8.379028e-05  3       6.693788e+00  0.035588048  5.455230e+00          
  1.151155e-04  1       2.303595e+03  0.004063815  1.994706e+03          
  1.677762e-04  2       8.455853e+00  0.265243232  6.783380e+00          
  3.055212e-04  3       1.981195e+00  0.047122555  1.657403e+00          
  7.674838e-03  3       6.525848e-01  0.048625043  5.403314e-01          
  8.527699e-03  3       6.510042e-01  0.047278782  5.388932e-01          
  8.812371e-03  3       6.505484e-01  0.046868851  5.384882e-01          
  1.355387e-02  2       5.569615e-01  0.499315628  4.408655e-01  *       
  4.152354e-02  1       6.648404e+00  0.005341233  5.754300e+00          
  4.707283e-02  2       5.947348e-01  0.592864309  4.767532e-01          
  4.847573e-02  1       5.719541e+00  0.005332782  4.953521e+00          
  5.324651e-02  2       5.975134e-01  0.596211020  4.797309e-01          
  7.858537e-02  3       6.371135e-01  0.485556716  5.242745e-01          
  1.226160e-01  3       6.361858e-01  0.599173762  5.230698e-01          
  6.635549e-01  3       6.348158e-01  0.606803087  5.213133e-01          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 0.01355387 and degree = 2.
[1] "Sun Mar 11 19:31:26 2018"

Attaching package: 'kernlab'

The following object is masked from 'package:ggplot2':

    alpha


 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.152201631  0.129492867  0.419100806  0.273771892  0.003163159  0.002946103 
          V8           V9          V10 
-0.074955893  0.048310618  0.108246363 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7          V8
25% 0.1515568 0.1289196 0.4174270 0.2721402 0.002732914 0.002526181 -0.07547100
50% 0.1526695 0.1299334 0.4204725 0.2747459 0.003518128 0.003300911 -0.07458287
75% 0.1531824 0.1303867 0.4223954 0.2763519 0.003899085 0.003685081 -0.07412107
            V9       V10
25% 0.04789252 0.1077661
50% 0.04873594 0.1086500
75% 0.04913013 0.1091658

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.245557258  0.115571334  0.096538855  0.488976330  0.022567909 -0.008334054 
          V8           V9          V10 
-0.117395494  0.088797784  0.054129881 

 Quartiles of Marginal Effects:
 
           V2         V3         V4        V5           V6           V7
25% 0.1929200 0.08072513 0.03403609 0.3443371 -0.005063958 -0.042631679
50% 0.2506646 0.10937440 0.09165101 0.5016822  0.014790791 -0.007873721
75% 0.2964149 0.16035049 0.15229827 0.6396466  0.045777629  0.027543634
             V8         V9         V10
25% -0.17946346 0.05962368 -0.01599745
50% -0.12602120 0.09531619  0.06948726
75% -0.06856519 0.11519401  0.12366405

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.132166730  0.110638468  0.443819487  0.113142324  0.009999353  0.010670899 
          V8           V9          V10 
-0.060595361  0.068495560  0.097398367 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6           V7
25% 0.1267816 0.1036181 0.3661125 -0.0170769 -0.008301450 -0.008787882
50% 0.1430290 0.1151935 0.4816238  0.1418278  0.008666667  0.010263006
75% 0.1534718 0.1256439 0.5835149  0.2270356  0.030648664  0.030676790
             V8         V9        V10
25% -0.08733747 0.03852672 0.08053342
50% -0.06707868 0.06945859 0.10725726
75% -0.04252669 0.09680325 0.12556090

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.221681736  0.167644319  0.174624886  0.375133870 -0.006070414 -0.018067440 
          V8           V9          V10 
-0.053863521  0.069094804  0.048745723 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5            V6          V7
25% 0.2067657 0.1423054 0.1254667 0.3311034 -0.0123163723 -0.02672259
50% 0.2249299 0.1648471 0.1754970 0.3824092 -0.0065634781 -0.01965714
75% 0.2368595 0.1906566 0.2223216 0.4167305  0.0002107039 -0.01108587
             V8         V9        V10
25% -0.06506946 0.04932675 0.01249191
50% -0.05338200 0.06706059 0.04504695
75% -0.04242341 0.08782110 0.08397398

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.236206790  0.164968454  0.145640998  0.401170522 -0.006342167 -0.019699590 
          V8           V9          V10 
-0.052729511  0.070727762  0.040058654 

 Quartiles of Marginal Effects:
 
           V2        V3         V4        V5            V6          V7
25% 0.2199681 0.1398864 0.09853593 0.3627949 -0.0120257104 -0.02818713
50% 0.2404303 0.1630327 0.14949272 0.4044340 -0.0066450462 -0.02075359
75% 0.2534201 0.1877677 0.19079341 0.4361014 -0.0007274116 -0.01262337
             V8         V9         V10
25% -0.06296740 0.05494464 0.002685362
50% -0.05186701 0.06898983 0.036399164
75% -0.04183452 0.08534072 0.075719402

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.144614396  0.122500656  0.462851858  0.137099948  0.007571432  0.007825875 
          V8           V9          V10 
-0.059445444  0.067871007  0.099421061 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6           V7
25% 0.1377812 0.1134002 0.3916585 0.07624968 -0.006185438 -0.006008185
50% 0.1489454 0.1219189 0.4807777 0.16286457  0.005986134  0.006573924
75% 0.1566755 0.1361579 0.5702019 0.23034269  0.021602163  0.021011770
             V8         V9        V10
25% -0.07348280 0.04082412 0.08024213
50% -0.06214170 0.06284552 0.10311749
75% -0.05025114 0.09304431 0.12246183

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.226539689  0.086874425  0.129819663  0.474480673  0.035279697 -0.006055903 
          V8           V9          V10 
-0.137901215  0.131222974  0.072467824 

 Quartiles of Marginal Effects:
 
           V2         V3          V4        V5          V6          V7
25% 0.1410529 0.01489756 -0.03946832 0.2579438 -0.04821358 -0.10277179
50% 0.1892018 0.10543992  0.11537419 0.4257055  0.02864131 -0.00361880
75% 0.2959343 0.16063601  0.29224382 0.6759078  0.13594763  0.06786039
              V8         V9         V10
25% -0.281262461 0.01204482 -0.02795151
50% -0.154841499 0.13027738  0.09163780
75% -0.007608194 0.24823677  0.17534212

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.16156639  0.12551903  0.28462766  0.25357420  0.01263853 -0.00544391 
         V8          V9         V10 
-0.08687446  0.09240994  0.10129484 

 Quartiles of Marginal Effects:
 
           V2         V3        V4        V5           V6           V7
25% 0.1392591 0.09962074 0.1504538 0.1118491 -0.019738044 -0.032906159
50% 0.1553580 0.12643484 0.2686565 0.2659791 -0.001555077 -0.009845418
75% 0.1799423 0.14773665 0.3983969 0.3956618  0.038047263  0.022685009
             V8         V9        V10
25% -0.14589530 0.02069851 0.06798876
50% -0.07770201 0.09249227 0.11114520
75% -0.02550667 0.15224813 0.13735978

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.13292280  0.11014599  0.42049151  0.19250412  0.01159131  0.01227980 
         V8          V9         V10 
-0.05393351  0.06256742  0.10348967 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5         V6         V7          V8
25% 0.1327739 0.1100297 0.4201343 0.1921238 0.01151589 0.01220720 -0.05403096
50% 0.1330375 0.1102492 0.4208924 0.1928070 0.01165896 0.01234576 -0.05388050
75% 0.1331437 0.1103505 0.4213638 0.1931048 0.01171734 0.01240636 -0.05379926
            V9       V10
25% 0.06246049 0.1033510
50% 0.06268163 0.1036006
75% 0.06278254 0.1036934

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.154634226  0.131824883  0.413017522  0.172585898  0.006763185  0.004775525 
          V8           V9          V10 
-0.068801940  0.073508716  0.097959688 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6            V7
25% 0.1454612 0.1145719 0.3516476 0.1033850 -0.009179434 -0.0098380598
50% 0.1545980 0.1244878 0.4217714 0.2043523  0.003326679  0.0007605601
75% 0.1660848 0.1461571 0.4939839 0.2633306  0.021851825  0.0187546937
             V8         V9        V10
25% -0.08736957 0.03684771 0.07668085
50% -0.06876223 0.07068686 0.10571338
75% -0.04995278 0.10271113 0.12680365

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.178829514  0.154735809  0.354551727  0.268904922 -0.009138606 -0.008811857 
          V8           V9          V10 
-0.027349760  0.043619396  0.080425664 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6          V7
25% 0.1749057 0.1511027 0.3043361 0.2371178 -0.015611319 -0.01616370
50% 0.1811953 0.1562669 0.3564529 0.2651439 -0.010300483 -0.01000680
75% 0.1849443 0.1607499 0.4151539 0.3001688 -0.003290481 -0.00191592
             V8         V9        V10
25% -0.03282646 0.03953401 0.07188111
50% -0.02752200 0.04379416 0.08019358
75% -0.02317237 0.04834747 0.09065791

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.1612158750  0.1423998087  0.4129900585  0.1874174193  0.0029730293 
           V7            V8            V9           V10 
-0.0002422914 -0.0559776612  0.0662545207  0.0908334678 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5            V6           V7
25% 0.1541594 0.1272803 0.3485493 0.1429797 -0.0067971539 -0.010893216
50% 0.1612565 0.1380023 0.4140831 0.1999868  0.0009817955 -0.001599298
75% 0.1692108 0.1582936 0.5070482 0.2547222  0.0124042442  0.008787772
             V8         V9        V10
25% -0.06794832 0.03464250 0.06432769
50% -0.05541622 0.06060987 0.09701337
75% -0.04531988 0.09356281 0.12094312

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.285920375  0.189659279  0.233197165  0.291445897  0.008620347 -0.046568041 
          V8           V9          V10 
 0.014579484  0.066253181 -0.037117977 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7         V8
25% 0.2847191 0.1851886 0.2007410 0.2451015 0.006246038 -0.05097768 0.01052403
50% 0.2864623 0.1899792 0.2332136 0.2859548 0.008482114 -0.04706563 0.01380262
75% 0.2876958 0.1951833 0.2696804 0.3370742 0.010635569 -0.04240482 0.01849017
            V9         V10
25% 0.06212067 -0.04759665
50% 0.06662972 -0.03852969
75% 0.07023049 -0.02686527

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.132192668  0.107996862  0.446896064  0.123805826  0.009041271  0.010369841 
          V8           V9          V10 
-0.055137169  0.062859360  0.096281303 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6           V7
25% 0.1226543 0.1009604 0.3597172 0.01621831 -0.006836681 -0.006577428
50% 0.1426490 0.1160371 0.4832691 0.13788088  0.009204066  0.010635876
75% 0.1542230 0.1263400 0.5769988 0.22729804  0.026428451  0.028356725
             V8         V9       V10
25% -0.07832080 0.04450118 0.0846343
50% -0.06241901 0.06555317 0.1010461
75% -0.04007502 0.08283541 0.1193410

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.290845496  0.157820024  0.229925973  0.366433108 -0.037660202 -0.007028061 
          V8           V9          V10 
 0.033588157  0.031254560  0.001802013 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6           V7         V8
25% 0.2890638 0.1571812 0.2211730 0.3611731 -0.04004071 -0.009585504 0.03158236
50% 0.2919059 0.1579657 0.2290386 0.3681555 -0.03763199 -0.006883697 0.03423734
75% 0.2932981 0.1586530 0.2385090 0.3734649 -0.03487021 -0.003925736 0.03612486
            V9           V10
25% 0.03025728 -0.0008959531
50% 0.03133391  0.0021025699
75% 0.03226168  0.0048895531

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.21705145  0.10015584  0.15117452  0.39553578  0.01643936 -0.01504256 
         V8          V9         V10 
-0.10594158  0.12510995  0.08635787 

 Quartiles of Marginal Effects:
 
           V2         V3         V4        V5          V6          V7
25% 0.1599810 0.05784193 0.02826834 0.2446637 -0.03823964 -0.07182590
50% 0.1906156 0.11260950 0.13498745 0.3558381  0.01077848 -0.01399508
75% 0.2679947 0.14913322 0.25326701 0.5392875  0.07388524  0.03044747
             V8         V9        V10
25% -0.21132208 0.04104131 0.01035581
50% -0.12062390 0.14206563 0.10845939
75% -0.01952447 0.21102258 0.15698228

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.1617978201  0.1401841114  0.4107553457  0.2185061024 -0.0002548055 
           V7            V8            V9           V10 
-0.0007199045 -0.0426595142  0.0486120293  0.0904103115 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5            V6           V7
25% 0.1558943 0.1350290 0.3451666 0.1708772 -0.0086897633 -0.010182906
50% 0.1645485 0.1429578 0.4196443 0.2172719 -0.0008761079 -0.001633140
75% 0.1706338 0.1488001 0.4896035 0.2683158  0.0077589995  0.007728142
             V8         V9        V10
25% -0.05052349 0.04113754 0.07894430
50% -0.04403442 0.04813069 0.09048467
75% -0.03562050 0.05519253 0.10193178

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.157943281  0.135377025  0.433286165  0.193091006  0.003868165  0.002814133 
          V8           V9          V10 
-0.048159517  0.051269955  0.092274073 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1517632 0.1294595 0.3613624 0.1447604 -0.005710829 -0.007865121
50% 0.1606007 0.1378211 0.4455400 0.1919850  0.003170755  0.001654499
75% 0.1666132 0.1459939 0.5266062 0.2510203  0.012665766  0.012676882
             V8         V9        V10
25% -0.05591926 0.03777914 0.07639378
50% -0.05030887 0.04676574 0.09279539
75% -0.04124172 0.06332721 0.10714870

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.155744264  0.132491084  0.406245107  0.177655915  0.006666667  0.004391729 
          V8           V9          V10 
-0.069798191  0.074028329  0.097669318 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6            V7
25% 0.1462369 0.1149006 0.3453538 0.1098639 -0.009432817 -0.0102082856
50% 0.1556593 0.1248826 0.4154663 0.2097225  0.003309708  0.0002634688
75% 0.1674211 0.1471997 0.4837741 0.2711511  0.021823767  0.0182065231
             V8         V9        V10
25% -0.08877418 0.03690483 0.07598346
50% -0.06917390 0.07100534 0.10578180
75% -0.05034186 0.10356972 0.12675143

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.315214229  0.148566238  0.220083347  0.383452435 -0.039310514 -0.001325284 
          V8           V9          V10 
 0.036182321  0.030369709 -0.014482802 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6            V7
25% 0.3136786 0.1474077 0.2168297 0.3812561 -0.04091863 -0.0029622571
50% 0.3155312 0.1487981 0.2196970 0.3841102 -0.03926212 -0.0012588897
75% 0.3169257 0.1498473 0.2232361 0.3862399 -0.03763451  0.0004341239
            V8         V9         V10
25% 0.03447235 0.02896608 -0.01645962
50% 0.03633051 0.03056372 -0.01434831
75% 0.03816742 0.03184205 -0.01241495

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.146904971  0.129518377  0.445517849  0.245771244  0.001640319  0.001894223 
          V8           V9          V10 
-0.071728147  0.055111970  0.109972786 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7          V8
25% 0.1463708 0.1289605 0.4437861 0.2442987 0.001235627 0.001487226 -0.07224317
50% 0.1472914 0.1299237 0.4469688 0.2467250 0.001944435 0.002221832 -0.07135856
75% 0.1478260 0.1304090 0.4487591 0.2482656 0.002317607 0.002574403 -0.07096707
            V9       V10
25% 0.05470119 0.1094722
50% 0.05552539 0.1103259
75% 0.05594279 0.1108609

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.2242267245  0.1286677609  0.1128487385  0.4600093199  0.0006709014 
           V7            V8            V9           V10 
-0.0199299721 -0.0751218230  0.0934288834  0.0795591446 

 Quartiles of Marginal Effects:
 
           V2         V3        V4        V5          V6           V7
25% 0.1981074 0.09551149 0.0387823 0.3785322 -0.02925914 -0.036646127
50% 0.2345391 0.13958223 0.1048220 0.4931117 -0.01007823 -0.016762498
75% 0.2513722 0.16720969 0.1722253 0.5607466  0.02287553 -0.001492225
             V8         V9        V10
25% -0.13931673 0.06741063 0.05906669
50% -0.10388328 0.09152529 0.08622805
75% -0.02505857 0.12317661 0.09780619

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.127869126  0.110000449  0.441030420  0.105945197  0.008603773  0.010140333 
          V8           V9          V10 
-0.056177921  0.074549052  0.100791854 

 Quartiles of Marginal Effects:
 
           V2        V3        V4          V5           V6           V7
25% 0.1231678 0.1041417 0.3589574 -0.01041974 -0.011014331 -0.008831490
50% 0.1394899 0.1152156 0.4935419  0.13640123  0.008773705  0.009420331
75% 0.1478614 0.1263144 0.5789217  0.21888057  0.029738380  0.033581707
             V8         V9        V10
25% -0.08321759 0.04033192 0.08687434
50% -0.06468791 0.07421571 0.11006501
75% -0.03760022 0.10073562 0.12750548

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.20687132  0.16763798  0.19189736  0.34714271 -0.01124743 -0.02244831 
         V8          V9         V10 
-0.04405603  0.07563066  0.06562423 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6          V7
25% 0.1945953 0.1451355 0.1357436 0.3089777 -0.018342156 -0.03096506
50% 0.2084246 0.1659126 0.1898627 0.3524806 -0.012125349 -0.02353855
75% 0.2185788 0.1858198 0.2496613 0.3871403 -0.005054948 -0.01467981
             V8         V9        V10
25% -0.06121513 0.04778642 0.03708212
50% -0.04226982 0.07428356 0.06373546
75% -0.02438569 0.10066737 0.09554499

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.21852542  0.16664277  0.16811729  0.36614888 -0.01263475 -0.02491575 
         V8          V9         V10 
-0.04035953  0.07743976  0.05868250 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6          V7
25% 0.2062252 0.1455831 0.1149767 0.3328796 -0.019320649 -0.03297393
50% 0.2203739 0.1655029 0.1726761 0.3703746 -0.013304242 -0.02598150
75% 0.2313788 0.1834207 0.2219326 0.3967517 -0.006854946 -0.01788792
             V8         V9        V10
25% -0.05741003 0.05414328 0.03076220
50% -0.03899936 0.07739885 0.05628864
75% -0.02142202 0.09933632 0.08721418

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.140284915  0.122490937  0.461409472  0.128915613  0.006968720  0.007402484 
          V8           V9          V10 
-0.057282917  0.072784639  0.102992681 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6           V7
25% 0.1343550 0.1138428 0.3912997 0.06732741 -0.008017681 -0.007445688
50% 0.1441993 0.1225371 0.4824919 0.15632674  0.005879426  0.006559495
75% 0.1514581 0.1342567 0.5631544 0.21430819  0.021819688  0.022130895
             V8         V9        V10
25% -0.07353002 0.03624530 0.08564516
50% -0.05946361 0.06923958 0.10652660
75% -0.04914286 0.09883257 0.12475717

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.223631602  0.063663898  0.155331980  0.425207526  0.026197124  0.008728851 
          V8           V9          V10 
-0.142969895  0.161520914  0.079810188 

 Quartiles of Marginal Effects:
 
           V2          V3          V4        V5           V6           V7
25% 0.1444479 -0.00110994 -0.01311906 0.2097048 -0.036011750 -0.062918297
50% 0.1851724  0.06461051  0.15406621 0.4085314  0.004765572 -0.002102599
75% 0.2764051  0.12538582  0.29236786 0.6351224  0.096034874  0.087408674
              V8         V9        V10
25% -0.319720326 0.06878466 0.01528900
50% -0.164336385 0.13416401 0.09006317
75% -0.001337753 0.24832725 0.14138497

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.147712353  0.120794761  0.350086087  0.204989380  0.005666829  0.007220121 
          V8           V9          V10 
-0.101277825  0.111062170  0.101999239 

 Quartiles of Marginal Effects:
 
           V2         V3        V4         V5          V6           V7
25% 0.1257208 0.09685489 0.2401591 0.05977307 -0.01794181 -0.019911229
50% 0.1474823 0.12360781 0.3592048 0.22545186  0.00120170 -0.003024777
75% 0.1766257 0.14473992 0.4843425 0.36429858  0.02790979  0.030159843
             V8         V9        V10
25% -0.15896035 0.05563243 0.07668965
50% -0.09741726 0.10870340 0.10536721
75% -0.04400384 0.16509947 0.13721898

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.129949223  0.110826817  0.448666421  0.175144871  0.009617814  0.010700187 
          V8           V9          V10 
-0.055482185  0.066297006  0.105482733 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6         V7          V8
25% 0.1298159 0.1107219 0.4482709 0.1748048 0.009531214 0.01061298 -0.05557957
50% 0.1300483 0.1109201 0.4490944 0.1754233 0.009683881 0.01076675 -0.05544155
75% 0.1301584 0.1110256 0.4495312 0.1757352 0.009741774 0.01082639 -0.05535232
            V9       V10
25% 0.06618652 0.1053625
50% 0.06641038 0.1055783
75% 0.06651795 0.1056860

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.148216232  0.131488267  0.425789480  0.158632303  0.005195680  0.004415435 
          V8           V9          V10 
-0.064019486  0.077166408  0.104861360 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5            V6            V7
25% 0.1407226 0.1178028 0.3527895 0.09027649 -0.0118298389 -0.0104565918
50% 0.1490887 0.1260179 0.4279198 0.19126933 -0.0002112041  0.0008585585
75% 0.1568764 0.1432591 0.5081674 0.24403535  0.0212689397  0.0182446803
             V8         V9       V10
25% -0.08108120 0.02909600 0.0870523
50% -0.06028115 0.07422029 0.1114472
75% -0.04461774 0.10638994 0.1288674

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.170821951  0.155346368  0.365612521  0.253158206 -0.008114056 -0.007809017 
          V8           V9          V10 
-0.031847339  0.048719373  0.084077472 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1665576 0.1512864 0.3167949 0.2200298 -0.014583694 -0.014729494
50% 0.1728471 0.1567779 0.3659813 0.2497609 -0.008876002 -0.008880644
75% 0.1769454 0.1613492 0.4184806 0.2833442 -0.002889036 -0.001571979
             V8         V9        V10
25% -0.03735067 0.04455701 0.07535528
50% -0.03288759 0.04829085 0.08431377
75% -0.02800744 0.05249688 0.09424208

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.1559473272  0.1412207166  0.4117755090  0.1801292061  0.0020867280 
           V7            V8            V9           V10 
-0.0007826502 -0.0566762724  0.0722288106  0.0968218663 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5            V6           V7
25% 0.1500057 0.1268980 0.3454329 0.1355574 -0.0086559294 -0.011316371
50% 0.1563135 0.1365880 0.4148545 0.1970116  0.0006661872 -0.002007295
75% 0.1626801 0.1552711 0.4931369 0.2469836  0.0122507534  0.008510013
             V8         V9        V10
25% -0.06900901 0.03210634 0.07273688
50% -0.05425308 0.06709510 0.10184336
75% -0.04229297 0.10178316 0.12431794

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.2739537041  0.1916746978  0.2337186401  0.2929768730  0.0090246214 
           V7            V8            V9           V10 
-0.0419810517 -0.0005174852  0.0710399010 -0.0313018797 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7
25% 0.2728313 0.1874073 0.2028068 0.2461042 0.006232665 -0.04647993
50% 0.2745327 0.1921270 0.2326487 0.2907338 0.008960929 -0.04252227
75% 0.2755293 0.1966403 0.2655321 0.3388590 0.011837309 -0.03760843
               V8         V9         V10
25% -0.0049492711 0.06623594 -0.04065210
50% -0.0009728017 0.07091360 -0.03232735
75%  0.0038262545 0.07553599 -0.02239946

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.127044076  0.108477568  0.446476347  0.120477667  0.009224378  0.010525425 
          V8           V9          V10 
-0.051342736  0.065541804  0.097121430 

 Quartiles of Marginal Effects:
 
           V2         V3        V4         V5           V6           V7
25% 0.1156756 0.09950789 0.3611798 0.02537994 -0.007713023 -0.007011946
50% 0.1383897 0.11742319 0.4760722 0.14186583  0.009895444  0.011901217
75% 0.1498140 0.12778882 0.5662335 0.21533219  0.029528340  0.029773423
             V8         V9        V10
25% -0.07464234 0.04532647 0.08385242
50% -0.06114371 0.06508969 0.10234945
75% -0.03935518 0.08782768 0.11914446

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.282900982  0.162473619  0.239428017  0.351223023 -0.036853882 -0.006693389 
          V8           V9          V10 
 0.028896596  0.036217187  0.001949468 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6           V7         V8
25% 0.2813321 0.1618489 0.2314251 0.3447832 -0.03941586 -0.009363641 0.02701044
50% 0.2836018 0.1627297 0.2390761 0.3532633 -0.03683345 -0.006606423 0.02938746
75% 0.2850190 0.1634038 0.2463653 0.3590061 -0.03396102 -0.003562919 0.03116708
            V9           V10
25% 0.03526758 -0.0007630063
50% 0.03651220  0.0020490864
75% 0.03738417  0.0046898910

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.2003766011  0.0889364623  0.1827267585  0.3593140930  0.0088577108 
           V7            V8            V9           V10 
 0.0003198672 -0.1209269306  0.1515265032  0.0954962857 

 Quartiles of Marginal Effects:
 
           V2         V3         V4        V5           V6          V7
25% 0.1534766 0.05822473 0.03617351 0.1634309 -0.029861441 -0.04571398
50% 0.1838344 0.09326729 0.18082036 0.3639600 -0.004113388 -0.01195204
75% 0.2323133 0.13007467 0.30114002 0.5359479  0.050232852  0.04099592
             V8         V9        V10
25% -0.23352821 0.08423875 0.06142718
50% -0.14424547 0.14406637 0.10100522
75% -0.02715858 0.21880972 0.13737650

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 1.554363e-01  1.406562e-01  4.217618e-01  2.035412e-01  5.027537e-04 
           V7            V8            V9           V10 
-8.357127e-05 -4.466227e-02  5.353876e-02  9.316367e-02 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5            V6            V7
25% 0.1497132 0.1356439 0.3576497 0.1578539 -7.388686e-03 -9.033143e-03
50% 0.1578299 0.1434606 0.4272303 0.2033503  5.449501e-05 -1.712325e-05
75% 0.1641844 0.1492084 0.4954811 0.2505787  8.359928e-03  8.275046e-03
             V8         V9        V10
25% -0.05286670 0.04400991 0.08147686
50% -0.04749498 0.05139653 0.09344630
75% -0.03837827 0.06031797 0.10535495

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.152535399  0.135728443  0.440110034  0.182666648  0.004320274  0.003235540 
          V8           V9          V10 
-0.049384039  0.055492395  0.094974845 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1466330 0.1296743 0.3671131 0.1304838 -0.005281502 -0.006989787
50% 0.1552562 0.1380995 0.4464852 0.1859685  0.003751965  0.003318205
75% 0.1610778 0.1457746 0.5252115 0.2395874  0.013763722  0.013314883
             V8         V9        V10
25% -0.05841549 0.03729188 0.07949796
50% -0.05211522 0.05116282 0.09571710
75% -0.04349976 0.06967925 0.11078067

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.148966913  0.132222092  0.420470449  0.162606642  0.004979760  0.004006754 
          V8           V9          V10 
-0.064555623  0.077497474  0.104779200 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5            V6            V7
25% 0.1416449 0.1181656 0.3470457 0.09510234 -0.0122110256 -0.0108761775
50% 0.1499002 0.1264616 0.4223536 0.19692728 -0.0003725526  0.0002161398
75% 0.1577094 0.1440057 0.4966001 0.24845114  0.0208688068  0.0177056274
             V8         V9       V10
25% -0.08151843 0.02846956 0.0870622
50% -0.06103315 0.07411952 0.1113779
75% -0.04440236 0.10714144 0.1287285

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.309204319  0.152497954  0.229509548  0.369522828 -0.039134858 -0.001558017 
          V8           V9          V10 
 0.034251073  0.034294382 -0.014618173 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6            V7
25% 0.3078442 0.1513419 0.2269258 0.3666206 -0.04076843 -0.0032337561
50% 0.3093836 0.1526205 0.2293756 0.3705619 -0.03910663 -0.0014900445
75% 0.3107883 0.1538330 0.2322273 0.3727557 -0.03747857  0.0001761466
            V8         V9         V10
25% 0.03248063 0.03271424 -0.01652824
50% 0.03434643 0.03438365 -0.01471547
75% 0.03617477 0.03599236 -0.01269819

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.150035621  0.129729995  0.413615094  0.266871316  0.002065910  0.002288358 
          V8           V9          V10 
-0.074466065  0.050518939  0.112749960 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7          V8
25% 0.1494810 0.1291840 0.4119367 0.2652356 0.001648912 0.001880639 -0.07495122
50% 0.1504203 0.1301263 0.4149293 0.2678041 0.002414841 0.002629664 -0.07413737
75% 0.1509874 0.1305964 0.4168177 0.2693278 0.002743496 0.002979686 -0.07376358
            V9       V10
25% 0.05010771 0.1122152
50% 0.05090336 0.1131012
75% 0.05129144 0.1136255

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.225468459  0.123517989  0.138616061  0.458449236 -0.008001925  0.019445811 
          V8           V9          V10 
-0.108563184  0.105537117  0.054193255 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6         V7         V8
25% 0.1478915 0.1047685 0.0731030 0.3830335 -0.053132300 0.01203073 -0.1869020
50% 0.2401430 0.1286377 0.1274736 0.4830393 -0.005262406 0.01903271 -0.1363616
75% 0.2932585 0.1480824 0.1933933 0.5442771  0.027955303 0.02499948 -0.0125438
            V9         V10
25% 0.08133598 0.005362752
50% 0.10053984 0.050945575
75% 0.12233742 0.114124210

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.13027685  0.11120773  0.42264595  0.10855276  0.01071785  0.01143021 
         V8          V9         V10 
-0.05849604  0.07192666  0.10170895 

 Quartiles of Marginal Effects:
 
           V2        V3        V4          V5           V6           V7
25% 0.1256779 0.1052401 0.3599107 0.006731635 -0.009784886 -0.008928592
50% 0.1400822 0.1172420 0.4660677 0.145268282  0.008889654  0.010539254
75% 0.1514118 0.1289458 0.5666508 0.230602237  0.033133312  0.033621754
             V8         V9        V10
25% -0.08544153 0.04078558 0.08559794
50% -0.06567936 0.07172504 0.11234318
75% -0.04301214 0.09904898 0.12782993

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.20897830  0.16675881  0.21822972  0.31781592 -0.01065588 -0.01221804 
         V8          V9         V10 
-0.05516740  0.08741731  0.05004051 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1942813 0.1464585 0.1688277 0.2794982 -0.017046457 -0.021072012
50% 0.2115812 0.1655466 0.2190269 0.3202953 -0.011413804 -0.013124095
75% 0.2251364 0.1836644 0.2714163 0.3506859 -0.005051257 -0.004617906
             V8         V9        V10
25% -0.06798667 0.06924496 0.01474470
50% -0.05795640 0.08369823 0.04905765
75% -0.04114702 0.10262204 0.08390097

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.22064927  0.16659599  0.19832310  0.33539787 -0.01358362 -0.01156818 
         V8          V9         V10 
-0.05277749  0.08905260  0.04086909 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.2051927 0.1483322 0.1488642 0.2995681 -0.019708702 -0.020204434
50% 0.2240096 0.1667630 0.2005520 0.3347748 -0.013804769 -0.012770495
75% 0.2385292 0.1821195 0.2476941 0.3612220 -0.008120581 -0.004403724
             V8         V9         V10
25% -0.06556128 0.07549583 0.004814828
50% -0.05519726 0.08684412 0.038604063
75% -0.03849978 0.10160681 0.075242407

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.141758461  0.122133670  0.441350121  0.134966577  0.007947057  0.008553072 
          V8           V9          V10 
-0.057939647  0.073102752  0.103973247 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6           V7
25% 0.1346531 0.1128581 0.3630810 0.05903015 -0.007422156 -0.006236868
50% 0.1464324 0.1229685 0.4656072 0.16410405  0.006053829  0.006943712
75% 0.1530337 0.1357553 0.5584118 0.23079979  0.022466600  0.022323135
             V8         V9        V10
25% -0.07274327 0.04341625 0.08286457
50% -0.06181210 0.06831389 0.10759263
75% -0.04989166 0.09610833 0.12408016

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.206305192  0.120753884  0.145067667  0.464671390  0.008842549  0.033367546 
          V8           V9          V10 
-0.143504863  0.150967460  0.034728595 

 Quartiles of Marginal Effects:
 
           V2         V3           V4        V5          V6           V7
25% 0.1078721 0.06200925 -0.008361881 0.2449427 -0.09421947 -0.006091403
50% 0.1919705 0.12634782  0.155086239 0.4716071 -0.02384242  0.034541390
75% 0.2787085 0.19865367  0.321299967 0.7076213  0.09120384  0.082778578
             V8         V9         V10
25% -0.31136526 0.05665882 -0.02214871
50% -0.11631776 0.16525879  0.04622821
75%  0.04156804 0.25981308  0.10113404

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.1562906048  0.1295866680  0.2353009072  0.2808427028 -0.0014144622 
           V7            V8            V9           V10 
-0.0001656756 -0.0910884573  0.1103348790  0.1068491054 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1229799 0.1076449 0.1184162 0.1425529 -0.033123995 -0.021498262
50% 0.1599239 0.1324416 0.2461253 0.3080321 -0.009898207 -0.002530732
75% 0.1875213 0.1550416 0.3783676 0.4402262  0.024760339  0.025960609
             V8         V9        V10
25% -0.16087630 0.05579257 0.08397974
50% -0.07711180 0.10609713 0.10643654
75% -0.01768162 0.16135419 0.13508357

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.13243240  0.11184911  0.41982649  0.18857658  0.01042449  0.01137443 
         V8          V9         V10 
-0.05657793  0.06213832  0.10735784 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5         V6         V7          V8
25% 0.1322984 0.1117319 0.4194436 0.1881819 0.01033923 0.01129220 -0.05667490
50% 0.1325420 0.1119502 0.4202645 0.1888667 0.01048760 0.01143705 -0.05653830
75% 0.1326488 0.1120477 0.4206691 0.1891722 0.01054772 0.01149894 -0.05645247
            V9       V10
25% 0.06201797 0.1072452
50% 0.06224632 0.1074600
75% 0.06234894 0.1075614

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.150214086  0.131279759  0.405562089  0.160138377  0.006395201  0.005303390 
          V8           V9          V10 
-0.068301932  0.080997034  0.101871151 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6           V7
25% 0.1401793 0.1157705 0.3392228 0.09020781 -0.009429606 -0.009727383
50% 0.1505004 0.1265538 0.4111152 0.19875391  0.002686170  0.002617158
75% 0.1607929 0.1464692 0.4982178 0.25226008  0.021709646  0.019315814
             V8         V9        V10
25% -0.08600486 0.04257021 0.07592138
50% -0.06753640 0.07549629 0.10962733
75% -0.05124908 0.10621755 0.12769284

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.174222721  0.151769138  0.344078830  0.271119217 -0.008557879 -0.008254861 
          V8           V9          V10 
-0.033411643  0.049494834  0.087414005 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1702289 0.1480677 0.2943178 0.2416100 -0.014369779 -0.014918084
50% 0.1762192 0.1533528 0.3464357 0.2678016 -0.009269364 -0.009404461
75% 0.1797721 0.1570504 0.4037156 0.2992866 -0.003847341 -0.002168819
             V8         V9        V10
25% -0.03899056 0.04530348 0.07845613
50% -0.03405688 0.04934871 0.08771156
75% -0.02954510 0.05380776 0.09733564

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.157225974  0.139708877  0.404383690  0.180702195  0.002939856  0.000573748 
          V8           V9          V10 
-0.059088302  0.074742961  0.095650635 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1502757 0.1263711 0.3410689 0.1338786 -0.007155751 -0.010191776
50% 0.1577306 0.1358710 0.4117441 0.1995664  0.001263387 -0.001013154
75% 0.1646703 0.1539489 0.5002717 0.2491489  0.012599618  0.009734842
             V8         V9        V10
25% -0.07121405 0.04175298 0.06827372
50% -0.05744727 0.06486080 0.10125001
75% -0.04794535 0.09929355 0.12251967

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.275751693  0.194269377  0.254420294  0.266236865  0.013197009 -0.046318431 
          V8           V9          V10 
 0.007308508  0.071909700 -0.037665439 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5         V6          V7          V8
25% 0.2743571 0.1897470 0.2182564 0.2154872 0.01110232 -0.05099018 0.002828567
50% 0.2762493 0.1943595 0.2519608 0.2633728 0.01297199 -0.04682464 0.006637370
75% 0.2778010 0.1992037 0.2991665 0.3179270 0.01492236 -0.04193501 0.011457084
            V9         V10
25% 0.06645008 -0.04846686
50% 0.07233629 -0.03918307
75% 0.07779967 -0.02754963

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.129631908  0.108861801  0.424998098  0.127245785  0.009671248  0.011248859 
          V8           V9          V10 
-0.051247179  0.066425149  0.100015749 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6           V7
25% 0.1209489 0.1034837 0.3345605 0.01872964 -0.007755181 -0.006801925
50% 0.1400695 0.1168147 0.4677494 0.14582707  0.009173493  0.011139925
75% 0.1508583 0.1274472 0.5484354 0.23119281  0.029575772  0.030021803
             V8         V9        V10
25% -0.07569424 0.04983167 0.08949511
50% -0.06003817 0.06776764 0.10517852
75% -0.03736503 0.08553079 0.12220330

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.285594184  0.156698485  0.218680749  0.371251491 -0.037378219 -0.005922980 
          V8           V9          V10 
 0.024309858  0.038602936  0.006135426 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6           V7         V8
25% 0.2838211 0.1560378 0.2098489 0.3650989 -0.04013271 -0.008861009 0.02247638
50% 0.2862785 0.1568993 0.2178763 0.3729623 -0.03713994 -0.005697116 0.02478619
75% 0.2882258 0.1574835 0.2274708 0.3792906 -0.03442358 -0.002703825 0.02646784
            V9         V10
25% 0.03753715 0.003338630
50% 0.03855933 0.006267239
75% 0.03974686 0.009264869

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.194174214  0.128691796  0.145541540  0.413414057  0.005542158  0.010227099 
          V8           V9          V10 
-0.128623716  0.138994398  0.064560044 

 Quartiles of Marginal Effects:
 
           V2         V3         V4        V5          V6           V7
25% 0.1262178 0.09962423 0.01972463 0.2503190 -0.05572195 -0.009101393
50% 0.1960559 0.13559198 0.15226901 0.4393309 -0.01194064  0.012478836
75% 0.2533643 0.16775406 0.27204125 0.5819187  0.05209410  0.039588978
              V8         V9        V10
25% -0.238584822 0.06916568 0.01562387
50% -0.114653559 0.15753288 0.07389117
75%  0.006685301 0.21606622 0.11645060

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 1.582180e-01  1.379057e-01  3.982595e-01  2.207537e-01  4.883583e-05 
           V7            V8            V9           V10 
-1.618765e-04 -4.671594e-02  5.412829e-02  9.677590e-02 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6            V7
25% 0.1523786 0.1335487 0.3325583 0.1770346 -0.007461412 -0.0086954753
50% 0.1607479 0.1405591 0.4057781 0.2201160 -0.001085871 -0.0002844127
75% 0.1662482 0.1455482 0.4784340 0.2669246  0.007080670  0.0073605600
             V8         V9        V10
25% -0.05517097 0.04441880 0.08465312
50% -0.04831372 0.05212062 0.09682106
75% -0.04027069 0.06023820 0.10761294

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.154722459  0.133394584  0.420133882  0.195051001  0.004013536  0.003499110 
          V8           V9          V10 
-0.051310735  0.056413435  0.098123958 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1491109 0.1278097 0.3468038 0.1451070 -0.005270988 -0.006675960
50% 0.1573752 0.1355978 0.4314410 0.1977461  0.002697187  0.003074547
75% 0.1629166 0.1430391 0.5136577 0.2540681  0.012622983  0.012788744
             V8         V9        V10
25% -0.06009440 0.04036854 0.08210841
50% -0.05377070 0.04990693 0.09886458
75% -0.04562797 0.06948024 0.11154854

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.151183229  0.131996717  0.399182074  0.164796418  0.006141861  0.004910427 
          V8           V9          V10 
-0.069239001  0.081803942  0.101470346 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6           V7
25% 0.1405678 0.1161868 0.3318704 0.09435936 -0.009914394 -0.010111563
50% 0.1513268 0.1270182 0.4051114 0.20325594  0.002544918  0.002181321
75% 0.1622595 0.1473274 0.4892681 0.25542698  0.021403104  0.018575331
             V8         V9        V10
25% -0.08722313 0.04244454 0.07528507
50% -0.06832190 0.07632985 0.11006766
75% -0.05139676 0.10749335 0.12771924

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.31146096  0.14604473  0.20510848  0.39311298 -0.04013595  0.00113609 
         V8          V9         V10 
 0.02524122  0.03887464 -0.01048013 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6            V7
25% 0.3099967 0.1448546 0.2017795 0.3904095 -0.04190728 -0.0007048497
50% 0.3116171 0.1461280 0.2047647 0.3939543 -0.04010835  0.0012096810
75% 0.3132842 0.1474204 0.2083998 0.3964248 -0.03837032  0.0030074469
            V8         V9          V10
25% 0.02350977 0.03766304 -0.012485439
50% 0.02539958 0.03906965 -0.010612817
75% 0.02705075 0.04023652 -0.008334404

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.23159957  0.16092076  0.14849700  0.39414376 -0.01128822 -0.01687246 
         V8          V9         V10 
-0.05367531  0.07929290  0.04634701 

 Quartiles of Marginal Effects:
 
           V2        V3         V4        V5           V6           V7
25% 0.2157010 0.1377839 0.09896298 0.3563159 -0.017916781 -0.025685400
50% 0.2352249 0.1596450 0.15334839 0.3947420 -0.011931010 -0.018129917
75% 0.2494600 0.1801272 0.19688987 0.4250358 -0.004928726 -0.009126266
             V8         V9        V10
25% -0.06896521 0.06246354 0.01184855
50% -0.05493921 0.07757177 0.04368818
75% -0.03710958 0.09488354 0.08026090
Radial Basis Function Kernel Regularized Least Squares 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  lambda        sigma        RMSE         Rsquared   MAE          Selected
  1.273966e-05   858.915406  0.003276359  0.9999742  0.002568756          
  2.140217e-05     3.896353  0.006604391  0.9998973  0.004121757          
  3.800331e-05  7169.020009  0.005294833  0.9999323  0.003890277          
  3.864499e-05    15.418812  0.003233496  0.9999751  0.002571544          
  8.086287e-05    61.534133  0.002964908  0.9999790  0.002381103  *       
  8.379028e-05     3.553604  0.006942946  0.9998860  0.004097582          
  1.151155e-04  2239.233405  0.005205351  0.9999344  0.003795969          
  1.677762e-04    46.841312  0.003057846  0.9999777  0.002454568          
  3.055212e-04     1.318231  0.026691354  0.9982864  0.010745672          
  7.674838e-03     5.839987  0.008573953  0.9998242  0.005000681          
  8.527699e-03    15.573003  0.006422249  0.9999007  0.004289973          
  8.812371e-03     5.799272  0.008945544  0.9998087  0.005183569          
  1.355387e-02    93.452085  0.009665451  0.9997768  0.007071503          
  4.152354e-02  1760.966885  0.028349615  0.9984247  0.020078045          
  4.707283e-02    42.698450  0.014129523  0.9995254  0.009994737          
  4.847573e-02  6624.862813  0.065531906  0.9944681  0.052776086          
  5.324651e-02    28.422897  0.013501284  0.9995683  0.009160197          
  7.858537e-02     6.867362  0.017666206  0.9992671  0.010020774          
  1.226160e-01     1.800418  0.040253949  0.9963574  0.018360339          
  6.635549e-01     6.109386  0.044852208  0.9956640  0.024114538          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 8.086287e-05 and sigma
 = 61.53413.
[1] "Sun Mar 11 19:40:11 2018"
Loaded lars 1.2

Least Angle Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  fraction    RMSE         Rsquared   MAE          Selected
  0.02103156  0.616646054  0.9723675  0.508569762          
  0.06609158  0.555909750  0.9723675  0.457841749          
  0.11596427  0.488991619  0.9723675  0.402081133          
  0.11741865  0.487046264  0.9723675  0.400464263          
  0.18154983  0.401741673  0.9723675  0.329592613          
  0.18463873  0.397661920  0.9723675  0.326207456          
  0.21222674  0.361383877  0.9723675  0.296249589          
  0.24494607  0.318871435  0.9731567  0.261186072          
  0.29700826  0.252239813  0.9833061  0.206922433          
  0.57701384  0.009306640  0.9997906  0.007225522          
  0.58616638  0.008972305  0.9998059  0.007005626          
  0.58901856  0.008888303  0.9998095  0.006944332          
  0.62641267  0.008325771  0.9998331  0.006502035          
  0.72365888  0.007878587  0.9998508  0.006105242          
  0.73455407  0.007858552  0.9998516  0.006082309          
  0.73710487  0.007854384  0.9998518  0.006078102          
  0.74525822  0.007842402  0.9998523  0.006065699          
  0.77906834  0.007820292  0.9998531  0.006030632          
  0.81770940  0.007798628  0.9998540  0.006005574          
  0.96437537  0.007775180  0.9998551  0.005968663  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was fraction = 0.9643754.
[1] "Sun Mar 11 19:40:18 2018"
Least Angle Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  step  RMSE         Rsquared   MAE          Selected
  1     0.645062301        NaN  0.532301755          
  2     0.313020461  0.9723675  0.256381119          
  3     0.201296764  0.9889077  0.165565951          
  6     0.011977237  0.9996640  0.009153844          
  7     0.011236422  0.9996986  0.008607032          
  8     0.010758637  0.9997206  0.008282846          
  9     0.009261266  0.9997945  0.007225772  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was step = 9.
[1] "Sun Mar 11 19:40:24 2018"
The lasso 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  fraction    RMSE         Rsquared   MAE          Selected
  0.02103156  0.616646054  0.9723675  0.508569762          
  0.06609158  0.555909750  0.9723675  0.457841749          
  0.11596427  0.488991619  0.9723675  0.402081133          
  0.11741865  0.487046264  0.9723675  0.400464263          
  0.18154983  0.401741673  0.9723675  0.329592613          
  0.18463873  0.397661920  0.9723675  0.326207456          
  0.21222674  0.361383877  0.9723675  0.296249589          
  0.24494607  0.318871435  0.9731567  0.261186072          
  0.29700826  0.252239813  0.9833061  0.206922433          
  0.57701384  0.009306640  0.9997906  0.007225522          
  0.58616638  0.008972305  0.9998059  0.007005626          
  0.58901856  0.008888303  0.9998095  0.006944332          
  0.62641267  0.008325771  0.9998331  0.006502035          
  0.72365888  0.007878587  0.9998508  0.006105242          
  0.73455407  0.007858552  0.9998516  0.006082309          
  0.73710487  0.007854384  0.9998518  0.006078102          
  0.74525822  0.007842402  0.9998523  0.006065699          
  0.77906834  0.007820292  0.9998531  0.006030632          
  0.81770940  0.007798628  0.9998540  0.006005574          
  0.96437537  0.007775180  0.9998551  0.005968663  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was fraction = 0.9643754.
[1] "Sun Mar 11 19:40:32 2018"
Linear Regression with Backwards Selection 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  nvmax  RMSE         Rsquared   MAE          Selected
  2      0.019199455  0.9991206  0.014990047          
  3      0.013609103  0.9995645  0.010837060          
  4      0.011774994  0.9996708  0.009224249          
  6      0.008690782  0.9998194  0.006741608          
  7      0.007827115  0.9998528  0.006059662          
  8      0.007764027  0.9998555  0.005996707  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nvmax = 8.
[1] "Sun Mar 11 19:40:38 2018"
Linear Regression with Forward Selection 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  nvmax  RMSE         Rsquared   MAE          Selected
  2      0.050843607  0.9938826  0.040034169          
  3      0.026410930  0.9983642  0.019698567          
  4      0.011774994  0.9996708  0.009224249          
  6      0.008690782  0.9998194  0.006741608          
  7      0.007827115  0.9998528  0.006059662          
  8      0.007764027  0.9998555  0.005996707  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nvmax = 8.
[1] "Sun Mar 11 19:40:45 2018"
Linear Regression with Stepwise Selection 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  nvmax  RMSE         Rsquared   MAE          Selected
  2      0.019199455  0.9991206  0.014990047          
  3      0.013609103  0.9995645  0.010837060          
  4      0.011774994  0.9996708  0.009224249          
  6      0.008690782  0.9998194  0.006741608          
  7      0.007827115  0.9998528  0.006059662  *       
  8      0.008304738  0.9998352  0.006249553          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nvmax = 7.
[1] "Sun Mar 11 19:40:51 2018"
Linear Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results:

  RMSE        Rsquared   MAE        
  0.00778101  0.9998549  0.005966806

Tuning parameter 'intercept' was held constant at a value of TRUE
[1] "Sun Mar 11 19:40:56 2018"
Start:  AIC=-4868.25
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq      RSS     AIC
<none>              0.029002 -4868.3
- V9    1  0.000225 0.029227 -4866.4
- V7    1  0.001385 0.030387 -4846.9
- V6    1  0.002055 0.031057 -4836.0
- V10   1  0.004425 0.033427 -4799.1
- V8    1  0.005631 0.034633 -4781.4
- V3    1  0.026534 0.055536 -4544.8
- V4    1  0.030092 0.059094 -4513.7
- V5    1  0.088072 0.117074 -4171.1
- V2    1  0.106481 0.135483 -4098.0
Start:  AIC=-4899.61
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq      RSS     AIC
<none>              0.026660 -4899.6
- V9    1  0.000224 0.026884 -4897.4
- V7    1  0.000669 0.027328 -4889.2
- V6    1  0.001199 0.027858 -4879.6
- V10   1  0.002948 0.029608 -4849.2
- V8    1  0.005786 0.032446 -4803.4
- V3    1  0.024721 0.051381 -4573.6
- V4    1  0.026715 0.053374 -4554.5
- V5    1  0.076241 0.102901 -4226.3
- V2    1  0.101974 0.128634 -4114.7
Start:  AIC=-4866.58
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq      RSS     AIC
- V9    1  0.000006 0.030377 -4868.5
<none>              0.030371 -4866.6
- V7    1  0.001756 0.032127 -4840.3
- V6    1  0.002421 0.032791 -4830.0
- V10   1  0.003424 0.033795 -4814.8
- V8    1  0.003542 0.033912 -4813.1
- V3    1  0.027468 0.057839 -4544.6
- V4    1  0.029493 0.059864 -4527.2
- V2    1  0.096959 0.127330 -4147.6
- V5    1  0.097926 0.128296 -4143.8

Step:  AIC=-4868.47
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V10

       Df Sum of Sq      RSS     AIC
<none>              0.030377 -4868.5
- V7    1  0.001831 0.032208 -4841.0
- V6    1  0.002656 0.033033 -4828.3
- V10   1  0.005890 0.036267 -4781.3
- V8    1  0.008012 0.038389 -4752.7
- V3    1  0.034532 0.064909 -4488.5
- V4    1  0.076831 0.107208 -4236.1
- V5    1  0.098179 0.128556 -4144.8
- V2    1  0.148377 0.178754 -3979.0
Start:  AIC=-7318.55
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq      RSS     AIC
<none>              0.043457 -7318.6
- V9    1  0.000152 0.043610 -7317.9
- V7    1  0.001759 0.045216 -7290.7
- V6    1  0.002679 0.046136 -7275.6
- V10   1  0.005559 0.049016 -7230.0
- V8    1  0.007421 0.050878 -7202.0
- V3    1  0.039231 0.082689 -6836.8
- V4    1  0.043252 0.086709 -6801.1
- V5    1  0.133312 0.176770 -6265.4
- V2    1  0.155578 0.199035 -6176.2
Linear Regression with Stepwise Selection 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results:

  RMSE         Rsquared   MAE       
  0.007785598  0.9998547  0.00597113

[1] "Sun Mar 11 19:41:02 2018"
Loading required package: LogicReg
Loading required package: survival

Attaching package: 'survival'

The following object is masked from 'package:caret':

    cluster

Loading required package: mcbiopi
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :19    NA's   :19    NA's   :19   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :7     NA's   :7     NA's   :7    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Sun Mar 11 19:41:13 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "logicBag"                       
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Sun Mar 11 19:41:24 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "logreg"                         
Mar 11, 2018 7:41:33 PM com.github.fommil.netlib.BLAS <clinit>
WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
Mar 11, 2018 7:41:34 PM com.github.fommil.netlib.BLAS <clinit>
WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
Mar 11, 2018 7:41:34 PM com.github.fommil.netlib.LAPACK <clinit>
WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
Mar 11, 2018 7:41:34 PM com.github.fommil.netlib.LAPACK <clinit>
WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK
Model Tree 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  pruned  smoothed  rules  RMSE         Rsquared   MAE          Selected
  Yes     Yes       Yes    0.006796494  0.9998876  0.004980534          
  Yes     Yes       No     0.006754790  0.9998887  0.004966457  *       
  Yes     No        Yes    0.006791812  0.9998876  0.004979819          
  Yes     No        No     0.006791812  0.9998876  0.004979819          
  No      Yes       Yes    0.026142910  0.9983967  0.017122917          
  No      Yes       No     0.017832756  0.9992545  0.012669145          
  No      No        Yes    0.079215143  0.9852978  0.058179628          
  No      No        No     0.066767740  0.9894316  0.051588672          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were pruned = Yes, smoothed = Yes and
 rules = No.
[1] "Sun Mar 11 19:41:49 2018"
Model Rules 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  pruned  smoothed  RMSE         Rsquared   MAE          Selected
  Yes     Yes       0.006796494  0.9998876  0.004980534          
  Yes     No        0.006791812  0.9998876  0.004979819  *       
  No      Yes       0.026142910  0.9983967  0.017122917          
  No      No        0.079215143  0.9852978  0.058179628          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were pruned = Yes and smoothed = No.
[1] "Sun Mar 11 19:42:05 2018"
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                          "failed"                         
 [3] "Sun Mar 11 19:42:12 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "mlpKerasDecay"                  
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
In addition: Warning message:
In stop(e, call. = FALSE) : additional arguments ignored in stop()
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                          "failed"                         
 [3] "Sun Mar 11 19:42:26 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "mlpKerasDropout"                
Multi-Step Adaptive MCP-Net 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  alphas     nsteps  scale      RMSE        Rsquared   MAE         Selected
  0.0689284   4      1.6879696  0.05653563  0.9919742  0.04257307          
  0.1094824   9      1.4305181  0.02039556  0.9988685  0.01494732          
  0.1543678   2      1.5788699  0.03408030  0.9973566  0.02325614          
  0.1556768   8      2.8580192  0.06711287  0.9913335  0.05136106          
  0.2133948   6      1.2395089  0.02667431  0.9983337  0.01953173          
  0.2161749   9      1.4360608  0.02684548  0.9983139  0.01956171          
  0.2410041   3      1.2244157  0.02245203  0.9989324  0.01613175          
  0.2704515   7      0.5601783  0.02088047  0.9991082  0.01535667          
  0.3173074  10      2.1762773  0.01583844  0.9993841  0.01223728  *       
  0.5693125   9      0.6490389  0.01920274  0.9991205  0.01498741          
  0.5775497   8      1.3609400  0.01920463  0.9991204  0.01499093          
  0.5801167   9      2.2151496  0.01920960  0.9991201  0.01499736          
  0.6137714   6      3.0580202  0.01921537  0.9991199  0.01500552          
  0.7012930   3      1.6889670  0.01920252  0.9991204  0.01499151          
  0.7110987   7      0.8453995  0.01920080  0.9991205  0.01498900          
  0.7133944   2      2.1114538  0.01920336  0.9991204  0.01499298          
  0.7207324   7      0.7525569  0.01920069  0.9991205  0.01498881          
  0.7511615   9      3.2729335  0.01920649  0.9991202  0.01499771          
  0.7859385  10      3.0305479  0.01920390  0.9991203  0.01499513          
  0.9179378   9      2.4035113  0.01920009  0.9991205  0.01499074          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were alphas = 0.3173074, nsteps = 10
 and scale = 2.176277.
[1] "Sun Mar 11 19:42:47 2018"
Installing package into 'C:/Users/John/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Installing package into 'C:/Users/John/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
# weights:  166
initial  value 465.829501 
iter  10 value 371.163749
iter  20 value 370.545364
iter  30 value 370.537586
iter  40 value 355.819211
iter  50 value 353.112198
iter  60 value 352.843604
iter  70 value 352.799882
iter  80 value 352.779973
iter  90 value 352.770222
iter 100 value 352.763234
final  value 352.763234 
stopped after 100 iterations
# weights:  34
initial  value 852.616482 
iter  10 value 376.285101
iter  20 value 372.063225
iter  30 value 362.794446
final  value 362.700625 
converged
# weights:  188
initial  value 1191.598105 
iter  10 value 406.011774
iter  20 value 384.483113
iter  30 value 383.721313
iter  40 value 383.706795
final  value 383.706781 
converged
# weights:  56
initial  value 1103.373599 
iter  10 value 386.536335
iter  20 value 371.190117
iter  30 value 359.524162
iter  40 value 355.609954
iter  50 value 355.351246
iter  60 value 355.307173
iter  70 value 355.299517
iter  80 value 355.297863
iter  90 value 355.297222
iter 100 value 355.297023
final  value 355.297023 
stopped after 100 iterations
# weights:  45
initial  value 1015.303887 
iter  10 value 385.898872
iter  20 value 371.364706
iter  30 value 370.582515
iter  40 value 355.959009
iter  50 value 354.913434
iter  60 value 354.866538
iter  70 value 354.863737
iter  80 value 354.863384
final  value 354.863346 
converged
# weights:  177
initial  value 851.133539 
iter  10 value 377.693226
iter  20 value 373.256762
iter  30 value 373.111469
iter  30 value 373.111465
iter  30 value 373.111465
final  value 373.111465 
converged
# weights:  23
initial  value 800.061368 
iter  10 value 386.598616
final  value 385.237018 
converged
# weights:  67
initial  value 694.664259 
iter  10 value 419.110738
iter  20 value 400.038006
iter  30 value 399.628006
final  value 399.627934 
converged
# weights:  166
initial  value 686.379538 
iter  10 value 370.956384
iter  20 value 370.542685
final  value 370.539752 
converged
# weights:  133
initial  value 816.218382 
iter  10 value 382.347071
iter  20 value 374.413416
iter  30 value 374.214386
final  value 374.208822 
converged
# weights:  144
initial  value 717.337787 
iter  10 value 383.250033
iter  20 value 370.772456
iter  30 value 370.577557
iter  40 value 359.604886
iter  50 value 354.835593
iter  60 value 354.141975
iter  70 value 353.965184
iter  80 value 353.933811
iter  90 value 353.916418
iter 100 value 353.909286
final  value 353.909286 
stopped after 100 iterations
# weights:  133
initial  value 1176.028775 
iter  10 value 372.647111
iter  20 value 364.970783
iter  30 value 360.103441
iter  40 value 359.764172
iter  50 value 359.725321
iter  60 value 359.721098
final  value 359.721039 
converged
# weights:  12
initial  value 624.783763 
iter  10 value 371.392194
iter  20 value 370.601803
iter  30 value 370.591701
iter  40 value 370.581940
iter  50 value 370.568784
iter  60 value 370.568274
iter  70 value 370.567485
iter  80 value 370.566051
iter  90 value 370.562594
iter 100 value 370.547240
final  value 370.547240 
stopped after 100 iterations
# weights:  221
initial  value 427.402571 
iter  10 value 374.477110
iter  20 value 372.440863
iter  30 value 369.058226
iter  40 value 368.787538
iter  50 value 368.774295
final  value 368.773739 
converged
# weights:  56
initial  value 844.623461 
iter  10 value 371.959205
iter  20 value 370.554561
iter  30 value 370.548847
iter  40 value 370.548114
iter  50 value 370.543094
iter  60 value 368.847903
iter  70 value 355.518400
iter  80 value 353.118895
iter  90 value 352.962521
iter 100 value 352.854221
final  value 352.854221 
stopped after 100 iterations
# weights:  45
initial  value 825.960786 
iter  10 value 391.628232
iter  20 value 382.244715
final  value 382.197950 
converged
# weights:  166
initial  value 1197.421037 
iter  10 value 396.245159
iter  20 value 371.230692
iter  30 value 362.630600
iter  40 value 356.531656
iter  50 value 355.812334
iter  60 value 355.676404
iter  70 value 355.525855
iter  80 value 355.446000
iter  90 value 355.370093
iter 100 value 355.310540
final  value 355.310540 
stopped after 100 iterations
# weights:  166
initial  value 461.376618 
iter  10 value 371.173220
iter  20 value 367.421810
iter  30 value 357.950612
iter  40 value 357.044518
iter  50 value 356.635780
iter  60 value 356.491178
iter  70 value 356.461396
iter  80 value 356.457379
iter  90 value 356.455976
iter 100 value 356.455051
final  value 356.455051 
stopped after 100 iterations
# weights:  133
initial  value 837.969368 
iter  10 value 381.448021
iter  20 value 374.188613
iter  30 value 374.176255
final  value 374.176245 
converged
# weights:  34
initial  value 907.169776 
iter  10 value 370.612186
final  value 370.539874 
converged
# weights:  166
initial  value 1327.050375 
iter  10 value 362.419527
iter  20 value 362.021905
iter  30 value 362.011131
iter  40 value 361.998201
iter  50 value 350.804974
iter  60 value 344.671148
iter  70 value 344.050910
iter  80 value 343.924176
iter  90 value 343.889042
iter 100 value 343.880989
final  value 343.880989 
stopped after 100 iterations
# weights:  34
initial  value 721.245973 
iter  10 value 365.789738
iter  20 value 356.636932
iter  30 value 353.534031
iter  40 value 353.452559
iter  40 value 353.452557
iter  40 value 353.452557
final  value 353.452557 
converged
# weights:  188
initial  value 663.850148 
iter  10 value 383.224404
iter  20 value 375.397732
iter  30 value 375.176306
iter  40 value 375.175073
iter  40 value 375.175072
iter  40 value 375.175071
final  value 375.175071 
converged
# weights:  56
initial  value 1004.955175 
iter  10 value 363.454755
iter  20 value 359.446627
iter  30 value 347.936106
iter  40 value 346.353506
iter  50 value 346.330362
iter  60 value 346.328178
iter  70 value 346.327610
final  value 346.327572 
converged
# weights:  45
initial  value 653.928958 
iter  10 value 370.857523
iter  20 value 362.711938
iter  30 value 355.139784
iter  40 value 346.159748
iter  50 value 345.819425
iter  60 value 345.755942
iter  70 value 345.745351
iter  80 value 345.743839
iter  90 value 345.742968
iter 100 value 345.742797
final  value 345.742797 
stopped after 100 iterations
# weights:  177
initial  value 966.941920 
iter  10 value 366.885894
iter  20 value 364.607858
final  value 364.588978 
converged
# weights:  23
initial  value 844.863020 
iter  10 value 391.101769
iter  20 value 376.688380
final  value 376.685593 
converged
# weights:  67
initial  value 732.583957 
iter  10 value 421.620767
iter  20 value 391.954023
iter  30 value 391.058630
final  value 391.057779 
converged
# weights:  166
initial  value 586.002116 
iter  10 value 362.198217
iter  20 value 362.005922
final  value 362.004996 
converged
# weights:  133
initial  value 722.382531 
iter  10 value 380.926939
iter  20 value 366.301621
iter  30 value 365.777140
iter  40 value 365.687694
final  value 365.687573 
converged
# weights:  144
initial  value 1030.688400 
iter  10 value 356.090166
iter  20 value 347.592784
iter  30 value 345.357732
iter  40 value 345.046733
iter  50 value 345.008258
iter  60 value 344.979539
iter  70 value 344.971995
iter  80 value 344.968968
iter  90 value 344.966519
iter 100 value 344.964858
final  value 344.964858 
stopped after 100 iterations
# weights:  133
initial  value 873.712417 
iter  10 value 376.767517
iter  20 value 354.832716
iter  30 value 351.067571
iter  40 value 350.738863
iter  50 value 350.576115
iter  60 value 350.568572
final  value 350.568536 
converged
# weights:  12
initial  value 776.121379 
iter  10 value 364.803573
iter  20 value 362.118061
iter  30 value 362.057913
iter  40 value 362.051445
iter  50 value 362.035502
iter  60 value 362.034222
iter  70 value 362.033989
iter  80 value 362.033690
iter  90 value 362.033289
iter 100 value 362.032714
final  value 362.032714 
stopped after 100 iterations
# weights:  221
initial  value 916.715162 
iter  10 value 369.394005
iter  20 value 364.734427
final  value 364.730166 
converged
# weights:  56
initial  value 784.871927 
iter  10 value 363.069700
iter  20 value 362.017476
iter  30 value 362.013292
iter  40 value 362.011809
final  value 362.011785 
converged
# weights:  45
initial  value 609.743041 
iter  10 value 379.479035
iter  20 value 373.664266
final  value 373.662907 
converged
# weights:  166
initial  value 930.962030 
iter  10 value 363.649381
iter  20 value 362.152549
iter  30 value 348.022567
iter  40 value 346.520257
iter  50 value 346.332411
iter  60 value 346.272135
iter  70 value 346.242747
iter  80 value 346.229063
iter  90 value 346.225818
iter 100 value 346.223170
final  value 346.223170 
stopped after 100 iterations
# weights:  166
initial  value 584.988291 
iter  10 value 364.675733
iter  20 value 362.368752
iter  30 value 351.261356
iter  40 value 347.951473
iter  50 value 347.639519
iter  60 value 347.512906
iter  70 value 347.436763
iter  80 value 347.416085
iter  90 value 347.410529
iter 100 value 347.409750
final  value 347.409750 
stopped after 100 iterations
# weights:  133
initial  value 716.141444 
iter  10 value 378.010572
iter  20 value 365.945495
iter  30 value 365.664687
final  value 365.655025 
converged
# weights:  34
initial  value 742.391035 
iter  10 value 362.106726
final  value 362.005469 
converged
# weights:  166
initial  value 596.018319 
iter  10 value 366.713791
iter  20 value 364.254996
iter  30 value 364.231732
iter  40 value 364.230839
iter  50 value 364.230500
iter  60 value 364.230438
iter  70 value 364.230374
iter  80 value 364.230306
iter  90 value 364.230234
iter 100 value 364.230157
final  value 364.230157 
stopped after 100 iterations
# weights:  34
initial  value 904.789782 
iter  10 value 369.496389
iter  20 value 360.008477
iter  30 value 354.765504
final  value 354.751498 
converged
# weights:  188
initial  value 998.699757 
iter  10 value 419.145370
iter  20 value 380.508940
iter  30 value 377.625852
iter  40 value 377.449515
iter  50 value 377.413831
final  value 377.413106 
converged
# weights:  56
initial  value 930.946581 
iter  10 value 377.401717
iter  20 value 364.746999
iter  30 value 351.994788
iter  40 value 348.407087
iter  50 value 348.191567
iter  60 value 348.124229
iter  70 value 348.079069
iter  80 value 348.063540
iter  90 value 348.059629
iter 100 value 348.056081
final  value 348.056081 
stopped after 100 iterations
# weights:  45
initial  value 747.445641 
iter  10 value 372.741819
iter  20 value 364.972417
iter  30 value 361.066417
iter  40 value 348.067737
iter  50 value 347.594164
iter  60 value 347.489509
iter  70 value 347.451693
iter  80 value 347.443862
iter  90 value 347.442459
iter 100 value 347.442123
final  value 347.442123 
stopped after 100 iterations
# weights:  177
initial  value 956.935362 
iter  10 value 382.132256
iter  20 value 361.235236
iter  30 value 360.213946
iter  40 value 360.108393
iter  50 value 360.064587
final  value 360.064471 
converged
# weights:  23
initial  value 841.628656 
iter  10 value 388.259197
iter  20 value 379.002622
final  value 378.917658 
converged
# weights:  67
initial  value 1288.913654 
iter  10 value 407.097283
iter  20 value 393.412589
final  value 393.309603 
converged
# weights:  166
initial  value 683.390404 
iter  10 value 364.659244
iter  20 value 364.231308
iter  30 value 364.228060
final  value 364.227545 
converged
# weights:  133
initial  value 999.307273 
iter  10 value 374.462894
iter  20 value 368.289839
iter  30 value 367.923493
iter  40 value 367.916347
final  value 367.915112 
converged
# weights:  144
initial  value 1061.062341 
iter  10 value 377.564253
iter  20 value 358.147202
iter  30 value 347.782773
iter  40 value 346.812290
iter  50 value 346.686826
iter  60 value 346.657768
iter  70 value 346.649243
iter  80 value 346.647430
iter  90 value 346.646744
iter 100 value 346.646188
final  value 346.646188 
stopped after 100 iterations
# weights:  133
initial  value 678.828515 
iter  10 value 366.517717
iter  20 value 363.338741
iter  30 value 355.061615
iter  40 value 353.835391
iter  50 value 353.257502
iter  60 value 352.911456
iter  70 value 352.908142
iter  80 value 352.907874
iter  80 value 352.907871
iter  80 value 352.907871
final  value 352.907871 
converged
# weights:  12
initial  value 1002.642261 
iter  10 value 366.538691
iter  20 value 364.325455
iter  30 value 364.287789
iter  40 value 364.273241
iter  50 value 364.264713
iter  60 value 364.234327
iter  70 value 364.110819
iter  80 value 349.797014
iter  90 value 345.929065
iter 100 value 345.770597
final  value 345.770597 
stopped after 100 iterations
# weights:  221
initial  value 951.438187 
iter  10 value 373.247797
iter  20 value 362.428166
iter  30 value 361.054254
iter  40 value 361.005331
final  value 361.005025 
converged
# weights:  56
initial  value 729.711914 
iter  10 value 365.417184
iter  20 value 364.247335
iter  30 value 364.239060
iter  40 value 364.237276
iter  50 value 364.234888
iter  60 value 364.203214
iter  70 value 360.327625
iter  80 value 345.901994
iter  90 value 345.589064
iter 100 value 345.516337
final  value 345.516337 
stopped after 100 iterations
# weights:  45
initial  value 906.192729 
iter  10 value 382.910004
iter  20 value 376.059383
iter  30 value 375.895205
iter  30 value 375.895205
iter  30 value 375.895205
final  value 375.895205 
converged
# weights:  166
initial  value 804.283332 
iter  10 value 365.183786
iter  20 value 361.046858
iter  30 value 349.439812
iter  40 value 348.196121
iter  50 value 348.017710
iter  60 value 347.997840
iter  70 value 347.993286
iter  80 value 347.989836
iter  90 value 347.988598
iter 100 value 347.987886
final  value 347.987886 
stopped after 100 iterations
# weights:  166
initial  value 707.552144 
iter  10 value 365.643943
iter  20 value 362.782027
iter  30 value 353.444997
iter  40 value 350.424822
iter  50 value 349.859393
iter  60 value 349.450044
iter  70 value 349.273284
iter  80 value 349.198791
iter  90 value 349.179612
iter 100 value 349.170651
final  value 349.170651 
stopped after 100 iterations
# weights:  133
initial  value 987.368646 
iter  10 value 380.579440
iter  20 value 370.566868
iter  30 value 367.911520
iter  40 value 367.882524
iter  40 value 367.882522
iter  40 value 367.882522
final  value 367.882522 
converged
# weights:  34
initial  value 735.960973 
iter  10 value 364.363733
final  value 364.229074 
converged
# weights:  144
initial  value 1214.286400 
iter  10 value 549.995434
iter  20 value 548.524770
iter  30 value 533.756945
iter  40 value 523.651193
iter  50 value 522.623605
iter  60 value 522.419491
iter  70 value 522.378285
iter  80 value 522.361500
iter  90 value 522.344527
iter 100 value 522.333565
final  value 522.333565 
stopped after 100 iterations
Neural Network 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  size  decay         RMSE       Rsquared   MAE        Selected
   1    3.972596e-04  0.8472052  0.4126437  0.6866717          
   2    1.300207e+00  0.8584978  0.5545029  0.7113114          
   3    1.647443e-05  0.8539168  0.4969658  0.7078613          
   3    1.651671e-01  0.8357895  0.4747419  0.6553348          
   4    2.071695e-02  0.8330188  0.4623069  0.6435447          
   4    1.492782e+00  0.8571877  0.5766286  0.7101377          
   5    9.437369e-05  0.8395152  0.4480448  0.6612043          
   5    3.119295e-02  0.8332863  0.4567685  0.6453681          
   6    6.607126e+00  0.8632661  0.6761245  0.7151873          
  12    1.627202e-01  0.8350847  0.4531964  0.6534430          
  12    7.085691e-01  0.8545700  0.5881143  0.7080667          
  12    7.160442e-01  0.8545781  0.5887109  0.7080734          
  13    1.106921e-02  0.8327787  0.4631925  0.6417295  *       
  15    1.854533e-05  0.8539167  0.3419515  0.7078613          
  15    1.353235e-04  0.8391370  0.4083042  0.6594436          
  15    3.584114e-02  0.8335009  0.4463443  0.6472389          
  15    6.599294e-02  0.8338990  0.4464249  0.6487907          
  16    5.556676e-01  0.8492599  0.5523727  0.6933995          
  17    4.139424e+00  0.8572099  0.7114759  0.7103398          
  20    6.622221e-01  0.8442509  0.5193524  0.6811803          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 13 and decay = 0.01106921.
[1] "Sun Mar 11 19:43:25 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: package 'mxnet' is not available (for R version 3.4.2) 
2: package 'mxnet' is not available (for R version 3.4.2) 
3: predictions failed for Fold1: parameter=none Error in newdata %*% modelFit$x : 
  requires numeric/complex matrix/vector arguments
 
4: predictions failed for Fold2: parameter=none Error in newdata %*% modelFit$x : 
  requires numeric/complex matrix/vector arguments
 
5: predictions failed for Fold3: parameter=none Error in newdata %*% modelFit$x : 
  requires numeric/complex matrix/vector arguments
 
6: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: predictions failed for Fold1: parameter=none Error in newdata %*% modelFit$x : 
  requires numeric/complex matrix/vector arguments
 
2: predictions failed for Fold2: parameter=none Error in newdata %*% modelFit$x : 
  requires numeric/complex matrix/vector arguments
 
3: predictions failed for Fold3: parameter=none Error in newdata %*% modelFit$x : 
  requires numeric/complex matrix/vector arguments
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                          "failed"                         
 [3] "Sun Mar 11 19:43:32 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "nnls"                           
Non-Informative Model 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results:

  RMSE       Rsquared  MAE      
  0.6450623  NaN       0.5323018

[1] "Sun Mar 11 19:43:38 2018"
Error : The tuning parameter grid should have columns alpha, criteria, link
In addition: Warning message:
In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Error : The tuning parameter grid should have columns alpha, criteria, link
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :12    NA's   :12    NA's   :12   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Sun Mar 11 19:43:48 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "ordinalNet"                     
randomForest 4.6-12
Type rfNews() to see new features/changes/bug fixes.

Attaching package: 'randomForest'

The following object is masked from 'package:ggplot2':

    margin

The import package should not be attached.
Use "colon syntax" instead, e.g. import::from, or import:::from.

Attaching package: 'import'

The following object is masked from 'package:plyr':

    here

Parallel Random Forest 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE         Selected
  1     0.05301017  0.9948391  0.03838315          
  2     0.03964782  0.9966681  0.02845186          
  3     0.03708809  0.9969362  0.02713706  *       
  6     0.03811621  0.9966225  0.02761864          
  7     0.03990366  0.9962625  0.02893281          
  8     0.04052051  0.9961383  0.02962765          
  9     0.04225217  0.9957895  0.03095661          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 3.
[1] "Sun Mar 11 19:44:45 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: Warning message:
executing %dopar% sequentially: no parallel backend registered 
# weights:  91
initial  value 705.475917 
iter  10 value 372.673628
iter  20 value 370.562484
iter  30 value 370.541187
iter  40 value 370.534531
iter  50 value 355.082343
iter  60 value 352.945467
iter  70 value 352.876741
iter  80 value 352.824300
iter  90 value 352.800760
iter 100 value 352.788981
final  value 352.788981 
stopped after 100 iterations
# weights:  19
initial  value 845.896703 
iter  10 value 375.183112
iter  20 value 361.525958
iter  30 value 360.320293
iter  40 value 360.245300
iter  40 value 360.245299
final  value 360.245299 
converged
# weights:  103
initial  value 755.990052 
iter  10 value 390.710326
iter  20 value 388.473192
final  value 388.471958 
converged
# weights:  31
initial  value 698.844085 
iter  10 value 377.752982
iter  20 value 357.226422
iter  30 value 354.601503
iter  40 value 354.513916
iter  50 value 354.497847
final  value 354.497574 
converged
# weights:  25
initial  value 746.973223 
iter  10 value 371.630862
iter  20 value 354.846846
iter  30 value 354.182065
iter  40 value 354.134416
iter  50 value 354.129917
iter  60 value 354.129461
final  value 354.129419 
converged
# weights:  97
initial  value 836.387295 
iter  10 value 370.082824
iter  20 value 364.580896
iter  30 value 364.504486
final  value 364.502261 
converged
# weights:  13
initial  value 701.710524 
iter  10 value 388.051304
iter  20 value 384.313269
final  value 384.300783 
converged
# weights:  37
initial  value 830.363261 
iter  10 value 415.773163
iter  20 value 409.824564
iter  30 value 409.759299
final  value 409.758138 
converged
# weights:  91
initial  value 1009.596127 
iter  10 value 370.854913
iter  20 value 370.541515
final  value 370.538900 
converged
# weights:  73
initial  value 586.084380 
iter  10 value 380.491211
iter  20 value 367.527217
iter  30 value 367.465227
final  value 367.465155 
converged
# weights:  79
initial  value 560.255791 
iter  10 value 374.944534
iter  20 value 362.837641
iter  30 value 353.692192
iter  40 value 353.479506
iter  50 value 353.439425
iter  60 value 353.427740
iter  70 value 353.423942
iter  80 value 353.422171
iter  90 value 353.421239
iter 100 value 353.420703
final  value 353.420703 
stopped after 100 iterations
# weights:  73
initial  value 703.698480 
iter  10 value 367.569623
iter  20 value 358.186180
iter  30 value 357.773565
iter  40 value 357.753567
final  value 357.753321 
converged
# weights:  7
initial  value 946.927055 
iter  10 value 372.429830
iter  20 value 370.604526
iter  30 value 370.570672
iter  40 value 353.951631
iter  50 value 353.275980
iter  60 value 353.087652
iter  70 value 353.052529
final  value 353.051674 
converged
# weights:  121
initial  value 962.501667 
iter  10 value 376.890960
iter  20 value 366.029056
iter  30 value 366.019362
iter  30 value 366.019360
iter  30 value 366.019360
final  value 366.019360 
converged
# weights:  31
initial  value 739.359637 
iter  10 value 371.165530
iter  20 value 370.547612
iter  30 value 361.204970
iter  40 value 353.078417
iter  50 value 352.911551
iter  60 value 352.836293
iter  70 value 352.780519
iter  80 value 352.777146
iter  90 value 352.770648
iter 100 value 352.765928
final  value 352.765928 
stopped after 100 iterations
# weights:  25
initial  value 1002.665181 
iter  10 value 384.726158
iter  20 value 381.224050
final  value 381.219989 
converged
# weights:  91
initial  value 520.722073 
iter  10 value 369.373423
iter  20 value 354.490875
iter  30 value 354.424823
iter  40 value 354.422595
final  value 354.422567 
converged
# weights:  91
initial  value 585.580807 
iter  10 value 367.313102
iter  20 value 355.641765
iter  30 value 355.444025
iter  40 value 355.429435
final  value 355.429295 
converged
# weights:  73
initial  value 817.412646 
iter  10 value 372.997081
iter  20 value 367.260011
iter  30 value 366.986856
final  value 366.983595 
converged
# weights:  19
initial  value 757.806838 
iter  10 value 370.636126
iter  20 value 370.541619
iter  30 value 370.539520
iter  40 value 353.784215
iter  50 value 352.908404
iter  60 value 352.779464
iter  70 value 352.754173
final  value 352.751932 
converged
# weights:  91
initial  value 982.370621 
iter  10 value 363.829767
iter  20 value 362.024732
iter  30 value 362.006446
iter  40 value 361.980823
iter  50 value 345.892114
iter  60 value 343.993251
iter  70 value 343.947783
iter  80 value 343.908251
iter  90 value 343.887457
iter 100 value 343.861699
final  value 343.861699 
stopped after 100 iterations
# weights:  19
initial  value 963.684567 
iter  10 value 366.669457
iter  20 value 352.356943
iter  30 value 351.099818
final  value 351.098361 
converged
# weights:  103
initial  value 873.517384 
iter  10 value 386.655583
iter  20 value 379.872851
iter  30 value 379.840420
final  value 379.840375 
converged
# weights:  31
initial  value 735.242976 
iter  10 value 364.528843
iter  20 value 348.479779
iter  30 value 345.681636
iter  40 value 345.588868
iter  50 value 345.577096
iter  60 value 345.576916
iter  60 value 345.576914
iter  60 value 345.576913
final  value 345.576913 
converged
# weights:  25
initial  value 668.961913 
iter  10 value 352.827519
iter  20 value 345.589952
iter  30 value 345.181966
iter  40 value 345.162182
iter  50 value 345.161511
final  value 345.161484 
converged
# weights:  97
initial  value 969.818024 
iter  10 value 371.837488
iter  20 value 356.964108
iter  30 value 356.081701
iter  40 value 355.448379
iter  50 value 355.171520
iter  60 value 355.145489
final  value 355.140516 
converged
# weights:  13
initial  value 945.284424 
iter  10 value 376.727168
iter  20 value 375.205949
final  value 375.205752 
converged
# weights:  37
initial  value 1055.212556 
iter  10 value 405.287507
iter  20 value 401.060105
iter  30 value 401.035509
final  value 401.035357 
converged
# weights:  91
initial  value 781.209060 
iter  10 value 362.269471
iter  20 value 362.006743
final  value 362.004795 
converged
# weights:  73
initial  value 754.279128 
iter  10 value 360.335068
iter  20 value 358.028859
iter  30 value 357.989197
final  value 357.988781 
converged
# weights:  79
initial  value 878.426828 
iter  10 value 369.057606
iter  20 value 351.598977
iter  30 value 344.719321
iter  40 value 344.551850
iter  50 value 344.513017
iter  60 value 344.496046
iter  70 value 344.492240
iter  80 value 344.489772
iter  90 value 344.488541
iter 100 value 344.487820
final  value 344.487820 
stopped after 100 iterations
# weights:  73
initial  value 690.053029 
iter  10 value 359.123708
iter  20 value 349.144050
iter  30 value 348.746083
iter  40 value 348.733988
final  value 348.733980 
converged
# weights:  7
initial  value 675.422115 
iter  10 value 363.215775
iter  20 value 362.071379
iter  30 value 362.056770
iter  40 value 362.045696
iter  50 value 362.033755
iter  60 value 362.027843
iter  70 value 362.018822
iter  80 value 362.002469
iter  90 value 361.960528
iter 100 value 361.578666
final  value 361.578666 
stopped after 100 iterations
# weights:  121
initial  value 570.432894 
iter  10 value 359.784423
iter  20 value 356.076119
iter  30 value 355.856632
final  value 355.852847 
converged
# weights:  31
initial  value 594.193371 
iter  10 value 362.366342
iter  20 value 362.012328
iter  30 value 361.996709
iter  40 value 361.514248
iter  50 value 344.083713
iter  60 value 343.989094
iter  70 value 343.929586
iter  80 value 343.875234
iter  90 value 343.856244
iter 100 value 343.839037
final  value 343.839037 
stopped after 100 iterations
# weights:  25
initial  value 741.396826 
iter  10 value 376.345665
iter  20 value 372.118961
final  value 372.118222 
converged
# weights:  91
initial  value 655.343753 
iter  10 value 362.685543
iter  20 value 346.061621
iter  30 value 345.412327
iter  40 value 345.313798
iter  50 value 345.289268
iter  60 value 345.282401
iter  70 value 345.281249
final  value 345.281204 
converged
# weights:  91
initial  value 1086.335727 
iter  10 value 363.041896
iter  20 value 346.667342
iter  30 value 346.408846
iter  40 value 346.384150
final  value 346.383905 
converged
# weights:  73
initial  value 619.010171 
iter  10 value 359.417597
iter  20 value 357.844405
iter  30 value 357.806122
final  value 357.806096 
converged
# weights:  19
initial  value 739.299068 
iter  10 value 362.090596
iter  20 value 362.007091
iter  30 value 362.005700
iter  40 value 361.992696
iter  50 value 361.987587
iter  60 value 361.973750
iter  70 value 361.885714
iter  80 value 345.264241
iter  90 value 344.032765
iter 100 value 343.861689
final  value 343.861689 
stopped after 100 iterations
# weights:  91
initial  value 651.843804 
iter  10 value 365.748040
iter  20 value 364.243861
iter  30 value 364.231277
iter  40 value 364.227944
iter  50 value 363.487012
iter  60 value 345.717418
iter  70 value 345.677315
iter  80 value 345.617138
iter  90 value 345.558758
iter 100 value 345.539212
final  value 345.539212 
stopped after 100 iterations
# weights:  19
initial  value 930.116962 
iter  10 value 362.584036
iter  20 value 352.968820
iter  30 value 352.841945
final  value 352.841853 
converged
# weights:  103
initial  value 1053.672232 
iter  10 value 385.443708
iter  20 value 382.079254
iter  30 value 382.070640
iter  30 value 382.070639
iter  30 value 382.070638
final  value 382.070638 
converged
# weights:  31
initial  value 942.849654 
iter  10 value 357.926925
iter  20 value 347.839226
iter  30 value 347.379302
iter  40 value 347.309905
iter  50 value 347.307612
iter  60 value 347.307221
final  value 347.307196 
converged
# weights:  25
initial  value 727.601171 
iter  10 value 370.549819
iter  20 value 360.022322
iter  30 value 347.540227
iter  40 value 346.970213
iter  50 value 346.888362
iter  60 value 346.881438
iter  70 value 346.880963
final  value 346.880958 
converged
# weights:  97
initial  value 751.697181 
iter  10 value 362.304025
iter  20 value 357.297472
iter  30 value 357.014212
iter  40 value 356.974162
final  value 356.973990 
converged
# weights:  13
initial  value 769.715618 
iter  10 value 378.368180
iter  20 value 377.203768
final  value 377.202952 
converged
# weights:  37
initial  value 1075.631751 
iter  10 value 405.660014
iter  20 value 403.236068
final  value 403.218024 
converged
# weights:  91
initial  value 710.860132 
iter  10 value 364.430561
iter  20 value 364.228672
final  value 364.227208 
converged
# weights:  73
initial  value 486.136086 
iter  10 value 366.105851
iter  20 value 360.243254
final  value 360.236500 
converged
# weights:  79
initial  value 912.047514 
iter  10 value 365.373624
iter  20 value 363.295274
iter  30 value 346.537343
iter  40 value 346.265271
iter  50 value 346.224557
iter  60 value 346.199058
iter  70 value 346.185532
iter  80 value 346.178341
iter  90 value 346.170617
iter 100 value 346.165614
final  value 346.165614 
stopped after 100 iterations
# weights:  73
initial  value 724.076349 
iter  10 value 367.655640
iter  20 value 351.286737
iter  30 value 350.819596
final  value 350.818095 
converged
# weights:  7
initial  value 645.907680 
iter  10 value 365.525056
iter  20 value 364.240963
iter  30 value 363.543701
iter  40 value 346.198411
iter  50 value 345.812240
final  value 345.809673 
converged
# weights:  121
initial  value 1368.775980 
iter  10 value 373.333778
iter  20 value 358.467232
iter  30 value 358.179940
iter  40 value 358.047831
final  value 358.039735 
converged
# weights:  31
initial  value 703.131362 
iter  10 value 364.813685
iter  20 value 364.237352
iter  30 value 364.231460
iter  40 value 364.230188
iter  50 value 364.227964
iter  60 value 364.223867
iter  70 value 364.215270
iter  80 value 364.120208
iter  90 value 346.076247
iter 100 value 345.677905
final  value 345.677905 
stopped after 100 iterations
# weights:  25
initial  value 680.349897 
iter  10 value 379.081020
iter  20 value 374.138642
final  value 374.134437 
converged
# weights:  91
initial  value 687.854894 
iter  10 value 364.692229
iter  20 value 347.544170
iter  30 value 347.139364
iter  40 value 347.117696
iter  50 value 347.116773
final  value 347.116764 
converged
# weights:  91
initial  value 703.838044 
iter  10 value 365.750745
iter  20 value 349.145051
iter  30 value 348.204916
iter  40 value 347.948339
iter  50 value 347.853279
iter  60 value 347.812794
iter  70 value 347.807948
final  value 347.807896 
converged
# weights:  73
initial  value 412.851973 
iter  10 value 362.252861
iter  20 value 360.148130
iter  30 value 360.139446
iter  30 value 360.139444
iter  30 value 360.139444
final  value 360.139444 
converged
# weights:  19
initial  value 740.710417 
iter  10 value 364.315771
iter  20 value 364.228624
iter  20 value 364.228623
iter  20 value 364.228623
final  value 364.228623 
converged
# weights:  31
initial  value 1217.662321 
iter  10 value 550.182585
iter  20 value 548.404665
iter  30 value 548.391377
iter  40 value 548.391274
iter  50 value 548.391158
iter  60 value 548.391025
iter  70 value 548.390867
iter  80 value 548.390672
iter  90 value 548.390416
iter 100 value 548.390055
final  value 548.390055 
stopped after 100 iterations
Neural Networks with Feature Extraction 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  size  decay         RMSE       Rsquared   MAE        Selected
   1    3.972596e-04  0.8394894  0.5331281  0.6633394          
   2    1.300207e+00  0.8498104  0.7321405  0.6924279          
   3    1.647443e-05  0.8391589  0.3230450  0.6602524          
   3    1.651671e-01  0.8346710  0.5510199  0.6503911          
   4    2.071695e-02  0.8327966  0.4799526  0.6417082          
   4    1.492782e+00  0.8474942  0.7141103  0.6890895          
   5    9.437369e-05  0.8323837  0.4755896  0.6377325  *       
   5    3.119295e-02  0.8328681  0.4865888  0.6416500          
   6    6.607126e+00  0.8639793  0.8017433  0.7134922          
  12    1.627202e-01  0.8338548  0.5328839  0.6475564          
  12    7.085691e-01  0.8383909  0.6063538  0.6677637          
  12    7.160442e-01  0.8387190  0.6085856  0.6689950          
  13    1.106921e-02  0.8326456  0.4727698  0.6411081          
  15    1.854533e-05  0.8539167  0.3198410  0.7078613          
  15    1.353235e-04  0.8323910  0.4671047  0.6382137          
  15    3.584114e-02  0.8327957  0.4896677  0.6411614          
  15    6.599294e-02  0.8330453  0.5035640  0.6424987          
  16    5.556676e-01  0.8364917  0.5745432  0.6599219          
  17    4.139424e+00  0.8557875  0.7214515  0.7063322          
  20    6.622221e-01  0.8374941  0.5724051  0.6645980          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 5 and decay = 9.437369e-05.
[1] "Sun Mar 11 19:45:01 2018"

Attaching package: 'pls'

The following object is masked from 'package:caret':

    R2

The following object is masked from 'package:stats':

    loadings

Principal Component Analysis 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  ncomp  RMSE         Rsquared   MAE          Selected
  1      0.275979305  0.8172603  0.214242052          
  2      0.128945591  0.9605230  0.103944494          
  3      0.055148632  0.9927537  0.044156712          
  5      0.018894411  0.9991517  0.014706515          
  6      0.008199933  0.9998376  0.006188692          
  7      0.008155241  0.9998394  0.006176956          
  8      0.007929546  0.9998489  0.006142287  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 8.
[1] "Sun Mar 11 19:45:08 2018"
Installing package into 'C:/Users/John/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
No more significant predictors (<0.160700123151764) found
Warning only 8 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
No more significant predictors (<0.160700123151764) found
Warning only 8 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Component____ 9 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
No more significant predictors (<0.160700123151764) found
Warning only 8 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

Partial Least Squares Generalized Linear Models  

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  nt  alpha.pvals.expli  RMSE         Rsquared   MAE          Selected
  1   0.053302480        0.220523705  0.8824497  0.175465972          
  1   0.170467085        0.220523705  0.8824497  0.175465972          
  2   0.007227010        0.044386051  0.9953425  0.032527963          
  2   0.032495032        0.044386051  0.9953425  0.032527963          
  2   0.110544196        0.044386051  0.9953425  0.032527963          
  2   0.140597454        0.044386051  0.9953425  0.032527963          
  2   0.172466551        0.044386051  0.9953425  0.032527963          
  3   0.116468547        0.026382359  0.9983381  0.018671655          
  3   0.194000420        0.026382359  0.9983381  0.018671655          
  6   0.101470550        0.008541171  0.9998259  0.006558395          
  6   0.140381381        0.008499613  0.9998276  0.006528531          
  6   0.161679406        0.008464753  0.9998289  0.006503688          
  6   0.161831328        0.008464753  0.9998289  0.006503688          
  7   0.008941156        0.007878932  0.9998510  0.006083166          
  7   0.037712441        0.007882617  0.9998507  0.006082514          
  7   0.118479395        0.007876701  0.9998509  0.006077144          
  7   0.127316583        0.007876701  0.9998509  0.006077144          
  8   0.158160502        0.007820170  0.9998535  0.005993979          
  8   0.187231331        0.007820170  0.9998535  0.005993979          
  9   0.160700123        0.007789222  0.9998546  0.005967778  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nt = 9 and alpha.pvals.expli
 = 0.1607001.
[1] "Sun Mar 11 19:45:58 2018"
Projection Pursuit Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  nterms  RMSE         Rsquared   MAE          Selected
   1      0.007279615  0.9998683  0.004562589          
   2      0.006896208  0.9998798  0.004312034          
   3      0.006270007  0.9998961  0.003875570          
   4      0.005679111  0.9999134  0.003115946          
   5      0.005275795  0.9999230  0.002987819  *       
   6      0.005438115  0.9999209  0.003120481          
   7      0.005511128  0.9999161  0.003084251          
   8      0.005543091  0.9999141  0.003140007          
   9      0.005289866  0.9999226  0.003041402          
  10      0.005388067  0.9999213  0.003105225          
  11      0.005422541  0.9999177  0.003097878          
  12      0.005497756  0.9999141  0.003078904          
  13      0.005432223  0.9999156  0.003034742          
  14      0.005447173  0.9999162  0.003070763          
  15      0.005387270  0.9999182  0.003086051          
  16      0.005397392  0.9999183  0.003095603          
  17      0.005414429  0.9999183  0.003114610          
  18      0.005588464  0.9999131  0.003208635          
  19      0.005538911  0.9999153  0.003204376          
  20      0.005555809  0.9999144  0.003243837          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nterms = 5.
[1] "Sun Mar 11 19:46:16 2018"
Installing package into 'C:/Users/John/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Loading required package: RColorBrewer
Quantile Random Forest 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE         Selected
  1     0.19262893  0.9877816  0.17675877          
  2     0.14012818  0.9917123  0.12593931          
  3     0.12644558  0.9924645  0.11229195          
  6     0.10598341  0.9935819  0.09242365          
  7     0.10218750  0.9938116  0.08892925          
  8     0.09899237  0.9938043  0.08588531          
  9     0.09638261  0.9936194  0.08317135  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 9.
[1] "Sun Mar 11 19:47:37 2018"

Attaching package: 'ranger'

The following object is masked from 'package:randomForest':

    importance


Attaching package: 'dplyr'

The following object is masked from 'package:randomForest':

    combine

The following objects are masked from 'package:plyr':

    arrange, count, desc, failwith, id, mutate, rename, summarise,
    summarize

The following object is masked from 'package:MASS':

    select

The following objects are masked from 'package:stats':

    filter, lag

The following objects are masked from 'package:base':

    intersect, setdiff, setequal, union

Random Forest 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE        Rsquared   MAE         Selected
  1     maxstat     0.07148748  0.9917419  0.05221195          
  1     variance    0.05315497  0.9947837  0.03862953          
  2     extratrees  0.03486841  0.9975802  0.02473671          
  2     maxstat     0.04223783  0.9964826  0.03026410          
  2     variance    0.03964433  0.9966773  0.02863191          
  3     extratrees  0.03119973  0.9979613  0.02218061          
  3     maxstat     0.03736008  0.9970598  0.02702247          
  6     extratrees  0.02831012  0.9982159  0.02020051          
  6     maxstat     0.03594109  0.9971293  0.02601049          
  7     extratrees  0.02799718  0.9982445  0.01988434  *       
  7     variance    0.03938334  0.9963529  0.02847478          
  8     maxstat     0.03773421  0.9967778  0.02717914          
  9     maxstat     0.03922297  0.9964989  0.02817142          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mtry = 7 and splitrule = extratrees.
[1] "Sun Mar 11 19:48:37 2018"
Error in code$varImp(object$finalModel, ...) : 
  No importance values available
In addition: Warning messages:
1: package 'gpls' is not available (for R version 3.4.2) 
2: package 'rPython' is not available (for R version 3.4.2) 
Loading required package: Rcpp
Rborist 0.1-8
Type RboristNews() to see new features/changes/bug fixes.
Random Forest 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  predFixed  RMSE        Rsquared   MAE         Selected
  1          0.05397131  0.9945524  0.03912724          
  2          0.04019072  0.9965255  0.02946703          
  3          0.03811320  0.9967298  0.02791884  *       
  6          0.03975914  0.9963019  0.02837435          
  7          0.04105900  0.9960444  0.02952019          
  8          0.04281293  0.9956877  0.03081992          
  9          0.04392324  0.9954459  0.03186174          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was predFixed = 3.
[1] "Sun Mar 11 19:51:17 2018"
Relaxed Lasso 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  phi         lambda     RMSE        Rsquared   MAE         Selected
  0.03613505   163.4027  0.01717740  0.9994067  0.01419840  *       
  0.04470578  1572.2807  0.20763614  0.9723675  0.17939829          
  0.16247516   232.0831  0.51986548  0.9980371  0.51175469          
  0.18856220  1497.0798  0.29761927  0.9723675  0.27436184          
  0.26651240   115.6059  0.02880337  0.9996076  0.02587399          
  0.50735275  1050.2759  0.51964884  0.9723675  0.48846313          
  0.55272098   207.5304  0.06075530  0.9996458  0.05947169          
  0.58234274   261.4800  0.52546151  0.9980062  0.52161249          
  0.59239697  1557.7299  0.58072226  0.9723675  0.54557925          
  0.63658292  1619.7087  0.61260111  0.9723675  0.57525474          
  0.70190691   906.9705  0.33073368  0.9911787  0.29000366          
  0.70298727   164.2712  0.07729085  0.9996680  0.07637399          
  0.79080251  1832.1398  0.72443243  0.9723675  0.67882938          
  0.80350062  3599.9896  1.68770325        NaN  1.55957432          
  0.80839703   877.2121  0.37903872  0.9904863  0.33673062          
  0.80915664   916.4487  0.37938473  0.9904809  0.33706590          
  0.85233542   136.2418  0.09209019  0.9996772  0.09134976          
  0.86233276   209.8802  0.09503551  0.9996679  0.09429754          
  0.93615665  2109.2485  1.68770325        NaN  1.55957432          
  0.97000210   316.1210  0.28727378  0.9983583  0.28599715          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 163.4027 and phi = 0.03613505.
[1] "Sun Mar 11 19:51:25 2018"
Random Forest 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE         Selected
  1     0.05305748  0.9948087  0.03833693          
  2     0.04043494  0.9965211  0.02927589          
  3     0.03763012  0.9968626  0.02743613  *       
  6     0.03858720  0.9965267  0.02778302          
  7     0.03890016  0.9964402  0.02809492          
  8     0.04097702  0.9960467  0.02975847          
  9     0.04209807  0.9958123  0.03074777          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 3.
[1] "Sun Mar 11 19:52:21 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Ridge Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  lambda        RMSE         Rsquared   MAE          Selected
  1.337178e-05  0.007770377  0.9998552  0.005962295  *       
  2.492008e-05  0.007779966  0.9998548  0.005967804          
  4.963473e-05  0.007813564  0.9998535  0.005991432          
  5.064212e-05  0.007815097  0.9998534  0.005992377          
  1.228284e-04  0.007942648  0.9998486  0.006080904          
  1.281836e-04  0.007953288  0.9998482  0.006088657          
  1.876551e-04  0.008079837  0.9998433  0.006177751          
  2.949011e-04  0.008333543  0.9998334  0.006363969          
  6.054099e-04  0.009100458  0.9998015  0.006974669          
  2.897898e-02  0.024993986  0.9989612  0.018758494          
  3.288503e-02  0.026571805  0.9988982  0.020023768          
  3.420671e-02  0.027110162  0.9988773  0.020453273          
  5.734242e-02  0.036851335  0.9985384  0.028505563          
  2.197623e-01  0.110310796  0.9962176  0.089264215          
  2.554609e-01  0.126506531  0.9956255  0.102466566          
  2.646240e-01  0.130647300  0.9954683  0.105846940          
  2.961756e-01  0.144847079  0.9949117  0.117437469          
  4.725089e-01  0.222276034  0.9914561  0.180314178          
  8.058540e-01  0.359126149  0.9841898  0.291047813          
  6.112970e+00  1.574911217  0.9241586  1.279525326          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was lambda = 1.337178e-05.
[1] "Sun Mar 11 19:52:29 2018"
Robust Linear Model 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  intercept  psi           RMSE         Rsquared   MAE          Selected
  FALSE      psi.huber     0.009625828  0.9997883  0.007176975          
  FALSE      psi.hampel    0.009554837  0.9997897  0.007174439          
  FALSE      psi.bisquare  0.009769080  0.9997868  0.007182614          
   TRUE      psi.huber     0.007905680  0.9998508  0.005887893          
   TRUE      psi.hampel    0.007817489  0.9998538  0.005923692  *       
   TRUE      psi.bisquare  0.008829375  0.9998163  0.005922672          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were intercept = TRUE and psi = psi.hampel.
[1] "Sun Mar 11 19:52:36 2018"
CART 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  cp            RMSE        Rsquared   MAE         Selected
  3.790024e-05  0.08584364  0.9825300  0.06672881          
  3.923193e-05  0.08584364  0.9825300  0.06672881  *       
  6.321005e-05  0.08591116  0.9825012  0.06682452          
  6.681161e-05  0.08592892  0.9824919  0.06682452          
  8.792425e-05  0.08592892  0.9824919  0.06682452          
  1.352386e-04  0.08628672  0.9823607  0.06686861          
  1.525275e-04  0.08654855  0.9822464  0.06703460          
  1.574513e-04  0.08654855  0.9822464  0.06703460          
  1.728973e-04  0.08671588  0.9821737  0.06724544          
  1.913568e-04  0.08702835  0.9820334  0.06757693          
  2.258950e-04  0.08705723  0.9820144  0.06734909          
  2.831296e-04  0.08886945  0.9813818  0.06895429          
  3.786235e-04  0.09066778  0.9806057  0.07071053          
  4.823128e-04  0.09454761  0.9789405  0.07443560          
  5.536130e-04  0.09527733  0.9786241  0.07511188          
  6.618716e-04  0.09760666  0.9775271  0.07664130          
  1.464931e-03  0.11376327  0.9696257  0.09096948          
  1.828733e-03  0.11698537  0.9678280  0.09337907          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was cp = 3.923193e-05.
[1] "Sun Mar 11 19:52:43 2018"
CART 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results:

  RMSE       Rsquared   MAE      
  0.1513554  0.9452343  0.1225557

[1] "Sun Mar 11 19:52:50 2018"
CART 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  maxdepth  RMSE       Rsquared   MAE        Selected
   7        0.1513554  0.9452343  0.1225557  *       
   8        0.1513554  0.9452343  0.1225557          
  12        0.1513554  0.9452343  0.1225557          
  13        0.1513554  0.9452343  0.1225557          
  15        0.1513554  0.9452343  0.1225557          
  16        0.1513554  0.9452343  0.1225557          
  19        0.1513554  0.9452343  0.1225557          
  20        0.1513554  0.9452343  0.1225557          
  21        0.1513554  0.9452343  0.1225557          
  22        0.1513554  0.9452343  0.1225557          
  23        0.1513554  0.9452343  0.1225557          
  25        0.1513554  0.9452343  0.1225557          
  26        0.1513554  0.9452343  0.1225557          
  27        0.1513554  0.9452343  0.1225557          
  29        0.1513554  0.9452343  0.1225557          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was maxdepth = 7.
[1] "Sun Mar 11 19:52:57 2018"
Loading required package: quantreg
Loading required package: SparseM

Attaching package: 'SparseM'

The following object is masked from 'package:base':

    backsolve


Attaching package: 'quantreg'

The following object is masked from 'package:survival':

    untangle.specials

Loading required package: regpro
Loading required package: denpro
Quantile Regression with LASSO penalty 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  lambda        RMSE         Rsquared   MAE          Selected
  1.337178e-05  0.008186837  0.9998406  0.005862601          
  2.492008e-05  0.008173376  0.9998413  0.005866687          
  4.963473e-05  0.008143616  0.9998419  0.005861036          
  5.064212e-05  0.008143124  0.9998419  0.005860616          
  1.228284e-04  0.008045715  0.9998453  0.005854244          
  1.281836e-04  0.008025635  0.9998462  0.005855724          
  1.876551e-04  0.007972544  0.9998482  0.005869156          
  2.949011e-04  0.007904756  0.9998506  0.005928164  *       
  6.054099e-04  0.007958502  0.9998489  0.006000756          
  2.897898e-02  0.012685488  0.9996262  0.009162559          
  3.288503e-02  0.013699784  0.9995613  0.009904321          
  3.420671e-02  0.013961832  0.9995430  0.010100386          
  5.734242e-02  0.046100210  0.9953015  0.034006126          
  2.197623e-01  0.119888617  0.9723906  0.095298686          
  2.554609e-01  0.123266831  0.9723713  0.098174232          
  2.646240e-01  0.124620896  0.9723715  0.099470118          
  2.961756e-01  0.129552535  0.9723675  0.103721765          
  4.725089e-01  0.267094858  0.9723675  0.218454811          
  8.058540e-01  0.645991046        NaN  0.531824337          
  6.112970e+00  0.645991289        NaN  0.531824337          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was lambda = 0.0002949011.
[1] "Sun Mar 11 19:53:05 2018"
Non-Convex Penalized Quantile Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  lambda        penalty  RMSE         Rsquared   MAE         Selected
  1.337178e-05  MCP      0.008188321  0.9998406  0.00586397          
  2.492008e-05  SCAD     0.008188321  0.9998406  0.00586397          
  4.963473e-05  MCP      0.008188321  0.9998406  0.00586397          
  5.064212e-05  SCAD     0.008188321  0.9998406  0.00586397          
  1.228284e-04  SCAD     0.008188321  0.9998406  0.00586397          
  1.281836e-04  SCAD     0.008188321  0.9998406  0.00586397          
  1.876551e-04  MCP      0.008188321  0.9998406  0.00586397          
  2.949011e-04  SCAD     0.008188321  0.9998406  0.00586397          
  6.054099e-04  SCAD     0.008188321  0.9998406  0.00586397  *       
  2.897898e-02  SCAD     0.014146662  0.9995431  0.01049143          
  3.288503e-02  SCAD     0.014062745  0.9995466  0.01050602          
  3.420671e-02  SCAD     0.014012569  0.9995496  0.01054688          
  5.734242e-02  SCAD     0.031322133  0.9978070  0.02402586          
  2.197623e-01  MCP      0.111079843  0.9723675  0.08808447          
  2.554609e-01  SCAD     0.116174495  0.9723675  0.09217960          
  2.646240e-01  MCP      0.113327168  0.9723675  0.08992091          
  2.961756e-01  SCAD     0.121014106  0.9723675  0.09618792          
  4.725089e-01  SCAD     0.267094858  0.9723675  0.21845481          
  8.058540e-01  SCAD     0.645991046        NaN  0.53182434          
  6.112970e+00  SCAD     0.645991289        NaN  0.53182434          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 0.0006054099 and penalty
 = SCAD.
[1] "Sun Mar 11 19:53:15 2018"
RRF 1.7
Type rrfNews() to see new features/changes/bug fixes.

Attaching package: 'RRF'

The following object is masked from 'package:dplyr':

    combine

The following object is masked from 'package:ranger':

    importance

The following objects are masked from 'package:randomForest':

    classCenter, combine, getTree, grow, importance, margin, MDSplot,
    na.roughfix, outlier, partialPlot, treesize, varImpPlot, varUsed

The following object is masked from 'package:ggplot2':

    margin

Regularized Random Forest 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  mtry  coefReg     coefImp     RMSE        Rsquared   MAE         Selected
  1     0.26651240  0.38345857  0.05253573  0.9949336  0.03809103          
  1     0.85233542  0.31480481  0.05284120  0.9948824  0.03786767          
  2     0.03613505  0.35436531  0.04038844  0.9965529  0.02919971          
  2     0.16247516  0.25984419  0.03990889  0.9966447  0.02924051          
  2     0.55272098  0.26386904  0.03970865  0.9966611  0.02897792          
  2     0.70298727  0.69547179  0.03971705  0.9966504  0.02877683          
  2     0.86233276  0.31628287  0.03971931  0.9966792  0.02880512          
  3     0.58234274  0.08271422  0.03750277  0.9968727  0.02720590          
  3     0.97000210  0.51367395  0.03716511  0.9969408  0.02712945  *       
  6     0.50735275  0.74880540  0.03821942  0.9965920  0.02768207          
  6     0.70190691  0.29625066  0.03818172  0.9966063  0.02761673          
  6     0.80839703  0.10641038  0.03868776  0.9965206  0.02801733          
  6     0.80915664  0.52403988  0.03812793  0.9966184  0.02759896          
  7     0.04470578  0.49638768  0.03965947  0.9963071  0.02884801          
  7     0.18856220  0.38372453  0.03916492  0.9964030  0.02847579          
  7     0.59239697  0.15877320  0.03957551  0.9963330  0.02872148          
  7     0.63658292  0.13401517  0.03933382  0.9963686  0.02848207          
  8     0.79080251  0.80611560  0.04059190  0.9961202  0.02957648          
  8     0.93615665  0.74147944  0.04078524  0.9960939  0.02978103          
  9     0.80350062  0.57426968  0.04203380  0.9958286  0.03072105          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mtry = 3, coefReg = 0.9700021
 and coefImp = 0.5136739.
[1] "Sun Mar 11 19:58:10 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: Warning messages:
1: In rlm.default(x, y, weights, method = method, wt.method = wt.method,  :
  'rlm' failed to converge in 20 steps
2: In rlm.default(x, y, weights, method = method, wt.method = wt.method,  :
  'rlm' failed to converge in 20 steps
3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Regularized Random Forest 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  mtry  coefReg     RMSE        Rsquared   MAE         Selected
  1     0.26651240  0.05272843  0.9949173  0.03813703          
  1     0.85233542  0.05248035  0.9949423  0.03826318          
  2     0.03613505  0.03933289  0.9967487  0.02853499          
  2     0.16247516  0.03957172  0.9966991  0.02866481          
  2     0.55272098  0.03969375  0.9966732  0.02854040          
  2     0.70298727  0.03964217  0.9966865  0.02863586          
  2     0.86233276  0.03961069  0.9966755  0.02882004          
  3     0.58234274  0.03721392  0.9969357  0.02719854  *       
  3     0.97000210  0.03747135  0.9968808  0.02727235          
  6     0.50735275  0.03842076  0.9965686  0.02792704          
  6     0.70190691  0.03852715  0.9965499  0.02774260          
  6     0.80839703  0.03841870  0.9965738  0.02780568          
  6     0.80915664  0.03839525  0.9965632  0.02769368          
  7     0.04470578  0.03875604  0.9964221  0.02785713          
  7     0.18856220  0.03939142  0.9963546  0.02856788          
  7     0.59239697  0.03955405  0.9963444  0.02874186          
  7     0.63658292  0.03975189  0.9962902  0.02881616          
  8     0.79080251  0.04073398  0.9961007  0.02970982          
  8     0.93615665  0.04037071  0.9961529  0.02945543          
  9     0.80350062  0.04161332  0.9959091  0.03050457          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mtry = 3 and coefReg = 0.5823427.
[1] "Sun Mar 11 20:00:26 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
In addition: Warning messages:
1: model fit failed for Resample12: parameter=none Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
2: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
 [1] "failed"                          "failed"                         
 [3] "Sun Mar 11 20:08:23 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "rvmLinear"                      
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
In addition: There were 19 warnings (use warnings() to see them)
 [1] "failed"                          "failed"                         
 [3] "Sun Mar 11 22:23:45 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "rvmPoly"                        
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                          "failed"                         
 [3] "Sun Mar 11 22:47:02 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "rvmRadial"                      
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%
Subtractive Clustering and Fuzzy c-Means Rules 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  r.a  eps.high   eps.low     RMSE       Rsquared   MAE         Selected
   1   0.3501535  0.16518386  0.1146415  0.9774237  0.08264312          
   1   0.3630871  0.32718465  0.1146415  0.9774237  0.08264312  *       
   4   0.9832347  0.91051607        NaN        NaN         NaN          
   5   0.4281914  0.03016347        NaN        NaN         NaN          
   5   0.6902389  0.10662953        NaN        NaN         NaN          
   6   0.6500471  0.30724413        NaN        NaN         NaN          
   8   0.4987622  0.47048865        NaN        NaN         NaN          
  11   0.2494972  0.18125247        NaN        NaN         NaN          
  12   0.6392140  0.54501450        NaN        NaN         NaN          
  12   0.8133406  0.26535019        NaN        NaN         NaN          
  12   0.9919220  0.53255198        NaN        NaN         NaN          
  13   0.4835909  0.09509000        NaN        NaN         NaN          
  15   0.6621386  0.54452535        NaN        NaN         NaN          
  15   0.7488304  0.49758555        NaN        NaN         NaN          
  15   0.8275551  0.22616478        NaN        NaN         NaN          
  16   0.7632575  0.53445636        NaN        NaN         NaN          
  16   0.9363683  0.68592545        NaN        NaN         NaN          
  17   0.5363728  0.08326398        NaN        NaN         NaN          
  18   0.7248577  0.14182631        NaN        NaN         NaN          
  20   0.1304501  0.09640529        NaN        NaN         NaN          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were r.a = 1, eps.high = 0.3630871
 and eps.low = 0.3271847.
[1] "Sun Mar 11 23:15:13 2018"
Partial Least Squares 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  ncomp  RMSE         Rsquared   MAE          Selected
  1      0.228134712  0.8752981  0.175878213          
  2      0.111305657  0.9704956  0.089091896          
  3      0.049264528  0.9942218  0.039108529          
  5      0.014069671  0.9995287  0.010711682          
  6      0.008063394  0.9998432  0.006131154          
  7      0.007915838  0.9998494  0.006122313          
  8      0.007824482  0.9998532  0.006034916  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 8.
[1] "Sun Mar 11 23:15:19 2018"
Loading required package: parallel

 spikeslab 1.1.5 
 
 Type spikeslab.news() to see new features, changes, and bug fixes. 
 

Spike and Slab Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  vars  RMSE         Rsquared   MAE          Selected
  1     0.313015778  0.9723675  0.256377246          
  2     0.201595636  0.9888849  0.165809085          
  3     0.029036662  0.9983491  0.022396715          
  6     0.009953513  0.9997645  0.007822843          
  7     0.009090368  0.9998019  0.007071995          
  8     0.008729412  0.9998175  0.006745810          
  9     0.008514608  0.9998266  0.006554154  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was vars = 9.
[1] "Sun Mar 11 23:15:43 2018"
Sparse Partial Least Squares (SPLS) Regression and
Classification (version 2.2-1)


Attaching package: 'spls'

The following object is masked from 'package:caret':

    splsda

Sparse Partial Least Squares 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  kappa       eta         K  RMSE         Rsquared   MAE          Selected
  0.01051578  0.26651240  4  0.018758989  0.9991595  0.014737360          
  0.03304579  0.85233542  3  0.026890057  0.9982905  0.019528441          
  0.05798214  0.03613505  4  0.018758989  0.9991595  0.014737360          
  0.05870932  0.70298727  7  0.008527872  0.9998241  0.006337054          
  0.09077491  0.55272098  3  0.030966335  0.9977033  0.021500140          
  0.09231936  0.86233276  3  0.026890057  0.9982905  0.019528441          
  0.10611337  0.16247516  3  0.030966335  0.9977033  0.021500140          
  0.12247304  0.58234274  1  0.190662664  0.9124153  0.151463760          
  0.14850413  0.97000210  5  0.021606633  0.9987867  0.016265024          
  0.28850692  0.80839703  1  0.071039526  0.9878504  0.054524512          
  0.29308319  0.70190691  3  0.030966335  0.9977033  0.021500140          
  0.29450928  0.80915664  5  0.016010191  0.9993841  0.012128314          
  0.31320634  0.50735275  7  0.008527872  0.9998241  0.006337054          
  0.36182944  0.18856220  4  0.018758989  0.9991595  0.014737360          
  0.36727703  0.59239697  2  0.042587954  0.9957383  0.030937426          
  0.36855244  0.04470578  5  0.016473021  0.9993465  0.012518035          
  0.37262911  0.63658292  2  0.045789870  0.9950360  0.033563835          
  0.38953417  0.79080251  8  0.007833469  0.9998526  0.006047829  *       
  0.40885470  0.93615665  7  0.008517040  0.9998248  0.006343063          
  0.48218769  0.80350062  6  0.010357540  0.9997448  0.007927197          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were K = 8, eta = 0.7908025 and kappa
 = 0.3895342.
[1] "Sun Mar 11 23:15:53 2018"
Supervised Principal Component Analysis 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  threshold   n.components  RMSE      Rsquared   MAE       Selected
  0.02103156  1             1.583714  0.8172603  1.559535          
  0.06609158  3             1.560702  0.9927537  1.559727  *       
  0.11596427  1             1.583714  0.8172603  1.559535          
  0.11741865  3             1.560702  0.9927537  1.559727          
  0.18154983  2             1.564778  0.9605230  1.559493          
  0.18463873  3             1.560702  0.9927537  1.559727          
  0.21222674  1             1.583714  0.8172603  1.559535          
  0.24494607  2             1.564778  0.9605230  1.559493          
  0.29700826  3             1.560702  0.9927537  1.559727          
  0.57701384  3             1.560702  0.9927537  1.559727          
  0.58616638  3             1.560702  0.9927537  1.559727          
  0.58901856  3             1.560702  0.9927537  1.559727          
  0.62641267  2             1.564778  0.9605230  1.559493          
  0.72365888  1             1.583714  0.8172603  1.559535          
  0.73455407  2             1.564778  0.9605230  1.559493          
  0.73710487  1             1.583714  0.8172603  1.559535          
  0.74525822  2             1.564778  0.9605230  1.559493          
  0.77906834  3             1.560702  0.9927537  1.559727          
  0.81770940  3             1.560702  0.9927537  1.559727          
  0.96437537  3             1.560702  0.9927537  1.559727          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were threshold = 0.06609158
 and n.components = 3.
[1] "Sun Mar 11 23:16:00 2018"
Error : 'x' should be a character matrix with a single column for string kernel methods
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                          "failed"                         
 [3] "Sun Mar 11 23:16:06 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "svmBoundrangeString"            
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                          "failed"                         
 [3] "Sun Mar 11 23:16:12 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "svmExpoString"                  
Support Vector Machines with Linear Kernel 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  C             RMSE        Rsquared   MAE         Selected
    0.03888812  0.02796972  0.9986641  0.02208144          
    0.06212741  0.02706930  0.9987929  0.02149522          
    0.10434722  0.02600647  0.9990075  0.02105369  *       
    0.10593709  0.02600647  0.9990075  0.02105369          
    0.20636174  0.02600647  0.9990075  0.02105369          
    0.21309682  0.02600647  0.9990075  0.02105369          
    0.28388943  0.02600647  0.9990075  0.02105369          
    0.39892545  0.02600647  0.9990075  0.02105369          
    0.68545024  0.02600647  0.9990075  0.02105369          
   12.59874103  0.02600647  0.9990075  0.02105369          
   13.85654457  0.02600647  0.9990075  0.02105369          
   14.27361001  0.02600647  0.9990075  0.02105369          
   21.05644617  0.02600647  0.9990075  0.02105369          
   57.87559214  0.02600647  0.9990075  0.02105369          
   64.81747840  0.02600647  0.9990075  0.02105369          
   66.55951764  0.02600647  0.9990075  0.02105369          
   72.44796818  0.02600647  0.9990075  0.02105369          
  102.96606046  0.02600647  0.9990075  0.02105369          
  153.87776347  0.02600647  0.9990075  0.02105369          
  707.03146752  0.02600647  0.9990075  0.02105369          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was C = 0.1043472.
[1] "Sun Mar 11 23:16:22 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Linear Kernel 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  cost          RMSE        Rsquared   MAE         Selected
    0.03888812  0.02796972  0.9986641  0.02208144          
    0.06212741  0.02706930  0.9987929  0.02149522          
    0.10434722  0.02600647  0.9990075  0.02105369  *       
    0.10593709  0.02600647  0.9990075  0.02105369          
    0.20636174  0.02600647  0.9990075  0.02105369          
    0.21309682  0.02600647  0.9990075  0.02105369          
    0.28388943  0.02600647  0.9990075  0.02105369          
    0.39892545  0.02600647  0.9990075  0.02105369          
    0.68545024  0.02600647  0.9990075  0.02105369          
   12.59874103  0.02600647  0.9990075  0.02105369          
   13.85654457  0.02600647  0.9990075  0.02105369          
   14.27361001  0.02600647  0.9990075  0.02105369          
   21.05644617  0.02600647  0.9990075  0.02105369          
   57.87559214  0.02600647  0.9990075  0.02105369          
   64.81747840  0.02600647  0.9990075  0.02105369          
   66.55951764  0.02600647  0.9990075  0.02105369          
   72.44796818  0.02600647  0.9990075  0.02105369          
  102.96606046  0.02600647  0.9990075  0.02105369          
  153.87776347  0.02600647  0.9990075  0.02105369          
  707.03146752  0.02600647  0.9990075  0.02105369          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was cost = 0.1043472.
[1] "Sun Mar 11 23:16:30 2018"
L2 Regularized Support Vector Machine (dual) with Linear Kernel 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  cost          Loss  RMSE        Rsquared   MAE         Selected
  1.307142e-03  L1    0.17565282  0.9366565  0.13695995          
  2.441243e-03  L2    0.07822367  0.9872284  0.06340884          
  4.873877e-03  L1    0.14250640  0.9565879  0.10763677          
  4.973141e-03  L2    0.06689003  0.9909648  0.05628738          
  1.209871e-02  L2    0.05692495  0.9935567  0.04761618          
  1.262804e-02  L2    0.05647778  0.9936583  0.04747064          
  1.851110e-02  L1    0.10423757  0.9745201  0.08496772          
  2.913549e-02  L2    0.05070208  0.9949805  0.04135098          
  5.996087e-02  L2    0.04713197  0.9955002  0.03798738          
  2.908503e+00  L2    0.04725770  0.9955722  0.03778315          
  3.301971e+00  L2    0.04779280  0.9954254  0.03928080          
  3.435145e+00  L2    0.04731568  0.9956625  0.03779927          
  5.768729e+00  L2    0.04701632  0.9955307  0.03838281  *       
  2.221062e+01  L1    0.10193769  0.9761184  0.08411632          
  2.583189e+01  L2    0.04733929  0.9954268  0.03873475          
  2.676169e+01  L1    0.10193762  0.9761185  0.08411633          
  2.996413e+01  L2    0.04763165  0.9954474  0.03875656          
  4.788051e+01  L2    0.04748468  0.9955300  0.03829656          
  8.180902e+01  L2    0.04730488  0.9955719  0.03864751          
  6.249112e+02  L2    0.04733049  0.9955215  0.03819413          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were cost = 5.768729 and Loss = L2.
[1] "Sun Mar 11 23:16:41 2018"
Support Vector Machines with Polynomial Kernel 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  degree  scale         C             RMSE        Rsquared   MAE       
  1       1.554365e-05    1.24442536  0.61739979  0.8986007  0.50918991
  1       7.265838e-05    0.46576134  0.59670931  0.9007898  0.49254614
  1       2.586963e-04    1.68398413  0.19788633  0.9634668  0.16343645
  1       8.511194e-03    0.48566573  0.04075642  0.9973895  0.03154572
  1       1.221846e-02    0.07384854  0.08541484  0.9908988  0.06951387
  1       5.327907e-02   43.17358277  0.02599905  0.9990097  0.02106805
  1       3.298038e-01    0.82477483  0.02598247  0.9990082  0.02103213
  1       3.726077e-01    0.83754761  0.02596944  0.9990101  0.02102960
  1       1.386789e+00    6.52107217  0.02600060  0.9990077  0.02104899
  2       4.892064e-03   75.16978515  0.03684149  0.9991344  0.03373268
  2       5.258109e-02    0.68007224  0.03368889  0.9984451  0.02881174
  2       1.929021e-01    0.09448019  0.03562466  0.9981043  0.03035951
  2       1.946990e-01    7.26316439  0.03536605  0.9981382  0.03011805
  3       1.725785e-05    5.44833392  0.30247711  0.9376640  0.25208813
  3       9.990193e-05    1.68864713  0.16475700  0.9723445  0.13550668
  3       1.381383e-02    0.16284801  0.03791764  0.9984377  0.03196520
  3       2.368891e-02    0.12588929  0.03943039  0.9982863  0.03404375
  3       1.556209e-01  136.40323495  0.04119809  0.9974885  0.03620113
  3       1.817109e-01   12.24435808  0.04131935  0.9974635  0.03639463
  3       9.174756e-01   69.65676834  0.03810710  0.9969969  0.03219160
  Selected
          
          
          
          
          
          
          
  *       
          
          
          
          
          
          
          
          
          
          
          
          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were degree = 1, scale = 0.3726077 and C
 = 0.8375476.
[1] "Sun Mar 11 23:16:52 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Support Vector Machines with Radial Basis Function Kernel 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  sigma       C            RMSE        Rsquared   MAE         Selected
  0.01483872  198.6024899  0.08103502  0.9876297  0.05525728          
  0.01696558   41.4219593  0.08103502  0.9876297  0.05525728          
  0.04300049   79.9992944  0.08103502  0.9876297  0.05525728          
  0.04416450    6.5725868  0.08103502  0.9876297  0.05525728          
  0.05572696   77.7400453  0.08103502  0.9876297  0.05525728          
  0.08604015   21.8805198  0.08103502  0.9876297  0.05525728          
  0.12159755    3.4232550  0.08103502  0.9876297  0.05525728  *       
  0.20398120  449.0184993  0.08103502  0.9876297  0.05525728          
  0.23368460   28.3254705  0.08103502  0.9876297  0.05525728          
  0.26839102   42.2356132  0.08103502  0.9876297  0.05525728          
  0.27662002    2.1141812  0.08106623  0.9876073  0.05525761          
  0.28040981    0.1542330  0.20128250  0.9354169  0.11389189          
  0.33324325    0.9428070  0.08841375  0.9858684  0.05779778          
  0.39262482   11.4169624  0.08103502  0.9876297  0.05525728          
  0.48969116   30.9133416  0.08103502  0.9876297  0.05525728          
  0.81849243   79.6818204  0.08103502  0.9876297  0.05525728          
  1.05057421  115.9270388  0.08103502  0.9876297  0.05525728          
  1.20228160   14.6089959  0.08103502  0.9876297  0.05525728          
  2.03949294    0.2549662  0.15156296  0.9614279  0.08541869          
  2.22612213  568.5960191  0.08103502  0.9876297  0.05525728          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were sigma = 0.1215975 and C = 3.423255.
[1] "Sun Mar 11 23:17:02 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  C             RMSE        Rsquared   MAE         Selected
    0.03888812  0.41116659  0.7833153  0.28022040          
    0.06212741  0.33029077  0.8455637  0.20903014          
    0.10434722  0.25041434  0.9038278  0.14705883          
    0.10593709  0.24828176  0.9052151  0.14549618          
    0.20636174  0.16966471  0.9523185  0.09501694          
    0.21309682  0.16631458  0.9538960  0.09317393          
    0.28388943  0.14279937  0.9656103  0.08090417          
    0.39892545  0.12135162  0.9747172  0.07021466          
    0.68545024  0.09705934  0.9834063  0.06058074          
   12.59874103  0.08103502  0.9876297  0.05525728  *       
   13.85654457  0.08103502  0.9876297  0.05525728          
   14.27361001  0.08103502  0.9876297  0.05525728          
   21.05644617  0.08103502  0.9876297  0.05525728          
   57.87559214  0.08103502  0.9876297  0.05525728          
   64.81747840  0.08103502  0.9876297  0.05525728          
   66.55951764  0.08103502  0.9876297  0.05525728          
   72.44796818  0.08103502  0.9876297  0.05525728          
  102.96606046  0.08103502  0.9876297  0.05525728          
  153.87776347  0.08103502  0.9876297  0.05525728          
  707.03146752  0.08103502  0.9876297  0.05525728          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was C = 12.59874.
[1] "Sun Mar 11 23:17:12 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  sigma       C            RMSE        Rsquared   MAE         Selected
  0.01483872  198.6024899  0.08103502  0.9876297  0.05525728          
  0.01696558   41.4219593  0.08103502  0.9876297  0.05525728          
  0.04300049   79.9992944  0.08103502  0.9876297  0.05525728          
  0.04416450    6.5725868  0.08103502  0.9876297  0.05525728          
  0.05572696   77.7400453  0.08103502  0.9876297  0.05525728          
  0.08604015   21.8805198  0.08103502  0.9876297  0.05525728          
  0.12159755    3.4232550  0.08103502  0.9876297  0.05525728  *       
  0.20398120  449.0184993  0.08103502  0.9876297  0.05525728          
  0.23368460   28.3254705  0.08103502  0.9876297  0.05525728          
  0.26839102   42.2356132  0.08103502  0.9876297  0.05525728          
  0.27662002    2.1141812  0.08106623  0.9876073  0.05525761          
  0.28040981    0.1542330  0.20128250  0.9354169  0.11389189          
  0.33324325    0.9428070  0.08841375  0.9858684  0.05779778          
  0.39262482   11.4169624  0.08103502  0.9876297  0.05525728          
  0.48969116   30.9133416  0.08103502  0.9876297  0.05525728          
  0.81849243   79.6818204  0.08103502  0.9876297  0.05525728          
  1.05057421  115.9270388  0.08103502  0.9876297  0.05525728          
  1.20228160   14.6089959  0.08103502  0.9876297  0.05525728          
  2.03949294    0.2549662  0.15156296  0.9614279  0.08541869          
  2.22612213  568.5960191  0.08103502  0.9876297  0.05525728          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were sigma = 0.1215975 and C = 3.423255.
[1] "Sun Mar 11 23:17:21 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                          "failed"                         
 [3] "Sun Mar 11 23:17:27 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "svmSpectrumString"              
Bagged CART 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results:

  RMSE       Rsquared   MAE      
  0.1262115  0.9623243  0.1006572

[1] "Sun Mar 11 23:17:34 2018"
Partial Least Squares 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  ncomp  RMSE         Rsquared   MAE          Selected
  1      0.228134712  0.8752981  0.175878213          
  2      0.111305657  0.9704956  0.089091896          
  3      0.049264528  0.9942218  0.039108529          
  5      0.014069671  0.9995287  0.010711682          
  6      0.008063394  0.9998432  0.006131154          
  7      0.007915838  0.9998494  0.006122313          
  8      0.007824483  0.9998532  0.006034916  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 8.
[1] "Sun Mar 11 23:19:15 2018"
Wang and Mendel Fuzzy Rules 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  num.labels  type.mf    RMSE        Rsquared   MAE         Selected
   2          GAUSSIAN   0.51852383  0.8498245  0.48794820          
   3          TRIANGLE   0.18152619  0.9257821  0.14435494          
   4          GAUSSIAN   0.14832080  0.9493782  0.11770635          
   4          TRIANGLE   0.15780371  0.9408589  0.12011105          
   5          TRAPEZOID  0.16160475  0.9637138  0.13290068          
   5          TRIANGLE   0.13790965  0.9546571  0.10186320          
   6          GAUSSIAN   0.10509365  0.9741873  0.08201792          
   6          TRAPEZOID  0.14248933  0.9750930  0.11839111          
   7          TRIANGLE   0.11537648  0.9673785  0.08315499          
  12          TRIANGLE   0.16333681  0.9361196  0.08841742          
  13          TRAPEZOID  0.15170739  0.9495717  0.09448860          
  13          TRIANGLE   0.17140790  0.9272982  0.09559550          
  15          GAUSSIAN   0.08174168  0.9839951  0.06316303          
  15          TRAPEZOID  0.18423903  0.9227516  0.10881566          
  16          GAUSSIAN   0.07775150  0.9855998  0.06216908  *       
  16          TRAPEZOID  0.22867900  0.8745987  0.12095828          
  16          TRIANGLE   0.26372520  0.8329175  0.13745913          
  17          TRIANGLE   0.26904862  0.8232070  0.13963603          
  20          TRIANGLE   0.36381849  0.6811836  0.20562322          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were num.labels = 16 and type.mf = GAUSSIAN.
[1] "Mon Mar 12 00:07:17 2018"

Attaching package: 'xgboost'

The following object is masked from 'package:dplyr':

    slice

eXtreme Gradient Boosting 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  lambda        alpha         nrounds  eta        RMSE        Rsquared 
  1.273966e-05  2.150612e-04  39       1.9930254  0.03990507  0.9962173
  2.140217e-05  1.826742e-01  32       2.4606498  0.04175000  0.9958635
  3.800331e-05  1.515916e-05  36       1.7316336  0.03997063  0.9962138
  3.864499e-05  3.272927e-02  70       1.1152396  0.03968026  0.9962389
  8.086287e-05  5.802363e-03  27       2.9265258  0.04010951  0.9962175
  8.379028e-05  2.049575e-01  32       0.8815348  0.04223353  0.9958091
  1.151155e-04  6.491959e-05  26       1.1598674  0.04031114  0.9961543
  1.677762e-04  8.160442e-03   9       0.6263118  0.06943221  0.9953465
  3.055212e-04  7.079629e-01  52       0.1431706  0.05081895  0.9938211
  7.674838e-03  1.101502e-01  11       0.7115683  0.05418294  0.9949870
  8.527699e-03  3.232470e-02  30       0.1623218  0.03989600  0.9961920
  8.812371e-03  1.111177e-01  53       2.9002008  0.04070576  0.9959985
  1.355387e-02  3.441627e-03  75       1.1573802  0.04056892  0.9961018
  4.152354e-02  8.766193e-05  39       2.0422405  0.03906732  0.9964139
  4.707283e-02  9.161886e-03  16       0.6120398  0.04059543  0.9961828
  4.847573e-02  1.673127e-05  50       1.5513728  0.03940715  0.9963265
  5.324651e-02  1.523753e-02  14       2.3974698  0.04209890  0.9961027
  7.858537e-02  8.995236e-02  81       0.7096658  0.04055665  0.9960525
  1.226160e-01  4.794941e-01  75       1.8951558  0.05033075  0.9939810
  6.635549e-01  1.041125e-01  58       0.9274663  0.04085136  0.9960459
  MAE         Selected
  0.02964972          
  0.03032039          
  0.02955608          
  0.02911361          
  0.03003142          
  0.03044432          
  0.02980391          
  0.05520100          
  0.03682681          
  0.04143406          
  0.02926883          
  0.02973935          
  0.02993553          
  0.02823693  *       
  0.02972390          
  0.02887137          
  0.03079211          
  0.03022016          
  0.03726476          
  0.03047304          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 39, lambda =
 0.04152354, alpha = 8.766193e-05 and eta = 2.042241.
[1] "Mon Mar 12 00:07:36 2018"
eXtreme Gradient Boosting 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  eta         max_depth  gamma      colsample_bytree  min_child_weight
  0.05054582   6         2.0877060  0.6874463          3              
  0.06473982   9         2.3718944  0.3119612          1              
  0.08127509   7         7.9915659  0.5748025         17              
  0.09610515   6         2.0401326  0.5483215         20              
  0.15664667   2         3.8662248  0.3955257          4              
  0.15905755   6         9.7550862  0.6620779         16              
  0.17845415   8         0.5410728  0.5215509         17              
  0.18956808   9         8.2021659  0.5749910         19              
  0.19045344   9         2.9384493  0.6203642          6              
  0.21326482   1         5.7721120  0.4857247          2              
  0.23069168   3         6.6434179  0.5902315          5              
  0.23085099   2         6.8074684  0.6518726          7              
  0.29833622   1         5.1712427  0.4097254          5              
  0.30869069  10         0.4772353  0.3116056         14              
  0.31489989   9         9.6673360  0.3179525          3              
  0.34498754   9         3.0915542  0.3451696         10              
  0.41758760   8         3.7174654  0.3828630          6              
  0.44514619  10         6.3171859  0.5660546          0              
  0.44953443   6         3.8579341  0.6919721         11              
  0.48386325   8         2.3655526  0.3600199          9              
  subsample  nrounds  RMSE        Rsquared   MAE         Selected
  0.3735994  245      0.14296295  0.9794443  0.09676966          
  0.6482935  578      0.14559302  0.9669169  0.10550272          
  0.8279910  746      0.20556940  0.9621627  0.13787709          
  0.6490315  735      0.12225764  0.9797260  0.08162705          
  0.9865624  213      0.15070442  0.9726935  0.10112760          
  0.6454090  182      0.23191322  0.9415616  0.15875716          
  0.7810665  587      0.08173653  0.9873491  0.05944727  *       
  0.9057130   67      0.20165652  0.9504778  0.13547855          
  0.4830348  185      0.14618600  0.9746924  0.09980313          
  0.7376292  116      0.17808045  0.9524856  0.12463820          
  0.3631492   22      0.24928236  0.9240691  0.17831376          
  0.6882544  724      0.18631963  0.9609462  0.12951026          
  0.8528642  738      0.16955502  0.9496055  0.12121924          
  0.8323497  298      0.10011407  0.9772705  0.07681745          
  0.9873000  590      0.19909661  0.9351471  0.14180995          
  0.6886767  965      0.14728784  0.9549128  0.10792064          
  0.5497066  118      0.20209099  0.9099867  0.15452151          
  0.4758577  818      0.20655842  0.9234270  0.15124194          
  0.8791228  627      0.15290446  0.9553183  0.11233139          
  0.4722353  780      0.15457087  0.9457175  0.11871705          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 587, max_depth = 8, eta
 = 0.1784541, gamma = 0.5410728, colsample_bytree = 0.5215509,
 min_child_weight = 17 and subsample = 0.7810665.
[1] "Mon Mar 12 00:10:08 2018"
Self-Organizing Maps 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 503 
Resampling results across tuning parameters:

  xdim  ydim  topo         user.weights  RMSE       Rsquared   MAE       
   1     2    rectangular  0.26711090    0.4106915  0.6004337  0.33451417
   1     8    rectangular  0.53857658    0.2191024  0.8850871  0.17810948
   2     2    rectangular  0.39258962    0.3083130  0.7729344  0.25317604
   3     8    rectangular  0.76978704    0.1558358  0.9424793  0.12548697
   3    11    hexagonal    0.55344734    0.1489415  0.9473349  0.12025192
   4     8    hexagonal    0.22526694    0.1518143  0.9452017  0.12105230
   4    10    rectangular  0.69647782    0.1372174  0.9552848  0.11189315
   4    20    hexagonal    0.96662918    0.1245149  0.9632303  0.10008759
   5     9    rectangular  0.11998055    0.1448483  0.9507161  0.11805261
   5    14    rectangular  0.49266344    0.1238980  0.9632892  0.09895969
   6    14    rectangular  0.60833686    0.1215123  0.9651268  0.09646952
  12    13    hexagonal    0.94588930    0.1098625  0.9719671  0.08637381
  12    17    rectangular  0.87518600    0.1071562  0.9740384  0.08327385
  12    19    hexagonal    0.68972641    0.1080059  0.9734354  0.08437044
  12    20    hexagonal    0.10856487    0.1010254  0.9762304  0.07933286
  13    15    hexagonal    0.05057702    0.1022105  0.9758367  0.08136716
  15    15    rectangular  0.75324286    0.1066032  0.9744244  0.08271425
  15    17    rectangular  0.64972764    0.1025877  0.9763410  0.08102126
  16    16    hexagonal    0.86490760    0.1054470  0.9751141  0.08074115
  16    19    hexagonal    0.98090389    0.1102357  0.9735854  0.08630627
  Selected
          
          
          
          
          
          
          
          
          
          
          
          
          
          
  *       
          
          
          
          
          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were xdim = 12, ydim = 20, user.weights
 = 0.1085649 and topo = hexagonal.
[1] "Mon Mar 12 00:10:39 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: There were 20 warnings (use warnings() to see them)
Fitting Repeat 1 

# weights:  188
initial  value 712.434102 
iter  10 value 391.863961
iter  20 value 389.277843
iter  30 value 389.262460
final  value 389.262329 
converged
Fitting Repeat 2 

# weights:  188
initial  value 793.465406 
iter  10 value 401.735394
iter  20 value 389.406389
iter  30 value 389.263063
final  value 389.262328 
converged
Fitting Repeat 3 

# weights:  188
initial  value 1582.886586 
iter  10 value 396.494116
iter  20 value 389.963786
iter  30 value 389.275584
iter  40 value 389.262574
final  value 389.262328 
converged
Fitting Repeat 4 

# weights:  188
initial  value 1474.422640 
iter  10 value 411.968087
iter  20 value 389.453798
iter  30 value 389.263438
final  value 389.262330 
converged
Fitting Repeat 5 

# weights:  188
initial  value 1297.045397 
iter  10 value 437.817044
iter  20 value 391.326573
iter  30 value 389.418218
iter  40 value 389.267741
iter  50 value 389.262393
final  value 389.262328 
converged
Fitting Repeat 1 

# weights:  188
initial  value 636.546107 
iter  10 value 367.040325
iter  20 value 364.486974
iter  30 value 363.823349
iter  40 value 363.700498
iter  50 value 363.636898
iter  60 value 353.340893
iter  70 value 348.525646
iter  80 value 348.062644
iter  90 value 347.955424
iter 100 value 347.919039
final  value 347.919039 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 504.439837 
iter  10 value 364.988247
iter  20 value 364.332381
iter  30 value 351.358006
iter  40 value 348.377614
iter  50 value 348.082709
iter  60 value 347.956959
iter  70 value 347.905524
iter  80 value 347.887259
iter  90 value 347.878106
iter 100 value 347.874261
final  value 347.874261 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 435.349366 
iter  10 value 364.102690
iter  20 value 363.717192
iter  30 value 363.689844
iter  40 value 363.556967
iter  50 value 350.770632
iter  60 value 348.260475
iter  70 value 348.036796
iter  80 value 347.966745
iter  90 value 347.916992
iter 100 value 347.893277
final  value 347.893277 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 453.384322 
iter  10 value 364.367453
iter  20 value 354.333506
iter  30 value 348.968275
iter  40 value 348.280109
iter  50 value 348.059419
iter  60 value 347.960505
iter  70 value 347.918150
iter  80 value 347.894241
iter  90 value 347.880755
iter 100 value 347.870029
final  value 347.870029 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 1264.245017 
iter  10 value 363.871755
iter  20 value 363.692326
iter  30 value 362.838770
iter  40 value 348.878923
iter  50 value 348.067324
iter  60 value 347.997869
iter  70 value 347.958529
iter  80 value 347.929737
iter  90 value 347.910080
iter 100 value 347.900123
final  value 347.900123 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  199
initial  value 941.450216 
iter  10 value 403.602554
iter  20 value 395.532399
iter  30 value 395.447395
iter  40 value 395.443926
iter  50 value 395.437486
iter  60 value 395.428426
iter  70 value 394.623347
iter  80 value 382.127882
iter  90 value 378.335785
iter 100 value 377.995993
final  value 377.995993 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 773.526895 
iter  10 value 352.500582
iter  20 value 345.412781
iter  30 value 345.346215
iter  40 value 345.335235
iter  50 value 345.329881
iter  60 value 345.322704
iter  70 value 345.249455
iter  80 value 330.815826
iter  90 value 328.460397
iter 100 value 328.183380
final  value 328.183380 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 644.085195 
iter  10 value 317.405580
iter  20 value 312.177058
iter  30 value 312.121801
iter  40 value 306.962178
iter  50 value 300.701876
iter  60 value 296.429075
iter  70 value 294.067163
iter  80 value 293.668368
iter  90 value 293.574828
iter 100 value 293.533999
final  value 293.533999 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 473.166111 
iter  10 value 383.406313
iter  20 value 382.677063
iter  30 value 382.669381
iter  40 value 382.661731
iter  50 value 382.613663
iter  60 value 368.715539
iter  70 value 365.843541
iter  80 value 365.559585
iter  90 value 365.504543
iter 100 value 365.487143
final  value 365.487143 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 1103.976876 
iter  10 value 373.071607
iter  20 value 365.960974
iter  30 value 365.885935
iter  40 value 365.884992
iter  50 value 365.871427
iter  60 value 365.814948
iter  70 value 355.572272
iter  80 value 352.943085
iter  90 value 352.405176
iter 100 value 352.329753
final  value 352.329753 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  133
initial  value 1036.031615 
iter  10 value 364.545039
iter  20 value 362.909777
iter  30 value 353.057125
iter  40 value 352.069485
iter  50 value 351.671237
iter  60 value 351.490120
iter  70 value 351.470579
iter  80 value 351.469716
final  value 351.469695 
converged
Fitting Repeat 2 

# weights:  133
initial  value 815.953226 
iter  10 value 371.337839
iter  20 value 363.038682
iter  30 value 353.623794
iter  40 value 352.165598
iter  50 value 351.724680
iter  60 value 351.455920
iter  70 value 351.421236
iter  80 value 351.404754
iter  90 value 351.401090
final  value 351.401005 
converged
Fitting Repeat 3 

# weights:  133
initial  value 669.947009 
iter  10 value 365.767206
iter  20 value 362.912077
iter  30 value 352.993174
iter  40 value 351.761480
iter  50 value 351.486704
iter  60 value 351.424908
iter  70 value 351.405044
iter  80 value 351.401561
iter  90 value 351.401035
final  value 351.401016 
converged
Fitting Repeat 4 

# weights:  133
initial  value 717.540399 
iter  10 value 365.362846
iter  20 value 363.213784
iter  30 value 353.274904
iter  40 value 352.337973
iter  50 value 351.713298
iter  60 value 351.616658
iter  70 value 351.604536
iter  80 value 351.604098
final  value 351.604082 
converged
Fitting Repeat 5 

# weights:  133
initial  value 986.826137 
iter  10 value 365.444793
iter  20 value 358.476595
iter  30 value 352.997541
iter  40 value 352.746934
iter  50 value 352.676538
iter  60 value 352.012109
iter  70 value 351.677205
iter  80 value 351.526011
iter  90 value 351.478522
iter 100 value 351.472981
final  value 351.472981 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  210
initial  value 880.767592 
iter  10 value 364.934845
iter  20 value 364.074698
iter  30 value 363.787011
iter  40 value 363.695593
iter  50 value 363.671115
iter  60 value 363.640101
iter  70 value 352.930860
iter  80 value 349.410709
iter  90 value 347.815112
iter 100 value 347.718313
final  value 347.718313 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  210
initial  value 820.526566 
iter  10 value 373.848963
iter  20 value 363.780297
iter  30 value 363.676899
iter  40 value 363.646630
iter  50 value 350.712709
iter  60 value 348.017951
iter  70 value 347.783055
iter  80 value 347.702046
iter  90 value 347.667303
iter 100 value 347.642899
final  value 347.642899 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  210
initial  value 710.730321 
iter  10 value 355.606201
iter  20 value 349.056878
iter  30 value 348.092756
iter  40 value 347.967883
iter  50 value 347.797672
iter  60 value 347.733571
iter  70 value 347.682728
iter  80 value 347.659476
iter  90 value 347.642789
iter 100 value 347.633819
final  value 347.633819 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  210
initial  value 837.451867 
iter  10 value 364.764933
iter  20 value 363.995757
iter  30 value 363.837851
iter  40 value 352.440024
iter  50 value 348.429288
iter  60 value 347.830003
iter  70 value 347.715958
iter  80 value 347.677299
iter  90 value 347.645880
iter 100 value 347.632482
final  value 347.632482 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  210
initial  value 1197.464898 
iter  10 value 373.143730
iter  20 value 363.772166
iter  30 value 363.670975
iter  40 value 363.628180
iter  50 value 352.353233
iter  60 value 348.793025
iter  70 value 347.913554
iter  80 value 347.789573
iter  90 value 347.729652
iter 100 value 347.683368
final  value 347.683368 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 513.623522 
iter  10 value 364.844303
iter  20 value 354.745013
iter  30 value 348.832992
iter  40 value 348.260445
iter  50 value 347.996989
iter  60 value 347.862551
iter  70 value 347.821613
iter  80 value 347.796739
iter  90 value 347.785116
iter 100 value 347.780479
final  value 347.780479 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  177
initial  value 575.152362 
iter  10 value 365.409938
iter  20 value 364.361864
iter  30 value 363.866684
iter  40 value 363.695452
iter  50 value 363.659838
iter  60 value 359.634653
iter  70 value 350.764464
iter  80 value 348.070032
iter  90 value 347.872728
iter 100 value 347.821464
final  value 347.821464 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  177
initial  value 812.592204 
iter  10 value 366.106408
iter  20 value 364.240307
iter  30 value 363.801192
iter  40 value 363.686891
iter  50 value 355.119145
iter  60 value 348.636658
iter  70 value 347.991544
iter  80 value 347.908471
iter  90 value 347.858706
iter 100 value 347.832189
final  value 347.832189 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  177
initial  value 679.641120 
iter  10 value 365.540366
iter  20 value 364.114480
iter  30 value 363.796334
iter  40 value 363.736978
iter  50 value 350.925906
iter  60 value 348.241310
iter  70 value 347.915310
iter  80 value 347.846002
iter  90 value 347.820970
iter 100 value 347.806070
final  value 347.806070 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  177
initial  value 723.942246 
iter  10 value 366.228033
iter  20 value 364.135994
iter  30 value 354.615519
iter  40 value 348.999246
iter  50 value 348.036287
iter  60 value 347.888299
iter  70 value 347.823570
iter  80 value 347.801197
iter  90 value 347.790419
iter 100 value 347.786578
final  value 347.786578 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  78
initial  value 868.630017 
iter  10 value 364.246303
iter  20 value 363.179472
iter  30 value 354.436930
iter  40 value 349.025732
iter  50 value 348.441501
iter  60 value 348.258452
iter  70 value 348.208558
iter  80 value 348.177400
iter  90 value 348.158586
iter 100 value 348.135070
final  value 348.135070 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  78
initial  value 662.989880 
iter  10 value 367.056739
iter  20 value 364.148633
iter  30 value 363.835980
iter  40 value 363.749580
iter  50 value 360.861607
iter  60 value 349.712693
iter  70 value 348.207664
iter  80 value 348.150047
iter  90 value 348.133645
iter 100 value 348.128649
final  value 348.128649 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  78
initial  value 1237.771623 
iter  10 value 365.100792
iter  20 value 363.760869
iter  30 value 363.322140
iter  40 value 351.415156
iter  50 value 348.631248
iter  60 value 348.269153
iter  70 value 348.194687
iter  80 value 348.175732
iter  90 value 348.159155
iter 100 value 348.143902
final  value 348.143902 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  78
initial  value 941.965071 
iter  10 value 367.308169
iter  20 value 364.200622
iter  30 value 363.769864
iter  40 value 352.832608
iter  50 value 348.655059
iter  60 value 348.309027
iter  70 value 348.161017
iter  80 value 348.144102
iter  90 value 348.138010
iter 100 value 348.131378
final  value 348.131378 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  78
initial  value 696.846643 
iter  10 value 366.624183
iter  20 value 364.034676
iter  30 value 363.734317
iter  40 value 352.255827
iter  50 value 348.725040
iter  60 value 348.281104
iter  70 value 348.184820
iter  80 value 348.159340
iter  90 value 348.145691
iter 100 value 348.138388
final  value 348.138388 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  221
initial  value 723.893220 
iter  10 value 367.590102
iter  20 value 363.708137
iter  30 value 363.668404
iter  40 value 363.662427
iter  50 value 363.660413
iter  60 value 363.655703
iter  70 value 363.625358
iter  80 value 350.689287
iter  90 value 348.154532
iter 100 value 347.696848
final  value 347.696848 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  221
initial  value 762.140273 
iter  10 value 370.157393
iter  20 value 363.737736
iter  30 value 363.668956
iter  40 value 363.663596
iter  50 value 363.662837
iter  60 value 363.661768
iter  70 value 363.659982
iter  80 value 363.656062
iter  90 value 363.639894
iter 100 value 354.405182
final  value 354.405182 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  221
initial  value 633.639829 
iter  10 value 366.576856
iter  20 value 363.696455
iter  30 value 363.670586
iter  40 value 363.667220
iter  50 value 363.657675
iter  60 value 353.019742
iter  70 value 348.791699
iter  80 value 347.699051
iter  90 value 347.601461
iter 100 value 347.577730
final  value 347.577730 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  221
initial  value 717.702131 
iter  10 value 368.317114
iter  20 value 363.716519
iter  30 value 363.665814
iter  40 value 363.662989
iter  50 value 363.662238
iter  60 value 363.660995
iter  70 value 363.658528
iter  80 value 363.651409
iter  90 value 363.559893
iter 100 value 351.450635
final  value 351.450635 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  221
initial  value 672.919268 
iter  10 value 367.858045
iter  20 value 363.711226
iter  30 value 363.667026
iter  40 value 363.664401
iter  50 value 352.025386
iter  60 value 348.409982
iter  70 value 347.764378
iter  80 value 347.624744
iter  90 value 347.566345
iter 100 value 347.553315
final  value 347.553315 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  144
initial  value 826.400718 
iter  10 value 350.919868
iter  20 value 347.891389
final  value 347.875152 
converged
Fitting Repeat 2 

# weights:  144
initial  value 807.402590 
iter  10 value 364.323597
iter  20 value 360.515839
iter  30 value 356.162657
iter  40 value 356.099109
final  value 356.098902 
converged
Fitting Repeat 3 

# weights:  144
initial  value 1134.549128 
iter  10 value 392.956088
iter  20 value 391.157528
final  value 391.156720 
converged
Fitting Repeat 4 

# weights:  144
initial  value 480.129861 
iter  10 value 377.248790
iter  20 value 375.089449
final  value 375.087716 
converged
Fitting Repeat 5 

# weights:  144
initial  value 1174.228183 
iter  10 value 400.152253
iter  20 value 374.798542
iter  30 value 374.535836
final  value 374.532367 
converged
Fitting Repeat 1 

# weights:  56
initial  value 748.965653 
iter  10 value 349.736377
iter  20 value 347.859348
iter  30 value 347.851098
iter  40 value 347.847334
iter  50 value 347.844900
iter  60 value 347.830424
iter  70 value 337.254201
iter  80 value 330.940311
iter  90 value 330.668312
iter 100 value 330.600813
final  value 330.600813 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 823.569185 
iter  10 value 327.961509
iter  20 value 326.312869
iter  30 value 326.294048
iter  40 value 326.289014
iter  50 value 326.284065
iter  60 value 326.246831
iter  70 value 313.433646
iter  80 value 310.753182
iter  90 value 310.587493
iter 100 value 310.546503
final  value 310.546503 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 621.861601 
iter  10 value 362.620629
iter  20 value 360.960201
iter  30 value 360.947875
iter  40 value 360.933193
iter  50 value 353.636346
iter  60 value 348.622564
iter  70 value 345.789363
iter  80 value 345.282207
iter  90 value 345.188743
iter 100 value 345.153378
final  value 345.153378 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 542.389199 
iter  10 value 352.179411
iter  20 value 351.031825
iter  30 value 351.022343
iter  40 value 351.014330
iter  50 value 351.006437
iter  60 value 350.907699
iter  70 value 339.279660
iter  80 value 334.224363
iter  90 value 334.024047
iter 100 value 333.999350
final  value 333.999350 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 827.942845 
iter  10 value 348.257752
iter  20 value 346.367675
iter  30 value 346.356921
iter  40 value 346.347914
iter  50 value 346.336921
iter  60 value 346.242071
iter  70 value 331.962113
iter  80 value 329.870075
iter  90 value 329.769890
iter 100 value 329.745600
final  value 329.745600 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  67
initial  value 899.064292 
iter  10 value 377.001366
iter  20 value 375.450575
iter  30 value 375.440136
iter  40 value 375.437798
iter  50 value 375.434535
iter  60 value 375.433823
iter  70 value 375.432722
iter  80 value 375.430829
iter  90 value 375.426991
iter 100 value 375.416337
final  value 375.416337 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  67
initial  value 969.548167 
iter  10 value 382.918140
iter  20 value 382.098400
iter  30 value 382.093620
final  value 382.093528 
converged
Fitting Repeat 3 

# weights:  67
initial  value 831.109967 
iter  10 value 394.015480
iter  20 value 392.869230
iter  30 value 392.861145
final  value 392.861117 
converged
Fitting Repeat 4 

# weights:  67
initial  value 555.009319 
iter  10 value 364.204394
iter  20 value 363.894432
iter  30 value 363.891342
iter  40 value 363.888414
iter  50 value 363.887983
iter  60 value 363.887887
iter  70 value 363.887777
iter  80 value 363.887646
iter  90 value 363.887490
iter 100 value 363.887300
final  value 363.887300 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  67
initial  value 790.271791 
iter  10 value 383.392956
iter  20 value 382.081854
iter  30 value 382.071528
iter  40 value 382.069860
iter  50 value 382.069693
iter  60 value 382.069490
iter  70 value 382.069240
iter  80 value 382.068923
iter  90 value 382.068508
iter 100 value 382.067942
final  value 382.067942 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  78
initial  value 664.547167 
iter  10 value 371.855361
iter  20 value 367.059874
iter  30 value 366.550592
iter  40 value 364.093686
iter  50 value 363.684442
final  value 363.684304 
converged
Fitting Repeat 2 

# weights:  78
initial  value 848.068846 
iter  10 value 390.224414
iter  20 value 366.947790
iter  30 value 366.559434
iter  40 value 364.151015
iter  50 value 363.686385
final  value 363.684304 
converged
Fitting Repeat 3 

# weights:  78
initial  value 949.696462 
iter  10 value 368.285829
iter  20 value 365.172847
iter  30 value 361.127032
iter  40 value 360.911957
final  value 360.911945 
converged
Fitting Repeat 4 

# weights:  78
initial  value 658.369140 
iter  10 value 375.328607
iter  20 value 366.959508
iter  30 value 364.288368
iter  40 value 361.533913
final  value 361.510474 
converged
Fitting Repeat 5 

# weights:  78
initial  value 815.474724 
iter  10 value 372.294206
iter  20 value 366.783477
iter  30 value 366.549864
iter  40 value 366.546073
iter  50 value 364.125613
iter  60 value 363.685450
final  value 363.684304 
converged
Fitting Repeat 1 

# weights:  67
initial  value 795.357831 
iter  10 value 352.751763
iter  20 value 352.584522
final  value 352.583963 
converged
Fitting Repeat 2 

# weights:  67
initial  value 915.365071 
iter  10 value 372.151782
iter  20 value 372.061105
final  value 372.060211 
converged
Fitting Repeat 3 

# weights:  67
initial  value 1205.444106 
iter  10 value 377.649282
iter  20 value 377.577230
iter  30 value 377.572227
iter  40 value 377.571329
iter  40 value 377.571329
final  value 377.571329 
converged
Fitting Repeat 4 

# weights:  67
initial  value 760.869890 
iter  10 value 370.546819
iter  20 value 370.390925
final  value 370.390918 
converged
Fitting Repeat 5 

# weights:  67
initial  value 599.969731 
iter  10 value 378.034766
iter  20 value 377.956381
final  value 377.955058 
converged
Fitting Repeat 1 

# weights:  188
initial  value 972.056018 
iter  10 value 364.549317
iter  20 value 357.863528
iter  30 value 357.795259
iter  40 value 357.792848
iter  50 value 357.787962
iter  60 value 357.785862
iter  70 value 357.781204
iter  80 value 357.763073
iter  90 value 353.132258
iter 100 value 343.010823
final  value 343.010823 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 992.869726 
iter  10 value 397.893776
iter  20 value 386.298858
iter  30 value 386.174484
iter  40 value 386.171486
iter  50 value 386.167922
iter  60 value 383.197331
iter  70 value 370.931263
iter  80 value 370.417035
iter  90 value 370.297738
iter 100 value 370.259174
final  value 370.259174 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 675.337481 
iter  10 value 358.143563
iter  20 value 350.770207
iter  30 value 350.693738
iter  40 value 350.687303
iter  50 value 350.685515
iter  60 value 350.681833
iter  70 value 350.667883
iter  80 value 346.427384
iter  90 value 332.137805
iter 100 value 331.726515
final  value 331.726515 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 590.868889 
iter  10 value 342.638979
iter  20 value 337.196337
iter  30 value 337.140999
iter  40 value 337.139936
iter  50 value 337.139826
iter  60 value 337.139714
iter  70 value 337.139599
iter  80 value 337.139481
iter  90 value 337.139359
iter 100 value 337.139233
final  value 337.139233 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 1115.660959 
iter  10 value 371.078940
iter  20 value 363.534577
iter  30 value 363.455224
iter  40 value 363.448837
iter  50 value 363.427674
iter  60 value 356.904071
iter  70 value 349.126144
iter  80 value 348.639730
iter  90 value 348.601791
iter 100 value 348.575459
final  value 348.575459 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 866.583801 
iter  10 value 421.311217
iter  20 value 413.730087
iter  30 value 412.993088
iter  40 value 412.757378
iter  50 value 403.456177
iter  60 value 398.702793
iter  70 value 398.404842
iter  80 value 398.339733
iter  90 value 398.302592
iter 100 value 398.285222
final  value 398.285222 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 648.124511 
iter  10 value 353.230162
iter  20 value 353.089081
iter  30 value 342.997219
iter  40 value 338.833095
iter  50 value 338.641022
iter  60 value 338.622953
iter  70 value 338.616870
iter  80 value 338.612832
iter  90 value 338.609406
iter 100 value 338.607185
final  value 338.607185 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  89
initial  value 860.711415 
iter  10 value 384.803884
iter  20 value 376.612682
iter  30 value 375.796823
iter  40 value 367.817104
iter  50 value 361.669246
iter  60 value 360.644480
iter  70 value 360.255076
iter  80 value 360.207801
iter  90 value 360.186339
iter 100 value 360.178873
final  value 360.178873 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 758.103888 
iter  10 value 383.240516
iter  20 value 382.598691
iter  30 value 379.241515
iter  40 value 369.401391
iter  50 value 368.573974
iter  60 value 368.478157
iter  70 value 368.430529
iter  80 value 368.404121
iter  90 value 368.387004
iter 100 value 368.378809
final  value 368.378809 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 1170.256336 
iter  10 value 372.564525
iter  20 value 368.237769
iter  30 value 367.844468
iter  40 value 366.369241
iter  50 value 354.183175
iter  60 value 352.741696
iter  70 value 352.650742
iter  80 value 352.635314
iter  90 value 352.631153
iter 100 value 352.624897
final  value 352.624897 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  199
initial  value 985.847307 
iter  10 value 365.442280
iter  20 value 349.930741
iter  30 value 348.524747
iter  40 value 348.248381
iter  50 value 348.149015
iter  60 value 348.129762
iter  70 value 348.118930
iter  80 value 348.109506
iter  90 value 348.105251
iter 100 value 348.103088
final  value 348.103088 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 1038.670907 
iter  10 value 364.326394
iter  20 value 363.697385
iter  30 value 355.795378
iter  40 value 349.428184
iter  50 value 348.358294
iter  60 value 348.220612
iter  70 value 348.173555
iter  80 value 348.152932
iter  90 value 348.142923
iter 100 value 348.133127
final  value 348.133127 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 799.599112 
iter  10 value 370.835094
iter  20 value 365.006743
iter  30 value 363.755279
iter  40 value 362.613875
iter  50 value 350.169074
iter  60 value 348.424493
iter  70 value 348.284302
iter  80 value 348.215114
iter  90 value 348.171276
iter 100 value 348.138647
final  value 348.138647 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 1097.790761 
iter  10 value 371.580965
iter  20 value 364.440490
iter  30 value 352.297904
iter  40 value 348.679512
iter  50 value 348.289922
iter  60 value 348.167771
iter  70 value 348.133508
iter  80 value 348.117152
iter  90 value 348.106733
iter 100 value 348.102554
final  value 348.102554 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 579.430032 
iter  10 value 368.975566
iter  20 value 364.527835
iter  30 value 350.230647
iter  40 value 349.012190
iter  50 value 348.362521
iter  60 value 348.172863
iter  70 value 348.130283
iter  80 value 348.117266
iter  90 value 348.112882
iter 100 value 348.111130
final  value 348.111130 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  144
initial  value 664.918284 
iter  10 value 358.489029
iter  20 value 357.017585
iter  30 value 356.574299
iter  40 value 356.467182
iter  50 value 356.416130
iter  60 value 348.071111
iter  70 value 343.619709
iter  80 value 343.472812
iter  90 value 343.441987
iter 100 value 343.423037
final  value 343.423037 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  144
initial  value 845.894816 
iter  10 value 376.652153
iter  20 value 374.141482
iter  30 value 373.634373
iter  40 value 373.375500
iter  50 value 373.275808
iter  60 value 373.235635
iter  70 value 373.218677
iter  80 value 373.206824
iter  90 value 373.194834
iter 100 value 373.188694
final  value 373.188694 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  144
initial  value 661.870899 
iter  10 value 371.557869
iter  20 value 370.075939
iter  30 value 359.285663
iter  40 value 351.457985
iter  50 value 351.200142
iter  60 value 351.112346
iter  70 value 351.081835
iter  80 value 351.070851
iter  90 value 351.061338
iter 100 value 351.054128
final  value 351.054128 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  144
initial  value 947.356867 
iter  10 value 373.922215
iter  20 value 370.840552
iter  30 value 370.201844
iter  40 value 370.082885
iter  50 value 370.035406
iter  60 value 359.236547
iter  70 value 355.582376
iter  80 value 355.081623
iter  90 value 354.966115
iter 100 value 354.914570
final  value 354.914570 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  144
initial  value 1046.438370 
iter  10 value 373.711355
iter  20 value 373.406654
iter  30 value 369.579133
iter  40 value 361.129532
iter  50 value 360.175289
iter  60 value 359.979151
iter  70 value 359.945021
iter  80 value 359.915895
iter  90 value 359.906857
iter 100 value 359.901965
final  value 359.901965 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  56
initial  value 963.567638 
iter  10 value 375.386279
iter  20 value 364.151790
iter  30 value 359.293720
iter  40 value 350.137645
iter  50 value 349.329881
iter  60 value 349.253752
iter  70 value 349.237708
iter  80 value 349.229092
iter  90 value 349.223714
iter 100 value 349.220928
final  value 349.220928 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 532.142781 
iter  10 value 367.847521
iter  20 value 363.846866
iter  30 value 349.669020
iter  40 value 349.274605
iter  50 value 349.239785
iter  60 value 349.226749
iter  70 value 349.219864
iter  80 value 349.217237
iter  90 value 349.216829
iter 100 value 349.216742
final  value 349.216742 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 805.858098 
iter  10 value 374.828016
iter  20 value 365.281563
iter  30 value 363.958395
iter  40 value 353.713897
iter  50 value 350.056747
iter  60 value 349.373591
iter  70 value 349.289242
iter  80 value 349.254043
iter  90 value 349.230414
iter 100 value 349.220795
final  value 349.220795 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 1073.613575 
iter  10 value 372.265463
iter  20 value 364.316435
iter  30 value 355.422257
iter  40 value 350.364935
iter  50 value 349.462124
iter  60 value 349.276630
iter  70 value 349.249296
iter  80 value 349.235018
iter  90 value 349.225975
iter 100 value 349.221035
final  value 349.221035 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 677.667748 
iter  10 value 371.393456
iter  20 value 364.332570
iter  30 value 353.148647
iter  40 value 349.353656
iter  50 value 349.229835
iter  60 value 349.220987
iter  70 value 349.217772
iter  80 value 349.216698
final  value 349.216637 
converged
Fitting Repeat 1 

# weights:  188
initial  value 583.400652 
iter  10 value 371.348146
iter  20 value 363.751464
iter  30 value 363.677886
iter  40 value 363.470412
iter  50 value 349.515363
iter  60 value 347.865265
iter  70 value 347.747982
iter  80 value 347.707121
iter  90 value 347.680015
iter 100 value 347.667055
final  value 347.667055 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 485.866466 
iter  10 value 364.144465
iter  20 value 363.930267
iter  30 value 350.548771
iter  40 value 348.041187
iter  50 value 347.833665
iter  60 value 347.780174
iter  70 value 347.720408
iter  80 value 347.673760
iter  90 value 347.657562
iter 100 value 347.643636
final  value 347.643636 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 544.237765 
iter  10 value 364.251820
iter  20 value 363.918319
iter  30 value 350.163950
iter  40 value 348.014515
iter  50 value 347.834806
iter  60 value 347.759840
iter  70 value 347.702617
iter  80 value 347.660190
iter  90 value 347.643969
iter 100 value 347.637079
final  value 347.637079 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 904.012982 
iter  10 value 364.790889
iter  20 value 363.941764
iter  30 value 363.813716
iter  40 value 363.675014
iter  50 value 356.397429
iter  60 value 349.625689
iter  70 value 347.964462
iter  80 value 347.766505
iter  90 value 347.700841
iter 100 value 347.677418
final  value 347.677418 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 835.908109 
iter  10 value 363.911037
iter  20 value 363.710842
iter  30 value 363.663789
iter  40 value 361.779371
iter  50 value 351.115436
iter  60 value 349.836806
iter  70 value 348.871194
iter  80 value 348.441159
iter  90 value 348.187050
iter 100 value 347.961294
final  value 347.961294 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  12
initial  value 870.322684 
iter  10 value 407.586093
final  value 407.553710 
converged
Fitting Repeat 2 

# weights:  12
initial  value 776.173269 
iter  10 value 390.794900
final  value 390.493292 
converged
Fitting Repeat 3 

# weights:  12
initial  value 694.179456 
iter  10 value 395.755617
final  value 395.469900 
converged
Fitting Repeat 4 

# weights:  12
initial  value 854.129343 
iter  10 value 417.161697
final  value 414.163665 
converged
Fitting Repeat 5 

# weights:  12
initial  value 726.284436 
iter  10 value 419.101648
final  value 418.880091 
converged
Fitting Repeat 1 

# weights:  188
initial  value 1087.973323 
iter  10 value 418.503651
iter  20 value 387.277924
iter  30 value 386.150568
iter  40 value 386.122573
final  value 386.121976 
converged
Fitting Repeat 2 

# weights:  188
initial  value 785.624046 
iter  10 value 393.694432
iter  20 value 386.243851
iter  30 value 386.122194
final  value 386.121976 
converged
Fitting Repeat 3 

# weights:  188
initial  value 1107.858181 
iter  10 value 391.346810
iter  20 value 387.814509
iter  30 value 386.165214
iter  40 value 386.122047
final  value 386.122031 
converged
Fitting Repeat 4 

# weights:  188
initial  value 1124.516949 
iter  10 value 414.524874
iter  20 value 386.890227
iter  30 value 386.136909
iter  40 value 386.122022
final  value 386.121994 
converged
Fitting Repeat 5 

# weights:  188
initial  value 1180.155255 
iter  10 value 422.501090
iter  20 value 386.467233
iter  30 value 386.127147
final  value 386.122019 
converged
Fitting Repeat 1 

# weights:  188
initial  value 471.959764 
iter  10 value 360.303814
iter  20 value 345.133344
iter  30 value 343.651934
iter  40 value 343.493901
iter  50 value 343.416536
iter  60 value 343.390420
iter  70 value 343.376261
iter  80 value 343.368535
iter  90 value 343.364705
iter 100 value 343.363036
final  value 343.363036 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 478.742707 
iter  10 value 361.956024
iter  20 value 349.845555
iter  30 value 344.865650
iter  40 value 343.992307
iter  50 value 343.593235
iter  60 value 343.443911
iter  70 value 343.401799
iter  80 value 343.388132
iter  90 value 343.380125
iter 100 value 343.374465
final  value 343.374465 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 746.167140 
iter  10 value 364.168056
iter  20 value 361.218622
iter  30 value 360.753424
iter  40 value 360.589625
iter  50 value 359.937083
iter  60 value 345.678115
iter  70 value 343.716773
iter  80 value 343.506679
iter  90 value 343.424438
iter 100 value 343.402170
final  value 343.402170 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 642.118519 
iter  10 value 363.521779
iter  20 value 361.231217
iter  30 value 360.670383
iter  40 value 360.589359
iter  50 value 352.008349
iter  60 value 344.849892
iter  70 value 343.754902
iter  80 value 343.538094
iter  90 value 343.438370
iter 100 value 343.407352
final  value 343.407352 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 978.015945 
iter  10 value 363.606694
iter  20 value 361.529754
iter  30 value 349.848664
iter  40 value 344.654221
iter  50 value 343.981954
iter  60 value 343.654253
iter  70 value 343.515217
iter  80 value 343.439189
iter  90 value 343.400546
iter 100 value 343.388618
final  value 343.388618 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  199
initial  value 618.453460 
iter  10 value 369.095245
iter  20 value 365.566050
iter  30 value 365.532094
iter  40 value 365.527878
iter  50 value 365.518241
iter  60 value 365.492437
iter  70 value 355.158225
iter  80 value 350.585575
iter  90 value 349.514192
iter 100 value 349.341980
final  value 349.341980 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 940.432215 
iter  10 value 331.839558
iter  20 value 324.708152
iter  30 value 324.639988
iter  40 value 324.633531
iter  50 value 324.621633
iter  60 value 317.466425
iter  70 value 309.510562
iter  80 value 307.522001
iter  90 value 307.344501
iter 100 value 307.306027
final  value 307.306027 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 1310.092095 
iter  10 value 368.965106
iter  20 value 367.274623
iter  30 value 367.254281
iter  40 value 359.233096
iter  50 value 349.019482
iter  60 value 348.495160
iter  70 value 348.417647
iter  80 value 348.390914
iter  90 value 348.381738
iter 100 value 348.376103
final  value 348.376103 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 760.383836 
iter  10 value 387.686332
iter  20 value 381.390839
iter  30 value 381.320290
iter  40 value 381.223231
iter  50 value 361.719934
iter  60 value 359.910255
iter  70 value 359.740022
iter  80 value 359.703487
iter  90 value 359.687122
iter 100 value 359.677437
final  value 359.677437 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 710.064819 
iter  10 value 379.252672
iter  20 value 372.659520
iter  30 value 372.593063
iter  40 value 372.590210
iter  50 value 372.584094
iter  60 value 372.579717
iter  70 value 372.568536
iter  80 value 366.117727
iter  90 value 357.748014
iter 100 value 356.556414
final  value 356.556414 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  133
initial  value 952.462267 
iter  10 value 364.318994
iter  20 value 361.135806
iter  30 value 353.993086
iter  40 value 347.701142
iter  50 value 347.116519
iter  60 value 347.000617
iter  70 value 346.948626
iter  80 value 346.932827
iter  90 value 346.931365
iter 100 value 346.931071
final  value 346.931071 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  133
initial  value 583.777298 
iter  10 value 362.169062
iter  20 value 359.294328
iter  30 value 348.399209
iter  40 value 347.283237
iter  50 value 347.048107
iter  60 value 346.981521
iter  70 value 346.968325
iter  80 value 346.964234
iter  90 value 346.962156
iter 100 value 346.961577
final  value 346.961577 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 1174.770678 
iter  10 value 362.900749
iter  20 value 360.620810
iter  30 value 352.982892
iter  40 value 348.146490
iter  50 value 347.566902
iter  60 value 347.330711
iter  70 value 347.230837
iter  80 value 347.205557
iter  90 value 347.177205
iter 100 value 347.169552
final  value 347.169552 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  133
initial  value 1256.944472 
iter  10 value 388.625987
iter  20 value 361.781654
iter  30 value 360.685279
iter  40 value 350.556335
iter  50 value 347.892984
iter  60 value 347.249155
iter  70 value 347.042166
iter  80 value 346.941747
iter  90 value 346.932556
iter 100 value 346.931406
final  value 346.931406 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  133
initial  value 481.292594 
iter  10 value 361.241476
iter  20 value 356.230349
iter  30 value 348.263804
iter  40 value 347.379701
iter  50 value 347.145636
iter  60 value 347.039092
iter  70 value 347.031435
final  value 347.031033 
converged
Fitting Repeat 1 

# weights:  210
initial  value 931.071416 
iter  10 value 378.371786
iter  20 value 360.759365
iter  30 value 360.566725
iter  40 value 360.548600
iter  50 value 360.053476
iter  60 value 345.480824
iter  70 value 343.428771
iter  80 value 343.259682
iter  90 value 343.217738
iter 100 value 343.192548
final  value 343.192548 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  210
initial  value 985.152075 
iter  10 value 361.787407
iter  20 value 360.879080
iter  30 value 353.209787
iter  40 value 344.448154
iter  50 value 343.680068
iter  60 value 343.468015
iter  70 value 343.398089
iter  80 value 343.282626
iter  90 value 343.193299
iter 100 value 343.163389
final  value 343.163389 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  210
initial  value 655.286741 
iter  10 value 360.611351
iter  20 value 360.543978
iter  30 value 355.544127
iter  40 value 344.108301
iter  50 value 343.409301
iter  60 value 343.237863
iter  70 value 343.210427
iter  80 value 343.181040
iter  90 value 343.167849
iter 100 value 343.147313
final  value 343.147313 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  210
initial  value 1345.813311 
iter  10 value 361.930350
iter  20 value 360.586190
iter  30 value 360.565028
iter  40 value 360.557638
iter  50 value 360.550012
iter  60 value 360.425568
iter  70 value 348.942371
iter  80 value 343.605205
iter  90 value 343.303739
iter 100 value 343.226697
final  value 343.226697 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  210
initial  value 652.516996 
iter  10 value 370.439349
iter  20 value 360.667910
iter  30 value 360.572670
iter  40 value 360.551273
iter  50 value 360.524593
iter  60 value 349.648037
iter  70 value 343.679936
iter  80 value 343.254501
iter  90 value 343.208830
iter 100 value 343.182257
final  value 343.182257 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 791.307936 
iter  10 value 362.941908
iter  20 value 361.038915
iter  30 value 360.653495
iter  40 value 360.559594
iter  50 value 351.934304
iter  60 value 343.668738
iter  70 value 343.468197
iter  80 value 343.393480
iter  90 value 343.355832
iter 100 value 343.333606
final  value 343.333606 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  177
initial  value 1329.940502 
iter  10 value 365.700874
iter  20 value 360.681423
iter  30 value 360.586430
iter  40 value 360.447670
iter  50 value 349.599872
iter  60 value 343.699570
iter  70 value 343.438820
iter  80 value 343.395234
iter  90 value 343.373992
iter 100 value 343.355698
final  value 343.355698 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  177
initial  value 822.698621 
iter  10 value 364.097850
iter  20 value 361.321628
iter  30 value 350.886745
iter  40 value 344.412193
iter  50 value 343.748202
iter  60 value 343.503562
iter  70 value 343.400737
iter  80 value 343.336128
iter  90 value 343.312805
iter 100 value 343.300982
final  value 343.300982 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  177
initial  value 1201.602106 
iter  10 value 360.922120
iter  20 value 360.578081
iter  30 value 358.891197
iter  40 value 345.549205
iter  50 value 343.692128
iter  60 value 343.488337
iter  70 value 343.405806
iter  80 value 343.359483
iter  90 value 343.342014
iter 100 value 343.327867
final  value 343.327867 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  177
initial  value 1077.056850 
iter  10 value 362.897395
iter  20 value 360.750722
iter  30 value 346.775546
iter  40 value 344.042407
iter  50 value 343.591797
iter  60 value 343.433356
iter  70 value 343.371298
iter  80 value 343.331061
iter  90 value 343.311807
iter 100 value 343.304893
final  value 343.304893 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  78
initial  value 796.700462 
iter  10 value 364.340958
iter  20 value 361.095407
iter  30 value 360.746440
iter  40 value 358.195575
iter  50 value 344.790980
iter  60 value 343.856174
iter  70 value 343.690962
iter  80 value 343.646782
iter  90 value 343.630701
iter 100 value 343.626651
final  value 343.626651 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  78
initial  value 521.347920 
iter  10 value 362.591242
iter  20 value 360.871235
iter  30 value 360.394433
iter  40 value 346.900003
iter  50 value 343.976356
iter  60 value 343.762048
iter  70 value 343.708871
iter  80 value 343.686487
iter  90 value 343.670025
iter 100 value 343.651938
final  value 343.651938 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  78
initial  value 788.302108 
iter  10 value 366.073136
iter  20 value 361.148299
iter  30 value 360.714464
iter  40 value 346.796993
iter  50 value 344.067296
iter  60 value 343.767245
iter  70 value 343.703665
iter  80 value 343.684962
iter  90 value 343.670217
iter 100 value 343.654623
final  value 343.654623 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  78
initial  value 979.290342 
iter  10 value 363.947219
iter  20 value 359.796629
iter  30 value 346.757800
iter  40 value 344.196110
iter  50 value 343.779358
iter  60 value 343.675121
iter  70 value 343.642319
iter  80 value 343.633369
iter  90 value 343.628730
iter 100 value 343.626957
final  value 343.626957 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  78
initial  value 535.800259 
iter  10 value 361.958278
iter  20 value 360.903982
iter  30 value 360.522417
iter  40 value 346.132368
iter  50 value 344.049042
iter  60 value 343.768166
iter  70 value 343.700052
iter  80 value 343.680099
iter  90 value 343.656870
iter 100 value 343.641034
final  value 343.641034 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  221
initial  value 451.872966 
iter  10 value 360.965320
iter  20 value 360.560642
iter  30 value 360.554974
iter  40 value 360.553822
iter  50 value 360.551832
iter  60 value 360.547016
iter  70 value 360.518050
iter  80 value 347.611177
iter  90 value 343.658969
iter 100 value 343.162806
final  value 343.162806 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  221
initial  value 1127.032100 
iter  10 value 363.383484
iter  20 value 360.586561
iter  30 value 360.555640
iter  40 value 360.543541
iter  50 value 360.059896
iter  60 value 345.094672
iter  70 value 343.622915
iter  80 value 343.241227
iter  90 value 343.091962
iter 100 value 343.062562
final  value 343.062562 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  221
initial  value 505.745381 
iter  10 value 361.569413
iter  20 value 360.565647
iter  30 value 360.550753
iter  40 value 358.576273
iter  50 value 346.931228
iter  60 value 343.436546
iter  70 value 343.142698
iter  80 value 343.090443
iter  90 value 343.080046
iter 100 value 343.071051
final  value 343.071051 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  221
initial  value 532.041191 
iter  10 value 362.301267
iter  20 value 360.574084
iter  30 value 360.560051
iter  40 value 360.556336
iter  50 value 355.711675
iter  60 value 345.226176
iter  70 value 343.629357
iter  80 value 343.109701
iter  90 value 343.085191
iter 100 value 343.072559
final  value 343.072559 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  221
initial  value 736.774659 
iter  10 value 365.572311
iter  20 value 360.611797
iter  30 value 360.560282
iter  40 value 360.556086
iter  50 value 347.591078
iter  60 value 343.824327
iter  70 value 343.263407
iter  80 value 343.101643
iter  90 value 343.080193
iter 100 value 343.070669
final  value 343.070669 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  144
initial  value 1228.623563 
iter  10 value 398.245940
iter  20 value 396.831065
final  value 396.829505 
converged
Fitting Repeat 2 

# weights:  144
initial  value 434.472427 
iter  10 value 370.013250
iter  20 value 369.688627
iter  20 value 369.688626
iter  20 value 369.688626
final  value 369.688626 
converged
Fitting Repeat 3 

# weights:  144
initial  value 705.764943 
iter  10 value 377.269792
iter  20 value 368.810793
iter  30 value 368.056904
final  value 368.046851 
converged
Fitting Repeat 4 

# weights:  144
initial  value 572.375504 
iter  10 value 366.784222
iter  20 value 363.920083
iter  30 value 363.604351
final  value 363.604319 
converged
Fitting Repeat 5 

# weights:  144
initial  value 944.060210 
iter  10 value 368.147612
iter  20 value 359.572111
iter  30 value 358.782737
iter  40 value 358.755935
final  value 358.753097 
converged
Fitting Repeat 1 

# weights:  56
initial  value 1048.708071 
iter  10 value 357.288348
iter  20 value 354.693256
iter  30 value 354.669774
iter  40 value 354.660506
iter  50 value 354.632823
iter  60 value 352.559061
iter  70 value 338.029352
iter  80 value 333.540273
iter  90 value 333.353654
iter 100 value 333.321938
final  value 333.321938 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 807.615918 
iter  10 value 335.864958
iter  20 value 333.736593
iter  30 value 333.721725
iter  40 value 333.715662
iter  50 value 333.712519
iter  60 value 333.705083
iter  70 value 333.660571
iter  80 value 323.644900
iter  90 value 319.637957
iter 100 value 319.404754
final  value 319.404754 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 850.516423 
iter  10 value 387.805698
iter  20 value 385.602681
iter  30 value 385.558277
iter  40 value 370.709155
iter  50 value 366.112894
iter  60 value 365.985521
iter  70 value 365.978373
iter  80 value 365.973508
iter  90 value 365.969258
iter 100 value 365.967200
final  value 365.967200 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 799.879887 
iter  10 value 387.990938
iter  20 value 385.127188
iter  30 value 385.106173
iter  40 value 385.105291
iter  50 value 385.097660
iter  60 value 385.054547
iter  70 value 376.239475
iter  80 value 369.799406
iter  90 value 369.315403
iter 100 value 369.278373
final  value 369.278373 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 1048.322283 
iter  10 value 347.213758
iter  20 value 345.308904
iter  30 value 345.290098
iter  40 value 345.283424
iter  50 value 345.273087
iter  60 value 344.905934
iter  70 value 329.792219
iter  80 value 328.302518
iter  90 value 328.187910
iter 100 value 328.153406
final  value 328.153406 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  67
initial  value 1145.176481 
iter  10 value 361.282324
iter  20 value 360.946446
iter  30 value 360.930514
iter  40 value 359.740351
iter  50 value 340.534133
iter  60 value 339.062382
iter  70 value 338.974203
iter  80 value 338.966567
iter  90 value 338.962717
iter 100 value 338.960710
final  value 338.960710 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  67
initial  value 983.405159 
iter  10 value 332.175337
iter  20 value 331.488069
iter  30 value 331.483399
iter  40 value 331.483251
iter  50 value 331.480906
final  value 331.480804 
converged
Fitting Repeat 3 

# weights:  67
initial  value 715.534772 
iter  10 value 374.479785
iter  20 value 373.682652
iter  30 value 373.677186
final  value 373.677006 
converged
Fitting Repeat 4 

# weights:  67
initial  value 1002.300306 
iter  10 value 373.425111
iter  20 value 372.626102
iter  30 value 372.616544
iter  40 value 372.615888
final  value 372.615010 
converged
Fitting Repeat 5 

# weights:  67
initial  value 1032.639406 
iter  10 value 391.777018
iter  20 value 390.861847
iter  30 value 390.856229
iter  40 value 390.851039
iter  50 value 390.846536
iter  60 value 390.562372
iter  70 value 375.198728
iter  80 value 374.307497
iter  90 value 374.213690
iter 100 value 374.158253
final  value 374.158253 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  78
initial  value 803.602402 
iter  10 value 386.780460
iter  20 value 365.357624
iter  30 value 361.711060
iter  40 value 356.726917
iter  50 value 356.469041
final  value 356.459640 
converged
Fitting Repeat 2 

# weights:  78
initial  value 834.508184 
iter  10 value 366.140805
iter  20 value 361.837215
iter  30 value 357.300764
iter  40 value 356.460484
final  value 356.459640 
converged
Fitting Repeat 3 

# weights:  78
initial  value 1037.061612 
iter  10 value 366.314304
iter  20 value 362.344134
iter  30 value 359.394937
iter  40 value 359.367366
final  value 359.367318 
converged
Fitting Repeat 4 

# weights:  78
initial  value 815.699360 
iter  10 value 370.126621
iter  20 value 363.678657
iter  30 value 362.397327
iter  40 value 359.491166
iter  50 value 359.374134
iter  60 value 357.856656
iter  70 value 357.090812
final  value 357.089032 
converged
Fitting Repeat 5 

# weights:  78
initial  value 629.793140 
iter  10 value 370.730489
iter  20 value 363.753139
iter  30 value 360.141826
iter  40 value 358.784849
iter  50 value 358.407153
iter  60 value 357.781206
iter  70 value 357.163807
iter  80 value 356.482667
iter  90 value 356.459755
final  value 356.459640 
converged
Fitting Repeat 1 

# weights:  67
initial  value 702.649910 
iter  10 value 332.663432
iter  20 value 332.518942
final  value 332.518271 
converged
Fitting Repeat 2 

# weights:  67
initial  value 953.704616 
iter  10 value 334.912703
iter  20 value 334.796238
iter  30 value 334.794860
final  value 334.794831 
converged
Fitting Repeat 3 

# weights:  67
initial  value 760.111413 
iter  10 value 369.875079
iter  20 value 369.709062
iter  20 value 369.709062
final  value 369.708908 
converged
Fitting Repeat 4 

# weights:  67
initial  value 525.288242 
iter  10 value 382.217239
iter  20 value 382.184269
final  value 382.184245 
converged
Fitting Repeat 5 

# weights:  67
initial  value 728.721456 
iter  10 value 352.212717
iter  20 value 352.072911
iter  20 value 352.072911
iter  20 value 352.072910
final  value 352.072910 
converged
Fitting Repeat 1 

# weights:  188
initial  value 396.761202 
iter  10 value 370.223141
iter  20 value 370.143914
iter  30 value 370.136017
iter  40 value 370.131394
iter  50 value 370.109163
iter  60 value 359.194859
iter  70 value 355.128894
iter  80 value 354.519255
iter  90 value 354.466211
iter 100 value 354.445127
final  value 354.445127 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 554.787339 
iter  10 value 362.212736
iter  20 value 358.928483
iter  30 value 358.897390
iter  40 value 358.891381
iter  50 value 358.884711
iter  60 value 358.808356
iter  70 value 346.754032
iter  80 value 345.015353
iter  90 value 344.800197
iter 100 value 344.777196
final  value 344.777196 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 420.771395 
iter  10 value 364.101931
iter  20 value 363.553937
iter  30 value 363.536395
iter  40 value 363.470783
iter  50 value 347.715256
iter  60 value 343.533103
iter  70 value 343.107927
iter  80 value 343.018977
iter  90 value 343.002423
iter 100 value 342.991462
final  value 342.991462 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 1259.727796 
iter  10 value 344.159554
iter  20 value 342.559130
iter  30 value 342.525677
iter  40 value 342.524495
iter  50 value 342.519913
iter  60 value 342.518308
iter  70 value 342.514664
iter  80 value 342.497569
iter  90 value 330.479445
iter 100 value 327.089822
final  value 327.089822 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 1048.243805 
iter  10 value 364.863240
iter  20 value 354.339329
iter  30 value 354.229663
iter  40 value 354.221288
iter  50 value 354.220387
iter  60 value 354.218989
iter  70 value 354.216412
iter  80 value 354.209680
iter  90 value 354.148411
iter 100 value 337.973104
final  value 337.973104 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 675.797452 
iter  10 value 369.727297
iter  20 value 365.015915
iter  30 value 353.215334
iter  40 value 350.867367
iter  50 value 350.407638
iter  60 value 350.333657
iter  70 value 350.311736
iter  80 value 350.299618
iter  90 value 350.296089
iter 100 value 350.295084
final  value 350.295084 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 648.647738 
iter  10 value 380.097539
iter  20 value 376.702322
iter  30 value 376.251053
iter  40 value 362.074284
iter  50 value 360.512951
iter  60 value 360.399090
iter  70 value 360.363832
iter  80 value 360.345917
iter  90 value 360.338314
iter 100 value 360.334168
final  value 360.334168 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  89
initial  value 584.828845 
iter  10 value 359.359945
iter  20 value 350.504512
iter  30 value 341.217824
iter  40 value 338.939975
iter  50 value 338.581646
iter  60 value 338.532588
iter  70 value 338.524420
iter  80 value 338.519661
iter  90 value 338.517202
iter 100 value 338.516036
final  value 338.516036 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 645.136669 
iter  10 value 360.552343
iter  20 value 347.751771
iter  30 value 341.695382
iter  40 value 341.046640
iter  50 value 340.912241
iter  60 value 340.879462
iter  70 value 340.862256
iter  80 value 340.852682
iter  90 value 340.846831
iter 100 value 340.843571
final  value 340.843571 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 971.661186 
iter  10 value 371.315368
iter  20 value 366.503106
iter  30 value 353.896008
iter  40 value 351.577001
iter  50 value 351.014209
iter  60 value 350.918517
iter  70 value 350.899414
iter  80 value 350.891848
iter  90 value 350.882629
iter 100 value 350.869591
final  value 350.869591 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  199
initial  value 867.489544 
iter  10 value 360.828617
iter  20 value 360.655629
iter  30 value 360.577278
iter  40 value 347.886643
iter  50 value 344.720716
iter  60 value 343.898221
iter  70 value 343.735415
iter  80 value 343.650877
iter  90 value 343.626955
iter 100 value 343.617480
final  value 343.617480 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 1160.502767 
iter  10 value 362.818585
iter  20 value 361.222286
iter  30 value 360.620504
iter  40 value 360.377174
iter  50 value 347.049661
iter  60 value 344.384656
iter  70 value 343.891537
iter  80 value 343.723777
iter  90 value 343.665958
iter 100 value 343.641402
final  value 343.641402 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 725.002482 
iter  10 value 367.836681
iter  20 value 360.742682
iter  30 value 360.463096
iter  40 value 345.790747
iter  50 value 343.905358
iter  60 value 343.746127
iter  70 value 343.662890
iter  80 value 343.635452
iter  90 value 343.625502
iter 100 value 343.620182
final  value 343.620182 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 530.630853 
iter  10 value 364.456927
iter  20 value 361.332507
iter  30 value 360.822026
iter  40 value 360.607522
iter  50 value 360.237512
iter  60 value 347.111973
iter  70 value 344.045819
iter  80 value 343.747218
iter  90 value 343.664499
iter 100 value 343.637485
final  value 343.637485 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 536.061332 
iter  10 value 364.362559
iter  20 value 361.313466
iter  30 value 360.648842
iter  40 value 354.265438
iter  50 value 344.684865
iter  60 value 343.907221
iter  70 value 343.778580
iter  80 value 343.737713
iter  90 value 343.700255
iter 100 value 343.664548
final  value 343.664548 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  144
initial  value 713.599266 
iter  10 value 359.816565
iter  20 value 359.738587
iter  30 value 359.533542
iter  40 value 344.922909
iter  50 value 342.799260
iter  60 value 342.571809
iter  70 value 342.544729
iter  80 value 342.529002
iter  90 value 342.521809
iter 100 value 342.515237
final  value 342.515237 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  144
initial  value 646.365180 
iter  10 value 369.040322
iter  20 value 367.751035
iter  30 value 367.230420
iter  40 value 367.095868
iter  50 value 353.803687
iter  60 value 351.753563
iter  70 value 351.529578
iter  80 value 351.488327
iter  90 value 351.460213
iter 100 value 351.445065
final  value 351.445065 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  144
initial  value 517.689379 
iter  10 value 340.278738
iter  20 value 339.229189
iter  30 value 328.042151
iter  40 value 324.087993
iter  50 value 323.502596
iter  60 value 323.308685
iter  70 value 323.199866
iter  80 value 323.153573
iter  90 value 323.127493
iter 100 value 323.106177
final  value 323.106177 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  144
initial  value 1119.205670 
iter  10 value 377.820130
iter  20 value 374.994404
iter  30 value 354.767977
iter  40 value 354.028534
iter  50 value 353.808665
iter  60 value 353.706897
iter  70 value 353.650737
iter  80 value 353.635920
iter  90 value 353.627059
iter 100 value 353.622846
final  value 353.622846 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  144
initial  value 1013.285154 
iter  10 value 334.557242
iter  20 value 326.156901
iter  30 value 324.541057
iter  40 value 324.388868
iter  50 value 324.351680
iter  60 value 324.327844
iter  70 value 324.314716
iter  80 value 324.306929
iter  90 value 324.303265
iter 100 value 324.300680
final  value 324.300680 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  56
initial  value 562.613413 
iter  10 value 365.449211
iter  20 value 361.080589
iter  30 value 359.972001
iter  40 value 346.125836
iter  50 value 345.085544
iter  60 value 344.886926
iter  70 value 344.805963
iter  80 value 344.769466
iter  90 value 344.750496
iter 100 value 344.741293
final  value 344.741293 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 453.840106 
iter  10 value 364.198839
iter  20 value 361.296696
iter  30 value 351.918958
iter  40 value 346.436573
iter  50 value 344.891173
iter  60 value 344.801836
iter  70 value 344.769977
iter  80 value 344.750081
iter  90 value 344.740853
iter 100 value 344.738791
final  value 344.738791 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 1074.893041 
iter  10 value 371.130808
iter  20 value 355.826421
iter  30 value 345.707434
iter  40 value 344.940416
iter  50 value 344.865656
iter  60 value 344.839266
iter  70 value 344.813034
iter  80 value 344.788267
iter  90 value 344.781676
iter 100 value 344.780826
final  value 344.780826 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 725.804762 
iter  10 value 370.406579
iter  20 value 352.916500
iter  30 value 345.326908
iter  40 value 344.929837
iter  50 value 344.859168
iter  60 value 344.827836
iter  70 value 344.805349
iter  80 value 344.788478
iter  90 value 344.781896
iter 100 value 344.780386
final  value 344.780386 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 1076.835843 
iter  10 value 364.822088
iter  20 value 361.048992
iter  30 value 354.635517
iter  40 value 345.534238
iter  50 value 345.032693
iter  60 value 344.948960
iter  70 value 344.923938
iter  80 value 344.908994
iter  90 value 344.903262
iter 100 value 344.901975
final  value 344.901975 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  188
initial  value 656.277381 
iter  10 value 371.508013
iter  20 value 360.680231
iter  30 value 360.572013
iter  40 value 360.555300
iter  50 value 360.538185
iter  60 value 352.943268
iter  70 value 343.939684
iter  80 value 343.310728
iter  90 value 343.250243
iter 100 value 343.212229
final  value 343.212229 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 1022.629940 
iter  10 value 377.573385
iter  20 value 360.750160
iter  30 value 360.568041
iter  40 value 360.525012
iter  50 value 348.628723
iter  60 value 343.675756
iter  70 value 343.285841
iter  80 value 343.255258
iter  90 value 343.229322
iter 100 value 343.210957
final  value 343.210957 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 509.609631 
iter  10 value 361.054047
iter  20 value 360.734529
iter  30 value 360.642054
iter  40 value 346.616958
iter  50 value 343.475130
iter  60 value 343.226827
iter  70 value 343.192243
iter  80 value 343.172450
iter  90 value 343.161635
iter 100 value 343.153986
final  value 343.153986 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 637.912611 
iter  10 value 361.346877
iter  20 value 360.783109
iter  30 value 360.619444
iter  40 value 360.263128
iter  50 value 344.000223
iter  60 value 343.328461
iter  70 value 343.221786
iter  80 value 343.195123
iter  90 value 343.179130
iter 100 value 343.170142
final  value 343.170142 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 549.347711 
iter  10 value 361.209321
iter  20 value 360.810491
iter  30 value 360.691828
iter  40 value 360.570429
iter  50 value 360.545602
iter  60 value 346.078127
iter  70 value 343.529538
iter  80 value 343.208679
iter  90 value 343.170760
iter 100 value 343.158735
final  value 343.158735 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  12
initial  value 848.828740 
iter  10 value 389.583349
final  value 389.572041 
converged
Fitting Repeat 2 

# weights:  12
initial  value 689.252994 
iter  10 value 418.393020
final  value 418.259183 
converged
Fitting Repeat 3 

# weights:  12
initial  value 869.031749 
iter  10 value 429.866680
final  value 429.268438 
converged
Fitting Repeat 4 

# weights:  12
initial  value 871.986856 
iter  10 value 423.709628
final  value 423.702567 
converged
Fitting Repeat 5 

# weights:  12
initial  value 657.293332 
iter  10 value 425.334018
final  value 425.092391 
converged
Fitting Repeat 1 

# weights:  188
initial  value 1014.313447 
iter  10 value 402.012619
iter  20 value 397.991441
iter  30 value 396.104821
iter  40 value 395.831993
final  value 395.831629 
converged
Fitting Repeat 2 

# weights:  188
initial  value 688.281779 
iter  10 value 398.238891
iter  20 value 395.927564
iter  30 value 395.832100
final  value 395.831634 
converged
Fitting Repeat 3 

# weights:  188
initial  value 798.982456 
iter  10 value 406.899423
iter  20 value 396.418620
iter  30 value 395.838128
final  value 395.831632 
converged
Fitting Repeat 4 

# weights:  188
initial  value 989.062784 
iter  10 value 403.194235
iter  20 value 400.103963
iter  30 value 396.215890
iter  40 value 395.837935
final  value 395.831631 
converged
Fitting Repeat 5 

# weights:  188
initial  value 1386.040696 
iter  10 value 416.420672
iter  20 value 396.274604
iter  30 value 395.877863
iter  40 value 395.832023
final  value 395.831634 
converged
Fitting Repeat 1 

# weights:  188
initial  value 1270.712283 
iter  10 value 388.273799
iter  20 value 370.543380
iter  30 value 370.357726
iter  40 value 359.890097
iter  50 value 352.864174
iter  60 value 352.445946
iter  70 value 352.335207
iter  80 value 352.298924
iter  90 value 352.284466
iter 100 value 352.276249
final  value 352.276249 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 931.478135 
iter  10 value 374.436499
iter  20 value 368.581867
iter  30 value 355.463623
iter  40 value 353.184573
iter  50 value 352.667160
iter  60 value 352.388763
iter  70 value 352.312359
iter  80 value 352.269664
iter  90 value 352.248372
iter 100 value 352.239232
final  value 352.239232 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 792.263294 
iter  10 value 371.476053
iter  20 value 356.437162
iter  30 value 353.308045
iter  40 value 352.594179
iter  50 value 352.372594
iter  60 value 352.283381
iter  70 value 352.252057
iter  80 value 352.244263
iter  90 value 352.237848
iter 100 value 352.232588
final  value 352.232588 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 626.416828 
iter  10 value 372.992941
iter  20 value 370.760569
iter  30 value 355.456044
iter  40 value 352.753394
iter  50 value 352.444275
iter  60 value 352.308739
iter  70 value 352.271023
iter  80 value 352.249965
iter  90 value 352.243607
iter 100 value 352.238837
final  value 352.238837 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 678.313733 
iter  10 value 373.334896
iter  20 value 371.310927
iter  30 value 370.540701
iter  40 value 370.284226
iter  50 value 353.891438
iter  60 value 352.519828
iter  70 value 352.328132
iter  80 value 352.285078
iter  90 value 352.261747
iter 100 value 352.247238
final  value 352.247238 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  199
initial  value 753.968310 
iter  10 value 382.023783
iter  20 value 376.414656
iter  30 value 376.356623
iter  40 value 376.352252
iter  50 value 376.351249
iter  60 value 376.349438
iter  70 value 376.345085
iter  80 value 376.321480
iter  90 value 361.393880
iter 100 value 356.977326
final  value 356.977326 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 488.254334 
iter  10 value 371.100292
iter  20 value 369.642860
iter  30 value 369.626302
iter  40 value 369.624973
iter  50 value 369.622672
iter  60 value 369.617183
iter  70 value 369.586287
iter  80 value 363.910553
iter  90 value 355.360107
iter 100 value 351.661072
final  value 351.661072 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 1084.498488 
iter  10 value 357.157705
iter  20 value 352.646628
iter  30 value 352.605928
iter  40 value 352.599061
iter  50 value 352.580774
iter  60 value 346.927251
iter  70 value 336.953747
iter  80 value 334.257667
iter  90 value 334.144642
iter 100 value 334.099546
final  value 334.099546 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 734.571389 
iter  10 value 380.526448
iter  20 value 373.067330
iter  30 value 372.984247
iter  40 value 372.920348
iter  50 value 355.750927
iter  60 value 351.251692
iter  70 value 350.672137
iter  80 value 350.506296
iter  90 value 350.452559
iter 100 value 350.433223
final  value 350.433223 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 580.117654 
iter  10 value 401.428011
iter  20 value 398.234185
iter  30 value 398.200561
iter  40 value 398.171789
iter  50 value 381.937666
iter  60 value 378.165101
iter  70 value 377.775526
iter  80 value 377.722831
iter  90 value 377.698078
iter 100 value 377.684691
final  value 377.684691 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  133
initial  value 859.646667 
iter  10 value 371.863672
iter  20 value 370.406826
iter  30 value 358.110060
iter  40 value 356.439939
iter  50 value 355.958683
iter  60 value 355.853059
iter  70 value 355.832548
iter  80 value 355.825956
iter  90 value 355.825620
final  value 355.825615 
converged
Fitting Repeat 2 

# weights:  133
initial  value 692.129102 
iter  10 value 372.282344
iter  20 value 364.386027
iter  30 value 357.853723
iter  40 value 356.898349
iter  50 value 356.204283
iter  60 value 356.072135
iter  70 value 356.033758
iter  80 value 356.028443
iter  90 value 356.027773
iter 100 value 356.027633
final  value 356.027633 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 1069.447592 
iter  10 value 371.709664
iter  20 value 370.388969
iter  30 value 359.601073
iter  40 value 356.490852
iter  50 value 356.182255
iter  60 value 356.063056
iter  70 value 355.992655
iter  80 value 355.929633
iter  90 value 355.838101
iter 100 value 355.829410
final  value 355.829410 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  133
initial  value 1209.068136 
iter  10 value 372.819158
iter  20 value 370.655429
iter  30 value 357.103745
iter  40 value 355.957526
iter  50 value 355.817026
iter  60 value 355.797146
iter  70 value 355.796203
final  value 355.796194 
converged
Fitting Repeat 5 

# weights:  133
initial  value 1107.684835 
iter  10 value 371.358503
iter  20 value 367.048595
iter  30 value 358.157528
iter  40 value 356.741846
iter  50 value 356.181381
iter  60 value 355.945095
iter  70 value 355.898784
iter  80 value 355.894675
iter  90 value 355.893207
final  value 355.893090 
converged
Fitting Repeat 1 

# weights:  210
initial  value 624.174147 
iter  10 value 371.195663
iter  20 value 370.632330
iter  30 value 370.480597
iter  40 value 360.078983
iter  50 value 353.147586
iter  60 value 352.290848
iter  70 value 352.178894
iter  80 value 352.084458
iter  90 value 352.045380
iter 100 value 352.021931
final  value 352.021931 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  210
initial  value 480.469697 
iter  10 value 373.318096
iter  20 value 370.370941
iter  30 value 370.333447
iter  40 value 361.636035
iter  50 value 353.627530
iter  60 value 352.197401
iter  70 value 352.084521
iter  80 value 352.019312
iter  90 value 351.999762
iter 100 value 351.991078
final  value 351.991078 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  210
initial  value 1054.893204 
iter  10 value 387.995963
iter  20 value 370.540177
iter  30 value 370.357137
iter  40 value 370.340889
iter  50 value 370.323352
iter  60 value 356.361236
iter  70 value 352.920226
iter  80 value 352.266802
iter  90 value 352.069028
iter 100 value 352.035250
final  value 352.035250 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  210
initial  value 615.771561 
iter  10 value 378.327283
iter  20 value 370.428705
iter  30 value 370.343414
iter  40 value 370.268616
iter  50 value 362.382775
iter  60 value 354.630120
iter  70 value 353.107991
iter  80 value 352.604704
iter  90 value 352.452593
iter 100 value 352.209145
final  value 352.209145 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  210
initial  value 459.929662 
iter  10 value 373.204468
iter  20 value 370.370410
iter  30 value 370.343843
iter  40 value 364.952378
iter  50 value 353.968985
iter  60 value 352.211305
iter  70 value 352.117030
iter  80 value 352.071676
iter  90 value 352.045481
iter 100 value 352.028279
final  value 352.028279 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 683.360389 
iter  10 value 372.431729
iter  20 value 370.852498
iter  30 value 370.538925
iter  40 value 370.460910
iter  50 value 363.672089
iter  60 value 353.792321
iter  70 value 352.373603
iter  80 value 352.268664
iter  90 value 352.197738
iter 100 value 352.177333
final  value 352.177333 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  177
initial  value 885.426685 
iter  10 value 372.730463
iter  20 value 370.930390
iter  30 value 354.345553
iter  40 value 352.539755
iter  50 value 352.355345
iter  60 value 352.227155
iter  70 value 352.185367
iter  80 value 352.163922
iter  90 value 352.155641
iter 100 value 352.152039
final  value 352.152039 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  177
initial  value 738.895432 
iter  10 value 370.987929
iter  20 value 354.865408
iter  30 value 352.960577
iter  40 value 352.537187
iter  50 value 352.322485
iter  60 value 352.223671
iter  70 value 352.180025
iter  80 value 352.165994
iter  90 value 352.157118
iter 100 value 352.151795
final  value 352.151795 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  177
initial  value 784.049833 
iter  10 value 372.667948
iter  20 value 365.419522
iter  30 value 353.877133
iter  40 value 352.745522
iter  50 value 352.476999
iter  60 value 352.251185
iter  70 value 352.195105
iter  80 value 352.168121
iter  90 value 352.158206
iter 100 value 352.150996
final  value 352.150996 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  177
initial  value 1055.795686 
iter  10 value 372.965591
iter  20 value 371.138091
iter  30 value 355.807563
iter  40 value 353.112791
iter  50 value 352.588660
iter  60 value 352.338336
iter  70 value 352.217537
iter  80 value 352.187421
iter  90 value 352.172505
iter 100 value 352.161703
final  value 352.161703 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  78
initial  value 912.584457 
iter  10 value 355.305161
iter  20 value 353.331677
iter  30 value 352.770684
iter  40 value 352.594200
iter  50 value 352.563387
iter  60 value 352.552482
iter  70 value 352.545895
iter  80 value 352.536194
iter  90 value 352.532335
iter 100 value 352.530397
final  value 352.530397 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  78
initial  value 1010.564462 
iter  10 value 368.567785
iter  20 value 353.929741
iter  30 value 352.732344
iter  40 value 352.559713
iter  50 value 352.531347
iter  60 value 352.518988
iter  70 value 352.512765
iter  80 value 352.509331
iter  90 value 352.506237
iter 100 value 352.501942
final  value 352.501942 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  78
initial  value 698.505134 
iter  10 value 372.879878
iter  20 value 369.650385
iter  30 value 353.081965
iter  40 value 352.669968
iter  50 value 352.540578
iter  60 value 352.509240
iter  70 value 352.499274
iter  80 value 352.493854
iter  90 value 352.490259
iter 100 value 352.488991
final  value 352.488991 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  78
initial  value 551.309007 
iter  10 value 364.663224
iter  20 value 354.579398
iter  30 value 352.861406
iter  40 value 352.577866
iter  50 value 352.514794
iter  60 value 352.504580
iter  70 value 352.499186
iter  80 value 352.493611
iter  90 value 352.490814
iter 100 value 352.489378
final  value 352.489378 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  78
initial  value 511.189297 
iter  10 value 371.816685
iter  20 value 370.604621
iter  30 value 370.459386
iter  40 value 363.975962
iter  50 value 353.270886
iter  60 value 352.606403
iter  70 value 352.536391
iter  80 value 352.514313
iter  90 value 352.505838
iter 100 value 352.499646
final  value 352.499646 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  221
initial  value 799.759124 
iter  10 value 374.966101
iter  20 value 370.389953
iter  30 value 370.338517
iter  40 value 370.236751
iter  50 value 357.415558
iter  60 value 352.440092
iter  70 value 352.071498
iter  80 value 351.986266
iter  90 value 351.951525
iter 100 value 351.938207
final  value 351.938207 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  221
initial  value 1249.332820 
iter  10 value 371.853258
iter  20 value 370.354064
iter  30 value 370.342233
iter  40 value 370.339027
iter  50 value 370.338974
iter  60 value 370.338918
iter  70 value 370.338859
iter  80 value 370.338795
iter  90 value 370.338726
iter 100 value 370.338650
final  value 370.338650 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  221
initial  value 709.405185 
iter  10 value 373.701753
iter  20 value 370.375376
iter  30 value 370.345026
iter  40 value 370.341717
iter  50 value 370.340104
iter  60 value 370.339200
iter  70 value 370.339079
iter  80 value 370.338956
iter  90 value 370.338823
iter 100 value 370.338698
final  value 370.338698 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  221
initial  value 1082.125808 
iter  10 value 375.004385
iter  20 value 370.390394
iter  30 value 370.343503
iter  40 value 370.339794
iter  50 value 370.338736
iter  60 value 370.338628
iter  70 value 370.338505
iter  80 value 370.338362
iter  90 value 370.338193
iter 100 value 370.337989
final  value 370.337989 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  221
initial  value 983.212489 
iter  10 value 374.727409
iter  20 value 370.387201
iter  30 value 370.343139
iter  40 value 370.341068
iter  50 value 370.284191
iter  60 value 354.855091
iter  70 value 352.442567
iter  80 value 352.091850
iter  90 value 351.976381
iter 100 value 351.938744
final  value 351.938744 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  144
initial  value 665.922256 
iter  10 value 389.436714
iter  20 value 385.304919
iter  30 value 385.296488
iter  30 value 385.296488
iter  30 value 385.296488
final  value 385.296488 
converged
Fitting Repeat 2 

# weights:  144
initial  value 574.822746 
iter  10 value 367.383097
iter  20 value 365.629900
final  value 365.627952 
converged
Fitting Repeat 3 

# weights:  144
initial  value 726.262383 
iter  10 value 369.828016
iter  20 value 367.288181
iter  30 value 367.262414
iter  30 value 367.262411
iter  30 value 367.262411
final  value 367.262411 
converged
Fitting Repeat 4 

# weights:  144
initial  value 828.943624 
iter  10 value 413.853276
iter  20 value 400.234444
iter  30 value 395.265751
iter  40 value 394.372667
iter  50 value 394.307667
final  value 394.307657 
converged
Fitting Repeat 5 

# weights:  144
initial  value 1255.050817 
iter  10 value 388.063503
iter  20 value 381.719865
final  value 381.706871 
converged
Fitting Repeat 1 

# weights:  56
initial  value 833.235399 
iter  10 value 362.827697
iter  20 value 360.364836
iter  30 value 360.312892
iter  40 value 360.303898
iter  50 value 354.924289
iter  60 value 343.876067
iter  70 value 342.450683
iter  80 value 342.363095
iter  90 value 342.337395
iter 100 value 342.334203
final  value 342.334203 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 699.696085 
iter  10 value 360.635515
iter  20 value 359.222606
iter  30 value 359.178347
iter  40 value 359.166694
iter  50 value 359.142825
iter  60 value 349.996768
iter  70 value 337.500688
iter  80 value 336.863307
iter  90 value 336.811577
iter 100 value 336.786151
final  value 336.786151 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 823.299115 
iter  10 value 389.132694
iter  20 value 387.305235
iter  30 value 387.261781
iter  40 value 387.250077
iter  50 value 387.243731
iter  60 value 387.024749
iter  70 value 374.005921
iter  80 value 372.348519
iter  90 value 372.182744
iter 100 value 372.151626
final  value 372.151626 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 636.655310 
iter  10 value 359.955159
iter  20 value 358.281815
iter  30 value 358.276492
iter  40 value 358.275837
iter  50 value 358.275641
iter  60 value 358.275406
iter  70 value 358.275111
iter  80 value 358.274721
iter  90 value 358.274160
iter 100 value 358.273252
final  value 358.273252 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 1199.980331 
iter  10 value 393.249603
iter  20 value 391.743718
iter  30 value 391.725076
iter  40 value 391.694931
iter  50 value 385.166453
iter  60 value 370.346397
iter  70 value 367.426907
iter  80 value 367.274299
iter  90 value 367.171302
iter 100 value 367.129831
final  value 367.129831 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  67
initial  value 528.285424 
iter  10 value 389.709995
iter  20 value 389.478907
iter  30 value 389.478234
iter  40 value 389.476712
iter  50 value 389.476431
iter  60 value 389.476088
iter  70 value 389.475654
iter  80 value 389.475084
iter  90 value 389.474292
iter 100 value 389.473113
final  value 389.473113 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  67
initial  value 725.784481 
iter  10 value 385.235023
iter  20 value 384.746209
iter  30 value 384.726445
iter  40 value 384.722905
iter  50 value 384.710142
iter  60 value 384.699774
iter  70 value 384.583249
iter  80 value 369.948705
iter  90 value 366.479727
iter 100 value 366.267096
final  value 366.267096 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  67
initial  value 848.828636 
iter  10 value 360.641945
iter  20 value 359.478812
iter  30 value 359.471528
final  value 359.471179 
converged
Fitting Repeat 4 

# weights:  67
initial  value 969.996934 
iter  10 value 371.321225
iter  20 value 370.041330
iter  30 value 370.029161
iter  40 value 370.017822
iter  50 value 369.996089
iter  60 value 362.869572
iter  70 value 354.364735
iter  80 value 353.794082
iter  90 value 353.670392
iter 100 value 353.634260
final  value 353.634260 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  67
initial  value 1008.526295 
iter  10 value 375.069528
iter  20 value 373.712594
iter  30 value 373.701492
final  value 373.701410 
converged
Fitting Repeat 1 

# weights:  78
initial  value 1073.426485 
iter  10 value 375.210700
iter  20 value 372.579838
iter  30 value 368.463144
iter  40 value 366.154121
iter  50 value 366.026134
final  value 366.025822 
converged
Fitting Repeat 2 

# weights:  78
initial  value 676.312812 
iter  10 value 376.337686
iter  20 value 373.135294
iter  30 value 369.912927
iter  40 value 367.071457
iter  50 value 366.027428
final  value 366.025822 
converged
Fitting Repeat 3 

# weights:  78
initial  value 1015.066959 
iter  10 value 376.272452
iter  20 value 373.410394
iter  30 value 370.256060
iter  40 value 368.374642
iter  50 value 368.331082
final  value 368.329865 
converged
Fitting Repeat 4 

# weights:  78
initial  value 683.485517 
iter  10 value 389.195999
iter  20 value 374.056353
iter  30 value 368.110006
iter  40 value 366.077489
iter  50 value 366.025824
iter  50 value 366.025821
iter  50 value 366.025821
final  value 366.025821 
converged
Fitting Repeat 5 

# weights:  78
initial  value 1034.549262 
iter  10 value 377.342191
iter  20 value 373.523043
iter  30 value 369.257745
iter  40 value 365.787614
iter  50 value 365.398545
final  value 365.389456 
converged
Fitting Repeat 1 

# weights:  67
initial  value 791.829160 
iter  10 value 373.243981
iter  20 value 373.083533
final  value 373.083095 
converged
Fitting Repeat 2 

# weights:  67
initial  value 852.324906 
iter  10 value 353.208532
iter  20 value 353.053080
iter  20 value 353.053080
final  value 353.052966 
converged
Fitting Repeat 3 

# weights:  67
initial  value 984.680205 
iter  10 value 376.510424
iter  20 value 376.407779
final  value 376.407764 
converged
Fitting Repeat 4 

# weights:  67
initial  value 616.402571 
iter  10 value 345.187946
iter  20 value 345.087756
final  value 345.087198 
converged
Fitting Repeat 5 

# weights:  67
initial  value 779.529126 
iter  10 value 386.323892
iter  20 value 386.206693
iter  20 value 386.206690
iter  20 value 386.206689
final  value 386.206689 
converged
Fitting Repeat 1 

# weights:  188
initial  value 1160.431829 
iter  10 value 369.902969
iter  20 value 365.804321
iter  30 value 365.769688
iter  40 value 365.766891
iter  50 value 365.764493
iter  60 value 365.764286
iter  70 value 365.764059
iter  80 value 365.763808
iter  90 value 365.763521
iter 100 value 365.763188
final  value 365.763188 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 1068.677392 
iter  10 value 316.887438
iter  20 value 312.543157
iter  30 value 312.500128
iter  40 value 312.469866
iter  50 value 299.833017
iter  60 value 292.024332
iter  70 value 291.847193
iter  80 value 291.801175
iter  90 value 291.769905
iter 100 value 291.756895
final  value 291.756895 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 1040.764759 
iter  10 value 406.296652
iter  20 value 394.556468
iter  30 value 394.429031
iter  40 value 394.421716
iter  50 value 394.418991
iter  60 value 394.410486
iter  70 value 394.148876
iter  80 value 376.687930
iter  90 value 375.554782
iter 100 value 375.490523
final  value 375.490523 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 652.888859 
iter  10 value 363.815153
iter  20 value 360.213737
iter  30 value 360.176427
iter  40 value 343.689774
iter  50 value 340.927767
iter  60 value 340.639559
iter  70 value 340.600625
iter  80 value 340.587046
iter  90 value 340.580607
iter 100 value 340.569428
final  value 340.569428 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 1069.793087 
iter  10 value 360.262658
iter  20 value 352.214847
iter  30 value 352.130136
iter  40 value 352.127690
iter  50 value 352.121024
iter  60 value 346.179362
iter  70 value 341.196048
iter  80 value 340.002413
iter  90 value 339.849580
iter 100 value 339.806779
final  value 339.806779 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 792.300973 
iter  10 value 376.784865
iter  20 value 364.420050
iter  30 value 362.950154
iter  40 value 362.746226
iter  50 value 362.728339
iter  60 value 362.724428
iter  70 value 362.722955
iter  80 value 362.722135
iter  90 value 362.721715
iter 100 value 362.721372
final  value 362.721372 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 753.847745 
iter  10 value 397.636056
iter  20 value 394.142588
iter  30 value 393.474835
iter  40 value 382.023188
iter  50 value 377.444424
iter  60 value 377.241130
iter  70 value 377.209830
iter  80 value 377.196386
iter  90 value 377.185627
iter 100 value 377.181751
final  value 377.181751 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  89
initial  value 1153.776181 
iter  10 value 408.455845
iter  20 value 394.615140
iter  30 value 384.230910
iter  40 value 382.534663
iter  50 value 382.213978
iter  60 value 382.162751
iter  70 value 382.151468
iter  80 value 382.146755
iter  90 value 382.143326
iter 100 value 382.140937
final  value 382.140937 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 896.077075 
iter  10 value 381.134164
iter  20 value 374.602375
iter  30 value 362.591349
iter  40 value 359.467478
iter  50 value 358.835453
iter  60 value 358.723129
iter  70 value 358.698721
iter  80 value 358.692529
iter  90 value 358.689767
iter 100 value 358.688363
final  value 358.688363 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 762.638523 
iter  10 value 377.344231
iter  20 value 373.417611
iter  30 value 373.025764
iter  40 value 363.181607
iter  50 value 354.110306
iter  60 value 352.132236
iter  70 value 351.945134
iter  80 value 351.913000
iter  90 value 351.904959
iter 100 value 351.898457
final  value 351.898457 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  199
initial  value 1106.428874 
iter  10 value 377.151363
iter  20 value 370.471150
iter  30 value 370.379145
iter  40 value 370.191029
iter  50 value 357.835524
iter  60 value 353.857510
iter  70 value 352.793028
iter  80 value 352.561021
iter  90 value 352.524950
iter 100 value 352.511257
final  value 352.511257 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 685.210732 
iter  10 value 377.471388
iter  20 value 371.838901
iter  30 value 370.524537
iter  40 value 370.390629
iter  50 value 354.336299
iter  60 value 352.948880
iter  70 value 352.662755
iter  80 value 352.575350
iter  90 value 352.544369
iter 100 value 352.521573
final  value 352.521573 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 481.033542 
iter  10 value 370.476228
iter  20 value 370.307311
iter  30 value 353.720638
iter  40 value 352.687580
iter  50 value 352.570472
iter  60 value 352.513673
iter  70 value 352.492994
iter  80 value 352.481709
iter  90 value 352.475236
iter 100 value 352.471889
final  value 352.471889 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 494.889908 
iter  10 value 371.149411
iter  20 value 370.381086
iter  30 value 363.937048
iter  40 value 356.446351
iter  50 value 353.940483
iter  60 value 352.995729
iter  70 value 352.640742
iter  80 value 352.543480
iter  90 value 352.504304
iter 100 value 352.487727
final  value 352.487727 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 523.948152 
iter  10 value 374.398719
iter  20 value 362.722957
iter  30 value 356.589844
iter  40 value 354.170838
iter  50 value 353.015012
iter  60 value 352.643037
iter  70 value 352.538727
iter  80 value 352.503242
iter  90 value 352.494634
iter 100 value 352.487088
final  value 352.487088 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  144
initial  value 959.052851 
iter  10 value 364.572728
iter  20 value 364.287078
iter  30 value 352.052116
iter  40 value 350.289192
iter  50 value 349.930125
iter  60 value 349.749631
iter  70 value 349.704986
iter  80 value 349.680164
iter  90 value 349.674127
iter 100 value 349.670685
final  value 349.670685 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  144
initial  value 901.404357 
iter  10 value 349.588854
iter  20 value 347.112327
iter  30 value 346.631597
iter  40 value 344.359226
iter  50 value 331.692642
iter  60 value 327.997915
iter  70 value 327.420221
iter  80 value 327.240862
iter  90 value 327.186840
iter 100 value 327.151667
final  value 327.151667 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  144
initial  value 720.159239 
iter  10 value 381.259714
iter  20 value 379.306532
iter  30 value 378.971587
iter  40 value 378.846419
iter  50 value 378.831495
iter  60 value 378.815373
iter  70 value 378.580250
iter  80 value 361.919753
iter  90 value 360.832579
iter 100 value 360.773847
final  value 360.773847 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  144
initial  value 591.720756 
iter  10 value 356.437477
iter  20 value 355.028179
iter  30 value 354.667794
iter  40 value 354.503591
iter  50 value 354.150741
iter  60 value 344.122053
iter  70 value 339.365692
iter  80 value 338.582826
iter  90 value 338.131739
iter 100 value 337.994574
final  value 337.994574 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  144
initial  value 400.960778 
iter  10 value 344.239046
iter  20 value 344.068166
iter  30 value 327.444745
iter  40 value 325.732484
iter  50 value 325.416880
iter  60 value 325.263889
iter  70 value 325.206583
iter  80 value 325.180500
iter  90 value 325.165583
iter 100 value 325.158939
final  value 325.158939 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  56
initial  value 892.748997 
iter  10 value 382.934107
iter  20 value 371.444196
iter  30 value 370.253635
iter  40 value 356.607454
iter  50 value 353.971371
iter  60 value 353.685248
iter  70 value 353.669446
iter  80 value 353.656461
iter  90 value 353.649068
iter 100 value 353.643985
final  value 353.643985 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 972.034677 
iter  10 value 372.117537
iter  20 value 370.369490
iter  30 value 356.530752
iter  40 value 353.978625
iter  50 value 353.805185
iter  60 value 353.796844
iter  70 value 353.789257
iter  80 value 353.703272
iter  90 value 353.624021
iter 100 value 353.618520
final  value 353.618520 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 558.460747 
iter  10 value 376.281856
iter  20 value 370.814008
iter  30 value 368.886436
iter  40 value 355.483300
iter  50 value 353.783420
iter  60 value 353.679985
iter  70 value 353.655650
iter  80 value 353.648849
iter  90 value 353.644260
iter 100 value 353.643162
final  value 353.643162 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 783.245993 
iter  10 value 371.341742
iter  20 value 367.546418
iter  30 value 354.986267
iter  40 value 353.807101
iter  50 value 353.648138
iter  60 value 353.611733
iter  70 value 353.607568
iter  80 value 353.606114
iter  90 value 353.605320
iter 100 value 353.605165
final  value 353.605165 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 647.754744 
iter  10 value 378.298965
iter  20 value 370.406762
iter  30 value 355.824676
iter  40 value 354.273720
iter  50 value 353.852927
iter  60 value 353.656159
iter  70 value 353.618870
iter  80 value 353.609698
iter  90 value 353.605792
iter 100 value 353.604626
final  value 353.604626 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  188
initial  value 608.834500 
iter  10 value 371.023408
iter  20 value 369.107696
iter  30 value 353.378701
iter  40 value 352.401347
iter  50 value 352.245730
iter  60 value 352.142112
iter  70 value 352.094350
iter  80 value 352.044368
iter  90 value 352.020580
iter 100 value 352.011438
final  value 352.011438 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 531.770790 
iter  10 value 370.864774
iter  20 value 370.571544
iter  30 value 370.445696
iter  40 value 370.349490
iter  50 value 358.028491
iter  60 value 352.954372
iter  70 value 352.288759
iter  80 value 352.081844
iter  90 value 352.052927
iter 100 value 352.032782
final  value 352.032782 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 1212.883790 
iter  10 value 378.639930
iter  20 value 370.432309
iter  30 value 370.347894
iter  40 value 370.322246
iter  50 value 367.214343
iter  60 value 355.645919
iter  70 value 352.705373
iter  80 value 352.376498
iter  90 value 352.286122
iter 100 value 352.158798
final  value 352.158798 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 805.845985 
iter  10 value 386.133223
iter  20 value 370.518701
iter  30 value 370.351103
iter  40 value 370.291267
iter  50 value 355.908169
iter  60 value 352.388137
iter  70 value 352.161866
iter  80 value 352.112852
iter  90 value 352.066438
iter 100 value 352.044325
final  value 352.044325 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 386.998030 
iter  10 value 370.471697
iter  20 value 370.346609
iter  30 value 370.316111
iter  40 value 361.615779
iter  50 value 352.782605
iter  60 value 352.205885
iter  70 value 352.110366
iter  80 value 352.048865
iter  90 value 352.030906
iter 100 value 352.020336
final  value 352.020336 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  12
initial  value 774.904158 
iter  10 value 387.377120
final  value 387.373920 
converged
Fitting Repeat 2 

# weights:  12
initial  value 778.493056 
iter  10 value 425.349126
final  value 424.634409 
converged
Fitting Repeat 3 

# weights:  12
initial  value 927.561456 
iter  10 value 401.982924
final  value 400.710796 
converged
Fitting Repeat 4 

# weights:  12
initial  value 713.620630 
iter  10 value 441.668457
final  value 440.540487 
converged
Fitting Repeat 5 

# weights:  12
initial  value 815.814658 
iter  10 value 447.774840
iter  20 value 437.630332
iter  20 value 437.630332
iter  20 value 437.630332
final  value 437.630332 
converged
Fitting Repeat 1 

# weights:  210
initial  value 1250.299475 
iter  10 value 550.043859
iter  20 value 547.526244
iter  30 value 528.539765
iter  40 value 523.118256
iter  50 value 521.839796
iter  60 value 521.551855
iter  70 value 521.460757
iter  80 value 521.382245
iter  90 value 521.344973
iter 100 value 521.323815
final  value 521.323815 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  210
initial  value 1244.586681 
iter  10 value 549.009445
iter  20 value 547.714524
iter  30 value 547.301555
iter  40 value 547.284545
iter  50 value 547.154326
iter  60 value 538.366102
iter  70 value 527.113152
iter  80 value 522.170967
iter  90 value 521.721275
iter 100 value 521.604318
final  value 521.604318 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  210
initial  value 950.744282 
iter  10 value 549.225571
iter  20 value 547.509194
iter  30 value 544.307022
iter  40 value 523.077628
iter  50 value 521.768942
iter  60 value 521.525881
iter  70 value 521.440379
iter  80 value 521.407157
iter  90 value 521.360565
iter 100 value 521.337304
final  value 521.337304 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  210
initial  value 1106.446462 
iter  10 value 549.958858
iter  20 value 547.566521
iter  30 value 547.458782
iter  40 value 547.292874
iter  50 value 538.328423
iter  60 value 523.449266
iter  70 value 521.760185
iter  80 value 521.490050
iter  90 value 521.404270
iter 100 value 521.375072
final  value 521.375072 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  210
initial  value 1415.813832 
iter  10 value 528.565404
iter  20 value 522.467941
iter  30 value 521.718796
iter  40 value 521.591638
iter  50 value 521.489816
iter  60 value 521.420444
iter  70 value 521.374667
iter  80 value 521.335592
iter  90 value 521.324275
iter 100 value 521.315250
final  value 521.315250 
stopped after 100 iterations
Model Averaged Neural Network 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  size  decay         bag    RMSE       Rsquared   MAE        Selected
   1    4.828817e+00   TRUE  0.8727031  0.5815972  0.7203295          
   5    2.001656e-04   TRUE  0.8332810  0.4499262  0.6414102          
   5    1.800074e-02  FALSE  0.8336462  0.4416285  0.6439238          
   6    1.021320e-05   TRUE  0.8535910  0.3989969  0.7045549          
   6    6.637047e-05   TRUE  0.8454426  0.3907330  0.6867681          
   7    4.343504e-03  FALSE  0.8332382  0.4480613  0.6404673          
   7    3.787248e-01  FALSE  0.8391993  0.4558757  0.6668093          
   8    7.006547e-03   TRUE  0.8333471  0.4459560  0.6416339          
  12    7.197236e-02  FALSE  0.8346226  0.4290585  0.6496283          
  13    1.969011e-03   TRUE  0.8331539  0.4470358  0.6396287          
  13    6.081004e-01   TRUE  0.8496879  0.4518456  0.6969069          
  16    1.553060e-03  FALSE  0.8330963  0.4498184  0.6384477          
  17    2.583383e-04   TRUE  0.8337474  0.4451293  0.6470890          
  17    6.085167e-04  FALSE  0.8330505  0.4503120  0.6376937          
  17    2.138257e-03  FALSE  0.8331371  0.4489950  0.6391356          
  17    9.728275e+00  FALSE  0.8610331  0.7495797  0.7103880          
  18    2.092909e-04   TRUE  0.8331756  0.4433797  0.6399614          
  18    4.458840e-03  FALSE  0.8332512  0.4470283  0.6406296          
  19    5.247859e-04  FALSE  0.8330492  0.4496698  0.6379410  *       
  20    1.238569e-04  FALSE  0.8355853  0.4332577  0.6541457          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 19, decay = 0.0005247859
 and bag = FALSE.
[1] "Mon Mar 12 00:12:02 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 00:12:08 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "bag"                            
Loading required package: plotmo
Loading required package: plotrix
Loading required package: TeachingDemos
Bagged MARS 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  degree  nprune  RMSE        Rsquared   MAE         Selected
  1       3       0.07229172  0.9873468  0.05610584          
  1       4       0.05043597  0.9938373  0.03981744          
  1       5       0.03131749  0.9976253  0.02438338          
  1       6       0.02241843  0.9987946  0.01720229          
  1       7       0.02048781  0.9989897  0.01583589          
  2       2       0.12868303  0.9608439  0.10110671          
  2       3       0.07104653  0.9877913  0.05463914          
  2       4       0.04946959  0.9940458  0.03922649          
  2       5       0.03066287  0.9977292  0.02391959          
  2       7       0.01952495  0.9990749  0.01493685  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nprune = 7 and degree = 2.
[1] "Mon Mar 12 00:12:47 2018"
Bagged MARS using gCV Pruning 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results:

  RMSE       Rsquared   MAE       
  0.0201288  0.9990198  0.01549045

Tuning parameter 'degree' was held constant at a value of 1
[1] "Mon Mar 12 00:13:20 2018"
Loading required package: nlme

Attaching package: 'nlme'

The following object is masked from 'package:dplyr':

    collapse

This is mgcv 1.8-20. For overview type 'help("mgcv-package")'.

Attaching package: 'mgcv'

The following object is masked from 'package:nnet':

    multinom

Generalized Additive Model using Splines 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  select  method  RMSE         Rsquared   MAE          Selected
  FALSE   GCV.Cp  0.003299285  0.9999741  0.002655987          
  FALSE   ML      0.003278597  0.9999744  0.002642069          
  FALSE   REML    0.003268447  0.9999746  0.002638416          
   TRUE   GCV.Cp  0.003328527  0.9999737  0.002662683          
   TRUE   ML      0.003265554  0.9999747  0.002634282          
   TRUE   REML    0.003265508  0.9999747  0.002634296  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were select = TRUE and method = REML.
[1] "Mon Mar 12 00:16:41 2018"
Loading required package: rJava
Loading required package: bartMachineJARs
Loading required package: car

Attaching package: 'car'

The following object is masked from 'package:dplyr':

    recode

Loading required package: missForest
Loading required package: itertools
Loading required package: iterators
Welcome to bartMachine v1.2.3! You have 0.48GB memory available.

If you run out of memory, restart R, and use e.g.
'options(java.parameters = "-Xmx5g")' for 5GB of RAM before you call
'library(bartMachine)'.

bartMachine initializing with 86 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 67.2/477.6MB
Iteration 200/1250  mem: 72.3/477.6MB
Iteration 300/1250  mem: 142.5/477.6MB
Iteration 400/1250  mem: 161.5/477.6MB
Iteration 500/1250  mem: 154.9/477.6MB
Iteration 600/1250  mem: 145.2/477.6MB
Iteration 700/1250  mem: 132.7/477.6MB
Iteration 800/1250  mem: 89.6/477.6MB
Iteration 900/1250  mem: 82.6/477.6MB
Iteration 1000/1250  mem: 73.8/477.6MB
Iteration 1100/1250  mem: 222.4/477.6MB
Iteration 1200/1250  mem: 214.2/477.6MB
done building BART in 4.75 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 84 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 240.6/477.6MB
Iteration 200/1250  mem: 248.8/477.6MB
Iteration 300/1250  mem: 250.3/477.6MB
Iteration 400/1250  mem: 253.4/477.6MB
Iteration 500/1250  mem: 251.9/477.6MB
Iteration 600/1250  mem: 248.9/477.6MB
Iteration 700/1250  mem: 245/477.6MB
Iteration 800/1250  mem: 235.2/477.6MB
Iteration 900/1250  mem: 225.6/477.6MB
Iteration 1000/1250  mem: 199/477.6MB
Iteration 1100/1250  mem: 182.3/477.6MB
Iteration 1200/1250  mem: 165.9/477.6MB
done building BART in 3.516 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 207.7/477.6MB
Iteration 200/1250  mem: 235.8/477.6MB
Iteration 300/1250  mem: 256.9/477.6MB
Iteration 400/1250  mem: 278.6/477.6MB
Iteration 500/1250  mem: 202.4/477.6MB
Iteration 600/1250  mem: 217.2/477.6MB
Iteration 700/1250  mem: 224.6/477.6MB
Iteration 800/1250  mem: 229.9/477.6MB
Iteration 900/1250  mem: 236.2/477.6MB
Iteration 1000/1250  mem: 242.9/477.6MB
Iteration 1100/1250  mem: 242.2/477.6MB
Iteration 1200/1250  mem: 244.5/477.6MB
done building BART in 3.344 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 63 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 194.4/477.6MB
Iteration 200/1250  mem: 266.2/477.6MB
Iteration 300/1250  mem: 181.1/477.6MB
Iteration 400/1250  mem: 186.2/477.6MB
Iteration 500/1250  mem: 254.2/477.6MB
Iteration 600/1250  mem: 164.6/477.6MB
Iteration 700/1250  mem: 228.9/477.6MB
Iteration 800/1250  mem: 136.1/477.6MB
Iteration 900/1250  mem: 198.5/477.6MB
Iteration 1000/1250  mem: 260.4/477.6MB
Iteration 1100/1250  mem: 163.1/477.6MB
Iteration 1200/1250  mem: 221.3/477.6MB
done building BART in 2.312 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 94 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 253/477.6MB
Iteration 200/1250  mem: 298/477.6MB
Iteration 300/1250  mem: 180.2/477.6MB
Iteration 400/1250  mem: 133.2/477.6MB
Iteration 500/1250  mem: 166.1/477.6MB
Iteration 600/1250  mem: 191.5/477.6MB
Iteration 700/1250  mem: 213.5/477.6MB
Iteration 800/1250  mem: 231/477.6MB
Iteration 900/1250  mem: 250.4/477.6MB
Iteration 1000/1250  mem: 262.2/477.6MB
Iteration 1100/1250  mem: 278.1/477.6MB
Iteration 1200/1250  mem: 288.8/477.6MB
done building BART in 3.469 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 80 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 307.2/477.6MB
Iteration 200/1250  mem: 284.3/477.6MB
Iteration 300/1250  mem: 266/477.6MB
Iteration 400/1250  mem: 249.1/477.6MB
Iteration 500/1250  mem: 233/477.6MB
Iteration 600/1250  mem: 209.7/477.6MB
Iteration 700/1250  mem: 182.2/477.6MB
Iteration 800/1250  mem: 315.8/477.6MB
Iteration 900/1250  mem: 212.3/477.6MB
Iteration 1000/1250  mem: 178.5/477.6MB
Iteration 1100/1250  mem: 140.4/477.6MB
Iteration 1200/1250  mem: 261.5/477.6MB
done building BART in 3.11 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 41 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 252.8/477.6MB
Iteration 200/1250  mem: 241.8/477.6MB
Iteration 300/1250  mem: 230.1/477.6MB
Iteration 400/1250  mem: 220/477.6MB
Iteration 500/1250  mem: 205.7/477.6MB
Iteration 600/1250  mem: 192.2/477.6MB
Iteration 700/1250  mem: 176.9/477.6MB
Iteration 800/1250  mem: 164.3/477.6MB
Iteration 900/1250  mem: 309.1/477.6MB
Iteration 1000/1250  mem: 296.6/477.6MB
Iteration 1100/1250  mem: 285/477.6MB
Iteration 1200/1250  mem: 269/477.6MB
done building BART in 1.438 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 98 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 264/477.6MB
Iteration 200/1250  mem: 326.5/477.6MB
Iteration 300/1250  mem: 231.3/477.6MB
Iteration 400/1250  mem: 283.8/477.6MB
Iteration 500/1250  mem: 199.7/477.6MB
Iteration 600/1250  mem: 86.1/477.6MB
Iteration 700/1250  mem: 123.8/477.6MB
Iteration 800/1250  mem: 155.6/477.6MB
Iteration 900/1250  mem: 186.9/477.6MB
Iteration 1000/1250  mem: 216.3/477.6MB
Iteration 1100/1250  mem: 244.9/477.6MB
Iteration 1200/1250  mem: 270.6/477.6MB
done building BART in 3.578 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 65 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 230.8/477.6MB
Iteration 200/1250  mem: 151/477.6MB
Iteration 300/1250  mem: 234.3/477.6MB
Iteration 400/1250  mem: 160.8/477.6MB
Iteration 500/1250  mem: 239.6/477.6MB
Iteration 600/1250  mem: 157.4/477.6MB
Iteration 700/1250  mem: 234.5/477.6MB
Iteration 800/1250  mem: 308.6/477.6MB
Iteration 900/1250  mem: 221.5/477.6MB
Iteration 1000/1250  mem: 293.6/477.6MB
Iteration 1100/1250  mem: 205.4/477.6MB
Iteration 1200/1250  mem: 217.2/477.6MB
done building BART in 2.453 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 30 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 193.5/477.6MB
Iteration 200/1250  mem: 198.2/477.6MB
Iteration 300/1250  mem: 204.3/477.6MB
Iteration 400/1250  mem: 212.5/477.6MB
Iteration 500/1250  mem: 220.7/477.6MB
Iteration 600/1250  mem: 228.4/477.6MB
Iteration 700/1250  mem: 235.9/477.6MB
Iteration 800/1250  mem: 243.1/477.6MB
Iteration 900/1250  mem: 250.3/477.6MB
Iteration 1000/1250  mem: 258.5/477.6MB
Iteration 1100/1250  mem: 265.6/477.6MB
Iteration 1200/1250  mem: 274/477.6MB
done building BART in 2.125 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 36 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 187.3/477.6MB
Iteration 200/1250  mem: 160.7/477.6MB
Iteration 300/1250  mem: 291.7/477.6MB
Iteration 400/1250  mem: 264.6/477.6MB
Iteration 500/1250  mem: 245.7/477.6MB
Iteration 600/1250  mem: 222.6/477.6MB
Iteration 700/1250  mem: 208.8/477.6MB
Iteration 800/1250  mem: 196.6/477.6MB
Iteration 900/1250  mem: 185/477.6MB
Iteration 1000/1250  mem: 175.7/477.6MB
Iteration 1100/1250  mem: 166.6/477.6MB
Iteration 1200/1250  mem: 299.2/477.6MB
done building BART in 1.312 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 41 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 252.3/477.6MB
Iteration 200/1250  mem: 257.1/477.6MB
Iteration 300/1250  mem: 259.2/477.6MB
Iteration 400/1250  mem: 264.4/477.6MB
Iteration 500/1250  mem: 267.5/477.6MB
Iteration 600/1250  mem: 270.8/477.6MB
Iteration 700/1250  mem: 272.6/477.6MB
Iteration 800/1250  mem: 274.8/477.6MB
Iteration 900/1250  mem: 272.8/477.6MB
Iteration 1000/1250  mem: 271.3/477.6MB
Iteration 1100/1250  mem: 268.8/477.6MB
Iteration 1200/1250  mem: 267.5/477.6MB
done building BART in 1.437 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 35 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 339.6/477.6MB
Iteration 200/1250  mem: 317.2/477.6MB
Iteration 300/1250  mem: 293.2/477.6MB
Iteration 400/1250  mem: 267.7/477.6MB
Iteration 500/1250  mem: 242.8/477.6MB
Iteration 600/1250  mem: 373.5/477.6MB
Iteration 700/1250  mem: 346.9/477.6MB
Iteration 800/1250  mem: 320.4/477.6MB
Iteration 900/1250  mem: 296/477.6MB
Iteration 1000/1250  mem: 267.2/477.6MB
Iteration 1100/1250  mem: 68.9/477.6MB
Iteration 1200/1250  mem: 198.9/477.6MB
done building BART in 1.422 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 84 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 147.5/477.6MB
Iteration 200/1250  mem: 154/477.6MB
Iteration 300/1250  mem: 154.3/477.6MB
Iteration 400/1250  mem: 149.3/477.6MB
Iteration 500/1250  mem: 137/477.6MB
Iteration 600/1250  mem: 123/477.6MB
Iteration 700/1250  mem: 265.7/477.6MB
Iteration 800/1250  mem: 249/477.6MB
Iteration 900/1250  mem: 228.2/477.6MB
Iteration 1000/1250  mem: 203.9/477.6MB
Iteration 1100/1250  mem: 179/477.6MB
Iteration 1200/1250  mem: 153.8/477.6MB
done building BART in 3.0 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 45 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 168.6/477.6MB
Iteration 200/1250  mem: 174.5/477.6MB
Iteration 300/1250  mem: 176.5/477.6MB
Iteration 400/1250  mem: 183.5/477.6MB
Iteration 500/1250  mem: 187.2/477.6MB
Iteration 600/1250  mem: 190.6/477.6MB
Iteration 700/1250  mem: 193.6/477.6MB
Iteration 800/1250  mem: 196.5/477.6MB
Iteration 900/1250  mem: 196.4/477.6MB
Iteration 1000/1250  mem: 198.9/477.6MB
Iteration 1100/1250  mem: 199/477.6MB
Iteration 1200/1250  mem: 201.7/477.6MB
done building BART in 1.578 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 193.7/477.6MB
Iteration 200/1250  mem: 218.8/477.6MB
Iteration 300/1250  mem: 88.6/477.6MB
Iteration 400/1250  mem: 105.9/477.6MB
Iteration 500/1250  mem: 123.1/477.6MB
Iteration 600/1250  mem: 129.9/477.6MB
Iteration 700/1250  mem: 141.2/477.6MB
Iteration 800/1250  mem: 143.9/477.6MB
Iteration 900/1250  mem: 147.3/477.6MB
Iteration 1000/1250  mem: 150.3/477.6MB
Iteration 1100/1250  mem: 149.9/477.6MB
Iteration 1200/1250  mem: 148.9/477.6MB
done building BART in 3.282 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 68 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 272.1/477.6MB
Iteration 200/1250  mem: 210/477.6MB
Iteration 300/1250  mem: 301.4/477.6MB
Iteration 400/1250  mem: 239.4/477.6MB
Iteration 500/1250  mem: 179.2/477.6MB
Iteration 600/1250  mem: 267.8/477.6MB
Iteration 700/1250  mem: 135/477.6MB
Iteration 800/1250  mem: 223/477.6MB
Iteration 900/1250  mem: 150/477.6MB
Iteration 1000/1250  mem: 234.4/477.6MB
Iteration 1100/1250  mem: 160/477.6MB
Iteration 1200/1250  mem: 244.8/477.6MB
done building BART in 2.594 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 31 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 180.8/477.6MB
Iteration 200/1250  mem: 295.4/477.6MB
Iteration 300/1250  mem: 245.7/477.6MB
Iteration 400/1250  mem: 202.3/477.6MB
Iteration 500/1250  mem: 313.8/477.6MB
Iteration 600/1250  mem: 274.5/477.6MB
Iteration 700/1250  mem: 230.2/477.6MB
Iteration 800/1250  mem: 189.8/477.6MB
Iteration 900/1250  mem: 305.7/477.6MB
Iteration 1000/1250  mem: 261.4/477.6MB
Iteration 1100/1250  mem: 219.5/477.6MB
Iteration 1200/1250  mem: 331.6/477.6MB
done building BART in 1.047 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 85 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 275.1/477.6MB
Iteration 200/1250  mem: 289.9/477.6MB
Iteration 300/1250  mem: 296.1/477.6MB
Iteration 400/1250  mem: 305.2/477.6MB
Iteration 500/1250  mem: 310.7/477.6MB
Iteration 600/1250  mem: 313.8/477.6MB
Iteration 700/1250  mem: 313.7/477.6MB
Iteration 800/1250  mem: 306.7/477.6MB
Iteration 900/1250  mem: 296.6/477.6MB
Iteration 1000/1250  mem: 126.8/477.6MB
Iteration 1100/1250  mem: 116/477.6MB
Iteration 1200/1250  mem: 262/477.6MB
done building BART in 3.25 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 10 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 145.8/477.6MB
Iteration 200/1250  mem: 182.2/477.6MB
Iteration 300/1250  mem: 220.2/477.6MB
Iteration 400/1250  mem: 258.2/477.6MB
Iteration 500/1250  mem: 137.5/477.6MB
Iteration 600/1250  mem: 177.5/477.6MB
Iteration 700/1250  mem: 215.3/477.6MB
Iteration 800/1250  mem: 253.1/477.6MB
Iteration 900/1250  mem: 135/477.6MB
Iteration 1000/1250  mem: 173.7/477.6MB
Iteration 1100/1250  mem: 212.3/477.6MB
Iteration 1200/1250  mem: 251/477.6MB
done building BART in 0.328 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 86 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 169.6/477.6MB
Iteration 200/1250  mem: 199.6/477.6MB
Iteration 300/1250  mem: 221.7/477.6MB
Iteration 400/1250  mem: 240.1/477.6MB
Iteration 500/1250  mem: 252.8/477.6MB
Iteration 600/1250  mem: 263.8/477.6MB
Iteration 700/1250  mem: 272.1/477.6MB
Iteration 800/1250  mem: 276.4/477.6MB
Iteration 900/1250  mem: 274.3/477.6MB
Iteration 1000/1250  mem: 272.4/477.6MB
Iteration 1100/1250  mem: 267.1/477.6MB
Iteration 1200/1250  mem: 258.4/477.6MB
done building BART in 3.062 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 84 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 142.5/477.6MB
Iteration 200/1250  mem: 150.6/477.6MB
Iteration 300/1250  mem: 154.7/477.6MB
Iteration 400/1250  mem: 163/477.6MB
Iteration 500/1250  mem: 164.6/477.6MB
Iteration 600/1250  mem: 163.3/477.6MB
Iteration 700/1250  mem: 158.5/477.6MB
Iteration 800/1250  mem: 153.3/477.6MB
Iteration 900/1250  mem: 144.8/477.6MB
Iteration 1000/1250  mem: 132.5/477.6MB
Iteration 1100/1250  mem: 278.6/477.6MB
Iteration 1200/1250  mem: 256.3/477.6MB
done building BART in 3.219 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 301.6/477.6MB
Iteration 200/1250  mem: 191.2/477.6MB
Iteration 300/1250  mem: 216.8/477.6MB
Iteration 400/1250  mem: 242.4/477.6MB
Iteration 500/1250  mem: 262.1/477.6MB
Iteration 600/1250  mem: 275/477.6MB
Iteration 700/1250  mem: 284.7/477.6MB
Iteration 800/1250  mem: 287.5/477.6MB
Iteration 900/1250  mem: 290.5/477.6MB
Iteration 1000/1250  mem: 293.4/477.6MB
Iteration 1100/1250  mem: 295.6/477.6MB
Iteration 1200/1250  mem: 291.1/477.6MB
done building BART in 3.235 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 63 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 387.7/477.6MB
Iteration 200/1250  mem: 147.2/477.6MB
Iteration 300/1250  mem: 227.7/477.6MB
Iteration 400/1250  mem: 154.9/477.6MB
Iteration 500/1250  mem: 226/477.6MB
Iteration 600/1250  mem: 142.7/477.6MB
Iteration 700/1250  mem: 216.7/477.6MB
Iteration 800/1250  mem: 128.4/477.6MB
Iteration 900/1250  mem: 199/477.6MB
Iteration 1000/1250  mem: 266.8/477.6MB
Iteration 1100/1250  mem: 176.6/477.6MB
Iteration 1200/1250  mem: 245/477.6MB
done building BART in 2.328 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 94 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 274/477.6MB
Iteration 200/1250  mem: 163.8/477.6MB
Iteration 300/1250  mem: 207/477.6MB
Iteration 400/1250  mem: 250.2/477.6MB
Iteration 500/1250  mem: 287.3/477.6MB
Iteration 600/1250  mem: 323/477.6MB
Iteration 700/1250  mem: 194.5/477.6MB
Iteration 800/1250  mem: 218.9/477.6MB
Iteration 900/1250  mem: 243/477.6MB
Iteration 1000/1250  mem: 264.8/477.6MB
Iteration 1100/1250  mem: 283.4/477.6MB
Iteration 1200/1250  mem: 297.5/477.6MB
done building BART in 3.422 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 80 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 326.2/477.6MB
Iteration 200/1250  mem: 313.9/477.6MB
Iteration 300/1250  mem: 139.1/477.6MB
Iteration 400/1250  mem: 120.2/477.6MB
Iteration 500/1250  mem: 251.6/477.6MB
Iteration 600/1250  mem: 227.6/477.6MB
Iteration 700/1250  mem: 197.9/477.6MB
Iteration 800/1250  mem: 167.4/477.6MB
Iteration 900/1250  mem: 133/477.6MB
Iteration 1000/1250  mem: 260/477.6MB
Iteration 1100/1250  mem: 222.8/477.6MB
Iteration 1200/1250  mem: 184.4/477.6MB
done building BART in 3.031 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 41 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 182.4/477.6MB
Iteration 200/1250  mem: 170.5/477.6MB
Iteration 300/1250  mem: 158.6/477.6MB
Iteration 400/1250  mem: 149.4/477.6MB
Iteration 500/1250  mem: 294.8/477.6MB
Iteration 600/1250  mem: 281.4/477.6MB
Iteration 700/1250  mem: 266.3/477.6MB
Iteration 800/1250  mem: 257.1/477.6MB
Iteration 900/1250  mem: 241.4/477.6MB
Iteration 1000/1250  mem: 228.1/477.6MB
Iteration 1100/1250  mem: 212.2/477.6MB
Iteration 1200/1250  mem: 195.3/477.6MB
done building BART in 1.406 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 98 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 203.2/477.6MB
Iteration 200/1250  mem: 256.7/477.6MB
Iteration 300/1250  mem: 309/477.6MB
Iteration 400/1250  mem: 357.8/477.6MB
Iteration 500/1250  mem: 250.6/477.6MB
Iteration 600/1250  mem: 292.6/477.6MB
Iteration 700/1250  mem: 331.4/477.6MB
Iteration 800/1250  mem: 367.8/477.6MB
Iteration 900/1250  mem: 243.1/477.6MB
Iteration 1000/1250  mem: 272.2/477.6MB
Iteration 1100/1250  mem: 150.2/477.6MB
Iteration 1200/1250  mem: 174.5/477.6MB
done building BART in 3.688 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 65 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 144.4/477.6MB
Iteration 200/1250  mem: 229.7/477.6MB
Iteration 300/1250  mem: 151.7/477.6MB
Iteration 400/1250  mem: 232/477.6MB
Iteration 500/1250  mem: 155.3/477.6MB
Iteration 600/1250  mem: 231.3/477.6MB
Iteration 700/1250  mem: 308/477.6MB
Iteration 800/1250  mem: 226/477.6MB
Iteration 900/1250  mem: 300.8/477.6MB
Iteration 1000/1250  mem: 213/477.6MB
Iteration 1100/1250  mem: 285.2/477.6MB
Iteration 1200/1250  mem: 196.2/477.6MB
done building BART in 2.297 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 30 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 337.1/477.6MB
Iteration 200/1250  mem: 344.2/477.6MB
Iteration 300/1250  mem: 188.8/477.6MB
Iteration 400/1250  mem: 195.6/477.6MB
Iteration 500/1250  mem: 202.5/477.6MB
Iteration 600/1250  mem: 212.6/477.6MB
Iteration 700/1250  mem: 219.8/477.6MB
Iteration 800/1250  mem: 226.8/477.6MB
Iteration 900/1250  mem: 234/477.6MB
Iteration 1000/1250  mem: 244.5/477.6MB
Iteration 1100/1250  mem: 251.6/477.6MB
Iteration 1200/1250  mem: 255.8/477.6MB
done building BART in 2.11 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 36 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 337.5/477.6MB
Iteration 200/1250  mem: 311.9/477.6MB
Iteration 300/1250  mem: 285.5/477.6MB
Iteration 400/1250  mem: 259.9/477.6MB
Iteration 500/1250  mem: 234.2/477.6MB
Iteration 600/1250  mem: 204.9/477.6MB
Iteration 700/1250  mem: 335/477.6MB
Iteration 800/1250  mem: 305.5/477.6MB
Iteration 900/1250  mem: 279/477.6MB
Iteration 1000/1250  mem: 249/477.6MB
Iteration 1100/1250  mem: 222.5/477.6MB
Iteration 1200/1250  mem: 354.2/477.6MB
done building BART in 1.281 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 41 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 273.4/477.6MB
Iteration 200/1250  mem: 268.2/477.6MB
Iteration 300/1250  mem: 260.7/477.6MB
Iteration 400/1250  mem: 253.5/477.6MB
Iteration 500/1250  mem: 245/477.6MB
Iteration 600/1250  mem: 392.2/477.6MB
Iteration 700/1250  mem: 382.2/477.6MB
Iteration 800/1250  mem: 371.2/477.6MB
Iteration 900/1250  mem: 359.4/477.6MB
Iteration 1000/1250  mem: 351.7/477.6MB
Iteration 1100/1250  mem: 339.3/477.6MB
Iteration 1200/1250  mem: 324.5/477.6MB
done building BART in 1.406 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 35 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 392.4/477.6MB
Iteration 200/1250  mem: 142.6/477.6MB
Iteration 300/1250  mem: 114.3/477.6MB
Iteration 400/1250  mem: 84.3/477.6MB
Iteration 500/1250  mem: 57.9/477.6MB
Iteration 600/1250  mem: 188.8/477.6MB
Iteration 700/1250  mem: 163.9/477.6MB
Iteration 800/1250  mem: 133.3/477.6MB
Iteration 900/1250  mem: 104.6/477.6MB
Iteration 1000/1250  mem: 74.7/477.6MB
Iteration 1100/1250  mem: 205.7/477.6MB
Iteration 1200/1250  mem: 176.8/477.6MB
done building BART in 1.406 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 84 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 124/477.6MB
Iteration 200/1250  mem: 122.4/477.6MB
Iteration 300/1250  mem: 118.6/477.6MB
Iteration 400/1250  mem: 111.9/477.6MB
Iteration 500/1250  mem: 258.1/477.6MB
Iteration 600/1250  mem: 243.5/477.6MB
Iteration 700/1250  mem: 226.9/477.6MB
Iteration 800/1250  mem: 203.7/477.6MB
Iteration 900/1250  mem: 179.1/477.6MB
Iteration 1000/1250  mem: 151/477.6MB
Iteration 1100/1250  mem: 285.7/477.6MB
Iteration 1200/1250  mem: 260.7/477.6MB
done building BART in 2.985 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 45 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 274.5/477.6MB
Iteration 200/1250  mem: 279/477.6MB
Iteration 300/1250  mem: 283.9/477.6MB
Iteration 400/1250  mem: 289/477.6MB
Iteration 500/1250  mem: 294.3/477.6MB
Iteration 600/1250  mem: 297.6/477.6MB
Iteration 700/1250  mem: 302.8/477.6MB
Iteration 800/1250  mem: 305/477.6MB
Iteration 900/1250  mem: 308.8/477.6MB
Iteration 1000/1250  mem: 310.1/477.6MB
Iteration 1100/1250  mem: 309.7/477.6MB
Iteration 1200/1250  mem: 313.8/477.6MB
done building BART in 1.594 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 291.4/477.6MB
Iteration 200/1250  mem: 310.8/477.6MB
Iteration 300/1250  mem: 321.3/477.6MB
Iteration 400/1250  mem: 334.2/477.6MB
Iteration 500/1250  mem: 191.4/477.6MB
Iteration 600/1250  mem: 200.2/477.6MB
Iteration 700/1250  mem: 201.4/477.6MB
Iteration 800/1250  mem: 202.7/477.6MB
Iteration 900/1250  mem: 202.2/477.6MB
Iteration 1000/1250  mem: 197/477.6MB
Iteration 1100/1250  mem: 194.6/477.6MB
Iteration 1200/1250  mem: 188.6/477.6MB
done building BART in 3.235 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 68 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 301.1/477.6MB
Iteration 200/1250  mem: 237.3/477.6MB
Iteration 300/1250  mem: 177.3/477.6MB
Iteration 400/1250  mem: 270.5/477.6MB
Iteration 500/1250  mem: 211.6/477.6MB
Iteration 600/1250  mem: 300.6/477.6MB
Iteration 700/1250  mem: 232.7/477.6MB
Iteration 800/1250  mem: 316.8/477.6MB
Iteration 900/1250  mem: 246.5/477.6MB
Iteration 1000/1250  mem: 330.4/477.6MB
Iteration 1100/1250  mem: 260/477.6MB
Iteration 1200/1250  mem: 342.4/477.6MB
done building BART in 2.406 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 31 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 279.5/477.6MB
Iteration 200/1250  mem: 238.3/477.6MB
Iteration 300/1250  mem: 352.5/477.6MB
Iteration 400/1250  mem: 309.5/477.6MB
Iteration 500/1250  mem: 267.9/477.6MB
Iteration 600/1250  mem: 383.7/477.6MB
Iteration 700/1250  mem: 340.9/477.6MB
Iteration 800/1250  mem: 299/477.6MB
Iteration 900/1250  mem: 258.1/477.6MB
Iteration 1000/1250  mem: 371.1/477.6MB
Iteration 1100/1250  mem: 329.7/477.6MB
Iteration 1200/1250  mem: 283.6/477.6MB
done building BART in 1.062 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 85 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 167.2/477.6MB
Iteration 200/1250  mem: 177/477.6MB
Iteration 300/1250  mem: 180.9/477.6MB
Iteration 400/1250  mem: 186.5/477.6MB
Iteration 500/1250  mem: 191.1/477.6MB
Iteration 600/1250  mem: 191.3/477.6MB
Iteration 700/1250  mem: 187.9/477.6MB
Iteration 800/1250  mem: 178.2/477.6MB
Iteration 900/1250  mem: 167/477.6MB
Iteration 1000/1250  mem: 153.8/477.6MB
Iteration 1100/1250  mem: 139.3/477.6MB
Iteration 1200/1250  mem: 121.8/477.6MB
done building BART in 3.11 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 10 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 163.9/477.6MB
Iteration 200/1250  mem: 203.3/477.6MB
Iteration 300/1250  mem: 239.5/477.6MB
Iteration 400/1250  mem: 278.9/477.6MB
Iteration 500/1250  mem: 155.9/477.6MB
Iteration 600/1250  mem: 194.2/477.6MB
Iteration 700/1250  mem: 232.6/477.6MB
Iteration 800/1250  mem: 271/477.6MB
Iteration 900/1250  mem: 155.3/477.6MB
Iteration 1000/1250  mem: 193.7/477.6MB
Iteration 1100/1250  mem: 232.2/477.6MB
Iteration 1200/1250  mem: 270.7/477.6MB
done building BART in 0.344 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 86 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 174.3/477.6MB
Iteration 200/1250  mem: 194.8/477.6MB
Iteration 300/1250  mem: 206.6/477.6MB
Iteration 400/1250  mem: 221.1/477.6MB
Iteration 500/1250  mem: 231.7/477.6MB
Iteration 600/1250  mem: 235.8/477.6MB
Iteration 700/1250  mem: 237.3/477.6MB
Iteration 800/1250  mem: 235.2/477.6MB
Iteration 900/1250  mem: 230/477.6MB
Iteration 1000/1250  mem: 374.4/477.6MB
Iteration 1100/1250  mem: 363.5/477.6MB
Iteration 1200/1250  mem: 348.6/477.6MB
done building BART in 3.032 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 84 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 224.7/477.6MB
Iteration 200/1250  mem: 232.9/477.6MB
Iteration 300/1250  mem: 238.1/477.6MB
Iteration 400/1250  mem: 243.1/477.6MB
Iteration 500/1250  mem: 247.2/477.6MB
Iteration 600/1250  mem: 247.3/477.6MB
Iteration 700/1250  mem: 243.6/477.6MB
Iteration 800/1250  mem: 241.6/477.6MB
Iteration 900/1250  mem: 231.2/477.6MB
Iteration 1000/1250  mem: 216.5/477.6MB
Iteration 1100/1250  mem: 200.6/477.6MB
Iteration 1200/1250  mem: 185.7/477.6MB
done building BART in 3.218 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 236.2/477.6MB
Iteration 200/1250  mem: 260.7/477.6MB
Iteration 300/1250  mem: 277/477.6MB
Iteration 400/1250  mem: 294.6/477.6MB
Iteration 500/1250  mem: 306.2/477.6MB
Iteration 600/1250  mem: 316.2/477.6MB
Iteration 700/1250  mem: 322.5/477.6MB
Iteration 800/1250  mem: 329/477.6MB
Iteration 900/1250  mem: 332.1/477.6MB
Iteration 1000/1250  mem: 333/477.6MB
Iteration 1100/1250  mem: 330.8/477.6MB
Iteration 1200/1250  mem: 327.4/477.6MB
done building BART in 3.234 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 63 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 268.4/477.6MB
Iteration 200/1250  mem: 345.3/477.6MB
Iteration 300/1250  mem: 266.4/477.6MB
Iteration 400/1250  mem: 345.1/477.6MB
Iteration 500/1250  mem: 268.8/477.6MB
Iteration 600/1250  mem: 343.1/477.6MB
Iteration 700/1250  mem: 417.4/477.6MB
Iteration 800/1250  mem: 332.9/477.6MB
Iteration 900/1250  mem: 229.8/477.6MB
Iteration 1000/1250  mem: 148.5/477.6MB
Iteration 1100/1250  mem: 219.4/477.6MB
Iteration 1200/1250  mem: 129.6/477.6MB
done building BART in 2.438 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 94 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 172.6/477.6MB
Iteration 200/1250  mem: 227.8/477.6MB
Iteration 300/1250  mem: 275.5/477.6MB
Iteration 400/1250  mem: 172.7/477.6MB
Iteration 500/1250  mem: 214.8/477.6MB
Iteration 600/1250  mem: 251/477.6MB
Iteration 700/1250  mem: 283.1/477.6MB
Iteration 800/1250  mem: 312.2/477.6MB
Iteration 900/1250  mem: 330.3/477.6MB
Iteration 1000/1250  mem: 348.9/477.6MB
Iteration 1100/1250  mem: 206.7/477.6MB
Iteration 1200/1250  mem: 217.7/477.6MB
done building BART in 3.36 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 80 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 237.4/477.6MB
Iteration 200/1250  mem: 376.4/477.6MB
Iteration 300/1250  mem: 363.7/477.6MB
Iteration 400/1250  mem: 349/477.6MB
Iteration 500/1250  mem: 329.5/477.6MB
Iteration 600/1250  mem: 309.5/477.6MB
Iteration 700/1250  mem: 289.3/477.6MB
Iteration 800/1250  mem: 261.2/477.6MB
Iteration 900/1250  mem: 395.4/477.6MB
Iteration 1000/1250  mem: 364.8/477.6MB
Iteration 1100/1250  mem: 332.8/477.6MB
Iteration 1200/1250  mem: 298/477.6MB
done building BART in 2.953 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 41 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 299.1/477.6MB
Iteration 200/1250  mem: 295.7/477.6MB
Iteration 300/1250  mem: 448.7/477.6MB
Iteration 400/1250  mem: 440.2/477.6MB
Iteration 500/1250  mem: 437.5/477.6MB
Iteration 600/1250  mem: 431.5/477.6MB
Iteration 700/1250  mem: 198.7/477.6MB
Iteration 800/1250  mem: 193.5/477.6MB
Iteration 900/1250  mem: 183.5/477.6MB
Iteration 1000/1250  mem: 173.7/477.6MB
Iteration 1100/1250  mem: 163/477.6MB
Iteration 1200/1250  mem: 156.1/477.6MB
done building BART in 1.594 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 98 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 164.7/477.6MB
Iteration 200/1250  mem: 227.5/477.6MB
Iteration 300/1250  mem: 129.9/477.6MB
Iteration 400/1250  mem: 175.1/477.6MB
Iteration 500/1250  mem: 216.4/477.6MB
Iteration 600/1250  mem: 259.5/477.6MB
Iteration 700/1250  mem: 286.2/477.6MB
Iteration 800/1250  mem: 316.9/477.6MB
Iteration 900/1250  mem: 185.9/477.6MB
Iteration 1000/1250  mem: 212.7/477.6MB
Iteration 1100/1250  mem: 239/477.6MB
Iteration 1200/1250  mem: 257.9/477.6MB
done building BART in 3.453 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 65 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 223.9/477.6MB
Iteration 200/1250  mem: 306.4/477.6MB
Iteration 300/1250  mem: 225.9/477.6MB
Iteration 400/1250  mem: 304/477.6MB
Iteration 500/1250  mem: 376.5/477.6MB
Iteration 600/1250  mem: 297.2/477.6MB
Iteration 700/1250  mem: 374/477.6MB
Iteration 800/1250  mem: 291.2/477.6MB
Iteration 900/1250  mem: 364.8/477.6MB
Iteration 1000/1250  mem: 285.1/477.6MB
Iteration 1100/1250  mem: 356.7/477.6MB
Iteration 1200/1250  mem: 268.1/477.6MB
done building BART in 2.281 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 30 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 406.5/477.6MB
Iteration 200/1250  mem: 415.6/477.6MB
Iteration 300/1250  mem: 423.1/477.6MB
Iteration 400/1250  mem: 268.4/477.6MB
Iteration 500/1250  mem: 275.3/477.6MB
Iteration 600/1250  mem: 285.3/477.6MB
Iteration 700/1250  mem: 292.3/477.6MB
Iteration 800/1250  mem: 302.4/477.6MB
Iteration 900/1250  mem: 312.8/477.6MB
Iteration 1000/1250  mem: 319.8/477.6MB
Iteration 1100/1250  mem: 327.3/477.6MB
Iteration 1200/1250  mem: 337.4/477.6MB
done building BART in 2.125 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 36 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 412.9/477.6MB
Iteration 200/1250  mem: 387.9/477.6MB
Iteration 300/1250  mem: 98.2/477.6MB
Iteration 400/1250  mem: 67.1/477.6MB
Iteration 500/1250  mem: 41.1/477.6MB
Iteration 600/1250  mem: 170.2/477.6MB
Iteration 700/1250  mem: 141.2/477.6MB
Iteration 800/1250  mem: 115/477.6MB
Iteration 900/1250  mem: 85.8/477.6MB
Iteration 1000/1250  mem: 60.1/477.6MB
Iteration 1100/1250  mem: 190.6/477.6MB
Iteration 1200/1250  mem: 157.6/477.6MB
done building BART in 1.282 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 41 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 74/477.6MB
Iteration 200/1250  mem: 62.1/477.6MB
Iteration 300/1250  mem: 211/477.6MB
Iteration 400/1250  mem: 199.3/477.6MB
Iteration 500/1250  mem: 193.7/477.6MB
Iteration 600/1250  mem: 183.6/477.6MB
Iteration 700/1250  mem: 172.7/477.6MB
Iteration 800/1250  mem: 164.5/477.6MB
Iteration 900/1250  mem: 156.1/477.6MB
Iteration 1000/1250  mem: 144/477.6MB
Iteration 1100/1250  mem: 132.1/477.6MB
Iteration 1200/1250  mem: 119.8/477.6MB
done building BART in 1.391 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 35 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 187.4/477.6MB
Iteration 200/1250  mem: 165.5/477.6MB
Iteration 300/1250  mem: 140.7/477.6MB
Iteration 400/1250  mem: 113.6/477.6MB
Iteration 500/1250  mem: 244.7/477.6MB
Iteration 600/1250  mem: 219.8/477.6MB
Iteration 700/1250  mem: 190.8/477.6MB
Iteration 800/1250  mem: 163.3/477.6MB
Iteration 900/1250  mem: 134.4/477.6MB
Iteration 1000/1250  mem: 265.1/477.6MB
Iteration 1100/1250  mem: 234.7/477.6MB
Iteration 1200/1250  mem: 205.5/477.6MB
done building BART in 1.313 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 84 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 152.9/477.6MB
Iteration 200/1250  mem: 160.2/477.6MB
Iteration 300/1250  mem: 167.7/477.6MB
Iteration 400/1250  mem: 167.4/477.6MB
Iteration 500/1250  mem: 163.6/477.6MB
Iteration 600/1250  mem: 314/477.6MB
Iteration 700/1250  mem: 304.1/477.6MB
Iteration 800/1250  mem: 287.2/477.6MB
Iteration 900/1250  mem: 272.8/477.6MB
Iteration 1000/1250  mem: 254.7/477.6MB
Iteration 1100/1250  mem: 234.2/477.6MB
Iteration 1200/1250  mem: 211.9/477.6MB
done building BART in 3.031 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 45 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 230.7/477.6MB
Iteration 200/1250  mem: 233.3/477.6MB
Iteration 300/1250  mem: 239/477.6MB
Iteration 400/1250  mem: 246.1/477.6MB
Iteration 500/1250  mem: 253/477.6MB
Iteration 600/1250  mem: 257/477.6MB
Iteration 700/1250  mem: 259.8/477.6MB
Iteration 800/1250  mem: 259.5/477.6MB
Iteration 900/1250  mem: 259.7/477.6MB
Iteration 1000/1250  mem: 128.3/477.6MB
Iteration 1100/1250  mem: 128.9/477.6MB
Iteration 1200/1250  mem: 128.7/477.6MB
done building BART in 1.703 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 260.7/477.6MB
Iteration 200/1250  mem: 149.6/477.6MB
Iteration 300/1250  mem: 180.1/477.6MB
Iteration 400/1250  mem: 207/477.6MB
Iteration 500/1250  mem: 230.2/477.6MB
Iteration 600/1250  mem: 247.8/477.6MB
Iteration 700/1250  mem: 262.4/477.6MB
Iteration 800/1250  mem: 273.4/477.6MB
Iteration 900/1250  mem: 280.6/477.6MB
Iteration 1000/1250  mem: 288.1/477.6MB
Iteration 1100/1250  mem: 299.2/477.6MB
Iteration 1200/1250  mem: 305.2/477.6MB
done building BART in 3.219 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 68 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 271.9/477.6MB
Iteration 200/1250  mem: 370.7/477.6MB
Iteration 300/1250  mem: 313.8/477.6MB
Iteration 400/1250  mem: 259.5/477.6MB
Iteration 500/1250  mem: 354.2/477.6MB
Iteration 600/1250  mem: 299/477.6MB
Iteration 700/1250  mem: 388.8/477.6MB
Iteration 800/1250  mem: 322.5/477.6MB
Iteration 900/1250  mem: 408.1/477.6MB
Iteration 1000/1250  mem: 339.9/477.6MB
Iteration 1100/1250  mem: 422.8/477.6MB
Iteration 1200/1250  mem: 350.1/477.6MB
done building BART in 2.468 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 31 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 291.3/477.6MB
Iteration 200/1250  mem: 408.8/477.6MB
Iteration 300/1250  mem: 365.5/477.6MB
Iteration 400/1250  mem: 323.6/477.6MB
Iteration 500/1250  mem: 435.6/477.6MB
Iteration 600/1250  mem: 396.2/477.6MB
Iteration 700/1250  mem: 355.9/477.6MB
Iteration 800/1250  mem: 95.8/477.6MB
Iteration 900/1250  mem: 208.5/477.6MB
Iteration 1000/1250  mem: 167.2/477.6MB
Iteration 1100/1250  mem: 124.5/477.6MB
Iteration 1200/1250  mem: 236.4/477.6MB
done building BART in 1.219 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 85 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 181.5/477.6MB
Iteration 200/1250  mem: 197.4/477.6MB
Iteration 300/1250  mem: 203/477.6MB
Iteration 400/1250  mem: 209.8/477.6MB
Iteration 500/1250  mem: 216.2/477.6MB
Iteration 600/1250  mem: 221.9/477.6MB
Iteration 700/1250  mem: 221.7/477.6MB
Iteration 800/1250  mem: 218.3/477.6MB
Iteration 900/1250  mem: 207.6/477.6MB
Iteration 1000/1250  mem: 196.9/477.6MB
Iteration 1100/1250  mem: 182.2/477.6MB
Iteration 1200/1250  mem: 329.1/477.6MB
done building BART in 3.047 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 10 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 212.8/477.6MB
Iteration 200/1250  mem: 250.1/477.6MB
Iteration 300/1250  mem: 287.4/477.6MB
Iteration 400/1250  mem: 324.7/477.6MB
Iteration 500/1250  mem: 204.6/477.6MB
Iteration 600/1250  mem: 243/477.6MB
Iteration 700/1250  mem: 281.4/477.6MB
Iteration 800/1250  mem: 322.1/477.6MB
Iteration 900/1250  mem: 200.9/477.6MB
Iteration 1000/1250  mem: 240/477.6MB
Iteration 1100/1250  mem: 279.1/477.6MB
Iteration 1200/1250  mem: 318.3/477.6MB
done building BART in 0.344 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 84 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 253.8/477.6MB
Iteration 200/1250  mem: 275.5/477.6MB
Iteration 300/1250  mem: 293.8/477.6MB
Iteration 400/1250  mem: 295.1/477.6MB
Iteration 500/1250  mem: 289.6/477.6MB
Iteration 600/1250  mem: 267.5/477.6MB
Iteration 700/1250  mem: 396.5/477.6MB
Iteration 800/1250  mem: 361.5/477.6MB
Iteration 900/1250  mem: 321.8/477.6MB
Iteration 1000/1250  mem: 273.6/477.6MB
Iteration 1100/1250  mem: 390.7/477.6MB
Iteration 1200/1250  mem: 343.1/477.6MB
done building BART in 4.297 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
Bayesian Additive Regression Trees 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  num_trees  k            alpha      beta        nu          RMSE      
  10         4.736533924  0.9269769  2.59448446  0.86038832  0.05093929
  30         1.084491273  0.9192564  0.08710547  0.06660514         NaN
  31         2.712742025  0.9657557  0.35244772  3.49821404  0.03249946
  35         0.007634845  0.9144984  0.80262361  0.71562033  0.04774396
  36         0.684979067  0.9031039  0.09716453  2.52335507  0.04027034
  41         2.198200171  0.9941425  2.99856779  0.50976923  0.03232928
  41         3.815269765  0.9744364  1.50406339  2.16159705  0.03185292
  45         2.371253351  0.9346117  2.51116548  1.84756948  0.03210273
  63         3.214304779  0.9554629  1.54083808  1.09512063  0.02970081
  65         3.986646066  0.9430268  1.95928332  2.58004428  0.02833795
  68         1.911873380  0.9366643  0.90009304  1.90567688  0.03243678
  80         1.825990198  0.9745044  0.11764532  0.37415796  0.03083305
  84         1.176824056  0.9495845  1.75279302  0.18049822  0.03123196
  84         1.941716562  0.9883017  3.95802206  3.34006886  0.02676381
  85         1.486893723  0.9703335  1.85690176  3.25940332  0.02942075
  86         4.990029881  0.9873095  1.45332716  0.67601611  0.02785799
  90         1.100625346  0.9354396  0.65560856  2.62304250  0.03988696
  90         2.207684906  0.9812115  1.04151407  1.68556992  0.03001777
  94         1.433318449  0.9970572  2.19998108  3.02407786  0.02870909
  98         0.910766952  0.9980278  0.69931606  2.02554266  0.03574845
  Rsquared   MAE         Selected
  0.9937998  0.03959337          
        NaN         NaN          
  0.9974985  0.02587025          
  0.9946461  0.03704058          
  0.9961093  0.03155015          
  0.9974624  0.02591770          
  0.9976004  0.02509291          
  0.9975192  0.02490057          
  0.9978844  0.02278651          
  0.9980750  0.02158774          
  0.9974968  0.02489710          
  0.9977011  0.02407937          
  0.9976416  0.02271223          
  0.9982958  0.02096627  *       
  0.9979106  0.02265333          
  0.9981412  0.02186874          
  0.9961962  0.02728065          
  0.9978161  0.02297072          
  0.9979902  0.02171769          
  0.9969563  0.02524414          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were num_trees = 84, k = 1.941717, alpha
 = 0.9883017, beta = 3.958022 and nu = 3.340069.
[1] "Mon Mar 12 00:25:00 2018"
.....
Loading required package: Matrix
Loading required package: lme4

Attaching package: 'lme4'

The following object is masked from 'package:nlme':

    lmList


arm (Version 1.9-3, built: 2016-11-21)

Working directory is C:/Users/John/Documents/GitHub/GeneratedRegMLBenchmark/ACEREBOUT


Attaching package: 'arm'

The following object is masked from 'package:car':

    logit

The following object is masked from 'package:plotrix':

    rescale

The following objects are masked from 'package:pls':

    coefplot, corrplot

Bayesian Generalized Linear Model 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results:

  RMSE         Rsquared   MAE        
  0.007977266  0.9998473  0.006072955

[1] "Mon Mar 12 00:25:15 2018"
t=100, m=8
t=200, m=8
t=300, m=8
t=400, m=8
t=500, m=8
t=600, m=8
t=700, m=8
t=800, m=8
t=900, m=8
t=100, m=8
t=200, m=8
t=300, m=8
t=400, m=8
t=500, m=9
t=600, m=8
t=700, m=8
t=800, m=8
t=900, m=8
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=8
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=8
t=200, m=9
t=300, m=8
t=400, m=8
t=500, m=8
t=600, m=8
t=700, m=9
t=800, m=9
t=900, m=8
The Bayesian lasso 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  sparsity     RMSE         Rsquared   MAE          Selected
  0.009869629  0.008156519  0.9998464  0.006403035          
  0.221141259  0.008143504  0.9998451  0.006288890          
  0.233456740  0.008143504  0.9998451  0.006288890          
  0.278890499  0.008143504  0.9998451  0.006288890          
  0.295011987  0.008143504  0.9998451  0.006288890          
  0.342562624  0.008143504  0.9998451  0.006288890          
  0.343677541  0.008143504  0.9998451  0.006288890          
  0.394752379  0.008143504  0.9998451  0.006288890          
  0.593184605  0.008143504  0.9998451  0.006288890          
  0.604462799  0.008143504  0.9998451  0.006288890          
  0.644371057  0.008143504  0.9998451  0.006288890          
  0.771025799  0.008143504  0.9998451  0.006288890          
  0.822559009  0.008143504  0.9998451  0.006288890          
  0.824171485  0.008143504  0.9998451  0.006288890          
  0.826242191  0.008143504  0.9998451  0.006288890          
  0.843276577  0.008143504  0.9998451  0.006288890          
  0.883317267  0.008143504  0.9998451  0.006288890          
  0.887864152  0.008143504  0.9998451  0.006288890          
  0.923531971  0.008143504  0.9998451  0.006288890  *       
  0.969344636  0.027385798  0.9997318  0.024927996          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was sparsity = 0.923532.
[1] "Mon Mar 12 00:25:29 2018"
t=100, m=8
t=200, m=8
t=300, m=8
t=400, m=8
t=500, m=8
t=600, m=8
t=700, m=8
t=800, m=9
t=900, m=8
t=100, m=8
t=200, m=8
t=300, m=8
t=400, m=8
t=500, m=8
t=600, m=8
t=700, m=8
t=800, m=8
t=900, m=8
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=8
t=500, m=8
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=8
t=200, m=9
t=300, m=9
t=400, m=8
t=500, m=8
t=600, m=8
t=700, m=8
t=800, m=8
t=900, m=8
Bayesian Ridge Regression (Model Averaged) 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results:

  RMSE         Rsquared   MAE        
  0.008164754  0.9998461  0.006409242

[1] "Mon Mar 12 00:25:42 2018"
t=100, m=8
t=200, m=8
t=300, m=8
t=400, m=8
t=500, m=8
t=600, m=8
t=700, m=8
t=800, m=8
t=900, m=8
t=100, m=8
t=200, m=8
t=300, m=8
t=400, m=8
t=500, m=8
t=600, m=8
t=700, m=8
t=800, m=8
t=900, m=8
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=8
t=200, m=8
t=300, m=8
t=400, m=8
t=500, m=8
t=600, m=8
t=700, m=9
t=800, m=8
t=900, m=8
Bayesian Ridge Regression 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results:

  RMSE         Rsquared   MAE        
  0.008160163  0.9998463  0.006409334

[1] "Mon Mar 12 00:25:55 2018"
Loading required package: gbm
Loading required package: splines
Loaded gbm 2.1.3
Boosted Linear Model 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  nu           mstop  RMSE       Rsquared   MAE        Selected
  0.001914654  140    0.6178393  0.9267460  0.5071147          
  0.083060492  148    0.5167436  0.8208691  0.4183156          
  0.110109881  484    0.3767014  0.8397568  0.3046041          
  0.130922054  111    0.5029767  0.8038654  0.4074212          
  0.132854916  444    0.3620335  0.8519027  0.2917538          
  0.141983522  412    0.3624316  0.8515328  0.2921075          
  0.172711550  461    0.3179976  0.8765407  0.2528490          
  0.179129868  413    0.3277260  0.8714015  0.2615537          
  0.219753626  385    0.3058972  0.8813796  0.2419024          
  0.230042431  322    0.3235463  0.8757189  0.2577813          
  0.233617644  411    0.2873461  0.8881570  0.2248010          
  0.264344380  172    0.3860186  0.8282913  0.3127981          
  0.265480652  441    0.2591750  0.8988762  0.1978516          
  0.285076152  197    0.3573311  0.8563008  0.2875704          
  0.325986495  117    0.4027879  0.8120197  0.3272691          
  0.386073713  296    0.2533584  0.8998422  0.1922034          
  0.458069318  171    0.2964889  0.8871960  0.2332907          
  0.478600199  302    0.2215139  0.9089628  0.1592064          
  0.568436764    5    0.5703028  0.8923806  0.4624012          
  0.598805580  421    0.1886920  0.9171032  0.1237494  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mstop = 421 and nu = 0.5988056.
[1] "Mon Mar 12 00:27:07 2018"
Loading required package: grid
Loading required package: mvtnorm
Loading required package: modeltools
Loading required package: stats4

Attaching package: 'modeltools'

The following object is masked from 'package:lme4':

    refit

The following object is masked from 'package:rJava':

    clone

The following object is masked from 'package:plyr':

    empty

The following object is masked from 'package:kernlab':

    prior

Loading required package: strucchange
Loading required package: zoo

Attaching package: 'zoo'

The following objects are masked from 'package:base':

    as.Date, as.Date.numeric

Loading required package: sandwich
Conditional Inference Random Forest 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE         Selected
  1     0.23781486  0.9314790  0.18158049          
  2     0.08413296  0.9888098  0.06446725          
  3     0.05719839  0.9931549  0.04228519          
  4     0.05175782  0.9940163  0.03733547  *       
  6     0.05395434  0.9931923  0.03954631          
  7     0.05591935  0.9926309  0.04098765          
  8     0.05798148  0.9920212  0.04296466          
  9     0.05947546  0.9915400  0.04422559          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 4.
[1] "Mon Mar 12 00:28:28 2018"
Conditional Inference Tree 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  mincriterion  RMSE        Rsquared   MAE         Selected
  0.009869629   0.08163682  0.9838953  0.06503848          
  0.221141259   0.08163682  0.9838953  0.06503848          
  0.233456740   0.08163682  0.9838953  0.06503848          
  0.278890499   0.08163682  0.9838953  0.06503848          
  0.295011987   0.08163682  0.9838953  0.06503848          
  0.342562624   0.08163682  0.9838953  0.06503848          
  0.343677541   0.08163682  0.9838953  0.06503848          
  0.394752379   0.08163682  0.9838953  0.06503848          
  0.593184605   0.08163682  0.9838953  0.06503848          
  0.604462799   0.08163682  0.9838953  0.06503848          
  0.644371057   0.08163682  0.9838953  0.06503848  *       
  0.771025799   0.08196015  0.9837719  0.06529283          
  0.822559009   0.08201784  0.9837475  0.06534728          
  0.824171485   0.08201784  0.9837475  0.06534728          
  0.826242191   0.08201784  0.9837475  0.06534728          
  0.843276577   0.08201784  0.9837475  0.06534728          
  0.883317267   0.08236515  0.9835962  0.06575317          
  0.887864152   0.08236515  0.9835962  0.06575317          
  0.923531971   0.08240317  0.9835794  0.06561403          
  0.969344636   0.08352044  0.9831303  0.06627904          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mincriterion = 0.6443711.
[1] "Mon Mar 12 00:28:41 2018"
Conditional Inference Tree 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  maxdepth  mincriterion  RMSE        Rsquared   MAE         Selected
   1        0.947306785   0.36785282  0.6705831  0.30614340          
   4        0.216898255   0.11125028  0.9704735  0.08965215          
   4        0.542548405   0.11125028  0.9704735  0.08965215          
   5        0.001526969   0.09155658  0.9797440  0.07268411          
   5        0.136995813   0.09155658  0.9797440  0.07268411          
   6        0.439640034   0.08357160  0.9831200  0.06665079          
   6        0.474250670   0.08357160  0.9831200  0.06665079          
   6        0.763053953   0.08357160  0.9831200  0.06665079          
   9        0.642860956   0.08163682  0.9838953  0.06503848  *       
  10        0.382374676   0.08163682  0.9838953  0.06503848          
  10        0.797329213   0.08196015  0.9837719  0.06529283          
  12        0.365198040   0.08163682  0.9838953  0.06503848          
  13        0.235364811   0.08163682  0.9838953  0.06503848          
  13        0.297378745   0.08163682  0.9838953  0.06503848          
  13        0.388343312   0.08163682  0.9838953  0.06503848          
  13        0.998005976   0.09388288  0.9787385  0.07426467          
  14        0.220125069   0.08163682  0.9838953  0.06503848          
  14        0.286663690   0.08163682  0.9838953  0.06503848          
  14        0.441536981   0.08163682  0.9838953  0.06503848          
  15        0.182153390   0.08163682  0.9838953  0.06503848          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were maxdepth = 9 and mincriterion
 = 0.642861.
[1] "Mon Mar 12 00:28:49 2018"
Cubist 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  committees  neighbors  RMSE         Rsquared   MAE          Selected
    1         2          0.009627142  0.9997579  0.006216945          
   14         2          0.004998869  0.9999409  0.003646064          
   19         9          0.004658898  0.9999490  0.003466324          
   22         2          0.004851214  0.9999445  0.003596824          
   23         8          0.004721247  0.9999478  0.003487728          
   24         8          0.004743016  0.9999473  0.003468857          
   29         9          0.004745553  0.9999472  0.003468482          
   30         8          0.004707073  0.9999481  0.003441327          
   37         7          0.004646727  0.9999493  0.003418014          
   39         6          0.004630726  0.9999496  0.003420089  *       
   39         8          0.004641775  0.9999494  0.003435823          
   44         3          0.004686022  0.9999483  0.003444107          
   45         8          0.004701791  0.9999482  0.003437150          
   48         3          0.004683707  0.9999485  0.003444662          
   55         2          0.004800339  0.9999457  0.003546673          
   65         5          0.004690155  0.9999484  0.003396169          
   77         3          0.004647390  0.9999493  0.003399606          
   80         6          0.004669044  0.9999490  0.003373303          
   95         0          0.005065587  0.9999399  0.003795135          
  100         8          0.004649514  0.9999493  0.003363070          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were committees = 39 and neighbors = 6.
[1] "Mon Mar 12 00:31:01 2018"
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
Stacked AutoEncoder Deep Neural Network 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  layer1  layer2  layer3  hidden_dropout  visible_dropout  RMSE     
   2      19       7      0.45403478      0.12045436       0.6570703
   6       6       5      0.01524346      0.00932472       0.7192004
   6      12      14      0.06167835      0.48974996       0.6725904
   7       2       4      0.14045913      0.10018685       0.6458645
   7       4       2      0.01700379      0.35326971       0.7638615
   8      10      19      0.52474936      0.07136769       2.2551264
   8      16      16      0.26321109      0.30262359       0.6845663
   9      11       8      0.43945396      0.25865973       0.6650082
  13      14      12      0.26964666      0.15331689       0.6560976
  13      17      10      0.34287458      0.36120620       0.6740608
  14       9       8      0.15751628      0.26679476       0.6649127
  16       8      16      0.02058793      0.05238211       0.6783513
  17       6      11      0.30673878      0.02526975       0.6495243
  17       7      15      0.32495781      0.45631647       0.7694203
  17       9      18      0.69265386      0.46760964       0.7398132
  18       6       8      0.11473150      0.36722595       0.6537772
  18      10      17      0.18226496      0.23597979       0.7053672
  18      20      18      0.25433225      0.09464226       0.6594123
  19       7      20      0.38499669      0.42337090       1.8371629
  20       5      20      0.12238031      0.28357597       0.7711716
  Rsquared    MAE        Selected
  0.15362937  0.5366682          
  0.09002051  0.5859693          
  0.10254529  0.5480160          
  0.06729835  0.5303665  *       
  0.03195780  0.6232458          
  0.11733593  2.1453986          
  0.04295737  0.5679036          
  0.10898277  0.5446646          
  0.08171385  0.5431923          
  0.06623072  0.5479012          
  0.13768466  0.5432930          
  0.06438881  0.5508725          
  0.09628621  0.5368410          
  0.04179423  0.6314409          
  0.33988416  0.6069845          
  0.05401364  0.5372090          
  0.03779653  0.5743122          
  0.15314167  0.5412656          
  0.07522722  1.7063628          
  0.06015719  0.6310495          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were layer1 = 7, layer2 = 2, layer3 =
 4, hidden_dropout = 0.1404591 and visible_dropout = 0.1001868.
[1] "Mon Mar 12 00:31:13 2018"
Multivariate Adaptive Regression Spline 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  degree  nprune  RMSE        Rsquared   MAE         Selected
  1       3       0.09506182  0.9781325  0.06517580          
  1       4       0.05905039  0.9915462  0.04651375          
  1       5       0.03974992  0.9961868  0.02784154          
  1       6       0.02462345  0.9985523  0.01902054          
  1       7       0.02227390  0.9988084  0.01717182  *       
  2       2       0.12945035  0.9593234  0.09978929          
  2       3       0.09506182  0.9781325  0.06517580          
  2       4       0.05905039  0.9915462  0.04651375          
  2       5       0.03974992  0.9961868  0.02784154          
  2       7       0.02227390  0.9988084  0.01717182          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nprune = 7 and degree = 1.
[1] "Mon Mar 12 00:31:22 2018"
Extreme Learning Machine 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  nhid  actfun   RMSE         Rsquared   MAE         Selected
   1    tansig   0.552290178  0.3267874  0.45660360          
   5    purelin  0.072624887  0.9880598  0.05524034          
   5    sin      0.478881534  0.6370558  0.37369625          
   6    sin      0.696991054  0.6434435  0.58464220          
   7    radbas   1.026734814  0.2396152  0.83548366          
   7    tansig   0.330707235  0.7023292  0.26107884          
   8    radbas   0.558897730  0.4661909  0.43041299          
  12    purelin  0.007977303  0.9998473  0.00607296  *       
  12    tansig   0.176071168  0.9242800  0.13537438          
  13    radbas   0.573704538  0.4363991  0.45623482          
  15    radbas   0.584158005  0.4252050  0.45481638          
  16    radbas   0.527923888  0.4573391  0.40418678          
  16    sin      0.244060504  0.8685736  0.16796749          
  17    radbas   0.663825465  0.5541491  0.54411560          
  17    sin      0.202176337  0.9041571  0.14729752          
  17    tansig   0.143152537  0.9506134  0.10920147          
  18    radbas   0.333924149  0.7422795  0.25458225          
  19    sin      0.161742987  0.9349257  0.11566239          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nhid = 12 and actfun = purelin.
[1] "Mon Mar 12 00:31:29 2018"
Elasticnet 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  lambda        fraction     RMSE         Rsquared   MAE          Selected
  1.146087e-05  0.947306785  0.007992075  0.9998466  0.006099335  *       
  2.122499e-04  0.216898255  0.432386893  0.9714814  0.354616194          
  2.516173e-04  0.542548405  0.143132588  0.9945752  0.117336385          
  4.713494e-04  0.001526969  0.639256215  0.9714814  0.527467588          
  5.889412e-04  0.136995813  0.520614602  0.9714814  0.428161335          
  1.135993e-03  0.763053953  0.026113678  0.9986719  0.020345177          
  1.153627e-03  0.439640034  0.269711848  0.9808250  0.220460853          
  2.336223e-03  0.474250670  0.260279329  0.9826034  0.212798213          
  3.623328e-02  0.642860956  0.149871006  0.9960006  0.122226598          
  4.234253e-02  0.797329213  0.044832276  0.9971106  0.034633075          
  7.348968e-02  0.382374676  0.335730407  0.9799535  0.274556818          
  4.228193e-01  0.365198040  0.295493373  0.9857071  0.241531000          
  8.616960e-01  0.388343312  0.209350542  0.9893934  0.170407013          
  8.811075e-01  0.235364811  0.374294438  0.9770052  0.306345169          
  9.066781e-01  0.297378745  0.303174743  0.9844660  0.247822456          
  1.147254e+00  0.998005976  0.481778806  0.9776161  0.392494313          
  1.994819e+00  0.441536981  0.058498877  0.9918723  0.042711342          
  2.124149e+00  0.220125069  0.308248995  0.9836832  0.251980221          
  3.476897e+00  0.286663690  0.122045436  0.9909610  0.098038186          
  6.547384e+00  0.182153390  0.208276095  0.9887378  0.169515742          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were fraction = 0.9473068 and lambda
 = 1.146087e-05.
[1] "Mon Mar 12 00:31:36 2018"
Loading required package: partykit

Attaching package: 'partykit'

The following objects are masked from 'package:party':

    cforest, ctree, ctree_control, edge_simple, mob, mob_control,
    node_barplot, node_bivplot, node_boxplot, node_inner, node_surv,
    node_terminal

Tree Models from Genetic Algorithms 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  alpha     RMSE       Rsquared   MAE         Selected
  1.039479  0.1048992  0.9735789  0.08374529  *       
  1.884565  0.1250913  0.9620062  0.10107521          
  1.933827  0.1291029  0.9595777  0.10320900          
  2.115562  0.1426035  0.9507558  0.11472597          
  2.180048  0.1389872  0.9533167  0.11101882          
  2.370250  0.1437919  0.9501086  0.11548897          
  2.374710  0.1386402  0.9531965  0.11088903          
  2.579010  0.1433136  0.9502689  0.11558128          
  3.372738  0.1615917  0.9369898  0.13074275          
  3.417851  0.1644034  0.9346190  0.13190700          
  3.577484  0.1622936  0.9363719  0.13149577          
  4.084103  0.1655513  0.9336456  0.13367868          
  4.290236  0.1937571  0.9088148  0.15570555          
  4.296686  0.1854606  0.9165135  0.15019875          
  4.304969  0.1835887  0.9178116  0.14744128          
  4.373106  0.1826481  0.9183466  0.14640916          
  4.533269  0.1937571  0.9088148  0.15570555          
  4.551457  0.1937571  0.9088148  0.15570555          
  4.694128  0.1937571  0.9088148  0.15570555          
  4.877379  0.1937571  0.9088148  0.15570555          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was alpha = 1.039479.
[1] "Mon Mar 12 01:01:07 2018"
Random Forest by Randomization 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  mtry  numRandomCuts  RMSE        Rsquared   MAE         Selected
  1     24             0.05233724  0.9944565  0.03273937          
  2      6             0.03238540  0.9977629  0.02219105          
  3      1             0.02911044  0.9982263  0.02022042          
  3      4             0.02812070  0.9982636  0.01984778  *       
  3     14             0.03200984  0.9976864  0.02193891          
  4     11             0.02860833  0.9981240  0.02055010          
  4     12             0.03130879  0.9977625  0.02207789          
  4     20             0.03043400  0.9978626  0.02195587          
  6     10             0.03058461  0.9978190  0.02220797          
  6     17             0.03070845  0.9977993  0.02237764          
  6     20             0.03135312  0.9977084  0.02301026          
  7     10             0.02989740  0.9979080  0.02166733          
  8      6             0.02875688  0.9980628  0.02075727          
  8      8             0.03103469  0.9977377  0.02273143          
  8     10             0.03083320  0.9977755  0.02251035          
  8     12             0.03289607  0.9974523  0.02452779          
  8     25             0.03434937  0.9972461  0.02515155          
  9      5             0.02836798  0.9981142  0.02066744          
  9      8             0.03058688  0.9978079  0.02219762          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mtry = 3 and numRandomCuts = 4.
[1] "Mon Mar 12 03:56:23 2018"
Ridge Regression with Variable Selection 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  lambda        k  RMSE         Rsquared   MAE          Selected
  1.146087e-05  9  0.008320376  0.9998335  0.006324244  *       
  2.122499e-04  2  0.019278687  0.9991054  0.014951306          
  2.516173e-04  5  0.011681213  0.9996679  0.008988553          
  4.713494e-04  1  0.108441077  0.9714814  0.088088985          
  5.889412e-04  2  0.040500379  0.9954851  0.031658225          
  1.135993e-03  7  0.010259452  0.9997470  0.007800111          
  1.153627e-03  4  0.012343179  0.9996321  0.009598921          
  2.336223e-03  5  0.012190875  0.9996429  0.009534506          
  3.623328e-02  6  0.019957451  0.9991987  0.015613642          
  4.234253e-02  8  0.020818102  0.9991706  0.016350611          
  7.348968e-02  4  0.025827990  0.9990195  0.020697477          
  4.228193e-01  4  0.094460082  0.9971135  0.077576602          
  8.616960e-01  4          NaN        NaN          NaN          
  8.811075e-01  3          NaN        NaN          NaN          
  9.066781e-01  3          NaN        NaN          NaN          
  1.147254e+00  9          NaN        NaN          NaN          
  1.994819e+00  4          NaN        NaN          NaN          
  2.124149e+00  2          NaN        NaN          NaN          
  3.476897e+00  3          NaN        NaN          NaN          
  6.547384e+00  2          NaN        NaN          NaN          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were k = 9 and lambda = 1.146087e-05.
[1] "Mon Mar 12 03:56:30 2018"
Generalized Additive Model using Splines 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  select  method  RMSE         Rsquared   MAE          Selected
  FALSE   GCV.Cp  0.003284111  0.9999744  0.002646920          
  FALSE   ML      0.003281405  0.9999744  0.002645381          
   TRUE   GCV.Cp  0.003324030  0.9999738  0.002665791          
   TRUE   ML      0.003267788  0.9999746  0.002634710  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were select = TRUE and method = ML.
[1] "Mon Mar 12 03:58:30 2018"
Loading required package: stabs

Attaching package: 'stabs'

The following object is masked from 'package:modeltools':

    parameters

This is mboost 2.8-1. See 'package?mboost' and 'news(package  = "mboost")'
for a complete list of changes.


Attaching package: 'mboost'

The following object is masked from 'package:party':

    varimp

The following object is masked from 'package:ipred':

    cv

The following object is masked from 'package:MLmetrics':

    AUC

The following object is masked from 'package:ggplot2':

    %+%

Boosted Generalized Additive Model 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  mstop  prune  RMSE        Rsquared   MAE          Selected
   10    no     0.24338070  0.9817703  0.199237206          
  222    yes    0.01503405  0.9994552  0.011186740          
  234    no     0.01495361  0.9994607  0.011128770          
  279    yes    0.01470348  0.9994785  0.010950225          
  296    yes    0.01462414  0.9994842  0.010887554          
  343    no     0.01441905  0.9994984  0.010730358          
  344    yes    0.01441600  0.9994988  0.010729396          
  395    yes    0.01422382  0.9995120  0.010580031          
  594    no     0.01363382  0.9995516  0.010152538          
  605    no     0.01360490  0.9995535  0.010132861          
  645    yes    0.01350804  0.9995599  0.010065168          
  772    yes    0.01321819  0.9995786  0.009863289          
  823    yes    0.01311152  0.9995855  0.009787493          
  825    yes    0.01310917  0.9995857  0.009785029          
  827    yes    0.01310413  0.9995861  0.009782370          
  844    no     0.01307024  0.9995882  0.009759939          
  884    yes    0.01298811  0.9995934  0.009702723          
  888    yes    0.01297815  0.9995938  0.009695074          
  924    yes    0.01291061  0.9995982  0.009649236          
  970    yes    0.01282317  0.9996038  0.009588281  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mstop = 970 and prune = yes.
[1] "Mon Mar 12 04:21:54 2018"
Loaded gam 1.14-4


Attaching package: 'gam'

The following objects are masked from 'package:mgcv':

    gam, gam.control, gam.fit, plot.gam, predict.gam, s, summary.gam

Generalized Additive Model using Splines 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  df          RMSE         Rsquared   MAE          Selected
  0.04934815  0.007977165  0.9998473  0.006072855          
  1.10570630  0.007075481  0.9998799  0.005388298          
  1.16728370  0.006685296  0.9998928  0.005100830          
  1.39445250  0.005766833  0.9999203  0.004414106          
  1.47505993  0.005565371  0.9999258  0.004262095          
  1.71281312  0.005167619  0.9999360  0.003988996          
  1.71838771  0.005160654  0.9999362  0.003984393          
  1.97376190  0.004905701  0.9999423  0.003812634          
  2.96592303  0.004274487  0.9999561  0.003363310          
  3.02231400  0.004243942  0.9999567  0.003342118          
  3.22185529  0.004140094  0.9999588  0.003269879          
  3.85512900  0.003863018  0.9999641  0.003085221          
  4.11279505  0.003773830  0.9999658  0.003024268          
  4.12085743  0.003771244  0.9999658  0.003022452          
  4.13121096  0.003767915  0.9999659  0.003020127          
  4.21638289  0.003741487  0.9999664  0.003001598          
  4.41658633  0.003684277  0.9999674  0.002961060          
  4.43932076  0.003678232  0.9999675  0.002956721          
  4.61765986  0.003633254  0.9999683  0.002924052          
  4.84672318  0.003582232  0.9999692  0.002887827  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was df = 4.846723.
[1] "Mon Mar 12 04:22:29 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: parameter=none Error in `kernlab::vanilladot`() : 
  could not find function "kernlab::vanilladot"
 
2: model fit failed for Fold2: parameter=none Error in `kernlab::vanilladot`() : 
  could not find function "kernlab::vanilladot"
 
3: model fit failed for Fold3: parameter=none Error in `kernlab::vanilladot`() : 
  could not find function "kernlab::vanilladot"
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 04:22:37 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "gaussprLinear"                  
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 04:29:02 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "gaussprPoly"                    
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 04:29:10 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "gaussprRadial"                  
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3949             nan     0.0196    0.0122
     2        0.3822             nan     0.0196    0.0120
     3        0.3704             nan     0.0196    0.0109
     4        0.3590             nan     0.0196    0.0113
     5        0.3480             nan     0.0196    0.0112
     6        0.3368             nan     0.0196    0.0104
     7        0.3264             nan     0.0196    0.0105
     8        0.3165             nan     0.0196    0.0099
     9        0.3066             nan     0.0196    0.0105
    10        0.2972             nan     0.0196    0.0092
    20        0.2202             nan     0.0196    0.0067
    40        0.1231             nan     0.0196    0.0031
    60        0.0722             nan     0.0196    0.0020
    80        0.0432             nan     0.0196    0.0010
   100        0.0269             nan     0.0196    0.0007
   120        0.0172             nan     0.0196    0.0003
   140        0.0112             nan     0.0196    0.0002
   160        0.0076             nan     0.0196    0.0001
   180        0.0052             nan     0.0196    0.0001
   200        0.0037             nan     0.0196    0.0000
   220        0.0028             nan     0.0196    0.0000
   240        0.0022             nan     0.0196    0.0000
   260        0.0018             nan     0.0196    0.0000
   280        0.0015             nan     0.0196    0.0000
   300        0.0013             nan     0.0196    0.0000
   320        0.0011             nan     0.0196    0.0000
   340        0.0010             nan     0.0196    0.0000
   360        0.0009             nan     0.0196    0.0000
   380        0.0008             nan     0.0196    0.0000
   400        0.0008             nan     0.0196    0.0000
   420        0.0007             nan     0.0196    0.0000
   440        0.0007             nan     0.0196    0.0000
   460        0.0006             nan     0.0196    0.0000
   480        0.0006             nan     0.0196    0.0000
   500        0.0006             nan     0.0196   -0.0000
   520        0.0006             nan     0.0196    0.0000
   540        0.0005             nan     0.0196    0.0000
   560        0.0005             nan     0.0196    0.0000
   580        0.0005             nan     0.0196    0.0000
   600        0.0005             nan     0.0196    0.0000
   620        0.0005             nan     0.0196   -0.0000
   640        0.0004             nan     0.0196    0.0000
   660        0.0004             nan     0.0196    0.0000
   680        0.0004             nan     0.0196    0.0000
   700        0.0004             nan     0.0196    0.0000
   720        0.0004             nan     0.0196   -0.0000
   740        0.0004             nan     0.0196    0.0000
   760        0.0004             nan     0.0196    0.0000
   780        0.0004             nan     0.0196    0.0000
   800        0.0003             nan     0.0196    0.0000
   820        0.0003             nan     0.0196   -0.0000
   840        0.0003             nan     0.0196   -0.0000
   860        0.0003             nan     0.0196   -0.0000
   880        0.0003             nan     0.0196   -0.0000
   900        0.0003             nan     0.0196   -0.0000
   920        0.0003             nan     0.0196   -0.0000
   940        0.0003             nan     0.0196   -0.0000
   960        0.0003             nan     0.0196   -0.0000
   980        0.0003             nan     0.0196   -0.0000
  1000        0.0003             nan     0.0196    0.0000
  1020        0.0003             nan     0.0196   -0.0000
  1040        0.0003             nan     0.0196   -0.0000
  1060        0.0003             nan     0.0196   -0.0000
  1080        0.0003             nan     0.0196   -0.0000
  1100        0.0002             nan     0.0196    0.0000
  1120        0.0002             nan     0.0196   -0.0000
  1140        0.0002             nan     0.0196   -0.0000
  1160        0.0002             nan     0.0196   -0.0000
  1180        0.0002             nan     0.0196   -0.0000
  1200        0.0002             nan     0.0196   -0.0000
  1220        0.0002             nan     0.0196   -0.0000
  1240        0.0002             nan     0.0196    0.0000
  1260        0.0002             nan     0.0196    0.0000
  1280        0.0002             nan     0.0196   -0.0000
  1300        0.0002             nan     0.0196   -0.0000
  1320        0.0002             nan     0.0196   -0.0000
  1340        0.0002             nan     0.0196   -0.0000
  1360        0.0002             nan     0.0196   -0.0000
  1380        0.0002             nan     0.0196    0.0000
  1400        0.0002             nan     0.0196   -0.0000
  1420        0.0002             nan     0.0196   -0.0000
  1440        0.0002             nan     0.0196   -0.0000
  1460        0.0002             nan     0.0196   -0.0000
  1475        0.0002             nan     0.0196    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3623             nan     0.0878    0.0457
     2        0.3217             nan     0.0878    0.0391
     3        0.2889             nan     0.0878    0.0331
     4        0.2608             nan     0.0878    0.0268
     5        0.2347             nan     0.0878    0.0260
     6        0.2135             nan     0.0878    0.0216
     7        0.1952             nan     0.0878    0.0189
     8        0.1779             nan     0.0878    0.0164
     9        0.1621             nan     0.0878    0.0151
    10        0.1478             nan     0.0878    0.0148
    20        0.0656             nan     0.0878    0.0045
    40        0.0169             nan     0.0878    0.0010
    60        0.0062             nan     0.0878    0.0001
    80        0.0031             nan     0.0878    0.0001
   100        0.0021             nan     0.0878    0.0000
   120        0.0016             nan     0.0878   -0.0000
   140        0.0014             nan     0.0878   -0.0000
   160        0.0013             nan     0.0878   -0.0000
   180        0.0012             nan     0.0878    0.0000
   200        0.0011             nan     0.0878   -0.0000
   220        0.0011             nan     0.0878   -0.0000
   240        0.0010             nan     0.0878   -0.0000
   260        0.0010             nan     0.0878   -0.0000
   280        0.0010             nan     0.0878   -0.0000
   300        0.0009             nan     0.0878   -0.0000
   320        0.0009             nan     0.0878   -0.0000
   340        0.0009             nan     0.0878   -0.0000
   360        0.0009             nan     0.0878   -0.0000
   380        0.0009             nan     0.0878   -0.0000
   400        0.0008             nan     0.0878   -0.0000
   420        0.0008             nan     0.0878   -0.0000
   440        0.0008             nan     0.0878   -0.0000
   460        0.0008             nan     0.0878   -0.0000
   480        0.0008             nan     0.0878   -0.0000
   500        0.0008             nan     0.0878   -0.0000
   520        0.0007             nan     0.0878   -0.0000
   540        0.0007             nan     0.0878   -0.0000
   560        0.0007             nan     0.0878   -0.0000
   580        0.0007             nan     0.0878   -0.0000
   600        0.0007             nan     0.0878   -0.0000
   620        0.0007             nan     0.0878   -0.0000
   640        0.0007             nan     0.0878   -0.0000
   660        0.0007             nan     0.0878   -0.0000
   680        0.0007             nan     0.0878   -0.0000
   700        0.0006             nan     0.0878   -0.0000
   720        0.0006             nan     0.0878   -0.0000
   740        0.0006             nan     0.0878   -0.0000
   760        0.0006             nan     0.0878   -0.0000
   780        0.0006             nan     0.0878   -0.0000
   800        0.0006             nan     0.0878   -0.0000
   820        0.0006             nan     0.0878   -0.0000
   840        0.0006             nan     0.0878   -0.0000
   860        0.0006             nan     0.0878   -0.0000
   880        0.0006             nan     0.0878   -0.0000
   900        0.0006             nan     0.0878    0.0000
   920        0.0006             nan     0.0878   -0.0000
   940        0.0006             nan     0.0878   -0.0000
   960        0.0005             nan     0.0878   -0.0000
   980        0.0005             nan     0.0878   -0.0000
  1000        0.0005             nan     0.0878   -0.0000
  1020        0.0005             nan     0.0878   -0.0000
  1040        0.0005             nan     0.0878   -0.0000
  1060        0.0005             nan     0.0878   -0.0000
  1080        0.0005             nan     0.0878   -0.0000
  1100        0.0005             nan     0.0878   -0.0000
  1120        0.0005             nan     0.0878   -0.0000
  1140        0.0005             nan     0.0878   -0.0000
  1160        0.0005             nan     0.0878   -0.0000
  1180        0.0005             nan     0.0878   -0.0000
  1200        0.0005             nan     0.0878   -0.0000
  1220        0.0005             nan     0.0878   -0.0000
  1240        0.0005             nan     0.0878   -0.0000
  1260        0.0005             nan     0.0878   -0.0000
  1280        0.0005             nan     0.0878   -0.0000
  1300        0.0005             nan     0.0878   -0.0000
  1320        0.0005             nan     0.0878   -0.0000
  1340        0.0005             nan     0.0878   -0.0000
  1360        0.0004             nan     0.0878   -0.0000
  1380        0.0004             nan     0.0878   -0.0000
  1395        0.0004             nan     0.0878   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3313             nan     0.1163    0.0745
     2        0.2672             nan     0.1163    0.0640
     3        0.2185             nan     0.1163    0.0519
     4        0.1776             nan     0.1163    0.0381
     5        0.1441             nan     0.1163    0.0316
     6        0.1186             nan     0.1163    0.0258
     7        0.0974             nan     0.1163    0.0209
     8        0.0800             nan     0.1163    0.0172
     9        0.0663             nan     0.1163    0.0130
    10        0.0548             nan     0.1163    0.0118
    20        0.0099             nan     0.1163    0.0018
    40        0.0014             nan     0.1163    0.0001
    60        0.0008             nan     0.1163   -0.0000
    80        0.0006             nan     0.1163    0.0000
   100        0.0005             nan     0.1163    0.0000
   120        0.0004             nan     0.1163   -0.0000
   140        0.0004             nan     0.1163   -0.0000
   160        0.0003             nan     0.1163   -0.0000
   180        0.0003             nan     0.1163   -0.0000
   200        0.0003             nan     0.1163    0.0000
   220        0.0002             nan     0.1163   -0.0000
   240        0.0002             nan     0.1163   -0.0000
   260        0.0002             nan     0.1163   -0.0000
   280        0.0002             nan     0.1163   -0.0000
   300        0.0002             nan     0.1163   -0.0000
   320        0.0002             nan     0.1163    0.0000
   340        0.0001             nan     0.1163   -0.0000
   360        0.0001             nan     0.1163   -0.0000
   380        0.0001             nan     0.1163   -0.0000
   400        0.0001             nan     0.1163   -0.0000
   420        0.0001             nan     0.1163   -0.0000
   440        0.0001             nan     0.1163   -0.0000
   460        0.0001             nan     0.1163   -0.0000
   480        0.0001             nan     0.1163   -0.0000
   500        0.0001             nan     0.1163   -0.0000
   520        0.0001             nan     0.1163    0.0000
   540        0.0001             nan     0.1163   -0.0000
   560        0.0001             nan     0.1163   -0.0000
   580        0.0001             nan     0.1163   -0.0000
   600        0.0001             nan     0.1163   -0.0000
   620        0.0001             nan     0.1163   -0.0000
   640        0.0001             nan     0.1163   -0.0000
   660        0.0001             nan     0.1163   -0.0000
   680        0.0001             nan     0.1163   -0.0000
   700        0.0001             nan     0.1163   -0.0000
   720        0.0001             nan     0.1163   -0.0000
   740        0.0000             nan     0.1163   -0.0000
   760        0.0000             nan     0.1163   -0.0000
   780        0.0000             nan     0.1163   -0.0000
   800        0.0000             nan     0.1163   -0.0000
   820        0.0000             nan     0.1163   -0.0000
   840        0.0000             nan     0.1163   -0.0000
   860        0.0000             nan     0.1163   -0.0000
   880        0.0000             nan     0.1163   -0.0000
   900        0.0000             nan     0.1163   -0.0000
   920        0.0000             nan     0.1163   -0.0000
   940        0.0000             nan     0.1163   -0.0000
   960        0.0000             nan     0.1163   -0.0000
   980        0.0000             nan     0.1163   -0.0000
  1000        0.0000             nan     0.1163   -0.0000
  1020        0.0000             nan     0.1163   -0.0000
  1040        0.0000             nan     0.1163   -0.0000
  1060        0.0000             nan     0.1163   -0.0000
  1080        0.0000             nan     0.1163   -0.0000
  1100        0.0000             nan     0.1163   -0.0000
  1106        0.0000             nan     0.1163   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2904             nan     0.1626    0.1053
     2        0.2077             nan     0.1626    0.0854
     3        0.1471             nan     0.1626    0.0613
     4        0.1069             nan     0.1626    0.0398
     5        0.0775             nan     0.1626    0.0299
     6        0.0566             nan     0.1626    0.0205
     7        0.0415             nan     0.1626    0.0139
     8        0.0312             nan     0.1626    0.0116
     9        0.0236             nan     0.1626    0.0076
    10        0.0180             nan     0.1626    0.0055
    20        0.0031             nan     0.1626    0.0003
    40        0.0015             nan     0.1626   -0.0000
    50        0.0012             nan     0.1626   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2672             nan     0.2083    0.1483
     2        0.1800             nan     0.2083    0.0987
     3        0.1191             nan     0.2083    0.0638
     4        0.0805             nan     0.2083    0.0406
     5        0.0563             nan     0.2083    0.0238
     6        0.0390             nan     0.2083    0.0183
     7        0.0278             nan     0.2083    0.0112
     8        0.0198             nan     0.2083    0.0077
     9        0.0147             nan     0.2083    0.0051
    10        0.0110             nan     0.2083    0.0032
    20        0.0030             nan     0.2083    0.0000
    40        0.0017             nan     0.2083    0.0000
    60        0.0013             nan     0.2083   -0.0000
    80        0.0010             nan     0.2083   -0.0000
   100        0.0008             nan     0.2083   -0.0000
   120        0.0007             nan     0.2083   -0.0000
   140        0.0006             nan     0.2083   -0.0000
   160        0.0006             nan     0.2083   -0.0000
   180        0.0005             nan     0.2083   -0.0000
   200        0.0004             nan     0.2083   -0.0000
   220        0.0004             nan     0.2083   -0.0000
   240        0.0004             nan     0.2083   -0.0000
   260        0.0003             nan     0.2083   -0.0000
   280        0.0003             nan     0.2083   -0.0000
   300        0.0003             nan     0.2083   -0.0000
   320        0.0003             nan     0.2083   -0.0000
   340        0.0003             nan     0.2083   -0.0000
   360        0.0003             nan     0.2083   -0.0000
   380        0.0002             nan     0.2083   -0.0000
   400        0.0002             nan     0.2083   -0.0000
   420        0.0002             nan     0.2083   -0.0000
   440        0.0002             nan     0.2083   -0.0000
   460        0.0002             nan     0.2083   -0.0000
   480        0.0002             nan     0.2083   -0.0000
   500        0.0002             nan     0.2083   -0.0000
   520        0.0002             nan     0.2083   -0.0000
   540        0.0002             nan     0.2083   -0.0000
   560        0.0002             nan     0.2083   -0.0000
   580        0.0002             nan     0.2083   -0.0000
   600        0.0002             nan     0.2083   -0.0000
   620        0.0001             nan     0.2083   -0.0000
   640        0.0001             nan     0.2083   -0.0000
   660        0.0001             nan     0.2083   -0.0000
   680        0.0001             nan     0.2083   -0.0000
   700        0.0001             nan     0.2083   -0.0000
   720        0.0001             nan     0.2083   -0.0000
   740        0.0001             nan     0.2083   -0.0000
   760        0.0001             nan     0.2083   -0.0000
   780        0.0001             nan     0.2083   -0.0000
   800        0.0001             nan     0.2083   -0.0000
   820        0.0001             nan     0.2083   -0.0000
   840        0.0001             nan     0.2083   -0.0000
   860        0.0001             nan     0.2083   -0.0000
   880        0.0001             nan     0.2083   -0.0000
   900        0.0001             nan     0.2083   -0.0000
   920        0.0001             nan     0.2083   -0.0000
   940        0.0001             nan     0.2083   -0.0000
   960        0.0001             nan     0.2083   -0.0000
   980        0.0001             nan     0.2083   -0.0000
  1000        0.0001             nan     0.2083   -0.0000
  1020        0.0001             nan     0.2083   -0.0000
  1040        0.0001             nan     0.2083   -0.0000
  1060        0.0001             nan     0.2083   -0.0000
  1080        0.0001             nan     0.2083   -0.0000
  1100        0.0001             nan     0.2083   -0.0000
  1120        0.0001             nan     0.2083   -0.0000
  1140        0.0001             nan     0.2083   -0.0000
  1160        0.0001             nan     0.2083   -0.0000
  1180        0.0001             nan     0.2083   -0.0000
  1200        0.0001             nan     0.2083   -0.0000
  1220        0.0001             nan     0.2083   -0.0000
  1240        0.0001             nan     0.2083   -0.0000
  1260        0.0001             nan     0.2083   -0.0000
  1280        0.0001             nan     0.2083   -0.0000
  1300        0.0001             nan     0.2083   -0.0000
  1320        0.0001             nan     0.2083   -0.0000
  1340        0.0001             nan     0.2083   -0.0000
  1360        0.0001             nan     0.2083   -0.0000
  1380        0.0001             nan     0.2083   -0.0000
  1400        0.0001             nan     0.2083   -0.0000
  1420        0.0001             nan     0.2083   -0.0000
  1440        0.0001             nan     0.2083   -0.0000
  1460        0.0001             nan     0.2083   -0.0000
  1480        0.0001             nan     0.2083   -0.0000
  1500        0.0001             nan     0.2083   -0.0000
  1520        0.0001             nan     0.2083   -0.0000
  1540        0.0001             nan     0.2083   -0.0000
  1560        0.0001             nan     0.2083   -0.0000
  1580        0.0001             nan     0.2083   -0.0000
  1600        0.0001             nan     0.2083   -0.0000
  1620        0.0001             nan     0.2083   -0.0000
  1640        0.0001             nan     0.2083   -0.0000
  1660        0.0001             nan     0.2083   -0.0000
  1680        0.0001             nan     0.2083   -0.0000
  1700        0.0001             nan     0.2083   -0.0000
  1720        0.0001             nan     0.2083   -0.0000
  1740        0.0001             nan     0.2083   -0.0000
  1760        0.0001             nan     0.2083   -0.0000
  1780        0.0001             nan     0.2083    0.0000
  1800        0.0001             nan     0.2083   -0.0000
  1820        0.0001             nan     0.2083   -0.0000
  1840        0.0001             nan     0.2083   -0.0000
  1860        0.0001             nan     0.2083   -0.0000
  1880        0.0001             nan     0.2083   -0.0000
  1900        0.0001             nan     0.2083   -0.0000
  1920        0.0001             nan     0.2083   -0.0000
  1940        0.0001             nan     0.2083   -0.0000
  1960        0.0001             nan     0.2083   -0.0000
  1974        0.0001             nan     0.2083   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2698             nan     0.2133    0.1379
     2        0.1834             nan     0.2133    0.0891
     3        0.1249             nan     0.2133    0.0548
     4        0.0855             nan     0.2133    0.0405
     5        0.0591             nan     0.2133    0.0211
     6        0.0421             nan     0.2133    0.0169
     7        0.0310             nan     0.2133    0.0106
     8        0.0232             nan     0.2133    0.0075
     9        0.0173             nan     0.2133    0.0060
    10        0.0132             nan     0.2133    0.0048
    20        0.0030             nan     0.2133    0.0001
    40        0.0015             nan     0.2133   -0.0000
    60        0.0010             nan     0.2133   -0.0000
    80        0.0008             nan     0.2133   -0.0000
   100        0.0006             nan     0.2133   -0.0000
   120        0.0005             nan     0.2133    0.0000
   140        0.0005             nan     0.2133   -0.0000
   160        0.0004             nan     0.2133   -0.0000
   180        0.0003             nan     0.2133   -0.0000
   200        0.0003             nan     0.2133   -0.0000
   220        0.0003             nan     0.2133   -0.0000
   240        0.0002             nan     0.2133    0.0000
   260        0.0002             nan     0.2133   -0.0000
   280        0.0002             nan     0.2133   -0.0000
   300        0.0002             nan     0.2133   -0.0000
   320        0.0002             nan     0.2133   -0.0000
   340        0.0001             nan     0.2133   -0.0000
   360        0.0001             nan     0.2133   -0.0000
   380        0.0001             nan     0.2133   -0.0000
   400        0.0001             nan     0.2133   -0.0000
   420        0.0001             nan     0.2133   -0.0000
   440        0.0001             nan     0.2133   -0.0000
   460        0.0001             nan     0.2133   -0.0000
   480        0.0001             nan     0.2133   -0.0000
   500        0.0001             nan     0.2133   -0.0000
   520        0.0001             nan     0.2133   -0.0000
   540        0.0001             nan     0.2133   -0.0000
   560        0.0001             nan     0.2133   -0.0000
   580        0.0001             nan     0.2133   -0.0000
   600        0.0001             nan     0.2133   -0.0000
   620        0.0001             nan     0.2133   -0.0000
   640        0.0001             nan     0.2133   -0.0000
   660        0.0001             nan     0.2133   -0.0000
   680        0.0000             nan     0.2133   -0.0000
   700        0.0000             nan     0.2133   -0.0000
   720        0.0000             nan     0.2133   -0.0000
   740        0.0000             nan     0.2133   -0.0000
   760        0.0000             nan     0.2133   -0.0000
   780        0.0000             nan     0.2133   -0.0000
   800        0.0000             nan     0.2133   -0.0000
   820        0.0000             nan     0.2133   -0.0000
   840        0.0000             nan     0.2133   -0.0000
   860        0.0000             nan     0.2133   -0.0000
   880        0.0000             nan     0.2133   -0.0000
   900        0.0000             nan     0.2133   -0.0000
   920        0.0000             nan     0.2133   -0.0000
   940        0.0000             nan     0.2133   -0.0000
   960        0.0000             nan     0.2133   -0.0000
   980        0.0000             nan     0.2133   -0.0000
  1000        0.0000             nan     0.2133   -0.0000
  1020        0.0000             nan     0.2133   -0.0000
  1040        0.0000             nan     0.2133   -0.0000
  1060        0.0000             nan     0.2133   -0.0000
  1080        0.0000             nan     0.2133   -0.0000
  1100        0.0000             nan     0.2133   -0.0000
  1120        0.0000             nan     0.2133   -0.0000
  1140        0.0000             nan     0.2133   -0.0000
  1160        0.0000             nan     0.2133   -0.0000
  1180        0.0000             nan     0.2133   -0.0000
  1200        0.0000             nan     0.2133   -0.0000
  1220        0.0000             nan     0.2133   -0.0000
  1240        0.0000             nan     0.2133   -0.0000
  1260        0.0000             nan     0.2133   -0.0000
  1280        0.0000             nan     0.2133   -0.0000
  1300        0.0000             nan     0.2133   -0.0000
  1320        0.0000             nan     0.2133   -0.0000
  1340        0.0000             nan     0.2133   -0.0000
  1360        0.0000             nan     0.2133   -0.0000
  1380        0.0000             nan     0.2133   -0.0000
  1400        0.0000             nan     0.2133   -0.0000
  1420        0.0000             nan     0.2133   -0.0000
  1440        0.0000             nan     0.2133   -0.0000
  1460        0.0000             nan     0.2133   -0.0000
  1480        0.0000             nan     0.2133   -0.0000
  1500        0.0000             nan     0.2133   -0.0000
  1520        0.0000             nan     0.2133   -0.0000
  1540        0.0000             nan     0.2133   -0.0000
  1560        0.0000             nan     0.2133   -0.0000
  1580        0.0000             nan     0.2133   -0.0000
  1600        0.0000             nan     0.2133   -0.0000
  1620        0.0000             nan     0.2133   -0.0000
  1640        0.0000             nan     0.2133   -0.0000
  1660        0.0000             nan     0.2133   -0.0000
  1680        0.0000             nan     0.2133   -0.0000
  1700        0.0000             nan     0.2133   -0.0000
  1720        0.0000             nan     0.2133   -0.0000
  1740        0.0000             nan     0.2133   -0.0000
  1760        0.0000             nan     0.2133   -0.0000
  1780        0.0000             nan     0.2133   -0.0000
  1800        0.0000             nan     0.2133   -0.0000
  1820        0.0000             nan     0.2133   -0.0000
  1840        0.0000             nan     0.2133   -0.0000
  1860        0.0000             nan     0.2133   -0.0000
  1880        0.0000             nan     0.2133   -0.0000
  1900        0.0000             nan     0.2133   -0.0000
  1920        0.0000             nan     0.2133   -0.0000
  1940        0.0000             nan     0.2133   -0.0000
  1960        0.0000             nan     0.2133   -0.0000
  1980        0.0000             nan     0.2133   -0.0000
  2000        0.0000             nan     0.2133   -0.0000
  2020        0.0000             nan     0.2133   -0.0000
  2040        0.0000             nan     0.2133   -0.0000
  2060        0.0000             nan     0.2133   -0.0000
  2080        0.0000             nan     0.2133   -0.0000
  2100        0.0000             nan     0.2133   -0.0000
  2120        0.0000             nan     0.2133   -0.0000
  2140        0.0000             nan     0.2133   -0.0000
  2160        0.0000             nan     0.2133   -0.0000
  2180        0.0000             nan     0.2133   -0.0000
  2200        0.0000             nan     0.2133   -0.0000
  2220        0.0000             nan     0.2133   -0.0000
  2240        0.0000             nan     0.2133   -0.0000
  2260        0.0000             nan     0.2133   -0.0000
  2280        0.0000             nan     0.2133   -0.0000
  2300        0.0000             nan     0.2133   -0.0000
  2320        0.0000             nan     0.2133   -0.0000
  2340        0.0000             nan     0.2133   -0.0000
  2360        0.0000             nan     0.2133   -0.0000
  2380        0.0000             nan     0.2133   -0.0000
  2400        0.0000             nan     0.2133   -0.0000
  2420        0.0000             nan     0.2133   -0.0000
  2440        0.0000             nan     0.2133   -0.0000
  2460        0.0000             nan     0.2133   -0.0000
  2480        0.0000             nan     0.2133   -0.0000
  2500        0.0000             nan     0.2133   -0.0000
  2520        0.0000             nan     0.2133   -0.0000
  2540        0.0000             nan     0.2133   -0.0000
  2560        0.0000             nan     0.2133   -0.0000
  2580        0.0000             nan     0.2133   -0.0000
  2600        0.0000             nan     0.2133   -0.0000
  2620        0.0000             nan     0.2133   -0.0000
  2640        0.0000             nan     0.2133   -0.0000
  2660        0.0000             nan     0.2133   -0.0000
  2680        0.0000             nan     0.2133   -0.0000
  2700        0.0000             nan     0.2133   -0.0000
  2720        0.0000             nan     0.2133   -0.0000
  2740        0.0000             nan     0.2133   -0.0000
  2760        0.0000             nan     0.2133   -0.0000
  2780        0.0000             nan     0.2133   -0.0000
  2800        0.0000             nan     0.2133   -0.0000
  2820        0.0000             nan     0.2133   -0.0000
  2840        0.0000             nan     0.2133   -0.0000
  2860        0.0000             nan     0.2133   -0.0000
  2880        0.0000             nan     0.2133   -0.0000
  2900        0.0000             nan     0.2133   -0.0000
  2920        0.0000             nan     0.2133   -0.0000
  2940        0.0000             nan     0.2133   -0.0000
  2960        0.0000             nan     0.2133   -0.0000
  2980        0.0000             nan     0.2133   -0.0000
  3000        0.0000             nan     0.2133   -0.0000
  3020        0.0000             nan     0.2133   -0.0000
  3040        0.0000             nan     0.2133   -0.0000
  3060        0.0000             nan     0.2133   -0.0000
  3080        0.0000             nan     0.2133   -0.0000
  3100        0.0000             nan     0.2133   -0.0000
  3120        0.0000             nan     0.2133   -0.0000
  3140        0.0000             nan     0.2133   -0.0000
  3160        0.0000             nan     0.2133   -0.0000
  3180        0.0000             nan     0.2133   -0.0000
  3200        0.0000             nan     0.2133   -0.0000
  3220        0.0000             nan     0.2133   -0.0000
  3240        0.0000             nan     0.2133   -0.0000
  3260        0.0000             nan     0.2133   -0.0000
  3280        0.0000             nan     0.2133   -0.0000
  3300        0.0000             nan     0.2133   -0.0000
  3320        0.0000             nan     0.2133   -0.0000
  3340        0.0000             nan     0.2133   -0.0000
  3360        0.0000             nan     0.2133   -0.0000
  3380        0.0000             nan     0.2133   -0.0000
  3400        0.0000             nan     0.2133   -0.0000
  3420        0.0000             nan     0.2133   -0.0000
  3440        0.0000             nan     0.2133   -0.0000
  3460        0.0000             nan     0.2133   -0.0000
  3480        0.0000             nan     0.2133   -0.0000
  3500        0.0000             nan     0.2133   -0.0000
  3520        0.0000             nan     0.2133   -0.0000
  3540        0.0000             nan     0.2133   -0.0000
  3560        0.0000             nan     0.2133   -0.0000
  3580        0.0000             nan     0.2133   -0.0000
  3600        0.0000             nan     0.2133   -0.0000
  3620        0.0000             nan     0.2133   -0.0000
  3640        0.0000             nan     0.2133   -0.0000
  3660        0.0000             nan     0.2133   -0.0000
  3680        0.0000             nan     0.2133   -0.0000
  3700        0.0000             nan     0.2133   -0.0000
  3720        0.0000             nan     0.2133   -0.0000
  3740        0.0000             nan     0.2133   -0.0000
  3760        0.0000             nan     0.2133   -0.0000
  3780        0.0000             nan     0.2133   -0.0000
  3800        0.0000             nan     0.2133   -0.0000
  3820        0.0000             nan     0.2133   -0.0000
  3840        0.0000             nan     0.2133   -0.0000
  3860        0.0000             nan     0.2133   -0.0000
  3880        0.0000             nan     0.2133   -0.0000
  3900        0.0000             nan     0.2133   -0.0000
  3920        0.0000             nan     0.2133   -0.0000
  3940        0.0000             nan     0.2133   -0.0000
  3960        0.0000             nan     0.2133   -0.0000
  3980        0.0000             nan     0.2133   -0.0000
  4000        0.0000             nan     0.2133   -0.0000
  4020        0.0000             nan     0.2133   -0.0000
  4040        0.0000             nan     0.2133   -0.0000
  4060        0.0000             nan     0.2133   -0.0000
  4080        0.0000             nan     0.2133   -0.0000
  4100        0.0000             nan     0.2133   -0.0000
  4120        0.0000             nan     0.2133   -0.0000
  4140        0.0000             nan     0.2133   -0.0000
  4160        0.0000             nan     0.2133   -0.0000
  4180        0.0000             nan     0.2133   -0.0000
  4200        0.0000             nan     0.2133   -0.0000
  4220        0.0000             nan     0.2133   -0.0000
  4240        0.0000             nan     0.2133   -0.0000
  4260        0.0000             nan     0.2133   -0.0000
  4280        0.0000             nan     0.2133   -0.0000
  4300        0.0000             nan     0.2133   -0.0000
  4320        0.0000             nan     0.2133   -0.0000
  4340        0.0000             nan     0.2133   -0.0000
  4360        0.0000             nan     0.2133   -0.0000
  4380        0.0000             nan     0.2133   -0.0000
  4400        0.0000             nan     0.2133   -0.0000
  4420        0.0000             nan     0.2133   -0.0000
  4439        0.0000             nan     0.2133   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2595             nan     0.2206    0.1535
     2        0.1675             nan     0.2206    0.0838
     3        0.1092             nan     0.2206    0.0555
     4        0.0737             nan     0.2206    0.0359
     5        0.0509             nan     0.2206    0.0240
     6        0.0344             nan     0.2206    0.0163
     7        0.0238             nan     0.2206    0.0101
     8        0.0168             nan     0.2206    0.0080
     9        0.0120             nan     0.2206    0.0043
    10        0.0088             nan     0.2206    0.0030
    20        0.0017             nan     0.2206    0.0001
    40        0.0010             nan     0.2206   -0.0000
    60        0.0008             nan     0.2206   -0.0000
    80        0.0006             nan     0.2206   -0.0000
   100        0.0005             nan     0.2206   -0.0000
   120        0.0004             nan     0.2206   -0.0000
   140        0.0004             nan     0.2206   -0.0000
   160        0.0003             nan     0.2206   -0.0000
   180        0.0003             nan     0.2206   -0.0000
   200        0.0002             nan     0.2206   -0.0000
   220        0.0002             nan     0.2206   -0.0000
   240        0.0002             nan     0.2206   -0.0000
   260        0.0002             nan     0.2206   -0.0000
   280        0.0001             nan     0.2206   -0.0000
   300        0.0001             nan     0.2206   -0.0000
   320        0.0001             nan     0.2206   -0.0000
   340        0.0001             nan     0.2206   -0.0000
   360        0.0001             nan     0.2206   -0.0000
   380        0.0001             nan     0.2206   -0.0000
   400        0.0001             nan     0.2206   -0.0000
   420        0.0001             nan     0.2206   -0.0000
   440        0.0001             nan     0.2206   -0.0000
   460        0.0001             nan     0.2206   -0.0000
   480        0.0001             nan     0.2206   -0.0000
   500        0.0001             nan     0.2206   -0.0000
   520        0.0000             nan     0.2206   -0.0000
   540        0.0000             nan     0.2206   -0.0000
   560        0.0000             nan     0.2206   -0.0000
   580        0.0000             nan     0.2206   -0.0000
   600        0.0000             nan     0.2206   -0.0000
   620        0.0000             nan     0.2206   -0.0000
   640        0.0000             nan     0.2206   -0.0000
   660        0.0000             nan     0.2206   -0.0000
   680        0.0000             nan     0.2206   -0.0000
   700        0.0000             nan     0.2206   -0.0000
   720        0.0000             nan     0.2206   -0.0000
   740        0.0000             nan     0.2206   -0.0000
   760        0.0000             nan     0.2206   -0.0000
   780        0.0000             nan     0.2206   -0.0000
   800        0.0000             nan     0.2206   -0.0000
   820        0.0000             nan     0.2206   -0.0000
   840        0.0000             nan     0.2206   -0.0000
   860        0.0000             nan     0.2206   -0.0000
   880        0.0000             nan     0.2206   -0.0000
   900        0.0000             nan     0.2206   -0.0000
   920        0.0000             nan     0.2206   -0.0000
   940        0.0000             nan     0.2206   -0.0000
   960        0.0000             nan     0.2206   -0.0000
   980        0.0000             nan     0.2206   -0.0000
  1000        0.0000             nan     0.2206   -0.0000
  1020        0.0000             nan     0.2206   -0.0000
  1040        0.0000             nan     0.2206   -0.0000
  1060        0.0000             nan     0.2206   -0.0000
  1080        0.0000             nan     0.2206   -0.0000
  1100        0.0000             nan     0.2206   -0.0000
  1120        0.0000             nan     0.2206    0.0000
  1140        0.0000             nan     0.2206   -0.0000
  1160        0.0000             nan     0.2206   -0.0000
  1180        0.0000             nan     0.2206   -0.0000
  1200        0.0000             nan     0.2206   -0.0000
  1220        0.0000             nan     0.2206   -0.0000
  1240        0.0000             nan     0.2206   -0.0000
  1260        0.0000             nan     0.2206   -0.0000
  1280        0.0000             nan     0.2206   -0.0000
  1300        0.0000             nan     0.2206   -0.0000
  1320        0.0000             nan     0.2206   -0.0000
  1340        0.0000             nan     0.2206   -0.0000
  1360        0.0000             nan     0.2206   -0.0000
  1380        0.0000             nan     0.2206   -0.0000
  1400        0.0000             nan     0.2206   -0.0000
  1420        0.0000             nan     0.2206   -0.0000
  1440        0.0000             nan     0.2206   -0.0000
  1460        0.0000             nan     0.2206   -0.0000
  1480        0.0000             nan     0.2206   -0.0000
  1500        0.0000             nan     0.2206   -0.0000
  1520        0.0000             nan     0.2206   -0.0000
  1540        0.0000             nan     0.2206   -0.0000
  1560        0.0000             nan     0.2206   -0.0000
  1580        0.0000             nan     0.2206   -0.0000
  1600        0.0000             nan     0.2206    0.0000
  1620        0.0000             nan     0.2206   -0.0000
  1640        0.0000             nan     0.2206   -0.0000
  1660        0.0000             nan     0.2206   -0.0000
  1680        0.0000             nan     0.2206   -0.0000
  1700        0.0000             nan     0.2206   -0.0000
  1720        0.0000             nan     0.2206   -0.0000
  1740        0.0000             nan     0.2206   -0.0000
  1760        0.0000             nan     0.2206   -0.0000
  1780        0.0000             nan     0.2206   -0.0000
  1800        0.0000             nan     0.2206   -0.0000
  1820        0.0000             nan     0.2206    0.0000
  1840        0.0000             nan     0.2206   -0.0000
  1860        0.0000             nan     0.2206   -0.0000
  1880        0.0000             nan     0.2206   -0.0000
  1900        0.0000             nan     0.2206   -0.0000
  1920        0.0000             nan     0.2206   -0.0000
  1940        0.0000             nan     0.2206    0.0000
  1960        0.0000             nan     0.2206   -0.0000
  1980        0.0000             nan     0.2206   -0.0000
  2000        0.0000             nan     0.2206   -0.0000
  2020        0.0000             nan     0.2206    0.0000
  2040        0.0000             nan     0.2206   -0.0000
  2060        0.0000             nan     0.2206   -0.0000
  2080        0.0000             nan     0.2206    0.0000
  2100        0.0000             nan     0.2206   -0.0000
  2120        0.0000             nan     0.2206   -0.0000
  2140        0.0000             nan     0.2206   -0.0000
  2160        0.0000             nan     0.2206   -0.0000
  2180        0.0000             nan     0.2206   -0.0000
  2200        0.0000             nan     0.2206   -0.0000
  2220        0.0000             nan     0.2206   -0.0000
  2240        0.0000             nan     0.2206   -0.0000
  2260        0.0000             nan     0.2206   -0.0000
  2280        0.0000             nan     0.2206   -0.0000
  2300        0.0000             nan     0.2206   -0.0000
  2320        0.0000             nan     0.2206   -0.0000
  2340        0.0000             nan     0.2206   -0.0000
  2360        0.0000             nan     0.2206   -0.0000
  2380        0.0000             nan     0.2206   -0.0000
  2400        0.0000             nan     0.2206   -0.0000
  2420        0.0000             nan     0.2206    0.0000
  2440        0.0000             nan     0.2206   -0.0000
  2460        0.0000             nan     0.2206   -0.0000
  2480        0.0000             nan     0.2206    0.0000
  2500        0.0000             nan     0.2206   -0.0000
  2520        0.0000             nan     0.2206   -0.0000
  2540        0.0000             nan     0.2206   -0.0000
  2560        0.0000             nan     0.2206   -0.0000
  2580        0.0000             nan     0.2206   -0.0000
  2600        0.0000             nan     0.2206   -0.0000
  2620        0.0000             nan     0.2206   -0.0000
  2640        0.0000             nan     0.2206   -0.0000
  2660        0.0000             nan     0.2206    0.0000
  2680        0.0000             nan     0.2206   -0.0000
  2700        0.0000             nan     0.2206   -0.0000
  2720        0.0000             nan     0.2206   -0.0000
  2740        0.0000             nan     0.2206   -0.0000
  2760        0.0000             nan     0.2206   -0.0000
  2780        0.0000             nan     0.2206    0.0000
  2800        0.0000             nan     0.2206   -0.0000
  2820        0.0000             nan     0.2206   -0.0000
  2840        0.0000             nan     0.2206   -0.0000
  2860        0.0000             nan     0.2206   -0.0000
  2880        0.0000             nan     0.2206    0.0000
  2900        0.0000             nan     0.2206   -0.0000
  2920        0.0000             nan     0.2206   -0.0000
  2940        0.0000             nan     0.2206   -0.0000
  2960        0.0000             nan     0.2206   -0.0000
  2980        0.0000             nan     0.2206   -0.0000
  3000        0.0000             nan     0.2206   -0.0000
  3020        0.0000             nan     0.2206    0.0000
  3040        0.0000             nan     0.2206   -0.0000
  3060        0.0000             nan     0.2206   -0.0000
  3080        0.0000             nan     0.2206   -0.0000
  3100        0.0000             nan     0.2206   -0.0000
  3120        0.0000             nan     0.2206   -0.0000
  3140        0.0000             nan     0.2206   -0.0000
  3160        0.0000             nan     0.2206   -0.0000
  3180        0.0000             nan     0.2206    0.0000
  3200        0.0000             nan     0.2206   -0.0000
  3220        0.0000             nan     0.2206   -0.0000
  3222        0.0000             nan     0.2206   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2330             nan     0.2587    0.1856
     2        0.1344             nan     0.2587    0.1035
     3        0.0787             nan     0.2587    0.0597
     4        0.0474             nan     0.2587    0.0323
     5        0.0291             nan     0.2587    0.0172
     6        0.0182             nan     0.2587    0.0119
     7        0.0123             nan     0.2587    0.0065
     8        0.0080             nan     0.2587    0.0035
     9        0.0055             nan     0.2587    0.0022
    10        0.0042             nan     0.2587    0.0012
    20        0.0016             nan     0.2587    0.0001
    40        0.0009             nan     0.2587    0.0000
    60        0.0006             nan     0.2587   -0.0000
    80        0.0005             nan     0.2587   -0.0000
   100        0.0004             nan     0.2587   -0.0000
   120        0.0003             nan     0.2587   -0.0000
   140        0.0003             nan     0.2587   -0.0000
   160        0.0003             nan     0.2587   -0.0000
   180        0.0002             nan     0.2587   -0.0000
   200        0.0002             nan     0.2587   -0.0000
   220        0.0002             nan     0.2587   -0.0000
   240        0.0002             nan     0.2587   -0.0000
   260        0.0002             nan     0.2587   -0.0000
   280        0.0001             nan     0.2587   -0.0000
   300        0.0001             nan     0.2587   -0.0000
   320        0.0001             nan     0.2587   -0.0000
   340        0.0001             nan     0.2587   -0.0000
   360        0.0001             nan     0.2587   -0.0000
   380        0.0001             nan     0.2587   -0.0000
   400        0.0001             nan     0.2587   -0.0000
   420        0.0001             nan     0.2587   -0.0000
   440        0.0001             nan     0.2587   -0.0000
   460        0.0001             nan     0.2587   -0.0000
   480        0.0001             nan     0.2587   -0.0000
   500        0.0001             nan     0.2587   -0.0000
   520        0.0001             nan     0.2587   -0.0000
   540        0.0001             nan     0.2587   -0.0000
   560        0.0001             nan     0.2587   -0.0000
   580        0.0001             nan     0.2587   -0.0000
   600        0.0001             nan     0.2587   -0.0000
   620        0.0001             nan     0.2587   -0.0000
   640        0.0001             nan     0.2587   -0.0000
   660        0.0001             nan     0.2587   -0.0000
   680        0.0001             nan     0.2587   -0.0000
   700        0.0001             nan     0.2587   -0.0000
   720        0.0001             nan     0.2587   -0.0000
   740        0.0001             nan     0.2587   -0.0000
   760        0.0001             nan     0.2587   -0.0000
   780        0.0001             nan     0.2587   -0.0000
   800        0.0001             nan     0.2587   -0.0000
   820        0.0001             nan     0.2587   -0.0000
   840        0.0001             nan     0.2587   -0.0000
   860        0.0001             nan     0.2587   -0.0000
   880        0.0001             nan     0.2587   -0.0000
   900        0.0001             nan     0.2587   -0.0000
   920        0.0001             nan     0.2587   -0.0000
   940        0.0001             nan     0.2587   -0.0000
   960        0.0001             nan     0.2587   -0.0000
   980        0.0001             nan     0.2587   -0.0000
  1000        0.0001             nan     0.2587   -0.0000
  1020        0.0001             nan     0.2587   -0.0000
  1040        0.0001             nan     0.2587   -0.0000
  1060        0.0001             nan     0.2587   -0.0000
  1080        0.0001             nan     0.2587   -0.0000
  1100        0.0001             nan     0.2587    0.0000
  1120        0.0001             nan     0.2587   -0.0000
  1140        0.0001             nan     0.2587   -0.0000
  1160        0.0001             nan     0.2587   -0.0000
  1180        0.0001             nan     0.2587   -0.0000
  1200        0.0001             nan     0.2587   -0.0000
  1220        0.0001             nan     0.2587   -0.0000
  1240        0.0001             nan     0.2587   -0.0000
  1260        0.0001             nan     0.2587   -0.0000
  1280        0.0001             nan     0.2587   -0.0000
  1300        0.0001             nan     0.2587   -0.0000
  1320        0.0001             nan     0.2587   -0.0000
  1340        0.0001             nan     0.2587   -0.0000
  1360        0.0001             nan     0.2587   -0.0000
  1380        0.0001             nan     0.2587   -0.0000
  1400        0.0001             nan     0.2587   -0.0000
  1420        0.0001             nan     0.2587   -0.0000
  1440        0.0001             nan     0.2587    0.0000
  1460        0.0001             nan     0.2587   -0.0000
  1480        0.0001             nan     0.2587   -0.0000
  1500        0.0001             nan     0.2587   -0.0000
  1520        0.0001             nan     0.2587   -0.0000
  1540        0.0001             nan     0.2587    0.0000
  1560        0.0001             nan     0.2587   -0.0000
  1580        0.0001             nan     0.2587   -0.0000
  1600        0.0001             nan     0.2587    0.0000
  1620        0.0001             nan     0.2587   -0.0000
  1640        0.0001             nan     0.2587   -0.0000
  1660        0.0001             nan     0.2587   -0.0000
  1680        0.0001             nan     0.2587   -0.0000
  1700        0.0001             nan     0.2587   -0.0000
  1720        0.0001             nan     0.2587   -0.0000
  1740        0.0001             nan     0.2587   -0.0000
  1760        0.0001             nan     0.2587   -0.0000
  1780        0.0001             nan     0.2587   -0.0000
  1800        0.0001             nan     0.2587   -0.0000
  1820        0.0001             nan     0.2587   -0.0000
  1840        0.0001             nan     0.2587    0.0000
  1860        0.0001             nan     0.2587   -0.0000
  1880        0.0001             nan     0.2587   -0.0000
  1900        0.0001             nan     0.2587   -0.0000
  1920        0.0001             nan     0.2587   -0.0000
  1940        0.0001             nan     0.2587   -0.0000
  1960        0.0001             nan     0.2587   -0.0000
  1980        0.0001             nan     0.2587   -0.0000
  2000        0.0001             nan     0.2587   -0.0000
  2020        0.0001             nan     0.2587   -0.0000
  2040        0.0001             nan     0.2587   -0.0000
  2060        0.0001             nan     0.2587   -0.0000
  2080        0.0001             nan     0.2587   -0.0000
  2100        0.0001             nan     0.2587   -0.0000
  2120        0.0001             nan     0.2587   -0.0000
  2140        0.0001             nan     0.2587   -0.0000
  2160        0.0001             nan     0.2587   -0.0000
  2180        0.0001             nan     0.2587   -0.0000
  2200        0.0001             nan     0.2587   -0.0000
  2220        0.0001             nan     0.2587   -0.0000
  2240        0.0001             nan     0.2587   -0.0000
  2260        0.0001             nan     0.2587   -0.0000
  2280        0.0001             nan     0.2587   -0.0000
  2300        0.0001             nan     0.2587    0.0000
  2320        0.0001             nan     0.2587   -0.0000
  2340        0.0001             nan     0.2587   -0.0000
  2360        0.0001             nan     0.2587   -0.0000
  2380        0.0001             nan     0.2587   -0.0000
  2400        0.0001             nan     0.2587    0.0000
  2420        0.0001             nan     0.2587    0.0000
  2440        0.0001             nan     0.2587   -0.0000
  2460        0.0001             nan     0.2587    0.0000
  2480        0.0001             nan     0.2587   -0.0000
  2500        0.0001             nan     0.2587   -0.0000
  2520        0.0001             nan     0.2587   -0.0000
  2540        0.0001             nan     0.2587    0.0000
  2560        0.0001             nan     0.2587   -0.0000
  2580        0.0001             nan     0.2587   -0.0000
  2600        0.0001             nan     0.2587   -0.0000
  2620        0.0001             nan     0.2587   -0.0000
  2640        0.0001             nan     0.2587   -0.0000
  2660        0.0001             nan     0.2587   -0.0000
  2680        0.0001             nan     0.2587   -0.0000
  2700        0.0001             nan     0.2587    0.0000
  2720        0.0001             nan     0.2587   -0.0000
  2740        0.0001             nan     0.2587   -0.0000
  2760        0.0001             nan     0.2587   -0.0000
  2780        0.0001             nan     0.2587   -0.0000
  2800        0.0001             nan     0.2587   -0.0000
  2820        0.0001             nan     0.2587   -0.0000
  2840        0.0001             nan     0.2587   -0.0000
  2860        0.0001             nan     0.2587   -0.0000
  2880        0.0001             nan     0.2587   -0.0000
  2900        0.0001             nan     0.2587   -0.0000
  2920        0.0001             nan     0.2587   -0.0000
  2940        0.0001             nan     0.2587   -0.0000
  2960        0.0001             nan     0.2587   -0.0000
  2980        0.0001             nan     0.2587   -0.0000
  3000        0.0001             nan     0.2587   -0.0000
  3020        0.0001             nan     0.2587   -0.0000
  3022        0.0001             nan     0.2587   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2213             nan     0.2980    0.1874
     2        0.1241             nan     0.2980    0.0963
     3        0.0740             nan     0.2980    0.0503
     4        0.0460             nan     0.2980    0.0268
     5        0.0285             nan     0.2980    0.0176
     6        0.0182             nan     0.2980    0.0086
     7        0.0124             nan     0.2980    0.0050
     8        0.0096             nan     0.2980    0.0031
     9        0.0074             nan     0.2980    0.0018
    10        0.0062             nan     0.2980    0.0012
    20        0.0030             nan     0.2980    0.0000
    40        0.0017             nan     0.2980   -0.0000
    60        0.0013             nan     0.2980   -0.0001
    80        0.0010             nan     0.2980   -0.0000
   100        0.0009             nan     0.2980   -0.0000
   120        0.0007             nan     0.2980   -0.0000
   140        0.0007             nan     0.2980   -0.0000
   160        0.0006             nan     0.2980   -0.0000
   180        0.0005             nan     0.2980   -0.0000
   200        0.0005             nan     0.2980   -0.0000
   220        0.0004             nan     0.2980   -0.0000
   240        0.0004             nan     0.2980   -0.0000
   260        0.0004             nan     0.2980   -0.0000
   280        0.0004             nan     0.2980   -0.0000
   300        0.0003             nan     0.2980   -0.0000
   320        0.0003             nan     0.2980   -0.0000
   340        0.0003             nan     0.2980   -0.0000
   360        0.0003             nan     0.2980   -0.0000
   380        0.0003             nan     0.2980   -0.0000
   400        0.0003             nan     0.2980   -0.0000
   420        0.0002             nan     0.2980   -0.0000
   440        0.0002             nan     0.2980   -0.0000
   460        0.0002             nan     0.2980   -0.0000
   480        0.0002             nan     0.2980   -0.0000
   500        0.0002             nan     0.2980   -0.0000
   520        0.0002             nan     0.2980   -0.0000
   540        0.0002             nan     0.2980   -0.0000
   560        0.0002             nan     0.2980   -0.0000
   580        0.0002             nan     0.2980   -0.0000
   600        0.0002             nan     0.2980   -0.0000
   620        0.0002             nan     0.2980   -0.0000
   640        0.0002             nan     0.2980   -0.0000
   660        0.0002             nan     0.2980   -0.0000
   680        0.0002             nan     0.2980   -0.0000
   700        0.0001             nan     0.2980   -0.0000
   720        0.0001             nan     0.2980   -0.0000
   740        0.0001             nan     0.2980   -0.0000
   760        0.0001             nan     0.2980   -0.0000
   780        0.0001             nan     0.2980   -0.0000
   800        0.0001             nan     0.2980   -0.0000
   820        0.0001             nan     0.2980   -0.0000
   840        0.0001             nan     0.2980   -0.0000
   860        0.0001             nan     0.2980   -0.0000
   880        0.0001             nan     0.2980   -0.0000
   900        0.0001             nan     0.2980   -0.0000
   920        0.0001             nan     0.2980   -0.0000
   940        0.0001             nan     0.2980   -0.0000
   960        0.0001             nan     0.2980   -0.0000
   980        0.0001             nan     0.2980   -0.0000
  1000        0.0001             nan     0.2980   -0.0000
  1020        0.0001             nan     0.2980   -0.0000
  1040        0.0001             nan     0.2980   -0.0000
  1060        0.0001             nan     0.2980   -0.0000
  1080        0.0001             nan     0.2980   -0.0000
  1100        0.0001             nan     0.2980   -0.0000
  1120        0.0001             nan     0.2980   -0.0000
  1140        0.0001             nan     0.2980    0.0000
  1160        0.0001             nan     0.2980   -0.0000
  1180        0.0001             nan     0.2980   -0.0000
  1200        0.0001             nan     0.2980   -0.0000
  1220        0.0001             nan     0.2980   -0.0000
  1240        0.0001             nan     0.2980   -0.0000
  1260        0.0001             nan     0.2980   -0.0000
  1280        0.0001             nan     0.2980   -0.0000
  1300        0.0001             nan     0.2980   -0.0000
  1320        0.0001             nan     0.2980   -0.0000
  1340        0.0001             nan     0.2980   -0.0000
  1360        0.0001             nan     0.2980   -0.0000
  1380        0.0001             nan     0.2980   -0.0000
  1400        0.0001             nan     0.2980   -0.0000
  1420        0.0001             nan     0.2980   -0.0000
  1440        0.0001             nan     0.2980   -0.0000
  1460        0.0001             nan     0.2980   -0.0000
  1480        0.0001             nan     0.2980   -0.0000
  1500        0.0001             nan     0.2980   -0.0000
  1520        0.0001             nan     0.2980   -0.0000
  1540        0.0001             nan     0.2980   -0.0000
  1560        0.0001             nan     0.2980   -0.0000
  1580        0.0001             nan     0.2980   -0.0000
  1600        0.0001             nan     0.2980   -0.0000
  1620        0.0001             nan     0.2980   -0.0000
  1640        0.0001             nan     0.2980   -0.0000
  1660        0.0001             nan     0.2980   -0.0000
  1680        0.0001             nan     0.2980   -0.0000
  1700        0.0001             nan     0.2980   -0.0000
  1720        0.0001             nan     0.2980   -0.0000
  1740        0.0001             nan     0.2980   -0.0000
  1760        0.0001             nan     0.2980   -0.0000
  1780        0.0001             nan     0.2980    0.0000
  1800        0.0001             nan     0.2980   -0.0000
  1820        0.0001             nan     0.2980   -0.0000
  1840        0.0001             nan     0.2980   -0.0000
  1860        0.0001             nan     0.2980   -0.0000
  1880        0.0001             nan     0.2980   -0.0000
  1900        0.0001             nan     0.2980   -0.0000
  1920        0.0001             nan     0.2980   -0.0000
  1940        0.0001             nan     0.2980   -0.0000
  1960        0.0001             nan     0.2980   -0.0000
  1980        0.0001             nan     0.2980   -0.0000
  2000        0.0001             nan     0.2980   -0.0000
  2020        0.0001             nan     0.2980   -0.0000
  2040        0.0001             nan     0.2980   -0.0000
  2060        0.0001             nan     0.2980   -0.0000
  2080        0.0001             nan     0.2980   -0.0000
  2100        0.0001             nan     0.2980   -0.0000
  2120        0.0001             nan     0.2980   -0.0000
  2140        0.0001             nan     0.2980   -0.0000
  2160        0.0001             nan     0.2980   -0.0000
  2180        0.0001             nan     0.2980   -0.0000
  2200        0.0001             nan     0.2980    0.0000
  2220        0.0001             nan     0.2980   -0.0000
  2240        0.0001             nan     0.2980   -0.0000
  2260        0.0001             nan     0.2980   -0.0000
  2280        0.0001             nan     0.2980   -0.0000
  2300        0.0001             nan     0.2980   -0.0000
  2320        0.0001             nan     0.2980   -0.0000
  2340        0.0001             nan     0.2980   -0.0000
  2360        0.0001             nan     0.2980   -0.0000
  2380        0.0001             nan     0.2980   -0.0000
  2400        0.0001             nan     0.2980   -0.0000
  2420        0.0001             nan     0.2980   -0.0000
  2440        0.0001             nan     0.2980   -0.0000
  2460        0.0001             nan     0.2980   -0.0000
  2480        0.0001             nan     0.2980   -0.0000
  2500        0.0001             nan     0.2980   -0.0000
  2520        0.0001             nan     0.2980   -0.0000
  2540        0.0001             nan     0.2980   -0.0000
  2560        0.0001             nan     0.2980   -0.0000
  2580        0.0001             nan     0.2980   -0.0000
  2600        0.0001             nan     0.2980   -0.0000
  2620        0.0001             nan     0.2980   -0.0000
  2640        0.0001             nan     0.2980   -0.0000
  2660        0.0001             nan     0.2980   -0.0000
  2680        0.0001             nan     0.2980   -0.0000
  2700        0.0001             nan     0.2980    0.0000
  2720        0.0001             nan     0.2980   -0.0000
  2740        0.0001             nan     0.2980   -0.0000
  2760        0.0001             nan     0.2980   -0.0000
  2780        0.0001             nan     0.2980   -0.0000
  2800        0.0001             nan     0.2980   -0.0000
  2820        0.0001             nan     0.2980   -0.0000
  2840        0.0001             nan     0.2980   -0.0000
  2860        0.0001             nan     0.2980   -0.0000
  2880        0.0001             nan     0.2980   -0.0000
  2900        0.0001             nan     0.2980    0.0000
  2920        0.0001             nan     0.2980   -0.0000
  2940        0.0001             nan     0.2980   -0.0000
  2960        0.0001             nan     0.2980   -0.0000
  2980        0.0001             nan     0.2980   -0.0000
  3000        0.0001             nan     0.2980   -0.0000
  3020        0.0001             nan     0.2980   -0.0000
  3040        0.0001             nan     0.2980   -0.0000
  3060        0.0001             nan     0.2980   -0.0000
  3080        0.0001             nan     0.2980   -0.0000
  3100        0.0001             nan     0.2980   -0.0000
  3120        0.0001             nan     0.2980   -0.0000
  3140        0.0001             nan     0.2980   -0.0000
  3160        0.0001             nan     0.2980   -0.0000
  3180        0.0001             nan     0.2980   -0.0000
  3200        0.0001             nan     0.2980   -0.0000
  3220        0.0001             nan     0.2980   -0.0000
  3240        0.0001             nan     0.2980   -0.0000
  3260        0.0001             nan     0.2980   -0.0000
  3280        0.0001             nan     0.2980   -0.0000
  3300        0.0001             nan     0.2980   -0.0000
  3320        0.0001             nan     0.2980   -0.0000
  3340        0.0001             nan     0.2980   -0.0000
  3360        0.0001             nan     0.2980    0.0000
  3380        0.0001             nan     0.2980   -0.0000
  3400        0.0001             nan     0.2980   -0.0000
  3420        0.0001             nan     0.2980    0.0000
  3440        0.0001             nan     0.2980   -0.0000
  3460        0.0001             nan     0.2980   -0.0000
  3480        0.0001             nan     0.2980   -0.0000
  3500        0.0001             nan     0.2980   -0.0000
  3520        0.0001             nan     0.2980    0.0000
  3540        0.0001             nan     0.2980   -0.0000
  3560        0.0001             nan     0.2980   -0.0000
  3580        0.0001             nan     0.2980   -0.0000
  3600        0.0001             nan     0.2980   -0.0000
  3620        0.0001             nan     0.2980   -0.0000
  3640        0.0001             nan     0.2980   -0.0000
  3660        0.0001             nan     0.2980   -0.0000
  3680        0.0001             nan     0.2980   -0.0000
  3700        0.0001             nan     0.2980   -0.0000
  3720        0.0001             nan     0.2980   -0.0000
  3740        0.0001             nan     0.2980   -0.0000
  3760        0.0001             nan     0.2980   -0.0000
  3780        0.0001             nan     0.2980    0.0000
  3800        0.0001             nan     0.2980   -0.0000
  3820        0.0001             nan     0.2980   -0.0000
  3840        0.0001             nan     0.2980   -0.0000
  3860        0.0001             nan     0.2980    0.0000
  3880        0.0001             nan     0.2980   -0.0000
  3900        0.0001             nan     0.2980   -0.0000
  3920        0.0001             nan     0.2980   -0.0000
  3940        0.0001             nan     0.2980   -0.0000
  3960        0.0001             nan     0.2980   -0.0000
  3980        0.0001             nan     0.2980    0.0000
  4000        0.0001             nan     0.2980   -0.0000
  4020        0.0001             nan     0.2980   -0.0000
  4040        0.0001             nan     0.2980    0.0000
  4060        0.0001             nan     0.2980   -0.0000
  4080        0.0001             nan     0.2980   -0.0000
  4100        0.0001             nan     0.2980   -0.0000
  4120        0.0001             nan     0.2980   -0.0000
  4121        0.0001             nan     0.2980   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1917             nan     0.3332    0.2225
     2        0.0927             nan     0.3332    0.0997
     3        0.0457             nan     0.3332    0.0478
     4        0.0232             nan     0.3332    0.0222
     5        0.0131             nan     0.3332    0.0102
     6        0.0079             nan     0.3332    0.0053
     7        0.0052             nan     0.3332    0.0021
     8        0.0040             nan     0.3332    0.0012
     9        0.0034             nan     0.3332    0.0004
    10        0.0029             nan     0.3332    0.0005
    20        0.0016             nan     0.3332    0.0000
    40        0.0009             nan     0.3332    0.0000
    60        0.0006             nan     0.3332   -0.0000
    80        0.0005             nan     0.3332   -0.0000
   100        0.0003             nan     0.3332   -0.0000
   120        0.0003             nan     0.3332   -0.0000
   140        0.0002             nan     0.3332   -0.0000
   160        0.0002             nan     0.3332   -0.0000
   180        0.0002             nan     0.3332   -0.0000
   200        0.0001             nan     0.3332   -0.0000
   220        0.0001             nan     0.3332   -0.0000
   240        0.0001             nan     0.3332   -0.0000
   260        0.0001             nan     0.3332   -0.0000
   280        0.0001             nan     0.3332   -0.0000
   300        0.0001             nan     0.3332   -0.0000
   320        0.0001             nan     0.3332   -0.0000
   340        0.0001             nan     0.3332   -0.0000
   360        0.0001             nan     0.3332   -0.0000
   380        0.0001             nan     0.3332   -0.0000
   400        0.0001             nan     0.3332   -0.0000
   420        0.0001             nan     0.3332   -0.0000
   440        0.0001             nan     0.3332   -0.0000
   460        0.0001             nan     0.3332   -0.0000
   480        0.0001             nan     0.3332   -0.0000
   500        0.0001             nan     0.3332   -0.0000
   520        0.0001             nan     0.3332   -0.0000
   540        0.0001             nan     0.3332   -0.0000
   560        0.0001             nan     0.3332   -0.0000
   580        0.0001             nan     0.3332   -0.0000
   600        0.0000             nan     0.3332   -0.0000
   620        0.0000             nan     0.3332    0.0000
   640        0.0000             nan     0.3332   -0.0000
   660        0.0000             nan     0.3332   -0.0000
   680        0.0000             nan     0.3332   -0.0000
   700        0.0000             nan     0.3332   -0.0000
   720        0.0000             nan     0.3332   -0.0000
   740        0.0000             nan     0.3332   -0.0000
   760        0.0000             nan     0.3332   -0.0000
   780        0.0000             nan     0.3332    0.0000
   800        0.0000             nan     0.3332   -0.0000
   820        0.0000             nan     0.3332   -0.0000
   840        0.0000             nan     0.3332   -0.0000
   860        0.0000             nan     0.3332   -0.0000
   880        0.0000             nan     0.3332   -0.0000
   900        0.0000             nan     0.3332   -0.0000
   920        0.0000             nan     0.3332   -0.0000
   940        0.0000             nan     0.3332   -0.0000
   960        0.0000             nan     0.3332    0.0000
   980        0.0000             nan     0.3332   -0.0000
  1000        0.0000             nan     0.3332   -0.0000
  1020        0.0000             nan     0.3332   -0.0000
  1040        0.0000             nan     0.3332   -0.0000
  1060        0.0000             nan     0.3332   -0.0000
  1080        0.0000             nan     0.3332   -0.0000
  1100        0.0000             nan     0.3332   -0.0000
  1120        0.0000             nan     0.3332   -0.0000
  1140        0.0000             nan     0.3332   -0.0000
  1160        0.0000             nan     0.3332   -0.0000
  1180        0.0000             nan     0.3332   -0.0000
  1200        0.0000             nan     0.3332   -0.0000
  1220        0.0000             nan     0.3332   -0.0000
  1240        0.0000             nan     0.3332   -0.0000
  1260        0.0000             nan     0.3332   -0.0000
  1280        0.0000             nan     0.3332   -0.0000
  1300        0.0000             nan     0.3332   -0.0000
  1320        0.0000             nan     0.3332   -0.0000
  1340        0.0000             nan     0.3332    0.0000
  1360        0.0000             nan     0.3332    0.0000
  1380        0.0000             nan     0.3332   -0.0000
  1400        0.0000             nan     0.3332   -0.0000
  1420        0.0000             nan     0.3332   -0.0000
  1440        0.0000             nan     0.3332   -0.0000
  1460        0.0000             nan     0.3332   -0.0000
  1480        0.0000             nan     0.3332   -0.0000
  1500        0.0000             nan     0.3332    0.0000
  1520        0.0000             nan     0.3332   -0.0000
  1540        0.0000             nan     0.3332   -0.0000
  1560        0.0000             nan     0.3332   -0.0000
  1580        0.0000             nan     0.3332   -0.0000
  1600        0.0000             nan     0.3332   -0.0000
  1620        0.0000             nan     0.3332   -0.0000
  1640        0.0000             nan     0.3332    0.0000
  1660        0.0000             nan     0.3332   -0.0000
  1680        0.0000             nan     0.3332   -0.0000
  1700        0.0000             nan     0.3332   -0.0000
  1720        0.0000             nan     0.3332   -0.0000
  1740        0.0000             nan     0.3332   -0.0000
  1760        0.0000             nan     0.3332    0.0000
  1780        0.0000             nan     0.3332   -0.0000
  1800        0.0000             nan     0.3332   -0.0000
  1820        0.0000             nan     0.3332   -0.0000
  1840        0.0000             nan     0.3332    0.0000
  1860        0.0000             nan     0.3332   -0.0000
  1880        0.0000             nan     0.3332    0.0000
  1900        0.0000             nan     0.3332   -0.0000
  1920        0.0000             nan     0.3332   -0.0000
  1940        0.0000             nan     0.3332   -0.0000
  1960        0.0000             nan     0.3332   -0.0000
  1980        0.0000             nan     0.3332   -0.0000
  2000        0.0000             nan     0.3332    0.0000
  2020        0.0000             nan     0.3332   -0.0000
  2040        0.0000             nan     0.3332   -0.0000
  2060        0.0000             nan     0.3332   -0.0000
  2080        0.0000             nan     0.3332   -0.0000
  2100        0.0000             nan     0.3332   -0.0000
  2120        0.0000             nan     0.3332   -0.0000
  2140        0.0000             nan     0.3332    0.0000
  2160        0.0000             nan     0.3332   -0.0000
  2180        0.0000             nan     0.3332   -0.0000
  2200        0.0000             nan     0.3332   -0.0000
  2220        0.0000             nan     0.3332   -0.0000
  2240        0.0000             nan     0.3332   -0.0000
  2260        0.0000             nan     0.3332   -0.0000
  2280        0.0000             nan     0.3332   -0.0000
  2300        0.0000             nan     0.3332   -0.0000
  2320        0.0000             nan     0.3332   -0.0000
  2340        0.0000             nan     0.3332   -0.0000
  2360        0.0000             nan     0.3332   -0.0000
  2380        0.0000             nan     0.3332   -0.0000
  2400        0.0000             nan     0.3332   -0.0000
  2420        0.0000             nan     0.3332   -0.0000
  2440        0.0000             nan     0.3332   -0.0000
  2460        0.0000             nan     0.3332    0.0000
  2480        0.0000             nan     0.3332   -0.0000
  2500        0.0000             nan     0.3332   -0.0000
  2520        0.0000             nan     0.3332   -0.0000
  2540        0.0000             nan     0.3332   -0.0000
  2560        0.0000             nan     0.3332    0.0000
  2580        0.0000             nan     0.3332   -0.0000
  2600        0.0000             nan     0.3332   -0.0000
  2620        0.0000             nan     0.3332   -0.0000
  2640        0.0000             nan     0.3332    0.0000
  2660        0.0000             nan     0.3332   -0.0000
  2680        0.0000             nan     0.3332   -0.0000
  2700        0.0000             nan     0.3332    0.0000
  2720        0.0000             nan     0.3332   -0.0000
  2740        0.0000             nan     0.3332   -0.0000
  2760        0.0000             nan     0.3332   -0.0000
  2780        0.0000             nan     0.3332   -0.0000
  2800        0.0000             nan     0.3332    0.0000
  2820        0.0000             nan     0.3332    0.0000
  2840        0.0000             nan     0.3332   -0.0000
  2860        0.0000             nan     0.3332    0.0000
  2880        0.0000             nan     0.3332   -0.0000
  2900        0.0000             nan     0.3332   -0.0000
  2920        0.0000             nan     0.3332    0.0000
  2940        0.0000             nan     0.3332   -0.0000
  2960        0.0000             nan     0.3332   -0.0000
  2966        0.0000             nan     0.3332   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1636             nan     0.3949    0.2270
     2        0.0683             nan     0.3949    0.0849
     3        0.0301             nan     0.3949    0.0391
     4        0.0147             nan     0.3949    0.0164
     5        0.0078             nan     0.3949    0.0065
     6        0.0047             nan     0.3949    0.0030
     7        0.0032             nan     0.3949    0.0011
     8        0.0026             nan     0.3949    0.0004
     9        0.0021             nan     0.3949    0.0003
    10        0.0020             nan     0.3949    0.0000
    20        0.0012             nan     0.3949   -0.0001
    40        0.0006             nan     0.3949   -0.0000
    60        0.0004             nan     0.3949   -0.0000
    80        0.0002             nan     0.3949   -0.0000
   100        0.0002             nan     0.3949   -0.0000
   120        0.0001             nan     0.3949   -0.0000
   140        0.0001             nan     0.3949   -0.0000
   160        0.0001             nan     0.3949   -0.0000
   180        0.0001             nan     0.3949   -0.0000
   200        0.0000             nan     0.3949   -0.0000
   220        0.0000             nan     0.3949   -0.0000
   240        0.0000             nan     0.3949   -0.0000
   260        0.0000             nan     0.3949   -0.0000
   280        0.0000             nan     0.3949   -0.0000
   300        0.0000             nan     0.3949   -0.0000
   320        0.0000             nan     0.3949   -0.0000
   340        0.0000             nan     0.3949   -0.0000
   360        0.0000             nan     0.3949   -0.0000
   380        0.0000             nan     0.3949   -0.0000
   400        0.0000             nan     0.3949   -0.0000
   420        0.0000             nan     0.3949   -0.0000
   440        0.0000             nan     0.3949   -0.0000
   460        0.0000             nan     0.3949   -0.0000
   480        0.0000             nan     0.3949   -0.0000
   500        0.0000             nan     0.3949   -0.0000
   520        0.0000             nan     0.3949   -0.0000
   540        0.0000             nan     0.3949   -0.0000
   560        0.0000             nan     0.3949   -0.0000
   580        0.0000             nan     0.3949   -0.0000
   600        0.0000             nan     0.3949   -0.0000
   620        0.0000             nan     0.3949   -0.0000
   640        0.0000             nan     0.3949   -0.0000
   660        0.0000             nan     0.3949   -0.0000
   680        0.0000             nan     0.3949   -0.0000
   700        0.0000             nan     0.3949   -0.0000
   720        0.0000             nan     0.3949   -0.0000
   740        0.0000             nan     0.3949   -0.0000
   760        0.0000             nan     0.3949   -0.0000
   780        0.0000             nan     0.3949   -0.0000
   800        0.0000             nan     0.3949   -0.0000
   820        0.0000             nan     0.3949   -0.0000
   840        0.0000             nan     0.3949   -0.0000
   860        0.0000             nan     0.3949   -0.0000
   880        0.0000             nan     0.3949   -0.0000
   900        0.0000             nan     0.3949   -0.0000
   920        0.0000             nan     0.3949   -0.0000
   940        0.0000             nan     0.3949   -0.0000
   960        0.0000             nan     0.3949   -0.0000
   980        0.0000             nan     0.3949   -0.0000
  1000        0.0000             nan     0.3949   -0.0000
  1020        0.0000             nan     0.3949   -0.0000
  1040        0.0000             nan     0.3949   -0.0000
  1060        0.0000             nan     0.3949   -0.0000
  1080        0.0000             nan     0.3949   -0.0000
  1100        0.0000             nan     0.3949   -0.0000
  1120        0.0000             nan     0.3949   -0.0000
  1140        0.0000             nan     0.3949   -0.0000
  1160        0.0000             nan     0.3949   -0.0000
  1168        0.0000             nan     0.3949   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1675             nan     0.4223    0.2407
     2        0.0727             nan     0.4223    0.0821
     3        0.0372             nan     0.4223    0.0341
     4        0.0205             nan     0.4223    0.0165
     5        0.0129             nan     0.4223    0.0070
     6        0.0091             nan     0.4223    0.0034
     7        0.0072             nan     0.4223    0.0016
     8        0.0062             nan     0.4223    0.0007
     9        0.0056             nan     0.4223    0.0004
    10        0.0053             nan     0.4223    0.0003
    20        0.0031             nan     0.4223    0.0000
    40        0.0019             nan     0.4223   -0.0001
    60        0.0013             nan     0.4223   -0.0000
    80        0.0010             nan     0.4223   -0.0000
   100        0.0008             nan     0.4223   -0.0000
   120        0.0007             nan     0.4223   -0.0000
   140        0.0006             nan     0.4223   -0.0000
   160        0.0005             nan     0.4223   -0.0000
   180        0.0004             nan     0.4223   -0.0000
   200        0.0004             nan     0.4223   -0.0000
   220        0.0004             nan     0.4223   -0.0000
   240        0.0003             nan     0.4223   -0.0000
   260        0.0003             nan     0.4223   -0.0000
   280        0.0003             nan     0.4223   -0.0000
   300        0.0003             nan     0.4223   -0.0000
   320        0.0002             nan     0.4223   -0.0000
   340        0.0002             nan     0.4223   -0.0000
   360        0.0002             nan     0.4223   -0.0000
   380        0.0002             nan     0.4223   -0.0000
   400        0.0002             nan     0.4223   -0.0000
   420        0.0002             nan     0.4223   -0.0000
   440        0.0002             nan     0.4223   -0.0000
   460        0.0002             nan     0.4223   -0.0000
   480        0.0002             nan     0.4223   -0.0000
   500        0.0002             nan     0.4223   -0.0000
   520        0.0001             nan     0.4223   -0.0000
   540        0.0001             nan     0.4223   -0.0000
   560        0.0001             nan     0.4223   -0.0000
   580        0.0001             nan     0.4223   -0.0000
   600        0.0001             nan     0.4223   -0.0000
   620        0.0001             nan     0.4223   -0.0000
   640        0.0001             nan     0.4223   -0.0000
   660        0.0001             nan     0.4223   -0.0000
   680        0.0001             nan     0.4223   -0.0000
   700        0.0001             nan     0.4223   -0.0000
   720        0.0001             nan     0.4223   -0.0000
   740        0.0001             nan     0.4223   -0.0000
   760        0.0001             nan     0.4223   -0.0000
   780        0.0001             nan     0.4223   -0.0000
   800        0.0001             nan     0.4223   -0.0000
   820        0.0001             nan     0.4223   -0.0000
   840        0.0001             nan     0.4223   -0.0000
   860        0.0001             nan     0.4223   -0.0000
   880        0.0001             nan     0.4223   -0.0000
   900        0.0001             nan     0.4223   -0.0000
   920        0.0001             nan     0.4223   -0.0000
   940        0.0001             nan     0.4223   -0.0000
   960        0.0001             nan     0.4223   -0.0000
   980        0.0001             nan     0.4223   -0.0000
  1000        0.0001             nan     0.4223   -0.0000
  1020        0.0001             nan     0.4223   -0.0000
  1040        0.0001             nan     0.4223   -0.0000
  1060        0.0001             nan     0.4223   -0.0000
  1080        0.0001             nan     0.4223    0.0000
  1100        0.0001             nan     0.4223   -0.0000
  1120        0.0001             nan     0.4223   -0.0000
  1140        0.0001             nan     0.4223   -0.0000
  1160        0.0001             nan     0.4223   -0.0000
  1180        0.0001             nan     0.4223   -0.0000
  1200        0.0001             nan     0.4223   -0.0000
  1220        0.0001             nan     0.4223   -0.0000
  1240        0.0001             nan     0.4223    0.0000
  1260        0.0001             nan     0.4223   -0.0000
  1280        0.0001             nan     0.4223   -0.0000
  1300        0.0001             nan     0.4223   -0.0000
  1320        0.0001             nan     0.4223   -0.0000
  1340        0.0001             nan     0.4223   -0.0000
  1360        0.0001             nan     0.4223   -0.0000
  1380        0.0001             nan     0.4223   -0.0000
  1400        0.0001             nan     0.4223   -0.0000
  1420        0.0001             nan     0.4223   -0.0000
  1440        0.0001             nan     0.4223   -0.0000
  1460        0.0001             nan     0.4223   -0.0000
  1480        0.0001             nan     0.4223    0.0000
  1500        0.0001             nan     0.4223   -0.0000
  1520        0.0001             nan     0.4223   -0.0000
  1540        0.0001             nan     0.4223   -0.0000
  1560        0.0001             nan     0.4223   -0.0000
  1580        0.0001             nan     0.4223   -0.0000
  1600        0.0001             nan     0.4223    0.0000
  1620        0.0001             nan     0.4223   -0.0000
  1640        0.0001             nan     0.4223    0.0000
  1660        0.0001             nan     0.4223   -0.0000
  1680        0.0001             nan     0.4223   -0.0000
  1700        0.0001             nan     0.4223   -0.0000
  1720        0.0001             nan     0.4223    0.0000
  1740        0.0001             nan     0.4223   -0.0000
  1760        0.0001             nan     0.4223   -0.0000
  1780        0.0001             nan     0.4223   -0.0000
  1800        0.0001             nan     0.4223    0.0000
  1820        0.0001             nan     0.4223   -0.0000
  1840        0.0001             nan     0.4223   -0.0000
  1860        0.0001             nan     0.4223    0.0000
  1880        0.0001             nan     0.4223   -0.0000
  1900        0.0001             nan     0.4223   -0.0000
  1920        0.0001             nan     0.4223    0.0000
  1940        0.0001             nan     0.4223    0.0000
  1960        0.0001             nan     0.4223    0.0000
  1980        0.0001             nan     0.4223   -0.0000
  2000        0.0001             nan     0.4223   -0.0000
  2020        0.0001             nan     0.4223   -0.0000
  2040        0.0001             nan     0.4223   -0.0000
  2060        0.0001             nan     0.4223   -0.0000
  2080        0.0001             nan     0.4223   -0.0000
  2100        0.0001             nan     0.4223   -0.0000
  2120        0.0001             nan     0.4223   -0.0000
  2140        0.0001             nan     0.4223   -0.0000
  2160        0.0001             nan     0.4223   -0.0000
  2180        0.0001             nan     0.4223   -0.0000
  2200        0.0001             nan     0.4223   -0.0000
  2220        0.0001             nan     0.4223   -0.0000
  2240        0.0001             nan     0.4223   -0.0000
  2260        0.0001             nan     0.4223   -0.0000
  2280        0.0001             nan     0.4223    0.0000
  2300        0.0001             nan     0.4223   -0.0000
  2320        0.0001             nan     0.4223    0.0000
  2340        0.0001             nan     0.4223   -0.0000
  2360        0.0001             nan     0.4223   -0.0000
  2380        0.0001             nan     0.4223   -0.0000
  2400        0.0001             nan     0.4223   -0.0000
  2420        0.0001             nan     0.4223    0.0000
  2440        0.0001             nan     0.4223   -0.0000
  2460        0.0001             nan     0.4223    0.0000
  2480        0.0001             nan     0.4223   -0.0000
  2500        0.0001             nan     0.4223   -0.0000
  2520        0.0001             nan     0.4223    0.0000
  2540        0.0001             nan     0.4223   -0.0000
  2560        0.0001             nan     0.4223   -0.0000
  2580        0.0001             nan     0.4223   -0.0000
  2600        0.0001             nan     0.4223   -0.0000
  2620        0.0001             nan     0.4223   -0.0000
  2640        0.0001             nan     0.4223   -0.0000
  2660        0.0001             nan     0.4223   -0.0000
  2680        0.0001             nan     0.4223   -0.0000
  2700        0.0001             nan     0.4223   -0.0000
  2720        0.0001             nan     0.4223    0.0000
  2740        0.0001             nan     0.4223   -0.0000
  2760        0.0001             nan     0.4223   -0.0000
  2780        0.0001             nan     0.4223   -0.0000
  2800        0.0001             nan     0.4223   -0.0000
  2820        0.0001             nan     0.4223   -0.0000
  2840        0.0001             nan     0.4223   -0.0000
  2860        0.0001             nan     0.4223   -0.0000
  2880        0.0001             nan     0.4223   -0.0000
  2900        0.0001             nan     0.4223   -0.0000
  2920        0.0001             nan     0.4223   -0.0000
  2940        0.0001             nan     0.4223    0.0000
  2960        0.0001             nan     0.4223   -0.0000
  2980        0.0001             nan     0.4223   -0.0000
  3000        0.0001             nan     0.4223   -0.0000
  3020        0.0001             nan     0.4223   -0.0000
  3040        0.0001             nan     0.4223   -0.0000
  3060        0.0001             nan     0.4223   -0.0000
  3080        0.0001             nan     0.4223   -0.0000
  3100        0.0001             nan     0.4223   -0.0000
  3120        0.0001             nan     0.4223   -0.0000
  3140        0.0001             nan     0.4223    0.0000
  3160        0.0001             nan     0.4223   -0.0000
  3180        0.0001             nan     0.4223   -0.0000
  3200        0.0001             nan     0.4223   -0.0000
  3220        0.0001             nan     0.4223   -0.0000
  3240        0.0001             nan     0.4223   -0.0000
  3260        0.0001             nan     0.4223   -0.0000
  3280        0.0001             nan     0.4223   -0.0000
  3300        0.0001             nan     0.4223    0.0000
  3320        0.0001             nan     0.4223   -0.0000
  3340        0.0001             nan     0.4223   -0.0000
  3360        0.0001             nan     0.4223    0.0000
  3380        0.0001             nan     0.4223    0.0000
  3400        0.0001             nan     0.4223   -0.0000
  3420        0.0001             nan     0.4223    0.0000
  3440        0.0001             nan     0.4223    0.0000
  3460        0.0001             nan     0.4223   -0.0000
  3480        0.0001             nan     0.4223   -0.0000
  3500        0.0001             nan     0.4223   -0.0000
  3520        0.0001             nan     0.4223   -0.0000
  3540        0.0001             nan     0.4223   -0.0000
  3560        0.0001             nan     0.4223   -0.0000
  3580        0.0001             nan     0.4223   -0.0000
  3600        0.0001             nan     0.4223   -0.0000
  3620        0.0001             nan     0.4223   -0.0000
  3640        0.0001             nan     0.4223    0.0000
  3660        0.0001             nan     0.4223    0.0000
  3680        0.0001             nan     0.4223   -0.0000
  3700        0.0001             nan     0.4223   -0.0000
  3720        0.0001             nan     0.4223   -0.0000
  3740        0.0001             nan     0.4223   -0.0000
  3760        0.0001             nan     0.4223    0.0000
  3780        0.0001             nan     0.4223   -0.0000
  3800        0.0001             nan     0.4223   -0.0000
  3820        0.0001             nan     0.4223   -0.0000
  3840        0.0001             nan     0.4223   -0.0000
  3860        0.0001             nan     0.4223   -0.0000
  3880        0.0001             nan     0.4223   -0.0000
  3900        0.0001             nan     0.4223   -0.0000
  3920        0.0001             nan     0.4223   -0.0000
  3940        0.0001             nan     0.4223   -0.0000
  3960        0.0001             nan     0.4223   -0.0000
  3980        0.0001             nan     0.4223   -0.0000
  4000        0.0001             nan     0.4223   -0.0000
  4020        0.0001             nan     0.4223   -0.0000
  4040        0.0001             nan     0.4223   -0.0000
  4060        0.0001             nan     0.4223   -0.0000
  4080        0.0001             nan     0.4223   -0.0000
  4100        0.0001             nan     0.4223   -0.0000
  4120        0.0001             nan     0.4223   -0.0000
  4131        0.0001             nan     0.4223   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1327             nan     0.4469    0.2830
     2        0.0493             nan     0.4469    0.0908
     3        0.0191             nan     0.4469    0.0288
     4        0.0090             nan     0.4469    0.0101
     5        0.0052             nan     0.4469    0.0034
     6        0.0037             nan     0.4469    0.0014
     7        0.0030             nan     0.4469    0.0003
     8        0.0028             nan     0.4469    0.0001
     9        0.0024             nan     0.4469    0.0003
    10        0.0022             nan     0.4469    0.0001
    20        0.0013             nan     0.4469   -0.0000
    40        0.0007             nan     0.4469   -0.0000
    60        0.0005             nan     0.4469   -0.0000
    80        0.0003             nan     0.4469   -0.0000
   100        0.0003             nan     0.4469   -0.0000
   120        0.0002             nan     0.4469   -0.0000
   140        0.0002             nan     0.4469   -0.0000
   160        0.0001             nan     0.4469   -0.0000
   180        0.0001             nan     0.4469   -0.0000
   200        0.0001             nan     0.4469   -0.0000
   220        0.0001             nan     0.4469   -0.0000
   240        0.0001             nan     0.4469   -0.0000
   260        0.0001             nan     0.4469   -0.0000
   280        0.0001             nan     0.4469   -0.0000
   300        0.0000             nan     0.4469   -0.0000
   320        0.0000             nan     0.4469   -0.0000
   340        0.0000             nan     0.4469   -0.0000
   360        0.0000             nan     0.4469   -0.0000
   380        0.0000             nan     0.4469   -0.0000
   400        0.0000             nan     0.4469   -0.0000
   420        0.0000             nan     0.4469   -0.0000
   440        0.0000             nan     0.4469   -0.0000
   460        0.0000             nan     0.4469   -0.0000
   480        0.0000             nan     0.4469    0.0000
   500        0.0000             nan     0.4469   -0.0000
   520        0.0000             nan     0.4469    0.0000
   540        0.0000             nan     0.4469   -0.0000
   560        0.0000             nan     0.4469   -0.0000
   580        0.0000             nan     0.4469   -0.0000
   600        0.0000             nan     0.4469   -0.0000
   620        0.0000             nan     0.4469   -0.0000
   640        0.0000             nan     0.4469   -0.0000
   660        0.0000             nan     0.4469   -0.0000
   680        0.0000             nan     0.4469   -0.0000
   700        0.0000             nan     0.4469   -0.0000
   720        0.0000             nan     0.4469   -0.0000
   740        0.0000             nan     0.4469   -0.0000
   760        0.0000             nan     0.4469   -0.0000
   780        0.0000             nan     0.4469   -0.0000
   800        0.0000             nan     0.4469   -0.0000
   820        0.0000             nan     0.4469   -0.0000
   840        0.0000             nan     0.4469   -0.0000
   860        0.0000             nan     0.4469   -0.0000
   880        0.0000             nan     0.4469    0.0000
   900        0.0000             nan     0.4469   -0.0000
   920        0.0000             nan     0.4469   -0.0000
   940        0.0000             nan     0.4469   -0.0000
   960        0.0000             nan     0.4469   -0.0000
   980        0.0000             nan     0.4469   -0.0000
  1000        0.0000             nan     0.4469   -0.0000
  1020        0.0000             nan     0.4469    0.0000
  1040        0.0000             nan     0.4469   -0.0000
  1060        0.0000             nan     0.4469   -0.0000
  1080        0.0000             nan     0.4469   -0.0000
  1100        0.0000             nan     0.4469   -0.0000
  1120        0.0000             nan     0.4469   -0.0000
  1140        0.0000             nan     0.4469   -0.0000
  1160        0.0000             nan     0.4469   -0.0000
  1180        0.0000             nan     0.4469   -0.0000
  1200        0.0000             nan     0.4469   -0.0000
  1220        0.0000             nan     0.4469   -0.0000
  1240        0.0000             nan     0.4469   -0.0000
  1260        0.0000             nan     0.4469   -0.0000
  1280        0.0000             nan     0.4469   -0.0000
  1300        0.0000             nan     0.4469   -0.0000
  1320        0.0000             nan     0.4469    0.0000
  1340        0.0000             nan     0.4469   -0.0000
  1360        0.0000             nan     0.4469    0.0000
  1380        0.0000             nan     0.4469   -0.0000
  1400        0.0000             nan     0.4469   -0.0000
  1420        0.0000             nan     0.4469   -0.0000
  1440        0.0000             nan     0.4469    0.0000
  1460        0.0000             nan     0.4469   -0.0000
  1480        0.0000             nan     0.4469    0.0000
  1500        0.0000             nan     0.4469   -0.0000
  1520        0.0000             nan     0.4469   -0.0000
  1540        0.0000             nan     0.4469    0.0000
  1560        0.0000             nan     0.4469    0.0000
  1580        0.0000             nan     0.4469   -0.0000
  1600        0.0000             nan     0.4469   -0.0000
  1620        0.0000             nan     0.4469    0.0000
  1640        0.0000             nan     0.4469   -0.0000
  1660        0.0000             nan     0.4469    0.0000
  1680        0.0000             nan     0.4469    0.0000
  1700        0.0000             nan     0.4469   -0.0000
  1713        0.0000             nan     0.4469   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1522             nan     0.4473    0.2264
     2        0.0599             nan     0.4473    0.0924
     3        0.0262             nan     0.4473    0.0325
     4        0.0132             nan     0.4473    0.0130
     5        0.0087             nan     0.4473    0.0039
     6        0.0066             nan     0.4473    0.0015
     7        0.0054             nan     0.4473    0.0010
     8        0.0048             nan     0.4473    0.0004
     9        0.0040             nan     0.4473    0.0006
    10        0.0035             nan     0.4473    0.0003
    20        0.0020             nan     0.4473   -0.0001
    40        0.0011             nan     0.4473   -0.0001
    60        0.0007             nan     0.4473   -0.0000
    80        0.0005             nan     0.4473   -0.0000
   100        0.0004             nan     0.4473   -0.0000
   120        0.0003             nan     0.4473   -0.0000
   140        0.0002             nan     0.4473   -0.0000
   160        0.0002             nan     0.4473   -0.0000
   180        0.0001             nan     0.4473   -0.0000
   200        0.0001             nan     0.4473   -0.0000
   220        0.0001             nan     0.4473   -0.0000
   240        0.0001             nan     0.4473   -0.0000
   260        0.0001             nan     0.4473   -0.0000
   280        0.0001             nan     0.4473   -0.0000
   300        0.0000             nan     0.4473   -0.0000
   320        0.0000             nan     0.4473   -0.0000
   340        0.0000             nan     0.4473   -0.0000
   360        0.0000             nan     0.4473   -0.0000
   380        0.0000             nan     0.4473   -0.0000
   400        0.0000             nan     0.4473   -0.0000
   420        0.0000             nan     0.4473   -0.0000
   440        0.0000             nan     0.4473   -0.0000
   460        0.0000             nan     0.4473   -0.0000
   480        0.0000             nan     0.4473   -0.0000
   500        0.0000             nan     0.4473   -0.0000
   520        0.0000             nan     0.4473   -0.0000
   540        0.0000             nan     0.4473   -0.0000
   560        0.0000             nan     0.4473   -0.0000
   580        0.0000             nan     0.4473   -0.0000
   600        0.0000             nan     0.4473   -0.0000
   620        0.0000             nan     0.4473   -0.0000
   640        0.0000             nan     0.4473   -0.0000
   660        0.0000             nan     0.4473   -0.0000
   680        0.0000             nan     0.4473   -0.0000
   700        0.0000             nan     0.4473   -0.0000
   720        0.0000             nan     0.4473   -0.0000
   740        0.0000             nan     0.4473   -0.0000
   760        0.0000             nan     0.4473   -0.0000
   780        0.0000             nan     0.4473   -0.0000
   800        0.0000             nan     0.4473   -0.0000
   820        0.0000             nan     0.4473   -0.0000
   840        0.0000             nan     0.4473   -0.0000
   860        0.0000             nan     0.4473   -0.0000
   880        0.0000             nan     0.4473   -0.0000
   900        0.0000             nan     0.4473   -0.0000
   920        0.0000             nan     0.4473   -0.0000
   940        0.0000             nan     0.4473   -0.0000
   960        0.0000             nan     0.4473   -0.0000
   980        0.0000             nan     0.4473   -0.0000
  1000        0.0000             nan     0.4473   -0.0000
  1020        0.0000             nan     0.4473   -0.0000
  1040        0.0000             nan     0.4473   -0.0000
  1060        0.0000             nan     0.4473   -0.0000
  1080        0.0000             nan     0.4473   -0.0000
  1100        0.0000             nan     0.4473   -0.0000
  1120        0.0000             nan     0.4473   -0.0000
  1140        0.0000             nan     0.4473   -0.0000
  1160        0.0000             nan     0.4473   -0.0000
  1180        0.0000             nan     0.4473   -0.0000
  1200        0.0000             nan     0.4473   -0.0000
  1220        0.0000             nan     0.4473   -0.0000
  1240        0.0000             nan     0.4473   -0.0000
  1260        0.0000             nan     0.4473   -0.0000
  1280        0.0000             nan     0.4473   -0.0000
  1300        0.0000             nan     0.4473   -0.0000
  1320        0.0000             nan     0.4473   -0.0000
  1340        0.0000             nan     0.4473   -0.0000
  1360        0.0000             nan     0.4473   -0.0000
  1380        0.0000             nan     0.4473   -0.0000
  1400        0.0000             nan     0.4473   -0.0000
  1420        0.0000             nan     0.4473   -0.0000
  1440        0.0000             nan     0.4473   -0.0000
  1460        0.0000             nan     0.4473   -0.0000
  1480        0.0000             nan     0.4473   -0.0000
  1500        0.0000             nan     0.4473   -0.0000
  1520        0.0000             nan     0.4473   -0.0000
  1540        0.0000             nan     0.4473   -0.0000
  1560        0.0000             nan     0.4473   -0.0000
  1580        0.0000             nan     0.4473   -0.0000
  1600        0.0000             nan     0.4473   -0.0000
  1620        0.0000             nan     0.4473   -0.0000
  1640        0.0000             nan     0.4473   -0.0000
  1660        0.0000             nan     0.4473   -0.0000
  1680        0.0000             nan     0.4473   -0.0000
  1700        0.0000             nan     0.4473   -0.0000
  1720        0.0000             nan     0.4473   -0.0000
  1740        0.0000             nan     0.4473   -0.0000
  1760        0.0000             nan     0.4473   -0.0000
  1780        0.0000             nan     0.4473   -0.0000
  1800        0.0000             nan     0.4473   -0.0000
  1820        0.0000             nan     0.4473   -0.0000
  1840        0.0000             nan     0.4473   -0.0000
  1860        0.0000             nan     0.4473   -0.0000
  1880        0.0000             nan     0.4473   -0.0000
  1900        0.0000             nan     0.4473   -0.0000
  1920        0.0000             nan     0.4473   -0.0000
  1940        0.0000             nan     0.4473   -0.0000
  1960        0.0000             nan     0.4473   -0.0000
  1980        0.0000             nan     0.4473   -0.0000
  2000        0.0000             nan     0.4473   -0.0000
  2020        0.0000             nan     0.4473   -0.0000
  2040        0.0000             nan     0.4473   -0.0000
  2060        0.0000             nan     0.4473   -0.0000
  2080        0.0000             nan     0.4473   -0.0000
  2100        0.0000             nan     0.4473   -0.0000
  2120        0.0000             nan     0.4473   -0.0000
  2140        0.0000             nan     0.4473   -0.0000
  2160        0.0000             nan     0.4473   -0.0000
  2180        0.0000             nan     0.4473   -0.0000
  2200        0.0000             nan     0.4473   -0.0000
  2220        0.0000             nan     0.4473   -0.0000
  2240        0.0000             nan     0.4473   -0.0000
  2260        0.0000             nan     0.4473   -0.0000
  2280        0.0000             nan     0.4473   -0.0000
  2300        0.0000             nan     0.4473   -0.0000
  2320        0.0000             nan     0.4473   -0.0000
  2340        0.0000             nan     0.4473   -0.0000
  2360        0.0000             nan     0.4473   -0.0000
  2380        0.0000             nan     0.4473   -0.0000
  2400        0.0000             nan     0.4473   -0.0000
  2420        0.0000             nan     0.4473   -0.0000
  2440        0.0000             nan     0.4473   -0.0000
  2460        0.0000             nan     0.4473   -0.0000
  2480        0.0000             nan     0.4473   -0.0000
  2500        0.0000             nan     0.4473   -0.0000
  2520        0.0000             nan     0.4473   -0.0000
  2540        0.0000             nan     0.4473   -0.0000
  2560        0.0000             nan     0.4473   -0.0000
  2580        0.0000             nan     0.4473   -0.0000
  2600        0.0000             nan     0.4473   -0.0000
  2620        0.0000             nan     0.4473   -0.0000
  2640        0.0000             nan     0.4473   -0.0000
  2660        0.0000             nan     0.4473   -0.0000
  2680        0.0000             nan     0.4473   -0.0000
  2700        0.0000             nan     0.4473   -0.0000
  2720        0.0000             nan     0.4473   -0.0000
  2740        0.0000             nan     0.4473   -0.0000
  2760        0.0000             nan     0.4473   -0.0000
  2780        0.0000             nan     0.4473   -0.0000
  2800        0.0000             nan     0.4473   -0.0000
  2820        0.0000             nan     0.4473   -0.0000
  2840        0.0000             nan     0.4473   -0.0000
  2860        0.0000             nan     0.4473   -0.0000
  2880        0.0000             nan     0.4473   -0.0000
  2900        0.0000             nan     0.4473   -0.0000
  2920        0.0000             nan     0.4473   -0.0000
  2940        0.0000             nan     0.4473   -0.0000
  2960        0.0000             nan     0.4473   -0.0000
  2980        0.0000             nan     0.4473   -0.0000
  3000        0.0000             nan     0.4473   -0.0000
  3020        0.0000             nan     0.4473   -0.0000
  3040        0.0000             nan     0.4473   -0.0000
  3060        0.0000             nan     0.4473   -0.0000
  3080        0.0000             nan     0.4473   -0.0000
  3100        0.0000             nan     0.4473   -0.0000
  3120        0.0000             nan     0.4473   -0.0000
  3140        0.0000             nan     0.4473   -0.0000
  3160        0.0000             nan     0.4473   -0.0000
  3180        0.0000             nan     0.4473   -0.0000
  3200        0.0000             nan     0.4473   -0.0000
  3220        0.0000             nan     0.4473   -0.0000
  3240        0.0000             nan     0.4473   -0.0000
  3260        0.0000             nan     0.4473   -0.0000
  3280        0.0000             nan     0.4473   -0.0000
  3300        0.0000             nan     0.4473   -0.0000
  3320        0.0000             nan     0.4473   -0.0000
  3340        0.0000             nan     0.4473   -0.0000
  3360        0.0000             nan     0.4473   -0.0000
  3380        0.0000             nan     0.4473   -0.0000
  3400        0.0000             nan     0.4473   -0.0000
  3420        0.0000             nan     0.4473   -0.0000
  3440        0.0000             nan     0.4473   -0.0000
  3460        0.0000             nan     0.4473   -0.0000
  3480        0.0000             nan     0.4473   -0.0000
  3500        0.0000             nan     0.4473   -0.0000
  3520        0.0000             nan     0.4473   -0.0000
  3540        0.0000             nan     0.4473   -0.0000
  3560        0.0000             nan     0.4473   -0.0000
  3580        0.0000             nan     0.4473   -0.0000
  3600        0.0000             nan     0.4473   -0.0000
  3620        0.0000             nan     0.4473   -0.0000
  3640        0.0000             nan     0.4473   -0.0000
  3660        0.0000             nan     0.4473   -0.0000
  3680        0.0000             nan     0.4473   -0.0000
  3700        0.0000             nan     0.4473   -0.0000
  3720        0.0000             nan     0.4473   -0.0000
  3740        0.0000             nan     0.4473   -0.0000
  3760        0.0000             nan     0.4473   -0.0000
  3780        0.0000             nan     0.4473   -0.0000
  3800        0.0000             nan     0.4473   -0.0000
  3820        0.0000             nan     0.4473   -0.0000
  3840        0.0000             nan     0.4473   -0.0000
  3855        0.0000             nan     0.4473   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1261             nan     0.4875    0.2697
     2        0.0480             nan     0.4875    0.0754
     3        0.0191             nan     0.4875    0.0257
     4        0.0102             nan     0.4875    0.0087
     5        0.0067             nan     0.4875    0.0028
     6        0.0050             nan     0.4875    0.0012
     7        0.0042             nan     0.4875    0.0005
     8        0.0039             nan     0.4875   -0.0001
     9        0.0035             nan     0.4875    0.0002
    10        0.0030             nan     0.4875    0.0003
    20        0.0015             nan     0.4875    0.0000
    40        0.0009             nan     0.4875   -0.0000
    60        0.0006             nan     0.4875   -0.0000
    80        0.0004             nan     0.4875   -0.0000
   100        0.0003             nan     0.4875   -0.0000
   120        0.0002             nan     0.4875   -0.0000
   140        0.0002             nan     0.4875   -0.0000
   160        0.0001             nan     0.4875   -0.0000
   180        0.0001             nan     0.4875   -0.0000
   200        0.0001             nan     0.4875   -0.0000
   220        0.0001             nan     0.4875   -0.0000
   240        0.0001             nan     0.4875   -0.0000
   260        0.0001             nan     0.4875   -0.0000
   280        0.0001             nan     0.4875   -0.0000
   300        0.0001             nan     0.4875   -0.0000
   320        0.0000             nan     0.4875   -0.0000
   340        0.0000             nan     0.4875   -0.0000
   360        0.0000             nan     0.4875   -0.0000
   380        0.0000             nan     0.4875   -0.0000
   400        0.0000             nan     0.4875   -0.0000
   420        0.0000             nan     0.4875   -0.0000
   440        0.0000             nan     0.4875   -0.0000
   460        0.0000             nan     0.4875   -0.0000
   480        0.0000             nan     0.4875   -0.0000
   500        0.0000             nan     0.4875   -0.0000
   520        0.0000             nan     0.4875   -0.0000
   540        0.0000             nan     0.4875    0.0000
   560        0.0000             nan     0.4875   -0.0000
   580        0.0000             nan     0.4875    0.0000
   600        0.0000             nan     0.4875   -0.0000
   620        0.0000             nan     0.4875   -0.0000
   640        0.0000             nan     0.4875   -0.0000
   660        0.0000             nan     0.4875   -0.0000
   680        0.0000             nan     0.4875   -0.0000
   700        0.0000             nan     0.4875   -0.0000
   720        0.0000             nan     0.4875   -0.0000
   740        0.0000             nan     0.4875   -0.0000
   760        0.0000             nan     0.4875   -0.0000
   780        0.0000             nan     0.4875    0.0000
   800        0.0000             nan     0.4875   -0.0000
   820        0.0000             nan     0.4875   -0.0000
   840        0.0000             nan     0.4875   -0.0000
   860        0.0000             nan     0.4875    0.0000
   880        0.0000             nan     0.4875   -0.0000
   900        0.0000             nan     0.4875   -0.0000
   920        0.0000             nan     0.4875   -0.0000
   940        0.0000             nan     0.4875    0.0000
   960        0.0000             nan     0.4875   -0.0000
   980        0.0000             nan     0.4875   -0.0000
  1000        0.0000             nan     0.4875    0.0000
  1020        0.0000             nan     0.4875   -0.0000
  1040        0.0000             nan     0.4875   -0.0000
  1060        0.0000             nan     0.4875   -0.0000
  1080        0.0000             nan     0.4875   -0.0000
  1100        0.0000             nan     0.4875   -0.0000
  1120        0.0000             nan     0.4875   -0.0000
  1140        0.0000             nan     0.4875    0.0000
  1160        0.0000             nan     0.4875   -0.0000
  1180        0.0000             nan     0.4875    0.0000
  1200        0.0000             nan     0.4875    0.0000
  1220        0.0000             nan     0.4875    0.0000
  1240        0.0000             nan     0.4875   -0.0000
  1260        0.0000             nan     0.4875    0.0000
  1280        0.0000             nan     0.4875   -0.0000
  1300        0.0000             nan     0.4875   -0.0000
  1320        0.0000             nan     0.4875   -0.0000
  1340        0.0000             nan     0.4875   -0.0000
  1360        0.0000             nan     0.4875   -0.0000
  1380        0.0000             nan     0.4875   -0.0000
  1400        0.0000             nan     0.4875    0.0000
  1420        0.0000             nan     0.4875   -0.0000
  1440        0.0000             nan     0.4875   -0.0000
  1460        0.0000             nan     0.4875   -0.0000
  1480        0.0000             nan     0.4875   -0.0000
  1500        0.0000             nan     0.4875   -0.0000
  1520        0.0000             nan     0.4875   -0.0000
  1540        0.0000             nan     0.4875   -0.0000
  1560        0.0000             nan     0.4875   -0.0000
  1580        0.0000             nan     0.4875    0.0000
  1600        0.0000             nan     0.4875   -0.0000
  1620        0.0000             nan     0.4875    0.0000
  1640        0.0000             nan     0.4875    0.0000
  1660        0.0000             nan     0.4875   -0.0000
  1680        0.0000             nan     0.4875    0.0000
  1700        0.0000             nan     0.4875   -0.0000
  1720        0.0000             nan     0.4875    0.0000
  1740        0.0000             nan     0.4875   -0.0000
  1760        0.0000             nan     0.4875   -0.0000
  1780        0.0000             nan     0.4875    0.0000
  1800        0.0000             nan     0.4875   -0.0000
  1820        0.0000             nan     0.4875   -0.0000
  1840        0.0000             nan     0.4875    0.0000
  1860        0.0000             nan     0.4875    0.0000
  1880        0.0000             nan     0.4875    0.0000
  1900        0.0000             nan     0.4875   -0.0000
  1920        0.0000             nan     0.4875    0.0000
  1940        0.0000             nan     0.4875   -0.0000
  1960        0.0000             nan     0.4875    0.0000
  1980        0.0000             nan     0.4875   -0.0000
  2000        0.0000             nan     0.4875   -0.0000
  2020        0.0000             nan     0.4875   -0.0000
  2040        0.0000             nan     0.4875   -0.0000
  2060        0.0000             nan     0.4875   -0.0000
  2080        0.0000             nan     0.4875   -0.0000
  2100        0.0000             nan     0.4875   -0.0000
  2120        0.0000             nan     0.4875   -0.0000
  2140        0.0000             nan     0.4875   -0.0000
  2160        0.0000             nan     0.4875   -0.0000
  2180        0.0000             nan     0.4875   -0.0000
  2200        0.0000             nan     0.4875    0.0000
  2220        0.0000             nan     0.4875   -0.0000
  2240        0.0000             nan     0.4875   -0.0000
  2260        0.0000             nan     0.4875   -0.0000
  2280        0.0000             nan     0.4875   -0.0000
  2300        0.0000             nan     0.4875   -0.0000
  2320        0.0000             nan     0.4875   -0.0000
  2340        0.0000             nan     0.4875   -0.0000
  2360        0.0000             nan     0.4875   -0.0000
  2380        0.0000             nan     0.4875   -0.0000
  2400        0.0000             nan     0.4875   -0.0000
  2420        0.0000             nan     0.4875   -0.0000
  2440        0.0000             nan     0.4875   -0.0000
  2460        0.0000             nan     0.4875    0.0000
  2480        0.0000             nan     0.4875    0.0000
  2500        0.0000             nan     0.4875    0.0000
  2520        0.0000             nan     0.4875   -0.0000
  2540        0.0000             nan     0.4875   -0.0000
  2560        0.0000             nan     0.4875   -0.0000
  2580        0.0000             nan     0.4875   -0.0000
  2600        0.0000             nan     0.4875   -0.0000
  2620        0.0000             nan     0.4875   -0.0000
  2640        0.0000             nan     0.4875   -0.0000
  2660        0.0000             nan     0.4875   -0.0000
  2680        0.0000             nan     0.4875   -0.0000
  2700        0.0000             nan     0.4875    0.0000
  2720        0.0000             nan     0.4875   -0.0000
  2740        0.0000             nan     0.4875   -0.0000
  2760        0.0000             nan     0.4875   -0.0000
  2780        0.0000             nan     0.4875   -0.0000
  2800        0.0000             nan     0.4875   -0.0000
  2820        0.0000             nan     0.4875   -0.0000
  2840        0.0000             nan     0.4875   -0.0000
  2860        0.0000             nan     0.4875   -0.0000
  2880        0.0000             nan     0.4875   -0.0000
  2900        0.0000             nan     0.4875   -0.0000
  2920        0.0000             nan     0.4875   -0.0000
  2940        0.0000             nan     0.4875   -0.0000
  2960        0.0000             nan     0.4875   -0.0000
  2980        0.0000             nan     0.4875   -0.0000
  3000        0.0000             nan     0.4875   -0.0000
  3020        0.0000             nan     0.4875    0.0000
  3040        0.0000             nan     0.4875   -0.0000
  3060        0.0000             nan     0.4875   -0.0000
  3080        0.0000             nan     0.4875   -0.0000
  3100        0.0000             nan     0.4875    0.0000
  3120        0.0000             nan     0.4875    0.0000
  3140        0.0000             nan     0.4875   -0.0000
  3160        0.0000             nan     0.4875    0.0000
  3180        0.0000             nan     0.4875   -0.0000
  3200        0.0000             nan     0.4875   -0.0000
  3220        0.0000             nan     0.4875   -0.0000
  3240        0.0000             nan     0.4875   -0.0000
  3260        0.0000             nan     0.4875    0.0000
  3280        0.0000             nan     0.4875   -0.0000
  3300        0.0000             nan     0.4875   -0.0000
  3320        0.0000             nan     0.4875   -0.0000
  3340        0.0000             nan     0.4875   -0.0000
  3360        0.0000             nan     0.4875   -0.0000
  3380        0.0000             nan     0.4875   -0.0000
  3400        0.0000             nan     0.4875    0.0000
  3420        0.0000             nan     0.4875   -0.0000
  3440        0.0000             nan     0.4875   -0.0000
  3460        0.0000             nan     0.4875    0.0000
  3480        0.0000             nan     0.4875    0.0000
  3500        0.0000             nan     0.4875   -0.0000
  3520        0.0000             nan     0.4875   -0.0000
  3540        0.0000             nan     0.4875   -0.0000
  3560        0.0000             nan     0.4875   -0.0000
  3580        0.0000             nan     0.4875   -0.0000
  3600        0.0000             nan     0.4875   -0.0000
  3620        0.0000             nan     0.4875   -0.0000
  3640        0.0000             nan     0.4875   -0.0000
  3660        0.0000             nan     0.4875   -0.0000
  3680        0.0000             nan     0.4875   -0.0000
  3700        0.0000             nan     0.4875   -0.0000
  3720        0.0000             nan     0.4875   -0.0000
  3740        0.0000             nan     0.4875    0.0000
  3760        0.0000             nan     0.4875    0.0000
  3780        0.0000             nan     0.4875    0.0000
  3800        0.0000             nan     0.4875   -0.0000
  3820        0.0000             nan     0.4875   -0.0000
  3840        0.0000             nan     0.4875   -0.0000
  3860        0.0000             nan     0.4875   -0.0000
  3880        0.0000             nan     0.4875    0.0000
  3900        0.0000             nan     0.4875   -0.0000
  3920        0.0000             nan     0.4875    0.0000
  3940        0.0000             nan     0.4875    0.0000
  3960        0.0000             nan     0.4875   -0.0000
  3980        0.0000             nan     0.4875   -0.0000
  4000        0.0000             nan     0.4875   -0.0000
  4020        0.0000             nan     0.4875    0.0000
  4040        0.0000             nan     0.4875   -0.0000
  4060        0.0000             nan     0.4875   -0.0000
  4080        0.0000             nan     0.4875   -0.0000
  4100        0.0000             nan     0.4875   -0.0000
  4120        0.0000             nan     0.4875   -0.0000
  4140        0.0000             nan     0.4875    0.0000
  4160        0.0000             nan     0.4875   -0.0000
  4180        0.0000             nan     0.4875   -0.0000
  4200        0.0000             nan     0.4875    0.0000
  4220        0.0000             nan     0.4875   -0.0000
  4240        0.0000             nan     0.4875   -0.0000
  4260        0.0000             nan     0.4875    0.0000
  4280        0.0000             nan     0.4875   -0.0000
  4300        0.0000             nan     0.4875   -0.0000
  4320        0.0000             nan     0.4875   -0.0000
  4340        0.0000             nan     0.4875   -0.0000
  4360        0.0000             nan     0.4875   -0.0000
  4380        0.0000             nan     0.4875   -0.0000
  4400        0.0000             nan     0.4875    0.0000
  4416        0.0000             nan     0.4875   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1047             nan     0.5240    0.2778
     2        0.0291             nan     0.5240    0.0748
     3        0.0096             nan     0.5240    0.0174
     4        0.0044             nan     0.5240    0.0042
     5        0.0029             nan     0.5240    0.0011
     6        0.0025             nan     0.5240    0.0003
     7        0.0022             nan     0.5240    0.0001
     8        0.0021             nan     0.5240    0.0000
     9        0.0019             nan     0.5240   -0.0002
    10        0.0018             nan     0.5240    0.0000
    20        0.0011             nan     0.5240   -0.0001
    40        0.0006             nan     0.5240   -0.0001
    60        0.0003             nan     0.5240   -0.0000
    80        0.0002             nan     0.5240   -0.0000
   100        0.0002             nan     0.5240   -0.0000
   120        0.0001             nan     0.5240   -0.0000
   140        0.0001             nan     0.5240   -0.0000
   160        0.0001             nan     0.5240   -0.0000
   180        0.0001             nan     0.5240   -0.0000
   200        0.0001             nan     0.5240   -0.0000
   220        0.0001             nan     0.5240   -0.0000
   240        0.0001             nan     0.5240   -0.0000
   260        0.0001             nan     0.5240   -0.0000
   280        0.0000             nan     0.5240   -0.0000
   300        0.0000             nan     0.5240   -0.0000
   320        0.0000             nan     0.5240   -0.0000
   340        0.0000             nan     0.5240   -0.0000
   360        0.0000             nan     0.5240   -0.0000
   380        0.0000             nan     0.5240   -0.0000
   400        0.0000             nan     0.5240   -0.0000
   420        0.0000             nan     0.5240   -0.0000
   440        0.0000             nan     0.5240    0.0000
   460        0.0000             nan     0.5240   -0.0000
   480        0.0000             nan     0.5240   -0.0000
   500        0.0000             nan     0.5240   -0.0000
   520        0.0000             nan     0.5240   -0.0000
   540        0.0000             nan     0.5240   -0.0000
   560        0.0000             nan     0.5240   -0.0000
   580        0.0000             nan     0.5240    0.0000
   600        0.0000             nan     0.5240   -0.0000
   620        0.0000             nan     0.5240    0.0000
   640        0.0000             nan     0.5240    0.0000
   660        0.0000             nan     0.5240   -0.0000
   680        0.0000             nan     0.5240   -0.0000
   700        0.0000             nan     0.5240   -0.0000
   720        0.0000             nan     0.5240    0.0000
   740        0.0000             nan     0.5240   -0.0000
   760        0.0000             nan     0.5240   -0.0000
   780        0.0000             nan     0.5240   -0.0000
   800        0.0000             nan     0.5240   -0.0000
   820        0.0000             nan     0.5240   -0.0000
   840        0.0000             nan     0.5240   -0.0000
   860        0.0000             nan     0.5240    0.0000
   880        0.0000             nan     0.5240   -0.0000
   900        0.0000             nan     0.5240   -0.0000
   920        0.0000             nan     0.5240   -0.0000
   940        0.0000             nan     0.5240   -0.0000
   960        0.0000             nan     0.5240    0.0000
   980        0.0000             nan     0.5240   -0.0000
  1000        0.0000             nan     0.5240   -0.0000
  1020        0.0000             nan     0.5240   -0.0000
  1040        0.0000             nan     0.5240   -0.0000
  1060        0.0000             nan     0.5240   -0.0000
  1080        0.0000             nan     0.5240   -0.0000
  1100        0.0000             nan     0.5240   -0.0000
  1120        0.0000             nan     0.5240   -0.0000
  1140        0.0000             nan     0.5240   -0.0000
  1160        0.0000             nan     0.5240    0.0000
  1180        0.0000             nan     0.5240   -0.0000
  1200        0.0000             nan     0.5240   -0.0000
  1220        0.0000             nan     0.5240    0.0000
  1240        0.0000             nan     0.5240   -0.0000
  1260        0.0000             nan     0.5240   -0.0000
  1280        0.0000             nan     0.5240   -0.0000
  1300        0.0000             nan     0.5240    0.0000
  1320        0.0000             nan     0.5240   -0.0000
  1340        0.0000             nan     0.5240   -0.0000
  1360        0.0000             nan     0.5240   -0.0000
  1380        0.0000             nan     0.5240   -0.0000
  1400        0.0000             nan     0.5240   -0.0000
  1420        0.0000             nan     0.5240   -0.0000
  1440        0.0000             nan     0.5240   -0.0000
  1460        0.0000             nan     0.5240   -0.0000
  1480        0.0000             nan     0.5240   -0.0000
  1500        0.0000             nan     0.5240   -0.0000
  1520        0.0000             nan     0.5240   -0.0000
  1540        0.0000             nan     0.5240   -0.0000
  1560        0.0000             nan     0.5240   -0.0000
  1580        0.0000             nan     0.5240   -0.0000
  1600        0.0000             nan     0.5240   -0.0000
  1620        0.0000             nan     0.5240   -0.0000
  1640        0.0000             nan     0.5240   -0.0000
  1660        0.0000             nan     0.5240    0.0000
  1680        0.0000             nan     0.5240   -0.0000
  1700        0.0000             nan     0.5240    0.0000
  1720        0.0000             nan     0.5240   -0.0000
  1740        0.0000             nan     0.5240   -0.0000
  1760        0.0000             nan     0.5240    0.0000
  1780        0.0000             nan     0.5240   -0.0000
  1800        0.0000             nan     0.5240   -0.0000
  1820        0.0000             nan     0.5240    0.0000
  1840        0.0000             nan     0.5240   -0.0000
  1860        0.0000             nan     0.5240   -0.0000
  1880        0.0000             nan     0.5240   -0.0000
  1900        0.0000             nan     0.5240    0.0000
  1920        0.0000             nan     0.5240   -0.0000
  1940        0.0000             nan     0.5240    0.0000
  1960        0.0000             nan     0.5240   -0.0000
  1980        0.0000             nan     0.5240    0.0000
  2000        0.0000             nan     0.5240   -0.0000
  2020        0.0000             nan     0.5240   -0.0000
  2040        0.0000             nan     0.5240    0.0000
  2060        0.0000             nan     0.5240    0.0000
  2080        0.0000             nan     0.5240   -0.0000
  2100        0.0000             nan     0.5240    0.0000
  2120        0.0000             nan     0.5240   -0.0000
  2140        0.0000             nan     0.5240   -0.0000
  2160        0.0000             nan     0.5240   -0.0000
  2180        0.0000             nan     0.5240   -0.0000
  2200        0.0000             nan     0.5240   -0.0000
  2220        0.0000             nan     0.5240   -0.0000
  2240        0.0000             nan     0.5240   -0.0000
  2260        0.0000             nan     0.5240   -0.0000
  2280        0.0000             nan     0.5240    0.0000
  2300        0.0000             nan     0.5240    0.0000
  2320        0.0000             nan     0.5240   -0.0000
  2340        0.0000             nan     0.5240   -0.0000
  2360        0.0000             nan     0.5240   -0.0000
  2380        0.0000             nan     0.5240    0.0000
  2400        0.0000             nan     0.5240   -0.0000
  2420        0.0000             nan     0.5240   -0.0000
  2440        0.0000             nan     0.5240    0.0000
  2460        0.0000             nan     0.5240    0.0000
  2480        0.0000             nan     0.5240   -0.0000
  2500        0.0000             nan     0.5240   -0.0000
  2520        0.0000             nan     0.5240   -0.0000
  2540        0.0000             nan     0.5240   -0.0000
  2560        0.0000             nan     0.5240   -0.0000
  2580        0.0000             nan     0.5240    0.0000
  2600        0.0000             nan     0.5240    0.0000
  2620        0.0000             nan     0.5240   -0.0000
  2640        0.0000             nan     0.5240    0.0000
  2660        0.0000             nan     0.5240   -0.0000
  2680        0.0000             nan     0.5240   -0.0000
  2700        0.0000             nan     0.5240   -0.0000
  2720        0.0000             nan     0.5240   -0.0000
  2740        0.0000             nan     0.5240   -0.0000
  2760        0.0000             nan     0.5240   -0.0000
  2780        0.0000             nan     0.5240   -0.0000
  2800        0.0000             nan     0.5240   -0.0000
  2820        0.0000             nan     0.5240   -0.0000
  2840        0.0000             nan     0.5240   -0.0000
  2860        0.0000             nan     0.5240   -0.0000
  2880        0.0000             nan     0.5240   -0.0000
  2900        0.0000             nan     0.5240   -0.0000
  2920        0.0000             nan     0.5240   -0.0000
  2940        0.0000             nan     0.5240    0.0000
  2960        0.0000             nan     0.5240   -0.0000
  2980        0.0000             nan     0.5240   -0.0000
  3000        0.0000             nan     0.5240   -0.0000
  3020        0.0000             nan     0.5240   -0.0000
  3040        0.0000             nan     0.5240   -0.0000
  3060        0.0000             nan     0.5240   -0.0000
  3080        0.0000             nan     0.5240   -0.0000
  3100        0.0000             nan     0.5240    0.0000
  3120        0.0000             nan     0.5240   -0.0000
  3140        0.0000             nan     0.5240   -0.0000
  3160        0.0000             nan     0.5240   -0.0000
  3180        0.0000             nan     0.5240   -0.0000
  3200        0.0000             nan     0.5240   -0.0000
  3220        0.0000             nan     0.5240   -0.0000
  3240        0.0000             nan     0.5240    0.0000
  3260        0.0000             nan     0.5240   -0.0000
  3280        0.0000             nan     0.5240    0.0000
  3300        0.0000             nan     0.5240   -0.0000
  3320        0.0000             nan     0.5240   -0.0000
  3340        0.0000             nan     0.5240   -0.0000
  3360        0.0000             nan     0.5240   -0.0000
  3380        0.0000             nan     0.5240   -0.0000
  3400        0.0000             nan     0.5240   -0.0000
  3420        0.0000             nan     0.5240   -0.0000
  3440        0.0000             nan     0.5240   -0.0000
  3460        0.0000             nan     0.5240   -0.0000
  3480        0.0000             nan     0.5240   -0.0000
  3500        0.0000             nan     0.5240   -0.0000
  3520        0.0000             nan     0.5240    0.0000
  3540        0.0000             nan     0.5240    0.0000
  3560        0.0000             nan     0.5240   -0.0000
  3580        0.0000             nan     0.5240   -0.0000
  3600        0.0000             nan     0.5240   -0.0000
  3620        0.0000             nan     0.5240   -0.0000
  3640        0.0000             nan     0.5240    0.0000
  3660        0.0000             nan     0.5240    0.0000
  3680        0.0000             nan     0.5240   -0.0000
  3700        0.0000             nan     0.5240   -0.0000
  3720        0.0000             nan     0.5240   -0.0000
  3740        0.0000             nan     0.5240   -0.0000
  3760        0.0000             nan     0.5240   -0.0000
  3780        0.0000             nan     0.5240    0.0000
  3800        0.0000             nan     0.5240    0.0000
  3820        0.0000             nan     0.5240   -0.0000
  3840        0.0000             nan     0.5240   -0.0000
  3860        0.0000             nan     0.5240   -0.0000
  3880        0.0000             nan     0.5240   -0.0000
  3900        0.0000             nan     0.5240   -0.0000
  3920        0.0000             nan     0.5240   -0.0000
  3940        0.0000             nan     0.5240   -0.0000
  3960        0.0000             nan     0.5240   -0.0000
  3980        0.0000             nan     0.5240   -0.0000
  4000        0.0000             nan     0.5240   -0.0000
  4020        0.0000             nan     0.5240    0.0000
  4040        0.0000             nan     0.5240   -0.0000
  4060        0.0000             nan     0.5240   -0.0000
  4080        0.0000             nan     0.5240   -0.0000
  4100        0.0000             nan     0.5240   -0.0000
  4120        0.0000             nan     0.5240   -0.0000
  4140        0.0000             nan     0.5240   -0.0000
  4160        0.0000             nan     0.5240   -0.0000
  4180        0.0000             nan     0.5240   -0.0000
  4200        0.0000             nan     0.5240   -0.0000
  4216        0.0000             nan     0.5240   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1162             nan     0.5299    0.2926
     2        0.0429             nan     0.5299    0.0766
     3        0.0202             nan     0.5299    0.0207
     4        0.0127             nan     0.5299    0.0069
     5        0.0101             nan     0.5299    0.0025
     6        0.0090             nan     0.5299    0.0007
     7        0.0075             nan     0.5299    0.0015
     8        0.0067             nan     0.5299    0.0007
     9        0.0061             nan     0.5299    0.0002
    10        0.0057             nan     0.5299    0.0001
    20        0.0041             nan     0.5299   -0.0001
    40        0.0028             nan     0.5299   -0.0001
    60        0.0021             nan     0.5299   -0.0001
    80        0.0017             nan     0.5299   -0.0000
   100        0.0014             nan     0.5299   -0.0001
   120        0.0012             nan     0.5299   -0.0000
   140        0.0011             nan     0.5299   -0.0001
   160        0.0009             nan     0.5299   -0.0000
   180        0.0008             nan     0.5299   -0.0000
   200        0.0008             nan     0.5299   -0.0001
   220        0.0007             nan     0.5299   -0.0000
   240        0.0006             nan     0.5299   -0.0000
   260        0.0006             nan     0.5299   -0.0000
   280        0.0005             nan     0.5299   -0.0000
   300        0.0005             nan     0.5299   -0.0000
   320        0.0004             nan     0.5299   -0.0000
   340        0.0004             nan     0.5299   -0.0000
   360        0.0004             nan     0.5299   -0.0000
   380        0.0004             nan     0.5299   -0.0000
   400        0.0003             nan     0.5299   -0.0000
   420        0.0003             nan     0.5299   -0.0000
   440        0.0003             nan     0.5299   -0.0000
   460        0.0003             nan     0.5299   -0.0000
   480        0.0003             nan     0.5299   -0.0000
   500        0.0003             nan     0.5299   -0.0000
   520        0.0002             nan     0.5299   -0.0000
   540        0.0002             nan     0.5299   -0.0000
   560        0.0002             nan     0.5299   -0.0000
   580        0.0002             nan     0.5299   -0.0000
   600        0.0002             nan     0.5299   -0.0000
   620        0.0002             nan     0.5299   -0.0000
   640        0.0002             nan     0.5299   -0.0000
   660        0.0002             nan     0.5299   -0.0000
   680        0.0002             nan     0.5299   -0.0000
   700        0.0002             nan     0.5299   -0.0000
   720        0.0002             nan     0.5299   -0.0000
   740        0.0002             nan     0.5299   -0.0000
   760        0.0002             nan     0.5299   -0.0000
   780        0.0001             nan     0.5299   -0.0000
   800        0.0001             nan     0.5299   -0.0000
   820        0.0001             nan     0.5299   -0.0000
   840        0.0001             nan     0.5299   -0.0000
   860        0.0001             nan     0.5299   -0.0000
   880        0.0001             nan     0.5299    0.0000
   900        0.0001             nan     0.5299   -0.0000
   920        0.0001             nan     0.5299   -0.0000
   940        0.0001             nan     0.5299   -0.0000
   960        0.0001             nan     0.5299   -0.0000
   980        0.0001             nan     0.5299   -0.0000
  1000        0.0001             nan     0.5299    0.0000
  1020        0.0001             nan     0.5299   -0.0000
  1040        0.0001             nan     0.5299   -0.0000
  1060        0.0001             nan     0.5299   -0.0000
  1080        0.0001             nan     0.5299   -0.0000
  1100        0.0001             nan     0.5299   -0.0000
  1120        0.0001             nan     0.5299   -0.0000
  1140        0.0001             nan     0.5299   -0.0000
  1160        0.0001             nan     0.5299   -0.0000
  1180        0.0001             nan     0.5299   -0.0000
  1200        0.0001             nan     0.5299   -0.0000
  1220        0.0001             nan     0.5299   -0.0000
  1240        0.0001             nan     0.5299   -0.0000
  1260        0.0001             nan     0.5299   -0.0000
  1280        0.0001             nan     0.5299   -0.0000
  1300        0.0001             nan     0.5299   -0.0000
  1320        0.0001             nan     0.5299   -0.0000
  1340        0.0001             nan     0.5299   -0.0000
  1360        0.0001             nan     0.5299   -0.0000
  1380        0.0001             nan     0.5299   -0.0000
  1400        0.0001             nan     0.5299   -0.0000
  1420        0.0001             nan     0.5299   -0.0000
  1440        0.0001             nan     0.5299   -0.0000
  1460        0.0001             nan     0.5299    0.0000
  1480        0.0001             nan     0.5299   -0.0000
  1500        0.0001             nan     0.5299   -0.0000
  1520        0.0001             nan     0.5299   -0.0000
  1540        0.0001             nan     0.5299   -0.0000
  1560        0.0001             nan     0.5299   -0.0000
  1580        0.0001             nan     0.5299    0.0000
  1600        0.0001             nan     0.5299   -0.0000
  1620        0.0001             nan     0.5299   -0.0000
  1640        0.0001             nan     0.5299   -0.0000
  1660        0.0001             nan     0.5299   -0.0000
  1680        0.0001             nan     0.5299   -0.0000
  1700        0.0001             nan     0.5299   -0.0000
  1720        0.0001             nan     0.5299   -0.0000
  1740        0.0001             nan     0.5299   -0.0000
  1760        0.0001             nan     0.5299    0.0000
  1780        0.0001             nan     0.5299   -0.0000
  1800        0.0001             nan     0.5299   -0.0000
  1820        0.0001             nan     0.5299   -0.0000
  1840        0.0001             nan     0.5299    0.0000
  1860        0.0001             nan     0.5299   -0.0000
  1880        0.0001             nan     0.5299   -0.0000
  1900        0.0001             nan     0.5299   -0.0000
  1920        0.0001             nan     0.5299    0.0000
  1940        0.0001             nan     0.5299    0.0000
  1960        0.0001             nan     0.5299   -0.0000
  1980        0.0001             nan     0.5299   -0.0000
  2000        0.0001             nan     0.5299   -0.0000
  2020        0.0001             nan     0.5299   -0.0000
  2040        0.0001             nan     0.5299   -0.0000
  2060        0.0001             nan     0.5299   -0.0000
  2080        0.0001             nan     0.5299   -0.0000
  2100        0.0001             nan     0.5299   -0.0000
  2120        0.0001             nan     0.5299   -0.0000
  2140        0.0001             nan     0.5299   -0.0000
  2160        0.0001             nan     0.5299   -0.0000
  2180        0.0001             nan     0.5299   -0.0000
  2200        0.0001             nan     0.5299   -0.0000
  2220        0.0001             nan     0.5299    0.0000
  2240        0.0001             nan     0.5299   -0.0000
  2260        0.0001             nan     0.5299   -0.0000
  2280        0.0001             nan     0.5299    0.0000
  2300        0.0001             nan     0.5299   -0.0000
  2320        0.0001             nan     0.5299   -0.0000
  2340        0.0001             nan     0.5299    0.0000
  2360        0.0001             nan     0.5299   -0.0000
  2380        0.0001             nan     0.5299   -0.0000
  2400        0.0001             nan     0.5299   -0.0000
  2420        0.0001             nan     0.5299   -0.0000
  2440        0.0001             nan     0.5299   -0.0000
  2460        0.0001             nan     0.5299    0.0000
  2480        0.0001             nan     0.5299   -0.0000
  2500        0.0001             nan     0.5299   -0.0000
  2520        0.0001             nan     0.5299   -0.0000
  2540        0.0001             nan     0.5299   -0.0000
  2560        0.0001             nan     0.5299   -0.0000
  2580        0.0001             nan     0.5299   -0.0000
  2600        0.0001             nan     0.5299   -0.0000
  2620        0.0001             nan     0.5299   -0.0000
  2640        0.0001             nan     0.5299   -0.0000
  2660        0.0001             nan     0.5299   -0.0000
  2680        0.0001             nan     0.5299   -0.0000
  2700        0.0001             nan     0.5299   -0.0000
  2720        0.0001             nan     0.5299   -0.0000
  2740        0.0001             nan     0.5299    0.0000
  2760        0.0001             nan     0.5299   -0.0000
  2780        0.0001             nan     0.5299   -0.0000
  2800        0.0001             nan     0.5299   -0.0000
  2820        0.0001             nan     0.5299   -0.0000
  2840        0.0001             nan     0.5299   -0.0000
  2860        0.0001             nan     0.5299   -0.0000
  2880        0.0001             nan     0.5299   -0.0000
  2900        0.0001             nan     0.5299   -0.0000
  2920        0.0001             nan     0.5299   -0.0000
  2940        0.0001             nan     0.5299   -0.0000
  2960        0.0001             nan     0.5299   -0.0000
  2980        0.0001             nan     0.5299   -0.0000
  3000        0.0001             nan     0.5299   -0.0000
  3020        0.0001             nan     0.5299   -0.0000
  3040        0.0001             nan     0.5299   -0.0000
  3060        0.0001             nan     0.5299   -0.0000
  3080        0.0001             nan     0.5299   -0.0000
  3100        0.0001             nan     0.5299    0.0000
  3120        0.0001             nan     0.5299   -0.0000
  3140        0.0001             nan     0.5299   -0.0000
  3160        0.0001             nan     0.5299   -0.0000
  3180        0.0001             nan     0.5299   -0.0000
  3200        0.0001             nan     0.5299   -0.0000
  3220        0.0001             nan     0.5299   -0.0000
  3240        0.0001             nan     0.5299    0.0000
  3260        0.0001             nan     0.5299   -0.0000
  3280        0.0001             nan     0.5299   -0.0000
  3300        0.0001             nan     0.5299   -0.0000
  3320        0.0001             nan     0.5299    0.0000
  3340        0.0001             nan     0.5299   -0.0000
  3360        0.0001             nan     0.5299   -0.0000
  3380        0.0001             nan     0.5299    0.0000
  3400        0.0001             nan     0.5299   -0.0000
  3420        0.0001             nan     0.5299   -0.0000
  3440        0.0001             nan     0.5299    0.0000
  3460        0.0001             nan     0.5299   -0.0000
  3480        0.0001             nan     0.5299   -0.0000
  3500        0.0001             nan     0.5299    0.0000
  3520        0.0001             nan     0.5299   -0.0000
  3540        0.0001             nan     0.5299   -0.0000
  3560        0.0001             nan     0.5299    0.0000
  3580        0.0001             nan     0.5299   -0.0000
  3600        0.0001             nan     0.5299    0.0000
  3620        0.0001             nan     0.5299   -0.0000
  3640        0.0001             nan     0.5299    0.0000
  3660        0.0001             nan     0.5299   -0.0000
  3680        0.0001             nan     0.5299   -0.0000
  3700        0.0001             nan     0.5299   -0.0000
  3720        0.0001             nan     0.5299   -0.0000
  3740        0.0001             nan     0.5299   -0.0000
  3760        0.0001             nan     0.5299   -0.0000
  3780        0.0001             nan     0.5299   -0.0000
  3800        0.0001             nan     0.5299   -0.0000
  3820        0.0001             nan     0.5299    0.0000
  3840        0.0001             nan     0.5299    0.0000
  3860        0.0001             nan     0.5299   -0.0000
  3880        0.0001             nan     0.5299   -0.0000
  3900        0.0001             nan     0.5299   -0.0000
  3920        0.0001             nan     0.5299    0.0000
  3940        0.0001             nan     0.5299   -0.0000
  3960        0.0001             nan     0.5299    0.0000
  3980        0.0001             nan     0.5299   -0.0000
  4000        0.0001             nan     0.5299    0.0000
  4020        0.0001             nan     0.5299   -0.0000
  4040        0.0001             nan     0.5299   -0.0000
  4060        0.0001             nan     0.5299   -0.0000
  4080        0.0001             nan     0.5299   -0.0000
  4100        0.0001             nan     0.5299   -0.0000
  4112        0.0001             nan     0.5299    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0984             nan     0.5649    0.3189
     2        0.0310             nan     0.5649    0.0650
     3        0.0130             nan     0.5649    0.0188
     4        0.0082             nan     0.5649    0.0039
     5        0.0065             nan     0.5649    0.0013
     6        0.0059             nan     0.5649    0.0004
     7        0.0050             nan     0.5649    0.0007
     8        0.0048             nan     0.5649   -0.0000
     9        0.0043             nan     0.5649    0.0001
    10        0.0042             nan     0.5649   -0.0000
    20        0.0027             nan     0.5649   -0.0001
    40        0.0016             nan     0.5649   -0.0002
    60        0.0012             nan     0.5649   -0.0001
    80        0.0010             nan     0.5649   -0.0000
   100        0.0008             nan     0.5649   -0.0000
   120        0.0006             nan     0.5649   -0.0000
   140        0.0005             nan     0.5649   -0.0001
   160        0.0005             nan     0.5649   -0.0000
   180        0.0004             nan     0.5649   -0.0000
   200        0.0003             nan     0.5649   -0.0000
   220        0.0003             nan     0.5649   -0.0000
   240        0.0003             nan     0.5649   -0.0000
   260        0.0002             nan     0.5649   -0.0000
   280        0.0002             nan     0.5649   -0.0000
   300        0.0002             nan     0.5649   -0.0000
   320        0.0002             nan     0.5649   -0.0000
   340        0.0002             nan     0.5649   -0.0000
   360        0.0001             nan     0.5649    0.0000
   380        0.0001             nan     0.5649   -0.0000
   400        0.0001             nan     0.5649   -0.0000
   420        0.0001             nan     0.5649   -0.0000
   440        0.0001             nan     0.5649   -0.0000
   460        0.0001             nan     0.5649   -0.0000
   480        0.0001             nan     0.5649   -0.0000
   500        0.0001             nan     0.5649   -0.0000
   520        0.0001             nan     0.5649   -0.0000
   540        0.0001             nan     0.5649   -0.0000
   560        0.0001             nan     0.5649   -0.0000
   580        0.0001             nan     0.5649   -0.0000
   600        0.0001             nan     0.5649   -0.0000
   620        0.0001             nan     0.5649   -0.0000
   640        0.0001             nan     0.5649   -0.0000
   660        0.0001             nan     0.5649   -0.0000
   680        0.0001             nan     0.5649   -0.0000
   700        0.0001             nan     0.5649   -0.0000
   720        0.0001             nan     0.5649   -0.0000
   740        0.0001             nan     0.5649   -0.0000
   760        0.0001             nan     0.5649   -0.0000
   780        0.0001             nan     0.5649   -0.0000
   800        0.0001             nan     0.5649   -0.0000
   820        0.0001             nan     0.5649   -0.0000
   840        0.0001             nan     0.5649   -0.0000
   860        0.0001             nan     0.5649   -0.0000
   880        0.0001             nan     0.5649   -0.0000
   900        0.0001             nan     0.5649   -0.0000
   920        0.0001             nan     0.5649   -0.0000
   940        0.0001             nan     0.5649   -0.0000
   960        0.0001             nan     0.5649   -0.0000
   980        0.0001             nan     0.5649   -0.0000
  1000        0.0001             nan     0.5649   -0.0000
  1020        0.0001             nan     0.5649   -0.0000
  1040        0.0001             nan     0.5649   -0.0000
  1060        0.0001             nan     0.5649   -0.0000
  1080        0.0001             nan     0.5649   -0.0000
  1100        0.0001             nan     0.5649   -0.0000
  1120        0.0001             nan     0.5649   -0.0000
  1140        0.0001             nan     0.5649   -0.0000
  1160        0.0001             nan     0.5649   -0.0000
  1180        0.0001             nan     0.5649   -0.0000
  1200        0.0001             nan     0.5649   -0.0000
  1220        0.0001             nan     0.5649   -0.0000
  1240        0.0001             nan     0.5649   -0.0000
  1260        0.0001             nan     0.5649   -0.0000
  1280        0.0001             nan     0.5649   -0.0000
  1300        0.0001             nan     0.5649   -0.0000
  1320        0.0001             nan     0.5649    0.0000
  1340        0.0001             nan     0.5649   -0.0000
  1360        0.0001             nan     0.5649    0.0000
  1380        0.0001             nan     0.5649   -0.0000
  1400        0.0001             nan     0.5649   -0.0000
  1420        0.0001             nan     0.5649   -0.0000
  1440        0.0001             nan     0.5649    0.0000
  1460        0.0001             nan     0.5649    0.0000
  1480        0.0001             nan     0.5649   -0.0000
  1500        0.0001             nan     0.5649   -0.0000
  1520        0.0001             nan     0.5649   -0.0000
  1540        0.0001             nan     0.5649   -0.0000
  1560        0.0001             nan     0.5649    0.0000
  1580        0.0001             nan     0.5649   -0.0000
  1600        0.0001             nan     0.5649   -0.0000
  1620        0.0001             nan     0.5649   -0.0000
  1640        0.0001             nan     0.5649    0.0000
  1660        0.0001             nan     0.5649   -0.0000
  1680        0.0001             nan     0.5649   -0.0000
  1700        0.0001             nan     0.5649   -0.0000
  1719        0.0001             nan     0.5649    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1087             nan     0.5824    0.3290
     2        0.0407             nan     0.5824    0.0611
     3        0.0186             nan     0.5824    0.0202
     4        0.0139             nan     0.5824    0.0045
     5        0.0115             nan     0.5824    0.0016
     6        0.0101             nan     0.5824    0.0013
     7        0.0093             nan     0.5824    0.0003
     8        0.0081             nan     0.5824    0.0008
     9        0.0075             nan     0.5824    0.0002
    10        0.0067             nan     0.5824    0.0004
    20        0.0038             nan     0.5824    0.0000
    40        0.0019             nan     0.5824   -0.0002
    60        0.0014             nan     0.5824   -0.0001
    80        0.0011             nan     0.5824   -0.0000
   100        0.0009             nan     0.5824   -0.0000
   120        0.0007             nan     0.5824   -0.0000
   140        0.0006             nan     0.5824   -0.0000
   160        0.0005             nan     0.5824   -0.0000
   180        0.0005             nan     0.5824   -0.0000
   200        0.0004             nan     0.5824   -0.0000
   220        0.0004             nan     0.5824   -0.0000
   240        0.0003             nan     0.5824   -0.0000
   260        0.0003             nan     0.5824   -0.0000
   280        0.0003             nan     0.5824   -0.0000
   300        0.0003             nan     0.5824   -0.0000
   320        0.0002             nan     0.5824   -0.0000
   340        0.0002             nan     0.5824   -0.0000
   360        0.0002             nan     0.5824   -0.0000
   380        0.0002             nan     0.5824   -0.0000
   400        0.0002             nan     0.5824   -0.0000
   420        0.0002             nan     0.5824   -0.0000
   440        0.0002             nan     0.5824   -0.0000
   460        0.0002             nan     0.5824   -0.0000
   480        0.0001             nan     0.5824   -0.0000
   500        0.0001             nan     0.5824   -0.0000
   520        0.0001             nan     0.5824   -0.0000
   540        0.0001             nan     0.5824   -0.0000
   560        0.0001             nan     0.5824   -0.0000
   580        0.0001             nan     0.5824   -0.0000
   600        0.0001             nan     0.5824   -0.0000
   620        0.0001             nan     0.5824   -0.0000
   640        0.0001             nan     0.5824   -0.0000
   660        0.0001             nan     0.5824   -0.0000
   680        0.0001             nan     0.5824   -0.0000
   700        0.0001             nan     0.5824   -0.0000
   720        0.0001             nan     0.5824   -0.0000
   740        0.0001             nan     0.5824   -0.0000
   760        0.0001             nan     0.5824   -0.0000
   780        0.0001             nan     0.5824   -0.0000
   800        0.0001             nan     0.5824    0.0000
   820        0.0001             nan     0.5824    0.0000
   840        0.0001             nan     0.5824   -0.0000
   860        0.0001             nan     0.5824   -0.0000
   880        0.0001             nan     0.5824   -0.0000
   900        0.0001             nan     0.5824   -0.0000
   920        0.0001             nan     0.5824   -0.0000
   940        0.0001             nan     0.5824   -0.0000
   960        0.0001             nan     0.5824   -0.0000
   980        0.0001             nan     0.5824   -0.0000
  1000        0.0001             nan     0.5824   -0.0000
  1020        0.0001             nan     0.5824   -0.0000
  1040        0.0001             nan     0.5824   -0.0000
  1060        0.0001             nan     0.5824   -0.0000
  1080        0.0001             nan     0.5824   -0.0000
  1100        0.0001             nan     0.5824   -0.0000
  1120        0.0001             nan     0.5824   -0.0000
  1140        0.0001             nan     0.5824   -0.0000
  1160        0.0001             nan     0.5824    0.0000
  1180        0.0001             nan     0.5824   -0.0000
  1200        0.0001             nan     0.5824    0.0000
  1220        0.0001             nan     0.5824   -0.0000
  1240        0.0001             nan     0.5824   -0.0000
  1260        0.0001             nan     0.5824   -0.0000
  1280        0.0001             nan     0.5824   -0.0000
  1300        0.0001             nan     0.5824   -0.0000
  1320        0.0001             nan     0.5824   -0.0000
  1340        0.0001             nan     0.5824   -0.0000
  1360        0.0001             nan     0.5824    0.0000
  1380        0.0001             nan     0.5824   -0.0000
  1400        0.0001             nan     0.5824   -0.0000
  1420        0.0001             nan     0.5824   -0.0000
  1440        0.0001             nan     0.5824   -0.0000
  1460        0.0001             nan     0.5824   -0.0000
  1480        0.0001             nan     0.5824    0.0000
  1500        0.0001             nan     0.5824    0.0000
  1520        0.0001             nan     0.5824   -0.0000
  1540        0.0001             nan     0.5824   -0.0000
  1560        0.0001             nan     0.5824    0.0000
  1580        0.0001             nan     0.5824    0.0000
  1600        0.0001             nan     0.5824    0.0000
  1620        0.0001             nan     0.5824   -0.0000
  1640        0.0001             nan     0.5824   -0.0000
  1660        0.0001             nan     0.5824    0.0000
  1680        0.0001             nan     0.5824   -0.0000
  1700        0.0001             nan     0.5824   -0.0000
  1720        0.0001             nan     0.5824   -0.0000
  1740        0.0001             nan     0.5824   -0.0000
  1760        0.0001             nan     0.5824   -0.0000
  1780        0.0001             nan     0.5824   -0.0000
  1800        0.0001             nan     0.5824    0.0000
  1820        0.0001             nan     0.5824   -0.0000
  1840        0.0001             nan     0.5824   -0.0000
  1860        0.0001             nan     0.5824   -0.0000
  1880        0.0001             nan     0.5824    0.0000
  1900        0.0001             nan     0.5824   -0.0000
  1920        0.0001             nan     0.5824   -0.0000
  1940        0.0001             nan     0.5824    0.0000
  1960        0.0001             nan     0.5824    0.0000
  1980        0.0001             nan     0.5824   -0.0000
  2000        0.0001             nan     0.5824    0.0000
  2020        0.0001             nan     0.5824   -0.0000
  2040        0.0001             nan     0.5824   -0.0000
  2060        0.0001             nan     0.5824   -0.0000
  2080        0.0001             nan     0.5824   -0.0000
  2100        0.0001             nan     0.5824   -0.0000
  2120        0.0001             nan     0.5824   -0.0000
  2140        0.0001             nan     0.5824   -0.0000
  2160        0.0001             nan     0.5824   -0.0000
  2180        0.0001             nan     0.5824    0.0000
  2200        0.0001             nan     0.5824   -0.0000
  2220        0.0001             nan     0.5824   -0.0000
  2240        0.0001             nan     0.5824   -0.0000
  2260        0.0001             nan     0.5824   -0.0000
  2280        0.0001             nan     0.5824   -0.0000
  2300        0.0001             nan     0.5824   -0.0000
  2320        0.0001             nan     0.5824   -0.0000
  2340        0.0001             nan     0.5824   -0.0000
  2360        0.0001             nan     0.5824   -0.0000
  2380        0.0001             nan     0.5824   -0.0000
  2400        0.0001             nan     0.5824   -0.0000
  2420        0.0001             nan     0.5824    0.0000
  2440        0.0001             nan     0.5824    0.0000
  2460        0.0001             nan     0.5824    0.0000
  2480        0.0001             nan     0.5824   -0.0000
  2500        0.0001             nan     0.5824   -0.0000
  2520        0.0001             nan     0.5824   -0.0000
  2540        0.0001             nan     0.5824   -0.0000
  2560        0.0001             nan     0.5824   -0.0000
  2580        0.0001             nan     0.5824   -0.0000
  2600        0.0001             nan     0.5824   -0.0000
  2620        0.0001             nan     0.5824    0.0000
  2640        0.0001             nan     0.5824   -0.0000
  2660        0.0001             nan     0.5824   -0.0000
  2680        0.0001             nan     0.5824   -0.0000
  2700        0.0001             nan     0.5824   -0.0000
  2720        0.0001             nan     0.5824   -0.0000
  2740        0.0001             nan     0.5824   -0.0000
  2760        0.0001             nan     0.5824   -0.0000
  2780        0.0001             nan     0.5824   -0.0000
  2800        0.0001             nan     0.5824   -0.0000
  2820        0.0001             nan     0.5824   -0.0000
  2840        0.0001             nan     0.5824   -0.0000
  2860        0.0001             nan     0.5824   -0.0000
  2880        0.0001             nan     0.5824   -0.0000
  2900        0.0001             nan     0.5824   -0.0000
  2920        0.0001             nan     0.5824   -0.0000
  2940        0.0001             nan     0.5824   -0.0000
  2960        0.0001             nan     0.5824   -0.0000
  2980        0.0001             nan     0.5824    0.0000
  3000        0.0001             nan     0.5824    0.0000
  3020        0.0001             nan     0.5824   -0.0000
  3040        0.0001             nan     0.5824   -0.0000
  3060        0.0001             nan     0.5824   -0.0000
  3080        0.0001             nan     0.5824   -0.0000
  3100        0.0001             nan     0.5824   -0.0000
  3120        0.0001             nan     0.5824   -0.0000
  3140        0.0001             nan     0.5824   -0.0000
  3160        0.0001             nan     0.5824   -0.0000
  3180        0.0001             nan     0.5824   -0.0000
  3200        0.0001             nan     0.5824   -0.0000
  3220        0.0001             nan     0.5824   -0.0000
  3240        0.0001             nan     0.5824   -0.0000
  3260        0.0001             nan     0.5824    0.0000
  3280        0.0001             nan     0.5824    0.0000
  3300        0.0001             nan     0.5824   -0.0000
  3320        0.0001             nan     0.5824   -0.0000
  3340        0.0001             nan     0.5824    0.0000
  3360        0.0001             nan     0.5824   -0.0000
  3380        0.0001             nan     0.5824   -0.0000
  3400        0.0001             nan     0.5824   -0.0000
  3420        0.0001             nan     0.5824   -0.0000
  3440        0.0001             nan     0.5824    0.0000
  3460        0.0001             nan     0.5824    0.0000
  3480        0.0001             nan     0.5824   -0.0000
  3500        0.0001             nan     0.5824   -0.0000
  3520        0.0001             nan     0.5824   -0.0000
  3540        0.0001             nan     0.5824   -0.0000
  3560        0.0001             nan     0.5824   -0.0000
  3580        0.0001             nan     0.5824   -0.0000
  3600        0.0001             nan     0.5824   -0.0000
  3620        0.0001             nan     0.5824   -0.0000
  3640        0.0001             nan     0.5824   -0.0000
  3660        0.0001             nan     0.5824   -0.0000
  3680        0.0001             nan     0.5824   -0.0000
  3700        0.0001             nan     0.5824   -0.0000
  3720        0.0001             nan     0.5824   -0.0000
  3740        0.0001             nan     0.5824   -0.0000
  3760        0.0001             nan     0.5824   -0.0000
  3780        0.0001             nan     0.5824   -0.0000
  3800        0.0001             nan     0.5824   -0.0000
  3820        0.0001             nan     0.5824   -0.0000
  3840        0.0001             nan     0.5824   -0.0000
  3860        0.0001             nan     0.5824   -0.0000
  3880        0.0001             nan     0.5824    0.0000
  3900        0.0001             nan     0.5824   -0.0000
  3920        0.0001             nan     0.5824   -0.0000
  3940        0.0001             nan     0.5824   -0.0000
  3960        0.0001             nan     0.5824   -0.0000
  3980        0.0001             nan     0.5824   -0.0000
  4000        0.0001             nan     0.5824   -0.0000
  4020        0.0001             nan     0.5824   -0.0000
  4040        0.0001             nan     0.5824    0.0000
  4060        0.0001             nan     0.5824   -0.0000
  4080        0.0001             nan     0.5824   -0.0000
  4100        0.0001             nan     0.5824   -0.0000
  4120        0.0001             nan     0.5824   -0.0000
  4140        0.0001             nan     0.5824   -0.0000
  4160        0.0001             nan     0.5824   -0.0000
  4180        0.0001             nan     0.5824   -0.0000
  4200        0.0001             nan     0.5824    0.0000
  4220        0.0001             nan     0.5824   -0.0000
  4240        0.0001             nan     0.5824   -0.0000
  4260        0.0001             nan     0.5824   -0.0000
  4280        0.0001             nan     0.5824   -0.0000
  4300        0.0001             nan     0.5824   -0.0000
  4320        0.0001             nan     0.5824   -0.0000
  4340        0.0001             nan     0.5824   -0.0000
  4360        0.0001             nan     0.5824    0.0000
  4380        0.0001             nan     0.5824   -0.0000
  4400        0.0001             nan     0.5824   -0.0000
  4420        0.0001             nan     0.5824   -0.0000
  4440        0.0001             nan     0.5824   -0.0000
  4460        0.0001             nan     0.5824   -0.0000
  4480        0.0001             nan     0.5824   -0.0000
  4500        0.0001             nan     0.5824   -0.0000
  4520        0.0001             nan     0.5824    0.0000
  4540        0.0001             nan     0.5824   -0.0000
  4560        0.0001             nan     0.5824    0.0000
  4580        0.0001             nan     0.5824   -0.0000
  4600        0.0001             nan     0.5824   -0.0000
  4617        0.0001             nan     0.5824   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1458             nan     0.5882    0.2626
     2        0.0537             nan     0.5882    0.0906
     3        0.0280             nan     0.5882    0.0235
     4        0.0207             nan     0.5882    0.0057
     5        0.0164             nan     0.5882    0.0037
     6        0.0145             nan     0.5882    0.0014
     7        0.0130             nan     0.5882    0.0010
     8        0.0120             nan     0.5882    0.0005
     9        0.0105             nan     0.5882    0.0013
    10        0.0101             nan     0.5882   -0.0001
    20        0.0051             nan     0.5882   -0.0004
    40        0.0030             nan     0.5882   -0.0001
    60        0.0018             nan     0.5882   -0.0001
    80        0.0014             nan     0.5882   -0.0000
   100        0.0011             nan     0.5882    0.0000
   120        0.0009             nan     0.5882   -0.0000
   140        0.0007             nan     0.5882   -0.0000
   160        0.0006             nan     0.5882   -0.0000
   180        0.0005             nan     0.5882   -0.0000
   200        0.0005             nan     0.5882   -0.0000
   220        0.0004             nan     0.5882   -0.0000
   240        0.0004             nan     0.5882   -0.0000
   260        0.0003             nan     0.5882   -0.0000
   280        0.0003             nan     0.5882   -0.0000
   300        0.0003             nan     0.5882   -0.0000
   320        0.0002             nan     0.5882   -0.0000
   340        0.0002             nan     0.5882   -0.0000
   360        0.0002             nan     0.5882   -0.0000
   380        0.0002             nan     0.5882   -0.0000
   400        0.0002             nan     0.5882   -0.0000
   420        0.0001             nan     0.5882   -0.0000
   440        0.0001             nan     0.5882   -0.0000
   460        0.0001             nan     0.5882   -0.0000
   480        0.0001             nan     0.5882   -0.0000
   500        0.0001             nan     0.5882   -0.0000
   520        0.0001             nan     0.5882   -0.0000
   540        0.0001             nan     0.5882   -0.0000
   560        0.0001             nan     0.5882   -0.0000
   580        0.0001             nan     0.5882   -0.0000
   600        0.0001             nan     0.5882   -0.0000
   620        0.0001             nan     0.5882   -0.0000
   640        0.0001             nan     0.5882   -0.0000
   660        0.0001             nan     0.5882   -0.0000
   680        0.0001             nan     0.5882   -0.0000
   700        0.0001             nan     0.5882   -0.0000
   720        0.0000             nan     0.5882   -0.0000
   740        0.0000             nan     0.5882   -0.0000
   760        0.0000             nan     0.5882   -0.0000
   780        0.0000             nan     0.5882   -0.0000
   800        0.0000             nan     0.5882   -0.0000
   820        0.0000             nan     0.5882   -0.0000
   840        0.0000             nan     0.5882   -0.0000
   860        0.0000             nan     0.5882   -0.0000
   880        0.0000             nan     0.5882   -0.0000
   900        0.0000             nan     0.5882   -0.0000
   920        0.0000             nan     0.5882   -0.0000
   940        0.0000             nan     0.5882   -0.0000
   960        0.0000             nan     0.5882   -0.0000
   980        0.0000             nan     0.5882   -0.0000
  1000        0.0000             nan     0.5882   -0.0000
  1020        0.0000             nan     0.5882   -0.0000
  1040        0.0000             nan     0.5882   -0.0000
  1060        0.0000             nan     0.5882   -0.0000
  1080        0.0000             nan     0.5882   -0.0000
  1100        0.0000             nan     0.5882   -0.0000
  1120        0.0000             nan     0.5882   -0.0000
  1140        0.0000             nan     0.5882   -0.0000
  1160        0.0000             nan     0.5882   -0.0000
  1180        0.0000             nan     0.5882   -0.0000
  1200        0.0000             nan     0.5882   -0.0000
  1220        0.0000             nan     0.5882   -0.0000
  1240        0.0000             nan     0.5882   -0.0000
  1260        0.0000             nan     0.5882   -0.0000
  1280        0.0000             nan     0.5882   -0.0000
  1300        0.0000             nan     0.5882   -0.0000
  1320        0.0000             nan     0.5882   -0.0000
  1340        0.0000             nan     0.5882   -0.0000
  1360        0.0000             nan     0.5882   -0.0000
  1380        0.0000             nan     0.5882   -0.0000
  1400        0.0000             nan     0.5882   -0.0000
  1420        0.0000             nan     0.5882   -0.0000
  1440        0.0000             nan     0.5882   -0.0000
  1460        0.0000             nan     0.5882   -0.0000
  1480        0.0000             nan     0.5882   -0.0000
  1500        0.0000             nan     0.5882   -0.0000
  1520        0.0000             nan     0.5882   -0.0000
  1540        0.0000             nan     0.5882    0.0000
  1560        0.0000             nan     0.5882   -0.0000
  1580        0.0000             nan     0.5882   -0.0000
  1600        0.0000             nan     0.5882   -0.0000
  1620        0.0000             nan     0.5882   -0.0000
  1640        0.0000             nan     0.5882   -0.0000
  1660        0.0000             nan     0.5882   -0.0000
  1680        0.0000             nan     0.5882   -0.0000
  1700        0.0000             nan     0.5882   -0.0000
  1720        0.0000             nan     0.5882   -0.0000
  1740        0.0000             nan     0.5882   -0.0000
  1760        0.0000             nan     0.5882   -0.0000
  1780        0.0000             nan     0.5882   -0.0000
  1800        0.0000             nan     0.5882   -0.0000
  1820        0.0000             nan     0.5882   -0.0000
  1840        0.0000             nan     0.5882   -0.0000
  1860        0.0000             nan     0.5882   -0.0000
  1880        0.0000             nan     0.5882   -0.0000
  1900        0.0000             nan     0.5882   -0.0000
  1920        0.0000             nan     0.5882   -0.0000
  1940        0.0000             nan     0.5882   -0.0000
  1960        0.0000             nan     0.5882   -0.0000
  1980        0.0000             nan     0.5882   -0.0000
  2000        0.0000             nan     0.5882   -0.0000
  2020        0.0000             nan     0.5882   -0.0000
  2040        0.0000             nan     0.5882   -0.0000
  2060        0.0000             nan     0.5882   -0.0000
  2080        0.0000             nan     0.5882   -0.0000
  2100        0.0000             nan     0.5882   -0.0000
  2120        0.0000             nan     0.5882   -0.0000
  2140        0.0000             nan     0.5882   -0.0000
  2160        0.0000             nan     0.5882   -0.0000
  2180        0.0000             nan     0.5882   -0.0000
  2200        0.0000             nan     0.5882   -0.0000
  2220        0.0000             nan     0.5882   -0.0000
  2240        0.0000             nan     0.5882   -0.0000
  2260        0.0000             nan     0.5882   -0.0000
  2280        0.0000             nan     0.5882   -0.0000
  2300        0.0000             nan     0.5882   -0.0000
  2320        0.0000             nan     0.5882   -0.0000
  2340        0.0000             nan     0.5882   -0.0000
  2360        0.0000             nan     0.5882   -0.0000
  2380        0.0000             nan     0.5882   -0.0000
  2400        0.0000             nan     0.5882   -0.0000
  2420        0.0000             nan     0.5882   -0.0000
  2440        0.0000             nan     0.5882    0.0000
  2460        0.0000             nan     0.5882   -0.0000
  2480        0.0000             nan     0.5882   -0.0000
  2500        0.0000             nan     0.5882   -0.0000
  2520        0.0000             nan     0.5882   -0.0000
  2540        0.0000             nan     0.5882   -0.0000
  2560        0.0000             nan     0.5882   -0.0000
  2580        0.0000             nan     0.5882   -0.0000
  2600        0.0000             nan     0.5882   -0.0000
  2620        0.0000             nan     0.5882   -0.0000
  2640        0.0000             nan     0.5882   -0.0000
  2660        0.0000             nan     0.5882   -0.0000
  2680        0.0000             nan     0.5882   -0.0000
  2700        0.0000             nan     0.5882   -0.0000
  2720        0.0000             nan     0.5882   -0.0000
  2740        0.0000             nan     0.5882   -0.0000
  2760        0.0000             nan     0.5882   -0.0000
  2780        0.0000             nan     0.5882   -0.0000
  2800        0.0000             nan     0.5882   -0.0000
  2820        0.0000             nan     0.5882   -0.0000
  2840        0.0000             nan     0.5882   -0.0000
  2860        0.0000             nan     0.5882   -0.0000
  2880        0.0000             nan     0.5882   -0.0000
  2900        0.0000             nan     0.5882   -0.0000
  2920        0.0000             nan     0.5882   -0.0000
  2940        0.0000             nan     0.5882   -0.0000
  2960        0.0000             nan     0.5882   -0.0000
  2980        0.0000             nan     0.5882   -0.0000
  3000        0.0000             nan     0.5882   -0.0000
  3020        0.0000             nan     0.5882   -0.0000
  3040        0.0000             nan     0.5882   -0.0000
  3060        0.0000             nan     0.5882   -0.0000
  3080        0.0000             nan     0.5882   -0.0000
  3100        0.0000             nan     0.5882   -0.0000
  3120        0.0000             nan     0.5882   -0.0000
  3140        0.0000             nan     0.5882   -0.0000
  3160        0.0000             nan     0.5882   -0.0000
  3180        0.0000             nan     0.5882   -0.0000
  3200        0.0000             nan     0.5882   -0.0000
  3220        0.0000             nan     0.5882   -0.0000
  3240        0.0000             nan     0.5882   -0.0000
  3260        0.0000             nan     0.5882    0.0000
  3280        0.0000             nan     0.5882   -0.0000
  3300        0.0000             nan     0.5882   -0.0000
  3320        0.0000             nan     0.5882   -0.0000
  3340        0.0000             nan     0.5882   -0.0000
  3360        0.0000             nan     0.5882   -0.0000
  3380        0.0000             nan     0.5882   -0.0000
  3400        0.0000             nan     0.5882   -0.0000
  3420        0.0000             nan     0.5882   -0.0000
  3440        0.0000             nan     0.5882   -0.0000
  3460        0.0000             nan     0.5882   -0.0000
  3480        0.0000             nan     0.5882   -0.0000
  3500        0.0000             nan     0.5882   -0.0000
  3520        0.0000             nan     0.5882   -0.0000
  3540        0.0000             nan     0.5882   -0.0000
  3560        0.0000             nan     0.5882   -0.0000
  3580        0.0000             nan     0.5882   -0.0000
  3600        0.0000             nan     0.5882   -0.0000
  3620        0.0000             nan     0.5882   -0.0000
  3640        0.0000             nan     0.5882   -0.0000
  3660        0.0000             nan     0.5882   -0.0000
  3680        0.0000             nan     0.5882   -0.0000
  3700        0.0000             nan     0.5882   -0.0000
  3720        0.0000             nan     0.5882   -0.0000
  3740        0.0000             nan     0.5882   -0.0000
  3760        0.0000             nan     0.5882   -0.0000
  3780        0.0000             nan     0.5882   -0.0000
  3800        0.0000             nan     0.5882   -0.0000
  3820        0.0000             nan     0.5882   -0.0000
  3840        0.0000             nan     0.5882   -0.0000
  3860        0.0000             nan     0.5882   -0.0000
  3880        0.0000             nan     0.5882   -0.0000
  3900        0.0000             nan     0.5882   -0.0000
  3920        0.0000             nan     0.5882   -0.0000
  3940        0.0000             nan     0.5882   -0.0000
  3960        0.0000             nan     0.5882   -0.0000
  3980        0.0000             nan     0.5882   -0.0000
  4000        0.0000             nan     0.5882   -0.0000
  4020        0.0000             nan     0.5882   -0.0000
  4040        0.0000             nan     0.5882   -0.0000
  4060        0.0000             nan     0.5882   -0.0000
  4080        0.0000             nan     0.5882   -0.0000
  4100        0.0000             nan     0.5882   -0.0000
  4120        0.0000             nan     0.5882   -0.0000
  4140        0.0000             nan     0.5882   -0.0000
  4160        0.0000             nan     0.5882   -0.0000
  4180        0.0000             nan     0.5882   -0.0000
  4200        0.0000             nan     0.5882   -0.0000
  4220        0.0000             nan     0.5882   -0.0000
  4240        0.0000             nan     0.5882   -0.0000
  4260        0.0000             nan     0.5882   -0.0000
  4280        0.0000             nan     0.5882   -0.0000
  4300        0.0000             nan     0.5882   -0.0000
  4320        0.0000             nan     0.5882   -0.0000
  4340        0.0000             nan     0.5882   -0.0000
  4360        0.0000             nan     0.5882   -0.0000
  4380        0.0000             nan     0.5882   -0.0000
  4400        0.0000             nan     0.5882   -0.0000
  4420        0.0000             nan     0.5882   -0.0000
  4440        0.0000             nan     0.5882   -0.0000
  4460        0.0000             nan     0.5882   -0.0000
  4480        0.0000             nan     0.5882   -0.0000
  4500        0.0000             nan     0.5882   -0.0000
  4520        0.0000             nan     0.5882   -0.0000
  4540        0.0000             nan     0.5882   -0.0000
  4560        0.0000             nan     0.5882   -0.0000
  4580        0.0000             nan     0.5882   -0.0000
  4600        0.0000             nan     0.5882   -0.0000
  4620        0.0000             nan     0.5882   -0.0000
  4640        0.0000             nan     0.5882   -0.0000
  4660        0.0000             nan     0.5882   -0.0000
  4680        0.0000             nan     0.5882   -0.0000
  4700        0.0000             nan     0.5882   -0.0000
  4720        0.0000             nan     0.5882   -0.0000
  4740        0.0000             nan     0.5882   -0.0000
  4760        0.0000             nan     0.5882   -0.0000
  4780        0.0000             nan     0.5882    0.0000
  4800        0.0000             nan     0.5882   -0.0000
  4820        0.0000             nan     0.5882   -0.0000
  4840        0.0000             nan     0.5882   -0.0000
  4846        0.0000             nan     0.5882   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3908             nan     0.0196    0.0122
     2        0.3792             nan     0.0196    0.0118
     3        0.3681             nan     0.0196    0.0113
     4        0.3570             nan     0.0196    0.0112
     5        0.3467             nan     0.0196    0.0104
     6        0.3364             nan     0.0196    0.0101
     7        0.3263             nan     0.0196    0.0094
     8        0.3167             nan     0.0196    0.0100
     9        0.3070             nan     0.0196    0.0095
    10        0.2978             nan     0.0196    0.0093
    20        0.2217             nan     0.0196    0.0068
    40        0.1246             nan     0.0196    0.0036
    60        0.0717             nan     0.0196    0.0019
    80        0.0438             nan     0.0196    0.0011
   100        0.0271             nan     0.0196    0.0007
   120        0.0173             nan     0.0196    0.0004
   140        0.0113             nan     0.0196    0.0002
   160        0.0077             nan     0.0196    0.0001
   180        0.0053             nan     0.0196    0.0001
   200        0.0037             nan     0.0196    0.0001
   220        0.0028             nan     0.0196    0.0000
   240        0.0021             nan     0.0196    0.0000
   260        0.0017             nan     0.0196    0.0000
   280        0.0014             nan     0.0196    0.0000
   300        0.0012             nan     0.0196    0.0000
   320        0.0010             nan     0.0196    0.0000
   340        0.0009             nan     0.0196    0.0000
   360        0.0008             nan     0.0196    0.0000
   380        0.0007             nan     0.0196    0.0000
   400        0.0007             nan     0.0196    0.0000
   420        0.0006             nan     0.0196    0.0000
   440        0.0006             nan     0.0196    0.0000
   460        0.0006             nan     0.0196    0.0000
   480        0.0006             nan     0.0196    0.0000
   500        0.0005             nan     0.0196    0.0000
   520        0.0005             nan     0.0196   -0.0000
   540        0.0005             nan     0.0196   -0.0000
   560        0.0005             nan     0.0196   -0.0000
   580        0.0005             nan     0.0196   -0.0000
   600        0.0005             nan     0.0196    0.0000
   620        0.0004             nan     0.0196   -0.0000
   640        0.0004             nan     0.0196   -0.0000
   660        0.0004             nan     0.0196   -0.0000
   680        0.0004             nan     0.0196   -0.0000
   700        0.0004             nan     0.0196   -0.0000
   720        0.0004             nan     0.0196   -0.0000
   740        0.0004             nan     0.0196    0.0000
   760        0.0004             nan     0.0196    0.0000
   780        0.0004             nan     0.0196   -0.0000
   800        0.0004             nan     0.0196    0.0000
   820        0.0004             nan     0.0196    0.0000
   840        0.0003             nan     0.0196   -0.0000
   860        0.0003             nan     0.0196   -0.0000
   880        0.0003             nan     0.0196   -0.0000
   900        0.0003             nan     0.0196   -0.0000
   920        0.0003             nan     0.0196   -0.0000
   940        0.0003             nan     0.0196   -0.0000
   960        0.0003             nan     0.0196   -0.0000
   980        0.0003             nan     0.0196   -0.0000
  1000        0.0003             nan     0.0196   -0.0000
  1020        0.0003             nan     0.0196   -0.0000
  1040        0.0003             nan     0.0196   -0.0000
  1060        0.0003             nan     0.0196   -0.0000
  1080        0.0003             nan     0.0196   -0.0000
  1100        0.0003             nan     0.0196    0.0000
  1120        0.0003             nan     0.0196   -0.0000
  1140        0.0003             nan     0.0196   -0.0000
  1160        0.0003             nan     0.0196   -0.0000
  1180        0.0003             nan     0.0196   -0.0000
  1200        0.0003             nan     0.0196   -0.0000
  1220        0.0003             nan     0.0196   -0.0000
  1240        0.0002             nan     0.0196   -0.0000
  1260        0.0002             nan     0.0196   -0.0000
  1280        0.0002             nan     0.0196   -0.0000
  1300        0.0002             nan     0.0196   -0.0000
  1320        0.0002             nan     0.0196   -0.0000
  1340        0.0002             nan     0.0196    0.0000
  1360        0.0002             nan     0.0196   -0.0000
  1380        0.0002             nan     0.0196   -0.0000
  1400        0.0002             nan     0.0196   -0.0000
  1420        0.0002             nan     0.0196   -0.0000
  1440        0.0002             nan     0.0196   -0.0000
  1460        0.0002             nan     0.0196    0.0000
  1475        0.0002             nan     0.0196   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3578             nan     0.0878    0.0457
     2        0.3173             nan     0.0878    0.0380
     3        0.2855             nan     0.0878    0.0319
     4        0.2558             nan     0.0878    0.0288
     5        0.2331             nan     0.0878    0.0252
     6        0.2103             nan     0.0878    0.0219
     7        0.1917             nan     0.0878    0.0181
     8        0.1746             nan     0.0878    0.0171
     9        0.1601             nan     0.0878    0.0144
    10        0.1465             nan     0.0878    0.0105
    20        0.0658             nan     0.0878    0.0052
    40        0.0176             nan     0.0878    0.0009
    60        0.0064             nan     0.0878    0.0002
    80        0.0032             nan     0.0878    0.0001
   100        0.0021             nan     0.0878    0.0000
   120        0.0017             nan     0.0878    0.0000
   140        0.0015             nan     0.0878   -0.0000
   160        0.0014             nan     0.0878    0.0000
   180        0.0013             nan     0.0878    0.0000
   200        0.0013             nan     0.0878   -0.0000
   220        0.0012             nan     0.0878   -0.0000
   240        0.0012             nan     0.0878   -0.0000
   260        0.0012             nan     0.0878   -0.0000
   280        0.0011             nan     0.0878   -0.0000
   300        0.0011             nan     0.0878   -0.0000
   320        0.0011             nan     0.0878   -0.0000
   340        0.0011             nan     0.0878   -0.0000
   360        0.0011             nan     0.0878   -0.0000
   380        0.0010             nan     0.0878   -0.0000
   400        0.0010             nan     0.0878   -0.0000
   420        0.0010             nan     0.0878   -0.0000
   440        0.0010             nan     0.0878   -0.0000
   460        0.0010             nan     0.0878    0.0000
   480        0.0009             nan     0.0878   -0.0000
   500        0.0009             nan     0.0878   -0.0000
   520        0.0009             nan     0.0878   -0.0000
   540        0.0009             nan     0.0878   -0.0000
   560        0.0009             nan     0.0878   -0.0000
   580        0.0009             nan     0.0878   -0.0000
   600        0.0009             nan     0.0878   -0.0000
   620        0.0009             nan     0.0878   -0.0000
   640        0.0008             nan     0.0878   -0.0000
   660        0.0008             nan     0.0878   -0.0000
   680        0.0008             nan     0.0878   -0.0000
   700        0.0008             nan     0.0878   -0.0000
   720        0.0008             nan     0.0878   -0.0000
   740        0.0008             nan     0.0878   -0.0000
   760        0.0008             nan     0.0878   -0.0000
   780        0.0008             nan     0.0878   -0.0000
   800        0.0008             nan     0.0878   -0.0000
   820        0.0007             nan     0.0878   -0.0000
   840        0.0007             nan     0.0878   -0.0000
   860        0.0007             nan     0.0878   -0.0000
   880        0.0007             nan     0.0878   -0.0000
   900        0.0007             nan     0.0878   -0.0000
   920        0.0007             nan     0.0878   -0.0000
   940        0.0007             nan     0.0878   -0.0000
   960        0.0007             nan     0.0878   -0.0000
   980        0.0007             nan     0.0878   -0.0000
  1000        0.0007             nan     0.0878   -0.0000
  1020        0.0007             nan     0.0878   -0.0000
  1040        0.0007             nan     0.0878   -0.0000
  1060        0.0007             nan     0.0878   -0.0000
  1080        0.0006             nan     0.0878   -0.0000
  1100        0.0006             nan     0.0878   -0.0000
  1120        0.0006             nan     0.0878   -0.0000
  1140        0.0006             nan     0.0878   -0.0000
  1160        0.0006             nan     0.0878   -0.0000
  1180        0.0006             nan     0.0878   -0.0000
  1200        0.0006             nan     0.0878   -0.0000
  1220        0.0006             nan     0.0878   -0.0000
  1240        0.0006             nan     0.0878   -0.0000
  1260        0.0006             nan     0.0878   -0.0000
  1280        0.0006             nan     0.0878   -0.0000
  1300        0.0006             nan     0.0878   -0.0000
  1320        0.0006             nan     0.0878   -0.0000
  1340        0.0006             nan     0.0878   -0.0000
  1360        0.0006             nan     0.0878   -0.0000
  1380        0.0006             nan     0.0878   -0.0000
  1395        0.0006             nan     0.0878   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3255             nan     0.1163    0.0731
     2        0.2638             nan     0.1163    0.0570
     3        0.2142             nan     0.1163    0.0536
     4        0.1750             nan     0.1163    0.0397
     5        0.1450             nan     0.1163    0.0299
     6        0.1192             nan     0.1163    0.0245
     7        0.0976             nan     0.1163    0.0202
     8        0.0807             nan     0.1163    0.0159
     9        0.0676             nan     0.1163    0.0133
    10        0.0562             nan     0.1163    0.0116
    20        0.0107             nan     0.1163    0.0014
    40        0.0015             nan     0.1163    0.0001
    60        0.0009             nan     0.1163    0.0000
    80        0.0007             nan     0.1163   -0.0000
   100        0.0006             nan     0.1163   -0.0000
   120        0.0005             nan     0.1163   -0.0000
   140        0.0004             nan     0.1163   -0.0000
   160        0.0004             nan     0.1163   -0.0000
   180        0.0003             nan     0.1163   -0.0000
   200        0.0003             nan     0.1163   -0.0000
   220        0.0003             nan     0.1163    0.0000
   240        0.0002             nan     0.1163   -0.0000
   260        0.0002             nan     0.1163   -0.0000
   280        0.0002             nan     0.1163   -0.0000
   300        0.0002             nan     0.1163    0.0000
   320        0.0002             nan     0.1163   -0.0000
   340        0.0002             nan     0.1163   -0.0000
   360        0.0001             nan     0.1163   -0.0000
   380        0.0001             nan     0.1163   -0.0000
   400        0.0001             nan     0.1163   -0.0000
   420        0.0001             nan     0.1163   -0.0000
   440        0.0001             nan     0.1163   -0.0000
   460        0.0001             nan     0.1163   -0.0000
   480        0.0001             nan     0.1163   -0.0000
   500        0.0001             nan     0.1163   -0.0000
   520        0.0001             nan     0.1163   -0.0000
   540        0.0001             nan     0.1163   -0.0000
   560        0.0001             nan     0.1163   -0.0000
   580        0.0001             nan     0.1163   -0.0000
   600        0.0001             nan     0.1163   -0.0000
   620        0.0001             nan     0.1163   -0.0000
   640        0.0001             nan     0.1163   -0.0000
   660        0.0001             nan     0.1163   -0.0000
   680        0.0001             nan     0.1163   -0.0000
   700        0.0001             nan     0.1163   -0.0000
   720        0.0001             nan     0.1163   -0.0000
   740        0.0001             nan     0.1163   -0.0000
   760        0.0001             nan     0.1163   -0.0000
   780        0.0000             nan     0.1163   -0.0000
   800        0.0000             nan     0.1163   -0.0000
   820        0.0000             nan     0.1163   -0.0000
   840        0.0000             nan     0.1163   -0.0000
   860        0.0000             nan     0.1163   -0.0000
   880        0.0000             nan     0.1163   -0.0000
   900        0.0000             nan     0.1163   -0.0000
   920        0.0000             nan     0.1163   -0.0000
   940        0.0000             nan     0.1163   -0.0000
   960        0.0000             nan     0.1163   -0.0000
   980        0.0000             nan     0.1163   -0.0000
  1000        0.0000             nan     0.1163   -0.0000
  1020        0.0000             nan     0.1163   -0.0000
  1040        0.0000             nan     0.1163   -0.0000
  1060        0.0000             nan     0.1163   -0.0000
  1080        0.0000             nan     0.1163   -0.0000
  1100        0.0000             nan     0.1163   -0.0000
  1106        0.0000             nan     0.1163   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2888             nan     0.1626    0.1079
     2        0.2071             nan     0.1626    0.0830
     3        0.1470             nan     0.1626    0.0573
     4        0.1060             nan     0.1626    0.0407
     5        0.0766             nan     0.1626    0.0293
     6        0.0559             nan     0.1626    0.0206
     7        0.0419             nan     0.1626    0.0143
     8        0.0308             nan     0.1626    0.0103
     9        0.0233             nan     0.1626    0.0078
    10        0.0179             nan     0.1626    0.0061
    20        0.0033             nan     0.1626    0.0003
    40        0.0015             nan     0.1626   -0.0000
    50        0.0013             nan     0.1626   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2668             nan     0.2083    0.1413
     2        0.1778             nan     0.2083    0.0978
     3        0.1207             nan     0.2083    0.0598
     4        0.0803             nan     0.2083    0.0360
     5        0.0555             nan     0.2083    0.0254
     6        0.0380             nan     0.2083    0.0166
     7        0.0263             nan     0.2083    0.0122
     8        0.0187             nan     0.2083    0.0071
     9        0.0136             nan     0.2083    0.0041
    10        0.0107             nan     0.2083    0.0033
    20        0.0027             nan     0.2083    0.0002
    40        0.0015             nan     0.2083    0.0000
    60        0.0012             nan     0.2083   -0.0000
    80        0.0009             nan     0.2083   -0.0000
   100        0.0008             nan     0.2083   -0.0000
   120        0.0007             nan     0.2083   -0.0000
   140        0.0006             nan     0.2083   -0.0000
   160        0.0005             nan     0.2083   -0.0000
   180        0.0005             nan     0.2083   -0.0000
   200        0.0005             nan     0.2083   -0.0000
   220        0.0004             nan     0.2083   -0.0000
   240        0.0004             nan     0.2083   -0.0000
   260        0.0004             nan     0.2083   -0.0000
   280        0.0003             nan     0.2083   -0.0000
   300        0.0003             nan     0.2083   -0.0000
   320        0.0003             nan     0.2083   -0.0000
   340        0.0003             nan     0.2083   -0.0000
   360        0.0003             nan     0.2083   -0.0000
   380        0.0003             nan     0.2083   -0.0000
   400        0.0002             nan     0.2083   -0.0000
   420        0.0002             nan     0.2083   -0.0000
   440        0.0002             nan     0.2083   -0.0000
   460        0.0002             nan     0.2083   -0.0000
   480        0.0002             nan     0.2083   -0.0000
   500        0.0002             nan     0.2083   -0.0000
   520        0.0002             nan     0.2083   -0.0000
   540        0.0002             nan     0.2083   -0.0000
   560        0.0002             nan     0.2083   -0.0000
   580        0.0002             nan     0.2083   -0.0000
   600        0.0002             nan     0.2083   -0.0000
   620        0.0002             nan     0.2083   -0.0000
   640        0.0001             nan     0.2083   -0.0000
   660        0.0001             nan     0.2083   -0.0000
   680        0.0001             nan     0.2083   -0.0000
   700        0.0001             nan     0.2083   -0.0000
   720        0.0001             nan     0.2083   -0.0000
   740        0.0001             nan     0.2083   -0.0000
   760        0.0001             nan     0.2083   -0.0000
   780        0.0001             nan     0.2083   -0.0000
   800        0.0001             nan     0.2083   -0.0000
   820        0.0001             nan     0.2083   -0.0000
   840        0.0001             nan     0.2083   -0.0000
   860        0.0001             nan     0.2083   -0.0000
   880        0.0001             nan     0.2083   -0.0000
   900        0.0001             nan     0.2083   -0.0000
   920        0.0001             nan     0.2083   -0.0000
   940        0.0001             nan     0.2083   -0.0000
   960        0.0001             nan     0.2083   -0.0000
   980        0.0001             nan     0.2083   -0.0000
  1000        0.0001             nan     0.2083   -0.0000
  1020        0.0001             nan     0.2083   -0.0000
  1040        0.0001             nan     0.2083   -0.0000
  1060        0.0001             nan     0.2083   -0.0000
  1080        0.0001             nan     0.2083   -0.0000
  1100        0.0001             nan     0.2083   -0.0000
  1120        0.0001             nan     0.2083   -0.0000
  1140        0.0001             nan     0.2083   -0.0000
  1160        0.0001             nan     0.2083   -0.0000
  1180        0.0001             nan     0.2083   -0.0000
  1200        0.0001             nan     0.2083   -0.0000
  1220        0.0001             nan     0.2083   -0.0000
  1240        0.0001             nan     0.2083   -0.0000
  1260        0.0001             nan     0.2083   -0.0000
  1280        0.0001             nan     0.2083   -0.0000
  1300        0.0001             nan     0.2083   -0.0000
  1320        0.0001             nan     0.2083   -0.0000
  1340        0.0001             nan     0.2083   -0.0000
  1360        0.0001             nan     0.2083   -0.0000
  1380        0.0001             nan     0.2083   -0.0000
  1400        0.0001             nan     0.2083   -0.0000
  1420        0.0001             nan     0.2083   -0.0000
  1440        0.0001             nan     0.2083   -0.0000
  1460        0.0001             nan     0.2083   -0.0000
  1480        0.0001             nan     0.2083   -0.0000
  1500        0.0001             nan     0.2083   -0.0000
  1520        0.0001             nan     0.2083   -0.0000
  1540        0.0001             nan     0.2083   -0.0000
  1560        0.0001             nan     0.2083   -0.0000
  1580        0.0001             nan     0.2083   -0.0000
  1600        0.0001             nan     0.2083    0.0000
  1620        0.0001             nan     0.2083   -0.0000
  1640        0.0001             nan     0.2083   -0.0000
  1660        0.0001             nan     0.2083   -0.0000
  1680        0.0001             nan     0.2083   -0.0000
  1700        0.0001             nan     0.2083   -0.0000
  1720        0.0001             nan     0.2083   -0.0000
  1740        0.0001             nan     0.2083   -0.0000
  1760        0.0001             nan     0.2083   -0.0000
  1780        0.0001             nan     0.2083   -0.0000
  1800        0.0001             nan     0.2083   -0.0000
  1820        0.0001             nan     0.2083   -0.0000
  1840        0.0001             nan     0.2083   -0.0000
  1860        0.0001             nan     0.2083   -0.0000
  1880        0.0001             nan     0.2083   -0.0000
  1900        0.0001             nan     0.2083   -0.0000
  1920        0.0001             nan     0.2083   -0.0000
  1940        0.0001             nan     0.2083    0.0000
  1960        0.0001             nan     0.2083   -0.0000
  1974        0.0001             nan     0.2083   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2670             nan     0.2133    0.1421
     2        0.1785             nan     0.2133    0.0852
     3        0.1201             nan     0.2133    0.0590
     4        0.0841             nan     0.2133    0.0381
     5        0.0594             nan     0.2133    0.0229
     6        0.0412             nan     0.2133    0.0172
     7        0.0301             nan     0.2133    0.0112
     8        0.0219             nan     0.2133    0.0078
     9        0.0167             nan     0.2133    0.0061
    10        0.0129             nan     0.2133    0.0037
    20        0.0023             nan     0.2133    0.0001
    40        0.0013             nan     0.2133    0.0000
    60        0.0010             nan     0.2133   -0.0000
    80        0.0008             nan     0.2133   -0.0000
   100        0.0007             nan     0.2133   -0.0000
   120        0.0006             nan     0.2133   -0.0000
   140        0.0005             nan     0.2133   -0.0000
   160        0.0005             nan     0.2133    0.0000
   180        0.0004             nan     0.2133   -0.0000
   200        0.0004             nan     0.2133   -0.0000
   220        0.0003             nan     0.2133   -0.0000
   240        0.0003             nan     0.2133   -0.0000
   260        0.0003             nan     0.2133   -0.0000
   280        0.0003             nan     0.2133   -0.0000
   300        0.0002             nan     0.2133   -0.0000
   320        0.0002             nan     0.2133   -0.0000
   340        0.0002             nan     0.2133   -0.0000
   360        0.0002             nan     0.2133   -0.0000
   380        0.0002             nan     0.2133   -0.0000
   400        0.0002             nan     0.2133   -0.0000
   420        0.0002             nan     0.2133   -0.0000
   440        0.0001             nan     0.2133   -0.0000
   460        0.0001             nan     0.2133   -0.0000
   480        0.0001             nan     0.2133   -0.0000
   500        0.0001             nan     0.2133   -0.0000
   520        0.0001             nan     0.2133   -0.0000
   540        0.0001             nan     0.2133   -0.0000
   560        0.0001             nan     0.2133   -0.0000
   580        0.0001             nan     0.2133   -0.0000
   600        0.0001             nan     0.2133   -0.0000
   620        0.0001             nan     0.2133   -0.0000
   640        0.0001             nan     0.2133   -0.0000
   660        0.0001             nan     0.2133   -0.0000
   680        0.0001             nan     0.2133   -0.0000
   700        0.0001             nan     0.2133   -0.0000
   720        0.0001             nan     0.2133   -0.0000
   740        0.0001             nan     0.2133   -0.0000
   760        0.0001             nan     0.2133   -0.0000
   780        0.0001             nan     0.2133   -0.0000
   800        0.0001             nan     0.2133   -0.0000
   820        0.0001             nan     0.2133   -0.0000
   840        0.0001             nan     0.2133   -0.0000
   860        0.0001             nan     0.2133   -0.0000
   880        0.0000             nan     0.2133   -0.0000
   900        0.0000             nan     0.2133   -0.0000
   920        0.0000             nan     0.2133   -0.0000
   940        0.0000             nan     0.2133   -0.0000
   960        0.0000             nan     0.2133   -0.0000
   980        0.0000             nan     0.2133   -0.0000
  1000        0.0000             nan     0.2133   -0.0000
  1020        0.0000             nan     0.2133   -0.0000
  1040        0.0000             nan     0.2133   -0.0000
  1060        0.0000             nan     0.2133   -0.0000
  1080        0.0000             nan     0.2133   -0.0000
  1100        0.0000             nan     0.2133   -0.0000
  1120        0.0000             nan     0.2133   -0.0000
  1140        0.0000             nan     0.2133   -0.0000
  1160        0.0000             nan     0.2133   -0.0000
  1180        0.0000             nan     0.2133   -0.0000
  1200        0.0000             nan     0.2133   -0.0000
  1220        0.0000             nan     0.2133   -0.0000
  1240        0.0000             nan     0.2133   -0.0000
  1260        0.0000             nan     0.2133   -0.0000
  1280        0.0000             nan     0.2133   -0.0000
  1300        0.0000             nan     0.2133   -0.0000
  1320        0.0000             nan     0.2133   -0.0000
  1340        0.0000             nan     0.2133   -0.0000
  1360        0.0000             nan     0.2133   -0.0000
  1380        0.0000             nan     0.2133   -0.0000
  1400        0.0000             nan     0.2133   -0.0000
  1420        0.0000             nan     0.2133   -0.0000
  1440        0.0000             nan     0.2133   -0.0000
  1460        0.0000             nan     0.2133   -0.0000
  1480        0.0000             nan     0.2133   -0.0000
  1500        0.0000             nan     0.2133   -0.0000
  1520        0.0000             nan     0.2133   -0.0000
  1540        0.0000             nan     0.2133   -0.0000
  1560        0.0000             nan     0.2133   -0.0000
  1580        0.0000             nan     0.2133    0.0000
  1600        0.0000             nan     0.2133   -0.0000
  1620        0.0000             nan     0.2133   -0.0000
  1640        0.0000             nan     0.2133   -0.0000
  1660        0.0000             nan     0.2133   -0.0000
  1680        0.0000             nan     0.2133   -0.0000
  1700        0.0000             nan     0.2133   -0.0000
  1720        0.0000             nan     0.2133   -0.0000
  1740        0.0000             nan     0.2133   -0.0000
  1760        0.0000             nan     0.2133   -0.0000
  1780        0.0000             nan     0.2133   -0.0000
  1800        0.0000             nan     0.2133   -0.0000
  1820        0.0000             nan     0.2133   -0.0000
  1840        0.0000             nan     0.2133   -0.0000
  1860        0.0000             nan     0.2133   -0.0000
  1880        0.0000             nan     0.2133   -0.0000
  1900        0.0000             nan     0.2133   -0.0000
  1920        0.0000             nan     0.2133   -0.0000
  1940        0.0000             nan     0.2133   -0.0000
  1960        0.0000             nan     0.2133   -0.0000
  1980        0.0000             nan     0.2133   -0.0000
  2000        0.0000             nan     0.2133   -0.0000
  2020        0.0000             nan     0.2133   -0.0000
  2040        0.0000             nan     0.2133   -0.0000
  2060        0.0000             nan     0.2133   -0.0000
  2080        0.0000             nan     0.2133   -0.0000
  2100        0.0000             nan     0.2133   -0.0000
  2120        0.0000             nan     0.2133   -0.0000
  2140        0.0000             nan     0.2133   -0.0000
  2160        0.0000             nan     0.2133   -0.0000
  2180        0.0000             nan     0.2133   -0.0000
  2200        0.0000             nan     0.2133   -0.0000
  2220        0.0000             nan     0.2133   -0.0000
  2240        0.0000             nan     0.2133   -0.0000
  2260        0.0000             nan     0.2133   -0.0000
  2280        0.0000             nan     0.2133   -0.0000
  2300        0.0000             nan     0.2133   -0.0000
  2320        0.0000             nan     0.2133   -0.0000
  2340        0.0000             nan     0.2133   -0.0000
  2360        0.0000             nan     0.2133   -0.0000
  2380        0.0000             nan     0.2133   -0.0000
  2400        0.0000             nan     0.2133   -0.0000
  2420        0.0000             nan     0.2133   -0.0000
  2440        0.0000             nan     0.2133   -0.0000
  2460        0.0000             nan     0.2133   -0.0000
  2480        0.0000             nan     0.2133   -0.0000
  2500        0.0000             nan     0.2133   -0.0000
  2520        0.0000             nan     0.2133   -0.0000
  2540        0.0000             nan     0.2133   -0.0000
  2560        0.0000             nan     0.2133   -0.0000
  2580        0.0000             nan     0.2133   -0.0000
  2600        0.0000             nan     0.2133   -0.0000
  2620        0.0000             nan     0.2133   -0.0000
  2640        0.0000             nan     0.2133   -0.0000
  2660        0.0000             nan     0.2133   -0.0000
  2680        0.0000             nan     0.2133   -0.0000
  2700        0.0000             nan     0.2133   -0.0000
  2720        0.0000             nan     0.2133   -0.0000
  2740        0.0000             nan     0.2133   -0.0000
  2760        0.0000             nan     0.2133   -0.0000
  2780        0.0000             nan     0.2133   -0.0000
  2800        0.0000             nan     0.2133   -0.0000
  2820        0.0000             nan     0.2133   -0.0000
  2840        0.0000             nan     0.2133   -0.0000
  2860        0.0000             nan     0.2133   -0.0000
  2880        0.0000             nan     0.2133   -0.0000
  2900        0.0000             nan     0.2133   -0.0000
  2920        0.0000             nan     0.2133   -0.0000
  2940        0.0000             nan     0.2133   -0.0000
  2960        0.0000             nan     0.2133   -0.0000
  2980        0.0000             nan     0.2133   -0.0000
  3000        0.0000             nan     0.2133   -0.0000
  3020        0.0000             nan     0.2133   -0.0000
  3040        0.0000             nan     0.2133   -0.0000
  3060        0.0000             nan     0.2133   -0.0000
  3080        0.0000             nan     0.2133   -0.0000
  3100        0.0000             nan     0.2133   -0.0000
  3120        0.0000             nan     0.2133   -0.0000
  3140        0.0000             nan     0.2133   -0.0000
  3160        0.0000             nan     0.2133   -0.0000
  3180        0.0000             nan     0.2133   -0.0000
  3200        0.0000             nan     0.2133   -0.0000
  3220        0.0000             nan     0.2133   -0.0000
  3240        0.0000             nan     0.2133   -0.0000
  3260        0.0000             nan     0.2133   -0.0000
  3280        0.0000             nan     0.2133   -0.0000
  3300        0.0000             nan     0.2133   -0.0000
  3320        0.0000             nan     0.2133   -0.0000
  3340        0.0000             nan     0.2133   -0.0000
  3360        0.0000             nan     0.2133   -0.0000
  3380        0.0000             nan     0.2133   -0.0000
  3400        0.0000             nan     0.2133   -0.0000
  3420        0.0000             nan     0.2133   -0.0000
  3440        0.0000             nan     0.2133   -0.0000
  3460        0.0000             nan     0.2133   -0.0000
  3480        0.0000             nan     0.2133   -0.0000
  3500        0.0000             nan     0.2133   -0.0000
  3520        0.0000             nan     0.2133   -0.0000
  3540        0.0000             nan     0.2133   -0.0000
  3560        0.0000             nan     0.2133   -0.0000
  3580        0.0000             nan     0.2133   -0.0000
  3600        0.0000             nan     0.2133   -0.0000
  3620        0.0000             nan     0.2133   -0.0000
  3640        0.0000             nan     0.2133   -0.0000
  3660        0.0000             nan     0.2133   -0.0000
  3680        0.0000             nan     0.2133    0.0000
  3700        0.0000             nan     0.2133   -0.0000
  3720        0.0000             nan     0.2133   -0.0000
  3740        0.0000             nan     0.2133   -0.0000
  3760        0.0000             nan     0.2133    0.0000
  3780        0.0000             nan     0.2133   -0.0000
  3800        0.0000             nan     0.2133   -0.0000
  3820        0.0000             nan     0.2133   -0.0000
  3840        0.0000             nan     0.2133   -0.0000
  3860        0.0000             nan     0.2133   -0.0000
  3880        0.0000             nan     0.2133   -0.0000
  3900        0.0000             nan     0.2133   -0.0000
  3920        0.0000             nan     0.2133   -0.0000
  3940        0.0000             nan     0.2133   -0.0000
  3960        0.0000             nan     0.2133   -0.0000
  3980        0.0000             nan     0.2133   -0.0000
  4000        0.0000             nan     0.2133   -0.0000
  4020        0.0000             nan     0.2133   -0.0000
  4040        0.0000             nan     0.2133   -0.0000
  4060        0.0000             nan     0.2133   -0.0000
  4080        0.0000             nan     0.2133   -0.0000
  4100        0.0000             nan     0.2133   -0.0000
  4120        0.0000             nan     0.2133   -0.0000
  4140        0.0000             nan     0.2133    0.0000
  4160        0.0000             nan     0.2133   -0.0000
  4180        0.0000             nan     0.2133   -0.0000
  4200        0.0000             nan     0.2133   -0.0000
  4220        0.0000             nan     0.2133   -0.0000
  4240        0.0000             nan     0.2133   -0.0000
  4260        0.0000             nan     0.2133   -0.0000
  4280        0.0000             nan     0.2133   -0.0000
  4300        0.0000             nan     0.2133   -0.0000
  4320        0.0000             nan     0.2133   -0.0000
  4340        0.0000             nan     0.2133    0.0000
  4360        0.0000             nan     0.2133   -0.0000
  4380        0.0000             nan     0.2133   -0.0000
  4400        0.0000             nan     0.2133   -0.0000
  4420        0.0000             nan     0.2133   -0.0000
  4439        0.0000             nan     0.2133   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2583             nan     0.2206    0.1341
     2        0.1669             nan     0.2206    0.0857
     3        0.1086             nan     0.2206    0.0622
     4        0.0720             nan     0.2206    0.0340
     5        0.0490             nan     0.2206    0.0233
     6        0.0337             nan     0.2206    0.0149
     7        0.0238             nan     0.2206    0.0093
     8        0.0169             nan     0.2206    0.0068
     9        0.0123             nan     0.2206    0.0038
    10        0.0089             nan     0.2206    0.0023
    20        0.0022             nan     0.2206    0.0001
    40        0.0012             nan     0.2206    0.0000
    60        0.0008             nan     0.2206   -0.0000
    80        0.0006             nan     0.2206   -0.0000
   100        0.0005             nan     0.2206   -0.0000
   120        0.0004             nan     0.2206   -0.0000
   140        0.0004             nan     0.2206   -0.0000
   160        0.0003             nan     0.2206   -0.0000
   180        0.0003             nan     0.2206   -0.0000
   200        0.0003             nan     0.2206   -0.0000
   220        0.0002             nan     0.2206   -0.0000
   240        0.0002             nan     0.2206   -0.0000
   260        0.0002             nan     0.2206   -0.0000
   280        0.0002             nan     0.2206   -0.0000
   300        0.0002             nan     0.2206   -0.0000
   320        0.0001             nan     0.2206   -0.0000
   340        0.0001             nan     0.2206   -0.0000
   360        0.0001             nan     0.2206   -0.0000
   380        0.0001             nan     0.2206   -0.0000
   400        0.0001             nan     0.2206   -0.0000
   420        0.0001             nan     0.2206   -0.0000
   440        0.0001             nan     0.2206   -0.0000
   460        0.0001             nan     0.2206   -0.0000
   480        0.0001             nan     0.2206   -0.0000
   500        0.0001             nan     0.2206   -0.0000
   520        0.0001             nan     0.2206   -0.0000
   540        0.0001             nan     0.2206   -0.0000
   560        0.0001             nan     0.2206   -0.0000
   580        0.0001             nan     0.2206   -0.0000
   600        0.0000             nan     0.2206   -0.0000
   620        0.0000             nan     0.2206   -0.0000
   640        0.0000             nan     0.2206   -0.0000
   660        0.0000             nan     0.2206   -0.0000
   680        0.0000             nan     0.2206   -0.0000
   700        0.0000             nan     0.2206   -0.0000
   720        0.0000             nan     0.2206   -0.0000
   740        0.0000             nan     0.2206   -0.0000
   760        0.0000             nan     0.2206   -0.0000
   780        0.0000             nan     0.2206   -0.0000
   800        0.0000             nan     0.2206   -0.0000
   820        0.0000             nan     0.2206   -0.0000
   840        0.0000             nan     0.2206   -0.0000
   860        0.0000             nan     0.2206   -0.0000
   880        0.0000             nan     0.2206   -0.0000
   900        0.0000             nan     0.2206   -0.0000
   920        0.0000             nan     0.2206   -0.0000
   940        0.0000             nan     0.2206   -0.0000
   960        0.0000             nan     0.2206   -0.0000
   980        0.0000             nan     0.2206   -0.0000
  1000        0.0000             nan     0.2206   -0.0000
  1020        0.0000             nan     0.2206   -0.0000
  1040        0.0000             nan     0.2206   -0.0000
  1060        0.0000             nan     0.2206   -0.0000
  1080        0.0000             nan     0.2206   -0.0000
  1100        0.0000             nan     0.2206   -0.0000
  1120        0.0000             nan     0.2206   -0.0000
  1140        0.0000             nan     0.2206   -0.0000
  1160        0.0000             nan     0.2206   -0.0000
  1180        0.0000             nan     0.2206   -0.0000
  1200        0.0000             nan     0.2206   -0.0000
  1220        0.0000             nan     0.2206   -0.0000
  1240        0.0000             nan     0.2206   -0.0000
  1260        0.0000             nan     0.2206   -0.0000
  1280        0.0000             nan     0.2206   -0.0000
  1300        0.0000             nan     0.2206   -0.0000
  1320        0.0000             nan     0.2206   -0.0000
  1340        0.0000             nan     0.2206   -0.0000
  1360        0.0000             nan     0.2206   -0.0000
  1380        0.0000             nan     0.2206   -0.0000
  1400        0.0000             nan     0.2206   -0.0000
  1420        0.0000             nan     0.2206   -0.0000
  1440        0.0000             nan     0.2206   -0.0000
  1460        0.0000             nan     0.2206   -0.0000
  1480        0.0000             nan     0.2206   -0.0000
  1500        0.0000             nan     0.2206   -0.0000
  1520        0.0000             nan     0.2206   -0.0000
  1540        0.0000             nan     0.2206    0.0000
  1560        0.0000             nan     0.2206   -0.0000
  1580        0.0000             nan     0.2206   -0.0000
  1600        0.0000             nan     0.2206   -0.0000
  1620        0.0000             nan     0.2206   -0.0000
  1640        0.0000             nan     0.2206   -0.0000
  1660        0.0000             nan     0.2206   -0.0000
  1680        0.0000             nan     0.2206   -0.0000
  1700        0.0000             nan     0.2206   -0.0000
  1720        0.0000             nan     0.2206   -0.0000
  1740        0.0000             nan     0.2206   -0.0000
  1760        0.0000             nan     0.2206   -0.0000
  1780        0.0000             nan     0.2206   -0.0000
  1800        0.0000             nan     0.2206   -0.0000
  1820        0.0000             nan     0.2206   -0.0000
  1840        0.0000             nan     0.2206   -0.0000
  1860        0.0000             nan     0.2206   -0.0000
  1880        0.0000             nan     0.2206   -0.0000
  1900        0.0000             nan     0.2206   -0.0000
  1920        0.0000             nan     0.2206   -0.0000
  1940        0.0000             nan     0.2206    0.0000
  1960        0.0000             nan     0.2206   -0.0000
  1980        0.0000             nan     0.2206   -0.0000
  2000        0.0000             nan     0.2206   -0.0000
  2020        0.0000             nan     0.2206   -0.0000
  2040        0.0000             nan     0.2206   -0.0000
  2060        0.0000             nan     0.2206   -0.0000
  2080        0.0000             nan     0.2206   -0.0000
  2100        0.0000             nan     0.2206   -0.0000
  2120        0.0000             nan     0.2206   -0.0000
  2140        0.0000             nan     0.2206   -0.0000
  2160        0.0000             nan     0.2206   -0.0000
  2180        0.0000             nan     0.2206   -0.0000
  2200        0.0000             nan     0.2206   -0.0000
  2220        0.0000             nan     0.2206   -0.0000
  2240        0.0000             nan     0.2206   -0.0000
  2260        0.0000             nan     0.2206   -0.0000
  2280        0.0000             nan     0.2206   -0.0000
  2300        0.0000             nan     0.2206   -0.0000
  2320        0.0000             nan     0.2206    0.0000
  2340        0.0000             nan     0.2206   -0.0000
  2360        0.0000             nan     0.2206   -0.0000
  2380        0.0000             nan     0.2206   -0.0000
  2400        0.0000             nan     0.2206   -0.0000
  2420        0.0000             nan     0.2206   -0.0000
  2440        0.0000             nan     0.2206   -0.0000
  2460        0.0000             nan     0.2206   -0.0000
  2480        0.0000             nan     0.2206   -0.0000
  2500        0.0000             nan     0.2206   -0.0000
  2520        0.0000             nan     0.2206   -0.0000
  2540        0.0000             nan     0.2206   -0.0000
  2560        0.0000             nan     0.2206   -0.0000
  2580        0.0000             nan     0.2206   -0.0000
  2600        0.0000             nan     0.2206   -0.0000
  2620        0.0000             nan     0.2206   -0.0000
  2640        0.0000             nan     0.2206   -0.0000
  2660        0.0000             nan     0.2206   -0.0000
  2680        0.0000             nan     0.2206   -0.0000
  2700        0.0000             nan     0.2206   -0.0000
  2720        0.0000             nan     0.2206    0.0000
  2740        0.0000             nan     0.2206    0.0000
  2760        0.0000             nan     0.2206   -0.0000
  2780        0.0000             nan     0.2206   -0.0000
  2800        0.0000             nan     0.2206   -0.0000
  2820        0.0000             nan     0.2206   -0.0000
  2840        0.0000             nan     0.2206   -0.0000
  2860        0.0000             nan     0.2206   -0.0000
  2880        0.0000             nan     0.2206   -0.0000
  2900        0.0000             nan     0.2206   -0.0000
  2920        0.0000             nan     0.2206   -0.0000
  2940        0.0000             nan     0.2206    0.0000
  2960        0.0000             nan     0.2206   -0.0000
  2980        0.0000             nan     0.2206   -0.0000
  3000        0.0000             nan     0.2206   -0.0000
  3020        0.0000             nan     0.2206    0.0000
  3040        0.0000             nan     0.2206   -0.0000
  3060        0.0000             nan     0.2206   -0.0000
  3080        0.0000             nan     0.2206   -0.0000
  3100        0.0000             nan     0.2206   -0.0000
  3120        0.0000             nan     0.2206   -0.0000
  3140        0.0000             nan     0.2206   -0.0000
  3160        0.0000             nan     0.2206   -0.0000
  3180        0.0000             nan     0.2206   -0.0000
  3200        0.0000             nan     0.2206   -0.0000
  3220        0.0000             nan     0.2206   -0.0000
  3222        0.0000             nan     0.2206   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2304             nan     0.2587    0.1753
     2        0.1317             nan     0.2587    0.1061
     3        0.0777             nan     0.2587    0.0548
     4        0.0457             nan     0.2587    0.0315
     5        0.0280             nan     0.2587    0.0178
     6        0.0176             nan     0.2587    0.0100
     7        0.0110             nan     0.2587    0.0057
     8        0.0076             nan     0.2587    0.0032
     9        0.0056             nan     0.2587    0.0020
    10        0.0044             nan     0.2587    0.0011
    20        0.0017             nan     0.2587   -0.0000
    40        0.0011             nan     0.2587   -0.0000
    60        0.0007             nan     0.2587   -0.0000
    80        0.0006             nan     0.2587   -0.0000
   100        0.0005             nan     0.2587   -0.0000
   120        0.0004             nan     0.2587   -0.0000
   140        0.0003             nan     0.2587   -0.0000
   160        0.0003             nan     0.2587   -0.0000
   180        0.0003             nan     0.2587   -0.0000
   200        0.0002             nan     0.2587   -0.0000
   220        0.0002             nan     0.2587   -0.0000
   240        0.0002             nan     0.2587   -0.0000
   260        0.0002             nan     0.2587   -0.0000
   280        0.0002             nan     0.2587   -0.0000
   300        0.0002             nan     0.2587   -0.0000
   320        0.0001             nan     0.2587   -0.0000
   340        0.0001             nan     0.2587   -0.0000
   360        0.0001             nan     0.2587   -0.0000
   380        0.0001             nan     0.2587   -0.0000
   400        0.0001             nan     0.2587   -0.0000
   420        0.0001             nan     0.2587   -0.0000
   440        0.0001             nan     0.2587   -0.0000
   460        0.0001             nan     0.2587   -0.0000
   480        0.0001             nan     0.2587   -0.0000
   500        0.0001             nan     0.2587   -0.0000
   520        0.0001             nan     0.2587   -0.0000
   540        0.0001             nan     0.2587   -0.0000
   560        0.0001             nan     0.2587   -0.0000
   580        0.0001             nan     0.2587   -0.0000
   600        0.0001             nan     0.2587   -0.0000
   620        0.0001             nan     0.2587    0.0000
   640        0.0001             nan     0.2587    0.0000
   660        0.0001             nan     0.2587   -0.0000
   680        0.0001             nan     0.2587   -0.0000
   700        0.0001             nan     0.2587   -0.0000
   720        0.0001             nan     0.2587   -0.0000
   740        0.0001             nan     0.2587   -0.0000
   760        0.0001             nan     0.2587   -0.0000
   780        0.0001             nan     0.2587   -0.0000
   800        0.0001             nan     0.2587   -0.0000
   820        0.0001             nan     0.2587   -0.0000
   840        0.0001             nan     0.2587   -0.0000
   860        0.0001             nan     0.2587   -0.0000
   880        0.0001             nan     0.2587   -0.0000
   900        0.0001             nan     0.2587   -0.0000
   920        0.0001             nan     0.2587   -0.0000
   940        0.0001             nan     0.2587   -0.0000
   960        0.0001             nan     0.2587   -0.0000
   980        0.0001             nan     0.2587   -0.0000
  1000        0.0001             nan     0.2587   -0.0000
  1020        0.0001             nan     0.2587   -0.0000
  1040        0.0001             nan     0.2587   -0.0000
  1060        0.0001             nan     0.2587   -0.0000
  1080        0.0001             nan     0.2587   -0.0000
  1100        0.0001             nan     0.2587   -0.0000
  1120        0.0001             nan     0.2587   -0.0000
  1140        0.0001             nan     0.2587    0.0000
  1160        0.0001             nan     0.2587   -0.0000
  1180        0.0001             nan     0.2587   -0.0000
  1200        0.0001             nan     0.2587   -0.0000
  1220        0.0001             nan     0.2587   -0.0000
  1240        0.0001             nan     0.2587   -0.0000
  1260        0.0001             nan     0.2587   -0.0000
  1280        0.0001             nan     0.2587   -0.0000
  1300        0.0001             nan     0.2587   -0.0000
  1320        0.0001             nan     0.2587   -0.0000
  1340        0.0001             nan     0.2587   -0.0000
  1360        0.0001             nan     0.2587   -0.0000
  1380        0.0001             nan     0.2587   -0.0000
  1400        0.0001             nan     0.2587   -0.0000
  1420        0.0001             nan     0.2587    0.0000
  1440        0.0001             nan     0.2587   -0.0000
  1460        0.0001             nan     0.2587   -0.0000
  1480        0.0001             nan     0.2587   -0.0000
  1500        0.0001             nan     0.2587   -0.0000
  1520        0.0001             nan     0.2587   -0.0000
  1540        0.0001             nan     0.2587   -0.0000
  1560        0.0001             nan     0.2587   -0.0000
  1580        0.0001             nan     0.2587   -0.0000
  1600        0.0001             nan     0.2587   -0.0000
  1620        0.0001             nan     0.2587   -0.0000
  1640        0.0001             nan     0.2587   -0.0000
  1660        0.0001             nan     0.2587    0.0000
  1680        0.0001             nan     0.2587   -0.0000
  1700        0.0001             nan     0.2587   -0.0000
  1720        0.0001             nan     0.2587   -0.0000
  1740        0.0001             nan     0.2587   -0.0000
  1760        0.0001             nan     0.2587   -0.0000
  1780        0.0001             nan     0.2587   -0.0000
  1800        0.0001             nan     0.2587   -0.0000
  1820        0.0001             nan     0.2587    0.0000
  1840        0.0001             nan     0.2587   -0.0000
  1860        0.0001             nan     0.2587   -0.0000
  1880        0.0001             nan     0.2587   -0.0000
  1900        0.0001             nan     0.2587   -0.0000
  1920        0.0001             nan     0.2587   -0.0000
  1940        0.0001             nan     0.2587   -0.0000
  1960        0.0001             nan     0.2587    0.0000
  1980        0.0001             nan     0.2587   -0.0000
  2000        0.0001             nan     0.2587    0.0000
  2020        0.0001             nan     0.2587   -0.0000
  2040        0.0001             nan     0.2587   -0.0000
  2060        0.0001             nan     0.2587   -0.0000
  2080        0.0001             nan     0.2587    0.0000
  2100        0.0001             nan     0.2587   -0.0000
  2120        0.0001             nan     0.2587   -0.0000
  2140        0.0001             nan     0.2587   -0.0000
  2160        0.0001             nan     0.2587   -0.0000
  2180        0.0001             nan     0.2587   -0.0000
  2200        0.0001             nan     0.2587   -0.0000
  2220        0.0001             nan     0.2587   -0.0000
  2240        0.0001             nan     0.2587    0.0000
  2260        0.0001             nan     0.2587   -0.0000
  2280        0.0001             nan     0.2587    0.0000
  2300        0.0001             nan     0.2587   -0.0000
  2320        0.0001             nan     0.2587   -0.0000
  2340        0.0001             nan     0.2587   -0.0000
  2360        0.0001             nan     0.2587   -0.0000
  2380        0.0001             nan     0.2587   -0.0000
  2400        0.0001             nan     0.2587   -0.0000
  2420        0.0001             nan     0.2587   -0.0000
  2440        0.0001             nan     0.2587   -0.0000
  2460        0.0001             nan     0.2587   -0.0000
  2480        0.0001             nan     0.2587    0.0000
  2500        0.0001             nan     0.2587   -0.0000
  2520        0.0001             nan     0.2587   -0.0000
  2540        0.0001             nan     0.2587   -0.0000
  2560        0.0001             nan     0.2587    0.0000
  2580        0.0001             nan     0.2587   -0.0000
  2600        0.0001             nan     0.2587   -0.0000
  2620        0.0001             nan     0.2587    0.0000
  2640        0.0001             nan     0.2587   -0.0000
  2660        0.0001             nan     0.2587   -0.0000
  2680        0.0001             nan     0.2587   -0.0000
  2700        0.0001             nan     0.2587   -0.0000
  2720        0.0001             nan     0.2587    0.0000
  2740        0.0001             nan     0.2587   -0.0000
  2760        0.0001             nan     0.2587    0.0000
  2780        0.0001             nan     0.2587    0.0000
  2800        0.0001             nan     0.2587   -0.0000
  2820        0.0001             nan     0.2587   -0.0000
  2840        0.0001             nan     0.2587   -0.0000
  2860        0.0001             nan     0.2587    0.0000
  2880        0.0001             nan     0.2587   -0.0000
  2900        0.0001             nan     0.2587    0.0000
  2920        0.0001             nan     0.2587    0.0000
  2940        0.0001             nan     0.2587    0.0000
  2960        0.0001             nan     0.2587   -0.0000
  2980        0.0001             nan     0.2587   -0.0000
  3000        0.0001             nan     0.2587   -0.0000
  3020        0.0001             nan     0.2587   -0.0000
  3022        0.0001             nan     0.2587   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2204             nan     0.2980    0.1976
     2        0.1295             nan     0.2980    0.0937
     3        0.0766             nan     0.2980    0.0487
     4        0.0474             nan     0.2980    0.0289
     5        0.0304             nan     0.2980    0.0153
     6        0.0199             nan     0.2980    0.0096
     7        0.0136             nan     0.2980    0.0057
     8        0.0095             nan     0.2980    0.0034
     9        0.0073             nan     0.2980    0.0016
    10        0.0059             nan     0.2980    0.0011
    20        0.0032             nan     0.2980   -0.0000
    40        0.0019             nan     0.2980    0.0000
    60        0.0014             nan     0.2980   -0.0000
    80        0.0011             nan     0.2980   -0.0000
   100        0.0009             nan     0.2980   -0.0000
   120        0.0008             nan     0.2980   -0.0000
   140        0.0007             nan     0.2980   -0.0000
   160        0.0006             nan     0.2980   -0.0000
   180        0.0005             nan     0.2980   -0.0000
   200        0.0005             nan     0.2980   -0.0000
   220        0.0004             nan     0.2980   -0.0000
   240        0.0004             nan     0.2980   -0.0000
   260        0.0004             nan     0.2980   -0.0000
   280        0.0003             nan     0.2980   -0.0000
   300        0.0003             nan     0.2980   -0.0000
   320        0.0003             nan     0.2980   -0.0000
   340        0.0003             nan     0.2980   -0.0000
   360        0.0003             nan     0.2980   -0.0000
   380        0.0003             nan     0.2980   -0.0000
   400        0.0002             nan     0.2980   -0.0000
   420        0.0002             nan     0.2980   -0.0000
   440        0.0002             nan     0.2980   -0.0000
   460        0.0002             nan     0.2980   -0.0000
   480        0.0002             nan     0.2980   -0.0000
   500        0.0002             nan     0.2980   -0.0000
   520        0.0002             nan     0.2980   -0.0000
   540        0.0002             nan     0.2980   -0.0000
   560        0.0002             nan     0.2980   -0.0000
   580        0.0002             nan     0.2980   -0.0000
   600        0.0001             nan     0.2980   -0.0000
   620        0.0001             nan     0.2980   -0.0000
   640        0.0001             nan     0.2980   -0.0000
   660        0.0001             nan     0.2980   -0.0000
   680        0.0001             nan     0.2980   -0.0000
   700        0.0001             nan     0.2980   -0.0000
   720        0.0001             nan     0.2980   -0.0000
   740        0.0001             nan     0.2980   -0.0000
   760        0.0001             nan     0.2980   -0.0000
   780        0.0001             nan     0.2980   -0.0000
   800        0.0001             nan     0.2980   -0.0000
   820        0.0001             nan     0.2980   -0.0000
   840        0.0001             nan     0.2980   -0.0000
   860        0.0001             nan     0.2980   -0.0000
   880        0.0001             nan     0.2980   -0.0000
   900        0.0001             nan     0.2980   -0.0000
   920        0.0001             nan     0.2980   -0.0000
   940        0.0001             nan     0.2980   -0.0000
   960        0.0001             nan     0.2980   -0.0000
   980        0.0001             nan     0.2980   -0.0000
  1000        0.0001             nan     0.2980   -0.0000
  1020        0.0001             nan     0.2980   -0.0000
  1040        0.0001             nan     0.2980   -0.0000
  1060        0.0001             nan     0.2980   -0.0000
  1080        0.0001             nan     0.2980   -0.0000
  1100        0.0001             nan     0.2980   -0.0000
  1120        0.0001             nan     0.2980   -0.0000
  1140        0.0001             nan     0.2980   -0.0000
  1160        0.0001             nan     0.2980   -0.0000
  1180        0.0001             nan     0.2980   -0.0000
  1200        0.0001             nan     0.2980   -0.0000
  1220        0.0001             nan     0.2980   -0.0000
  1240        0.0001             nan     0.2980   -0.0000
  1260        0.0001             nan     0.2980   -0.0000
  1280        0.0001             nan     0.2980   -0.0000
  1300        0.0001             nan     0.2980   -0.0000
  1320        0.0001             nan     0.2980   -0.0000
  1340        0.0001             nan     0.2980   -0.0000
  1360        0.0001             nan     0.2980   -0.0000
  1380        0.0001             nan     0.2980   -0.0000
  1400        0.0001             nan     0.2980   -0.0000
  1420        0.0001             nan     0.2980   -0.0000
  1440        0.0001             nan     0.2980   -0.0000
  1460        0.0001             nan     0.2980   -0.0000
  1480        0.0001             nan     0.2980   -0.0000
  1500        0.0001             nan     0.2980   -0.0000
  1520        0.0000             nan     0.2980   -0.0000
  1540        0.0000             nan     0.2980   -0.0000
  1560        0.0000             nan     0.2980   -0.0000
  1580        0.0000             nan     0.2980   -0.0000
  1600        0.0000             nan     0.2980   -0.0000
  1620        0.0000             nan     0.2980   -0.0000
  1640        0.0000             nan     0.2980   -0.0000
  1660        0.0000             nan     0.2980   -0.0000
  1680        0.0000             nan     0.2980   -0.0000
  1700        0.0000             nan     0.2980   -0.0000
  1720        0.0000             nan     0.2980   -0.0000
  1740        0.0000             nan     0.2980   -0.0000
  1760        0.0000             nan     0.2980   -0.0000
  1780        0.0000             nan     0.2980   -0.0000
  1800        0.0000             nan     0.2980   -0.0000
  1820        0.0000             nan     0.2980   -0.0000
  1840        0.0000             nan     0.2980   -0.0000
  1860        0.0000             nan     0.2980   -0.0000
  1880        0.0000             nan     0.2980   -0.0000
  1900        0.0000             nan     0.2980   -0.0000
  1920        0.0000             nan     0.2980   -0.0000
  1940        0.0000             nan     0.2980   -0.0000
  1960        0.0000             nan     0.2980   -0.0000
  1980        0.0000             nan     0.2980   -0.0000
  2000        0.0000             nan     0.2980   -0.0000
  2020        0.0000             nan     0.2980   -0.0000
  2040        0.0000             nan     0.2980   -0.0000
  2060        0.0000             nan     0.2980   -0.0000
  2080        0.0000             nan     0.2980   -0.0000
  2100        0.0000             nan     0.2980   -0.0000
  2120        0.0000             nan     0.2980   -0.0000
  2140        0.0000             nan     0.2980   -0.0000
  2160        0.0000             nan     0.2980    0.0000
  2180        0.0000             nan     0.2980    0.0000
  2200        0.0000             nan     0.2980   -0.0000
  2220        0.0000             nan     0.2980   -0.0000
  2240        0.0000             nan     0.2980   -0.0000
  2260        0.0000             nan     0.2980   -0.0000
  2280        0.0000             nan     0.2980   -0.0000
  2300        0.0000             nan     0.2980   -0.0000
  2320        0.0000             nan     0.2980   -0.0000
  2340        0.0000             nan     0.2980   -0.0000
  2360        0.0000             nan     0.2980    0.0000
  2380        0.0000             nan     0.2980    0.0000
  2400        0.0000             nan     0.2980   -0.0000
  2420        0.0000             nan     0.2980   -0.0000
  2440        0.0000             nan     0.2980    0.0000
  2460        0.0000             nan     0.2980   -0.0000
  2480        0.0000             nan     0.2980   -0.0000
  2500        0.0000             nan     0.2980   -0.0000
  2520        0.0000             nan     0.2980   -0.0000
  2540        0.0000             nan     0.2980    0.0000
  2560        0.0000             nan     0.2980    0.0000
  2580        0.0000             nan     0.2980   -0.0000
  2600        0.0000             nan     0.2980   -0.0000
  2620        0.0000             nan     0.2980   -0.0000
  2640        0.0000             nan     0.2980   -0.0000
  2660        0.0000             nan     0.2980   -0.0000
  2680        0.0000             nan     0.2980   -0.0000
  2700        0.0000             nan     0.2980   -0.0000
  2720        0.0000             nan     0.2980    0.0000
  2740        0.0000             nan     0.2980   -0.0000
  2760        0.0000             nan     0.2980   -0.0000
  2780        0.0000             nan     0.2980   -0.0000
  2800        0.0000             nan     0.2980   -0.0000
  2820        0.0000             nan     0.2980   -0.0000
  2840        0.0000             nan     0.2980   -0.0000
  2860        0.0000             nan     0.2980   -0.0000
  2880        0.0000             nan     0.2980   -0.0000
  2900        0.0000             nan     0.2980   -0.0000
  2920        0.0000             nan     0.2980   -0.0000
  2940        0.0000             nan     0.2980   -0.0000
  2960        0.0000             nan     0.2980   -0.0000
  2980        0.0000             nan     0.2980    0.0000
  3000        0.0000             nan     0.2980   -0.0000
  3020        0.0000             nan     0.2980   -0.0000
  3040        0.0000             nan     0.2980   -0.0000
  3060        0.0000             nan     0.2980   -0.0000
  3080        0.0000             nan     0.2980   -0.0000
  3100        0.0000             nan     0.2980   -0.0000
  3120        0.0000             nan     0.2980    0.0000
  3140        0.0000             nan     0.2980   -0.0000
  3160        0.0000             nan     0.2980   -0.0000
  3180        0.0000             nan     0.2980   -0.0000
  3200        0.0000             nan     0.2980   -0.0000
  3220        0.0000             nan     0.2980   -0.0000
  3240        0.0000             nan     0.2980   -0.0000
  3260        0.0000             nan     0.2980   -0.0000
  3280        0.0000             nan     0.2980   -0.0000
  3300        0.0000             nan     0.2980   -0.0000
  3320        0.0000             nan     0.2980   -0.0000
  3340        0.0000             nan     0.2980    0.0000
  3360        0.0000             nan     0.2980   -0.0000
  3380        0.0000             nan     0.2980   -0.0000
  3400        0.0000             nan     0.2980   -0.0000
  3420        0.0000             nan     0.2980   -0.0000
  3440        0.0000             nan     0.2980   -0.0000
  3460        0.0000             nan     0.2980   -0.0000
  3480        0.0000             nan     0.2980   -0.0000
  3500        0.0000             nan     0.2980   -0.0000
  3520        0.0000             nan     0.2980   -0.0000
  3540        0.0000             nan     0.2980    0.0000
  3560        0.0000             nan     0.2980   -0.0000
  3580        0.0000             nan     0.2980   -0.0000
  3600        0.0000             nan     0.2980   -0.0000
  3620        0.0000             nan     0.2980   -0.0000
  3640        0.0000             nan     0.2980   -0.0000
  3660        0.0000             nan     0.2980   -0.0000
  3680        0.0000             nan     0.2980   -0.0000
  3700        0.0000             nan     0.2980   -0.0000
  3720        0.0000             nan     0.2980   -0.0000
  3740        0.0000             nan     0.2980   -0.0000
  3760        0.0000             nan     0.2980   -0.0000
  3780        0.0000             nan     0.2980    0.0000
  3800        0.0000             nan     0.2980   -0.0000
  3820        0.0000             nan     0.2980   -0.0000
  3840        0.0000             nan     0.2980    0.0000
  3860        0.0000             nan     0.2980   -0.0000
  3880        0.0000             nan     0.2980   -0.0000
  3900        0.0000             nan     0.2980   -0.0000
  3920        0.0000             nan     0.2980   -0.0000
  3940        0.0000             nan     0.2980   -0.0000
  3960        0.0000             nan     0.2980   -0.0000
  3980        0.0000             nan     0.2980    0.0000
  4000        0.0000             nan     0.2980   -0.0000
  4020        0.0000             nan     0.2980   -0.0000
  4040        0.0000             nan     0.2980   -0.0000
  4060        0.0000             nan     0.2980    0.0000
  4080        0.0000             nan     0.2980   -0.0000
  4100        0.0000             nan     0.2980   -0.0000
  4120        0.0000             nan     0.2980   -0.0000
  4121        0.0000             nan     0.2980   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1914             nan     0.3332    0.2025
     2        0.0929             nan     0.3332    0.0971
     3        0.0463             nan     0.3332    0.0460
     4        0.0242             nan     0.3332    0.0221
     5        0.0133             nan     0.3332    0.0105
     6        0.0078             nan     0.3332    0.0061
     7        0.0050             nan     0.3332    0.0024
     8        0.0037             nan     0.3332    0.0014
     9        0.0029             nan     0.3332    0.0005
    10        0.0026             nan     0.3332    0.0003
    20        0.0015             nan     0.3332   -0.0001
    40        0.0009             nan     0.3332   -0.0000
    60        0.0006             nan     0.3332   -0.0000
    80        0.0005             nan     0.3332   -0.0000
   100        0.0004             nan     0.3332   -0.0000
   120        0.0003             nan     0.3332   -0.0000
   140        0.0003             nan     0.3332   -0.0000
   160        0.0002             nan     0.3332   -0.0000
   180        0.0002             nan     0.3332   -0.0000
   200        0.0002             nan     0.3332   -0.0000
   220        0.0001             nan     0.3332   -0.0000
   240        0.0001             nan     0.3332   -0.0000
   260        0.0001             nan     0.3332   -0.0000
   280        0.0001             nan     0.3332   -0.0000
   300        0.0001             nan     0.3332   -0.0000
   320        0.0001             nan     0.3332   -0.0000
   340        0.0001             nan     0.3332   -0.0000
   360        0.0001             nan     0.3332   -0.0000
   380        0.0001             nan     0.3332   -0.0000
   400        0.0001             nan     0.3332   -0.0000
   420        0.0001             nan     0.3332   -0.0000
   440        0.0001             nan     0.3332   -0.0000
   460        0.0000             nan     0.3332   -0.0000
   480        0.0000             nan     0.3332   -0.0000
   500        0.0000             nan     0.3332   -0.0000
   520        0.0000             nan     0.3332   -0.0000
   540        0.0000             nan     0.3332    0.0000
   560        0.0000             nan     0.3332   -0.0000
   580        0.0000             nan     0.3332   -0.0000
   600        0.0000             nan     0.3332   -0.0000
   620        0.0000             nan     0.3332   -0.0000
   640        0.0000             nan     0.3332   -0.0000
   660        0.0000             nan     0.3332   -0.0000
   680        0.0000             nan     0.3332    0.0000
   700        0.0000             nan     0.3332   -0.0000
   720        0.0000             nan     0.3332   -0.0000
   740        0.0000             nan     0.3332   -0.0000
   760        0.0000             nan     0.3332   -0.0000
   780        0.0000             nan     0.3332   -0.0000
   800        0.0000             nan     0.3332   -0.0000
   820        0.0000             nan     0.3332   -0.0000
   840        0.0000             nan     0.3332   -0.0000
   860        0.0000             nan     0.3332   -0.0000
   880        0.0000             nan     0.3332   -0.0000
   900        0.0000             nan     0.3332   -0.0000
   920        0.0000             nan     0.3332   -0.0000
   940        0.0000             nan     0.3332   -0.0000
   960        0.0000             nan     0.3332   -0.0000
   980        0.0000             nan     0.3332   -0.0000
  1000        0.0000             nan     0.3332    0.0000
  1020        0.0000             nan     0.3332    0.0000
  1040        0.0000             nan     0.3332   -0.0000
  1060        0.0000             nan     0.3332   -0.0000
  1080        0.0000             nan     0.3332    0.0000
  1100        0.0000             nan     0.3332   -0.0000
  1120        0.0000             nan     0.3332   -0.0000
  1140        0.0000             nan     0.3332   -0.0000
  1160        0.0000             nan     0.3332   -0.0000
  1180        0.0000             nan     0.3332   -0.0000
  1200        0.0000             nan     0.3332   -0.0000
  1220        0.0000             nan     0.3332   -0.0000
  1240        0.0000             nan     0.3332   -0.0000
  1260        0.0000             nan     0.3332   -0.0000
  1280        0.0000             nan     0.3332   -0.0000
  1300        0.0000             nan     0.3332   -0.0000
  1320        0.0000             nan     0.3332   -0.0000
  1340        0.0000             nan     0.3332   -0.0000
  1360        0.0000             nan     0.3332   -0.0000
  1380        0.0000             nan     0.3332   -0.0000
  1400        0.0000             nan     0.3332   -0.0000
  1420        0.0000             nan     0.3332   -0.0000
  1440        0.0000             nan     0.3332    0.0000
  1460        0.0000             nan     0.3332   -0.0000
  1480        0.0000             nan     0.3332   -0.0000
  1500        0.0000             nan     0.3332   -0.0000
  1520        0.0000             nan     0.3332   -0.0000
  1540        0.0000             nan     0.3332   -0.0000
  1560        0.0000             nan     0.3332   -0.0000
  1580        0.0000             nan     0.3332    0.0000
  1600        0.0000             nan     0.3332   -0.0000
  1620        0.0000             nan     0.3332   -0.0000
  1640        0.0000             nan     0.3332   -0.0000
  1660        0.0000             nan     0.3332   -0.0000
  1680        0.0000             nan     0.3332   -0.0000
  1700        0.0000             nan     0.3332   -0.0000
  1720        0.0000             nan     0.3332   -0.0000
  1740        0.0000             nan     0.3332   -0.0000
  1760        0.0000             nan     0.3332    0.0000
  1780        0.0000             nan     0.3332   -0.0000
  1800        0.0000             nan     0.3332    0.0000
  1820        0.0000             nan     0.3332   -0.0000
  1840        0.0000             nan     0.3332   -0.0000
  1860        0.0000             nan     0.3332   -0.0000
  1880        0.0000             nan     0.3332   -0.0000
  1900        0.0000             nan     0.3332   -0.0000
  1920        0.0000             nan     0.3332   -0.0000
  1940        0.0000             nan     0.3332   -0.0000
  1960        0.0000             nan     0.3332   -0.0000
  1980        0.0000             nan     0.3332   -0.0000
  2000        0.0000             nan     0.3332   -0.0000
  2020        0.0000             nan     0.3332   -0.0000
  2040        0.0000             nan     0.3332   -0.0000
  2060        0.0000             nan     0.3332   -0.0000
  2080        0.0000             nan     0.3332    0.0000
  2100        0.0000             nan     0.3332   -0.0000
  2120        0.0000             nan     0.3332   -0.0000
  2140        0.0000             nan     0.3332   -0.0000
  2160        0.0000             nan     0.3332   -0.0000
  2180        0.0000             nan     0.3332   -0.0000
  2200        0.0000             nan     0.3332   -0.0000
  2220        0.0000             nan     0.3332    0.0000
  2240        0.0000             nan     0.3332   -0.0000
  2260        0.0000             nan     0.3332    0.0000
  2280        0.0000             nan     0.3332   -0.0000
  2300        0.0000             nan     0.3332    0.0000
  2320        0.0000             nan     0.3332   -0.0000
  2340        0.0000             nan     0.3332   -0.0000
  2360        0.0000             nan     0.3332   -0.0000
  2380        0.0000             nan     0.3332   -0.0000
  2400        0.0000             nan     0.3332    0.0000
  2420        0.0000             nan     0.3332   -0.0000
  2440        0.0000             nan     0.3332   -0.0000
  2460        0.0000             nan     0.3332   -0.0000
  2480        0.0000             nan     0.3332   -0.0000
  2500        0.0000             nan     0.3332   -0.0000
  2520        0.0000             nan     0.3332   -0.0000
  2540        0.0000             nan     0.3332   -0.0000
  2560        0.0000             nan     0.3332   -0.0000
  2580        0.0000             nan     0.3332   -0.0000
  2600        0.0000             nan     0.3332   -0.0000
  2620        0.0000             nan     0.3332   -0.0000
  2640        0.0000             nan     0.3332   -0.0000
  2660        0.0000             nan     0.3332    0.0000
  2680        0.0000             nan     0.3332   -0.0000
  2700        0.0000             nan     0.3332   -0.0000
  2720        0.0000             nan     0.3332   -0.0000
  2740        0.0000             nan     0.3332   -0.0000
  2760        0.0000             nan     0.3332   -0.0000
  2780        0.0000             nan     0.3332    0.0000
  2800        0.0000             nan     0.3332   -0.0000
  2820        0.0000             nan     0.3332   -0.0000
  2840        0.0000             nan     0.3332   -0.0000
  2860        0.0000             nan     0.3332   -0.0000
  2880        0.0000             nan     0.3332    0.0000
  2900        0.0000             nan     0.3332   -0.0000
  2920        0.0000             nan     0.3332   -0.0000
  2940        0.0000             nan     0.3332   -0.0000
  2960        0.0000             nan     0.3332    0.0000
  2966        0.0000             nan     0.3332    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1611             nan     0.3949    0.2643
     2        0.0698             nan     0.3949    0.0913
     3        0.0303             nan     0.3949    0.0389
     4        0.0139             nan     0.3949    0.0170
     5        0.0079             nan     0.3949    0.0060
     6        0.0049             nan     0.3949    0.0029
     7        0.0036             nan     0.3949    0.0011
     8        0.0030             nan     0.3949    0.0006
     9        0.0026             nan     0.3949    0.0002
    10        0.0023             nan     0.3949    0.0002
    20        0.0013             nan     0.3949   -0.0000
    40        0.0006             nan     0.3949   -0.0000
    60        0.0004             nan     0.3949   -0.0000
    80        0.0003             nan     0.3949   -0.0000
   100        0.0002             nan     0.3949   -0.0000
   120        0.0001             nan     0.3949   -0.0000
   140        0.0001             nan     0.3949   -0.0000
   160        0.0001             nan     0.3949   -0.0000
   180        0.0001             nan     0.3949   -0.0000
   200        0.0001             nan     0.3949   -0.0000
   220        0.0000             nan     0.3949   -0.0000
   240        0.0000             nan     0.3949   -0.0000
   260        0.0000             nan     0.3949   -0.0000
   280        0.0000             nan     0.3949   -0.0000
   300        0.0000             nan     0.3949   -0.0000
   320        0.0000             nan     0.3949   -0.0000
   340        0.0000             nan     0.3949   -0.0000
   360        0.0000             nan     0.3949   -0.0000
   380        0.0000             nan     0.3949   -0.0000
   400        0.0000             nan     0.3949   -0.0000
   420        0.0000             nan     0.3949   -0.0000
   440        0.0000             nan     0.3949   -0.0000
   460        0.0000             nan     0.3949   -0.0000
   480        0.0000             nan     0.3949   -0.0000
   500        0.0000             nan     0.3949   -0.0000
   520        0.0000             nan     0.3949   -0.0000
   540        0.0000             nan     0.3949   -0.0000
   560        0.0000             nan     0.3949   -0.0000
   580        0.0000             nan     0.3949   -0.0000
   600        0.0000             nan     0.3949   -0.0000
   620        0.0000             nan     0.3949   -0.0000
   640        0.0000             nan     0.3949   -0.0000
   660        0.0000             nan     0.3949   -0.0000
   680        0.0000             nan     0.3949   -0.0000
   700        0.0000             nan     0.3949   -0.0000
   720        0.0000             nan     0.3949   -0.0000
   740        0.0000             nan     0.3949   -0.0000
   760        0.0000             nan     0.3949   -0.0000
   780        0.0000             nan     0.3949   -0.0000
   800        0.0000             nan     0.3949   -0.0000
   820        0.0000             nan     0.3949   -0.0000
   840        0.0000             nan     0.3949   -0.0000
   860        0.0000             nan     0.3949   -0.0000
   880        0.0000             nan     0.3949   -0.0000
   900        0.0000             nan     0.3949   -0.0000
   920        0.0000             nan     0.3949   -0.0000
   940        0.0000             nan     0.3949   -0.0000
   960        0.0000             nan     0.3949   -0.0000
   980        0.0000             nan     0.3949   -0.0000
  1000        0.0000             nan     0.3949   -0.0000
  1020        0.0000             nan     0.3949   -0.0000
  1040        0.0000             nan     0.3949   -0.0000
  1060        0.0000             nan     0.3949   -0.0000
  1080        0.0000             nan     0.3949   -0.0000
  1100        0.0000             nan     0.3949   -0.0000
  1120        0.0000             nan     0.3949   -0.0000
  1140        0.0000             nan     0.3949   -0.0000
  1160        0.0000             nan     0.3949   -0.0000
  1168        0.0000             nan     0.3949   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1628             nan     0.4223    0.2261
     2        0.0728             nan     0.4223    0.0910
     3        0.0375             nan     0.4223    0.0370
     4        0.0205             nan     0.4223    0.0142
     5        0.0129             nan     0.4223    0.0079
     6        0.0092             nan     0.4223    0.0025
     7        0.0073             nan     0.4223    0.0018
     8        0.0066             nan     0.4223    0.0007
     9        0.0058             nan     0.4223    0.0006
    10        0.0056             nan     0.4223    0.0000
    20        0.0035             nan     0.4223   -0.0000
    40        0.0019             nan     0.4223    0.0000
    60        0.0014             nan     0.4223    0.0000
    80        0.0011             nan     0.4223   -0.0000
   100        0.0009             nan     0.4223   -0.0000
   120        0.0008             nan     0.4223   -0.0000
   140        0.0007             nan     0.4223   -0.0000
   160        0.0006             nan     0.4223   -0.0000
   180        0.0005             nan     0.4223   -0.0000
   200        0.0005             nan     0.4223   -0.0000
   220        0.0004             nan     0.4223   -0.0000
   240        0.0004             nan     0.4223   -0.0000
   260        0.0004             nan     0.4223   -0.0000
   280        0.0003             nan     0.4223   -0.0000
   300        0.0003             nan     0.4223   -0.0000
   320        0.0003             nan     0.4223   -0.0000
   340        0.0002             nan     0.4223   -0.0000
   360        0.0002             nan     0.4223   -0.0000
   380        0.0002             nan     0.4223   -0.0000
   400        0.0002             nan     0.4223   -0.0000
   420        0.0002             nan     0.4223   -0.0000
   440        0.0002             nan     0.4223   -0.0000
   460        0.0002             nan     0.4223   -0.0000
   480        0.0002             nan     0.4223   -0.0000
   500        0.0002             nan     0.4223   -0.0000
   520        0.0001             nan     0.4223   -0.0000
   540        0.0001             nan     0.4223   -0.0000
   560        0.0001             nan     0.4223   -0.0000
   580        0.0001             nan     0.4223   -0.0000
   600        0.0001             nan     0.4223   -0.0000
   620        0.0001             nan     0.4223   -0.0000
   640        0.0001             nan     0.4223   -0.0000
   660        0.0001             nan     0.4223   -0.0000
   680        0.0001             nan     0.4223   -0.0000
   700        0.0001             nan     0.4223   -0.0000
   720        0.0001             nan     0.4223   -0.0000
   740        0.0001             nan     0.4223   -0.0000
   760        0.0001             nan     0.4223   -0.0000
   780        0.0001             nan     0.4223   -0.0000
   800        0.0001             nan     0.4223   -0.0000
   820        0.0001             nan     0.4223   -0.0000
   840        0.0001             nan     0.4223   -0.0000
   860        0.0001             nan     0.4223   -0.0000
   880        0.0001             nan     0.4223   -0.0000
   900        0.0001             nan     0.4223   -0.0000
   920        0.0001             nan     0.4223   -0.0000
   940        0.0001             nan     0.4223   -0.0000
   960        0.0001             nan     0.4223   -0.0000
   980        0.0001             nan     0.4223   -0.0000
  1000        0.0001             nan     0.4223   -0.0000
  1020        0.0001             nan     0.4223   -0.0000
  1040        0.0001             nan     0.4223    0.0000
  1060        0.0001             nan     0.4223   -0.0000
  1080        0.0001             nan     0.4223   -0.0000
  1100        0.0001             nan     0.4223   -0.0000
  1120        0.0001             nan     0.4223   -0.0000
  1140        0.0001             nan     0.4223   -0.0000
  1160        0.0000             nan     0.4223   -0.0000
  1180        0.0000             nan     0.4223   -0.0000
  1200        0.0000             nan     0.4223   -0.0000
  1220        0.0000             nan     0.4223   -0.0000
  1240        0.0000             nan     0.4223   -0.0000
  1260        0.0000             nan     0.4223   -0.0000
  1280        0.0000             nan     0.4223   -0.0000
  1300        0.0000             nan     0.4223   -0.0000
  1320        0.0000             nan     0.4223   -0.0000
  1340        0.0000             nan     0.4223   -0.0000
  1360        0.0000             nan     0.4223   -0.0000
  1380        0.0000             nan     0.4223   -0.0000
  1400        0.0000             nan     0.4223   -0.0000
  1420        0.0000             nan     0.4223   -0.0000
  1440        0.0000             nan     0.4223   -0.0000
  1460        0.0000             nan     0.4223    0.0000
  1480        0.0000             nan     0.4223   -0.0000
  1500        0.0000             nan     0.4223   -0.0000
  1520        0.0000             nan     0.4223   -0.0000
  1540        0.0000             nan     0.4223   -0.0000
  1560        0.0000             nan     0.4223   -0.0000
  1580        0.0000             nan     0.4223   -0.0000
  1600        0.0000             nan     0.4223   -0.0000
  1620        0.0000             nan     0.4223   -0.0000
  1640        0.0000             nan     0.4223   -0.0000
  1660        0.0000             nan     0.4223   -0.0000
  1680        0.0000             nan     0.4223   -0.0000
  1700        0.0000             nan     0.4223   -0.0000
  1720        0.0000             nan     0.4223    0.0000
  1740        0.0000             nan     0.4223    0.0000
  1760        0.0000             nan     0.4223   -0.0000
  1780        0.0000             nan     0.4223   -0.0000
  1800        0.0000             nan     0.4223   -0.0000
  1820        0.0000             nan     0.4223   -0.0000
  1840        0.0000             nan     0.4223   -0.0000
  1860        0.0000             nan     0.4223   -0.0000
  1880        0.0000             nan     0.4223   -0.0000
  1900        0.0000             nan     0.4223   -0.0000
  1920        0.0000             nan     0.4223   -0.0000
  1940        0.0000             nan     0.4223    0.0000
  1960        0.0000             nan     0.4223   -0.0000
  1980        0.0000             nan     0.4223   -0.0000
  2000        0.0000             nan     0.4223   -0.0000
  2020        0.0000             nan     0.4223   -0.0000
  2040        0.0000             nan     0.4223   -0.0000
  2060        0.0000             nan     0.4223    0.0000
  2080        0.0000             nan     0.4223   -0.0000
  2100        0.0000             nan     0.4223   -0.0000
  2120        0.0000             nan     0.4223   -0.0000
  2140        0.0000             nan     0.4223   -0.0000
  2160        0.0000             nan     0.4223   -0.0000
  2180        0.0000             nan     0.4223   -0.0000
  2200        0.0000             nan     0.4223   -0.0000
  2220        0.0000             nan     0.4223   -0.0000
  2240        0.0000             nan     0.4223   -0.0000
  2260        0.0000             nan     0.4223   -0.0000
  2280        0.0000             nan     0.4223   -0.0000
  2300        0.0000             nan     0.4223   -0.0000
  2320        0.0000             nan     0.4223   -0.0000
  2340        0.0000             nan     0.4223   -0.0000
  2360        0.0000             nan     0.4223    0.0000
  2380        0.0000             nan     0.4223   -0.0000
  2400        0.0000             nan     0.4223   -0.0000
  2420        0.0000             nan     0.4223    0.0000
  2440        0.0000             nan     0.4223    0.0000
  2460        0.0000             nan     0.4223   -0.0000
  2480        0.0000             nan     0.4223   -0.0000
  2500        0.0000             nan     0.4223    0.0000
  2520        0.0000             nan     0.4223    0.0000
  2540        0.0000             nan     0.4223   -0.0000
  2560        0.0000             nan     0.4223   -0.0000
  2580        0.0000             nan     0.4223   -0.0000
  2600        0.0000             nan     0.4223   -0.0000
  2620        0.0000             nan     0.4223   -0.0000
  2640        0.0000             nan     0.4223   -0.0000
  2660        0.0000             nan     0.4223    0.0000
  2680        0.0000             nan     0.4223   -0.0000
  2700        0.0000             nan     0.4223    0.0000
  2720        0.0000             nan     0.4223   -0.0000
  2740        0.0000             nan     0.4223   -0.0000
  2760        0.0000             nan     0.4223   -0.0000
  2780        0.0000             nan     0.4223   -0.0000
  2800        0.0000             nan     0.4223   -0.0000
  2820        0.0000             nan     0.4223   -0.0000
  2840        0.0000             nan     0.4223    0.0000
  2860        0.0000             nan     0.4223   -0.0000
  2880        0.0000             nan     0.4223   -0.0000
  2900        0.0000             nan     0.4223    0.0000
  2920        0.0000             nan     0.4223   -0.0000
  2940        0.0000             nan     0.4223   -0.0000
  2960        0.0000             nan     0.4223   -0.0000
  2980        0.0000             nan     0.4223   -0.0000
  3000        0.0000             nan     0.4223   -0.0000
  3020        0.0000             nan     0.4223   -0.0000
  3040        0.0000             nan     0.4223   -0.0000
  3060        0.0000             nan     0.4223   -0.0000
  3080        0.0000             nan     0.4223   -0.0000
  3100        0.0000             nan     0.4223   -0.0000
  3120        0.0000             nan     0.4223   -0.0000
  3140        0.0000             nan     0.4223   -0.0000
  3160        0.0000             nan     0.4223   -0.0000
  3180        0.0000             nan     0.4223   -0.0000
  3200        0.0000             nan     0.4223   -0.0000
  3220        0.0000             nan     0.4223   -0.0000
  3240        0.0000             nan     0.4223   -0.0000
  3260        0.0000             nan     0.4223   -0.0000
  3280        0.0000             nan     0.4223   -0.0000
  3300        0.0000             nan     0.4223   -0.0000
  3320        0.0000             nan     0.4223   -0.0000
  3340        0.0000             nan     0.4223   -0.0000
  3360        0.0000             nan     0.4223   -0.0000
  3380        0.0000             nan     0.4223   -0.0000
  3400        0.0000             nan     0.4223   -0.0000
  3420        0.0000             nan     0.4223   -0.0000
  3440        0.0000             nan     0.4223   -0.0000
  3460        0.0000             nan     0.4223   -0.0000
  3480        0.0000             nan     0.4223   -0.0000
  3500        0.0000             nan     0.4223   -0.0000
  3520        0.0000             nan     0.4223   -0.0000
  3540        0.0000             nan     0.4223   -0.0000
  3560        0.0000             nan     0.4223   -0.0000
  3580        0.0000             nan     0.4223   -0.0000
  3600        0.0000             nan     0.4223   -0.0000
  3620        0.0000             nan     0.4223   -0.0000
  3640        0.0000             nan     0.4223   -0.0000
  3660        0.0000             nan     0.4223    0.0000
  3680        0.0000             nan     0.4223   -0.0000
  3700        0.0000             nan     0.4223   -0.0000
  3720        0.0000             nan     0.4223   -0.0000
  3740        0.0000             nan     0.4223   -0.0000
  3760        0.0000             nan     0.4223   -0.0000
  3780        0.0000             nan     0.4223    0.0000
  3800        0.0000             nan     0.4223   -0.0000
  3820        0.0000             nan     0.4223   -0.0000
  3840        0.0000             nan     0.4223   -0.0000
  3860        0.0000             nan     0.4223   -0.0000
  3880        0.0000             nan     0.4223   -0.0000
  3900        0.0000             nan     0.4223   -0.0000
  3920        0.0000             nan     0.4223   -0.0000
  3940        0.0000             nan     0.4223   -0.0000
  3960        0.0000             nan     0.4223    0.0000
  3980        0.0000             nan     0.4223   -0.0000
  4000        0.0000             nan     0.4223   -0.0000
  4020        0.0000             nan     0.4223   -0.0000
  4040        0.0000             nan     0.4223   -0.0000
  4060        0.0000             nan     0.4223   -0.0000
  4080        0.0000             nan     0.4223   -0.0000
  4100        0.0000             nan     0.4223   -0.0000
  4120        0.0000             nan     0.4223   -0.0000
  4131        0.0000             nan     0.4223   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1329             nan     0.4469    0.2529
     2        0.0486             nan     0.4469    0.0820
     3        0.0187             nan     0.4469    0.0286
     4        0.0083             nan     0.4469    0.0096
     5        0.0046             nan     0.4469    0.0035
     6        0.0032             nan     0.4469    0.0011
     7        0.0027             nan     0.4469    0.0004
     8        0.0024             nan     0.4469    0.0002
     9        0.0022             nan     0.4469   -0.0001
    10        0.0022             nan     0.4469   -0.0001
    20        0.0014             nan     0.4469   -0.0001
    40        0.0008             nan     0.4469   -0.0000
    60        0.0005             nan     0.4469   -0.0000
    80        0.0004             nan     0.4469   -0.0000
   100        0.0003             nan     0.4469   -0.0000
   120        0.0002             nan     0.4469   -0.0000
   140        0.0002             nan     0.4469   -0.0000
   160        0.0001             nan     0.4469   -0.0000
   180        0.0001             nan     0.4469   -0.0000
   200        0.0001             nan     0.4469   -0.0000
   220        0.0001             nan     0.4469   -0.0000
   240        0.0001             nan     0.4469   -0.0000
   260        0.0001             nan     0.4469   -0.0000
   280        0.0001             nan     0.4469    0.0000
   300        0.0000             nan     0.4469   -0.0000
   320        0.0000             nan     0.4469   -0.0000
   340        0.0000             nan     0.4469   -0.0000
   360        0.0000             nan     0.4469   -0.0000
   380        0.0000             nan     0.4469   -0.0000
   400        0.0000             nan     0.4469   -0.0000
   420        0.0000             nan     0.4469   -0.0000
   440        0.0000             nan     0.4469   -0.0000
   460        0.0000             nan     0.4469   -0.0000
   480        0.0000             nan     0.4469   -0.0000
   500        0.0000             nan     0.4469   -0.0000
   520        0.0000             nan     0.4469   -0.0000
   540        0.0000             nan     0.4469   -0.0000
   560        0.0000             nan     0.4469   -0.0000
   580        0.0000             nan     0.4469   -0.0000
   600        0.0000             nan     0.4469   -0.0000
   620        0.0000             nan     0.4469   -0.0000
   640        0.0000             nan     0.4469    0.0000
   660        0.0000             nan     0.4469   -0.0000
   680        0.0000             nan     0.4469   -0.0000
   700        0.0000             nan     0.4469   -0.0000
   720        0.0000             nan     0.4469   -0.0000
   740        0.0000             nan     0.4469    0.0000
   760        0.0000             nan     0.4469   -0.0000
   780        0.0000             nan     0.4469    0.0000
   800        0.0000             nan     0.4469   -0.0000
   820        0.0000             nan     0.4469   -0.0000
   840        0.0000             nan     0.4469   -0.0000
   860        0.0000             nan     0.4469   -0.0000
   880        0.0000             nan     0.4469   -0.0000
   900        0.0000             nan     0.4469   -0.0000
   920        0.0000             nan     0.4469   -0.0000
   940        0.0000             nan     0.4469   -0.0000
   960        0.0000             nan     0.4469    0.0000
   980        0.0000             nan     0.4469   -0.0000
  1000        0.0000             nan     0.4469   -0.0000
  1020        0.0000             nan     0.4469   -0.0000
  1040        0.0000             nan     0.4469   -0.0000
  1060        0.0000             nan     0.4469   -0.0000
  1080        0.0000             nan     0.4469   -0.0000
  1100        0.0000             nan     0.4469   -0.0000
  1120        0.0000             nan     0.4469   -0.0000
  1140        0.0000             nan     0.4469   -0.0000
  1160        0.0000             nan     0.4469   -0.0000
  1180        0.0000             nan     0.4469    0.0000
  1200        0.0000             nan     0.4469   -0.0000
  1220        0.0000             nan     0.4469    0.0000
  1240        0.0000             nan     0.4469   -0.0000
  1260        0.0000             nan     0.4469   -0.0000
  1280        0.0000             nan     0.4469    0.0000
  1300        0.0000             nan     0.4469   -0.0000
  1320        0.0000             nan     0.4469   -0.0000
  1340        0.0000             nan     0.4469   -0.0000
  1360        0.0000             nan     0.4469    0.0000
  1380        0.0000             nan     0.4469   -0.0000
  1400        0.0000             nan     0.4469    0.0000
  1420        0.0000             nan     0.4469   -0.0000
  1440        0.0000             nan     0.4469   -0.0000
  1460        0.0000             nan     0.4469   -0.0000
  1480        0.0000             nan     0.4469   -0.0000
  1500        0.0000             nan     0.4469    0.0000
  1520        0.0000             nan     0.4469    0.0000
  1540        0.0000             nan     0.4469   -0.0000
  1560        0.0000             nan     0.4469    0.0000
  1580        0.0000             nan     0.4469   -0.0000
  1600        0.0000             nan     0.4469   -0.0000
  1620        0.0000             nan     0.4469   -0.0000
  1640        0.0000             nan     0.4469   -0.0000
  1660        0.0000             nan     0.4469   -0.0000
  1680        0.0000             nan     0.4469   -0.0000
  1700        0.0000             nan     0.4469   -0.0000
  1713        0.0000             nan     0.4469   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1528             nan     0.4473    0.2527
     2        0.0595             nan     0.4473    0.0928
     3        0.0270             nan     0.4473    0.0325
     4        0.0129             nan     0.4473    0.0141
     5        0.0082             nan     0.4473    0.0049
     6        0.0060             nan     0.4473    0.0015
     7        0.0051             nan     0.4473    0.0005
     8        0.0045             nan     0.4473    0.0006
     9        0.0039             nan     0.4473    0.0003
    10        0.0034             nan     0.4473    0.0004
    20        0.0020             nan     0.4473   -0.0001
    40        0.0010             nan     0.4473   -0.0000
    60        0.0006             nan     0.4473   -0.0000
    80        0.0004             nan     0.4473   -0.0000
   100        0.0003             nan     0.4473   -0.0000
   120        0.0002             nan     0.4473   -0.0000
   140        0.0002             nan     0.4473   -0.0000
   160        0.0002             nan     0.4473   -0.0000
   180        0.0001             nan     0.4473   -0.0000
   200        0.0001             nan     0.4473   -0.0000
   220        0.0001             nan     0.4473   -0.0000
   240        0.0001             nan     0.4473   -0.0000
   260        0.0001             nan     0.4473   -0.0000
   280        0.0000             nan     0.4473   -0.0000
   300        0.0000             nan     0.4473   -0.0000
   320        0.0000             nan     0.4473   -0.0000
   340        0.0000             nan     0.4473   -0.0000
   360        0.0000             nan     0.4473   -0.0000
   380        0.0000             nan     0.4473   -0.0000
   400        0.0000             nan     0.4473   -0.0000
   420        0.0000             nan     0.4473   -0.0000
   440        0.0000             nan     0.4473   -0.0000
   460        0.0000             nan     0.4473   -0.0000
   480        0.0000             nan     0.4473   -0.0000
   500        0.0000             nan     0.4473   -0.0000
   520        0.0000             nan     0.4473   -0.0000
   540        0.0000             nan     0.4473   -0.0000
   560        0.0000             nan     0.4473   -0.0000
   580        0.0000             nan     0.4473   -0.0000
   600        0.0000             nan     0.4473   -0.0000
   620        0.0000             nan     0.4473   -0.0000
   640        0.0000             nan     0.4473   -0.0000
   660        0.0000             nan     0.4473   -0.0000
   680        0.0000             nan     0.4473   -0.0000
   700        0.0000             nan     0.4473   -0.0000
   720        0.0000             nan     0.4473   -0.0000
   740        0.0000             nan     0.4473   -0.0000
   760        0.0000             nan     0.4473   -0.0000
   780        0.0000             nan     0.4473   -0.0000
   800        0.0000             nan     0.4473   -0.0000
   820        0.0000             nan     0.4473   -0.0000
   840        0.0000             nan     0.4473   -0.0000
   860        0.0000             nan     0.4473   -0.0000
   880        0.0000             nan     0.4473   -0.0000
   900        0.0000             nan     0.4473   -0.0000
   920        0.0000             nan     0.4473   -0.0000
   940        0.0000             nan     0.4473   -0.0000
   960        0.0000             nan     0.4473   -0.0000
   980        0.0000             nan     0.4473   -0.0000
  1000        0.0000             nan     0.4473   -0.0000
  1020        0.0000             nan     0.4473   -0.0000
  1040        0.0000             nan     0.4473   -0.0000
  1060        0.0000             nan     0.4473   -0.0000
  1080        0.0000             nan     0.4473   -0.0000
  1100        0.0000             nan     0.4473   -0.0000
  1120        0.0000             nan     0.4473   -0.0000
  1140        0.0000             nan     0.4473   -0.0000
  1160        0.0000             nan     0.4473   -0.0000
  1180        0.0000             nan     0.4473   -0.0000
  1200        0.0000             nan     0.4473   -0.0000
  1220        0.0000             nan     0.4473   -0.0000
  1240        0.0000             nan     0.4473   -0.0000
  1260        0.0000             nan     0.4473   -0.0000
  1280        0.0000             nan     0.4473   -0.0000
  1300        0.0000             nan     0.4473   -0.0000
  1320        0.0000             nan     0.4473   -0.0000
  1340        0.0000             nan     0.4473   -0.0000
  1360        0.0000             nan     0.4473   -0.0000
  1380        0.0000             nan     0.4473   -0.0000
  1400        0.0000             nan     0.4473   -0.0000
  1420        0.0000             nan     0.4473   -0.0000
  1440        0.0000             nan     0.4473   -0.0000
  1460        0.0000             nan     0.4473   -0.0000
  1480        0.0000             nan     0.4473   -0.0000
  1500        0.0000             nan     0.4473   -0.0000
  1520        0.0000             nan     0.4473   -0.0000
  1540        0.0000             nan     0.4473   -0.0000
  1560        0.0000             nan     0.4473   -0.0000
  1580        0.0000             nan     0.4473   -0.0000
  1600        0.0000             nan     0.4473   -0.0000
  1620        0.0000             nan     0.4473   -0.0000
  1640        0.0000             nan     0.4473   -0.0000
  1660        0.0000             nan     0.4473   -0.0000
  1680        0.0000             nan     0.4473   -0.0000
  1700        0.0000             nan     0.4473   -0.0000
  1720        0.0000             nan     0.4473   -0.0000
  1740        0.0000             nan     0.4473   -0.0000
  1760        0.0000             nan     0.4473   -0.0000
  1780        0.0000             nan     0.4473    0.0000
  1800        0.0000             nan     0.4473   -0.0000
  1820        0.0000             nan     0.4473   -0.0000
  1840        0.0000             nan     0.4473   -0.0000
  1860        0.0000             nan     0.4473   -0.0000
  1880        0.0000             nan     0.4473   -0.0000
  1900        0.0000             nan     0.4473   -0.0000
  1920        0.0000             nan     0.4473   -0.0000
  1940        0.0000             nan     0.4473   -0.0000
  1960        0.0000             nan     0.4473   -0.0000
  1980        0.0000             nan     0.4473    0.0000
  2000        0.0000             nan     0.4473    0.0000
  2020        0.0000             nan     0.4473   -0.0000
  2040        0.0000             nan     0.4473    0.0000
  2060        0.0000             nan     0.4473   -0.0000
  2080        0.0000             nan     0.4473   -0.0000
  2100        0.0000             nan     0.4473   -0.0000
  2120        0.0000             nan     0.4473   -0.0000
  2140        0.0000             nan     0.4473   -0.0000
  2160        0.0000             nan     0.4473   -0.0000
  2180        0.0000             nan     0.4473   -0.0000
  2200        0.0000             nan     0.4473   -0.0000
  2220        0.0000             nan     0.4473   -0.0000
  2240        0.0000             nan     0.4473    0.0000
  2260        0.0000             nan     0.4473   -0.0000
  2280        0.0000             nan     0.4473    0.0000
  2300        0.0000             nan     0.4473    0.0000
  2320        0.0000             nan     0.4473   -0.0000
  2340        0.0000             nan     0.4473   -0.0000
  2360        0.0000             nan     0.4473   -0.0000
  2380        0.0000             nan     0.4473   -0.0000
  2400        0.0000             nan     0.4473   -0.0000
  2420        0.0000             nan     0.4473    0.0000
  2440        0.0000             nan     0.4473    0.0000
  2460        0.0000             nan     0.4473    0.0000
  2480        0.0000             nan     0.4473   -0.0000
  2500        0.0000             nan     0.4473    0.0000
  2520        0.0000             nan     0.4473   -0.0000
  2540        0.0000             nan     0.4473   -0.0000
  2560        0.0000             nan     0.4473   -0.0000
  2580        0.0000             nan     0.4473   -0.0000
  2600        0.0000             nan     0.4473    0.0000
  2620        0.0000             nan     0.4473   -0.0000
  2640        0.0000             nan     0.4473   -0.0000
  2660        0.0000             nan     0.4473    0.0000
  2680        0.0000             nan     0.4473   -0.0000
  2700        0.0000             nan     0.4473   -0.0000
  2720        0.0000             nan     0.4473   -0.0000
  2740        0.0000             nan     0.4473   -0.0000
  2760        0.0000             nan     0.4473    0.0000
  2780        0.0000             nan     0.4473   -0.0000
  2800        0.0000             nan     0.4473    0.0000
  2820        0.0000             nan     0.4473    0.0000
  2840        0.0000             nan     0.4473   -0.0000
  2860        0.0000             nan     0.4473    0.0000
  2880        0.0000             nan     0.4473    0.0000
  2900        0.0000             nan     0.4473   -0.0000
  2920        0.0000             nan     0.4473   -0.0000
  2940        0.0000             nan     0.4473   -0.0000
  2960        0.0000             nan     0.4473   -0.0000
  2980        0.0000             nan     0.4473   -0.0000
  3000        0.0000             nan     0.4473    0.0000
  3020        0.0000             nan     0.4473   -0.0000
  3040        0.0000             nan     0.4473   -0.0000
  3060        0.0000             nan     0.4473   -0.0000
  3080        0.0000             nan     0.4473   -0.0000
  3100        0.0000             nan     0.4473    0.0000
  3120        0.0000             nan     0.4473   -0.0000
  3140        0.0000             nan     0.4473   -0.0000
  3160        0.0000             nan     0.4473    0.0000
  3180        0.0000             nan     0.4473   -0.0000
  3200        0.0000             nan     0.4473   -0.0000
  3220        0.0000             nan     0.4473    0.0000
  3240        0.0000             nan     0.4473    0.0000
  3260        0.0000             nan     0.4473    0.0000
  3280        0.0000             nan     0.4473    0.0000
  3300        0.0000             nan     0.4473   -0.0000
  3320        0.0000             nan     0.4473   -0.0000
  3340        0.0000             nan     0.4473   -0.0000
  3360        0.0000             nan     0.4473   -0.0000
  3380        0.0000             nan     0.4473   -0.0000
  3400        0.0000             nan     0.4473    0.0000
  3420        0.0000             nan     0.4473   -0.0000
  3440        0.0000             nan     0.4473   -0.0000
  3460        0.0000             nan     0.4473    0.0000
  3480        0.0000             nan     0.4473    0.0000
  3500        0.0000             nan     0.4473   -0.0000
  3520        0.0000             nan     0.4473    0.0000
  3540        0.0000             nan     0.4473   -0.0000
  3560        0.0000             nan     0.4473   -0.0000
  3580        0.0000             nan     0.4473   -0.0000
  3600        0.0000             nan     0.4473   -0.0000
  3620        0.0000             nan     0.4473    0.0000
  3640        0.0000             nan     0.4473    0.0000
  3660        0.0000             nan     0.4473   -0.0000
  3680        0.0000             nan     0.4473   -0.0000
  3700        0.0000             nan     0.4473   -0.0000
  3720        0.0000             nan     0.4473    0.0000
  3740        0.0000             nan     0.4473   -0.0000
  3760        0.0000             nan     0.4473   -0.0000
  3780        0.0000             nan     0.4473   -0.0000
  3800        0.0000             nan     0.4473    0.0000
  3820        0.0000             nan     0.4473   -0.0000
  3840        0.0000             nan     0.4473   -0.0000
  3855        0.0000             nan     0.4473   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1301             nan     0.4875    0.2940
     2        0.0484             nan     0.4875    0.0863
     3        0.0207             nan     0.4875    0.0290
     4        0.0106             nan     0.4875    0.0098
     5        0.0064             nan     0.4875    0.0030
     6        0.0048             nan     0.4875    0.0010
     7        0.0041             nan     0.4875    0.0004
     8        0.0034             nan     0.4875    0.0002
     9        0.0031             nan     0.4875    0.0001
    10        0.0029             nan     0.4875   -0.0001
    20        0.0018             nan     0.4875   -0.0001
    40        0.0010             nan     0.4875   -0.0001
    60        0.0007             nan     0.4875   -0.0000
    80        0.0005             nan     0.4875   -0.0000
   100        0.0004             nan     0.4875   -0.0000
   120        0.0003             nan     0.4875   -0.0000
   140        0.0002             nan     0.4875   -0.0000
   160        0.0002             nan     0.4875   -0.0000
   180        0.0002             nan     0.4875   -0.0000
   200        0.0001             nan     0.4875   -0.0000
   220        0.0001             nan     0.4875   -0.0000
   240        0.0001             nan     0.4875   -0.0000
   260        0.0001             nan     0.4875   -0.0000
   280        0.0001             nan     0.4875   -0.0000
   300        0.0001             nan     0.4875   -0.0000
   320        0.0000             nan     0.4875   -0.0000
   340        0.0000             nan     0.4875   -0.0000
   360        0.0000             nan     0.4875   -0.0000
   380        0.0000             nan     0.4875    0.0000
   400        0.0000             nan     0.4875   -0.0000
   420        0.0000             nan     0.4875   -0.0000
   440        0.0000             nan     0.4875   -0.0000
   460        0.0000             nan     0.4875   -0.0000
   480        0.0000             nan     0.4875   -0.0000
   500        0.0000             nan     0.4875   -0.0000
   520        0.0000             nan     0.4875   -0.0000
   540        0.0000             nan     0.4875    0.0000
   560        0.0000             nan     0.4875   -0.0000
   580        0.0000             nan     0.4875   -0.0000
   600        0.0000             nan     0.4875   -0.0000
   620        0.0000             nan     0.4875   -0.0000
   640        0.0000             nan     0.4875   -0.0000
   660        0.0000             nan     0.4875    0.0000
   680        0.0000             nan     0.4875    0.0000
   700        0.0000             nan     0.4875   -0.0000
   720        0.0000             nan     0.4875   -0.0000
   740        0.0000             nan     0.4875   -0.0000
   760        0.0000             nan     0.4875    0.0000
   780        0.0000             nan     0.4875   -0.0000
   800        0.0000             nan     0.4875   -0.0000
   820        0.0000             nan     0.4875    0.0000
   840        0.0000             nan     0.4875   -0.0000
   860        0.0000             nan     0.4875   -0.0000
   880        0.0000             nan     0.4875   -0.0000
   900        0.0000             nan     0.4875   -0.0000
   920        0.0000             nan     0.4875   -0.0000
   940        0.0000             nan     0.4875    0.0000
   960        0.0000             nan     0.4875   -0.0000
   980        0.0000             nan     0.4875   -0.0000
  1000        0.0000             nan     0.4875    0.0000
  1020        0.0000             nan     0.4875   -0.0000
  1040        0.0000             nan     0.4875   -0.0000
  1060        0.0000             nan     0.4875   -0.0000
  1080        0.0000             nan     0.4875   -0.0000
  1100        0.0000             nan     0.4875   -0.0000
  1120        0.0000             nan     0.4875   -0.0000
  1140        0.0000             nan     0.4875   -0.0000
  1160        0.0000             nan     0.4875    0.0000
  1180        0.0000             nan     0.4875   -0.0000
  1200        0.0000             nan     0.4875    0.0000
  1220        0.0000             nan     0.4875   -0.0000
  1240        0.0000             nan     0.4875   -0.0000
  1260        0.0000             nan     0.4875   -0.0000
  1280        0.0000             nan     0.4875   -0.0000
  1300        0.0000             nan     0.4875   -0.0000
  1320        0.0000             nan     0.4875   -0.0000
  1340        0.0000             nan     0.4875   -0.0000
  1360        0.0000             nan     0.4875   -0.0000
  1380        0.0000             nan     0.4875   -0.0000
  1400        0.0000             nan     0.4875   -0.0000
  1420        0.0000             nan     0.4875   -0.0000
  1440        0.0000             nan     0.4875   -0.0000
  1460        0.0000             nan     0.4875   -0.0000
  1480        0.0000             nan     0.4875   -0.0000
  1500        0.0000             nan     0.4875   -0.0000
  1520        0.0000             nan     0.4875   -0.0000
  1540        0.0000             nan     0.4875   -0.0000
  1560        0.0000             nan     0.4875   -0.0000
  1580        0.0000             nan     0.4875    0.0000
  1600        0.0000             nan     0.4875    0.0000
  1620        0.0000             nan     0.4875   -0.0000
  1640        0.0000             nan     0.4875    0.0000
  1660        0.0000             nan     0.4875   -0.0000
  1680        0.0000             nan     0.4875    0.0000
  1700        0.0000             nan     0.4875   -0.0000
  1720        0.0000             nan     0.4875    0.0000
  1740        0.0000             nan     0.4875   -0.0000
  1760        0.0000             nan     0.4875   -0.0000
  1780        0.0000             nan     0.4875   -0.0000
  1800        0.0000             nan     0.4875   -0.0000
  1820        0.0000             nan     0.4875   -0.0000
  1840        0.0000             nan     0.4875   -0.0000
  1860        0.0000             nan     0.4875   -0.0000
  1880        0.0000             nan     0.4875    0.0000
  1900        0.0000             nan     0.4875   -0.0000
  1920        0.0000             nan     0.4875   -0.0000
  1940        0.0000             nan     0.4875   -0.0000
  1960        0.0000             nan     0.4875   -0.0000
  1980        0.0000             nan     0.4875   -0.0000
  2000        0.0000             nan     0.4875   -0.0000
  2020        0.0000             nan     0.4875   -0.0000
  2040        0.0000             nan     0.4875   -0.0000
  2060        0.0000             nan     0.4875   -0.0000
  2080        0.0000             nan     0.4875    0.0000
  2100        0.0000             nan     0.4875   -0.0000
  2120        0.0000             nan     0.4875   -0.0000
  2140        0.0000             nan     0.4875   -0.0000
  2160        0.0000             nan     0.4875   -0.0000
  2180        0.0000             nan     0.4875   -0.0000
  2200        0.0000             nan     0.4875   -0.0000
  2220        0.0000             nan     0.4875   -0.0000
  2240        0.0000             nan     0.4875   -0.0000
  2260        0.0000             nan     0.4875   -0.0000
  2280        0.0000             nan     0.4875    0.0000
  2300        0.0000             nan     0.4875   -0.0000
  2320        0.0000             nan     0.4875   -0.0000
  2340        0.0000             nan     0.4875   -0.0000
  2360        0.0000             nan     0.4875   -0.0000
  2380        0.0000             nan     0.4875    0.0000
  2400        0.0000             nan     0.4875   -0.0000
  2420        0.0000             nan     0.4875   -0.0000
  2440        0.0000             nan     0.4875   -0.0000
  2460        0.0000             nan     0.4875    0.0000
  2480        0.0000             nan     0.4875   -0.0000
  2500        0.0000             nan     0.4875    0.0000
  2520        0.0000             nan     0.4875   -0.0000
  2540        0.0000             nan     0.4875   -0.0000
  2560        0.0000             nan     0.4875   -0.0000
  2580        0.0000             nan     0.4875   -0.0000
  2600        0.0000             nan     0.4875   -0.0000
  2620        0.0000             nan     0.4875   -0.0000
  2640        0.0000             nan     0.4875   -0.0000
  2660        0.0000             nan     0.4875   -0.0000
  2680        0.0000             nan     0.4875   -0.0000
  2700        0.0000             nan     0.4875    0.0000
  2720        0.0000             nan     0.4875   -0.0000
  2740        0.0000             nan     0.4875   -0.0000
  2760        0.0000             nan     0.4875    0.0000
  2780        0.0000             nan     0.4875   -0.0000
  2800        0.0000             nan     0.4875   -0.0000
  2820        0.0000             nan     0.4875   -0.0000
  2840        0.0000             nan     0.4875   -0.0000
  2860        0.0000             nan     0.4875   -0.0000
  2880        0.0000             nan     0.4875    0.0000
  2900        0.0000             nan     0.4875   -0.0000
  2920        0.0000             nan     0.4875   -0.0000
  2940        0.0000             nan     0.4875   -0.0000
  2960        0.0000             nan     0.4875   -0.0000
  2980        0.0000             nan     0.4875   -0.0000
  3000        0.0000             nan     0.4875   -0.0000
  3020        0.0000             nan     0.4875   -0.0000
  3040        0.0000             nan     0.4875   -0.0000
  3060        0.0000             nan     0.4875    0.0000
  3080        0.0000             nan     0.4875   -0.0000
  3100        0.0000             nan     0.4875   -0.0000
  3120        0.0000             nan     0.4875   -0.0000
  3140        0.0000             nan     0.4875   -0.0000
  3160        0.0000             nan     0.4875    0.0000
  3180        0.0000             nan     0.4875   -0.0000
  3200        0.0000             nan     0.4875   -0.0000
  3220        0.0000             nan     0.4875   -0.0000
  3240        0.0000             nan     0.4875    0.0000
  3260        0.0000             nan     0.4875   -0.0000
  3280        0.0000             nan     0.4875   -0.0000
  3300        0.0000             nan     0.4875   -0.0000
  3320        0.0000             nan     0.4875    0.0000
  3340        0.0000             nan     0.4875   -0.0000
  3360        0.0000             nan     0.4875   -0.0000
  3380        0.0000             nan     0.4875   -0.0000
  3400        0.0000             nan     0.4875    0.0000
  3420        0.0000             nan     0.4875   -0.0000
  3440        0.0000             nan     0.4875   -0.0000
  3460        0.0000             nan     0.4875   -0.0000
  3480        0.0000             nan     0.4875   -0.0000
  3500        0.0000             nan     0.4875   -0.0000
  3520        0.0000             nan     0.4875   -0.0000
  3540        0.0000             nan     0.4875   -0.0000
  3560        0.0000             nan     0.4875   -0.0000
  3580        0.0000             nan     0.4875   -0.0000
  3600        0.0000             nan     0.4875   -0.0000
  3620        0.0000             nan     0.4875   -0.0000
  3640        0.0000             nan     0.4875   -0.0000
  3660        0.0000             nan     0.4875   -0.0000
  3680        0.0000             nan     0.4875   -0.0000
  3700        0.0000             nan     0.4875    0.0000
  3720        0.0000             nan     0.4875   -0.0000
  3740        0.0000             nan     0.4875   -0.0000
  3760        0.0000             nan     0.4875   -0.0000
  3780        0.0000             nan     0.4875   -0.0000
  3800        0.0000             nan     0.4875   -0.0000
  3820        0.0000             nan     0.4875   -0.0000
  3840        0.0000             nan     0.4875   -0.0000
  3860        0.0000             nan     0.4875   -0.0000
  3880        0.0000             nan     0.4875   -0.0000
  3900        0.0000             nan     0.4875   -0.0000
  3920        0.0000             nan     0.4875   -0.0000
  3940        0.0000             nan     0.4875   -0.0000
  3960        0.0000             nan     0.4875   -0.0000
  3980        0.0000             nan     0.4875   -0.0000
  4000        0.0000             nan     0.4875   -0.0000
  4020        0.0000             nan     0.4875   -0.0000
  4040        0.0000             nan     0.4875   -0.0000
  4060        0.0000             nan     0.4875    0.0000
  4080        0.0000             nan     0.4875    0.0000
  4100        0.0000             nan     0.4875   -0.0000
  4120        0.0000             nan     0.4875    0.0000
  4140        0.0000             nan     0.4875   -0.0000
  4160        0.0000             nan     0.4875    0.0000
  4180        0.0000             nan     0.4875   -0.0000
  4200        0.0000             nan     0.4875   -0.0000
  4220        0.0000             nan     0.4875   -0.0000
  4240        0.0000             nan     0.4875   -0.0000
  4260        0.0000             nan     0.4875    0.0000
  4280        0.0000             nan     0.4875    0.0000
  4300        0.0000             nan     0.4875   -0.0000
  4320        0.0000             nan     0.4875   -0.0000
  4340        0.0000             nan     0.4875   -0.0000
  4360        0.0000             nan     0.4875   -0.0000
  4380        0.0000             nan     0.4875   -0.0000
  4400        0.0000             nan     0.4875   -0.0000
  4416        0.0000             nan     0.4875    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1008             nan     0.5240    0.2813
     2        0.0289             nan     0.5240    0.0745
     3        0.0100             nan     0.5240    0.0187
     4        0.0049             nan     0.5240    0.0042
     5        0.0032             nan     0.5240    0.0014
     6        0.0026             nan     0.5240    0.0005
     7        0.0022             nan     0.5240    0.0003
     8        0.0021             nan     0.5240   -0.0002
     9        0.0019             nan     0.5240    0.0000
    10        0.0018             nan     0.5240   -0.0002
    20        0.0012             nan     0.5240   -0.0001
    40        0.0006             nan     0.5240   -0.0000
    60        0.0005             nan     0.5240   -0.0000
    80        0.0003             nan     0.5240   -0.0000
   100        0.0002             nan     0.5240   -0.0000
   120        0.0001             nan     0.5240   -0.0000
   140        0.0001             nan     0.5240   -0.0000
   160        0.0001             nan     0.5240   -0.0000
   180        0.0001             nan     0.5240   -0.0000
   200        0.0001             nan     0.5240   -0.0000
   220        0.0001             nan     0.5240   -0.0000
   240        0.0000             nan     0.5240   -0.0000
   260        0.0000             nan     0.5240   -0.0000
   280        0.0000             nan     0.5240   -0.0000
   300        0.0000             nan     0.5240   -0.0000
   320        0.0000             nan     0.5240   -0.0000
   340        0.0000             nan     0.5240   -0.0000
   360        0.0000             nan     0.5240   -0.0000
   380        0.0000             nan     0.5240    0.0000
   400        0.0000             nan     0.5240   -0.0000
   420        0.0000             nan     0.5240   -0.0000
   440        0.0000             nan     0.5240   -0.0000
   460        0.0000             nan     0.5240   -0.0000
   480        0.0000             nan     0.5240   -0.0000
   500        0.0000             nan     0.5240    0.0000
   520        0.0000             nan     0.5240   -0.0000
   540        0.0000             nan     0.5240   -0.0000
   560        0.0000             nan     0.5240   -0.0000
   580        0.0000             nan     0.5240   -0.0000
   600        0.0000             nan     0.5240   -0.0000
   620        0.0000             nan     0.5240   -0.0000
   640        0.0000             nan     0.5240    0.0000
   660        0.0000             nan     0.5240   -0.0000
   680        0.0000             nan     0.5240   -0.0000
   700        0.0000             nan     0.5240   -0.0000
   720        0.0000             nan     0.5240   -0.0000
   740        0.0000             nan     0.5240    0.0000
   760        0.0000             nan     0.5240    0.0000
   780        0.0000             nan     0.5240   -0.0000
   800        0.0000             nan     0.5240   -0.0000
   820        0.0000             nan     0.5240   -0.0000
   840        0.0000             nan     0.5240   -0.0000
   860        0.0000             nan     0.5240   -0.0000
   880        0.0000             nan     0.5240    0.0000
   900        0.0000             nan     0.5240   -0.0000
   920        0.0000             nan     0.5240   -0.0000
   940        0.0000             nan     0.5240   -0.0000
   960        0.0000             nan     0.5240   -0.0000
   980        0.0000             nan     0.5240   -0.0000
  1000        0.0000             nan     0.5240   -0.0000
  1020        0.0000             nan     0.5240   -0.0000
  1040        0.0000             nan     0.5240   -0.0000
  1060        0.0000             nan     0.5240   -0.0000
  1080        0.0000             nan     0.5240   -0.0000
  1100        0.0000             nan     0.5240   -0.0000
  1120        0.0000             nan     0.5240   -0.0000
  1140        0.0000             nan     0.5240   -0.0000
  1160        0.0000             nan     0.5240   -0.0000
  1180        0.0000             nan     0.5240   -0.0000
  1200        0.0000             nan     0.5240    0.0000
  1220        0.0000             nan     0.5240    0.0000
  1240        0.0000             nan     0.5240   -0.0000
  1260        0.0000             nan     0.5240   -0.0000
  1280        0.0000             nan     0.5240   -0.0000
  1300        0.0000             nan     0.5240   -0.0000
  1320        0.0000             nan     0.5240    0.0000
  1340        0.0000             nan     0.5240   -0.0000
  1360        0.0000             nan     0.5240   -0.0000
  1380        0.0000             nan     0.5240    0.0000
  1400        0.0000             nan     0.5240   -0.0000
  1420        0.0000             nan     0.5240    0.0000
  1440        0.0000             nan     0.5240   -0.0000
  1460        0.0000             nan     0.5240    0.0000
  1480        0.0000             nan     0.5240   -0.0000
  1500        0.0000             nan     0.5240   -0.0000
  1520        0.0000             nan     0.5240   -0.0000
  1540        0.0000             nan     0.5240   -0.0000
  1560        0.0000             nan     0.5240    0.0000
  1580        0.0000             nan     0.5240   -0.0000
  1600        0.0000             nan     0.5240   -0.0000
  1620        0.0000             nan     0.5240    0.0000
  1640        0.0000             nan     0.5240   -0.0000
  1660        0.0000             nan     0.5240   -0.0000
  1680        0.0000             nan     0.5240   -0.0000
  1700        0.0000             nan     0.5240   -0.0000
  1720        0.0000             nan     0.5240    0.0000
  1740        0.0000             nan     0.5240   -0.0000
  1760        0.0000             nan     0.5240    0.0000
  1780        0.0000             nan     0.5240   -0.0000
  1800        0.0000             nan     0.5240    0.0000
  1820        0.0000             nan     0.5240   -0.0000
  1840        0.0000             nan     0.5240   -0.0000
  1860        0.0000             nan     0.5240   -0.0000
  1880        0.0000             nan     0.5240   -0.0000
  1900        0.0000             nan     0.5240   -0.0000
  1920        0.0000             nan     0.5240    0.0000
  1940        0.0000             nan     0.5240   -0.0000
  1960        0.0000             nan     0.5240   -0.0000
  1980        0.0000             nan     0.5240    0.0000
  2000        0.0000             nan     0.5240   -0.0000
  2020        0.0000             nan     0.5240   -0.0000
  2040        0.0000             nan     0.5240   -0.0000
  2060        0.0000             nan     0.5240   -0.0000
  2080        0.0000             nan     0.5240    0.0000
  2100        0.0000             nan     0.5240   -0.0000
  2120        0.0000             nan     0.5240   -0.0000
  2140        0.0000             nan     0.5240    0.0000
  2160        0.0000             nan     0.5240    0.0000
  2180        0.0000             nan     0.5240    0.0000
  2200        0.0000             nan     0.5240   -0.0000
  2220        0.0000             nan     0.5240    0.0000
  2240        0.0000             nan     0.5240   -0.0000
  2260        0.0000             nan     0.5240   -0.0000
  2280        0.0000             nan     0.5240   -0.0000
  2300        0.0000             nan     0.5240   -0.0000
  2320        0.0000             nan     0.5240    0.0000
  2340        0.0000             nan     0.5240   -0.0000
  2360        0.0000             nan     0.5240   -0.0000
  2380        0.0000             nan     0.5240   -0.0000
  2400        0.0000             nan     0.5240   -0.0000
  2420        0.0000             nan     0.5240   -0.0000
  2440        0.0000             nan     0.5240   -0.0000
  2460        0.0000             nan     0.5240   -0.0000
  2480        0.0000             nan     0.5240   -0.0000
  2500        0.0000             nan     0.5240   -0.0000
  2520        0.0000             nan     0.5240    0.0000
  2540        0.0000             nan     0.5240    0.0000
  2560        0.0000             nan     0.5240    0.0000
  2580        0.0000             nan     0.5240    0.0000
  2600        0.0000             nan     0.5240   -0.0000
  2620        0.0000             nan     0.5240    0.0000
  2640        0.0000             nan     0.5240   -0.0000
  2660        0.0000             nan     0.5240   -0.0000
  2680        0.0000             nan     0.5240   -0.0000
  2700        0.0000             nan     0.5240   -0.0000
  2720        0.0000             nan     0.5240   -0.0000
  2740        0.0000             nan     0.5240    0.0000
  2760        0.0000             nan     0.5240   -0.0000
  2780        0.0000             nan     0.5240   -0.0000
  2800        0.0000             nan     0.5240   -0.0000
  2820        0.0000             nan     0.5240   -0.0000
  2840        0.0000             nan     0.5240   -0.0000
  2860        0.0000             nan     0.5240    0.0000
  2880        0.0000             nan     0.5240    0.0000
  2900        0.0000             nan     0.5240    0.0000
  2920        0.0000             nan     0.5240   -0.0000
  2940        0.0000             nan     0.5240   -0.0000
  2960        0.0000             nan     0.5240    0.0000
  2980        0.0000             nan     0.5240   -0.0000
  3000        0.0000             nan     0.5240   -0.0000
  3020        0.0000             nan     0.5240   -0.0000
  3040        0.0000             nan     0.5240   -0.0000
  3060        0.0000             nan     0.5240   -0.0000
  3080        0.0000             nan     0.5240   -0.0000
  3100        0.0000             nan     0.5240   -0.0000
  3120        0.0000             nan     0.5240   -0.0000
  3140        0.0000             nan     0.5240   -0.0000
  3160        0.0000             nan     0.5240   -0.0000
  3180        0.0000             nan     0.5240   -0.0000
  3200        0.0000             nan     0.5240   -0.0000
  3220        0.0000             nan     0.5240   -0.0000
  3240        0.0000             nan     0.5240   -0.0000
  3260        0.0000             nan     0.5240    0.0000
  3280        0.0000             nan     0.5240   -0.0000
  3300        0.0000             nan     0.5240   -0.0000
  3320        0.0000             nan     0.5240    0.0000
  3340        0.0000             nan     0.5240   -0.0000
  3360        0.0000             nan     0.5240    0.0000
  3380        0.0000             nan     0.5240   -0.0000
  3400        0.0000             nan     0.5240   -0.0000
  3420        0.0000             nan     0.5240    0.0000
  3440        0.0000             nan     0.5240   -0.0000
  3460        0.0000             nan     0.5240   -0.0000
  3480        0.0000             nan     0.5240    0.0000
  3500        0.0000             nan     0.5240   -0.0000
  3520        0.0000             nan     0.5240   -0.0000
  3540        0.0000             nan     0.5240   -0.0000
  3560        0.0000             nan     0.5240   -0.0000
  3580        0.0000             nan     0.5240   -0.0000
  3600        0.0000             nan     0.5240   -0.0000
  3620        0.0000             nan     0.5240    0.0000
  3640        0.0000             nan     0.5240   -0.0000
  3660        0.0000             nan     0.5240   -0.0000
  3680        0.0000             nan     0.5240   -0.0000
  3700        0.0000             nan     0.5240   -0.0000
  3720        0.0000             nan     0.5240   -0.0000
  3740        0.0000             nan     0.5240   -0.0000
  3760        0.0000             nan     0.5240   -0.0000
  3780        0.0000             nan     0.5240    0.0000
  3800        0.0000             nan     0.5240   -0.0000
  3820        0.0000             nan     0.5240   -0.0000
  3840        0.0000             nan     0.5240    0.0000
  3860        0.0000             nan     0.5240   -0.0000
  3880        0.0000             nan     0.5240   -0.0000
  3900        0.0000             nan     0.5240   -0.0000
  3920        0.0000             nan     0.5240   -0.0000
  3940        0.0000             nan     0.5240   -0.0000
  3960        0.0000             nan     0.5240   -0.0000
  3980        0.0000             nan     0.5240   -0.0000
  4000        0.0000             nan     0.5240   -0.0000
  4020        0.0000             nan     0.5240   -0.0000
  4040        0.0000             nan     0.5240   -0.0000
  4060        0.0000             nan     0.5240   -0.0000
  4080        0.0000             nan     0.5240   -0.0000
  4100        0.0000             nan     0.5240   -0.0000
  4120        0.0000             nan     0.5240   -0.0000
  4140        0.0000             nan     0.5240   -0.0000
  4160        0.0000             nan     0.5240   -0.0000
  4180        0.0000             nan     0.5240   -0.0000
  4200        0.0000             nan     0.5240   -0.0000
  4216        0.0000             nan     0.5240   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1116             nan     0.5299    0.2950
     2        0.0402             nan     0.5299    0.0715
     3        0.0190             nan     0.5299    0.0212
     4        0.0124             nan     0.5299    0.0061
     5        0.0095             nan     0.5299    0.0016
     6        0.0083             nan     0.5299    0.0010
     7        0.0078             nan     0.5299    0.0003
     8        0.0071             nan     0.5299    0.0004
     9        0.0067             nan     0.5299    0.0002
    10        0.0064             nan     0.5299   -0.0001
    20        0.0044             nan     0.5299   -0.0001
    40        0.0028             nan     0.5299   -0.0001
    60        0.0020             nan     0.5299   -0.0000
    80        0.0015             nan     0.5299   -0.0001
   100        0.0012             nan     0.5299   -0.0001
   120        0.0010             nan     0.5299   -0.0001
   140        0.0009             nan     0.5299   -0.0000
   160        0.0008             nan     0.5299   -0.0000
   180        0.0007             nan     0.5299   -0.0000
   200        0.0006             nan     0.5299   -0.0000
   220        0.0005             nan     0.5299   -0.0000
   240        0.0005             nan     0.5299   -0.0000
   260        0.0004             nan     0.5299   -0.0000
   280        0.0004             nan     0.5299   -0.0000
   300        0.0004             nan     0.5299   -0.0000
   320        0.0003             nan     0.5299   -0.0000
   340        0.0003             nan     0.5299   -0.0000
   360        0.0003             nan     0.5299   -0.0000
   380        0.0003             nan     0.5299   -0.0000
   400        0.0002             nan     0.5299   -0.0000
   420        0.0002             nan     0.5299   -0.0000
   440        0.0002             nan     0.5299   -0.0000
   460        0.0002             nan     0.5299   -0.0000
   480        0.0002             nan     0.5299   -0.0000
   500        0.0002             nan     0.5299   -0.0000
   520        0.0002             nan     0.5299   -0.0000
   540        0.0001             nan     0.5299   -0.0000
   560        0.0001             nan     0.5299   -0.0000
   580        0.0001             nan     0.5299    0.0000
   600        0.0001             nan     0.5299   -0.0000
   620        0.0001             nan     0.5299   -0.0000
   640        0.0001             nan     0.5299   -0.0000
   660        0.0001             nan     0.5299   -0.0000
   680        0.0001             nan     0.5299   -0.0000
   700        0.0001             nan     0.5299   -0.0000
   720        0.0001             nan     0.5299   -0.0000
   740        0.0001             nan     0.5299   -0.0000
   760        0.0001             nan     0.5299   -0.0000
   780        0.0001             nan     0.5299   -0.0000
   800        0.0001             nan     0.5299   -0.0000
   820        0.0001             nan     0.5299   -0.0000
   840        0.0001             nan     0.5299   -0.0000
   860        0.0001             nan     0.5299   -0.0000
   880        0.0001             nan     0.5299   -0.0000
   900        0.0001             nan     0.5299   -0.0000
   920        0.0001             nan     0.5299   -0.0000
   940        0.0001             nan     0.5299   -0.0000
   960        0.0001             nan     0.5299   -0.0000
   980        0.0001             nan     0.5299   -0.0000
  1000        0.0001             nan     0.5299   -0.0000
  1020        0.0001             nan     0.5299   -0.0000
  1040        0.0001             nan     0.5299   -0.0000
  1060        0.0001             nan     0.5299   -0.0000
  1080        0.0001             nan     0.5299   -0.0000
  1100        0.0001             nan     0.5299    0.0000
  1120        0.0001             nan     0.5299   -0.0000
  1140        0.0001             nan     0.5299   -0.0000
  1160        0.0001             nan     0.5299   -0.0000
  1180        0.0001             nan     0.5299   -0.0000
  1200        0.0001             nan     0.5299   -0.0000
  1220        0.0001             nan     0.5299   -0.0000
  1240        0.0001             nan     0.5299   -0.0000
  1260        0.0001             nan     0.5299   -0.0000
  1280        0.0001             nan     0.5299   -0.0000
  1300        0.0001             nan     0.5299   -0.0000
  1320        0.0001             nan     0.5299   -0.0000
  1340        0.0001             nan     0.5299   -0.0000
  1360        0.0001             nan     0.5299   -0.0000
  1380        0.0001             nan     0.5299   -0.0000
  1400        0.0001             nan     0.5299   -0.0000
  1420        0.0001             nan     0.5299   -0.0000
  1440        0.0001             nan     0.5299   -0.0000
  1460        0.0001             nan     0.5299   -0.0000
  1480        0.0001             nan     0.5299   -0.0000
  1500        0.0001             nan     0.5299   -0.0000
  1520        0.0001             nan     0.5299   -0.0000
  1540        0.0001             nan     0.5299   -0.0000
  1560        0.0001             nan     0.5299   -0.0000
  1580        0.0001             nan     0.5299   -0.0000
  1600        0.0001             nan     0.5299    0.0000
  1620        0.0001             nan     0.5299   -0.0000
  1640        0.0001             nan     0.5299    0.0000
  1660        0.0001             nan     0.5299   -0.0000
  1680        0.0001             nan     0.5299   -0.0000
  1700        0.0001             nan     0.5299   -0.0000
  1720        0.0001             nan     0.5299   -0.0000
  1740        0.0001             nan     0.5299    0.0000
  1760        0.0001             nan     0.5299    0.0000
  1780        0.0001             nan     0.5299   -0.0000
  1800        0.0001             nan     0.5299   -0.0000
  1820        0.0001             nan     0.5299    0.0000
  1840        0.0001             nan     0.5299   -0.0000
  1860        0.0001             nan     0.5299   -0.0000
  1880        0.0001             nan     0.5299   -0.0000
  1900        0.0001             nan     0.5299   -0.0000
  1920        0.0001             nan     0.5299    0.0000
  1940        0.0001             nan     0.5299   -0.0000
  1960        0.0001             nan     0.5299   -0.0000
  1980        0.0001             nan     0.5299   -0.0000
  2000        0.0001             nan     0.5299   -0.0000
  2020        0.0001             nan     0.5299   -0.0000
  2040        0.0001             nan     0.5299   -0.0000
  2060        0.0001             nan     0.5299   -0.0000
  2080        0.0001             nan     0.5299   -0.0000
  2100        0.0001             nan     0.5299    0.0000
  2120        0.0001             nan     0.5299   -0.0000
  2140        0.0001             nan     0.5299   -0.0000
  2160        0.0001             nan     0.5299    0.0000
  2180        0.0001             nan     0.5299   -0.0000
  2200        0.0001             nan     0.5299    0.0000
  2220        0.0001             nan     0.5299   -0.0000
  2240        0.0001             nan     0.5299   -0.0000
  2260        0.0001             nan     0.5299   -0.0000
  2280        0.0001             nan     0.5299   -0.0000
  2300        0.0001             nan     0.5299    0.0000
  2320        0.0001             nan     0.5299   -0.0000
  2340        0.0001             nan     0.5299   -0.0000
  2360        0.0001             nan     0.5299   -0.0000
  2380        0.0001             nan     0.5299   -0.0000
  2400        0.0001             nan     0.5299   -0.0000
  2420        0.0001             nan     0.5299   -0.0000
  2440        0.0001             nan     0.5299   -0.0000
  2460        0.0001             nan     0.5299   -0.0000
  2480        0.0001             nan     0.5299   -0.0000
  2500        0.0001             nan     0.5299    0.0000
  2520        0.0001             nan     0.5299   -0.0000
  2540        0.0001             nan     0.5299   -0.0000
  2560        0.0001             nan     0.5299   -0.0000
  2580        0.0001             nan     0.5299    0.0000
  2600        0.0001             nan     0.5299   -0.0000
  2620        0.0001             nan     0.5299   -0.0000
  2640        0.0001             nan     0.5299    0.0000
  2660        0.0001             nan     0.5299   -0.0000
  2680        0.0001             nan     0.5299    0.0000
  2700        0.0001             nan     0.5299   -0.0000
  2720        0.0001             nan     0.5299   -0.0000
  2740        0.0001             nan     0.5299   -0.0000
  2760        0.0001             nan     0.5299   -0.0000
  2780        0.0001             nan     0.5299   -0.0000
  2800        0.0001             nan     0.5299   -0.0000
  2820        0.0001             nan     0.5299   -0.0000
  2840        0.0001             nan     0.5299   -0.0000
  2860        0.0001             nan     0.5299   -0.0000
  2880        0.0001             nan     0.5299    0.0000
  2900        0.0001             nan     0.5299   -0.0000
  2920        0.0001             nan     0.5299   -0.0000
  2940        0.0001             nan     0.5299   -0.0000
  2960        0.0001             nan     0.5299   -0.0000
  2980        0.0001             nan     0.5299   -0.0000
  3000        0.0001             nan     0.5299   -0.0000
  3020        0.0001             nan     0.5299   -0.0000
  3040        0.0001             nan     0.5299   -0.0000
  3060        0.0001             nan     0.5299   -0.0000
  3080        0.0001             nan     0.5299   -0.0000
  3100        0.0001             nan     0.5299   -0.0000
  3120        0.0001             nan     0.5299   -0.0000
  3140        0.0001             nan     0.5299   -0.0000
  3160        0.0001             nan     0.5299   -0.0000
  3180        0.0001             nan     0.5299   -0.0000
  3200        0.0001             nan     0.5299   -0.0000
  3220        0.0001             nan     0.5299   -0.0000
  3240        0.0001             nan     0.5299   -0.0000
  3260        0.0001             nan     0.5299    0.0000
  3280        0.0001             nan     0.5299   -0.0000
  3300        0.0001             nan     0.5299   -0.0000
  3320        0.0001             nan     0.5299   -0.0000
  3340        0.0001             nan     0.5299    0.0000
  3360        0.0001             nan     0.5299   -0.0000
  3380        0.0001             nan     0.5299   -0.0000
  3400        0.0001             nan     0.5299   -0.0000
  3420        0.0001             nan     0.5299    0.0000
  3440        0.0001             nan     0.5299   -0.0000
  3460        0.0001             nan     0.5299   -0.0000
  3480        0.0001             nan     0.5299   -0.0000
  3500        0.0001             nan     0.5299   -0.0000
  3520        0.0001             nan     0.5299   -0.0000
  3540        0.0001             nan     0.5299   -0.0000
  3560        0.0001             nan     0.5299   -0.0000
  3580        0.0001             nan     0.5299   -0.0000
  3600        0.0001             nan     0.5299   -0.0000
  3620        0.0001             nan     0.5299   -0.0000
  3640        0.0001             nan     0.5299   -0.0000
  3660        0.0001             nan     0.5299   -0.0000
  3680        0.0001             nan     0.5299   -0.0000
  3700        0.0001             nan     0.5299   -0.0000
  3720        0.0001             nan     0.5299   -0.0000
  3740        0.0001             nan     0.5299    0.0000
  3760        0.0001             nan     0.5299   -0.0000
  3780        0.0001             nan     0.5299   -0.0000
  3800        0.0001             nan     0.5299   -0.0000
  3820        0.0001             nan     0.5299   -0.0000
  3840        0.0001             nan     0.5299   -0.0000
  3860        0.0001             nan     0.5299    0.0000
  3880        0.0001             nan     0.5299    0.0000
  3900        0.0001             nan     0.5299   -0.0000
  3920        0.0001             nan     0.5299    0.0000
  3940        0.0001             nan     0.5299   -0.0000
  3960        0.0001             nan     0.5299   -0.0000
  3980        0.0001             nan     0.5299    0.0000
  4000        0.0001             nan     0.5299   -0.0000
  4020        0.0001             nan     0.5299   -0.0000
  4040        0.0001             nan     0.5299   -0.0000
  4060        0.0001             nan     0.5299   -0.0000
  4080        0.0001             nan     0.5299   -0.0000
  4100        0.0001             nan     0.5299    0.0000
  4112        0.0001             nan     0.5299   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0946             nan     0.5649    0.3073
     2        0.0301             nan     0.5649    0.0678
     3        0.0125             nan     0.5649    0.0137
     4        0.0081             nan     0.5649    0.0036
     5        0.0067             nan     0.5649    0.0012
     6        0.0060             nan     0.5649    0.0006
     7        0.0052             nan     0.5649    0.0006
     8        0.0051             nan     0.5649   -0.0002
     9        0.0048             nan     0.5649   -0.0001
    10        0.0044             nan     0.5649   -0.0001
    20        0.0030             nan     0.5649   -0.0002
    40        0.0018             nan     0.5649   -0.0001
    60        0.0013             nan     0.5649   -0.0001
    80        0.0011             nan     0.5649   -0.0000
   100        0.0008             nan     0.5649   -0.0000
   120        0.0007             nan     0.5649   -0.0000
   140        0.0006             nan     0.5649   -0.0000
   160        0.0005             nan     0.5649   -0.0000
   180        0.0004             nan     0.5649   -0.0000
   200        0.0003             nan     0.5649   -0.0000
   220        0.0003             nan     0.5649   -0.0000
   240        0.0003             nan     0.5649   -0.0000
   260        0.0002             nan     0.5649   -0.0000
   280        0.0002             nan     0.5649   -0.0000
   300        0.0002             nan     0.5649   -0.0000
   320        0.0002             nan     0.5649   -0.0000
   340        0.0002             nan     0.5649   -0.0000
   360        0.0001             nan     0.5649   -0.0000
   380        0.0001             nan     0.5649   -0.0000
   400        0.0001             nan     0.5649   -0.0000
   420        0.0001             nan     0.5649   -0.0000
   440        0.0001             nan     0.5649   -0.0000
   460        0.0001             nan     0.5649    0.0000
   480        0.0001             nan     0.5649   -0.0000
   500        0.0001             nan     0.5649   -0.0000
   520        0.0001             nan     0.5649   -0.0000
   540        0.0001             nan     0.5649   -0.0000
   560        0.0001             nan     0.5649   -0.0000
   580        0.0001             nan     0.5649   -0.0000
   600        0.0001             nan     0.5649    0.0000
   620        0.0001             nan     0.5649   -0.0000
   640        0.0001             nan     0.5649   -0.0000
   660        0.0001             nan     0.5649   -0.0000
   680        0.0001             nan     0.5649   -0.0000
   700        0.0001             nan     0.5649    0.0000
   720        0.0001             nan     0.5649    0.0000
   740        0.0001             nan     0.5649   -0.0000
   760        0.0001             nan     0.5649    0.0000
   780        0.0001             nan     0.5649   -0.0000
   800        0.0001             nan     0.5649   -0.0000
   820        0.0001             nan     0.5649   -0.0000
   840        0.0001             nan     0.5649   -0.0000
   860        0.0001             nan     0.5649   -0.0000
   880        0.0001             nan     0.5649   -0.0000
   900        0.0001             nan     0.5649   -0.0000
   920        0.0001             nan     0.5649   -0.0000
   940        0.0001             nan     0.5649   -0.0000
   960        0.0001             nan     0.5649   -0.0000
   980        0.0001             nan     0.5649   -0.0000
  1000        0.0001             nan     0.5649   -0.0000
  1020        0.0001             nan     0.5649   -0.0000
  1040        0.0001             nan     0.5649   -0.0000
  1060        0.0001             nan     0.5649   -0.0000
  1080        0.0001             nan     0.5649   -0.0000
  1100        0.0001             nan     0.5649   -0.0000
  1120        0.0001             nan     0.5649   -0.0000
  1140        0.0001             nan     0.5649   -0.0000
  1160        0.0001             nan     0.5649   -0.0000
  1180        0.0001             nan     0.5649   -0.0000
  1200        0.0001             nan     0.5649   -0.0000
  1220        0.0001             nan     0.5649   -0.0000
  1240        0.0001             nan     0.5649   -0.0000
  1260        0.0001             nan     0.5649   -0.0000
  1280        0.0001             nan     0.5649   -0.0000
  1300        0.0001             nan     0.5649   -0.0000
  1320        0.0001             nan     0.5649   -0.0000
  1340        0.0001             nan     0.5649   -0.0000
  1360        0.0001             nan     0.5649   -0.0000
  1380        0.0001             nan     0.5649   -0.0000
  1400        0.0001             nan     0.5649   -0.0000
  1420        0.0001             nan     0.5649   -0.0000
  1440        0.0001             nan     0.5649   -0.0000
  1460        0.0001             nan     0.5649   -0.0000
  1480        0.0001             nan     0.5649   -0.0000
  1500        0.0001             nan     0.5649   -0.0000
  1520        0.0001             nan     0.5649   -0.0000
  1540        0.0001             nan     0.5649   -0.0000
  1560        0.0001             nan     0.5649   -0.0000
  1580        0.0001             nan     0.5649   -0.0000
  1600        0.0001             nan     0.5649   -0.0000
  1620        0.0001             nan     0.5649   -0.0000
  1640        0.0001             nan     0.5649   -0.0000
  1660        0.0001             nan     0.5649   -0.0000
  1680        0.0001             nan     0.5649   -0.0000
  1700        0.0001             nan     0.5649   -0.0000
  1719        0.0001             nan     0.5649   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1069             nan     0.5824    0.2791
     2        0.0409             nan     0.5824    0.0658
     3        0.0196             nan     0.5824    0.0195
     4        0.0129             nan     0.5824    0.0079
     5        0.0106             nan     0.5824    0.0017
     6        0.0089             nan     0.5824    0.0007
     7        0.0079             nan     0.5824    0.0005
     8        0.0071             nan     0.5824    0.0005
     9        0.0064             nan     0.5824    0.0005
    10        0.0057             nan     0.5824    0.0005
    20        0.0037             nan     0.5824   -0.0002
    40        0.0021             nan     0.5824   -0.0001
    60        0.0017             nan     0.5824   -0.0001
    80        0.0013             nan     0.5824   -0.0001
   100        0.0010             nan     0.5824   -0.0000
   120        0.0009             nan     0.5824   -0.0000
   140        0.0007             nan     0.5824   -0.0001
   160        0.0006             nan     0.5824   -0.0000
   180        0.0005             nan     0.5824   -0.0000
   200        0.0005             nan     0.5824   -0.0000
   220        0.0004             nan     0.5824   -0.0000
   240        0.0004             nan     0.5824   -0.0000
   260        0.0003             nan     0.5824   -0.0000
   280        0.0003             nan     0.5824   -0.0000
   300        0.0003             nan     0.5824   -0.0000
   320        0.0003             nan     0.5824   -0.0000
   340        0.0002             nan     0.5824   -0.0000
   360        0.0002             nan     0.5824   -0.0000
   380        0.0002             nan     0.5824   -0.0000
   400        0.0002             nan     0.5824   -0.0000
   420        0.0002             nan     0.5824   -0.0000
   440        0.0002             nan     0.5824   -0.0000
   460        0.0002             nan     0.5824   -0.0000
   480        0.0001             nan     0.5824   -0.0000
   500        0.0001             nan     0.5824   -0.0000
   520        0.0001             nan     0.5824   -0.0000
   540        0.0001             nan     0.5824   -0.0000
   560        0.0001             nan     0.5824   -0.0000
   580        0.0001             nan     0.5824   -0.0000
   600        0.0001             nan     0.5824   -0.0000
   620        0.0001             nan     0.5824   -0.0000
   640        0.0001             nan     0.5824   -0.0000
   660        0.0001             nan     0.5824   -0.0000
   680        0.0001             nan     0.5824   -0.0000
   700        0.0001             nan     0.5824   -0.0000
   720        0.0001             nan     0.5824   -0.0000
   740        0.0001             nan     0.5824   -0.0000
   760        0.0001             nan     0.5824   -0.0000
   780        0.0001             nan     0.5824   -0.0000
   800        0.0001             nan     0.5824   -0.0000
   820        0.0001             nan     0.5824   -0.0000
   840        0.0001             nan     0.5824   -0.0000
   860        0.0001             nan     0.5824   -0.0000
   880        0.0001             nan     0.5824   -0.0000
   900        0.0001             nan     0.5824   -0.0000
   920        0.0001             nan     0.5824   -0.0000
   940        0.0001             nan     0.5824   -0.0000
   960        0.0001             nan     0.5824    0.0000
   980        0.0001             nan     0.5824   -0.0000
  1000        0.0001             nan     0.5824   -0.0000
  1020        0.0001             nan     0.5824   -0.0000
  1040        0.0001             nan     0.5824    0.0000
  1060        0.0001             nan     0.5824   -0.0000
  1080        0.0001             nan     0.5824   -0.0000
  1100        0.0001             nan     0.5824   -0.0000
  1120        0.0001             nan     0.5824   -0.0000
  1140        0.0001             nan     0.5824   -0.0000
  1160        0.0001             nan     0.5824   -0.0000
  1180        0.0001             nan     0.5824    0.0000
  1200        0.0001             nan     0.5824   -0.0000
  1220        0.0001             nan     0.5824    0.0000
  1240        0.0001             nan     0.5824   -0.0000
  1260        0.0001             nan     0.5824    0.0000
  1280        0.0001             nan     0.5824   -0.0000
  1300        0.0001             nan     0.5824   -0.0000
  1320        0.0001             nan     0.5824   -0.0000
  1340        0.0001             nan     0.5824   -0.0000
  1360        0.0001             nan     0.5824   -0.0000
  1380        0.0001             nan     0.5824   -0.0000
  1400        0.0001             nan     0.5824   -0.0000
  1420        0.0001             nan     0.5824   -0.0000
  1440        0.0001             nan     0.5824   -0.0000
  1460        0.0001             nan     0.5824   -0.0000
  1480        0.0001             nan     0.5824   -0.0000
  1500        0.0001             nan     0.5824    0.0000
  1520        0.0001             nan     0.5824   -0.0000
  1540        0.0001             nan     0.5824   -0.0000
  1560        0.0001             nan     0.5824   -0.0000
  1580        0.0001             nan     0.5824   -0.0000
  1600        0.0001             nan     0.5824   -0.0000
  1620        0.0001             nan     0.5824   -0.0000
  1640        0.0001             nan     0.5824   -0.0000
  1660        0.0001             nan     0.5824   -0.0000
  1680        0.0001             nan     0.5824   -0.0000
  1700        0.0001             nan     0.5824   -0.0000
  1720        0.0001             nan     0.5824   -0.0000
  1740        0.0001             nan     0.5824    0.0000
  1760        0.0001             nan     0.5824    0.0000
  1780        0.0001             nan     0.5824   -0.0000
  1800        0.0001             nan     0.5824   -0.0000
  1820        0.0001             nan     0.5824   -0.0000
  1840        0.0001             nan     0.5824   -0.0000
  1860        0.0001             nan     0.5824   -0.0000
  1880        0.0001             nan     0.5824    0.0000
  1900        0.0001             nan     0.5824   -0.0000
  1920        0.0001             nan     0.5824   -0.0000
  1940        0.0001             nan     0.5824   -0.0000
  1960        0.0001             nan     0.5824    0.0000
  1980        0.0001             nan     0.5824   -0.0000
  2000        0.0001             nan     0.5824   -0.0000
  2020        0.0001             nan     0.5824   -0.0000
  2040        0.0001             nan     0.5824    0.0000
  2060        0.0001             nan     0.5824   -0.0000
  2080        0.0001             nan     0.5824    0.0000
  2100        0.0001             nan     0.5824    0.0000
  2120        0.0001             nan     0.5824   -0.0000
  2140        0.0001             nan     0.5824   -0.0000
  2160        0.0001             nan     0.5824    0.0000
  2180        0.0001             nan     0.5824   -0.0000
  2200        0.0001             nan     0.5824   -0.0000
  2220        0.0001             nan     0.5824   -0.0000
  2240        0.0001             nan     0.5824    0.0000
  2260        0.0001             nan     0.5824   -0.0000
  2280        0.0001             nan     0.5824    0.0000
  2300        0.0001             nan     0.5824   -0.0000
  2320        0.0001             nan     0.5824    0.0000
  2340        0.0001             nan     0.5824   -0.0000
  2360        0.0001             nan     0.5824   -0.0000
  2380        0.0001             nan     0.5824   -0.0000
  2400        0.0001             nan     0.5824    0.0000
  2420        0.0001             nan     0.5824   -0.0000
  2440        0.0001             nan     0.5824   -0.0000
  2460        0.0001             nan     0.5824   -0.0000
  2480        0.0001             nan     0.5824   -0.0000
  2500        0.0001             nan     0.5824    0.0000
  2520        0.0001             nan     0.5824   -0.0000
  2540        0.0001             nan     0.5824   -0.0000
  2560        0.0001             nan     0.5824   -0.0000
  2580        0.0001             nan     0.5824   -0.0000
  2600        0.0001             nan     0.5824    0.0000
  2620        0.0001             nan     0.5824   -0.0000
  2640        0.0001             nan     0.5824   -0.0000
  2660        0.0001             nan     0.5824   -0.0000
  2680        0.0001             nan     0.5824    0.0000
  2700        0.0001             nan     0.5824   -0.0000
  2720        0.0001             nan     0.5824   -0.0000
  2740        0.0001             nan     0.5824   -0.0000
  2760        0.0001             nan     0.5824   -0.0000
  2780        0.0001             nan     0.5824   -0.0000
  2800        0.0001             nan     0.5824   -0.0000
  2820        0.0001             nan     0.5824   -0.0000
  2840        0.0001             nan     0.5824   -0.0000
  2860        0.0001             nan     0.5824   -0.0000
  2880        0.0001             nan     0.5824   -0.0000
  2900        0.0001             nan     0.5824   -0.0000
  2920        0.0001             nan     0.5824   -0.0000
  2940        0.0001             nan     0.5824   -0.0000
  2960        0.0001             nan     0.5824   -0.0000
  2980        0.0001             nan     0.5824   -0.0000
  3000        0.0001             nan     0.5824   -0.0000
  3020        0.0001             nan     0.5824    0.0000
  3040        0.0001             nan     0.5824    0.0000
  3060        0.0001             nan     0.5824   -0.0000
  3080        0.0001             nan     0.5824    0.0000
  3100        0.0001             nan     0.5824    0.0000
  3120        0.0001             nan     0.5824   -0.0000
  3140        0.0001             nan     0.5824    0.0000
  3160        0.0001             nan     0.5824   -0.0000
  3180        0.0001             nan     0.5824   -0.0000
  3200        0.0001             nan     0.5824   -0.0000
  3220        0.0001             nan     0.5824   -0.0000
  3240        0.0001             nan     0.5824   -0.0000
  3260        0.0001             nan     0.5824    0.0000
  3280        0.0001             nan     0.5824   -0.0000
  3300        0.0001             nan     0.5824   -0.0000
  3320        0.0001             nan     0.5824    0.0000
  3340        0.0001             nan     0.5824    0.0000
  3360        0.0001             nan     0.5824   -0.0000
  3380        0.0001             nan     0.5824   -0.0000
  3400        0.0001             nan     0.5824   -0.0000
  3420        0.0001             nan     0.5824    0.0000
  3440        0.0001             nan     0.5824    0.0000
  3460        0.0001             nan     0.5824    0.0000
  3480        0.0001             nan     0.5824   -0.0000
  3500        0.0001             nan     0.5824   -0.0000
  3520        0.0001             nan     0.5824   -0.0000
  3540        0.0001             nan     0.5824   -0.0000
  3560        0.0001             nan     0.5824   -0.0000
  3580        0.0001             nan     0.5824   -0.0000
  3600        0.0001             nan     0.5824    0.0000
  3620        0.0001             nan     0.5824    0.0000
  3640        0.0001             nan     0.5824   -0.0000
  3660        0.0001             nan     0.5824    0.0000
  3680        0.0001             nan     0.5824    0.0000
  3700        0.0001             nan     0.5824   -0.0000
  3720        0.0001             nan     0.5824   -0.0000
  3740        0.0001             nan     0.5824   -0.0000
  3760        0.0001             nan     0.5824   -0.0000
  3780        0.0001             nan     0.5824    0.0000
  3800        0.0001             nan     0.5824   -0.0000
  3820        0.0001             nan     0.5824   -0.0000
  3840        0.0001             nan     0.5824   -0.0000
  3860        0.0001             nan     0.5824   -0.0000
  3880        0.0001             nan     0.5824    0.0000
  3900        0.0001             nan     0.5824   -0.0000
  3920        0.0001             nan     0.5824    0.0000
  3940        0.0001             nan     0.5824   -0.0000
  3960        0.0001             nan     0.5824   -0.0000
  3980        0.0001             nan     0.5824   -0.0000
  4000        0.0001             nan     0.5824   -0.0000
  4020        0.0001             nan     0.5824   -0.0000
  4040        0.0001             nan     0.5824   -0.0000
  4060        0.0001             nan     0.5824   -0.0000
  4080        0.0001             nan     0.5824    0.0000
  4100        0.0001             nan     0.5824   -0.0000
  4120        0.0001             nan     0.5824   -0.0000
  4140        0.0001             nan     0.5824   -0.0000
  4160        0.0001             nan     0.5824   -0.0000
  4180        0.0001             nan     0.5824   -0.0000
  4200        0.0001             nan     0.5824    0.0000
  4220        0.0001             nan     0.5824   -0.0000
  4240        0.0001             nan     0.5824   -0.0000
  4260        0.0001             nan     0.5824   -0.0000
  4280        0.0001             nan     0.5824   -0.0000
  4300        0.0001             nan     0.5824    0.0000
  4320        0.0001             nan     0.5824    0.0000
  4340        0.0001             nan     0.5824    0.0000
  4360        0.0001             nan     0.5824   -0.0000
  4380        0.0001             nan     0.5824   -0.0000
  4400        0.0001             nan     0.5824   -0.0000
  4420        0.0001             nan     0.5824   -0.0000
  4440        0.0001             nan     0.5824   -0.0000
  4460        0.0001             nan     0.5824   -0.0000
  4480        0.0001             nan     0.5824   -0.0000
  4500        0.0001             nan     0.5824    0.0000
  4520        0.0001             nan     0.5824    0.0000
  4540        0.0001             nan     0.5824    0.0000
  4560        0.0001             nan     0.5824   -0.0000
  4580        0.0001             nan     0.5824   -0.0000
  4600        0.0001             nan     0.5824   -0.0000
  4617        0.0001             nan     0.5824   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1420             nan     0.5882    0.2609
     2        0.0567             nan     0.5882    0.0832
     3        0.0336             nan     0.5882    0.0288
     4        0.0195             nan     0.5882    0.0143
     5        0.0142             nan     0.5882    0.0044
     6        0.0123             nan     0.5882    0.0013
     7        0.0112             nan     0.5882    0.0007
     8        0.0106             nan     0.5882    0.0004
     9        0.0090             nan     0.5882    0.0015
    10        0.0084             nan     0.5882    0.0003
    20        0.0047             nan     0.5882   -0.0000
    40        0.0023             nan     0.5882   -0.0000
    60        0.0015             nan     0.5882   -0.0000
    80        0.0011             nan     0.5882   -0.0000
   100        0.0009             nan     0.5882    0.0000
   120        0.0008             nan     0.5882   -0.0000
   140        0.0006             nan     0.5882   -0.0000
   160        0.0006             nan     0.5882   -0.0000
   180        0.0005             nan     0.5882   -0.0000
   200        0.0004             nan     0.5882   -0.0000
   220        0.0004             nan     0.5882   -0.0000
   240        0.0004             nan     0.5882   -0.0000
   260        0.0003             nan     0.5882   -0.0000
   280        0.0003             nan     0.5882   -0.0000
   300        0.0003             nan     0.5882   -0.0000
   320        0.0002             nan     0.5882   -0.0000
   340        0.0002             nan     0.5882   -0.0000
   360        0.0002             nan     0.5882   -0.0000
   380        0.0002             nan     0.5882   -0.0000
   400        0.0002             nan     0.5882   -0.0000
   420        0.0002             nan     0.5882   -0.0000
   440        0.0001             nan     0.5882   -0.0000
   460        0.0001             nan     0.5882   -0.0000
   480        0.0001             nan     0.5882   -0.0000
   500        0.0001             nan     0.5882   -0.0000
   520        0.0001             nan     0.5882   -0.0000
   540        0.0001             nan     0.5882   -0.0000
   560        0.0001             nan     0.5882   -0.0000
   580        0.0001             nan     0.5882   -0.0000
   600        0.0001             nan     0.5882   -0.0000
   620        0.0001             nan     0.5882   -0.0000
   640        0.0001             nan     0.5882    0.0000
   660        0.0001             nan     0.5882   -0.0000
   680        0.0001             nan     0.5882   -0.0000
   700        0.0001             nan     0.5882   -0.0000
   720        0.0001             nan     0.5882   -0.0000
   740        0.0001             nan     0.5882   -0.0000
   760        0.0000             nan     0.5882   -0.0000
   780        0.0000             nan     0.5882   -0.0000
   800        0.0000             nan     0.5882   -0.0000
   820        0.0000             nan     0.5882   -0.0000
   840        0.0000             nan     0.5882   -0.0000
   860        0.0000             nan     0.5882   -0.0000
   880        0.0000             nan     0.5882   -0.0000
   900        0.0000             nan     0.5882   -0.0000
   920        0.0000             nan     0.5882   -0.0000
   940        0.0000             nan     0.5882   -0.0000
   960        0.0000             nan     0.5882   -0.0000
   980        0.0000             nan     0.5882   -0.0000
  1000        0.0000             nan     0.5882   -0.0000
  1020        0.0000             nan     0.5882   -0.0000
  1040        0.0000             nan     0.5882   -0.0000
  1060        0.0000             nan     0.5882   -0.0000
  1080        0.0000             nan     0.5882   -0.0000
  1100        0.0000             nan     0.5882   -0.0000
  1120        0.0000             nan     0.5882   -0.0000
  1140        0.0000             nan     0.5882   -0.0000
  1160        0.0000             nan     0.5882   -0.0000
  1180        0.0000             nan     0.5882   -0.0000
  1200        0.0000             nan     0.5882   -0.0000
  1220        0.0000             nan     0.5882   -0.0000
  1240        0.0000             nan     0.5882   -0.0000
  1260        0.0000             nan     0.5882   -0.0000
  1280        0.0000             nan     0.5882    0.0000
  1300        0.0000             nan     0.5882   -0.0000
  1320        0.0000             nan     0.5882   -0.0000
  1340        0.0000             nan     0.5882   -0.0000
  1360        0.0000             nan     0.5882   -0.0000
  1380        0.0000             nan     0.5882   -0.0000
  1400        0.0000             nan     0.5882   -0.0000
  1420        0.0000             nan     0.5882    0.0000
  1440        0.0000             nan     0.5882   -0.0000
  1460        0.0000             nan     0.5882   -0.0000
  1480        0.0000             nan     0.5882   -0.0000
  1500        0.0000             nan     0.5882   -0.0000
  1520        0.0000             nan     0.5882   -0.0000
  1540        0.0000             nan     0.5882   -0.0000
  1560        0.0000             nan     0.5882   -0.0000
  1580        0.0000             nan     0.5882   -0.0000
  1600        0.0000             nan     0.5882   -0.0000
  1620        0.0000             nan     0.5882   -0.0000
  1640        0.0000             nan     0.5882   -0.0000
  1660        0.0000             nan     0.5882   -0.0000
  1680        0.0000             nan     0.5882   -0.0000
  1700        0.0000             nan     0.5882   -0.0000
  1720        0.0000             nan     0.5882   -0.0000
  1740        0.0000             nan     0.5882   -0.0000
  1760        0.0000             nan     0.5882   -0.0000
  1780        0.0000             nan     0.5882   -0.0000
  1800        0.0000             nan     0.5882   -0.0000
  1820        0.0000             nan     0.5882   -0.0000
  1840        0.0000             nan     0.5882   -0.0000
  1860        0.0000             nan     0.5882   -0.0000
  1880        0.0000             nan     0.5882   -0.0000
  1900        0.0000             nan     0.5882   -0.0000
  1920        0.0000             nan     0.5882   -0.0000
  1940        0.0000             nan     0.5882   -0.0000
  1960        0.0000             nan     0.5882   -0.0000
  1980        0.0000             nan     0.5882   -0.0000
  2000        0.0000             nan     0.5882   -0.0000
  2020        0.0000             nan     0.5882   -0.0000
  2040        0.0000             nan     0.5882   -0.0000
  2060        0.0000             nan     0.5882    0.0000
  2080        0.0000             nan     0.5882   -0.0000
  2100        0.0000             nan     0.5882    0.0000
  2120        0.0000             nan     0.5882   -0.0000
  2140        0.0000             nan     0.5882   -0.0000
  2160        0.0000             nan     0.5882   -0.0000
  2180        0.0000             nan     0.5882   -0.0000
  2200        0.0000             nan     0.5882   -0.0000
  2220        0.0000             nan     0.5882   -0.0000
  2240        0.0000             nan     0.5882   -0.0000
  2260        0.0000             nan     0.5882   -0.0000
  2280        0.0000             nan     0.5882   -0.0000
  2300        0.0000             nan     0.5882   -0.0000
  2320        0.0000             nan     0.5882   -0.0000
  2340        0.0000             nan     0.5882   -0.0000
  2360        0.0000             nan     0.5882   -0.0000
  2380        0.0000             nan     0.5882   -0.0000
  2400        0.0000             nan     0.5882    0.0000
  2420        0.0000             nan     0.5882   -0.0000
  2440        0.0000             nan     0.5882   -0.0000
  2460        0.0000             nan     0.5882   -0.0000
  2480        0.0000             nan     0.5882   -0.0000
  2500        0.0000             nan     0.5882   -0.0000
  2520        0.0000             nan     0.5882   -0.0000
  2540        0.0000             nan     0.5882   -0.0000
  2560        0.0000             nan     0.5882   -0.0000
  2580        0.0000             nan     0.5882   -0.0000
  2600        0.0000             nan     0.5882   -0.0000
  2620        0.0000             nan     0.5882   -0.0000
  2640        0.0000             nan     0.5882   -0.0000
  2660        0.0000             nan     0.5882   -0.0000
  2680        0.0000             nan     0.5882   -0.0000
  2700        0.0000             nan     0.5882   -0.0000
  2720        0.0000             nan     0.5882   -0.0000
  2740        0.0000             nan     0.5882   -0.0000
  2760        0.0000             nan     0.5882   -0.0000
  2780        0.0000             nan     0.5882    0.0000
  2800        0.0000             nan     0.5882   -0.0000
  2820        0.0000             nan     0.5882   -0.0000
  2840        0.0000             nan     0.5882   -0.0000
  2860        0.0000             nan     0.5882   -0.0000
  2880        0.0000             nan     0.5882   -0.0000
  2900        0.0000             nan     0.5882   -0.0000
  2920        0.0000             nan     0.5882   -0.0000
  2940        0.0000             nan     0.5882   -0.0000
  2960        0.0000             nan     0.5882    0.0000
  2980        0.0000             nan     0.5882   -0.0000
  3000        0.0000             nan     0.5882    0.0000
  3020        0.0000             nan     0.5882   -0.0000
  3040        0.0000             nan     0.5882   -0.0000
  3060        0.0000             nan     0.5882   -0.0000
  3080        0.0000             nan     0.5882   -0.0000
  3100        0.0000             nan     0.5882   -0.0000
  3120        0.0000             nan     0.5882   -0.0000
  3140        0.0000             nan     0.5882    0.0000
  3160        0.0000             nan     0.5882   -0.0000
  3180        0.0000             nan     0.5882   -0.0000
  3200        0.0000             nan     0.5882   -0.0000
  3220        0.0000             nan     0.5882   -0.0000
  3240        0.0000             nan     0.5882   -0.0000
  3260        0.0000             nan     0.5882   -0.0000
  3280        0.0000             nan     0.5882   -0.0000
  3300        0.0000             nan     0.5882   -0.0000
  3320        0.0000             nan     0.5882    0.0000
  3340        0.0000             nan     0.5882   -0.0000
  3360        0.0000             nan     0.5882   -0.0000
  3380        0.0000             nan     0.5882   -0.0000
  3400        0.0000             nan     0.5882   -0.0000
  3420        0.0000             nan     0.5882   -0.0000
  3440        0.0000             nan     0.5882   -0.0000
  3460        0.0000             nan     0.5882    0.0000
  3480        0.0000             nan     0.5882   -0.0000
  3500        0.0000             nan     0.5882   -0.0000
  3520        0.0000             nan     0.5882   -0.0000
  3540        0.0000             nan     0.5882   -0.0000
  3560        0.0000             nan     0.5882   -0.0000
  3580        0.0000             nan     0.5882   -0.0000
  3600        0.0000             nan     0.5882   -0.0000
  3620        0.0000             nan     0.5882   -0.0000
  3640        0.0000             nan     0.5882   -0.0000
  3660        0.0000             nan     0.5882    0.0000
  3680        0.0000             nan     0.5882   -0.0000
  3700        0.0000             nan     0.5882   -0.0000
  3720        0.0000             nan     0.5882   -0.0000
  3740        0.0000             nan     0.5882   -0.0000
  3760        0.0000             nan     0.5882   -0.0000
  3780        0.0000             nan     0.5882   -0.0000
  3800        0.0000             nan     0.5882   -0.0000
  3820        0.0000             nan     0.5882    0.0000
  3840        0.0000             nan     0.5882   -0.0000
  3860        0.0000             nan     0.5882    0.0000
  3880        0.0000             nan     0.5882   -0.0000
  3900        0.0000             nan     0.5882    0.0000
  3920        0.0000             nan     0.5882   -0.0000
  3940        0.0000             nan     0.5882   -0.0000
  3960        0.0000             nan     0.5882    0.0000
  3980        0.0000             nan     0.5882   -0.0000
  4000        0.0000             nan     0.5882   -0.0000
  4020        0.0000             nan     0.5882   -0.0000
  4040        0.0000             nan     0.5882   -0.0000
  4060        0.0000             nan     0.5882   -0.0000
  4080        0.0000             nan     0.5882   -0.0000
  4100        0.0000             nan     0.5882   -0.0000
  4120        0.0000             nan     0.5882   -0.0000
  4140        0.0000             nan     0.5882   -0.0000
  4160        0.0000             nan     0.5882    0.0000
  4180        0.0000             nan     0.5882   -0.0000
  4200        0.0000             nan     0.5882   -0.0000
  4220        0.0000             nan     0.5882   -0.0000
  4240        0.0000             nan     0.5882   -0.0000
  4260        0.0000             nan     0.5882   -0.0000
  4280        0.0000             nan     0.5882    0.0000
  4300        0.0000             nan     0.5882    0.0000
  4320        0.0000             nan     0.5882   -0.0000
  4340        0.0000             nan     0.5882    0.0000
  4360        0.0000             nan     0.5882   -0.0000
  4380        0.0000             nan     0.5882    0.0000
  4400        0.0000             nan     0.5882   -0.0000
  4420        0.0000             nan     0.5882   -0.0000
  4440        0.0000             nan     0.5882   -0.0000
  4460        0.0000             nan     0.5882    0.0000
  4480        0.0000             nan     0.5882   -0.0000
  4500        0.0000             nan     0.5882   -0.0000
  4520        0.0000             nan     0.5882   -0.0000
  4540        0.0000             nan     0.5882   -0.0000
  4560        0.0000             nan     0.5882   -0.0000
  4580        0.0000             nan     0.5882   -0.0000
  4600        0.0000             nan     0.5882   -0.0000
  4620        0.0000             nan     0.5882   -0.0000
  4640        0.0000             nan     0.5882    0.0000
  4660        0.0000             nan     0.5882   -0.0000
  4680        0.0000             nan     0.5882   -0.0000
  4700        0.0000             nan     0.5882   -0.0000
  4720        0.0000             nan     0.5882   -0.0000
  4740        0.0000             nan     0.5882   -0.0000
  4760        0.0000             nan     0.5882   -0.0000
  4780        0.0000             nan     0.5882   -0.0000
  4800        0.0000             nan     0.5882   -0.0000
  4820        0.0000             nan     0.5882   -0.0000
  4840        0.0000             nan     0.5882   -0.0000
  4846        0.0000             nan     0.5882   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.4074             nan     0.0196    0.0127
     2        0.3943             nan     0.0196    0.0129
     3        0.3817             nan     0.0196    0.0112
     4        0.3698             nan     0.0196    0.0113
     5        0.3586             nan     0.0196    0.0107
     6        0.3480             nan     0.0196    0.0108
     7        0.3372             nan     0.0196    0.0104
     8        0.3273             nan     0.0196    0.0099
     9        0.3172             nan     0.0196    0.0100
    10        0.3075             nan     0.0196    0.0097
    20        0.2279             nan     0.0196    0.0068
    40        0.1280             nan     0.0196    0.0035
    60        0.0747             nan     0.0196    0.0019
    80        0.0451             nan     0.0196    0.0011
   100        0.0278             nan     0.0196    0.0006
   120        0.0176             nan     0.0196    0.0004
   140        0.0114             nan     0.0196    0.0002
   160        0.0075             nan     0.0196    0.0001
   180        0.0051             nan     0.0196    0.0001
   200        0.0035             nan     0.0196    0.0001
   220        0.0026             nan     0.0196    0.0000
   240        0.0020             nan     0.0196    0.0000
   260        0.0016             nan     0.0196    0.0000
   280        0.0013             nan     0.0196    0.0000
   300        0.0011             nan     0.0196    0.0000
   320        0.0010             nan     0.0196    0.0000
   340        0.0008             nan     0.0196    0.0000
   360        0.0008             nan     0.0196    0.0000
   380        0.0007             nan     0.0196    0.0000
   400        0.0006             nan     0.0196    0.0000
   420        0.0006             nan     0.0196    0.0000
   440        0.0006             nan     0.0196    0.0000
   460        0.0005             nan     0.0196    0.0000
   480        0.0005             nan     0.0196    0.0000
   500        0.0005             nan     0.0196    0.0000
   520        0.0005             nan     0.0196   -0.0000
   540        0.0005             nan     0.0196    0.0000
   560        0.0005             nan     0.0196    0.0000
   580        0.0004             nan     0.0196   -0.0000
   600        0.0004             nan     0.0196    0.0000
   620        0.0004             nan     0.0196    0.0000
   640        0.0004             nan     0.0196   -0.0000
   660        0.0004             nan     0.0196    0.0000
   680        0.0004             nan     0.0196   -0.0000
   700        0.0004             nan     0.0196    0.0000
   720        0.0004             nan     0.0196   -0.0000
   740        0.0004             nan     0.0196    0.0000
   760        0.0003             nan     0.0196   -0.0000
   780        0.0003             nan     0.0196   -0.0000
   800        0.0003             nan     0.0196   -0.0000
   820        0.0003             nan     0.0196   -0.0000
   840        0.0003             nan     0.0196   -0.0000
   860        0.0003             nan     0.0196   -0.0000
   880        0.0003             nan     0.0196    0.0000
   900        0.0003             nan     0.0196    0.0000
   920        0.0003             nan     0.0196   -0.0000
   940        0.0003             nan     0.0196    0.0000
   960        0.0003             nan     0.0196   -0.0000
   980        0.0003             nan     0.0196   -0.0000
  1000        0.0003             nan     0.0196   -0.0000
  1020        0.0003             nan     0.0196   -0.0000
  1040        0.0003             nan     0.0196   -0.0000
  1060        0.0003             nan     0.0196    0.0000
  1080        0.0003             nan     0.0196    0.0000
  1100        0.0003             nan     0.0196   -0.0000
  1120        0.0003             nan     0.0196   -0.0000
  1140        0.0003             nan     0.0196   -0.0000
  1160        0.0002             nan     0.0196   -0.0000
  1180        0.0002             nan     0.0196   -0.0000
  1200        0.0002             nan     0.0196   -0.0000
  1220        0.0002             nan     0.0196   -0.0000
  1240        0.0002             nan     0.0196    0.0000
  1260        0.0002             nan     0.0196    0.0000
  1280        0.0002             nan     0.0196    0.0000
  1300        0.0002             nan     0.0196   -0.0000
  1320        0.0002             nan     0.0196   -0.0000
  1340        0.0002             nan     0.0196   -0.0000
  1360        0.0002             nan     0.0196   -0.0000
  1380        0.0002             nan     0.0196   -0.0000
  1400        0.0002             nan     0.0196   -0.0000
  1420        0.0002             nan     0.0196   -0.0000
  1440        0.0002             nan     0.0196   -0.0000
  1460        0.0002             nan     0.0196   -0.0000
  1475        0.0002             nan     0.0196   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3748             nan     0.0878    0.0470
     2        0.3336             nan     0.0878    0.0390
     3        0.3015             nan     0.0878    0.0335
     4        0.2722             nan     0.0878    0.0265
     5        0.2450             nan     0.0878    0.0248
     6        0.2215             nan     0.0878    0.0221
     7        0.2015             nan     0.0878    0.0194
     8        0.1835             nan     0.0878    0.0171
     9        0.1679             nan     0.0878    0.0146
    10        0.1531             nan     0.0878    0.0145
    20        0.0676             nan     0.0878    0.0051
    40        0.0188             nan     0.0878    0.0012
    60        0.0065             nan     0.0878    0.0000
    80        0.0030             nan     0.0878    0.0001
   100        0.0019             nan     0.0878    0.0000
   120        0.0015             nan     0.0878    0.0000
   140        0.0013             nan     0.0878   -0.0000
   160        0.0012             nan     0.0878   -0.0000
   180        0.0012             nan     0.0878   -0.0000
   200        0.0011             nan     0.0878    0.0000
   220        0.0011             nan     0.0878   -0.0000
   240        0.0010             nan     0.0878   -0.0000
   260        0.0010             nan     0.0878   -0.0000
   280        0.0010             nan     0.0878   -0.0000
   300        0.0010             nan     0.0878   -0.0000
   320        0.0010             nan     0.0878   -0.0000
   340        0.0009             nan     0.0878   -0.0000
   360        0.0009             nan     0.0878   -0.0000
   380        0.0009             nan     0.0878   -0.0000
   400        0.0009             nan     0.0878   -0.0000
   420        0.0009             nan     0.0878   -0.0000
   440        0.0009             nan     0.0878   -0.0000
   460        0.0008             nan     0.0878   -0.0000
   480        0.0008             nan     0.0878   -0.0000
   500        0.0008             nan     0.0878   -0.0000
   520        0.0008             nan     0.0878   -0.0000
   540        0.0008             nan     0.0878   -0.0000
   560        0.0008             nan     0.0878   -0.0000
   580        0.0008             nan     0.0878   -0.0000
   600        0.0007             nan     0.0878    0.0000
   620        0.0007             nan     0.0878   -0.0000
   640        0.0007             nan     0.0878   -0.0000
   660        0.0007             nan     0.0878   -0.0000
   680        0.0007             nan     0.0878   -0.0000
   700        0.0007             nan     0.0878   -0.0000
   720        0.0007             nan     0.0878   -0.0000
   740        0.0007             nan     0.0878   -0.0000
   760        0.0007             nan     0.0878   -0.0000
   780        0.0007             nan     0.0878   -0.0000
   800        0.0007             nan     0.0878   -0.0000
   820        0.0006             nan     0.0878   -0.0000
   840        0.0006             nan     0.0878   -0.0000
   860        0.0006             nan     0.0878   -0.0000
   880        0.0006             nan     0.0878   -0.0000
   900        0.0006             nan     0.0878   -0.0000
   920        0.0006             nan     0.0878   -0.0000
   940        0.0006             nan     0.0878   -0.0000
   960        0.0006             nan     0.0878   -0.0000
   980        0.0006             nan     0.0878   -0.0000
  1000        0.0006             nan     0.0878   -0.0000
  1020        0.0006             nan     0.0878   -0.0000
  1040        0.0006             nan     0.0878   -0.0000
  1060        0.0006             nan     0.0878   -0.0000
  1080        0.0006             nan     0.0878   -0.0000
  1100        0.0006             nan     0.0878   -0.0000
  1120        0.0006             nan     0.0878   -0.0000
  1140        0.0005             nan     0.0878   -0.0000
  1160        0.0005             nan     0.0878   -0.0000
  1180        0.0005             nan     0.0878   -0.0000
  1200        0.0005             nan     0.0878   -0.0000
  1220        0.0005             nan     0.0878   -0.0000
  1240        0.0005             nan     0.0878   -0.0000
  1260        0.0005             nan     0.0878   -0.0000
  1280        0.0005             nan     0.0878   -0.0000
  1300        0.0005             nan     0.0878   -0.0000
  1320        0.0005             nan     0.0878   -0.0000
  1340        0.0005             nan     0.0878   -0.0000
  1360        0.0005             nan     0.0878   -0.0000
  1380        0.0005             nan     0.0878   -0.0000
  1395        0.0005             nan     0.0878   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3388             nan     0.1163    0.0824
     2        0.2736             nan     0.1163    0.0619
     3        0.2229             nan     0.1163    0.0528
     4        0.1822             nan     0.1163    0.0439
     5        0.1499             nan     0.1163    0.0321
     6        0.1227             nan     0.1163    0.0264
     7        0.0998             nan     0.1163    0.0197
     8        0.0831             nan     0.1163    0.0151
     9        0.0695             nan     0.1163    0.0144
    10        0.0577             nan     0.1163    0.0112
    20        0.0106             nan     0.1163    0.0015
    40        0.0016             nan     0.1163    0.0000
    60        0.0009             nan     0.1163    0.0000
    80        0.0006             nan     0.1163    0.0000
   100        0.0005             nan     0.1163   -0.0000
   120        0.0005             nan     0.1163   -0.0000
   140        0.0004             nan     0.1163   -0.0000
   160        0.0004             nan     0.1163   -0.0000
   180        0.0003             nan     0.1163   -0.0000
   200        0.0003             nan     0.1163   -0.0000
   220        0.0003             nan     0.1163   -0.0000
   240        0.0002             nan     0.1163   -0.0000
   260        0.0002             nan     0.1163   -0.0000
   280        0.0002             nan     0.1163   -0.0000
   300        0.0002             nan     0.1163   -0.0000
   320        0.0002             nan     0.1163   -0.0000
   340        0.0002             nan     0.1163   -0.0000
   360        0.0002             nan     0.1163   -0.0000
   380        0.0001             nan     0.1163   -0.0000
   400        0.0001             nan     0.1163   -0.0000
   420        0.0001             nan     0.1163   -0.0000
   440        0.0001             nan     0.1163   -0.0000
   460        0.0001             nan     0.1163   -0.0000
   480        0.0001             nan     0.1163   -0.0000
   500        0.0001             nan     0.1163   -0.0000
   520        0.0001             nan     0.1163   -0.0000
   540        0.0001             nan     0.1163   -0.0000
   560        0.0001             nan     0.1163   -0.0000
   580        0.0001             nan     0.1163   -0.0000
   600        0.0001             nan     0.1163   -0.0000
   620        0.0001             nan     0.1163   -0.0000
   640        0.0001             nan     0.1163   -0.0000
   660        0.0001             nan     0.1163   -0.0000
   680        0.0001             nan     0.1163   -0.0000
   700        0.0001             nan     0.1163   -0.0000
   720        0.0001             nan     0.1163   -0.0000
   740        0.0001             nan     0.1163   -0.0000
   760        0.0001             nan     0.1163   -0.0000
   780        0.0001             nan     0.1163   -0.0000
   800        0.0000             nan     0.1163   -0.0000
   820        0.0000             nan     0.1163   -0.0000
   840        0.0000             nan     0.1163   -0.0000
   860        0.0000             nan     0.1163   -0.0000
   880        0.0000             nan     0.1163   -0.0000
   900        0.0000             nan     0.1163   -0.0000
   920        0.0000             nan     0.1163   -0.0000
   940        0.0000             nan     0.1163   -0.0000
   960        0.0000             nan     0.1163   -0.0000
   980        0.0000             nan     0.1163   -0.0000
  1000        0.0000             nan     0.1163   -0.0000
  1020        0.0000             nan     0.1163   -0.0000
  1040        0.0000             nan     0.1163   -0.0000
  1060        0.0000             nan     0.1163   -0.0000
  1080        0.0000             nan     0.1163   -0.0000
  1100        0.0000             nan     0.1163   -0.0000
  1106        0.0000             nan     0.1163   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3003             nan     0.1626    0.1198
     2        0.2141             nan     0.1626    0.0842
     3        0.1531             nan     0.1626    0.0619
     4        0.1114             nan     0.1626    0.0435
     5        0.0812             nan     0.1626    0.0295
     6        0.0591             nan     0.1626    0.0205
     7        0.0432             nan     0.1626    0.0152
     8        0.0317             nan     0.1626    0.0113
     9        0.0238             nan     0.1626    0.0077
    10        0.0183             nan     0.1626    0.0052
    20        0.0031             nan     0.1626    0.0003
    40        0.0015             nan     0.1626   -0.0000
    50        0.0013             nan     0.1626   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2756             nan     0.2083    0.1347
     2        0.1822             nan     0.2083    0.0844
     3        0.1239             nan     0.2083    0.0596
     4        0.0826             nan     0.2083    0.0405
     5        0.0566             nan     0.2083    0.0263
     6        0.0394             nan     0.2083    0.0165
     7        0.0274             nan     0.2083    0.0119
     8        0.0194             nan     0.2083    0.0077
     9        0.0141             nan     0.2083    0.0050
    10        0.0105             nan     0.2083    0.0035
    20        0.0030             nan     0.2083    0.0001
    40        0.0018             nan     0.2083    0.0000
    60        0.0013             nan     0.2083   -0.0000
    80        0.0011             nan     0.2083   -0.0000
   100        0.0009             nan     0.2083   -0.0000
   120        0.0008             nan     0.2083   -0.0000
   140        0.0007             nan     0.2083   -0.0000
   160        0.0006             nan     0.2083   -0.0000
   180        0.0005             nan     0.2083   -0.0000
   200        0.0005             nan     0.2083   -0.0000
   220        0.0004             nan     0.2083   -0.0000
   240        0.0004             nan     0.2083   -0.0000
   260        0.0004             nan     0.2083   -0.0000
   280        0.0003             nan     0.2083   -0.0000
   300        0.0003             nan     0.2083   -0.0000
   320        0.0003             nan     0.2083   -0.0000
   340        0.0002             nan     0.2083   -0.0000
   360        0.0002             nan     0.2083   -0.0000
   380        0.0002             nan     0.2083   -0.0000
   400        0.0002             nan     0.2083   -0.0000
   420        0.0002             nan     0.2083   -0.0000
   440        0.0002             nan     0.2083   -0.0000
   460        0.0002             nan     0.2083   -0.0000
   480        0.0002             nan     0.2083   -0.0000
   500        0.0002             nan     0.2083   -0.0000
   520        0.0001             nan     0.2083   -0.0000
   540        0.0001             nan     0.2083   -0.0000
   560        0.0001             nan     0.2083   -0.0000
   580        0.0001             nan     0.2083   -0.0000
   600        0.0001             nan     0.2083   -0.0000
   620        0.0001             nan     0.2083   -0.0000
   640        0.0001             nan     0.2083   -0.0000
   660        0.0001             nan     0.2083   -0.0000
   680        0.0001             nan     0.2083   -0.0000
   700        0.0001             nan     0.2083   -0.0000
   720        0.0001             nan     0.2083   -0.0000
   740        0.0001             nan     0.2083   -0.0000
   760        0.0001             nan     0.2083   -0.0000
   780        0.0001             nan     0.2083   -0.0000
   800        0.0001             nan     0.2083   -0.0000
   820        0.0001             nan     0.2083   -0.0000
   840        0.0001             nan     0.2083   -0.0000
   860        0.0001             nan     0.2083   -0.0000
   880        0.0001             nan     0.2083   -0.0000
   900        0.0001             nan     0.2083   -0.0000
   920        0.0001             nan     0.2083   -0.0000
   940        0.0001             nan     0.2083   -0.0000
   960        0.0001             nan     0.2083   -0.0000
   980        0.0001             nan     0.2083   -0.0000
  1000        0.0001             nan     0.2083   -0.0000
  1020        0.0001             nan     0.2083   -0.0000
  1040        0.0001             nan     0.2083   -0.0000
  1060        0.0001             nan     0.2083   -0.0000
  1080        0.0001             nan     0.2083   -0.0000
  1100        0.0001             nan     0.2083   -0.0000
  1120        0.0001             nan     0.2083   -0.0000
  1140        0.0001             nan     0.2083   -0.0000
  1160        0.0001             nan     0.2083   -0.0000
  1180        0.0001             nan     0.2083   -0.0000
  1200        0.0001             nan     0.2083   -0.0000
  1220        0.0001             nan     0.2083   -0.0000
  1240        0.0001             nan     0.2083   -0.0000
  1260        0.0001             nan     0.2083   -0.0000
  1280        0.0001             nan     0.2083   -0.0000
  1300        0.0001             nan     0.2083   -0.0000
  1320        0.0001             nan     0.2083   -0.0000
  1340        0.0001             nan     0.2083   -0.0000
  1360        0.0001             nan     0.2083   -0.0000
  1380        0.0001             nan     0.2083   -0.0000
  1400        0.0001             nan     0.2083   -0.0000
  1420        0.0001             nan     0.2083   -0.0000
  1440        0.0001             nan     0.2083    0.0000
  1460        0.0001             nan     0.2083   -0.0000
  1480        0.0001             nan     0.2083   -0.0000
  1500        0.0001             nan     0.2083   -0.0000
  1520        0.0001             nan     0.2083   -0.0000
  1540        0.0001             nan     0.2083   -0.0000
  1560        0.0001             nan     0.2083   -0.0000
  1580        0.0001             nan     0.2083   -0.0000
  1600        0.0001             nan     0.2083   -0.0000
  1620        0.0001             nan     0.2083   -0.0000
  1640        0.0000             nan     0.2083   -0.0000
  1660        0.0000             nan     0.2083   -0.0000
  1680        0.0000             nan     0.2083   -0.0000
  1700        0.0000             nan     0.2083   -0.0000
  1720        0.0000             nan     0.2083   -0.0000
  1740        0.0000             nan     0.2083   -0.0000
  1760        0.0000             nan     0.2083   -0.0000
  1780        0.0000             nan     0.2083   -0.0000
  1800        0.0000             nan     0.2083   -0.0000
  1820        0.0000             nan     0.2083   -0.0000
  1840        0.0000             nan     0.2083   -0.0000
  1860        0.0000             nan     0.2083   -0.0000
  1880        0.0000             nan     0.2083   -0.0000
  1900        0.0000             nan     0.2083   -0.0000
  1920        0.0000             nan     0.2083   -0.0000
  1940        0.0000             nan     0.2083    0.0000
  1960        0.0000             nan     0.2083    0.0000
  1974        0.0000             nan     0.2083   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2731             nan     0.2133    0.1505
     2        0.1825             nan     0.2133    0.0823
     3        0.1221             nan     0.2133    0.0577
     4        0.0858             nan     0.2133    0.0387
     5        0.0608             nan     0.2133    0.0227
     6        0.0427             nan     0.2133    0.0124
     7        0.0308             nan     0.2133    0.0116
     8        0.0223             nan     0.2133    0.0077
     9        0.0166             nan     0.2133    0.0052
    10        0.0125             nan     0.2133    0.0041
    20        0.0023             nan     0.2133    0.0002
    40        0.0013             nan     0.2133    0.0000
    60        0.0010             nan     0.2133   -0.0000
    80        0.0008             nan     0.2133   -0.0000
   100        0.0006             nan     0.2133   -0.0000
   120        0.0005             nan     0.2133    0.0000
   140        0.0005             nan     0.2133   -0.0000
   160        0.0004             nan     0.2133   -0.0000
   180        0.0004             nan     0.2133   -0.0000
   200        0.0003             nan     0.2133   -0.0000
   220        0.0003             nan     0.2133   -0.0000
   240        0.0003             nan     0.2133   -0.0000
   260        0.0002             nan     0.2133   -0.0000
   280        0.0002             nan     0.2133   -0.0000
   300        0.0002             nan     0.2133   -0.0000
   320        0.0002             nan     0.2133   -0.0000
   340        0.0002             nan     0.2133   -0.0000
   360        0.0002             nan     0.2133   -0.0000
   380        0.0002             nan     0.2133   -0.0000
   400        0.0001             nan     0.2133   -0.0000
   420        0.0001             nan     0.2133    0.0000
   440        0.0001             nan     0.2133   -0.0000
   460        0.0001             nan     0.2133   -0.0000
   480        0.0001             nan     0.2133   -0.0000
   500        0.0001             nan     0.2133   -0.0000
   520        0.0001             nan     0.2133   -0.0000
   540        0.0001             nan     0.2133   -0.0000
   560        0.0001             nan     0.2133   -0.0000
   580        0.0001             nan     0.2133   -0.0000
   600        0.0001             nan     0.2133   -0.0000
   620        0.0001             nan     0.2133   -0.0000
   640        0.0001             nan     0.2133   -0.0000
   660        0.0001             nan     0.2133   -0.0000
   680        0.0001             nan     0.2133   -0.0000
   700        0.0001             nan     0.2133   -0.0000
   720        0.0001             nan     0.2133   -0.0000
   740        0.0001             nan     0.2133   -0.0000
   760        0.0001             nan     0.2133   -0.0000
   780        0.0001             nan     0.2133   -0.0000
   800        0.0000             nan     0.2133   -0.0000
   820        0.0000             nan     0.2133   -0.0000
   840        0.0000             nan     0.2133   -0.0000
   860        0.0000             nan     0.2133   -0.0000
   880        0.0000             nan     0.2133   -0.0000
   900        0.0000             nan     0.2133   -0.0000
   920        0.0000             nan     0.2133   -0.0000
   940        0.0000             nan     0.2133   -0.0000
   960        0.0000             nan     0.2133   -0.0000
   980        0.0000             nan     0.2133   -0.0000
  1000        0.0000             nan     0.2133   -0.0000
  1020        0.0000             nan     0.2133   -0.0000
  1040        0.0000             nan     0.2133   -0.0000
  1060        0.0000             nan     0.2133   -0.0000
  1080        0.0000             nan     0.2133   -0.0000
  1100        0.0000             nan     0.2133   -0.0000
  1120        0.0000             nan     0.2133   -0.0000
  1140        0.0000             nan     0.2133   -0.0000
  1160        0.0000             nan     0.2133   -0.0000
  1180        0.0000             nan     0.2133   -0.0000
  1200        0.0000             nan     0.2133   -0.0000
  1220        0.0000             nan     0.2133   -0.0000
  1240        0.0000             nan     0.2133   -0.0000
  1260        0.0000             nan     0.2133   -0.0000
  1280        0.0000             nan     0.2133   -0.0000
  1300        0.0000             nan     0.2133   -0.0000
  1320        0.0000             nan     0.2133   -0.0000
  1340        0.0000             nan     0.2133   -0.0000
  1360        0.0000             nan     0.2133   -0.0000
  1380        0.0000             nan     0.2133   -0.0000
  1400        0.0000             nan     0.2133   -0.0000
  1420        0.0000             nan     0.2133   -0.0000
  1440        0.0000             nan     0.2133   -0.0000
  1460        0.0000             nan     0.2133   -0.0000
  1480        0.0000             nan     0.2133   -0.0000
  1500        0.0000             nan     0.2133   -0.0000
  1520        0.0000             nan     0.2133   -0.0000
  1540        0.0000             nan     0.2133   -0.0000
  1560        0.0000             nan     0.2133   -0.0000
  1580        0.0000             nan     0.2133   -0.0000
  1600        0.0000             nan     0.2133   -0.0000
  1620        0.0000             nan     0.2133   -0.0000
  1640        0.0000             nan     0.2133   -0.0000
  1660        0.0000             nan     0.2133   -0.0000
  1680        0.0000             nan     0.2133   -0.0000
  1700        0.0000             nan     0.2133   -0.0000
  1720        0.0000             nan     0.2133   -0.0000
  1740        0.0000             nan     0.2133   -0.0000
  1760        0.0000             nan     0.2133   -0.0000
  1780        0.0000             nan     0.2133   -0.0000
  1800        0.0000             nan     0.2133   -0.0000
  1820        0.0000             nan     0.2133   -0.0000
  1840        0.0000             nan     0.2133   -0.0000
  1860        0.0000             nan     0.2133   -0.0000
  1880        0.0000             nan     0.2133   -0.0000
  1900        0.0000             nan     0.2133   -0.0000
  1920        0.0000             nan     0.2133   -0.0000
  1940        0.0000             nan     0.2133    0.0000
  1960        0.0000             nan     0.2133   -0.0000
  1980        0.0000             nan     0.2133   -0.0000
  2000        0.0000             nan     0.2133   -0.0000
  2020        0.0000             nan     0.2133   -0.0000
  2040        0.0000             nan     0.2133   -0.0000
  2060        0.0000             nan     0.2133   -0.0000
  2080        0.0000             nan     0.2133   -0.0000
  2100        0.0000             nan     0.2133   -0.0000
  2120        0.0000             nan     0.2133   -0.0000
  2140        0.0000             nan     0.2133   -0.0000
  2160        0.0000             nan     0.2133   -0.0000
  2180        0.0000             nan     0.2133   -0.0000
  2200        0.0000             nan     0.2133   -0.0000
  2220        0.0000             nan     0.2133   -0.0000
  2240        0.0000             nan     0.2133   -0.0000
  2260        0.0000             nan     0.2133   -0.0000
  2280        0.0000             nan     0.2133   -0.0000
  2300        0.0000             nan     0.2133   -0.0000
  2320        0.0000             nan     0.2133   -0.0000
  2340        0.0000             nan     0.2133   -0.0000
  2360        0.0000             nan     0.2133   -0.0000
  2380        0.0000             nan     0.2133   -0.0000
  2400        0.0000             nan     0.2133   -0.0000
  2420        0.0000             nan     0.2133   -0.0000
  2440        0.0000             nan     0.2133   -0.0000
  2460        0.0000             nan     0.2133   -0.0000
  2480        0.0000             nan     0.2133   -0.0000
  2500        0.0000             nan     0.2133   -0.0000
  2520        0.0000             nan     0.2133   -0.0000
  2540        0.0000             nan     0.2133   -0.0000
  2560        0.0000             nan     0.2133   -0.0000
  2580        0.0000             nan     0.2133    0.0000
  2600        0.0000             nan     0.2133   -0.0000
  2620        0.0000             nan     0.2133   -0.0000
  2640        0.0000             nan     0.2133   -0.0000
  2660        0.0000             nan     0.2133   -0.0000
  2680        0.0000             nan     0.2133   -0.0000
  2700        0.0000             nan     0.2133   -0.0000
  2720        0.0000             nan     0.2133   -0.0000
  2740        0.0000             nan     0.2133   -0.0000
  2760        0.0000             nan     0.2133   -0.0000
  2780        0.0000             nan     0.2133   -0.0000
  2800        0.0000             nan     0.2133   -0.0000
  2820        0.0000             nan     0.2133   -0.0000
  2840        0.0000             nan     0.2133   -0.0000
  2860        0.0000             nan     0.2133   -0.0000
  2880        0.0000             nan     0.2133   -0.0000
  2900        0.0000             nan     0.2133   -0.0000
  2920        0.0000             nan     0.2133   -0.0000
  2940        0.0000             nan     0.2133   -0.0000
  2960        0.0000             nan     0.2133   -0.0000
  2980        0.0000             nan     0.2133   -0.0000
  3000        0.0000             nan     0.2133   -0.0000
  3020        0.0000             nan     0.2133   -0.0000
  3040        0.0000             nan     0.2133   -0.0000
  3060        0.0000             nan     0.2133   -0.0000
  3080        0.0000             nan     0.2133   -0.0000
  3100        0.0000             nan     0.2133   -0.0000
  3120        0.0000             nan     0.2133   -0.0000
  3140        0.0000             nan     0.2133   -0.0000
  3160        0.0000             nan     0.2133   -0.0000
  3180        0.0000             nan     0.2133   -0.0000
  3200        0.0000             nan     0.2133   -0.0000
  3220        0.0000             nan     0.2133   -0.0000
  3240        0.0000             nan     0.2133   -0.0000
  3260        0.0000             nan     0.2133   -0.0000
  3280        0.0000             nan     0.2133   -0.0000
  3300        0.0000             nan     0.2133   -0.0000
  3320        0.0000             nan     0.2133   -0.0000
  3340        0.0000             nan     0.2133   -0.0000
  3360        0.0000             nan     0.2133    0.0000
  3380        0.0000             nan     0.2133   -0.0000
  3400        0.0000             nan     0.2133   -0.0000
  3420        0.0000             nan     0.2133   -0.0000
  3440        0.0000             nan     0.2133   -0.0000
  3460        0.0000             nan     0.2133   -0.0000
  3480        0.0000             nan     0.2133   -0.0000
  3500        0.0000             nan     0.2133   -0.0000
  3520        0.0000             nan     0.2133   -0.0000
  3540        0.0000             nan     0.2133   -0.0000
  3560        0.0000             nan     0.2133   -0.0000
  3580        0.0000             nan     0.2133   -0.0000
  3600        0.0000             nan     0.2133   -0.0000
  3620        0.0000             nan     0.2133   -0.0000
  3640        0.0000             nan     0.2133   -0.0000
  3660        0.0000             nan     0.2133   -0.0000
  3680        0.0000             nan     0.2133   -0.0000
  3700        0.0000             nan     0.2133   -0.0000
  3720        0.0000             nan     0.2133   -0.0000
  3740        0.0000             nan     0.2133   -0.0000
  3760        0.0000             nan     0.2133   -0.0000
  3780        0.0000             nan     0.2133   -0.0000
  3800        0.0000             nan     0.2133   -0.0000
  3820        0.0000             nan     0.2133   -0.0000
  3840        0.0000             nan     0.2133   -0.0000
  3860        0.0000             nan     0.2133   -0.0000
  3880        0.0000             nan     0.2133   -0.0000
  3900        0.0000             nan     0.2133   -0.0000
  3920        0.0000             nan     0.2133   -0.0000
  3940        0.0000             nan     0.2133   -0.0000
  3960        0.0000             nan     0.2133   -0.0000
  3980        0.0000             nan     0.2133   -0.0000
  4000        0.0000             nan     0.2133    0.0000
  4020        0.0000             nan     0.2133   -0.0000
  4040        0.0000             nan     0.2133   -0.0000
  4060        0.0000             nan     0.2133   -0.0000
  4080        0.0000             nan     0.2133   -0.0000
  4100        0.0000             nan     0.2133   -0.0000
  4120        0.0000             nan     0.2133   -0.0000
  4140        0.0000             nan     0.2133   -0.0000
  4160        0.0000             nan     0.2133   -0.0000
  4180        0.0000             nan     0.2133   -0.0000
  4200        0.0000             nan     0.2133   -0.0000
  4220        0.0000             nan     0.2133   -0.0000
  4240        0.0000             nan     0.2133   -0.0000
  4260        0.0000             nan     0.2133    0.0000
  4280        0.0000             nan     0.2133    0.0000
  4300        0.0000             nan     0.2133    0.0000
  4320        0.0000             nan     0.2133   -0.0000
  4340        0.0000             nan     0.2133   -0.0000
  4360        0.0000             nan     0.2133   -0.0000
  4380        0.0000             nan     0.2133   -0.0000
  4400        0.0000             nan     0.2133   -0.0000
  4420        0.0000             nan     0.2133   -0.0000
  4439        0.0000             nan     0.2133   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2699             nan     0.2206    0.1604
     2        0.1740             nan     0.2206    0.0959
     3        0.1159             nan     0.2206    0.0572
     4        0.0771             nan     0.2206    0.0399
     5        0.0519             nan     0.2206    0.0251
     6        0.0351             nan     0.2206    0.0147
     7        0.0248             nan     0.2206    0.0109
     8        0.0177             nan     0.2206    0.0062
     9        0.0125             nan     0.2206    0.0048
    10        0.0091             nan     0.2206    0.0032
    20        0.0019             nan     0.2206   -0.0000
    40        0.0011             nan     0.2206   -0.0000
    60        0.0008             nan     0.2206   -0.0000
    80        0.0006             nan     0.2206   -0.0000
   100        0.0005             nan     0.2206   -0.0000
   120        0.0004             nan     0.2206   -0.0000
   140        0.0004             nan     0.2206   -0.0000
   160        0.0003             nan     0.2206   -0.0000
   180        0.0003             nan     0.2206   -0.0000
   200        0.0002             nan     0.2206   -0.0000
   220        0.0002             nan     0.2206   -0.0000
   240        0.0002             nan     0.2206   -0.0000
   260        0.0002             nan     0.2206   -0.0000
   280        0.0002             nan     0.2206   -0.0000
   300        0.0001             nan     0.2206   -0.0000
   320        0.0001             nan     0.2206   -0.0000
   340        0.0001             nan     0.2206   -0.0000
   360        0.0001             nan     0.2206   -0.0000
   380        0.0001             nan     0.2206   -0.0000
   400        0.0001             nan     0.2206   -0.0000
   420        0.0001             nan     0.2206   -0.0000
   440        0.0001             nan     0.2206   -0.0000
   460        0.0001             nan     0.2206   -0.0000
   480        0.0001             nan     0.2206   -0.0000
   500        0.0001             nan     0.2206   -0.0000
   520        0.0001             nan     0.2206   -0.0000
   540        0.0001             nan     0.2206   -0.0000
   560        0.0000             nan     0.2206   -0.0000
   580        0.0000             nan     0.2206   -0.0000
   600        0.0000             nan     0.2206   -0.0000
   620        0.0000             nan     0.2206   -0.0000
   640        0.0000             nan     0.2206   -0.0000
   660        0.0000             nan     0.2206   -0.0000
   680        0.0000             nan     0.2206   -0.0000
   700        0.0000             nan     0.2206   -0.0000
   720        0.0000             nan     0.2206   -0.0000
   740        0.0000             nan     0.2206   -0.0000
   760        0.0000             nan     0.2206   -0.0000
   780        0.0000             nan     0.2206   -0.0000
   800        0.0000             nan     0.2206   -0.0000
   820        0.0000             nan     0.2206   -0.0000
   840        0.0000             nan     0.2206   -0.0000
   860        0.0000             nan     0.2206   -0.0000
   880        0.0000             nan     0.2206   -0.0000
   900        0.0000             nan     0.2206   -0.0000
   920        0.0000             nan     0.2206   -0.0000
   940        0.0000             nan     0.2206   -0.0000
   960        0.0000             nan     0.2206   -0.0000
   980        0.0000             nan     0.2206   -0.0000
  1000        0.0000             nan     0.2206   -0.0000
  1020        0.0000             nan     0.2206   -0.0000
  1040        0.0000             nan     0.2206   -0.0000
  1060        0.0000             nan     0.2206   -0.0000
  1080        0.0000             nan     0.2206   -0.0000
  1100        0.0000             nan     0.2206   -0.0000
  1120        0.0000             nan     0.2206   -0.0000
  1140        0.0000             nan     0.2206   -0.0000
  1160        0.0000             nan     0.2206   -0.0000
  1180        0.0000             nan     0.2206   -0.0000
  1200        0.0000             nan     0.2206   -0.0000
  1220        0.0000             nan     0.2206   -0.0000
  1240        0.0000             nan     0.2206   -0.0000
  1260        0.0000             nan     0.2206   -0.0000
  1280        0.0000             nan     0.2206   -0.0000
  1300        0.0000             nan     0.2206   -0.0000
  1320        0.0000             nan     0.2206   -0.0000
  1340        0.0000             nan     0.2206   -0.0000
  1360        0.0000             nan     0.2206   -0.0000
  1380        0.0000             nan     0.2206   -0.0000
  1400        0.0000             nan     0.2206   -0.0000
  1420        0.0000             nan     0.2206   -0.0000
  1440        0.0000             nan     0.2206   -0.0000
  1460        0.0000             nan     0.2206   -0.0000
  1480        0.0000             nan     0.2206   -0.0000
  1500        0.0000             nan     0.2206   -0.0000
  1520        0.0000             nan     0.2206   -0.0000
  1540        0.0000             nan     0.2206   -0.0000
  1560        0.0000             nan     0.2206   -0.0000
  1580        0.0000             nan     0.2206   -0.0000
  1600        0.0000             nan     0.2206   -0.0000
  1620        0.0000             nan     0.2206   -0.0000
  1640        0.0000             nan     0.2206   -0.0000
  1660        0.0000             nan     0.2206   -0.0000
  1680        0.0000             nan     0.2206   -0.0000
  1700        0.0000             nan     0.2206   -0.0000
  1720        0.0000             nan     0.2206   -0.0000
  1740        0.0000             nan     0.2206   -0.0000
  1760        0.0000             nan     0.2206   -0.0000
  1780        0.0000             nan     0.2206   -0.0000
  1800        0.0000             nan     0.2206   -0.0000
  1820        0.0000             nan     0.2206   -0.0000
  1840        0.0000             nan     0.2206   -0.0000
  1860        0.0000             nan     0.2206   -0.0000
  1880        0.0000             nan     0.2206    0.0000
  1900        0.0000             nan     0.2206   -0.0000
  1920        0.0000             nan     0.2206   -0.0000
  1940        0.0000             nan     0.2206   -0.0000
  1960        0.0000             nan     0.2206   -0.0000
  1980        0.0000             nan     0.2206   -0.0000
  2000        0.0000             nan     0.2206   -0.0000
  2020        0.0000             nan     0.2206   -0.0000
  2040        0.0000             nan     0.2206   -0.0000
  2060        0.0000             nan     0.2206   -0.0000
  2080        0.0000             nan     0.2206   -0.0000
  2100        0.0000             nan     0.2206   -0.0000
  2120        0.0000             nan     0.2206   -0.0000
  2140        0.0000             nan     0.2206   -0.0000
  2160        0.0000             nan     0.2206   -0.0000
  2180        0.0000             nan     0.2206   -0.0000
  2200        0.0000             nan     0.2206   -0.0000
  2220        0.0000             nan     0.2206   -0.0000
  2240        0.0000             nan     0.2206   -0.0000
  2260        0.0000             nan     0.2206   -0.0000
  2280        0.0000             nan     0.2206   -0.0000
  2300        0.0000             nan     0.2206   -0.0000
  2320        0.0000             nan     0.2206   -0.0000
  2340        0.0000             nan     0.2206   -0.0000
  2360        0.0000             nan     0.2206   -0.0000
  2380        0.0000             nan     0.2206   -0.0000
  2400        0.0000             nan     0.2206   -0.0000
  2420        0.0000             nan     0.2206   -0.0000
  2440        0.0000             nan     0.2206   -0.0000
  2460        0.0000             nan     0.2206   -0.0000
  2480        0.0000             nan     0.2206   -0.0000
  2500        0.0000             nan     0.2206   -0.0000
  2520        0.0000             nan     0.2206   -0.0000
  2540        0.0000             nan     0.2206   -0.0000
  2560        0.0000             nan     0.2206   -0.0000
  2580        0.0000             nan     0.2206   -0.0000
  2600        0.0000             nan     0.2206   -0.0000
  2620        0.0000             nan     0.2206   -0.0000
  2640        0.0000             nan     0.2206    0.0000
  2660        0.0000             nan     0.2206   -0.0000
  2680        0.0000             nan     0.2206   -0.0000
  2700        0.0000             nan     0.2206   -0.0000
  2720        0.0000             nan     0.2206   -0.0000
  2740        0.0000             nan     0.2206    0.0000
  2760        0.0000             nan     0.2206   -0.0000
  2780        0.0000             nan     0.2206   -0.0000
  2800        0.0000             nan     0.2206   -0.0000
  2820        0.0000             nan     0.2206   -0.0000
  2840        0.0000             nan     0.2206   -0.0000
  2860        0.0000             nan     0.2206   -0.0000
  2880        0.0000             nan     0.2206   -0.0000
  2900        0.0000             nan     0.2206   -0.0000
  2920        0.0000             nan     0.2206   -0.0000
  2940        0.0000             nan     0.2206   -0.0000
  2960        0.0000             nan     0.2206   -0.0000
  2980        0.0000             nan     0.2206   -0.0000
  3000        0.0000             nan     0.2206   -0.0000
  3020        0.0000             nan     0.2206   -0.0000
  3040        0.0000             nan     0.2206   -0.0000
  3060        0.0000             nan     0.2206   -0.0000
  3080        0.0000             nan     0.2206   -0.0000
  3100        0.0000             nan     0.2206   -0.0000
  3120        0.0000             nan     0.2206   -0.0000
  3140        0.0000             nan     0.2206   -0.0000
  3160        0.0000             nan     0.2206   -0.0000
  3180        0.0000             nan     0.2206   -0.0000
  3200        0.0000             nan     0.2206   -0.0000
  3220        0.0000             nan     0.2206   -0.0000
  3222        0.0000             nan     0.2206   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2370             nan     0.2587    0.1664
     2        0.1377             nan     0.2587    0.1026
     3        0.0793             nan     0.2587    0.0615
     4        0.0477             nan     0.2587    0.0323
     5        0.0291             nan     0.2587    0.0195
     6        0.0178             nan     0.2587    0.0110
     7        0.0115             nan     0.2587    0.0055
     8        0.0081             nan     0.2587    0.0037
     9        0.0058             nan     0.2587    0.0023
    10        0.0045             nan     0.2587    0.0012
    20        0.0020             nan     0.2587    0.0000
    40        0.0011             nan     0.2587   -0.0000
    60        0.0007             nan     0.2587   -0.0000
    80        0.0005             nan     0.2587   -0.0000
   100        0.0004             nan     0.2587   -0.0000
   120        0.0003             nan     0.2587   -0.0000
   140        0.0003             nan     0.2587   -0.0000
   160        0.0002             nan     0.2587   -0.0000
   180        0.0002             nan     0.2587   -0.0000
   200        0.0002             nan     0.2587   -0.0000
   220        0.0002             nan     0.2587   -0.0000
   240        0.0001             nan     0.2587   -0.0000
   260        0.0001             nan     0.2587   -0.0000
   280        0.0001             nan     0.2587   -0.0000
   300        0.0001             nan     0.2587   -0.0000
   320        0.0001             nan     0.2587   -0.0000
   340        0.0001             nan     0.2587   -0.0000
   360        0.0001             nan     0.2587   -0.0000
   380        0.0001             nan     0.2587   -0.0000
   400        0.0001             nan     0.2587   -0.0000
   420        0.0001             nan     0.2587   -0.0000
   440        0.0001             nan     0.2587   -0.0000
   460        0.0001             nan     0.2587   -0.0000
   480        0.0001             nan     0.2587   -0.0000
   500        0.0001             nan     0.2587   -0.0000
   520        0.0000             nan     0.2587   -0.0000
   540        0.0000             nan     0.2587   -0.0000
   560        0.0000             nan     0.2587   -0.0000
   580        0.0000             nan     0.2587   -0.0000
   600        0.0000             nan     0.2587   -0.0000
   620        0.0000             nan     0.2587   -0.0000
   640        0.0000             nan     0.2587   -0.0000
   660        0.0000             nan     0.2587   -0.0000
   680        0.0000             nan     0.2587   -0.0000
   700        0.0000             nan     0.2587   -0.0000
   720        0.0000             nan     0.2587   -0.0000
   740        0.0000             nan     0.2587   -0.0000
   760        0.0000             nan     0.2587   -0.0000
   780        0.0000             nan     0.2587   -0.0000
   800        0.0000             nan     0.2587   -0.0000
   820        0.0000             nan     0.2587   -0.0000
   840        0.0000             nan     0.2587   -0.0000
   860        0.0000             nan     0.2587   -0.0000
   880        0.0000             nan     0.2587   -0.0000
   900        0.0000             nan     0.2587    0.0000
   920        0.0000             nan     0.2587   -0.0000
   940        0.0000             nan     0.2587   -0.0000
   960        0.0000             nan     0.2587    0.0000
   980        0.0000             nan     0.2587   -0.0000
  1000        0.0000             nan     0.2587   -0.0000
  1020        0.0000             nan     0.2587   -0.0000
  1040        0.0000             nan     0.2587   -0.0000
  1060        0.0000             nan     0.2587   -0.0000
  1080        0.0000             nan     0.2587   -0.0000
  1100        0.0000             nan     0.2587   -0.0000
  1120        0.0000             nan     0.2587   -0.0000
  1140        0.0000             nan     0.2587   -0.0000
  1160        0.0000             nan     0.2587   -0.0000
  1180        0.0000             nan     0.2587   -0.0000
  1200        0.0000             nan     0.2587   -0.0000
  1220        0.0000             nan     0.2587   -0.0000
  1240        0.0000             nan     0.2587   -0.0000
  1260        0.0000             nan     0.2587   -0.0000
  1280        0.0000             nan     0.2587   -0.0000
  1300        0.0000             nan     0.2587   -0.0000
  1320        0.0000             nan     0.2587   -0.0000
  1340        0.0000             nan     0.2587   -0.0000
  1360        0.0000             nan     0.2587   -0.0000
  1380        0.0000             nan     0.2587   -0.0000
  1400        0.0000             nan     0.2587   -0.0000
  1420        0.0000             nan     0.2587   -0.0000
  1440        0.0000             nan     0.2587   -0.0000
  1460        0.0000             nan     0.2587    0.0000
  1480        0.0000             nan     0.2587   -0.0000
  1500        0.0000             nan     0.2587    0.0000
  1520        0.0000             nan     0.2587   -0.0000
  1540        0.0000             nan     0.2587   -0.0000
  1560        0.0000             nan     0.2587    0.0000
  1580        0.0000             nan     0.2587   -0.0000
  1600        0.0000             nan     0.2587   -0.0000
  1620        0.0000             nan     0.2587   -0.0000
  1640        0.0000             nan     0.2587   -0.0000
  1660        0.0000             nan     0.2587   -0.0000
  1680        0.0000             nan     0.2587   -0.0000
  1700        0.0000             nan     0.2587    0.0000
  1720        0.0000             nan     0.2587    0.0000
  1740        0.0000             nan     0.2587   -0.0000
  1760        0.0000             nan     0.2587   -0.0000
  1780        0.0000             nan     0.2587    0.0000
  1800        0.0000             nan     0.2587   -0.0000
  1820        0.0000             nan     0.2587   -0.0000
  1840        0.0000             nan     0.2587    0.0000
  1860        0.0000             nan     0.2587   -0.0000
  1880        0.0000             nan     0.2587   -0.0000
  1900        0.0000             nan     0.2587   -0.0000
  1920        0.0000             nan     0.2587   -0.0000
  1940        0.0000             nan     0.2587   -0.0000
  1960        0.0000             nan     0.2587   -0.0000
  1980        0.0000             nan     0.2587   -0.0000
  2000        0.0000             nan     0.2587   -0.0000
  2020        0.0000             nan     0.2587   -0.0000
  2040        0.0000             nan     0.2587   -0.0000
  2060        0.0000             nan     0.2587   -0.0000
  2080        0.0000             nan     0.2587   -0.0000
  2100        0.0000             nan     0.2587    0.0000
  2120        0.0000             nan     0.2587   -0.0000
  2140        0.0000             nan     0.2587   -0.0000
  2160        0.0000             nan     0.2587   -0.0000
  2180        0.0000             nan     0.2587   -0.0000
  2200        0.0000             nan     0.2587   -0.0000
  2220        0.0000             nan     0.2587   -0.0000
  2240        0.0000             nan     0.2587   -0.0000
  2260        0.0000             nan     0.2587   -0.0000
  2280        0.0000             nan     0.2587   -0.0000
  2300        0.0000             nan     0.2587   -0.0000
  2320        0.0000             nan     0.2587    0.0000
  2340        0.0000             nan     0.2587   -0.0000
  2360        0.0000             nan     0.2587   -0.0000
  2380        0.0000             nan     0.2587   -0.0000
  2400        0.0000             nan     0.2587   -0.0000
  2420        0.0000             nan     0.2587   -0.0000
  2440        0.0000             nan     0.2587   -0.0000
  2460        0.0000             nan     0.2587   -0.0000
  2480        0.0000             nan     0.2587   -0.0000
  2500        0.0000             nan     0.2587   -0.0000
  2520        0.0000             nan     0.2587   -0.0000
  2540        0.0000             nan     0.2587    0.0000
  2560        0.0000             nan     0.2587   -0.0000
  2580        0.0000             nan     0.2587   -0.0000
  2600        0.0000             nan     0.2587   -0.0000
  2620        0.0000             nan     0.2587    0.0000
  2640        0.0000             nan     0.2587   -0.0000
  2660        0.0000             nan     0.2587   -0.0000
  2680        0.0000             nan     0.2587   -0.0000
  2700        0.0000             nan     0.2587    0.0000
  2720        0.0000             nan     0.2587    0.0000
  2740        0.0000             nan     0.2587   -0.0000
  2760        0.0000             nan     0.2587   -0.0000
  2780        0.0000             nan     0.2587    0.0000
  2800        0.0000             nan     0.2587   -0.0000
  2820        0.0000             nan     0.2587   -0.0000
  2840        0.0000             nan     0.2587   -0.0000
  2860        0.0000             nan     0.2587   -0.0000
  2880        0.0000             nan     0.2587    0.0000
  2900        0.0000             nan     0.2587   -0.0000
  2920        0.0000             nan     0.2587   -0.0000
  2940        0.0000             nan     0.2587    0.0000
  2960        0.0000             nan     0.2587   -0.0000
  2980        0.0000             nan     0.2587   -0.0000
  3000        0.0000             nan     0.2587   -0.0000
  3020        0.0000             nan     0.2587   -0.0000
  3022        0.0000             nan     0.2587   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2305             nan     0.2980    0.1879
     2        0.1339             nan     0.2980    0.0837
     3        0.0800             nan     0.2980    0.0559
     4        0.0477             nan     0.2980    0.0319
     5        0.0300             nan     0.2980    0.0173
     6        0.0196             nan     0.2980    0.0113
     7        0.0135             nan     0.2980    0.0058
     8        0.0100             nan     0.2980    0.0031
     9        0.0077             nan     0.2980    0.0023
    10        0.0062             nan     0.2980    0.0014
    20        0.0032             nan     0.2980   -0.0000
    40        0.0019             nan     0.2980   -0.0000
    60        0.0014             nan     0.2980   -0.0000
    80        0.0011             nan     0.2980   -0.0000
   100        0.0009             nan     0.2980   -0.0000
   120        0.0008             nan     0.2980   -0.0000
   140        0.0007             nan     0.2980   -0.0000
   160        0.0007             nan     0.2980   -0.0000
   180        0.0006             nan     0.2980   -0.0000
   200        0.0005             nan     0.2980   -0.0000
   220        0.0005             nan     0.2980   -0.0000
   240        0.0004             nan     0.2980   -0.0000
   260        0.0004             nan     0.2980   -0.0000
   280        0.0004             nan     0.2980   -0.0000
   300        0.0003             nan     0.2980   -0.0000
   320        0.0003             nan     0.2980   -0.0000
   340        0.0003             nan     0.2980   -0.0000
   360        0.0003             nan     0.2980   -0.0000
   380        0.0002             nan     0.2980   -0.0000
   400        0.0002             nan     0.2980   -0.0000
   420        0.0002             nan     0.2980   -0.0000
   440        0.0002             nan     0.2980   -0.0000
   460        0.0002             nan     0.2980   -0.0000
   480        0.0002             nan     0.2980   -0.0000
   500        0.0002             nan     0.2980   -0.0000
   520        0.0002             nan     0.2980   -0.0000
   540        0.0001             nan     0.2980   -0.0000
   560        0.0001             nan     0.2980   -0.0000
   580        0.0001             nan     0.2980   -0.0000
   600        0.0001             nan     0.2980   -0.0000
   620        0.0001             nan     0.2980   -0.0000
   640        0.0001             nan     0.2980   -0.0000
   660        0.0001             nan     0.2980   -0.0000
   680        0.0001             nan     0.2980   -0.0000
   700        0.0001             nan     0.2980   -0.0000
   720        0.0001             nan     0.2980   -0.0000
   740        0.0001             nan     0.2980   -0.0000
   760        0.0001             nan     0.2980   -0.0000
   780        0.0001             nan     0.2980   -0.0000
   800        0.0001             nan     0.2980   -0.0000
   820        0.0001             nan     0.2980   -0.0000
   840        0.0001             nan     0.2980   -0.0000
   860        0.0001             nan     0.2980   -0.0000
   880        0.0001             nan     0.2980   -0.0000
   900        0.0001             nan     0.2980   -0.0000
   920        0.0001             nan     0.2980   -0.0000
   940        0.0001             nan     0.2980   -0.0000
   960        0.0001             nan     0.2980   -0.0000
   980        0.0001             nan     0.2980   -0.0000
  1000        0.0001             nan     0.2980   -0.0000
  1020        0.0001             nan     0.2980   -0.0000
  1040        0.0001             nan     0.2980   -0.0000
  1060        0.0001             nan     0.2980   -0.0000
  1080        0.0001             nan     0.2980   -0.0000
  1100        0.0001             nan     0.2980   -0.0000
  1120        0.0001             nan     0.2980   -0.0000
  1140        0.0001             nan     0.2980   -0.0000
  1160        0.0000             nan     0.2980   -0.0000
  1180        0.0000             nan     0.2980   -0.0000
  1200        0.0000             nan     0.2980   -0.0000
  1220        0.0000             nan     0.2980   -0.0000
  1240        0.0000             nan     0.2980   -0.0000
  1260        0.0000             nan     0.2980   -0.0000
  1280        0.0000             nan     0.2980   -0.0000
  1300        0.0000             nan     0.2980   -0.0000
  1320        0.0000             nan     0.2980   -0.0000
  1340        0.0000             nan     0.2980   -0.0000
  1360        0.0000             nan     0.2980   -0.0000
  1380        0.0000             nan     0.2980   -0.0000
  1400        0.0000             nan     0.2980   -0.0000
  1420        0.0000             nan     0.2980   -0.0000
  1440        0.0000             nan     0.2980   -0.0000
  1460        0.0000             nan     0.2980   -0.0000
  1480        0.0000             nan     0.2980   -0.0000
  1500        0.0000             nan     0.2980   -0.0000
  1520        0.0000             nan     0.2980   -0.0000
  1540        0.0000             nan     0.2980   -0.0000
  1560        0.0000             nan     0.2980   -0.0000
  1580        0.0000             nan     0.2980   -0.0000
  1600        0.0000             nan     0.2980   -0.0000
  1620        0.0000             nan     0.2980   -0.0000
  1640        0.0000             nan     0.2980   -0.0000
  1660        0.0000             nan     0.2980   -0.0000
  1680        0.0000             nan     0.2980   -0.0000
  1700        0.0000             nan     0.2980   -0.0000
  1720        0.0000             nan     0.2980    0.0000
  1740        0.0000             nan     0.2980   -0.0000
  1760        0.0000             nan     0.2980   -0.0000
  1780        0.0000             nan     0.2980   -0.0000
  1800        0.0000             nan     0.2980   -0.0000
  1820        0.0000             nan     0.2980   -0.0000
  1840        0.0000             nan     0.2980   -0.0000
  1860        0.0000             nan     0.2980   -0.0000
  1880        0.0000             nan     0.2980   -0.0000
  1900        0.0000             nan     0.2980   -0.0000
  1920        0.0000             nan     0.2980   -0.0000
  1940        0.0000             nan     0.2980    0.0000
  1960        0.0000             nan     0.2980   -0.0000
  1980        0.0000             nan     0.2980   -0.0000
  2000        0.0000             nan     0.2980   -0.0000
  2020        0.0000             nan     0.2980   -0.0000
  2040        0.0000             nan     0.2980   -0.0000
  2060        0.0000             nan     0.2980   -0.0000
  2080        0.0000             nan     0.2980   -0.0000
  2100        0.0000             nan     0.2980   -0.0000
  2120        0.0000             nan     0.2980   -0.0000
  2140        0.0000             nan     0.2980   -0.0000
  2160        0.0000             nan     0.2980   -0.0000
  2180        0.0000             nan     0.2980   -0.0000
  2200        0.0000             nan     0.2980   -0.0000
  2220        0.0000             nan     0.2980    0.0000
  2240        0.0000             nan     0.2980   -0.0000
  2260        0.0000             nan     0.2980   -0.0000
  2280        0.0000             nan     0.2980   -0.0000
  2300        0.0000             nan     0.2980   -0.0000
  2320        0.0000             nan     0.2980   -0.0000
  2340        0.0000             nan     0.2980   -0.0000
  2360        0.0000             nan     0.2980   -0.0000
  2380        0.0000             nan     0.2980    0.0000
  2400        0.0000             nan     0.2980   -0.0000
  2420        0.0000             nan     0.2980   -0.0000
  2440        0.0000             nan     0.2980   -0.0000
  2460        0.0000             nan     0.2980   -0.0000
  2480        0.0000             nan     0.2980   -0.0000
  2500        0.0000             nan     0.2980   -0.0000
  2520        0.0000             nan     0.2980   -0.0000
  2540        0.0000             nan     0.2980   -0.0000
  2560        0.0000             nan     0.2980   -0.0000
  2580        0.0000             nan     0.2980   -0.0000
  2600        0.0000             nan     0.2980   -0.0000
  2620        0.0000             nan     0.2980    0.0000
  2640        0.0000             nan     0.2980   -0.0000
  2660        0.0000             nan     0.2980   -0.0000
  2680        0.0000             nan     0.2980   -0.0000
  2700        0.0000             nan     0.2980    0.0000
  2720        0.0000             nan     0.2980   -0.0000
  2740        0.0000             nan     0.2980   -0.0000
  2760        0.0000             nan     0.2980   -0.0000
  2780        0.0000             nan     0.2980   -0.0000
  2800        0.0000             nan     0.2980   -0.0000
  2820        0.0000             nan     0.2980   -0.0000
  2840        0.0000             nan     0.2980   -0.0000
  2860        0.0000             nan     0.2980   -0.0000
  2880        0.0000             nan     0.2980   -0.0000
  2900        0.0000             nan     0.2980   -0.0000
  2920        0.0000             nan     0.2980   -0.0000
  2940        0.0000             nan     0.2980   -0.0000
  2960        0.0000             nan     0.2980   -0.0000
  2980        0.0000             nan     0.2980   -0.0000
  3000        0.0000             nan     0.2980   -0.0000
  3020        0.0000             nan     0.2980   -0.0000
  3040        0.0000             nan     0.2980   -0.0000
  3060        0.0000             nan     0.2980   -0.0000
  3080        0.0000             nan     0.2980   -0.0000
  3100        0.0000             nan     0.2980   -0.0000
  3120        0.0000             nan     0.2980   -0.0000
  3140        0.0000             nan     0.2980   -0.0000
  3160        0.0000             nan     0.2980   -0.0000
  3180        0.0000             nan     0.2980   -0.0000
  3200        0.0000             nan     0.2980   -0.0000
  3220        0.0000             nan     0.2980   -0.0000
  3240        0.0000             nan     0.2980   -0.0000
  3260        0.0000             nan     0.2980   -0.0000
  3280        0.0000             nan     0.2980   -0.0000
  3300        0.0000             nan     0.2980   -0.0000
  3320        0.0000             nan     0.2980   -0.0000
  3340        0.0000             nan     0.2980   -0.0000
  3360        0.0000             nan     0.2980   -0.0000
  3380        0.0000             nan     0.2980   -0.0000
  3400        0.0000             nan     0.2980    0.0000
  3420        0.0000             nan     0.2980   -0.0000
  3440        0.0000             nan     0.2980   -0.0000
  3460        0.0000             nan     0.2980    0.0000
  3480        0.0000             nan     0.2980   -0.0000
  3500        0.0000             nan     0.2980   -0.0000
  3520        0.0000             nan     0.2980   -0.0000
  3540        0.0000             nan     0.2980   -0.0000
  3560        0.0000             nan     0.2980   -0.0000
  3580        0.0000             nan     0.2980   -0.0000
  3600        0.0000             nan     0.2980   -0.0000
  3620        0.0000             nan     0.2980   -0.0000
  3640        0.0000             nan     0.2980   -0.0000
  3660        0.0000             nan     0.2980   -0.0000
  3680        0.0000             nan     0.2980   -0.0000
  3700        0.0000             nan     0.2980   -0.0000
  3720        0.0000             nan     0.2980   -0.0000
  3740        0.0000             nan     0.2980   -0.0000
  3760        0.0000             nan     0.2980   -0.0000
  3780        0.0000             nan     0.2980   -0.0000
  3800        0.0000             nan     0.2980   -0.0000
  3820        0.0000             nan     0.2980   -0.0000
  3840        0.0000             nan     0.2980   -0.0000
  3860        0.0000             nan     0.2980    0.0000
  3880        0.0000             nan     0.2980    0.0000
  3900        0.0000             nan     0.2980   -0.0000
  3920        0.0000             nan     0.2980   -0.0000
  3940        0.0000             nan     0.2980   -0.0000
  3960        0.0000             nan     0.2980   -0.0000
  3980        0.0000             nan     0.2980    0.0000
  4000        0.0000             nan     0.2980   -0.0000
  4020        0.0000             nan     0.2980   -0.0000
  4040        0.0000             nan     0.2980   -0.0000
  4060        0.0000             nan     0.2980   -0.0000
  4080        0.0000             nan     0.2980   -0.0000
  4100        0.0000             nan     0.2980   -0.0000
  4120        0.0000             nan     0.2980   -0.0000
  4121        0.0000             nan     0.2980   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1979             nan     0.3332    0.2167
     2        0.0988             nan     0.3332    0.0990
     3        0.0486             nan     0.3332    0.0494
     4        0.0257             nan     0.3332    0.0213
     5        0.0137             nan     0.3332    0.0118
     6        0.0080             nan     0.3332    0.0055
     7        0.0055             nan     0.3332    0.0026
     8        0.0040             nan     0.3332    0.0013
     9        0.0033             nan     0.3332    0.0006
    10        0.0028             nan     0.3332    0.0003
    20        0.0015             nan     0.3332   -0.0000
    40        0.0008             nan     0.3332   -0.0000
    60        0.0006             nan     0.3332   -0.0000
    80        0.0004             nan     0.3332   -0.0000
   100        0.0003             nan     0.3332   -0.0000
   120        0.0003             nan     0.3332   -0.0000
   140        0.0002             nan     0.3332   -0.0000
   160        0.0002             nan     0.3332   -0.0000
   180        0.0001             nan     0.3332   -0.0000
   200        0.0001             nan     0.3332   -0.0000
   220        0.0001             nan     0.3332   -0.0000
   240        0.0001             nan     0.3332   -0.0000
   260        0.0001             nan     0.3332   -0.0000
   280        0.0001             nan     0.3332   -0.0000
   300        0.0001             nan     0.3332   -0.0000
   320        0.0001             nan     0.3332   -0.0000
   340        0.0000             nan     0.3332   -0.0000
   360        0.0000             nan     0.3332   -0.0000
   380        0.0000             nan     0.3332   -0.0000
   400        0.0000             nan     0.3332   -0.0000
   420        0.0000             nan     0.3332   -0.0000
   440        0.0000             nan     0.3332   -0.0000
   460        0.0000             nan     0.3332   -0.0000
   480        0.0000             nan     0.3332   -0.0000
   500        0.0000             nan     0.3332   -0.0000
   520        0.0000             nan     0.3332   -0.0000
   540        0.0000             nan     0.3332   -0.0000
   560        0.0000             nan     0.3332   -0.0000
   580        0.0000             nan     0.3332   -0.0000
   600        0.0000             nan     0.3332   -0.0000
   620        0.0000             nan     0.3332    0.0000
   640        0.0000             nan     0.3332   -0.0000
   660        0.0000             nan     0.3332   -0.0000
   680        0.0000             nan     0.3332   -0.0000
   700        0.0000             nan     0.3332   -0.0000
   720        0.0000             nan     0.3332   -0.0000
   740        0.0000             nan     0.3332   -0.0000
   760        0.0000             nan     0.3332   -0.0000
   780        0.0000             nan     0.3332   -0.0000
   800        0.0000             nan     0.3332   -0.0000
   820        0.0000             nan     0.3332   -0.0000
   840        0.0000             nan     0.3332    0.0000
   860        0.0000             nan     0.3332   -0.0000
   880        0.0000             nan     0.3332    0.0000
   900        0.0000             nan     0.3332   -0.0000
   920        0.0000             nan     0.3332   -0.0000
   940        0.0000             nan     0.3332   -0.0000
   960        0.0000             nan     0.3332   -0.0000
   980        0.0000             nan     0.3332   -0.0000
  1000        0.0000             nan     0.3332   -0.0000
  1020        0.0000             nan     0.3332   -0.0000
  1040        0.0000             nan     0.3332    0.0000
  1060        0.0000             nan     0.3332   -0.0000
  1080        0.0000             nan     0.3332   -0.0000
  1100        0.0000             nan     0.3332   -0.0000
  1120        0.0000             nan     0.3332   -0.0000
  1140        0.0000             nan     0.3332   -0.0000
  1160        0.0000             nan     0.3332   -0.0000
  1180        0.0000             nan     0.3332   -0.0000
  1200        0.0000             nan     0.3332    0.0000
  1220        0.0000             nan     0.3332   -0.0000
  1240        0.0000             nan     0.3332   -0.0000
  1260        0.0000             nan     0.3332    0.0000
  1280        0.0000             nan     0.3332    0.0000
  1300        0.0000             nan     0.3332   -0.0000
  1320        0.0000             nan     0.3332   -0.0000
  1340        0.0000             nan     0.3332    0.0000
  1360        0.0000             nan     0.3332    0.0000
  1380        0.0000             nan     0.3332    0.0000
  1400        0.0000             nan     0.3332    0.0000
  1420        0.0000             nan     0.3332   -0.0000
  1440        0.0000             nan     0.3332    0.0000
  1460        0.0000             nan     0.3332    0.0000
  1480        0.0000             nan     0.3332   -0.0000
  1500        0.0000             nan     0.3332   -0.0000
  1520        0.0000             nan     0.3332   -0.0000
  1540        0.0000             nan     0.3332   -0.0000
  1560        0.0000             nan     0.3332   -0.0000
  1580        0.0000             nan     0.3332   -0.0000
  1600        0.0000             nan     0.3332   -0.0000
  1620        0.0000             nan     0.3332   -0.0000
  1640        0.0000             nan     0.3332   -0.0000
  1660        0.0000             nan     0.3332   -0.0000
  1680        0.0000             nan     0.3332   -0.0000
  1700        0.0000             nan     0.3332   -0.0000
  1720        0.0000             nan     0.3332   -0.0000
  1740        0.0000             nan     0.3332   -0.0000
  1760        0.0000             nan     0.3332    0.0000
  1780        0.0000             nan     0.3332   -0.0000
  1800        0.0000             nan     0.3332   -0.0000
  1820        0.0000             nan     0.3332   -0.0000
  1840        0.0000             nan     0.3332   -0.0000
  1860        0.0000             nan     0.3332   -0.0000
  1880        0.0000             nan     0.3332   -0.0000
  1900        0.0000             nan     0.3332   -0.0000
  1920        0.0000             nan     0.3332    0.0000
  1940        0.0000             nan     0.3332   -0.0000
  1960        0.0000             nan     0.3332    0.0000
  1980        0.0000             nan     0.3332   -0.0000
  2000        0.0000             nan     0.3332   -0.0000
  2020        0.0000             nan     0.3332   -0.0000
  2040        0.0000             nan     0.3332   -0.0000
  2060        0.0000             nan     0.3332   -0.0000
  2080        0.0000             nan     0.3332   -0.0000
  2100        0.0000             nan     0.3332    0.0000
  2120        0.0000             nan     0.3332   -0.0000
  2140        0.0000             nan     0.3332   -0.0000
  2160        0.0000             nan     0.3332   -0.0000
  2180        0.0000             nan     0.3332   -0.0000
  2200        0.0000             nan     0.3332    0.0000
  2220        0.0000             nan     0.3332   -0.0000
  2240        0.0000             nan     0.3332   -0.0000
  2260        0.0000             nan     0.3332   -0.0000
  2280        0.0000             nan     0.3332   -0.0000
  2300        0.0000             nan     0.3332   -0.0000
  2320        0.0000             nan     0.3332   -0.0000
  2340        0.0000             nan     0.3332   -0.0000
  2360        0.0000             nan     0.3332    0.0000
  2380        0.0000             nan     0.3332    0.0000
  2400        0.0000             nan     0.3332   -0.0000
  2420        0.0000             nan     0.3332   -0.0000
  2440        0.0000             nan     0.3332   -0.0000
  2460        0.0000             nan     0.3332    0.0000
  2480        0.0000             nan     0.3332   -0.0000
  2500        0.0000             nan     0.3332   -0.0000
  2520        0.0000             nan     0.3332    0.0000
  2540        0.0000             nan     0.3332   -0.0000
  2560        0.0000             nan     0.3332   -0.0000
  2580        0.0000             nan     0.3332   -0.0000
  2600        0.0000             nan     0.3332    0.0000
  2620        0.0000             nan     0.3332   -0.0000
  2640        0.0000             nan     0.3332    0.0000
  2660        0.0000             nan     0.3332   -0.0000
  2680        0.0000             nan     0.3332   -0.0000
  2700        0.0000             nan     0.3332    0.0000
  2720        0.0000             nan     0.3332   -0.0000
  2740        0.0000             nan     0.3332   -0.0000
  2760        0.0000             nan     0.3332   -0.0000
  2780        0.0000             nan     0.3332   -0.0000
  2800        0.0000             nan     0.3332   -0.0000
  2820        0.0000             nan     0.3332   -0.0000
  2840        0.0000             nan     0.3332    0.0000
  2860        0.0000             nan     0.3332   -0.0000
  2880        0.0000             nan     0.3332   -0.0000
  2900        0.0000             nan     0.3332    0.0000
  2920        0.0000             nan     0.3332   -0.0000
  2940        0.0000             nan     0.3332   -0.0000
  2960        0.0000             nan     0.3332   -0.0000
  2966        0.0000             nan     0.3332   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1650             nan     0.3949    0.2379
     2        0.0704             nan     0.3949    0.0938
     3        0.0296             nan     0.3949    0.0389
     4        0.0142             nan     0.3949    0.0177
     5        0.0076             nan     0.3949    0.0058
     6        0.0049             nan     0.3949    0.0025
     7        0.0035             nan     0.3949    0.0011
     8        0.0028             nan     0.3949    0.0005
     9        0.0024             nan     0.3949    0.0002
    10        0.0021             nan     0.3949    0.0001
    20        0.0013             nan     0.3949   -0.0000
    40        0.0006             nan     0.3949   -0.0000
    60        0.0004             nan     0.3949   -0.0000
    80        0.0003             nan     0.3949   -0.0000
   100        0.0002             nan     0.3949   -0.0000
   120        0.0001             nan     0.3949   -0.0000
   140        0.0001             nan     0.3949   -0.0000
   160        0.0001             nan     0.3949   -0.0000
   180        0.0001             nan     0.3949   -0.0000
   200        0.0000             nan     0.3949   -0.0000
   220        0.0000             nan     0.3949   -0.0000
   240        0.0000             nan     0.3949   -0.0000
   260        0.0000             nan     0.3949   -0.0000
   280        0.0000             nan     0.3949   -0.0000
   300        0.0000             nan     0.3949   -0.0000
   320        0.0000             nan     0.3949   -0.0000
   340        0.0000             nan     0.3949   -0.0000
   360        0.0000             nan     0.3949   -0.0000
   380        0.0000             nan     0.3949   -0.0000
   400        0.0000             nan     0.3949   -0.0000
   420        0.0000             nan     0.3949   -0.0000
   440        0.0000             nan     0.3949   -0.0000
   460        0.0000             nan     0.3949   -0.0000
   480        0.0000             nan     0.3949   -0.0000
   500        0.0000             nan     0.3949   -0.0000
   520        0.0000             nan     0.3949   -0.0000
   540        0.0000             nan     0.3949   -0.0000
   560        0.0000             nan     0.3949   -0.0000
   580        0.0000             nan     0.3949   -0.0000
   600        0.0000             nan     0.3949   -0.0000
   620        0.0000             nan     0.3949   -0.0000
   640        0.0000             nan     0.3949   -0.0000
   660        0.0000             nan     0.3949   -0.0000
   680        0.0000             nan     0.3949   -0.0000
   700        0.0000             nan     0.3949   -0.0000
   720        0.0000             nan     0.3949   -0.0000
   740        0.0000             nan     0.3949   -0.0000
   760        0.0000             nan     0.3949   -0.0000
   780        0.0000             nan     0.3949   -0.0000
   800        0.0000             nan     0.3949   -0.0000
   820        0.0000             nan     0.3949   -0.0000
   840        0.0000             nan     0.3949   -0.0000
   860        0.0000             nan     0.3949   -0.0000
   880        0.0000             nan     0.3949   -0.0000
   900        0.0000             nan     0.3949   -0.0000
   920        0.0000             nan     0.3949   -0.0000
   940        0.0000             nan     0.3949   -0.0000
   960        0.0000             nan     0.3949   -0.0000
   980        0.0000             nan     0.3949   -0.0000
  1000        0.0000             nan     0.3949   -0.0000
  1020        0.0000             nan     0.3949   -0.0000
  1040        0.0000             nan     0.3949   -0.0000
  1060        0.0000             nan     0.3949   -0.0000
  1080        0.0000             nan     0.3949   -0.0000
  1100        0.0000             nan     0.3949   -0.0000
  1120        0.0000             nan     0.3949   -0.0000
  1140        0.0000             nan     0.3949   -0.0000
  1160        0.0000             nan     0.3949   -0.0000
  1168        0.0000             nan     0.3949   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1684             nan     0.4223    0.2363
     2        0.0784             nan     0.4223    0.0881
     3        0.0380             nan     0.4223    0.0340
     4        0.0208             nan     0.4223    0.0153
     5        0.0134             nan     0.4223    0.0076
     6        0.0097             nan     0.4223    0.0031
     7        0.0081             nan     0.4223    0.0008
     8        0.0071             nan     0.4223    0.0008
     9        0.0066             nan     0.4223    0.0002
    10        0.0057             nan     0.4223    0.0005
    20        0.0032             nan     0.4223   -0.0000
    40        0.0019             nan     0.4223   -0.0001
    60        0.0014             nan     0.4223   -0.0000
    80        0.0010             nan     0.4223   -0.0000
   100        0.0008             nan     0.4223   -0.0000
   120        0.0008             nan     0.4223   -0.0000
   140        0.0007             nan     0.4223   -0.0000
   160        0.0006             nan     0.4223   -0.0000
   180        0.0005             nan     0.4223   -0.0000
   200        0.0005             nan     0.4223   -0.0000
   220        0.0004             nan     0.4223   -0.0000
   240        0.0004             nan     0.4223   -0.0000
   260        0.0004             nan     0.4223   -0.0000
   280        0.0003             nan     0.4223   -0.0000
   300        0.0003             nan     0.4223   -0.0000
   320        0.0003             nan     0.4223   -0.0000
   340        0.0002             nan     0.4223   -0.0000
   360        0.0002             nan     0.4223   -0.0000
   380        0.0002             nan     0.4223   -0.0000
   400        0.0002             nan     0.4223   -0.0000
   420        0.0002             nan     0.4223   -0.0000
   440        0.0002             nan     0.4223   -0.0000
   460        0.0002             nan     0.4223   -0.0000
   480        0.0001             nan     0.4223   -0.0000
   500        0.0001             nan     0.4223   -0.0000
   520        0.0001             nan     0.4223   -0.0000
   540        0.0001             nan     0.4223   -0.0000
   560        0.0001             nan     0.4223   -0.0000
   580        0.0001             nan     0.4223   -0.0000
   600        0.0001             nan     0.4223   -0.0000
   620        0.0001             nan     0.4223   -0.0000
   640        0.0001             nan     0.4223   -0.0000
   660        0.0001             nan     0.4223   -0.0000
   680        0.0001             nan     0.4223   -0.0000
   700        0.0001             nan     0.4223   -0.0000
   720        0.0001             nan     0.4223   -0.0000
   740        0.0001             nan     0.4223   -0.0000
   760        0.0001             nan     0.4223   -0.0000
   780        0.0001             nan     0.4223   -0.0000
   800        0.0001             nan     0.4223   -0.0000
   820        0.0001             nan     0.4223   -0.0000
   840        0.0001             nan     0.4223   -0.0000
   860        0.0001             nan     0.4223   -0.0000
   880        0.0000             nan     0.4223   -0.0000
   900        0.0000             nan     0.4223   -0.0000
   920        0.0000             nan     0.4223   -0.0000
   940        0.0000             nan     0.4223   -0.0000
   960        0.0000             nan     0.4223   -0.0000
   980        0.0000             nan     0.4223   -0.0000
  1000        0.0000             nan     0.4223   -0.0000
  1020        0.0000             nan     0.4223   -0.0000
  1040        0.0000             nan     0.4223    0.0000
  1060        0.0000             nan     0.4223   -0.0000
  1080        0.0000             nan     0.4223   -0.0000
  1100        0.0000             nan     0.4223   -0.0000
  1120        0.0000             nan     0.4223   -0.0000
  1140        0.0000             nan     0.4223    0.0000
  1160        0.0000             nan     0.4223   -0.0000
  1180        0.0000             nan     0.4223   -0.0000
  1200        0.0000             nan     0.4223   -0.0000
  1220        0.0000             nan     0.4223   -0.0000
  1240        0.0000             nan     0.4223   -0.0000
  1260        0.0000             nan     0.4223   -0.0000
  1280        0.0000             nan     0.4223   -0.0000
  1300        0.0000             nan     0.4223   -0.0000
  1320        0.0000             nan     0.4223   -0.0000
  1340        0.0000             nan     0.4223   -0.0000
  1360        0.0000             nan     0.4223   -0.0000
  1380        0.0000             nan     0.4223   -0.0000
  1400        0.0000             nan     0.4223   -0.0000
  1420        0.0000             nan     0.4223   -0.0000
  1440        0.0000             nan     0.4223   -0.0000
  1460        0.0000             nan     0.4223   -0.0000
  1480        0.0000             nan     0.4223   -0.0000
  1500        0.0000             nan     0.4223   -0.0000
  1520        0.0000             nan     0.4223   -0.0000
  1540        0.0000             nan     0.4223   -0.0000
  1560        0.0000             nan     0.4223    0.0000
  1580        0.0000             nan     0.4223   -0.0000
  1600        0.0000             nan     0.4223   -0.0000
  1620        0.0000             nan     0.4223   -0.0000
  1640        0.0000             nan     0.4223   -0.0000
  1660        0.0000             nan     0.4223    0.0000
  1680        0.0000             nan     0.4223   -0.0000
  1700        0.0000             nan     0.4223   -0.0000
  1720        0.0000             nan     0.4223   -0.0000
  1740        0.0000             nan     0.4223   -0.0000
  1760        0.0000             nan     0.4223   -0.0000
  1780        0.0000             nan     0.4223   -0.0000
  1800        0.0000             nan     0.4223   -0.0000
  1820        0.0000             nan     0.4223   -0.0000
  1840        0.0000             nan     0.4223   -0.0000
  1860        0.0000             nan     0.4223    0.0000
  1880        0.0000             nan     0.4223   -0.0000
  1900        0.0000             nan     0.4223   -0.0000
  1920        0.0000             nan     0.4223   -0.0000
  1940        0.0000             nan     0.4223    0.0000
  1960        0.0000             nan     0.4223   -0.0000
  1980        0.0000             nan     0.4223   -0.0000
  2000        0.0000             nan     0.4223    0.0000
  2020        0.0000             nan     0.4223   -0.0000
  2040        0.0000             nan     0.4223   -0.0000
  2060        0.0000             nan     0.4223   -0.0000
  2080        0.0000             nan     0.4223    0.0000
  2100        0.0000             nan     0.4223   -0.0000
  2120        0.0000             nan     0.4223    0.0000
  2140        0.0000             nan     0.4223   -0.0000
  2160        0.0000             nan     0.4223   -0.0000
  2180        0.0000             nan     0.4223   -0.0000
  2200        0.0000             nan     0.4223   -0.0000
  2220        0.0000             nan     0.4223   -0.0000
  2240        0.0000             nan     0.4223   -0.0000
  2260        0.0000             nan     0.4223   -0.0000
  2280        0.0000             nan     0.4223   -0.0000
  2300        0.0000             nan     0.4223   -0.0000
  2320        0.0000             nan     0.4223   -0.0000
  2340        0.0000             nan     0.4223   -0.0000
  2360        0.0000             nan     0.4223   -0.0000
  2380        0.0000             nan     0.4223   -0.0000
  2400        0.0000             nan     0.4223   -0.0000
  2420        0.0000             nan     0.4223   -0.0000
  2440        0.0000             nan     0.4223   -0.0000
  2460        0.0000             nan     0.4223   -0.0000
  2480        0.0000             nan     0.4223   -0.0000
  2500        0.0000             nan     0.4223   -0.0000
  2520        0.0000             nan     0.4223   -0.0000
  2540        0.0000             nan     0.4223   -0.0000
  2560        0.0000             nan     0.4223    0.0000
  2580        0.0000             nan     0.4223   -0.0000
  2600        0.0000             nan     0.4223   -0.0000
  2620        0.0000             nan     0.4223    0.0000
  2640        0.0000             nan     0.4223   -0.0000
  2660        0.0000             nan     0.4223   -0.0000
  2680        0.0000             nan     0.4223   -0.0000
  2700        0.0000             nan     0.4223   -0.0000
  2720        0.0000             nan     0.4223   -0.0000
  2740        0.0000             nan     0.4223   -0.0000
  2760        0.0000             nan     0.4223   -0.0000
  2780        0.0000             nan     0.4223   -0.0000
  2800        0.0000             nan     0.4223   -0.0000
  2820        0.0000             nan     0.4223   -0.0000
  2840        0.0000             nan     0.4223   -0.0000
  2860        0.0000             nan     0.4223   -0.0000
  2880        0.0000             nan     0.4223    0.0000
  2900        0.0000             nan     0.4223   -0.0000
  2920        0.0000             nan     0.4223   -0.0000
  2940        0.0000             nan     0.4223   -0.0000
  2960        0.0000             nan     0.4223   -0.0000
  2980        0.0000             nan     0.4223   -0.0000
  3000        0.0000             nan     0.4223   -0.0000
  3020        0.0000             nan     0.4223    0.0000
  3040        0.0000             nan     0.4223   -0.0000
  3060        0.0000             nan     0.4223   -0.0000
  3080        0.0000             nan     0.4223   -0.0000
  3100        0.0000             nan     0.4223    0.0000
  3120        0.0000             nan     0.4223   -0.0000
  3140        0.0000             nan     0.4223   -0.0000
  3160        0.0000             nan     0.4223   -0.0000
  3180        0.0000             nan     0.4223   -0.0000
  3200        0.0000             nan     0.4223   -0.0000
  3220        0.0000             nan     0.4223   -0.0000
  3240        0.0000             nan     0.4223    0.0000
  3260        0.0000             nan     0.4223   -0.0000
  3280        0.0000             nan     0.4223   -0.0000
  3300        0.0000             nan     0.4223   -0.0000
  3320        0.0000             nan     0.4223    0.0000
  3340        0.0000             nan     0.4223   -0.0000
  3360        0.0000             nan     0.4223   -0.0000
  3380        0.0000             nan     0.4223   -0.0000
  3400        0.0000             nan     0.4223   -0.0000
  3420        0.0000             nan     0.4223   -0.0000
  3440        0.0000             nan     0.4223    0.0000
  3460        0.0000             nan     0.4223   -0.0000
  3480        0.0000             nan     0.4223   -0.0000
  3500        0.0000             nan     0.4223   -0.0000
  3520        0.0000             nan     0.4223    0.0000
  3540        0.0000             nan     0.4223   -0.0000
  3560        0.0000             nan     0.4223   -0.0000
  3580        0.0000             nan     0.4223   -0.0000
  3600        0.0000             nan     0.4223   -0.0000
  3620        0.0000             nan     0.4223   -0.0000
  3640        0.0000             nan     0.4223   -0.0000
  3660        0.0000             nan     0.4223   -0.0000
  3680        0.0000             nan     0.4223   -0.0000
  3700        0.0000             nan     0.4223   -0.0000
  3720        0.0000             nan     0.4223   -0.0000
  3740        0.0000             nan     0.4223   -0.0000
  3760        0.0000             nan     0.4223   -0.0000
  3780        0.0000             nan     0.4223   -0.0000
  3800        0.0000             nan     0.4223   -0.0000
  3820        0.0000             nan     0.4223   -0.0000
  3840        0.0000             nan     0.4223    0.0000
  3860        0.0000             nan     0.4223    0.0000
  3880        0.0000             nan     0.4223   -0.0000
  3900        0.0000             nan     0.4223   -0.0000
  3920        0.0000             nan     0.4223   -0.0000
  3940        0.0000             nan     0.4223    0.0000
  3960        0.0000             nan     0.4223   -0.0000
  3980        0.0000             nan     0.4223   -0.0000
  4000        0.0000             nan     0.4223   -0.0000
  4020        0.0000             nan     0.4223    0.0000
  4040        0.0000             nan     0.4223   -0.0000
  4060        0.0000             nan     0.4223   -0.0000
  4080        0.0000             nan     0.4223   -0.0000
  4100        0.0000             nan     0.4223    0.0000
  4120        0.0000             nan     0.4223    0.0000
  4131        0.0000             nan     0.4223   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1428             nan     0.4469    0.2502
     2        0.0519             nan     0.4469    0.0935
     3        0.0200             nan     0.4469    0.0330
     4        0.0088             nan     0.4469    0.0112
     5        0.0051             nan     0.4469    0.0030
     6        0.0039             nan     0.4469    0.0008
     7        0.0033             nan     0.4469    0.0004
     8        0.0029             nan     0.4469    0.0001
     9        0.0027             nan     0.4469    0.0000
    10        0.0024             nan     0.4469    0.0002
    20        0.0014             nan     0.4469   -0.0000
    40        0.0007             nan     0.4469   -0.0000
    60        0.0004             nan     0.4469   -0.0000
    80        0.0003             nan     0.4469   -0.0000
   100        0.0002             nan     0.4469   -0.0000
   120        0.0001             nan     0.4469   -0.0000
   140        0.0001             nan     0.4469   -0.0000
   160        0.0001             nan     0.4469   -0.0000
   180        0.0001             nan     0.4469   -0.0000
   200        0.0001             nan     0.4469   -0.0000
   220        0.0000             nan     0.4469   -0.0000
   240        0.0000             nan     0.4469   -0.0000
   260        0.0000             nan     0.4469   -0.0000
   280        0.0000             nan     0.4469   -0.0000
   300        0.0000             nan     0.4469   -0.0000
   320        0.0000             nan     0.4469   -0.0000
   340        0.0000             nan     0.4469   -0.0000
   360        0.0000             nan     0.4469   -0.0000
   380        0.0000             nan     0.4469   -0.0000
   400        0.0000             nan     0.4469   -0.0000
   420        0.0000             nan     0.4469    0.0000
   440        0.0000             nan     0.4469   -0.0000
   460        0.0000             nan     0.4469    0.0000
   480        0.0000             nan     0.4469    0.0000
   500        0.0000             nan     0.4469   -0.0000
   520        0.0000             nan     0.4469   -0.0000
   540        0.0000             nan     0.4469   -0.0000
   560        0.0000             nan     0.4469   -0.0000
   580        0.0000             nan     0.4469   -0.0000
   600        0.0000             nan     0.4469   -0.0000
   620        0.0000             nan     0.4469   -0.0000
   640        0.0000             nan     0.4469   -0.0000
   660        0.0000             nan     0.4469   -0.0000
   680        0.0000             nan     0.4469   -0.0000
   700        0.0000             nan     0.4469   -0.0000
   720        0.0000             nan     0.4469   -0.0000
   740        0.0000             nan     0.4469   -0.0000
   760        0.0000             nan     0.4469   -0.0000
   780        0.0000             nan     0.4469   -0.0000
   800        0.0000             nan     0.4469   -0.0000
   820        0.0000             nan     0.4469    0.0000
   840        0.0000             nan     0.4469   -0.0000
   860        0.0000             nan     0.4469   -0.0000
   880        0.0000             nan     0.4469    0.0000
   900        0.0000             nan     0.4469    0.0000
   920        0.0000             nan     0.4469   -0.0000
   940        0.0000             nan     0.4469    0.0000
   960        0.0000             nan     0.4469   -0.0000
   980        0.0000             nan     0.4469    0.0000
  1000        0.0000             nan     0.4469   -0.0000
  1020        0.0000             nan     0.4469   -0.0000
  1040        0.0000             nan     0.4469   -0.0000
  1060        0.0000             nan     0.4469   -0.0000
  1080        0.0000             nan     0.4469   -0.0000
  1100        0.0000             nan     0.4469   -0.0000
  1120        0.0000             nan     0.4469    0.0000
  1140        0.0000             nan     0.4469    0.0000
  1160        0.0000             nan     0.4469   -0.0000
  1180        0.0000             nan     0.4469   -0.0000
  1200        0.0000             nan     0.4469   -0.0000
  1220        0.0000             nan     0.4469   -0.0000
  1240        0.0000             nan     0.4469   -0.0000
  1260        0.0000             nan     0.4469   -0.0000
  1280        0.0000             nan     0.4469    0.0000
  1300        0.0000             nan     0.4469   -0.0000
  1320        0.0000             nan     0.4469    0.0000
  1340        0.0000             nan     0.4469   -0.0000
  1360        0.0000             nan     0.4469    0.0000
  1380        0.0000             nan     0.4469   -0.0000
  1400        0.0000             nan     0.4469   -0.0000
  1420        0.0000             nan     0.4469   -0.0000
  1440        0.0000             nan     0.4469    0.0000
  1460        0.0000             nan     0.4469   -0.0000
  1480        0.0000             nan     0.4469   -0.0000
  1500        0.0000             nan     0.4469    0.0000
  1520        0.0000             nan     0.4469   -0.0000
  1540        0.0000             nan     0.4469   -0.0000
  1560        0.0000             nan     0.4469   -0.0000
  1580        0.0000             nan     0.4469   -0.0000
  1600        0.0000             nan     0.4469    0.0000
  1620        0.0000             nan     0.4469   -0.0000
  1640        0.0000             nan     0.4469   -0.0000
  1660        0.0000             nan     0.4469   -0.0000
  1680        0.0000             nan     0.4469   -0.0000
  1700        0.0000             nan     0.4469   -0.0000
  1713        0.0000             nan     0.4469   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1609             nan     0.4473    0.2510
     2        0.0636             nan     0.4473    0.1034
     3        0.0279             nan     0.4473    0.0322
     4        0.0142             nan     0.4473    0.0134
     5        0.0090             nan     0.4473    0.0051
     6        0.0067             nan     0.4473    0.0014
     7        0.0054             nan     0.4473    0.0011
     8        0.0048             nan     0.4473    0.0003
     9        0.0041             nan     0.4473    0.0007
    10        0.0038             nan     0.4473    0.0001
    20        0.0021             nan     0.4473   -0.0002
    40        0.0010             nan     0.4473   -0.0000
    60        0.0007             nan     0.4473   -0.0000
    80        0.0004             nan     0.4473   -0.0000
   100        0.0003             nan     0.4473   -0.0000
   120        0.0002             nan     0.4473   -0.0000
   140        0.0002             nan     0.4473   -0.0000
   160        0.0002             nan     0.4473   -0.0000
   180        0.0001             nan     0.4473   -0.0000
   200        0.0001             nan     0.4473   -0.0000
   220        0.0001             nan     0.4473   -0.0000
   240        0.0001             nan     0.4473   -0.0000
   260        0.0001             nan     0.4473   -0.0000
   280        0.0000             nan     0.4473   -0.0000
   300        0.0000             nan     0.4473   -0.0000
   320        0.0000             nan     0.4473   -0.0000
   340        0.0000             nan     0.4473   -0.0000
   360        0.0000             nan     0.4473   -0.0000
   380        0.0000             nan     0.4473   -0.0000
   400        0.0000             nan     0.4473   -0.0000
   420        0.0000             nan     0.4473   -0.0000
   440        0.0000             nan     0.4473   -0.0000
   460        0.0000             nan     0.4473   -0.0000
   480        0.0000             nan     0.4473   -0.0000
   500        0.0000             nan     0.4473   -0.0000
   520        0.0000             nan     0.4473   -0.0000
   540        0.0000             nan     0.4473   -0.0000
   560        0.0000             nan     0.4473   -0.0000
   580        0.0000             nan     0.4473   -0.0000
   600        0.0000             nan     0.4473   -0.0000
   620        0.0000             nan     0.4473   -0.0000
   640        0.0000             nan     0.4473   -0.0000
   660        0.0000             nan     0.4473   -0.0000
   680        0.0000             nan     0.4473   -0.0000
   700        0.0000             nan     0.4473   -0.0000
   720        0.0000             nan     0.4473   -0.0000
   740        0.0000             nan     0.4473   -0.0000
   760        0.0000             nan     0.4473   -0.0000
   780        0.0000             nan     0.4473   -0.0000
   800        0.0000             nan     0.4473   -0.0000
   820        0.0000             nan     0.4473   -0.0000
   840        0.0000             nan     0.4473   -0.0000
   860        0.0000             nan     0.4473   -0.0000
   880        0.0000             nan     0.4473   -0.0000
   900        0.0000             nan     0.4473   -0.0000
   920        0.0000             nan     0.4473   -0.0000
   940        0.0000             nan     0.4473   -0.0000
   960        0.0000             nan     0.4473   -0.0000
   980        0.0000             nan     0.4473   -0.0000
  1000        0.0000             nan     0.4473   -0.0000
  1020        0.0000             nan     0.4473   -0.0000
  1040        0.0000             nan     0.4473   -0.0000
  1060        0.0000             nan     0.4473   -0.0000
  1080        0.0000             nan     0.4473   -0.0000
  1100        0.0000             nan     0.4473   -0.0000
  1120        0.0000             nan     0.4473   -0.0000
  1140        0.0000             nan     0.4473   -0.0000
  1160        0.0000             nan     0.4473   -0.0000
  1180        0.0000             nan     0.4473   -0.0000
  1200        0.0000             nan     0.4473   -0.0000
  1220        0.0000             nan     0.4473   -0.0000
  1240        0.0000             nan     0.4473   -0.0000
  1260        0.0000             nan     0.4473   -0.0000
  1280        0.0000             nan     0.4473   -0.0000
  1300        0.0000             nan     0.4473   -0.0000
  1320        0.0000             nan     0.4473   -0.0000
  1340        0.0000             nan     0.4473   -0.0000
  1360        0.0000             nan     0.4473   -0.0000
  1380        0.0000             nan     0.4473   -0.0000
  1400        0.0000             nan     0.4473   -0.0000
  1420        0.0000             nan     0.4473   -0.0000
  1440        0.0000             nan     0.4473   -0.0000
  1460        0.0000             nan     0.4473   -0.0000
  1480        0.0000             nan     0.4473   -0.0000
  1500        0.0000             nan     0.4473   -0.0000
  1520        0.0000             nan     0.4473   -0.0000
  1540        0.0000             nan     0.4473   -0.0000
  1560        0.0000             nan     0.4473   -0.0000
  1580        0.0000             nan     0.4473   -0.0000
  1600        0.0000             nan     0.4473   -0.0000
  1620        0.0000             nan     0.4473   -0.0000
  1640        0.0000             nan     0.4473   -0.0000
  1660        0.0000             nan     0.4473   -0.0000
  1680        0.0000             nan     0.4473   -0.0000
  1700        0.0000             nan     0.4473   -0.0000
  1720        0.0000             nan     0.4473    0.0000
  1740        0.0000             nan     0.4473   -0.0000
  1760        0.0000             nan     0.4473   -0.0000
  1780        0.0000             nan     0.4473   -0.0000
  1800        0.0000             nan     0.4473   -0.0000
  1820        0.0000             nan     0.4473   -0.0000
  1840        0.0000             nan     0.4473   -0.0000
  1860        0.0000             nan     0.4473   -0.0000
  1880        0.0000             nan     0.4473   -0.0000
  1900        0.0000             nan     0.4473   -0.0000
  1920        0.0000             nan     0.4473   -0.0000
  1940        0.0000             nan     0.4473   -0.0000
  1960        0.0000             nan     0.4473   -0.0000
  1980        0.0000             nan     0.4473   -0.0000
  2000        0.0000             nan     0.4473   -0.0000
  2020        0.0000             nan     0.4473   -0.0000
  2040        0.0000             nan     0.4473   -0.0000
  2060        0.0000             nan     0.4473   -0.0000
  2080        0.0000             nan     0.4473   -0.0000
  2100        0.0000             nan     0.4473   -0.0000
  2120        0.0000             nan     0.4473   -0.0000
  2140        0.0000             nan     0.4473   -0.0000
  2160        0.0000             nan     0.4473   -0.0000
  2180        0.0000             nan     0.4473   -0.0000
  2200        0.0000             nan     0.4473   -0.0000
  2220        0.0000             nan     0.4473   -0.0000
  2240        0.0000             nan     0.4473   -0.0000
  2260        0.0000             nan     0.4473   -0.0000
  2280        0.0000             nan     0.4473   -0.0000
  2300        0.0000             nan     0.4473   -0.0000
  2320        0.0000             nan     0.4473   -0.0000
  2340        0.0000             nan     0.4473   -0.0000
  2360        0.0000             nan     0.4473   -0.0000
  2380        0.0000             nan     0.4473   -0.0000
  2400        0.0000             nan     0.4473   -0.0000
  2420        0.0000             nan     0.4473   -0.0000
  2440        0.0000             nan     0.4473   -0.0000
  2460        0.0000             nan     0.4473   -0.0000
  2480        0.0000             nan     0.4473   -0.0000
  2500        0.0000             nan     0.4473   -0.0000
  2520        0.0000             nan     0.4473   -0.0000
  2540        0.0000             nan     0.4473   -0.0000
  2560        0.0000             nan     0.4473   -0.0000
  2580        0.0000             nan     0.4473   -0.0000
  2600        0.0000             nan     0.4473   -0.0000
  2620        0.0000             nan     0.4473   -0.0000
  2640        0.0000             nan     0.4473   -0.0000
  2660        0.0000             nan     0.4473   -0.0000
  2680        0.0000             nan     0.4473   -0.0000
  2700        0.0000             nan     0.4473   -0.0000
  2720        0.0000             nan     0.4473   -0.0000
  2740        0.0000             nan     0.4473   -0.0000
  2760        0.0000             nan     0.4473   -0.0000
  2780        0.0000             nan     0.4473   -0.0000
  2800        0.0000             nan     0.4473   -0.0000
  2820        0.0000             nan     0.4473   -0.0000
  2840        0.0000             nan     0.4473   -0.0000
  2860        0.0000             nan     0.4473   -0.0000
  2880        0.0000             nan     0.4473   -0.0000
  2900        0.0000             nan     0.4473   -0.0000
  2920        0.0000             nan     0.4473   -0.0000
  2940        0.0000             nan     0.4473   -0.0000
  2960        0.0000             nan     0.4473   -0.0000
  2980        0.0000             nan     0.4473   -0.0000
  3000        0.0000             nan     0.4473   -0.0000
  3020        0.0000             nan     0.4473   -0.0000
  3040        0.0000             nan     0.4473   -0.0000
  3060        0.0000             nan     0.4473   -0.0000
  3080        0.0000             nan     0.4473   -0.0000
  3100        0.0000             nan     0.4473   -0.0000
  3120        0.0000             nan     0.4473   -0.0000
  3140        0.0000             nan     0.4473   -0.0000
  3160        0.0000             nan     0.4473   -0.0000
  3180        0.0000             nan     0.4473   -0.0000
  3200        0.0000             nan     0.4473   -0.0000
  3220        0.0000             nan     0.4473   -0.0000
  3240        0.0000             nan     0.4473   -0.0000
  3260        0.0000             nan     0.4473   -0.0000
  3280        0.0000             nan     0.4473   -0.0000
  3300        0.0000             nan     0.4473   -0.0000
  3320        0.0000             nan     0.4473   -0.0000
  3340        0.0000             nan     0.4473   -0.0000
  3360        0.0000             nan     0.4473   -0.0000
  3380        0.0000             nan     0.4473   -0.0000
  3400        0.0000             nan     0.4473   -0.0000
  3420        0.0000             nan     0.4473   -0.0000
  3440        0.0000             nan     0.4473   -0.0000
  3460        0.0000             nan     0.4473   -0.0000
  3480        0.0000             nan     0.4473   -0.0000
  3500        0.0000             nan     0.4473   -0.0000
  3520        0.0000             nan     0.4473   -0.0000
  3540        0.0000             nan     0.4473   -0.0000
  3560        0.0000             nan     0.4473   -0.0000
  3580        0.0000             nan     0.4473   -0.0000
  3600        0.0000             nan     0.4473   -0.0000
  3620        0.0000             nan     0.4473   -0.0000
  3640        0.0000             nan     0.4473   -0.0000
  3660        0.0000             nan     0.4473   -0.0000
  3680        0.0000             nan     0.4473   -0.0000
  3700        0.0000             nan     0.4473   -0.0000
  3720        0.0000             nan     0.4473   -0.0000
  3740        0.0000             nan     0.4473   -0.0000
  3760        0.0000             nan     0.4473   -0.0000
  3780        0.0000             nan     0.4473   -0.0000
  3800        0.0000             nan     0.4473   -0.0000
  3820        0.0000             nan     0.4473   -0.0000
  3840        0.0000             nan     0.4473   -0.0000
  3855        0.0000             nan     0.4473   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1347             nan     0.4875    0.2796
     2        0.0484             nan     0.4875    0.0871
     3        0.0201             nan     0.4875    0.0294
     4        0.0106             nan     0.4875    0.0107
     5        0.0070             nan     0.4875    0.0031
     6        0.0056             nan     0.4875    0.0012
     7        0.0048             nan     0.4875    0.0007
     8        0.0043             nan     0.4875   -0.0000
     9        0.0041             nan     0.4875   -0.0000
    10        0.0037             nan     0.4875    0.0002
    20        0.0020             nan     0.4875   -0.0001
    40        0.0011             nan     0.4875   -0.0000
    60        0.0007             nan     0.4875   -0.0000
    80        0.0005             nan     0.4875   -0.0000
   100        0.0004             nan     0.4875   -0.0000
   120        0.0003             nan     0.4875   -0.0000
   140        0.0002             nan     0.4875   -0.0000
   160        0.0002             nan     0.4875   -0.0000
   180        0.0002             nan     0.4875   -0.0000
   200        0.0001             nan     0.4875   -0.0000
   220        0.0001             nan     0.4875   -0.0000
   240        0.0001             nan     0.4875   -0.0000
   260        0.0001             nan     0.4875   -0.0000
   280        0.0001             nan     0.4875   -0.0000
   300        0.0000             nan     0.4875   -0.0000
   320        0.0000             nan     0.4875   -0.0000
   340        0.0000             nan     0.4875   -0.0000
   360        0.0000             nan     0.4875   -0.0000
   380        0.0000             nan     0.4875   -0.0000
   400        0.0000             nan     0.4875   -0.0000
   420        0.0000             nan     0.4875   -0.0000
   440        0.0000             nan     0.4875   -0.0000
   460        0.0000             nan     0.4875   -0.0000
   480        0.0000             nan     0.4875   -0.0000
   500        0.0000             nan     0.4875   -0.0000
   520        0.0000             nan     0.4875   -0.0000
   540        0.0000             nan     0.4875   -0.0000
   560        0.0000             nan     0.4875   -0.0000
   580        0.0000             nan     0.4875   -0.0000
   600        0.0000             nan     0.4875   -0.0000
   620        0.0000             nan     0.4875   -0.0000
   640        0.0000             nan     0.4875   -0.0000
   660        0.0000             nan     0.4875   -0.0000
   680        0.0000             nan     0.4875   -0.0000
   700        0.0000             nan     0.4875   -0.0000
   720        0.0000             nan     0.4875   -0.0000
   740        0.0000             nan     0.4875   -0.0000
   760        0.0000             nan     0.4875   -0.0000
   780        0.0000             nan     0.4875   -0.0000
   800        0.0000             nan     0.4875   -0.0000
   820        0.0000             nan     0.4875   -0.0000
   840        0.0000             nan     0.4875   -0.0000
   860        0.0000             nan     0.4875    0.0000
   880        0.0000             nan     0.4875    0.0000
   900        0.0000             nan     0.4875   -0.0000
   920        0.0000             nan     0.4875   -0.0000
   940        0.0000             nan     0.4875   -0.0000
   960        0.0000             nan     0.4875   -0.0000
   980        0.0000             nan     0.4875   -0.0000
  1000        0.0000             nan     0.4875    0.0000
  1020        0.0000             nan     0.4875   -0.0000
  1040        0.0000             nan     0.4875    0.0000
  1060        0.0000             nan     0.4875    0.0000
  1080        0.0000             nan     0.4875   -0.0000
  1100        0.0000             nan     0.4875   -0.0000
  1120        0.0000             nan     0.4875   -0.0000
  1140        0.0000             nan     0.4875   -0.0000
  1160        0.0000             nan     0.4875   -0.0000
  1180        0.0000             nan     0.4875   -0.0000
  1200        0.0000             nan     0.4875   -0.0000
  1220        0.0000             nan     0.4875   -0.0000
  1240        0.0000             nan     0.4875    0.0000
  1260        0.0000             nan     0.4875    0.0000
  1280        0.0000             nan     0.4875   -0.0000
  1300        0.0000             nan     0.4875    0.0000
  1320        0.0000             nan     0.4875    0.0000
  1340        0.0000             nan     0.4875   -0.0000
  1360        0.0000             nan     0.4875   -0.0000
  1380        0.0000             nan     0.4875   -0.0000
  1400        0.0000             nan     0.4875   -0.0000
  1420        0.0000             nan     0.4875   -0.0000
  1440        0.0000             nan     0.4875   -0.0000
  1460        0.0000             nan     0.4875   -0.0000
  1480        0.0000             nan     0.4875   -0.0000
  1500        0.0000             nan     0.4875   -0.0000
  1520        0.0000             nan     0.4875   -0.0000
  1540        0.0000             nan     0.4875   -0.0000
  1560        0.0000             nan     0.4875   -0.0000
  1580        0.0000             nan     0.4875   -0.0000
  1600        0.0000             nan     0.4875   -0.0000
  1620        0.0000             nan     0.4875   -0.0000
  1640        0.0000             nan     0.4875   -0.0000
  1660        0.0000             nan     0.4875   -0.0000
  1680        0.0000             nan     0.4875   -0.0000
  1700        0.0000             nan     0.4875   -0.0000
  1720        0.0000             nan     0.4875   -0.0000
  1740        0.0000             nan     0.4875    0.0000
  1760        0.0000             nan     0.4875   -0.0000
  1780        0.0000             nan     0.4875   -0.0000
  1800        0.0000             nan     0.4875   -0.0000
  1820        0.0000             nan     0.4875    0.0000
  1840        0.0000             nan     0.4875   -0.0000
  1860        0.0000             nan     0.4875   -0.0000
  1880        0.0000             nan     0.4875    0.0000
  1900        0.0000             nan     0.4875   -0.0000
  1920        0.0000             nan     0.4875   -0.0000
  1940        0.0000             nan     0.4875   -0.0000
  1960        0.0000             nan     0.4875    0.0000
  1980        0.0000             nan     0.4875    0.0000
  2000        0.0000             nan     0.4875   -0.0000
  2020        0.0000             nan     0.4875   -0.0000
  2040        0.0000             nan     0.4875   -0.0000
  2060        0.0000             nan     0.4875   -0.0000
  2080        0.0000             nan     0.4875   -0.0000
  2100        0.0000             nan     0.4875   -0.0000
  2120        0.0000             nan     0.4875   -0.0000
  2140        0.0000             nan     0.4875   -0.0000
  2160        0.0000             nan     0.4875   -0.0000
  2180        0.0000             nan     0.4875   -0.0000
  2200        0.0000             nan     0.4875   -0.0000
  2220        0.0000             nan     0.4875   -0.0000
  2240        0.0000             nan     0.4875   -0.0000
  2260        0.0000             nan     0.4875    0.0000
  2280        0.0000             nan     0.4875   -0.0000
  2300        0.0000             nan     0.4875   -0.0000
  2320        0.0000             nan     0.4875   -0.0000
  2340        0.0000             nan     0.4875   -0.0000
  2360        0.0000             nan     0.4875   -0.0000
  2380        0.0000             nan     0.4875   -0.0000
  2400        0.0000             nan     0.4875    0.0000
  2420        0.0000             nan     0.4875   -0.0000
  2440        0.0000             nan     0.4875   -0.0000
  2460        0.0000             nan     0.4875   -0.0000
  2480        0.0000             nan     0.4875   -0.0000
  2500        0.0000             nan     0.4875   -0.0000
  2520        0.0000             nan     0.4875   -0.0000
  2540        0.0000             nan     0.4875    0.0000
  2560        0.0000             nan     0.4875   -0.0000
  2580        0.0000             nan     0.4875    0.0000
  2600        0.0000             nan     0.4875   -0.0000
  2620        0.0000             nan     0.4875   -0.0000
  2640        0.0000             nan     0.4875    0.0000
  2660        0.0000             nan     0.4875    0.0000
  2680        0.0000             nan     0.4875   -0.0000
  2700        0.0000             nan     0.4875    0.0000
  2720        0.0000             nan     0.4875    0.0000
  2740        0.0000             nan     0.4875    0.0000
  2760        0.0000             nan     0.4875   -0.0000
  2780        0.0000             nan     0.4875   -0.0000
  2800        0.0000             nan     0.4875   -0.0000
  2820        0.0000             nan     0.4875   -0.0000
  2840        0.0000             nan     0.4875   -0.0000
  2860        0.0000             nan     0.4875    0.0000
  2880        0.0000             nan     0.4875    0.0000
  2900        0.0000             nan     0.4875   -0.0000
  2920        0.0000             nan     0.4875    0.0000
  2940        0.0000             nan     0.4875    0.0000
  2960        0.0000             nan     0.4875    0.0000
  2980        0.0000             nan     0.4875   -0.0000
  3000        0.0000             nan     0.4875   -0.0000
  3020        0.0000             nan     0.4875    0.0000
  3040        0.0000             nan     0.4875   -0.0000
  3060        0.0000             nan     0.4875   -0.0000
  3080        0.0000             nan     0.4875   -0.0000
  3100        0.0000             nan     0.4875   -0.0000
  3120        0.0000             nan     0.4875   -0.0000
  3140        0.0000             nan     0.4875   -0.0000
  3160        0.0000             nan     0.4875   -0.0000
  3180        0.0000             nan     0.4875   -0.0000
  3200        0.0000             nan     0.4875    0.0000
  3220        0.0000             nan     0.4875   -0.0000
  3240        0.0000             nan     0.4875   -0.0000
  3260        0.0000             nan     0.4875   -0.0000
  3280        0.0000             nan     0.4875   -0.0000
  3300        0.0000             nan     0.4875   -0.0000
  3320        0.0000             nan     0.4875   -0.0000
  3340        0.0000             nan     0.4875    0.0000
  3360        0.0000             nan     0.4875   -0.0000
  3380        0.0000             nan     0.4875    0.0000
  3400        0.0000             nan     0.4875   -0.0000
  3420        0.0000             nan     0.4875   -0.0000
  3440        0.0000             nan     0.4875    0.0000
  3460        0.0000             nan     0.4875   -0.0000
  3480        0.0000             nan     0.4875    0.0000
  3500        0.0000             nan     0.4875   -0.0000
  3520        0.0000             nan     0.4875   -0.0000
  3540        0.0000             nan     0.4875   -0.0000
  3560        0.0000             nan     0.4875   -0.0000
  3580        0.0000             nan     0.4875    0.0000
  3600        0.0000             nan     0.4875   -0.0000
  3620        0.0000             nan     0.4875   -0.0000
  3640        0.0000             nan     0.4875   -0.0000
  3660        0.0000             nan     0.4875    0.0000
  3680        0.0000             nan     0.4875   -0.0000
  3700        0.0000             nan     0.4875   -0.0000
  3720        0.0000             nan     0.4875    0.0000
  3740        0.0000             nan     0.4875   -0.0000
  3760        0.0000             nan     0.4875   -0.0000
  3780        0.0000             nan     0.4875   -0.0000
  3800        0.0000             nan     0.4875   -0.0000
  3820        0.0000             nan     0.4875   -0.0000
  3840        0.0000             nan     0.4875    0.0000
  3860        0.0000             nan     0.4875    0.0000
  3880        0.0000             nan     0.4875   -0.0000
  3900        0.0000             nan     0.4875   -0.0000
  3920        0.0000             nan     0.4875   -0.0000
  3940        0.0000             nan     0.4875    0.0000
  3960        0.0000             nan     0.4875   -0.0000
  3980        0.0000             nan     0.4875    0.0000
  4000        0.0000             nan     0.4875    0.0000
  4020        0.0000             nan     0.4875   -0.0000
  4040        0.0000             nan     0.4875   -0.0000
  4060        0.0000             nan     0.4875   -0.0000
  4080        0.0000             nan     0.4875   -0.0000
  4100        0.0000             nan     0.4875   -0.0000
  4120        0.0000             nan     0.4875   -0.0000
  4140        0.0000             nan     0.4875   -0.0000
  4160        0.0000             nan     0.4875   -0.0000
  4180        0.0000             nan     0.4875    0.0000
  4200        0.0000             nan     0.4875   -0.0000
  4220        0.0000             nan     0.4875   -0.0000
  4240        0.0000             nan     0.4875   -0.0000
  4260        0.0000             nan     0.4875   -0.0000
  4280        0.0000             nan     0.4875   -0.0000
  4300        0.0000             nan     0.4875   -0.0000
  4320        0.0000             nan     0.4875    0.0000
  4340        0.0000             nan     0.4875   -0.0000
  4360        0.0000             nan     0.4875    0.0000
  4380        0.0000             nan     0.4875   -0.0000
  4400        0.0000             nan     0.4875   -0.0000
  4416        0.0000             nan     0.4875   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1059             nan     0.5240    0.2749
     2        0.0292             nan     0.5240    0.0782
     3        0.0108             nan     0.5240    0.0171
     4        0.0054             nan     0.5240    0.0045
     5        0.0038             nan     0.5240    0.0010
     6        0.0032             nan     0.5240   -0.0000
     7        0.0030             nan     0.5240   -0.0002
     8        0.0028             nan     0.5240   -0.0001
     9        0.0026             nan     0.5240   -0.0001
    10        0.0025             nan     0.5240   -0.0002
    20        0.0014             nan     0.5240   -0.0001
    40        0.0007             nan     0.5240   -0.0001
    60        0.0004             nan     0.5240   -0.0001
    80        0.0003             nan     0.5240   -0.0000
   100        0.0002             nan     0.5240   -0.0000
   120        0.0001             nan     0.5240   -0.0000
   140        0.0001             nan     0.5240   -0.0000
   160        0.0001             nan     0.5240   -0.0000
   180        0.0001             nan     0.5240   -0.0000
   200        0.0000             nan     0.5240   -0.0000
   220        0.0000             nan     0.5240   -0.0000
   240        0.0000             nan     0.5240   -0.0000
   260        0.0000             nan     0.5240   -0.0000
   280        0.0000             nan     0.5240    0.0000
   300        0.0000             nan     0.5240   -0.0000
   320        0.0000             nan     0.5240    0.0000
   340        0.0000             nan     0.5240   -0.0000
   360        0.0000             nan     0.5240   -0.0000
   380        0.0000             nan     0.5240   -0.0000
   400        0.0000             nan     0.5240   -0.0000
   420        0.0000             nan     0.5240   -0.0000
   440        0.0000             nan     0.5240   -0.0000
   460        0.0000             nan     0.5240   -0.0000
   480        0.0000             nan     0.5240   -0.0000
   500        0.0000             nan     0.5240   -0.0000
   520        0.0000             nan     0.5240    0.0000
   540        0.0000             nan     0.5240   -0.0000
   560        0.0000             nan     0.5240    0.0000
   580        0.0000             nan     0.5240    0.0000
   600        0.0000             nan     0.5240   -0.0000
   620        0.0000             nan     0.5240   -0.0000
   640        0.0000             nan     0.5240    0.0000
   660        0.0000             nan     0.5240   -0.0000
   680        0.0000             nan     0.5240   -0.0000
   700        0.0000             nan     0.5240   -0.0000
   720        0.0000             nan     0.5240   -0.0000
   740        0.0000             nan     0.5240   -0.0000
   760        0.0000             nan     0.5240   -0.0000
   780        0.0000             nan     0.5240   -0.0000
   800        0.0000             nan     0.5240   -0.0000
   820        0.0000             nan     0.5240   -0.0000
   840        0.0000             nan     0.5240   -0.0000
   860        0.0000             nan     0.5240   -0.0000
   880        0.0000             nan     0.5240    0.0000
   900        0.0000             nan     0.5240    0.0000
   920        0.0000             nan     0.5240    0.0000
   940        0.0000             nan     0.5240   -0.0000
   960        0.0000             nan     0.5240   -0.0000
   980        0.0000             nan     0.5240   -0.0000
  1000        0.0000             nan     0.5240   -0.0000
  1020        0.0000             nan     0.5240    0.0000
  1040        0.0000             nan     0.5240   -0.0000
  1060        0.0000             nan     0.5240   -0.0000
  1080        0.0000             nan     0.5240   -0.0000
  1100        0.0000             nan     0.5240   -0.0000
  1120        0.0000             nan     0.5240   -0.0000
  1140        0.0000             nan     0.5240   -0.0000
  1160        0.0000             nan     0.5240    0.0000
  1180        0.0000             nan     0.5240   -0.0000
  1200        0.0000             nan     0.5240   -0.0000
  1220        0.0000             nan     0.5240   -0.0000
  1240        0.0000             nan     0.5240   -0.0000
  1260        0.0000             nan     0.5240   -0.0000
  1280        0.0000             nan     0.5240   -0.0000
  1300        0.0000             nan     0.5240    0.0000
  1320        0.0000             nan     0.5240   -0.0000
  1340        0.0000             nan     0.5240   -0.0000
  1360        0.0000             nan     0.5240   -0.0000
  1380        0.0000             nan     0.5240   -0.0000
  1400        0.0000             nan     0.5240    0.0000
  1420        0.0000             nan     0.5240   -0.0000
  1440        0.0000             nan     0.5240    0.0000
  1460        0.0000             nan     0.5240   -0.0000
  1480        0.0000             nan     0.5240   -0.0000
  1500        0.0000             nan     0.5240   -0.0000
  1520        0.0000             nan     0.5240   -0.0000
  1540        0.0000             nan     0.5240   -0.0000
  1560        0.0000             nan     0.5240   -0.0000
  1580        0.0000             nan     0.5240   -0.0000
  1600        0.0000             nan     0.5240   -0.0000
  1620        0.0000             nan     0.5240   -0.0000
  1640        0.0000             nan     0.5240   -0.0000
  1660        0.0000             nan     0.5240    0.0000
  1680        0.0000             nan     0.5240   -0.0000
  1700        0.0000             nan     0.5240   -0.0000
  1720        0.0000             nan     0.5240   -0.0000
  1740        0.0000             nan     0.5240   -0.0000
  1760        0.0000             nan     0.5240    0.0000
  1780        0.0000             nan     0.5240    0.0000
  1800        0.0000             nan     0.5240   -0.0000
  1820        0.0000             nan     0.5240   -0.0000
  1840        0.0000             nan     0.5240   -0.0000
  1860        0.0000             nan     0.5240   -0.0000
  1880        0.0000             nan     0.5240   -0.0000
  1900        0.0000             nan     0.5240   -0.0000
  1920        0.0000             nan     0.5240   -0.0000
  1940        0.0000             nan     0.5240   -0.0000
  1960        0.0000             nan     0.5240    0.0000
  1980        0.0000             nan     0.5240   -0.0000
  2000        0.0000             nan     0.5240   -0.0000
  2020        0.0000             nan     0.5240   -0.0000
  2040        0.0000             nan     0.5240   -0.0000
  2060        0.0000             nan     0.5240   -0.0000
  2080        0.0000             nan     0.5240    0.0000
  2100        0.0000             nan     0.5240    0.0000
  2120        0.0000             nan     0.5240   -0.0000
  2140        0.0000             nan     0.5240   -0.0000
  2160        0.0000             nan     0.5240   -0.0000
  2180        0.0000             nan     0.5240    0.0000
  2200        0.0000             nan     0.5240   -0.0000
  2220        0.0000             nan     0.5240    0.0000
  2240        0.0000             nan     0.5240   -0.0000
  2260        0.0000             nan     0.5240   -0.0000
  2280        0.0000             nan     0.5240   -0.0000
  2300        0.0000             nan     0.5240    0.0000
  2320        0.0000             nan     0.5240   -0.0000
  2340        0.0000             nan     0.5240    0.0000
  2360        0.0000             nan     0.5240   -0.0000
  2380        0.0000             nan     0.5240   -0.0000
  2400        0.0000             nan     0.5240   -0.0000
  2420        0.0000             nan     0.5240   -0.0000
  2440        0.0000             nan     0.5240    0.0000
  2460        0.0000             nan     0.5240    0.0000
  2480        0.0000             nan     0.5240   -0.0000
  2500        0.0000             nan     0.5240   -0.0000
  2520        0.0000             nan     0.5240   -0.0000
  2540        0.0000             nan     0.5240    0.0000
  2560        0.0000             nan     0.5240    0.0000
  2580        0.0000             nan     0.5240    0.0000
  2600        0.0000             nan     0.5240   -0.0000
  2620        0.0000             nan     0.5240   -0.0000
  2640        0.0000             nan     0.5240   -0.0000
  2660        0.0000             nan     0.5240   -0.0000
  2680        0.0000             nan     0.5240   -0.0000
  2700        0.0000             nan     0.5240   -0.0000
  2720        0.0000             nan     0.5240   -0.0000
  2740        0.0000             nan     0.5240    0.0000
  2760        0.0000             nan     0.5240    0.0000
  2780        0.0000             nan     0.5240   -0.0000
  2800        0.0000             nan     0.5240   -0.0000
  2820        0.0000             nan     0.5240   -0.0000
  2840        0.0000             nan     0.5240   -0.0000
  2860        0.0000             nan     0.5240    0.0000
  2880        0.0000             nan     0.5240   -0.0000
  2900        0.0000             nan     0.5240   -0.0000
  2920        0.0000             nan     0.5240   -0.0000
  2940        0.0000             nan     0.5240   -0.0000
  2960        0.0000             nan     0.5240   -0.0000
  2980        0.0000             nan     0.5240   -0.0000
  3000        0.0000             nan     0.5240   -0.0000
  3020        0.0000             nan     0.5240   -0.0000
  3040        0.0000             nan     0.5240   -0.0000
  3060        0.0000             nan     0.5240   -0.0000
  3080        0.0000             nan     0.5240   -0.0000
  3100        0.0000             nan     0.5240    0.0000
  3120        0.0000             nan     0.5240   -0.0000
  3140        0.0000             nan     0.5240   -0.0000
  3160        0.0000             nan     0.5240   -0.0000
  3180        0.0000             nan     0.5240   -0.0000
  3200        0.0000             nan     0.5240   -0.0000
  3220        0.0000             nan     0.5240   -0.0000
  3240        0.0000             nan     0.5240   -0.0000
  3260        0.0000             nan     0.5240   -0.0000
  3280        0.0000             nan     0.5240   -0.0000
  3300        0.0000             nan     0.5240   -0.0000
  3320        0.0000             nan     0.5240    0.0000
  3340        0.0000             nan     0.5240   -0.0000
  3360        0.0000             nan     0.5240    0.0000
  3380        0.0000             nan     0.5240   -0.0000
  3400        0.0000             nan     0.5240    0.0000
  3420        0.0000             nan     0.5240   -0.0000
  3440        0.0000             nan     0.5240   -0.0000
  3460        0.0000             nan     0.5240    0.0000
  3480        0.0000             nan     0.5240   -0.0000
  3500        0.0000             nan     0.5240   -0.0000
  3520        0.0000             nan     0.5240   -0.0000
  3540        0.0000             nan     0.5240   -0.0000
  3560        0.0000             nan     0.5240   -0.0000
  3580        0.0000             nan     0.5240   -0.0000
  3600        0.0000             nan     0.5240   -0.0000
  3620        0.0000             nan     0.5240   -0.0000
  3640        0.0000             nan     0.5240   -0.0000
  3660        0.0000             nan     0.5240   -0.0000
  3680        0.0000             nan     0.5240   -0.0000
  3700        0.0000             nan     0.5240   -0.0000
  3720        0.0000             nan     0.5240    0.0000
  3740        0.0000             nan     0.5240   -0.0000
  3760        0.0000             nan     0.5240   -0.0000
  3780        0.0000             nan     0.5240    0.0000
  3800        0.0000             nan     0.5240   -0.0000
  3820        0.0000             nan     0.5240   -0.0000
  3840        0.0000             nan     0.5240   -0.0000
  3860        0.0000             nan     0.5240   -0.0000
  3880        0.0000             nan     0.5240    0.0000
  3900        0.0000             nan     0.5240   -0.0000
  3920        0.0000             nan     0.5240   -0.0000
  3940        0.0000             nan     0.5240   -0.0000
  3960        0.0000             nan     0.5240   -0.0000
  3980        0.0000             nan     0.5240   -0.0000
  4000        0.0000             nan     0.5240   -0.0000
  4020        0.0000             nan     0.5240   -0.0000
  4040        0.0000             nan     0.5240   -0.0000
  4060        0.0000             nan     0.5240    0.0000
  4080        0.0000             nan     0.5240   -0.0000
  4100        0.0000             nan     0.5240   -0.0000
  4120        0.0000             nan     0.5240   -0.0000
  4140        0.0000             nan     0.5240   -0.0000
  4160        0.0000             nan     0.5240   -0.0000
  4180        0.0000             nan     0.5240    0.0000
  4200        0.0000             nan     0.5240   -0.0000
  4216        0.0000             nan     0.5240   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1217             nan     0.5299    0.3123
     2        0.0455             nan     0.5299    0.0837
     3        0.0192             nan     0.5299    0.0231
     4        0.0128             nan     0.5299    0.0058
     5        0.0109             nan     0.5299    0.0014
     6        0.0092             nan     0.5299    0.0015
     7        0.0085             nan     0.5299    0.0000
     8        0.0079             nan     0.5299    0.0003
     9        0.0071             nan     0.5299    0.0007
    10        0.0066             nan     0.5299    0.0000
    20        0.0046             nan     0.5299    0.0001
    40        0.0029             nan     0.5299   -0.0001
    60        0.0021             nan     0.5299   -0.0002
    80        0.0016             nan     0.5299   -0.0000
   100        0.0013             nan     0.5299   -0.0000
   120        0.0011             nan     0.5299   -0.0001
   140        0.0009             nan     0.5299   -0.0001
   160        0.0008             nan     0.5299   -0.0000
   180        0.0007             nan     0.5299   -0.0001
   200        0.0006             nan     0.5299   -0.0000
   220        0.0006             nan     0.5299   -0.0000
   240        0.0005             nan     0.5299   -0.0000
   260        0.0005             nan     0.5299   -0.0000
   280        0.0004             nan     0.5299   -0.0000
   300        0.0004             nan     0.5299   -0.0000
   320        0.0004             nan     0.5299   -0.0000
   340        0.0004             nan     0.5299   -0.0000
   360        0.0003             nan     0.5299   -0.0000
   380        0.0003             nan     0.5299   -0.0000
   400        0.0003             nan     0.5299   -0.0000
   420        0.0003             nan     0.5299   -0.0000
   440        0.0003             nan     0.5299   -0.0000
   460        0.0002             nan     0.5299   -0.0000
   480        0.0002             nan     0.5299   -0.0000
   500        0.0002             nan     0.5299   -0.0000
   520        0.0002             nan     0.5299   -0.0000
   540        0.0002             nan     0.5299   -0.0000
   560        0.0002             nan     0.5299   -0.0000
   580        0.0002             nan     0.5299   -0.0000
   600        0.0002             nan     0.5299   -0.0000
   620        0.0002             nan     0.5299   -0.0000
   640        0.0002             nan     0.5299   -0.0000
   660        0.0002             nan     0.5299   -0.0000
   680        0.0002             nan     0.5299   -0.0000
   700        0.0002             nan     0.5299   -0.0000
   720        0.0001             nan     0.5299   -0.0000
   740        0.0001             nan     0.5299   -0.0000
   760        0.0001             nan     0.5299   -0.0000
   780        0.0001             nan     0.5299   -0.0000
   800        0.0001             nan     0.5299   -0.0000
   820        0.0001             nan     0.5299   -0.0000
   840        0.0001             nan     0.5299   -0.0000
   860        0.0001             nan     0.5299   -0.0000
   880        0.0001             nan     0.5299   -0.0000
   900        0.0001             nan     0.5299   -0.0000
   920        0.0001             nan     0.5299   -0.0000
   940        0.0001             nan     0.5299   -0.0000
   960        0.0001             nan     0.5299   -0.0000
   980        0.0001             nan     0.5299   -0.0000
  1000        0.0001             nan     0.5299    0.0000
  1020        0.0001             nan     0.5299   -0.0000
  1040        0.0001             nan     0.5299   -0.0000
  1060        0.0001             nan     0.5299   -0.0000
  1080        0.0001             nan     0.5299   -0.0000
  1100        0.0001             nan     0.5299   -0.0000
  1120        0.0001             nan     0.5299   -0.0000
  1140        0.0001             nan     0.5299    0.0000
  1160        0.0001             nan     0.5299   -0.0000
  1180        0.0001             nan     0.5299   -0.0000
  1200        0.0001             nan     0.5299    0.0000
  1220        0.0001             nan     0.5299   -0.0000
  1240        0.0001             nan     0.5299   -0.0000
  1260        0.0001             nan     0.5299   -0.0000
  1280        0.0001             nan     0.5299   -0.0000
  1300        0.0001             nan     0.5299   -0.0000
  1320        0.0001             nan     0.5299   -0.0000
  1340        0.0001             nan     0.5299   -0.0000
  1360        0.0001             nan     0.5299    0.0000
  1380        0.0001             nan     0.5299   -0.0000
  1400        0.0001             nan     0.5299   -0.0000
  1420        0.0001             nan     0.5299   -0.0000
  1440        0.0001             nan     0.5299   -0.0000
  1460        0.0001             nan     0.5299   -0.0000
  1480        0.0001             nan     0.5299    0.0000
  1500        0.0001             nan     0.5299   -0.0000
  1520        0.0001             nan     0.5299    0.0000
  1540        0.0001             nan     0.5299   -0.0000
  1560        0.0001             nan     0.5299   -0.0000
  1580        0.0001             nan     0.5299   -0.0000
  1600        0.0001             nan     0.5299   -0.0000
  1620        0.0001             nan     0.5299   -0.0000
  1640        0.0001             nan     0.5299   -0.0000
  1660        0.0001             nan     0.5299   -0.0000
  1680        0.0001             nan     0.5299    0.0000
  1700        0.0001             nan     0.5299   -0.0000
  1720        0.0001             nan     0.5299   -0.0000
  1740        0.0001             nan     0.5299   -0.0000
  1760        0.0001             nan     0.5299   -0.0000
  1780        0.0001             nan     0.5299    0.0000
  1800        0.0001             nan     0.5299   -0.0000
  1820        0.0001             nan     0.5299   -0.0000
  1840        0.0001             nan     0.5299   -0.0000
  1860        0.0001             nan     0.5299   -0.0000
  1880        0.0001             nan     0.5299   -0.0000
  1900        0.0001             nan     0.5299    0.0000
  1920        0.0001             nan     0.5299   -0.0000
  1940        0.0001             nan     0.5299    0.0000
  1960        0.0001             nan     0.5299   -0.0000
  1980        0.0001             nan     0.5299    0.0000
  2000        0.0001             nan     0.5299   -0.0000
  2020        0.0001             nan     0.5299   -0.0000
  2040        0.0001             nan     0.5299   -0.0000
  2060        0.0001             nan     0.5299   -0.0000
  2080        0.0001             nan     0.5299    0.0000
  2100        0.0001             nan     0.5299    0.0000
  2120        0.0001             nan     0.5299   -0.0000
  2140        0.0001             nan     0.5299   -0.0000
  2160        0.0001             nan     0.5299   -0.0000
  2180        0.0001             nan     0.5299   -0.0000
  2200        0.0001             nan     0.5299   -0.0000
  2220        0.0001             nan     0.5299   -0.0000
  2240        0.0001             nan     0.5299   -0.0000
  2260        0.0001             nan     0.5299    0.0000
  2280        0.0001             nan     0.5299    0.0000
  2300        0.0001             nan     0.5299   -0.0000
  2320        0.0001             nan     0.5299   -0.0000
  2340        0.0001             nan     0.5299   -0.0000
  2360        0.0001             nan     0.5299   -0.0000
  2380        0.0001             nan     0.5299    0.0000
  2400        0.0001             nan     0.5299   -0.0000
  2420        0.0001             nan     0.5299   -0.0000
  2440        0.0001             nan     0.5299   -0.0000
  2460        0.0001             nan     0.5299   -0.0000
  2480        0.0001             nan     0.5299   -0.0000
  2500        0.0001             nan     0.5299   -0.0000
  2520        0.0001             nan     0.5299   -0.0000
  2540        0.0001             nan     0.5299   -0.0000
  2560        0.0001             nan     0.5299    0.0000
  2580        0.0001             nan     0.5299   -0.0000
  2600        0.0001             nan     0.5299   -0.0000
  2620        0.0001             nan     0.5299   -0.0000
  2640        0.0001             nan     0.5299    0.0000
  2660        0.0001             nan     0.5299   -0.0000
  2680        0.0001             nan     0.5299   -0.0000
  2700        0.0001             nan     0.5299   -0.0000
  2720        0.0001             nan     0.5299    0.0000
  2740        0.0001             nan     0.5299   -0.0000
  2760        0.0001             nan     0.5299   -0.0000
  2780        0.0001             nan     0.5299   -0.0000
  2800        0.0001             nan     0.5299   -0.0000
  2820        0.0001             nan     0.5299   -0.0000
  2840        0.0001             nan     0.5299   -0.0000
  2860        0.0001             nan     0.5299   -0.0000
  2880        0.0001             nan     0.5299   -0.0000
  2900        0.0001             nan     0.5299    0.0000
  2920        0.0001             nan     0.5299   -0.0000
  2940        0.0001             nan     0.5299   -0.0000
  2960        0.0001             nan     0.5299   -0.0000
  2980        0.0001             nan     0.5299    0.0000
  3000        0.0001             nan     0.5299   -0.0000
  3020        0.0001             nan     0.5299   -0.0000
  3040        0.0001             nan     0.5299    0.0000
  3060        0.0001             nan     0.5299   -0.0000
  3080        0.0001             nan     0.5299   -0.0000
  3100        0.0001             nan     0.5299   -0.0000
  3120        0.0001             nan     0.5299   -0.0000
  3140        0.0001             nan     0.5299    0.0000
  3160        0.0001             nan     0.5299   -0.0000
  3180        0.0001             nan     0.5299    0.0000
  3200        0.0001             nan     0.5299   -0.0000
  3220        0.0001             nan     0.5299   -0.0000
  3240        0.0001             nan     0.5299    0.0000
  3260        0.0001             nan     0.5299    0.0000
  3280        0.0001             nan     0.5299   -0.0000
  3300        0.0001             nan     0.5299   -0.0000
  3320        0.0001             nan     0.5299   -0.0000
  3340        0.0001             nan     0.5299   -0.0000
  3360        0.0001             nan     0.5299   -0.0000
  3380        0.0001             nan     0.5299   -0.0000
  3400        0.0001             nan     0.5299   -0.0000
  3420        0.0001             nan     0.5299    0.0000
  3440        0.0001             nan     0.5299   -0.0000
  3460        0.0001             nan     0.5299   -0.0000
  3480        0.0001             nan     0.5299   -0.0000
  3500        0.0001             nan     0.5299   -0.0000
  3520        0.0001             nan     0.5299   -0.0000
  3540        0.0001             nan     0.5299   -0.0000
  3560        0.0001             nan     0.5299   -0.0000
  3580        0.0001             nan     0.5299    0.0000
  3600        0.0001             nan     0.5299   -0.0000
  3620        0.0001             nan     0.5299   -0.0000
  3640        0.0001             nan     0.5299   -0.0000
  3660        0.0001             nan     0.5299    0.0000
  3680        0.0001             nan     0.5299   -0.0000
  3700        0.0001             nan     0.5299   -0.0000
  3720        0.0001             nan     0.5299   -0.0000
  3740        0.0001             nan     0.5299   -0.0000
  3760        0.0001             nan     0.5299   -0.0000
  3780        0.0001             nan     0.5299    0.0000
  3800        0.0001             nan     0.5299   -0.0000
  3820        0.0001             nan     0.5299   -0.0000
  3840        0.0001             nan     0.5299   -0.0000
  3860        0.0001             nan     0.5299   -0.0000
  3880        0.0001             nan     0.5299   -0.0000
  3900        0.0001             nan     0.5299   -0.0000
  3920        0.0001             nan     0.5299    0.0000
  3940        0.0001             nan     0.5299    0.0000
  3960        0.0001             nan     0.5299   -0.0000
  3980        0.0001             nan     0.5299   -0.0000
  4000        0.0001             nan     0.5299   -0.0000
  4020        0.0001             nan     0.5299    0.0000
  4040        0.0001             nan     0.5299   -0.0000
  4060        0.0001             nan     0.5299   -0.0000
  4080        0.0001             nan     0.5299   -0.0000
  4100        0.0001             nan     0.5299   -0.0000
  4112        0.0001             nan     0.5299    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1020             nan     0.5649    0.3120
     2        0.0317             nan     0.5649    0.0676
     3        0.0135             nan     0.5649    0.0162
     4        0.0093             nan     0.5649    0.0033
     5        0.0077             nan     0.5649    0.0013
     6        0.0068             nan     0.5649    0.0009
     7        0.0060             nan     0.5649    0.0006
     8        0.0057             nan     0.5649    0.0000
     9        0.0054             nan     0.5649    0.0001
    10        0.0047             nan     0.5649    0.0005
    20        0.0031             nan     0.5649    0.0000
    40        0.0019             nan     0.5649   -0.0001
    60        0.0013             nan     0.5649   -0.0001
    80        0.0010             nan     0.5649   -0.0000
   100        0.0008             nan     0.5649   -0.0000
   120        0.0007             nan     0.5649   -0.0000
   140        0.0006             nan     0.5649   -0.0000
   160        0.0004             nan     0.5649   -0.0000
   180        0.0004             nan     0.5649   -0.0000
   200        0.0003             nan     0.5649   -0.0000
   220        0.0003             nan     0.5649   -0.0000
   240        0.0002             nan     0.5649   -0.0000
   260        0.0002             nan     0.5649   -0.0000
   280        0.0002             nan     0.5649   -0.0000
   300        0.0002             nan     0.5649   -0.0000
   320        0.0002             nan     0.5649   -0.0000
   340        0.0002             nan     0.5649   -0.0000
   360        0.0001             nan     0.5649   -0.0000
   380        0.0001             nan     0.5649   -0.0000
   400        0.0001             nan     0.5649   -0.0000
   420        0.0001             nan     0.5649   -0.0000
   440        0.0001             nan     0.5649   -0.0000
   460        0.0001             nan     0.5649   -0.0000
   480        0.0001             nan     0.5649   -0.0000
   500        0.0001             nan     0.5649   -0.0000
   520        0.0001             nan     0.5649   -0.0000
   540        0.0001             nan     0.5649   -0.0000
   560        0.0001             nan     0.5649   -0.0000
   580        0.0001             nan     0.5649   -0.0000
   600        0.0001             nan     0.5649   -0.0000
   620        0.0001             nan     0.5649   -0.0000
   640        0.0001             nan     0.5649    0.0000
   660        0.0001             nan     0.5649   -0.0000
   680        0.0001             nan     0.5649   -0.0000
   700        0.0001             nan     0.5649   -0.0000
   720        0.0001             nan     0.5649   -0.0000
   740        0.0001             nan     0.5649   -0.0000
   760        0.0001             nan     0.5649   -0.0000
   780        0.0001             nan     0.5649   -0.0000
   800        0.0001             nan     0.5649   -0.0000
   820        0.0001             nan     0.5649   -0.0000
   840        0.0001             nan     0.5649   -0.0000
   860        0.0001             nan     0.5649   -0.0000
   880        0.0001             nan     0.5649   -0.0000
   900        0.0001             nan     0.5649   -0.0000
   920        0.0001             nan     0.5649    0.0000
   940        0.0001             nan     0.5649   -0.0000
   960        0.0001             nan     0.5649   -0.0000
   980        0.0001             nan     0.5649    0.0000
  1000        0.0001             nan     0.5649   -0.0000
  1020        0.0001             nan     0.5649   -0.0000
  1040        0.0001             nan     0.5649   -0.0000
  1060        0.0001             nan     0.5649   -0.0000
  1080        0.0001             nan     0.5649    0.0000
  1100        0.0001             nan     0.5649    0.0000
  1120        0.0001             nan     0.5649   -0.0000
  1140        0.0001             nan     0.5649   -0.0000
  1160        0.0001             nan     0.5649   -0.0000
  1180        0.0001             nan     0.5649   -0.0000
  1200        0.0001             nan     0.5649   -0.0000
  1220        0.0001             nan     0.5649   -0.0000
  1240        0.0001             nan     0.5649   -0.0000
  1260        0.0001             nan     0.5649   -0.0000
  1280        0.0001             nan     0.5649   -0.0000
  1300        0.0001             nan     0.5649   -0.0000
  1320        0.0001             nan     0.5649    0.0000
  1340        0.0001             nan     0.5649   -0.0000
  1360        0.0001             nan     0.5649    0.0000
  1380        0.0001             nan     0.5649   -0.0000
  1400        0.0001             nan     0.5649   -0.0000
  1420        0.0001             nan     0.5649   -0.0000
  1440        0.0001             nan     0.5649   -0.0000
  1460        0.0001             nan     0.5649   -0.0000
  1480        0.0001             nan     0.5649   -0.0000
  1500        0.0001             nan     0.5649    0.0000
  1520        0.0001             nan     0.5649    0.0000
  1540        0.0001             nan     0.5649   -0.0000
  1560        0.0001             nan     0.5649   -0.0000
  1580        0.0001             nan     0.5649   -0.0000
  1600        0.0001             nan     0.5649   -0.0000
  1620        0.0001             nan     0.5649   -0.0000
  1640        0.0001             nan     0.5649    0.0000
  1660        0.0001             nan     0.5649   -0.0000
  1680        0.0001             nan     0.5649    0.0000
  1700        0.0001             nan     0.5649   -0.0000
  1719        0.0001             nan     0.5649   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1140             nan     0.5824    0.3047
     2        0.0453             nan     0.5824    0.0683
     3        0.0218             nan     0.5824    0.0236
     4        0.0144             nan     0.5824    0.0062
     5        0.0121             nan     0.5824    0.0019
     6        0.0100             nan     0.5824    0.0014
     7        0.0092             nan     0.5824    0.0003
     8        0.0082             nan     0.5824    0.0006
     9        0.0069             nan     0.5824    0.0012
    10        0.0066             nan     0.5824    0.0001
    20        0.0041             nan     0.5824    0.0002
    40        0.0022             nan     0.5824    0.0000
    60        0.0016             nan     0.5824   -0.0000
    80        0.0012             nan     0.5824   -0.0000
   100        0.0009             nan     0.5824   -0.0000
   120        0.0008             nan     0.5824   -0.0000
   140        0.0007             nan     0.5824   -0.0000
   160        0.0006             nan     0.5824   -0.0000
   180        0.0005             nan     0.5824   -0.0000
   200        0.0004             nan     0.5824   -0.0000
   220        0.0003             nan     0.5824   -0.0000
   240        0.0003             nan     0.5824   -0.0000
   260        0.0003             nan     0.5824   -0.0000
   280        0.0003             nan     0.5824   -0.0000
   300        0.0002             nan     0.5824   -0.0000
   320        0.0002             nan     0.5824   -0.0000
   340        0.0002             nan     0.5824   -0.0000
   360        0.0002             nan     0.5824   -0.0000
   380        0.0002             nan     0.5824   -0.0000
   400        0.0002             nan     0.5824   -0.0000
   420        0.0001             nan     0.5824   -0.0000
   440        0.0001             nan     0.5824   -0.0000
   460        0.0001             nan     0.5824   -0.0000
   480        0.0001             nan     0.5824   -0.0000
   500        0.0001             nan     0.5824   -0.0000
   520        0.0001             nan     0.5824   -0.0000
   540        0.0001             nan     0.5824   -0.0000
   560        0.0001             nan     0.5824   -0.0000
   580        0.0001             nan     0.5824   -0.0000
   600        0.0001             nan     0.5824   -0.0000
   620        0.0001             nan     0.5824   -0.0000
   640        0.0001             nan     0.5824   -0.0000
   660        0.0001             nan     0.5824   -0.0000
   680        0.0001             nan     0.5824   -0.0000
   700        0.0001             nan     0.5824   -0.0000
   720        0.0001             nan     0.5824   -0.0000
   740        0.0001             nan     0.5824   -0.0000
   760        0.0001             nan     0.5824   -0.0000
   780        0.0001             nan     0.5824   -0.0000
   800        0.0001             nan     0.5824   -0.0000
   820        0.0001             nan     0.5824   -0.0000
   840        0.0001             nan     0.5824   -0.0000
   860        0.0001             nan     0.5824   -0.0000
   880        0.0000             nan     0.5824   -0.0000
   900        0.0000             nan     0.5824   -0.0000
   920        0.0000             nan     0.5824   -0.0000
   940        0.0000             nan     0.5824   -0.0000
   960        0.0000             nan     0.5824   -0.0000
   980        0.0000             nan     0.5824   -0.0000
  1000        0.0000             nan     0.5824   -0.0000
  1020        0.0000             nan     0.5824   -0.0000
  1040        0.0000             nan     0.5824   -0.0000
  1060        0.0000             nan     0.5824   -0.0000
  1080        0.0000             nan     0.5824   -0.0000
  1100        0.0000             nan     0.5824   -0.0000
  1120        0.0000             nan     0.5824   -0.0000
  1140        0.0000             nan     0.5824   -0.0000
  1160        0.0000             nan     0.5824   -0.0000
  1180        0.0000             nan     0.5824    0.0000
  1200        0.0000             nan     0.5824   -0.0000
  1220        0.0000             nan     0.5824    0.0000
  1240        0.0000             nan     0.5824   -0.0000
  1260        0.0000             nan     0.5824   -0.0000
  1280        0.0000             nan     0.5824   -0.0000
  1300        0.0000             nan     0.5824   -0.0000
  1320        0.0000             nan     0.5824    0.0000
  1340        0.0000             nan     0.5824   -0.0000
  1360        0.0000             nan     0.5824   -0.0000
  1380        0.0000             nan     0.5824   -0.0000
  1400        0.0000             nan     0.5824   -0.0000
  1420        0.0000             nan     0.5824   -0.0000
  1440        0.0000             nan     0.5824   -0.0000
  1460        0.0000             nan     0.5824    0.0000
  1480        0.0000             nan     0.5824   -0.0000
  1500        0.0000             nan     0.5824    0.0000
  1520        0.0000             nan     0.5824   -0.0000
  1540        0.0000             nan     0.5824   -0.0000
  1560        0.0000             nan     0.5824   -0.0000
  1580        0.0000             nan     0.5824   -0.0000
  1600        0.0000             nan     0.5824   -0.0000
  1620        0.0000             nan     0.5824   -0.0000
  1640        0.0000             nan     0.5824   -0.0000
  1660        0.0000             nan     0.5824   -0.0000
  1680        0.0000             nan     0.5824   -0.0000
  1700        0.0000             nan     0.5824    0.0000
  1720        0.0000             nan     0.5824   -0.0000
  1740        0.0000             nan     0.5824   -0.0000
  1760        0.0000             nan     0.5824   -0.0000
  1780        0.0000             nan     0.5824   -0.0000
  1800        0.0000             nan     0.5824   -0.0000
  1820        0.0000             nan     0.5824   -0.0000
  1840        0.0000             nan     0.5824   -0.0000
  1860        0.0000             nan     0.5824   -0.0000
  1880        0.0000             nan     0.5824    0.0000
  1900        0.0000             nan     0.5824   -0.0000
  1920        0.0000             nan     0.5824   -0.0000
  1940        0.0000             nan     0.5824   -0.0000
  1960        0.0000             nan     0.5824   -0.0000
  1980        0.0000             nan     0.5824   -0.0000
  2000        0.0000             nan     0.5824   -0.0000
  2020        0.0000             nan     0.5824   -0.0000
  2040        0.0000             nan     0.5824   -0.0000
  2060        0.0000             nan     0.5824   -0.0000
  2080        0.0000             nan     0.5824   -0.0000
  2100        0.0000             nan     0.5824   -0.0000
  2120        0.0000             nan     0.5824    0.0000
  2140        0.0000             nan     0.5824    0.0000
  2160        0.0000             nan     0.5824   -0.0000
  2180        0.0000             nan     0.5824   -0.0000
  2200        0.0000             nan     0.5824   -0.0000
  2220        0.0000             nan     0.5824   -0.0000
  2240        0.0000             nan     0.5824   -0.0000
  2260        0.0000             nan     0.5824   -0.0000
  2280        0.0000             nan     0.5824   -0.0000
  2300        0.0000             nan     0.5824    0.0000
  2320        0.0000             nan     0.5824   -0.0000
  2340        0.0000             nan     0.5824   -0.0000
  2360        0.0000             nan     0.5824   -0.0000
  2380        0.0000             nan     0.5824    0.0000
  2400        0.0000             nan     0.5824   -0.0000
  2420        0.0000             nan     0.5824   -0.0000
  2440        0.0000             nan     0.5824   -0.0000
  2460        0.0000             nan     0.5824   -0.0000
  2480        0.0000             nan     0.5824    0.0000
  2500        0.0000             nan     0.5824   -0.0000
  2520        0.0000             nan     0.5824    0.0000
  2540        0.0000             nan     0.5824   -0.0000
  2560        0.0000             nan     0.5824   -0.0000
  2580        0.0000             nan     0.5824   -0.0000
  2600        0.0000             nan     0.5824    0.0000
  2620        0.0000             nan     0.5824   -0.0000
  2640        0.0000             nan     0.5824   -0.0000
  2660        0.0000             nan     0.5824   -0.0000
  2680        0.0000             nan     0.5824   -0.0000
  2700        0.0000             nan     0.5824    0.0000
  2720        0.0000             nan     0.5824    0.0000
  2740        0.0000             nan     0.5824   -0.0000
  2760        0.0000             nan     0.5824   -0.0000
  2780        0.0000             nan     0.5824   -0.0000
  2800        0.0000             nan     0.5824   -0.0000
  2820        0.0000             nan     0.5824   -0.0000
  2840        0.0000             nan     0.5824   -0.0000
  2860        0.0000             nan     0.5824   -0.0000
  2880        0.0000             nan     0.5824   -0.0000
  2900        0.0000             nan     0.5824   -0.0000
  2920        0.0000             nan     0.5824   -0.0000
  2940        0.0000             nan     0.5824   -0.0000
  2960        0.0000             nan     0.5824   -0.0000
  2980        0.0000             nan     0.5824   -0.0000
  3000        0.0000             nan     0.5824   -0.0000
  3020        0.0000             nan     0.5824   -0.0000
  3040        0.0000             nan     0.5824    0.0000
  3060        0.0000             nan     0.5824    0.0000
  3080        0.0000             nan     0.5824    0.0000
  3100        0.0000             nan     0.5824   -0.0000
  3120        0.0000             nan     0.5824   -0.0000
  3140        0.0000             nan     0.5824   -0.0000
  3160        0.0000             nan     0.5824   -0.0000
  3180        0.0000             nan     0.5824    0.0000
  3200        0.0000             nan     0.5824   -0.0000
  3220        0.0000             nan     0.5824   -0.0000
  3240        0.0000             nan     0.5824   -0.0000
  3260        0.0000             nan     0.5824   -0.0000
  3280        0.0000             nan     0.5824   -0.0000
  3300        0.0000             nan     0.5824   -0.0000
  3320        0.0000             nan     0.5824   -0.0000
  3340        0.0000             nan     0.5824   -0.0000
  3360        0.0000             nan     0.5824   -0.0000
  3380        0.0000             nan     0.5824   -0.0000
  3400        0.0000             nan     0.5824    0.0000
  3420        0.0000             nan     0.5824   -0.0000
  3440        0.0000             nan     0.5824   -0.0000
  3460        0.0000             nan     0.5824    0.0000
  3480        0.0000             nan     0.5824   -0.0000
  3500        0.0000             nan     0.5824   -0.0000
  3520        0.0000             nan     0.5824   -0.0000
  3540        0.0000             nan     0.5824   -0.0000
  3560        0.0000             nan     0.5824    0.0000
  3580        0.0000             nan     0.5824   -0.0000
  3600        0.0000             nan     0.5824    0.0000
  3620        0.0000             nan     0.5824   -0.0000
  3640        0.0000             nan     0.5824   -0.0000
  3660        0.0000             nan     0.5824   -0.0000
  3680        0.0000             nan     0.5824   -0.0000
  3700        0.0000             nan     0.5824   -0.0000
  3720        0.0000             nan     0.5824   -0.0000
  3740        0.0000             nan     0.5824   -0.0000
  3760        0.0000             nan     0.5824   -0.0000
  3780        0.0000             nan     0.5824   -0.0000
  3800        0.0000             nan     0.5824   -0.0000
  3820        0.0000             nan     0.5824    0.0000
  3840        0.0000             nan     0.5824   -0.0000
  3860        0.0000             nan     0.5824    0.0000
  3880        0.0000             nan     0.5824   -0.0000
  3900        0.0000             nan     0.5824   -0.0000
  3920        0.0000             nan     0.5824   -0.0000
  3940        0.0000             nan     0.5824    0.0000
  3960        0.0000             nan     0.5824   -0.0000
  3980        0.0000             nan     0.5824   -0.0000
  4000        0.0000             nan     0.5824   -0.0000
  4020        0.0000             nan     0.5824   -0.0000
  4040        0.0000             nan     0.5824   -0.0000
  4060        0.0000             nan     0.5824   -0.0000
  4080        0.0000             nan     0.5824   -0.0000
  4100        0.0000             nan     0.5824   -0.0000
  4120        0.0000             nan     0.5824   -0.0000
  4140        0.0000             nan     0.5824    0.0000
  4160        0.0000             nan     0.5824   -0.0000
  4180        0.0000             nan     0.5824   -0.0000
  4200        0.0000             nan     0.5824   -0.0000
  4220        0.0000             nan     0.5824   -0.0000
  4240        0.0000             nan     0.5824   -0.0000
  4260        0.0000             nan     0.5824   -0.0000
  4280        0.0000             nan     0.5824   -0.0000
  4300        0.0000             nan     0.5824   -0.0000
  4320        0.0000             nan     0.5824   -0.0000
  4340        0.0000             nan     0.5824   -0.0000
  4360        0.0000             nan     0.5824   -0.0000
  4380        0.0000             nan     0.5824   -0.0000
  4400        0.0000             nan     0.5824   -0.0000
  4420        0.0000             nan     0.5824   -0.0000
  4440        0.0000             nan     0.5824   -0.0000
  4460        0.0000             nan     0.5824   -0.0000
  4480        0.0000             nan     0.5824   -0.0000
  4500        0.0000             nan     0.5824    0.0000
  4520        0.0000             nan     0.5824   -0.0000
  4540        0.0000             nan     0.5824   -0.0000
  4560        0.0000             nan     0.5824   -0.0000
  4580        0.0000             nan     0.5824    0.0000
  4600        0.0000             nan     0.5824   -0.0000
  4617        0.0000             nan     0.5824   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.1484             nan     0.5882    0.2694
     2        0.0580             nan     0.5882    0.0921
     3        0.0324             nan     0.5882    0.0270
     4        0.0220             nan     0.5882    0.0099
     5        0.0183             nan     0.5882    0.0030
     6        0.0156             nan     0.5882    0.0016
     7        0.0143             nan     0.5882    0.0009
     8        0.0139             nan     0.5882   -0.0003
     9        0.0124             nan     0.5882    0.0012
    10        0.0117             nan     0.5882    0.0003
    20        0.0048             nan     0.5882    0.0005
    40        0.0024             nan     0.5882   -0.0001
    60        0.0015             nan     0.5882   -0.0001
    80        0.0011             nan     0.5882   -0.0001
   100        0.0008             nan     0.5882   -0.0000
   120        0.0007             nan     0.5882    0.0000
   140        0.0006             nan     0.5882   -0.0000
   160        0.0005             nan     0.5882   -0.0000
   180        0.0005             nan     0.5882   -0.0000
   200        0.0004             nan     0.5882   -0.0000
   220        0.0004             nan     0.5882   -0.0000
   240        0.0003             nan     0.5882   -0.0000
   260        0.0003             nan     0.5882   -0.0000
   280        0.0003             nan     0.5882   -0.0000
   300        0.0002             nan     0.5882   -0.0000
   320        0.0002             nan     0.5882   -0.0000
   340        0.0002             nan     0.5882   -0.0000
   360        0.0002             nan     0.5882   -0.0000
   380        0.0002             nan     0.5882   -0.0000
   400        0.0001             nan     0.5882   -0.0000
   420        0.0001             nan     0.5882   -0.0000
   440        0.0001             nan     0.5882   -0.0000
   460        0.0001             nan     0.5882   -0.0000
   480        0.0001             nan     0.5882   -0.0000
   500        0.0001             nan     0.5882   -0.0000
   520        0.0001             nan     0.5882   -0.0000
   540        0.0001             nan     0.5882   -0.0000
   560        0.0001             nan     0.5882   -0.0000
   580        0.0001             nan     0.5882   -0.0000
   600        0.0001             nan     0.5882   -0.0000
   620        0.0001             nan     0.5882   -0.0000
   640        0.0001             nan     0.5882   -0.0000
   660        0.0001             nan     0.5882   -0.0000
   680        0.0001             nan     0.5882   -0.0000
   700        0.0001             nan     0.5882   -0.0000
   720        0.0001             nan     0.5882   -0.0000
   740        0.0001             nan     0.5882   -0.0000
   760        0.0001             nan     0.5882    0.0000
   780        0.0000             nan     0.5882   -0.0000
   800        0.0000             nan     0.5882   -0.0000
   820        0.0000             nan     0.5882   -0.0000
   840        0.0000             nan     0.5882   -0.0000
   860        0.0000             nan     0.5882   -0.0000
   880        0.0000             nan     0.5882   -0.0000
   900        0.0000             nan     0.5882   -0.0000
   920        0.0000             nan     0.5882   -0.0000
   940        0.0000             nan     0.5882   -0.0000
   960        0.0000             nan     0.5882   -0.0000
   980        0.0000             nan     0.5882   -0.0000
  1000        0.0000             nan     0.5882   -0.0000
  1020        0.0000             nan     0.5882   -0.0000
  1040        0.0000             nan     0.5882   -0.0000
  1060        0.0000             nan     0.5882   -0.0000
  1080        0.0000             nan     0.5882   -0.0000
  1100        0.0000             nan     0.5882   -0.0000
  1120        0.0000             nan     0.5882   -0.0000
  1140        0.0000             nan     0.5882   -0.0000
  1160        0.0000             nan     0.5882   -0.0000
  1180        0.0000             nan     0.5882    0.0000
  1200        0.0000             nan     0.5882   -0.0000
  1220        0.0000             nan     0.5882   -0.0000
  1240        0.0000             nan     0.5882   -0.0000
  1260        0.0000             nan     0.5882   -0.0000
  1280        0.0000             nan     0.5882   -0.0000
  1300        0.0000             nan     0.5882   -0.0000
  1320        0.0000             nan     0.5882   -0.0000
  1340        0.0000             nan     0.5882   -0.0000
  1360        0.0000             nan     0.5882   -0.0000
  1380        0.0000             nan     0.5882   -0.0000
  1400        0.0000             nan     0.5882   -0.0000
  1420        0.0000             nan     0.5882   -0.0000
  1440        0.0000             nan     0.5882   -0.0000
  1460        0.0000             nan     0.5882   -0.0000
  1480        0.0000             nan     0.5882   -0.0000
  1500        0.0000             nan     0.5882   -0.0000
  1520        0.0000             nan     0.5882   -0.0000
  1540        0.0000             nan     0.5882   -0.0000
  1560        0.0000             nan     0.5882    0.0000
  1580        0.0000             nan     0.5882   -0.0000
  1600        0.0000             nan     0.5882   -0.0000
  1620        0.0000             nan     0.5882   -0.0000
  1640        0.0000             nan     0.5882   -0.0000
  1660        0.0000             nan     0.5882   -0.0000
  1680        0.0000             nan     0.5882   -0.0000
  1700        0.0000             nan     0.5882    0.0000
  1720        0.0000             nan     0.5882    0.0000
  1740        0.0000             nan     0.5882   -0.0000
  1760        0.0000             nan     0.5882   -0.0000
  1780        0.0000             nan     0.5882   -0.0000
  1800        0.0000             nan     0.5882   -0.0000
  1820        0.0000             nan     0.5882   -0.0000
  1840        0.0000             nan     0.5882   -0.0000
  1860        0.0000             nan     0.5882   -0.0000
  1880        0.0000             nan     0.5882   -0.0000
  1900        0.0000             nan     0.5882   -0.0000
  1920        0.0000             nan     0.5882    0.0000
  1940        0.0000             nan     0.5882   -0.0000
  1960        0.0000             nan     0.5882   -0.0000
  1980        0.0000             nan     0.5882   -0.0000
  2000        0.0000             nan     0.5882   -0.0000
  2020        0.0000             nan     0.5882   -0.0000
  2040        0.0000             nan     0.5882   -0.0000
  2060        0.0000             nan     0.5882   -0.0000
  2080        0.0000             nan     0.5882   -0.0000
  2100        0.0000             nan     0.5882   -0.0000
  2120        0.0000             nan     0.5882   -0.0000
  2140        0.0000             nan     0.5882   -0.0000
  2160        0.0000             nan     0.5882   -0.0000
  2180        0.0000             nan     0.5882   -0.0000
  2200        0.0000             nan     0.5882   -0.0000
  2220        0.0000             nan     0.5882   -0.0000
  2240        0.0000             nan     0.5882   -0.0000
  2260        0.0000             nan     0.5882   -0.0000
  2280        0.0000             nan     0.5882   -0.0000
  2300        0.0000             nan     0.5882   -0.0000
  2320        0.0000             nan     0.5882    0.0000
  2340        0.0000             nan     0.5882    0.0000
  2360        0.0000             nan     0.5882   -0.0000
  2380        0.0000             nan     0.5882   -0.0000
  2400        0.0000             nan     0.5882   -0.0000
  2420        0.0000             nan     0.5882   -0.0000
  2440        0.0000             nan     0.5882    0.0000
  2460        0.0000             nan     0.5882   -0.0000
  2480        0.0000             nan     0.5882   -0.0000
  2500        0.0000             nan     0.5882   -0.0000
  2520        0.0000             nan     0.5882    0.0000
  2540        0.0000             nan     0.5882    0.0000
  2560        0.0000             nan     0.5882   -0.0000
  2580        0.0000             nan     0.5882   -0.0000
  2600        0.0000             nan     0.5882   -0.0000
  2620        0.0000             nan     0.5882   -0.0000
  2640        0.0000             nan     0.5882   -0.0000
  2660        0.0000             nan     0.5882   -0.0000
  2680        0.0000             nan     0.5882   -0.0000
  2700        0.0000             nan     0.5882   -0.0000
  2720        0.0000             nan     0.5882   -0.0000
  2740        0.0000             nan     0.5882   -0.0000
  2760        0.0000             nan     0.5882    0.0000
  2780        0.0000             nan     0.5882   -0.0000
  2800        0.0000             nan     0.5882   -0.0000
  2820        0.0000             nan     0.5882   -0.0000
  2840        0.0000             nan     0.5882   -0.0000
  2860        0.0000             nan     0.5882   -0.0000
  2880        0.0000             nan     0.5882    0.0000
  2900        0.0000             nan     0.5882   -0.0000
  2920        0.0000             nan     0.5882   -0.0000
  2940        0.0000             nan     0.5882   -0.0000
  2960        0.0000             nan     0.5882   -0.0000
  2980        0.0000             nan     0.5882   -0.0000
  3000        0.0000             nan     0.5882   -0.0000
  3020        0.0000             nan     0.5882   -0.0000
  3040        0.0000             nan     0.5882   -0.0000
  3060        0.0000             nan     0.5882   -0.0000
  3080        0.0000             nan     0.5882    0.0000
  3100        0.0000             nan     0.5882   -0.0000
  3120        0.0000             nan     0.5882   -0.0000
  3140        0.0000             nan     0.5882    0.0000
  3160        0.0000             nan     0.5882   -0.0000
  3180        0.0000             nan     0.5882    0.0000
  3200        0.0000             nan     0.5882   -0.0000
  3220        0.0000             nan     0.5882    0.0000
  3240        0.0000             nan     0.5882    0.0000
  3260        0.0000             nan     0.5882   -0.0000
  3280        0.0000             nan     0.5882   -0.0000
  3300        0.0000             nan     0.5882   -0.0000
  3320        0.0000             nan     0.5882   -0.0000
  3340        0.0000             nan     0.5882   -0.0000
  3360        0.0000             nan     0.5882   -0.0000
  3380        0.0000             nan     0.5882   -0.0000
  3400        0.0000             nan     0.5882   -0.0000
  3420        0.0000             nan     0.5882   -0.0000
  3440        0.0000             nan     0.5882   -0.0000
  3460        0.0000             nan     0.5882   -0.0000
  3480        0.0000             nan     0.5882   -0.0000
  3500        0.0000             nan     0.5882    0.0000
  3520        0.0000             nan     0.5882   -0.0000
  3540        0.0000             nan     0.5882   -0.0000
  3560        0.0000             nan     0.5882    0.0000
  3580        0.0000             nan     0.5882   -0.0000
  3600        0.0000             nan     0.5882   -0.0000
  3620        0.0000             nan     0.5882   -0.0000
  3640        0.0000             nan     0.5882   -0.0000
  3660        0.0000             nan     0.5882   -0.0000
  3680        0.0000             nan     0.5882   -0.0000
  3700        0.0000             nan     0.5882   -0.0000
  3720        0.0000             nan     0.5882   -0.0000
  3740        0.0000             nan     0.5882   -0.0000
  3760        0.0000             nan     0.5882   -0.0000
  3780        0.0000             nan     0.5882   -0.0000
  3800        0.0000             nan     0.5882   -0.0000
  3820        0.0000             nan     0.5882   -0.0000
  3840        0.0000             nan     0.5882   -0.0000
  3860        0.0000             nan     0.5882   -0.0000
  3880        0.0000             nan     0.5882   -0.0000
  3900        0.0000             nan     0.5882   -0.0000
  3920        0.0000             nan     0.5882   -0.0000
  3940        0.0000             nan     0.5882   -0.0000
  3960        0.0000             nan     0.5882   -0.0000
  3980        0.0000             nan     0.5882   -0.0000
  4000        0.0000             nan     0.5882   -0.0000
  4020        0.0000             nan     0.5882   -0.0000
  4040        0.0000             nan     0.5882   -0.0000
  4060        0.0000             nan     0.5882   -0.0000
  4080        0.0000             nan     0.5882   -0.0000
  4100        0.0000             nan     0.5882   -0.0000
  4120        0.0000             nan     0.5882    0.0000
  4140        0.0000             nan     0.5882   -0.0000
  4160        0.0000             nan     0.5882   -0.0000
  4180        0.0000             nan     0.5882   -0.0000
  4200        0.0000             nan     0.5882   -0.0000
  4220        0.0000             nan     0.5882   -0.0000
  4240        0.0000             nan     0.5882   -0.0000
  4260        0.0000             nan     0.5882   -0.0000
  4280        0.0000             nan     0.5882   -0.0000
  4300        0.0000             nan     0.5882   -0.0000
  4320        0.0000             nan     0.5882   -0.0000
  4340        0.0000             nan     0.5882   -0.0000
  4360        0.0000             nan     0.5882   -0.0000
  4380        0.0000             nan     0.5882   -0.0000
  4400        0.0000             nan     0.5882   -0.0000
  4420        0.0000             nan     0.5882   -0.0000
  4440        0.0000             nan     0.5882   -0.0000
  4460        0.0000             nan     0.5882   -0.0000
  4480        0.0000             nan     0.5882   -0.0000
  4500        0.0000             nan     0.5882    0.0000
  4520        0.0000             nan     0.5882   -0.0000
  4540        0.0000             nan     0.5882   -0.0000
  4560        0.0000             nan     0.5882    0.0000
  4580        0.0000             nan     0.5882   -0.0000
  4600        0.0000             nan     0.5882   -0.0000
  4620        0.0000             nan     0.5882   -0.0000
  4640        0.0000             nan     0.5882   -0.0000
  4660        0.0000             nan     0.5882   -0.0000
  4680        0.0000             nan     0.5882   -0.0000
  4700        0.0000             nan     0.5882   -0.0000
  4720        0.0000             nan     0.5882   -0.0000
  4740        0.0000             nan     0.5882   -0.0000
  4760        0.0000             nan     0.5882   -0.0000
  4780        0.0000             nan     0.5882   -0.0000
  4800        0.0000             nan     0.5882   -0.0000
  4820        0.0000             nan     0.5882   -0.0000
  4840        0.0000             nan     0.5882    0.0000
  4846        0.0000             nan     0.5882   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3981             nan     0.0196    0.0126
     2        0.3863             nan     0.0196    0.0125
     3        0.3737             nan     0.0196    0.0117
     4        0.3628             nan     0.0196    0.0115
     5        0.3520             nan     0.0196    0.0115
     6        0.3416             nan     0.0196    0.0109
     7        0.3314             nan     0.0196    0.0104
     8        0.3210             nan     0.0196    0.0106
     9        0.3108             nan     0.0196    0.0099
    10        0.3014             nan     0.0196    0.0099
    20        0.2236             nan     0.0196    0.0068
    40        0.1272             nan     0.0196    0.0038
    60        0.0742             nan     0.0196    0.0020
    80        0.0448             nan     0.0196    0.0011
   100        0.0278             nan     0.0196    0.0006
   120        0.0178             nan     0.0196    0.0004
   140        0.0116             nan     0.0196    0.0002
   160        0.0078             nan     0.0196    0.0001
   180        0.0054             nan     0.0196    0.0001
   200        0.0038             nan     0.0196    0.0000
   220        0.0028             nan     0.0196    0.0000
   240        0.0022             nan     0.0196    0.0000
   260        0.0018             nan     0.0196    0.0000
   280        0.0015             nan     0.0196    0.0000
   300        0.0012             nan     0.0196    0.0000
   320        0.0011             nan     0.0196    0.0000
   340        0.0009             nan     0.0196    0.0000
   360        0.0008             nan     0.0196    0.0000
   380        0.0008             nan     0.0196    0.0000
   400        0.0007             nan     0.0196    0.0000
   420        0.0006             nan     0.0196    0.0000
   440        0.0006             nan     0.0196    0.0000
   460        0.0006             nan     0.0196   -0.0000
   480        0.0005             nan     0.0196    0.0000
   500        0.0005             nan     0.0196    0.0000
   520        0.0005             nan     0.0196    0.0000
   540        0.0005             nan     0.0196    0.0000
   560        0.0005             nan     0.0196    0.0000
   580        0.0004             nan     0.0196    0.0000
   600        0.0004             nan     0.0196    0.0000
   620        0.0004             nan     0.0196    0.0000
   640        0.0004             nan     0.0196    0.0000
   660        0.0004             nan     0.0196    0.0000
   680        0.0004             nan     0.0196    0.0000
   700        0.0004             nan     0.0196    0.0000
   720        0.0003             nan     0.0196   -0.0000
   740        0.0003             nan     0.0196    0.0000
   760        0.0003             nan     0.0196    0.0000
   780        0.0003             nan     0.0196   -0.0000
   800        0.0003             nan     0.0196   -0.0000
   820        0.0003             nan     0.0196    0.0000
   840        0.0003             nan     0.0196    0.0000
   860        0.0003             nan     0.0196   -0.0000
   880        0.0003             nan     0.0196    0.0000
   900        0.0003             nan     0.0196    0.0000
   920        0.0003             nan     0.0196    0.0000
   940        0.0003             nan     0.0196   -0.0000
   960        0.0003             nan     0.0196    0.0000
   980        0.0002             nan     0.0196    0.0000
  1000        0.0002             nan     0.0196    0.0000
  1020        0.0002             nan     0.0196   -0.0000
  1040        0.0002             nan     0.0196   -0.0000
  1060        0.0002             nan     0.0196    0.0000
  1080        0.0002             nan     0.0196   -0.0000
  1100        0.0002             nan     0.0196   -0.0000
  1120        0.0002             nan     0.0196   -0.0000
  1140        0.0002             nan     0.0196    0.0000
  1160        0.0002             nan     0.0196    0.0000
  1180        0.0002             nan     0.0196    0.0000
  1200        0.0002             nan     0.0196   -0.0000
  1220        0.0002             nan     0.0196    0.0000
  1240        0.0002             nan     0.0196   -0.0000
  1260        0.0002             nan     0.0196    0.0000
  1280        0.0002             nan     0.0196   -0.0000
  1300        0.0002             nan     0.0196   -0.0000
  1320        0.0002             nan     0.0196    0.0000
  1340        0.0002             nan     0.0196    0.0000
  1360        0.0002             nan     0.0196    0.0000
  1380        0.0002             nan     0.0196   -0.0000
  1400        0.0002             nan     0.0196    0.0000
  1420        0.0002             nan     0.0196   -0.0000
  1440        0.0002             nan     0.0196    0.0000
  1460        0.0002             nan     0.0196    0.0000
  1475        0.0002             nan     0.0196   -0.0000

Stochastic Gradient Boosting 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  shrinkage   interaction.depth  n.minobsinnode  n.trees  RMSE        Rsquared 
  0.01959256   2                  5              1475     0.02380094  0.9986502
  0.08784565   1                  9              1395     0.03588558  0.9968863
  0.11634598   3                  5              1106     0.02464012  0.9985508
  0.16259136  10                 18                50     0.04741619  0.9945527
  0.20832400   5                 18              1974     0.04497436  0.9952369
  0.21328342   3                  8              4439     0.02980132  0.9978599
  0.22061933   4                  9              3222     0.03466845  0.9971422
  0.25873037   8                 15              3022     0.04605633  0.9949302
  0.29801143   3                 14              4121     0.04358563  0.9955069
  0.33322285   7                 13              2966     0.04604073  0.9948623
  0.39487685   6                  6              1168     0.04171890  0.9957947
  0.42229782   3                 14              4131     0.04973303  0.9940307
  0.44687400   8                 12              1713     0.05185590  0.9935410
  0.44728117   4                  5              3855     0.04258336  0.9955834
  0.48745716   5                 10              4416     0.05075877  0.9937104
  0.52398405  10                 12              4216     0.05509591  0.9927284
  0.52992747   4                 25              4112     0.06483567  0.9899205
  0.56491335   5                 20              1719     0.06250580  0.9907745
  0.58237267   3                 16              4617     0.05861563  0.9917284
  0.58818637   2                  8              4846     0.04460468  0.9952060
  MAE         Selected
  0.01807003  *       
  0.02741625          
  0.01877649          
  0.03207943          
  0.03031713          
  0.02269790          
  0.02605296          
  0.03212661          
  0.03061648          
  0.03311273          
  0.03225150          
  0.03552223          
  0.03801903          
  0.03276536          
  0.03871211          
  0.03990627          
  0.04710980          
  0.04608261          
  0.04179690          
  0.03374576          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were n.trees = 1475, interaction.depth =
 2, shrinkage = 0.01959256 and n.minobsinnode = 5.
[1] "Mon Mar 12 04:31:16 2018"

----------------------------------------------------------------------

Your next step is to start H2O:
    > h2o.init()

For H2O package documentation, ask for help:
    > ??h2o

After starting H2O, you can use the Web UI at http://localhost:54321
For more information visit http://docs.h2o.ai

----------------------------------------------------------------------


Attaching package: 'h2o'

The following objects are masked from 'package:stats':

    cor, sd, var

The following objects are masked from 'package:base':

    %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,
    colnames<-, ifelse, is.character, is.factor, is.numeric, log,
    log10, log1p, log2, round, signif, trunc

Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 04:34:29 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "gbm_h2o"                        
Multivariate Adaptive Regression Splines 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results:

  RMSE       Rsquared   MAE       
  0.0222739  0.9988084  0.01717182

Tuning parameter 'degree' was held constant at a value of 1
[1] "Mon Mar 12 04:34:36 2018"
Generalized Linear Model 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results:

  RMSE         Rsquared   MAE       
  0.007977303  0.9998473  0.00607296

[1] "Mon Mar 12 04:34:42 2018"
Negative Binomial Generalized Linear Model 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  link      RMSE         Rsquared   MAE          Selected
  identity  0.009051751  0.9998076  0.006826911  *       
  log       0.131680546  0.9598821  0.099797340          
  sqrt      0.076646814  0.9871501  0.058580181          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was link = identity.
[1] "Mon Mar 12 04:34:50 2018"
Boosted Generalized Linear Model 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  mstop  prune  RMSE        Rsquared   MAE         Selected
   10    no     0.24356269  0.9817832  0.19898120          
  222    yes    0.02597858  0.9983816  0.02058128          
  234    no     0.02595993  0.9983844  0.02056934          
  279    yes    0.02587640  0.9983947  0.02052203          
  296    yes    0.02584938  0.9983982  0.02050373          
  343    no     0.02576636  0.9984084  0.02044995          
  344    yes    0.02576646  0.9984087  0.02044825          
  395    yes    0.02568130  0.9984191  0.02039003          
  594    no     0.02535889  0.9984588  0.02013882          
  605    no     0.02533638  0.9984611  0.02012332          
  645    yes    0.02527331  0.9984690  0.02006983          
  772    yes    0.02505857  0.9984947  0.01989919          
  823    yes    0.02497430  0.9985047  0.01983244          
  825    yes    0.02496965  0.9985052  0.01983076          
  827    yes    0.02496625  0.9985056  0.01982750          
  844    no     0.02493819  0.9985089  0.01980645          
  884    yes    0.02487273  0.9985169  0.01975369          
  888    yes    0.02486551  0.9985178  0.01974920          
  924    yes    0.02480546  0.9985247  0.01969997          
  970    yes    0.02473031  0.9985338  0.01964098  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mstop = 970 and prune = yes.
[1] "Mon Mar 12 04:35:07 2018"
Loaded glmnet 2.0-13

glmnet 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  alpha        lambda        RMSE        Rsquared   MAE         Selected
  0.009869629  4.9760059219  0.41056416  0.9293205  0.34036227          
  0.221141259  0.0068945749  0.02011340  0.9990750  0.01507797          
  0.233456740  0.1296891117  0.06792261  0.9961555  0.05494138          
  0.278890499  0.0009900923  0.02023519  0.9990659  0.01513711          
  0.295011987  0.0033559701  0.02001785  0.9990843  0.01498282          
  0.342562624  0.9458618502  0.40698861  0.9903478  0.33447312          
  0.343677541  0.0513077455  0.04084893  0.9978050  0.03171760          
  0.394752379  0.0700854469  0.05154445  0.9974780  0.04078632          
  0.593184605  0.3202326748  0.22897576  0.9938457  0.18720670          
  0.604462799  1.2881314039  0.64060858        NaN  0.52859819          
  0.644371057  0.0306253637  0.03691663  0.9981326  0.02879382          
  0.771025799  0.0262338710  0.03645581  0.9981255  0.02862744          
  0.822559009  0.0323175818  0.04115548  0.9981022  0.03285634          
  0.824171485  0.0081428105  0.02133641  0.9990501  0.01659093          
  0.826242191  0.0142383293  0.02732697  0.9986401  0.02130124          
  0.843276577  7.8575398874  0.64060858        NaN  0.52859819          
  0.883317267  0.0521922987  0.05806409  0.9980762  0.04736624          
  0.887864152  0.0070979882  0.02094795  0.9990659  0.01626722          
  0.923531971  0.0129278661  0.02912891  0.9983631  0.02268447          
  0.969344636  0.0050412390  0.01972593  0.9991511  0.01529479  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were alpha = 0.9693446 and lambda
 = 0.005041239.
[1] "Mon Mar 12 04:35:16 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 04:38:29 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "glmnet_h2o"                     
Start:  AIC=-3466.2
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
<none>    0.027333 -3466.2
- V9    1 0.027523 -3464.7
- V7    1 0.029111 -3436.7
- V6    1 0.029810 -3424.8
- V10   1 0.030829 -3408.0
- V8    1 0.032111 -3387.6
- V3    1 0.052780 -3139.2
- V4    1 0.055871 -3110.7
- V5    1 0.108932 -2776.9
- V2    1 0.117808 -2737.7
Start:  AIC=-3406.92
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V9    1 0.030866 -3407.4
<none>    0.030773 -3406.9
- V7    1 0.032342 -3384.1
- V6    1 0.033028 -3373.6
- V10   1 0.034220 -3355.8
- V8    1 0.035755 -3333.9
- V3    1 0.057472 -3096.6
- V4    1 0.061733 -3060.8
- V5    1 0.113112 -2758.1
- V2    1 0.120441 -2726.7

Step:  AIC=-3407.41
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V10

       Df Deviance     AIC
<none>    0.030866 -3407.4
- V7    1 0.032342 -3386.1
- V6    1 0.033070 -3374.9
- V10   1 0.037775 -3308.4
- V8    1 0.040508 -3273.5
- V3    1 0.062948 -3053.1
- V4    1 0.109482 -2776.4
- V5    1 0.114322 -2754.7
- V2    1 0.170692 -2554.3
Start:  AIC=-3428.14
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
<none>    0.030438 -3428.1
- V9    1 0.031276 -3416.5
- V7    1 0.031561 -3411.9
- V6    1 0.032274 -3400.7
- V10   1 0.033546 -3381.3
- V8    1 0.037880 -3320.3
- V3    1 0.062338 -3070.3
- V4    1 0.065751 -3043.5
- V5    1 0.107108 -2798.6
- V2    1 0.132117 -2693.2
Start:  AIC=-5150.77
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
<none>    0.044855 -5150.8
- V9    1 0.045250 -5146.2
- V7    1 0.047009 -5117.6
- V6    1 0.048042 -5101.2
- V10   1 0.050141 -5069.1
- V8    1 0.053312 -5023.1
- V3    1 0.086622 -4658.5
- V4    1 0.091915 -4614.0
- V5    1 0.166042 -4169.9
- V2    1 0.188587 -4074.2
Generalized Linear Model with Stepwise Feature Selection 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results:

  RMSE         Rsquared   MAE        
  0.007996443  0.9998465  0.006094622

[1] "Mon Mar 12 04:38:35 2018"
Independent Component Regression 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  n.comp  RMSE         Rsquared   MAE          Selected
  1       0.284256253  0.8036439  0.226091622          
  2       0.066340720  0.9893435  0.049489770          
  3       0.033739494  0.9972350  0.023557278          
  4       0.022213852  0.9988171  0.017345410          
  6       0.013857505  0.9995419  0.010605413          
  7       0.011033305  0.9997106  0.008185952          
  8       0.008107057  0.9998418  0.006212901          
  9       0.007977303  0.9998473  0.006072960  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was n.comp = 9.
[1] "Mon Mar 12 04:38:46 2018"
Partial Least Squares 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  ncomp  RMSE         Rsquared   MAE          Selected
  1      0.220586693  0.8816199  0.168898378          
  2      0.110245877  0.9705719  0.088226323          
  3      0.048460227  0.9942869  0.038532560          
  4      0.025462455  0.9984341  0.019718348          
  6      0.008225863  0.9998372  0.006247250          
  7      0.008094245  0.9998423  0.006193119          
  8      0.008029662  0.9998448  0.006198711          
  9      0.007977303  0.9998473  0.006072960  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 9.
[1] "Mon Mar 12 04:38:52 2018"
k-Nearest Neighbors 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  kmax  distance     kernel        RMSE        Rsquared   MAE         Selected
    3   2.841920354  epanechnikov  0.05354206  0.9932317  0.04147263          
   56   0.650694764  triangular    0.04513727  0.9953833  0.03389283  *       
   59   1.627645215  cos           0.04643490  0.9952384  0.03474588          
   70   0.004580907  triangular    0.18465865  0.9581943  0.14116468          
   74   0.410987440  rectangular   0.05448119  0.9932063  0.04046380          
   86   1.318920103  gaussian      0.04849225  0.9948033  0.03582730          
   86   2.289161859  cos           0.04786612  0.9949704  0.03590440          
   99   1.422752011  epanechnikov  0.04618905  0.9952683  0.03478594          
  149   1.928582867  triweight     0.04572368  0.9954080  0.03436464          
  152   2.391987640  biweight      0.04675714  0.9951564  0.03538548          
  162   1.147124028  epanechnikov  0.04583205  0.9952940  0.03448153          
  193   1.095594119  cos           0.04542819  0.9953664  0.03419807          
  206   1.165029937  gaussian      0.04782755  0.9949076  0.03535105          
  207   0.706094434  biweight      0.04572646  0.9953710  0.03392080          
  207   0.892136234  cos           0.04538493  0.9953683  0.03407566          
  211   2.994017929  inv           0.05051948  0.9944190  0.03734080          
  221   1.324610943  inv           0.04767436  0.9949064  0.03517374          
  222   0.660375207  epanechnikov  0.04638892  0.9951437  0.03472891          
  231   0.859991070  gaussian      0.04822455  0.9947997  0.03542801          
  243   0.546460171  gaussian      0.04867630  0.9946351  0.03606821          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were kmax = 56, distance = 0.6506948
 and kernel = triangular.
[1] "Mon Mar 12 04:43:12 2018"
k-Nearest Neighbors 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  k    RMSE        Rsquared   MAE         Selected
    3  0.06126847  0.9911286  0.04839833  *       
   56  0.14816054  0.9603900  0.11233763          
   59  0.15234353  0.9585518  0.11533473          
   70  0.16600765  0.9519756  0.12497862          
   74  0.17094875  0.9494185  0.12846729          
   86  0.18495221  0.9425166  0.13859335          
   99  0.19987467  0.9349988  0.14852421          
  149  0.24826660  0.9097710  0.18410597          
  152  0.25118233  0.9083613  0.18620196          
  162  0.26051642  0.9037426  0.19312743          
  193  0.28977713  0.8890477  0.21563218          
  206  0.30213471  0.8832583  0.22597833          
  207  0.30301168  0.8828783  0.22664381          
  211  0.30689326  0.8811602  0.22979322          
  221  0.31702900  0.8756214  0.23770122          
  222  0.31800972  0.8752781  0.23854508          
  231  0.32663834  0.8711135  0.24529033          
  243  0.33797531  0.8663635  0.25430824          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was k = 3.
[1] "Mon Mar 12 04:43:21 2018"
Polynomial Kernel Regularized Least Squares 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  lambda        degree  RMSE          Rsquared     MAE           Selected
  1.120336e-05  3         34.6996255  0.047109550    29.0132906          
  1.275576e-04  1       2067.7218415  0.005535805  1774.1849950          
  1.469886e-04  2         10.8808359  0.035224525     9.0051016          
  2.480005e-04  1       1063.9206477  0.005595375   912.8415156          
  2.985795e-04  1        883.8320708  0.005620248   758.3108005          
  5.161942e-04  3          1.3276329  0.012402448     1.1338215          
  5.228628e-04  2          3.3393001  0.032297111     2.7628485          
  9.413733e-04  2          2.0076646  0.028503207     1.6773059          
  9.245343e-03  2          0.6296598  0.077933017     0.5300691          
  1.052723e-02  3          0.6303878  0.045446608     0.5272439          
  1.666692e-02  2          0.6014449  0.184354307     0.4963456  *       
  7.163562e-02  2          0.6046177  0.533459289     0.4901280          
  1.296567e-01  2          0.6110524  0.586620766     0.4963675          
  1.320862e-01  1          2.2581671  0.008676510     1.9670214          
  1.352729e-01  1          2.2117660  0.008565359     1.9271084          
  1.645822e-01  3          0.6295323  0.596496727     0.5173209          
  2.609674e-01  2          0.6172887  0.619653274     0.5030974          
  2.749924e-01  1          1.2691796  0.004714741     1.1288032          
  4.146292e-01  1          0.9923263  0.002739784     0.8872299          
  7.026243e-01  1          0.7957094  0.002096943     0.7013797          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 0.01666692 and degree = 2.
[1] "Mon Mar 12 04:45:08 2018"

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.12061088  0.10335579  0.41249554  0.09800722  0.01052572  0.01148049 
         V8          V9         V10 
-0.05480533  0.06689279  0.09540668 

 Quartiles of Marginal Effects:
 
           V2         V3        V4          V5           V6          V7
25% 0.1148663 0.09943499 0.3169874 -0.03055222 -0.012480574 -0.01243036
50% 0.1374753 0.11474477 0.4829420  0.12683691  0.008097293  0.01001236
75% 0.1474438 0.12644960 0.5848539  0.23999071  0.033798161  0.03542123
             V8         V9        V10
25% -0.08796542 0.03554753 0.08223696
50% -0.06624170 0.07037933 0.10978883
75% -0.02800453 0.09786233 0.12915505

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.149259809  0.134854866  0.385638337  0.268159942 -0.002146361 -0.001559867 
          V8           V9          V10 
-0.068211159  0.052779677  0.111026341 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5            V6            V7
25% 0.1464018 0.1322394 0.3749103 0.2597286 -0.0046601304 -0.0042495246
50% 0.1514561 0.1369600 0.3906799 0.2730791 -0.0003828157  0.0001927382
75% 0.1542714 0.1395948 0.4040724 0.2829030  0.0016874011  0.0024309680
             V8         V9       V10
25% -0.07077453 0.05055221 0.1081923
50% -0.06725406 0.05440576 0.1130185
75% -0.06441642 0.05683535 0.1157211

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.12843618  0.10882897  0.41214200  0.17176861  0.01151247  0.01268425 
         V8          V9         V10 
-0.05173043  0.06401764  0.10585422 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5         V6         V7          V8
25% 0.1278966 0.1083670 0.4102506 0.1701150 0.01113096 0.01230954 -0.05221219
50% 0.1289104 0.1093252 0.4142686 0.1729186 0.01179892 0.01297447 -0.05152102
75% 0.1294568 0.1097945 0.4162763 0.1746111 0.01210982 0.01328770 -0.05113392
            V9       V10
25% 0.06351279 0.1053506
50% 0.06452477 0.1063320
75% 0.06505859 0.1068305

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.160868116  0.143925198  0.408009305  0.201462083  0.003071863 -0.001809222 
          V8           V9          V10 
-0.049552470  0.058194364  0.091700023 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1559614 0.1308985 0.3412606 0.1546152 -0.005544417 -0.011480568
50% 0.1626401 0.1433528 0.4173210 0.2066985  0.002057039 -0.002906228
75% 0.1675685 0.1579866 0.4964462 0.2569448  0.010801309  0.007255203
             V8         V9        V10
25% -0.05582519 0.03189598 0.06781982
50% -0.05069023 0.05294914 0.09552870
75% -0.04152312 0.08076360 0.11759627

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.13117040  0.11129584  0.41798015  0.17734115  0.01000150  0.01113688 
         V8          V9         V10 
-0.05563282  0.06324795  0.10727274 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5         V6         V7          V8
25% 0.1301252 0.1104144 0.4143702 0.1741778 0.00934992 0.01047717 -0.05654646
50% 0.1320383 0.1122014 0.4218308 0.1795230 0.01051616 0.01168538 -0.05528385
75% 0.1330849 0.1130938 0.4257861 0.1826242 0.01113025 0.01227597 -0.05452373
            V9       V10
25% 0.06230072 0.1062784
50% 0.06417085 0.1081440
75% 0.06516264 0.1090811

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.153521037  0.139520359  0.364874318  0.292241478 -0.004469779 -0.003757416 
          V8           V9          V10 
-0.065927525  0.049874979  0.110358322 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6            V7
25% 0.1511290 0.1372234 0.3555679 0.2848611 -0.007130160 -0.0066174235
50% 0.1552992 0.1412118 0.3686060 0.2955402 -0.002865537 -0.0021701292
75% 0.1577734 0.1434535 0.3799538 0.3047844 -0.000678215  0.0002368571
             V8         V9       V10
25% -0.06831987 0.04801175 0.1078770
50% -0.06472729 0.05105100 0.1120443
75% -0.06226291 0.05325450 0.1142504

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.209354428  0.181539327  0.316877219  0.266940736 -0.005334587 -0.024462091 
          V8           V9          V10 
-0.002624142  0.042242536  0.040037480 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6          V7
25% 0.2077909 0.1744632 0.2588167 0.2388454 -0.008243198 -0.02944666
50% 0.2100576 0.1827694 0.3167043 0.2730763 -0.005771615 -0.02554319
75% 0.2118868 0.1895290 0.3798590 0.2980857 -0.002699591 -0.01987364
              V8         V9        V10
25% -0.007846045 0.03355920 0.02502295
50% -0.002802128 0.04132747 0.03799041
75%  0.002167614 0.04879081 0.05480099

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.09714270  0.08409343  0.32403058  0.13953341  0.02123930  0.02225256 
         V8          V9         V10 
-0.01588509  0.06315543  0.08502141 

 Quartiles of Marginal Effects:
 
            V2         V3        V4        V5         V6         V7          V8
25% 0.09685620 0.08385355 0.3230864 0.1385614 0.02101667 0.02203440 -0.01623665
50% 0.09743025 0.08438394 0.3252691 0.1402435 0.02144414 0.02246282 -0.01570117
75% 0.09773976 0.08465362 0.3263939 0.1412333 0.02162059 0.02263669 -0.01546972
            V9        V10
25% 0.06284707 0.08477004
50% 0.06347728 0.08531406
75% 0.06382612 0.08559029

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.147762574  0.131618013  0.419474725  0.163871734  0.004665459  0.003959919 
          V8           V9          V10 
-0.063157199  0.076069856  0.102708423 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6            V7
25% 0.1415725 0.1162746 0.3512798 0.1090184 -0.010315609 -0.0114497557
50% 0.1473974 0.1248656 0.4242978 0.1851365  0.001062618  0.0008845577
75% 0.1557580 0.1469707 0.5089745 0.2409975  0.018705306  0.0174198330
             V8         V9        V10
25% -0.07905522 0.03631159 0.08024814
50% -0.06021073 0.07444296 0.10853399
75% -0.04640610 0.10264697 0.12983042

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.27213051  0.16533461  0.24509737  0.34380285 -0.03703767 -0.01037831 
         V8          V9         V10 
 0.03417787  0.03154091  0.01088844 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7         V8
25% 0.2695311 0.1644807 0.2298685 0.3311090 -0.04020291 -0.01395354 0.03203357
50% 0.2730794 0.1653291 0.2452548 0.3486568 -0.03683468 -0.01011091 0.03498550
75% 0.2759204 0.1662830 0.2598543 0.3578572 -0.03362683 -0.00641298 0.03657648
            V9         V10
25% 0.02943286 0.007252285
50% 0.03165351 0.010903275
75% 0.03397258 0.015011544

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.254466463  0.150987490  0.247033421  0.382806005 -0.032383285 -0.009778783 
          V8           V9          V10 
 0.018207881  0.021958969  0.043313908 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6           V7         V8
25% 0.2530056 0.1497875 0.2416925 0.3787079 -0.03470372 -0.012112190 0.01596538
50% 0.2549780 0.1512744 0.2470031 0.3838149 -0.03233003 -0.009703473 0.01867131
75% 0.2562872 0.1525776 0.2522499 0.3873849 -0.02996718 -0.007215794 0.02082065
            V9        V10
25% 0.02091216 0.04140782
50% 0.02225518 0.04343421
75% 0.02319422 0.04578627

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.174497595  0.156160568  0.214040520  0.327050610 -0.010759073 -0.008444616 
          V8           V9          V10 
-0.072898800  0.084319928  0.092332631 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1625738 0.1262374 0.1633443 0.2675099 -0.021958652 -0.027063659
50% 0.1736380 0.1431577 0.2113442 0.3464969 -0.011476096 -0.009777493
75% 0.1851621 0.1842562 0.2656144 0.4059931 -0.001828147  0.009685079
             V8         V9        V10
25% -0.09450264 0.04650489 0.06711201
50% -0.07100319 0.08367850 0.10456897
75% -0.04126855 0.11158211 0.12255591

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.228339753  0.141617419  0.265227683  0.410379610 -0.024634668 -0.010877015 
          V8           V9          V10 
-0.001681353  0.011949246  0.077245394 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6           V7
25% 0.2275759 0.1407722 0.2640792 0.4085512 -0.02581141 -0.012069937
50% 0.2284521 0.1417150 0.2653764 0.4108357 -0.02468266 -0.010913274
75% 0.2293109 0.1426416 0.2664614 0.4125164 -0.02347152 -0.009700273
               V8         V9        V10
25% -0.0031549792 0.01076954 0.07619598
50% -0.0015765553 0.01204764 0.07727927
75% -0.0001406789 0.01322792 0.07840347

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.140558467  0.120488848  0.428107631  0.211080231  0.005880035  0.006631111 
          V8           V9          V10 
-0.066586233  0.058768837  0.111307789 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7          V8
25% 0.1398764 0.1198403 0.4256591 0.2090089 0.005446864 0.006182669 -0.06715155
50% 0.1411912 0.1210525 0.4304140 0.2124437 0.006252699 0.007010440 -0.06632994
75% 0.1418246 0.1217130 0.4329729 0.2146107 0.006689660 0.007431593 -0.06575356
            V9       V10
25% 0.05812044 0.1106851
50% 0.05936123 0.1118851
75% 0.05995761 0.1125328

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.198255314  0.176053693  0.332868156  0.258998449 -0.004367547 -0.019968091 
          V8           V9          V10 
-0.011822283  0.042870983  0.051670856 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6          V7
25% 0.1961545 0.1677677 0.2700935 0.2357515 -0.007883025 -0.02529321
50% 0.1990094 0.1774161 0.3332573 0.2657557 -0.005076184 -0.02093037
75% 0.2012647 0.1854137 0.4007517 0.2864331 -0.001024475 -0.01503182
              V8         V9        V10
25% -0.016980940 0.03225900 0.03455516
50% -0.011889752 0.04164658 0.04956038
75% -0.006742733 0.05145529 0.06817508

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.1447808336  0.1302919538  0.3998689003  0.2417504717 -0.0006326385 
           V7            V8            V9           V10 
 0.0001748967 -0.0675614650  0.0561972826  0.1103897857 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1404947 0.1264134 0.3828286 0.2289484 -0.003562855 -0.003102254
50% 0.1478152 0.1332810 0.4069884 0.2496239  0.001494878  0.002413696
75% 0.1522916 0.1370947 0.4272932 0.2631803  0.004365661  0.005413129
             V8         V9       V10
25% -0.07159198 0.05299202 0.1064430
50% -0.06694727 0.05894809 0.1132222
75% -0.06283976 0.06230862 0.1172550

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.170568195  0.151724587  0.311557200  0.324398090 -0.013908199 -0.009158561 
          V8           V9          V10 
-0.042314600  0.043950900  0.098235879 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1677775 0.1492386 0.2971541 0.3112845 -0.018889839 -0.014794052
50% 0.1723903 0.1530798 0.3111799 0.3257076 -0.013181896 -0.008589880
75% 0.1753064 0.1554977 0.3281833 0.3410239 -0.007932724 -0.002680436
             V8         V9        V10
25% -0.04611726 0.04064889 0.09599752
50% -0.04084231 0.04529157 0.09986676
75% -0.03815102 0.04827780 0.10238161

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.20354909  0.17274156  0.20635968  0.34153629 -0.00795109 -0.03130362 
         V8          V9         V10 
-0.03271323  0.06561958  0.06671257 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6          V7
25% 0.1956574 0.1512855 0.1632910 0.3021018 -0.011403579 -0.04043583
50% 0.2030863 0.1711187 0.2121969 0.3427895 -0.008508127 -0.03239348
75% 0.2119979 0.1920287 0.2576765 0.3761314 -0.005113962 -0.02324567
             V8         V9        V10
25% -0.04542402 0.04881988 0.03847880
50% -0.03672911 0.06297636 0.06616793
75% -0.02197845 0.07995246 0.09506947

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.145219903  0.126614096  0.417945950  0.238250488  0.002783109  0.003246683 
          V8           V9          V10 
-0.070471984  0.055105412  0.112165286 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7          V8
25% 0.1439200 0.1254201 0.4132204 0.2343490 0.001900456 0.002283406 -0.07159682
50% 0.1463257 0.1276253 0.4215304 0.2404641 0.003444578 0.003888961 -0.06993751
75% 0.1475293 0.1288554 0.4265333 0.2446631 0.004350056 0.004833995 -0.06886647
            V9       V10
25% 0.05395951 0.1109992
50% 0.05609682 0.1131817
75% 0.05713451 0.1143535

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.133729468  0.150511113  0.256278962  0.381960937 -0.050398063  0.051995186 
          V8           V9          V10 
-0.071189725  0.004998661  0.134949837 

 Quartiles of Marginal Effects:
 
            V2         V3         V4        V5           V6          V7
25% 0.08581482 0.08980847 0.06032436 0.1234260 -0.103179103 -0.01485116
50% 0.12746299 0.14168378 0.25061930 0.3446593 -0.041007377  0.04817056
75% 0.17716913 0.18732087 0.45790288 0.5878340  0.007694541  0.11800017
             V8          V9        V10
25% -0.16429800 -0.09544282 0.06882486
50% -0.02645427  0.01153195 0.13167044
75%  0.04275279  0.10264685 0.20057129

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.11637807  0.10100453  0.41927196  0.11325511  0.01007901  0.01084221 
         V8          V9         V10 
-0.05804050  0.06621190  0.09087361 

 Quartiles of Marginal Effects:
 
           V2         V3        V4          V5           V6           V7
25% 0.1064511 0.09491748 0.3419076 -0.01264365 -0.009605959 -0.008632951
50% 0.1342998 0.11183275 0.4801936  0.13073969  0.009549499  0.012090441
75% 0.1479495 0.12734242 0.5979732  0.23577423  0.034746304  0.036370243
             V8         V9        V10
25% -0.09132984 0.03899243 0.07711605
50% -0.06862405 0.06952161 0.10807294
75% -0.03202550 0.09879542 0.12440888

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.1491502260  0.1349695298  0.3922744999  0.2760384372 -0.0011865265 
           V7            V8            V9           V10 
-0.0005977961 -0.0733442211  0.0513192956  0.1093274034 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5            V6           V7
25% 0.1456631 0.1316375 0.3825088 0.2667912 -0.0038834530 -0.003457311
50% 0.1512683 0.1368877 0.3975653 0.2816452  0.0007192936  0.001243903
75% 0.1543948 0.1398825 0.4101663 0.2911353  0.0028794725  0.003603216
             V8         V9       V10
25% -0.07613422 0.04871306 0.1064582
50% -0.07198609 0.05295702 0.1112702
75% -0.06926203 0.05516525 0.1140656

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.12784002  0.10957062  0.41319063  0.17989248  0.01203171  0.01305985 
         V8          V9         V10 
-0.05544088  0.06421628  0.10439695 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5         V6         V7          V8
25% 0.1271848 0.1089420 0.4114543 0.1781596 0.01167242 0.01272065 -0.05591781
50% 0.1282799 0.1100093 0.4152104 0.1811422 0.01232042 0.01335274 -0.05522179
75% 0.1288667 0.1105630 0.4172552 0.1827311 0.01266996 0.01371403 -0.05477479
            V9       V10
25% 0.06361121 0.1038185
50% 0.06474487 0.1048471
75% 0.06526928 0.1053803

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 1.604066e-01  1.439340e-01  4.069597e-01  2.080540e-01  4.568740e-03 
           V7            V8            V9           V10 
-6.358487e-05 -5.784911e-02  5.789555e-02  9.144482e-02 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6            V7
25% 0.1556211 0.1316033 0.3417695 0.1550152 -0.003552121 -0.0087322180
50% 0.1612217 0.1426945 0.4120833 0.2132539  0.003882539 -0.0008366817
75% 0.1668759 0.1576817 0.4965857 0.2697252  0.011880343  0.0078465650
             V8         V9        V10
25% -0.06399980 0.02879053 0.06653064
50% -0.05966347 0.05171521 0.09196082
75% -0.05212637 0.08219989 0.11750458

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.13059265  0.11203101  0.41961247  0.18590595  0.01052270  0.01151958 
         V8          V9         V10 
-0.05965713  0.06327823  0.10576219 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5         V6         V7          V8
25% 0.1293994 0.1108787 0.4163421 0.1826228 0.00982127 0.01084151 -0.06052933
50% 0.1314540 0.1128452 0.4233111 0.1881960 0.01106609 0.01203184 -0.05928299
75% 0.1325169 0.1138700 0.4271864 0.1912079 0.01172801 0.01274470 -0.05840089
            V9       V10
25% 0.06211062 0.1046509
50% 0.06424973 0.1066151
75% 0.06520289 0.1075734

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.153481125  0.139664978  0.371337875  0.299437586 -0.003490289 -0.002730019 
          V8           V9          V10 
-0.070792350  0.048352473  0.108704401 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1507133 0.1368233 0.3635839 0.2906961 -0.006238061 -0.005719212
50% 0.1552667 0.1411853 0.3754728 0.3035914 -0.001746464 -0.001061322
75% 0.1578767 0.1437802 0.3859613 0.3123459  0.000440349  0.001362311
             V8         V9       V10
25% -0.07350906 0.04639673 0.1062781
50% -0.06929634 0.04951721 0.1102938
75% -0.06694268 0.05148136 0.1127606

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.209372151  0.178550117  0.312967768  0.271698636 -0.001939033 -0.024093068 
          V8           V9          V10 
-0.012232906  0.046285854  0.040498165 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5            V6          V7
25% 0.2072743 0.1722416 0.2602069 0.2480104 -0.0050266780 -0.02935743
50% 0.2099880 0.1801279 0.3116219 0.2781150 -0.0022297853 -0.02520065
75% 0.2125207 0.1863289 0.3738074 0.3000569  0.0007332005 -0.01973147
              V8         V9        V10
25% -0.017512450 0.03686516 0.02486141
50% -0.012444009 0.04668163 0.03778384
75% -0.006455935 0.05496893 0.05499202

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.09647102  0.08436761  0.32315750  0.14392919  0.02194013  0.02280398 
         V8          V9         V10 
-0.01704927  0.06390535  0.08387391 

 Quartiles of Marginal Effects:
 
            V2         V3        V4        V5         V6         V7          V8
25% 0.09613319 0.08404332 0.3220409 0.1429375 0.02168996 0.02255628 -0.01740052
50% 0.09674433 0.08464382 0.3243729 0.1446545 0.02215317 0.02301744 -0.01685230
75% 0.09706213 0.08494536 0.3254843 0.1456503 0.02233928 0.02319980 -0.01661023
            V9        V10
25% 0.06351092 0.08353632
50% 0.06424941 0.08415286
75% 0.06458132 0.08445080

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.147456753  0.131251626  0.432797688  0.164210199  0.006446857  0.006631667 
          V8           V9          V10 
-0.067298911  0.072286653  0.099615646 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1409551 0.1158184 0.3682810 0.1127952 -0.007624202 -0.007801705
50% 0.1482009 0.1260374 0.4320803 0.1876598  0.003647591  0.003783922
75% 0.1569368 0.1446638 0.5232884 0.2439959  0.019323738  0.018977451
             V8         V9        V10
25% -0.08485759 0.02718387 0.07868787
50% -0.06518822 0.07211846 0.10677573
75% -0.05190205 0.10336782 0.12595666

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.27315877  0.16350772  0.23814843  0.34693790 -0.03287740 -0.01237936 
         V8          V9         V10 
 0.02729116  0.03647730  0.01017918 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6           V7         V8
25% 0.2708869 0.1626171 0.2236063 0.3380561 -0.03621858 -0.015956610 0.02552132
50% 0.2742049 0.1635142 0.2365764 0.3498041 -0.03304322 -0.012506300 0.02799244
75% 0.2762045 0.1644931 0.2531666 0.3587468 -0.02926125 -0.008248543 0.02948014
            V9         V10
25% 0.03486771 0.006314639
50% 0.03644008 0.010309947
75% 0.03839385 0.014355896

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.252879528  0.153073092  0.245282489  0.383428354 -0.032600744 -0.009090021 
          V8           V9          V10 
 0.014462294  0.024361966  0.042433048 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6           V7         V8
25% 0.2513759 0.1518488 0.2401315 0.3792616 -0.03495724 -0.011503349 0.01212249
50% 0.2534013 0.1534765 0.2448757 0.3843208 -0.03259660 -0.009006624 0.01485543
75% 0.2547442 0.1546577 0.2501578 0.3882663 -0.03019959 -0.006556816 0.01713796
            V9        V10
25% 0.02311694 0.04029584
50% 0.02470098 0.04257836
75% 0.02582142 0.04491757

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.17677157  0.16871917  0.22504280  0.30958425 -0.01371939 -0.01000642 
         V8          V9         V10 
-0.06002352  0.07277079  0.08531060 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1695361 0.1260011 0.1714475 0.2406752 -0.028337164 -0.022542684
50% 0.1784482 0.1584619 0.2112161 0.3301902 -0.015215233 -0.013053272
75% 0.1872329 0.2026843 0.2695959 0.3993525 -0.001572363  0.001902329
             V8         V9        V10
25% -0.08985592 0.01704433 0.06109117
50% -0.04531632 0.07576020 0.09256462
75% -0.02500954 0.11793216 0.11829164

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.224722406  0.146655353  0.267104840  0.407928953 -0.024655932 -0.010438026 
          V8           V9          V10 
-0.005822329  0.014301234  0.075246209 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6           V7
25% 0.2240072 0.1458170 0.2659566 0.4062311 -0.02583780 -0.011634133
50% 0.2248494 0.1467839 0.2672394 0.4084666 -0.02469242 -0.010469660
75% 0.2256132 0.1475563 0.2684044 0.4098413 -0.02351395 -0.009279197
              V8         V9        V10
25% -0.007347076 0.01306924 0.07420251
50% -0.005821424 0.01440794 0.07528526
75% -0.004270603 0.01550036 0.07632036

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.140085601  0.121520965  0.430112838  0.220747287  0.006427086  0.007044064 
          V8           V9          V10 
-0.071501091  0.058328451  0.109753477 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7          V8
25% 0.1392217 0.1206699 0.4279431 0.2186107 0.005968385 0.006544035 -0.07215069
50% 0.1406891 0.1220623 0.4322511 0.2221372 0.006816745 0.007413324 -0.07120009
75% 0.1413870 0.1227428 0.4350295 0.2242671 0.007265624 0.007895215 -0.07061324
            V9       V10
25% 0.05756364 0.1089969
50% 0.05893150 0.1102967
75% 0.05953000 0.1109583

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.197910716  0.173147064  0.329511860  0.265468033 -0.001390999 -0.019244358 
          V8           V9          V10 
-0.021036635  0.045309408  0.052950280 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6          V7
25% 0.1953639 0.1656918 0.2731288 0.2457067 -0.004699586 -0.02472174
50% 0.1985293 0.1747317 0.3267272 0.2717401 -0.001882652 -0.02030387
75% 0.2017043 0.1821413 0.3947203 0.2911581  0.001775568 -0.01432011
             V8         V9        V10
25% -0.02611783 0.03396394 0.03539775
50% -0.02100378 0.04551489 0.05027439
75% -0.01508006 0.05569903 0.06924830

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.1446818287  0.1301420865  0.4074683192  0.2496165550  0.0002726375 
           V7            V8            V9           V10 
 0.0010670410 -0.0727943283  0.0549666980  0.1086495568 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1394819 0.1253711 0.3924958 0.2361751 -0.003147419 -0.002503473
50% 0.1477655 0.1329293 0.4164711 0.2573966  0.002621564  0.003425807
75% 0.1523378 0.1371595 0.4338199 0.2709732  0.005688016  0.006686645
             V8         V9       V10
25% -0.07708187 0.05162832 0.1041310
50% -0.07174903 0.05723670 0.1114196
75% -0.06764537 0.06077652 0.1155532

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.171482519  0.151063576  0.316189897  0.328431977 -0.013203708 -0.008251009 
          V8           V9          V10 
-0.045590025  0.043395323  0.096954400 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1681588 0.1486013 0.3022520 0.3150630 -0.018228327 -0.013729281
50% 0.1730222 0.1522851 0.3152098 0.3307145 -0.012525122 -0.007648128
75% 0.1762237 0.1549017 0.3312412 0.3473502 -0.007205749 -0.001715174
             V8         V9        V10
25% -0.04948264 0.04073247 0.09427052
50% -0.04389182 0.04402716 0.09861387
75% -0.04153905 0.04718819 0.10136044

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.20326615  0.18094063  0.20659248  0.34413399 -0.01027481 -0.02671082 
         V8          V9         V10 
-0.03550260  0.05966023  0.06102151 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6          V7
25% 0.1949588 0.1596003 0.1647522 0.3012191 -0.013733371 -0.03211645
50% 0.2032711 0.1826410 0.2069207 0.3467353 -0.010847751 -0.02673214
75% 0.2113450 0.2028552 0.2488661 0.3825828 -0.007588962 -0.02178330
             V8         V9        V10
25% -0.04667990 0.04270574 0.03091490
50% -0.03755876 0.05674406 0.05744519
75% -0.02635070 0.07529424 0.09039338

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.144818758  0.127537941  0.421211011  0.247880983  0.003473037  0.003832116 
          V8           V9          V10 
-0.075702855  0.054180442  0.110537405 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7          V8
25% 0.1433414 0.1261908 0.4174821 0.2436697 0.002441481 0.002702615 -0.07695287
50% 0.1458318 0.1284813 0.4245988 0.2503224 0.004214224 0.004608563 -0.07504289
75% 0.1471719 0.1297895 0.4296467 0.2541801 0.005139420 0.005524063 -0.07400518
            V9       V10
25% 0.05292924 0.1091656
50% 0.05522067 0.1114711
75% 0.05622903 0.1127324

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.16752022  0.16024892  0.25270677  0.30112528 -0.04884022  0.04386743 
         V8          V9         V10 
-0.07478974  0.08559894  0.07504700 

 Quartiles of Marginal Effects:
 
           V2        V3         V4         V5          V6          V7
25% 0.1176008 0.1210901 0.07400773 0.06340445 -0.11902395 -0.01827967
50% 0.1526990 0.1536536 0.27762750 0.26816927 -0.03571799  0.01810905
75% 0.2062019 0.1935627 0.44821487 0.45792308  0.02940819  0.07420755
             V8           V9        V10
25% -0.17681332 -0.002927131 0.03889147
50% -0.07368421  0.092168045 0.08938620
75%  0.04482192  0.175549357 0.13105354

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.11815833  0.10186361  0.40728882  0.11667707  0.01023775  0.01147792 
         V8          V9         V10 
-0.05865138  0.06698932  0.09184557 

 Quartiles of Marginal Effects:
 
           V2         V3        V4          V5          V6          V7
25% 0.1151310 0.09746884 0.3210191 -0.04229271 -0.01185429 -0.01169995
50% 0.1348094 0.11185754 0.4778627  0.14162065  0.01013449  0.01133789
75% 0.1476223 0.12498533 0.5857761  0.27090368  0.03623321  0.03770153
             V8         V9        V10
25% -0.09375013 0.03979996 0.08096722
50% -0.07048218 0.07197422 0.10578852
75% -0.03049946 0.10336509 0.12628448

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.1492895184  0.1352812200  0.3867924788  0.2803808742 -0.0008033055 
           V7            V8            V9           V10 
-0.0003262667 -0.0743448435  0.0534126008  0.1087295777 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1455997 0.1321125 0.3763929 0.2701026 -0.003086387 -0.002985049
50% 0.1517395 0.1374230 0.3920902 0.2856683  0.001154622  0.001647960
75% 0.1544691 0.1402245 0.4049211 0.2956661  0.003140145  0.003758811
             V8         V9       V10
25% -0.07740078 0.05091100 0.1055891
50% -0.07336114 0.05518059 0.1110382
75% -0.07043241 0.05765112 0.1134401

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.12739656  0.10865439  0.41406159  0.18576498  0.01246371  0.01344018 
         V8          V9         V10 
-0.05720477  0.06681900  0.10393430 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5         V6         V7          V8
25% 0.1266917 0.1080470 0.4122516 0.1841068 0.01210167 0.01307642 -0.05765997
50% 0.1279463 0.1091571 0.4161134 0.1872363 0.01276415 0.01374976 -0.05699621
75% 0.1284542 0.1096664 0.4181981 0.1886527 0.01309590 0.01407237 -0.05652256
            V9       V10
25% 0.06625063 0.1032929
50% 0.06739364 0.1044522
75% 0.06792098 0.1049285

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.161922000  0.144524698  0.412038023  0.203167889  0.003563640 -0.001417989 
          V8           V9          V10 
-0.054652430  0.061812415  0.088529306 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1570285 0.1311765 0.3462015 0.1544410 -0.004581837 -0.010506529
50% 0.1631772 0.1440808 0.4173347 0.2051118  0.001958636 -0.002170611
75% 0.1682584 0.1590707 0.4956531 0.2595701  0.011360985  0.006975908
             V8         V9        V10
25% -0.06070662 0.03286780 0.06291932
50% -0.05467658 0.05538400 0.08969563
75% -0.04760020 0.08588505 0.11554937

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.13016443  0.11118003  0.41977336  0.19165744  0.01099230  0.01192901 
         V8          V9         V10 
-0.06143011  0.06590151  0.10531747 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5         V6         V7          V8
25% 0.1288876 0.1100245 0.4163518 0.1884950 0.01033708 0.01131458 -0.06227616
50% 0.1312009 0.1120956 0.4234439 0.1942982 0.01154779 0.01249078 -0.06108452
75% 0.1321296 0.1130565 0.4273996 0.1970550 0.01217140 0.01312526 -0.06017043
            V9       V10
25% 0.06483819 0.1041321
50% 0.06694488 0.1062735
75% 0.06793950 0.1071649

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.153786643  0.140203266  0.366543538  0.302859242 -0.003161286 -0.002526617 
          V8           V9          V10 
-0.071492329  0.050026303  0.107808132 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5            V6            V7
25% 0.1508129 0.1374925 0.3582362 0.2937915 -0.0056773960 -0.0052874773
50% 0.1558246 0.1420312 0.3707165 0.3065249 -0.0012219598 -0.0006428772
75% 0.1581400 0.1442989 0.3812805 0.3159210  0.0007327662  0.0014423813
             V8         V9       V10
25% -0.07382003 0.04812345 0.1052279
50% -0.07020867 0.05126303 0.1096132
75% -0.06774351 0.05356322 0.1118099

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.213895553  0.182575316  0.317992866  0.261088260 -0.003809836 -0.025632546 
          V8           V9          V10 
-0.004004080  0.047174447  0.033009692 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6          V7
25% 0.2122065 0.1764884 0.2645938 0.2312353 -0.006771026 -0.03050928
50% 0.2146451 0.1839486 0.3177323 0.2672154 -0.004019967 -0.02646507
75% 0.2166282 0.1898618 0.3798858 0.2930719 -0.001323457 -0.02137422
               V8         V9        V10
25% -0.0086698564 0.03876890 0.01675130
50% -0.0036153526 0.04607977 0.03025259
75%  0.0009896235 0.05436111 0.04731245

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.09631900  0.08370777  0.32565761  0.14927310  0.02235245  0.02321988 
         V8          V9         V10 
-0.01795267  0.06638040  0.08372062 

 Quartiles of Marginal Effects:
 
            V2         V3        V4        V5         V6         V7          V8
25% 0.09594408 0.08340244 0.3245246 0.1482573 0.02213237 0.02300037 -0.01831115
50% 0.09662603 0.08400053 0.3268999 0.1501486 0.02258450 0.02345031 -0.01774873
75% 0.09693206 0.08429082 0.3280458 0.1510146 0.02275629 0.02362233 -0.01749571
            V9        V10
25% 0.06603799 0.08337986
50% 0.06675519 0.08401843
75% 0.06708809 0.08431912

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.148453090  0.131062229  0.415360505  0.177296897  0.005119857  0.006574148 
          V8           V9          V10 
-0.072283006  0.079772520  0.098939534 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1408385 0.1168456 0.3524513 0.1163383 -0.009185145 -0.008361902
50% 0.1489292 0.1263844 0.4151098 0.2024070  0.001384814  0.003739488
75% 0.1584643 0.1448508 0.4994792 0.2635450  0.019723091  0.020830106
             V8         V9        V10
25% -0.08759172 0.03427062 0.07717026
50% -0.06944087 0.07703719 0.10617734
75% -0.05594652 0.10759931 0.12585618

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.272708223  0.168970727  0.244937940  0.337362941 -0.034461051 -0.011357755 
          V8           V9          V10 
 0.030406026  0.035432466  0.005515032 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7         V8
25% 0.2705345 0.1681417 0.2307768 0.3258287 -0.03783754 -0.01517041 0.02843237
50% 0.2737795 0.1690611 0.2449313 0.3411397 -0.03454192 -0.01123500 0.03111319
75% 0.2761022 0.1700983 0.2580448 0.3509581 -0.03072032 -0.00703758 0.03274801
            V9         V10
25% 0.03374185 0.001873302
50% 0.03557486 0.005642165
75% 0.03763532 0.009537613

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.255179303  0.157532011  0.257925925  0.370972214 -0.032904630 -0.009855677 
          V8           V9          V10 
 0.023624586  0.018462386  0.037272973 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6           V7         V8
25% 0.2538944 0.1564914 0.2529122 0.3668148 -0.03516877 -0.012150535 0.02130100
50% 0.2557365 0.1579412 0.2577732 0.3715772 -0.03276165 -0.009694553 0.02413041
75% 0.2569944 0.1589429 0.2626877 0.3762637 -0.03054278 -0.007360055 0.02617553
            V9        V10
25% 0.01728208 0.03526559
50% 0.01880653 0.03751634
75% 0.01989356 0.03958772

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.186603147  0.154317546  0.181456375  0.346598569 -0.011347638 -0.003391268 
          V8           V9          V10 
-0.089670835  0.098961326  0.078885832 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1707414 0.1267810 0.1165361 0.2590625 -0.020153085 -0.022315289
50% 0.1881507 0.1457970 0.1759275 0.3597441 -0.010300293 -0.003873526
75% 0.2036967 0.1794857 0.2299574 0.4653649 -0.003100641  0.014076263
             V8         V9        V10
25% -0.12317290 0.05084697 0.05136778
50% -0.08689177 0.10477922 0.08487044
75% -0.05558929 0.13846144 0.10933474

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.228581828  0.149500547  0.281128347  0.397304433 -0.024440937 -0.011156005 
          V8           V9          V10 
 0.004090964  0.006641098  0.069936858 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7          V8
25% 0.2279219 0.1487008 0.2801312 0.3957441 -0.02564516 -0.01238595 0.002530756
50% 0.2287186 0.1496067 0.2812569 0.3978828 -0.02438902 -0.01109766 0.004244959
75% 0.2294410 0.1503688 0.2823897 0.3992544 -0.02333167 -0.01002441 0.005640617
             V9        V10
25% 0.005436562 0.06892162
50% 0.006760152 0.06999820
75% 0.007817075 0.07098751

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.139649679  0.120773744  0.429327520  0.226317950  0.007000184  0.007481619 
          V8           V9          V10 
-0.073540839  0.060570953  0.109088484 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7          V8
25% 0.1387269 0.1199808 0.4269433 0.2241128 0.006531615 0.007001236 -0.07416584
50% 0.1403107 0.1213782 0.4316925 0.2279755 0.007402613 0.007882558 -0.07325234
75% 0.1409660 0.1220190 0.4342260 0.2299141 0.007847296 0.008326152 -0.07263922
            V9       V10
25% 0.05987178 0.1082612
50% 0.06121415 0.1097404
75% 0.06181185 0.1102862

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.201846398  0.177330916  0.334945905  0.254122687 -0.003212707 -0.020938738 
          V8           V9          V10 
-0.013191341  0.047109881  0.045510493 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5            V6          V7
25% 0.1999274 0.1701486 0.2761337 0.2286792 -0.0067034254 -0.02633063
50% 0.2026125 0.1790696 0.3356154 0.2624955 -0.0036982855 -0.02184196
75% 0.2049948 0.1860690 0.4044523 0.2839354  0.0001186325 -0.01643981
              V8         V9        V10
25% -0.017846303 0.03674036 0.02721999
50% -0.012626097 0.04576320 0.04230056
75% -0.007687785 0.05664962 0.06175964

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.1448275362  0.1304039770  0.4006746631  0.2547839243  0.0005929442 
           V7            V8            V9           V10 
 0.0013458401 -0.0736296078  0.0575159738  0.1084184683 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1397627 0.1258512 0.3841097 0.2393373 -0.002259128 -0.001813996
50% 0.1483008 0.1337897 0.4085789 0.2631802  0.002940889  0.003747683
75% 0.1523690 0.1374027 0.4277177 0.2768477  0.005840095  0.006777804
             V8         V9       V10
25% -0.07819299 0.05408472 0.1037174
50% -0.07298828 0.06055031 0.1115182
75% -0.06830585 0.06378633 0.1150530

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.17240120  0.15311839  0.31382561  0.32823481 -0.01363079 -0.00849784 
         V8          V9         V10 
-0.04382729  0.04400090  0.09484445 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6           V7
25% 0.1691848 0.1506894 0.3004893 0.3136597 -0.019098312 -0.014241591
50% 0.1742967 0.1546755 0.3126757 0.3297861 -0.012739910 -0.007614810
75% 0.1772134 0.1570542 0.3289314 0.3470272 -0.007792203 -0.002195448
             V8         V9        V10
25% -0.04753585 0.04066428 0.09275703
50% -0.04235608 0.04528343 0.09660745
75% -0.03973953 0.04865677 0.09893553

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.217288217  0.163593864  0.213013539  0.347991055 -0.007585267 -0.025172999 
          V8           V9          V10 
-0.037323596  0.067772793  0.056986129 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6          V7
25% 0.2100079 0.1446992 0.1707285 0.3100748 -0.010889184 -0.03094096
50% 0.2172749 0.1631072 0.2151576 0.3523674 -0.007980593 -0.02510354
75% 0.2265543 0.1823129 0.2566194 0.3834837 -0.004952851 -0.02034216
             V8         V9        V10
25% -0.04500484 0.04764142 0.02687533
50% -0.03796563 0.06544176 0.05460835
75% -0.02936989 0.08646203 0.08719150

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.144524661  0.127126642  0.418867203  0.252918605  0.004088195  0.004271429 
          V8           V9          V10 
-0.077604717  0.056274360  0.109830611 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7          V8
25% 0.1428860 0.1256561 0.4145334 0.2486356 0.003197323 0.003343042 -0.07886136
50% 0.1456737 0.1283077 0.4225587 0.2555764 0.004847798 0.005031085 -0.07703002
75% 0.1469096 0.1293807 0.4274730 0.2595067 0.005727762 0.005942740 -0.07581188
            V9       V10
25% 0.05500861 0.1084052
50% 0.05730571 0.1109529
75% 0.05837767 0.1120331

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.176057203  0.184740144  0.223797073  0.262347991 -0.086834308  0.009520400 
          V8           V9          V10 
 0.009907687  0.081051845  0.088131237 

 Quartiles of Marginal Effects:
 
           V2        V3          V4         V5           V6           V7
25% 0.1030090 0.1319506 -0.03768987 0.01881607 -0.166709203 -0.053239495
50% 0.1608087 0.1742795  0.24835792 0.25190205 -0.072174428 -0.005137194
75% 0.2470304 0.2273982  0.48009324 0.51582239 -0.003005441  0.060388862
              V8         V9        V10
25% -0.071039757 0.01244108 0.04570735
50%  0.008540743 0.07777711 0.09882292
75%  0.103185028 0.14747478 0.14687625

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.21378881  0.17221327  0.17981384  0.36793597 -0.01206791 -0.02767151 
         V8          V9         V10 
-0.03599928  0.06526567  0.06065384 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6          V7
25% 0.2062497 0.1487803 0.1381651 0.3275271 -0.015334386 -0.03438095
50% 0.2132107 0.1719837 0.1842000 0.3691332 -0.012527023 -0.02799832
75% 0.2218668 0.1944125 0.2239591 0.4048853 -0.009513892 -0.02131443
             V8         V9        V10
25% -0.04854015 0.04858603 0.03050035
50% -0.03833446 0.06277308 0.05890094
75% -0.02646709 0.08036491 0.09145866
Radial Basis Function Kernel Regularized Least Squares 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  lambda        sigma        RMSE         Rsquared   MAE          Selected
  1.120336e-05     1.624699  0.019627438  0.9990328  0.009426013          
  1.275576e-04  1356.459969  0.004946071  0.9999428  0.003830811          
  1.469886e-04    67.578163  0.003237228  0.9999754  0.002579177  *       
  2.480005e-04  9860.345303  0.010028199  0.9997612  0.007601497          
  2.985795e-04  2831.501178  0.006939349  0.9998869  0.005282037          
  5.161942e-04     8.867153  0.004588116  0.9999515  0.003100551          
  5.228628e-04   174.357191  0.004188200  0.9999593  0.003333364          
  9.413733e-04   126.764404  0.004497782  0.9999532  0.003516049          
  9.245343e-03    26.826016  0.007171201  0.9998826  0.004653850          
  1.052723e-02     6.466706  0.010579550  0.9997306  0.005624450          
  1.666692e-02   295.461771  0.013735792  0.9995555  0.009951670          
  7.163562e-02   346.104974  0.021276554  0.9989619  0.015038281          
  1.296567e-01   279.657680  0.025331848  0.9985614  0.017714507          
  1.320862e-01  1144.302258  0.040161564  0.9973744  0.030403980          
  1.352729e-01   646.375700  0.030500852  0.9982347  0.021776405          
  1.645822e-01     1.018535  0.069272569  0.9890147  0.030928181          
  2.609674e-01   171.337362  0.031898171  0.9977336  0.021996813          
  2.749924e-01  1316.739074  0.070594567  0.9938316  0.057012606          
  4.146292e-01   713.419935  0.062172635  0.9949673  0.049333866          
  7.026243e-01  1868.041147  0.166779444  0.9742990  0.137517386          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 0.0001469886 and sigma
 = 67.57816.
[1] "Mon Mar 12 04:52:12 2018"
Least Angle Regression 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  fraction     RMSE         Rsquared   MAE          Selected
  0.009869629  0.626777153  0.9714814  0.517035884          
  0.221141259  0.334893595  0.9714814  0.273902103          
  0.233456740  0.318367758  0.9720164  0.260229942          
  0.278890499  0.257883479  0.9823447  0.210764057          
  0.295011987  0.236561226  0.9851894  0.193453830          
  0.342562624  0.174147520  0.9917814  0.142639758          
  0.343677541  0.172693576  0.9919301  0.141450087          
  0.394752379  0.106635466  0.9964970  0.087447067          
  0.593184605  0.008628346  0.9998214  0.006717704          
  0.604462799  0.008513262  0.9998261  0.006629246          
  0.644371057  0.008210142  0.9998379  0.006376009          
  0.771025799  0.008051575  0.9998441  0.006163867          
  0.822559009  0.008015964  0.9998456  0.006130370          
  0.824171485  0.008015025  0.9998456  0.006129446          
  0.826242191  0.008013835  0.9998457  0.006128260          
  0.843276577  0.008004717  0.9998460  0.006118699          
  0.883317267  0.007988005  0.9998467  0.006099245          
  0.887864152  0.007986527  0.9998468  0.006097302          
  0.923531971  0.007977916  0.9998472  0.006084324          
  0.969344636  0.007974625  0.9998473  0.006076343  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was fraction = 0.9693446.
[1] "Mon Mar 12 04:52:19 2018"
Least Angle Regression 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  step  RMSE         Rsquared   MAE          Selected
  1     0.640608583        NaN  0.528598190          
  2     0.311169318  0.9714814  0.254200119          
  3     0.200848633  0.9886743  0.164494719          
  4     0.028531216  0.9983461  0.022178692          
  6     0.011896763  0.9996628  0.009200417          
  7     0.010670988  0.9997218  0.008311305          
  8     0.010034102  0.9997525  0.007861980          
  9     0.009325201  0.9997906  0.007321951  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was step = 9.
[1] "Mon Mar 12 04:52:26 2018"
The lasso 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  fraction     RMSE         Rsquared   MAE          Selected
  0.009869629  0.626777153  0.9714814  0.517035884          
  0.221141259  0.334893595  0.9714814  0.273902103          
  0.233456740  0.318367758  0.9720164  0.260229942          
  0.278890499  0.257883479  0.9823447  0.210764057          
  0.295011987  0.236561226  0.9851894  0.193453830          
  0.342562624  0.174147520  0.9917814  0.142639758          
  0.343677541  0.172693576  0.9919301  0.141450087          
  0.394752379  0.106635466  0.9964970  0.087447067          
  0.593184605  0.008628346  0.9998214  0.006717704          
  0.604462799  0.008513262  0.9998261  0.006629246          
  0.644371057  0.008210142  0.9998379  0.006376009          
  0.771025799  0.008051575  0.9998441  0.006163867          
  0.822559009  0.008015964  0.9998456  0.006130370          
  0.824171485  0.008015025  0.9998456  0.006129446          
  0.826242191  0.008013835  0.9998457  0.006128260          
  0.843276577  0.008004717  0.9998460  0.006118699          
  0.883317267  0.007988005  0.9998467  0.006099245          
  0.887864152  0.007986527  0.9998468  0.006097302          
  0.923531971  0.007977916  0.9998472  0.006084324          
  0.969344636  0.007974625  0.9998473  0.006076343  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was fraction = 0.9693446.
[1] "Mon Mar 12 04:52:32 2018"
Linear Regression with Backwards Selection 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  nvmax  RMSE         Rsquared   MAE          Selected
  2      0.019278294  0.9991054  0.014956466          
  3      0.013626412  0.9995538  0.010895113          
  4      0.011680864  0.9996708  0.009210050          
  6      0.008938564  0.9998087  0.006899108          
  7      0.008049812  0.9998438  0.006202175          
  8      0.007930467  0.9998486  0.006102812  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nvmax = 8.
[1] "Mon Mar 12 04:52:38 2018"
Linear Regression with Forward Selection 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  nvmax  RMSE         Rsquared   MAE          Selected
  2      0.050403064  0.9938527  0.039330792          
  3      0.026098310  0.9983624  0.019547307          
  4      0.011680864  0.9996708  0.009210050          
  6      0.008938564  0.9998087  0.006899108          
  7      0.008049812  0.9998438  0.006202175          
  8      0.007930467  0.9998486  0.006102812  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nvmax = 8.
[1] "Mon Mar 12 04:52:45 2018"
Linear Regression with Stepwise Selection 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  nvmax  RMSE         Rsquared   MAE          Selected
  2      0.019278294  0.9991054  0.014956466          
  3      0.013626412  0.9995538  0.010895113          
  4      0.011680864  0.9996708  0.009210050          
  6      0.008938564  0.9998087  0.006899108          
  7      0.008049812  0.9998438  0.006202175  *       
  8      0.008511474  0.9998280  0.006407154          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nvmax = 7.
[1] "Mon Mar 12 04:52:51 2018"
Linear Regression 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results:

  RMSE         Rsquared   MAE       
  0.007977303  0.9998473  0.00607296

Tuning parameter 'intercept' was held constant at a value of TRUE
[1] "Mon Mar 12 04:52:57 2018"
Start:  AIC=-4887.14
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq      RSS     AIC
<none>              0.027333 -4887.1
- V9    1  0.000191 0.027523 -4885.7
- V7    1  0.001778 0.029111 -4857.6
- V6    1  0.002478 0.029810 -4845.8
- V10   1  0.003496 0.030829 -4829.0
- V8    1  0.004779 0.032111 -4808.6
- V3    1  0.025447 0.052780 -4560.1
- V4    1  0.028539 0.055871 -4531.7
- V5    1  0.081600 0.108932 -4197.8
- V2    1  0.090476 0.117808 -4158.7
Start:  AIC=-4827.86
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq      RSS     AIC
- V9    1  0.000093 0.030866 -4828.3
<none>              0.030773 -4827.9
- V7    1  0.001568 0.032342 -4805.0
- V6    1  0.002255 0.033028 -4794.5
- V10   1  0.003447 0.034220 -4776.8
- V8    1  0.004982 0.035755 -4754.8
- V3    1  0.026699 0.057472 -4517.5
- V4    1  0.030959 0.061733 -4481.8
- V5    1  0.082339 0.113112 -4179.0
- V2    1  0.089668 0.120441 -4147.6

Step:  AIC=-4828.35
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V10

       Df Sum of Sq      RSS     AIC
<none>              0.030866 -4828.3
- V7    1  0.001475 0.032342 -4807.0
- V6    1  0.002203 0.033070 -4795.9
- V10   1  0.006909 0.037775 -4729.4
- V8    1  0.009642 0.040508 -4694.4
- V3    1  0.032082 0.062948 -4474.0
- V4    1  0.078616 0.109482 -4197.3
- V5    1  0.083456 0.114322 -4175.7
- V2    1  0.139826 0.170692 -3975.3
Start:  AIC=-4854.75
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq      RSS     AIC
<none>              0.030438 -4854.8
- V9    1  0.000838 0.031276 -4843.1
- V7    1  0.001123 0.031561 -4838.6
- V6    1  0.001836 0.032274 -4827.3
- V10   1  0.003108 0.033546 -4807.9
- V8    1  0.007442 0.037880 -4747.0
- V3    1  0.031900 0.062338 -4496.9
- V4    1  0.035313 0.065751 -4470.1
- V5    1  0.076670 0.107108 -4225.2
- V2    1  0.101679 0.132117 -4119.8
Start:  AIC=-7284.02
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq      RSS     AIC
<none>              0.044855 -7284.0
- V9    1  0.000395 0.045250 -7279.4
- V7    1  0.002153 0.047009 -7250.8
- V6    1  0.003187 0.048042 -7234.5
- V10   1  0.005286 0.050141 -7202.4
- V8    1  0.008457 0.053312 -7156.3
- V3    1  0.041767 0.086622 -6791.8
- V4    1  0.047060 0.091915 -6747.2
- V5    1  0.121187 0.166042 -6303.1
- V2    1  0.143732 0.188587 -6207.5
Linear Regression with Stepwise Selection 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results:

  RMSE         Rsquared   MAE        
  0.007996443  0.9998465  0.006094622

[1] "Mon Mar 12 04:53:04 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :17    NA's   :17    NA's   :17   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 04:53:13 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "logicBag"                       
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 04:53:23 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "logreg"                         
Model Tree 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  pruned  smoothed  rules  RMSE         Rsquared   MAE          Selected
  Yes     Yes       Yes    0.007119934  0.9998761  0.005489308  *       
  Yes     Yes       No     0.007191767  0.9998741  0.005606401          
  Yes     No        Yes    0.007141825  0.9998754  0.005501752          
  Yes     No        No     0.007313674  0.9998705  0.005701137          
  No      Yes       Yes    0.037449747  0.9962251  0.019075164          
  No      Yes       No     0.018281902  0.9991948  0.013581780          
  No      No        Yes    0.088657024  0.9809402  0.062475005          
  No      No        No     0.061395655  0.9908647  0.049076620          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were pruned = Yes, smoothed = Yes and
 rules = Yes.
[1] "Mon Mar 12 04:53:39 2018"
Model Rules 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  pruned  smoothed  RMSE         Rsquared   MAE          Selected
  Yes     Yes       0.007119934  0.9998761  0.005489308  *       
  Yes     No        0.007141825  0.9998754  0.005501752          
  No      Yes       0.037449747  0.9962251  0.019075164          
  No      No        0.088657024  0.9809402  0.062475005          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were pruned = Yes and smoothed = Yes.
[1] "Mon Mar 12 04:53:54 2018"
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 04:54:00 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "mlpKerasDecay"                  
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 04:54:13 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "mlpKerasDropout"                
Multi-Step Adaptive MCP-Net 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  alphas      nsteps  scale      RMSE        Rsquared   MAE         Selected
  0.05888267  10      1.2616321  0.01448078  0.9995253  0.01152260  *       
  0.24902713   3      0.9721159  0.02208897  0.9990405  0.01671729          
  0.26011107   6      2.7158400  0.01934611  0.9991036  0.01500536          
  0.30100145   2      0.7936914  0.01918134  0.9992289  0.01497183          
  0.31551079   3      0.3663975  0.01866081  0.9992466  0.01446980          
  0.35830636   8      3.0413648  0.03043644  0.9972603  0.02367046          
  0.35930979   5      3.7803424  0.03045486  0.9972567  0.02372957          
  0.40527714   6      1.5479382  0.01928863  0.9991051  0.01495174          
  0.58386614   7      2.3298593  0.01928327  0.9991052  0.01495297          
  0.59401652   9      1.8635040  0.01928115  0.9991053  0.01495114          
  0.62993395   5      1.6249124  0.01927992  0.9991054  0.01495152          
  0.74392322   5      3.0439138  0.01928045  0.9991053  0.01495439          
  0.79030311   5      3.5613155  0.01928037  0.9991053  0.01495518          
  0.79175434   4      2.1094204  0.01927875  0.9991054  0.01495426          
  0.79361797   4      2.8875072  0.01927929  0.9991054  0.01495458          
  0.80894892  10      3.5241072  0.01927985  0.9991053  0.01495520          
  0.84498554   5      3.2954330  0.01927899  0.9991054  0.01495524          
  0.84907774   3      1.5789863  0.01927840  0.9991054  0.01495488          
  0.88117877   4      3.8896453  0.01927894  0.9991054  0.01495574          
  0.92241017   3      3.9260416  0.01927852  0.9991054  0.01495603          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were alphas = 0.05888267, nsteps = 10
 and scale = 1.261632.
[1] "Mon Mar 12 04:54:30 2018"
Installing package into 'C:/Users/John/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Installing package into 'C:/Users/John/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
# weights:  188
initial  value 737.278701 
iter  10 value 393.544658
iter  20 value 389.374626
iter  30 value 389.263499
final  value 389.262328 
converged
# weights:  188
initial  value 519.634005 
iter  10 value 365.326646
iter  20 value 364.329529
iter  30 value 363.742186
iter  40 value 363.655375
iter  50 value 354.525467
iter  60 value 348.313623
iter  70 value 348.071913
iter  80 value 347.989430
iter  90 value 347.941716
iter 100 value 347.910981
final  value 347.910981 
stopped after 100 iterations
# weights:  199
initial  value 671.562791 
iter  10 value 369.638043
iter  20 value 363.731748
iter  30 value 363.670310
iter  40 value 363.668134
iter  50 value 363.654070
iter  60 value 363.616008
iter  70 value 352.476576
iter  80 value 348.089574
iter  90 value 347.742208
iter 100 value 347.657559
final  value 347.657559 
stopped after 100 iterations
# weights:  133
initial  value 675.988332 
iter  10 value 367.201478
iter  20 value 364.164318
iter  30 value 354.798998
iter  40 value 352.801675
iter  50 value 352.651261
iter  60 value 352.023423
iter  70 value 351.768168
iter  80 value 351.527359
iter  90 value 351.480337
iter 100 value 351.473614
final  value 351.473614 
stopped after 100 iterations
# weights:  210
initial  value 739.090541 
iter  10 value 363.933120
iter  20 value 363.894525
iter  30 value 362.288981
iter  40 value 350.030198
iter  50 value 348.308346
iter  60 value 347.940658
iter  70 value 347.836534
iter  80 value 347.739454
iter  90 value 347.695170
iter 100 value 347.663329
final  value 347.663329 
stopped after 100 iterations
# weights:  177
initial  value 897.152920 
iter  10 value 366.449024
iter  20 value 364.274709
iter  30 value 353.104272
iter  40 value 348.844771
iter  50 value 348.241273
iter  60 value 347.942922
iter  70 value 347.858353
iter  80 value 347.817893
iter  90 value 347.802885
iter 100 value 347.795860
final  value 347.795860 
stopped after 100 iterations
# weights:  78
initial  value 966.154280 
iter  10 value 363.931750
iter  20 value 363.686094
iter  30 value 351.384339
iter  40 value 348.375390
iter  50 value 348.235423
iter  60 value 348.187693
iter  70 value 348.165746
iter  80 value 348.146977
iter  90 value 348.135536
iter 100 value 348.130894
final  value 348.130894 
stopped after 100 iterations
# weights:  221
initial  value 705.862851 
iter  10 value 368.453636
iter  20 value 363.718093
iter  30 value 363.668322
iter  40 value 363.664929
iter  50 value 363.661872
iter  60 value 363.660681
iter  70 value 363.658397
iter  80 value 363.652273
iter  90 value 363.599842
iter 100 value 350.887893
final  value 350.887893 
stopped after 100 iterations
# weights:  144
initial  value 507.468470 
iter  10 value 370.529745
iter  20 value 367.040897
iter  30 value 366.796464
final  value 366.796432 
converged
# weights:  56
initial  value 610.092825 
iter  10 value 364.627527
iter  20 value 363.673936
iter  30 value 363.669784
iter  40 value 363.654072
iter  50 value 363.554104
iter  60 value 350.544247
iter  70 value 347.805040
iter  80 value 347.609836
iter  90 value 347.562888
iter 100 value 347.551922
final  value 347.551922 
stopped after 100 iterations
# weights:  67
initial  value 937.753472 
iter  10 value 365.108187
iter  20 value 363.679522
iter  30 value 363.669748
iter  40 value 363.667693
iter  50 value 363.666563
iter  60 value 361.224559
iter  70 value 349.693721
iter  80 value 347.812977
iter  90 value 347.660632
iter 100 value 347.622055
final  value 347.622055 
stopped after 100 iterations
# weights:  78
initial  value 922.079558 
iter  10 value 368.764927
iter  20 value 366.672370
iter  30 value 366.549874
iter  40 value 363.896899
iter  50 value 363.684741
final  value 363.684306 
converged
# weights:  67
initial  value 1107.789291 
iter  10 value 363.772817
iter  20 value 363.665649
iter  20 value 363.665649
iter  20 value 363.665648
final  value 363.665648 
converged
# weights:  188
initial  value 418.334800 
iter  10 value 364.041204
iter  20 value 363.672116
iter  30 value 363.666721
iter  40 value 363.657848
iter  50 value 363.619323
iter  60 value 354.657085
iter  70 value 348.210269
iter  80 value 347.725215
iter  90 value 347.657700
iter 100 value 347.615001
final  value 347.615001 
stopped after 100 iterations
# weights:  89
initial  value 860.546628 
iter  10 value 372.812681
iter  20 value 364.383290
iter  30 value 363.820359
iter  40 value 355.311238
iter  50 value 349.444313
iter  60 value 348.508441
iter  70 value 348.397660
iter  80 value 348.375581
iter  90 value 348.371048
iter 100 value 348.369836
final  value 348.369836 
stopped after 100 iterations
# weights:  199
initial  value 432.109889 
iter  10 value 365.003123
iter  20 value 351.982357
iter  30 value 349.009982
iter  40 value 348.402561
iter  50 value 348.188466
iter  60 value 348.140563
iter  70 value 348.124479
iter  80 value 348.115426
iter  90 value 348.109215
iter 100 value 348.106578
final  value 348.106578 
stopped after 100 iterations
# weights:  144
initial  value 1170.251035 
iter  10 value 364.306053
iter  20 value 363.695994
iter  30 value 363.648512
iter  40 value 359.348313
iter  50 value 350.221657
iter  60 value 348.232055
iter  70 value 348.016952
iter  80 value 347.949284
iter  90 value 347.909809
iter 100 value 347.888216
final  value 347.888216 
stopped after 100 iterations
# weights:  56
initial  value 935.849372 
iter  10 value 373.901500
iter  20 value 364.087052
iter  30 value 351.366585
iter  40 value 349.305859
iter  50 value 349.231202
iter  60 value 349.221033
iter  70 value 349.218108
iter  80 value 349.217037
iter  90 value 349.216783
final  value 349.216767 
converged
# weights:  188
initial  value 568.136812 
iter  10 value 364.435359
iter  20 value 363.938456
iter  30 value 363.706331
iter  40 value 363.676180
iter  50 value 362.961579
iter  60 value 350.347484
iter  70 value 347.945035
iter  80 value 347.775252
iter  90 value 347.709827
iter 100 value 347.679458
final  value 347.679458 
stopped after 100 iterations
# weights:  12
initial  value 543.147419 
iter  10 value 411.019322
final  value 411.000796 
converged
# weights:  188
initial  value 908.208945 
iter  10 value 426.335810
iter  20 value 387.508389
iter  30 value 386.658506
iter  40 value 386.315502
iter  50 value 386.135518
final  value 386.121975 
converged
# weights:  188
initial  value 927.443465 
iter  10 value 363.865949
iter  20 value 361.242768
iter  30 value 360.686507
iter  40 value 360.581462
iter  50 value 347.433855
iter  60 value 343.969295
iter  70 value 343.550448
iter  80 value 343.499292
iter  90 value 343.451899
iter 100 value 343.426138
final  value 343.426138 
stopped after 100 iterations
# weights:  199
initial  value 1046.297340 
iter  10 value 366.706236
iter  20 value 360.624870
iter  30 value 360.561115
iter  40 value 360.557280
iter  50 value 360.470211
iter  60 value 345.051121
iter  70 value 343.485775
iter  80 value 343.175543
iter  90 value 343.140111
iter 100 value 343.114977
final  value 343.114977 
stopped after 100 iterations
# weights:  133
initial  value 821.473580 
iter  10 value 361.631316
iter  20 value 360.338025
iter  30 value 348.085917
iter  40 value 347.456655
iter  50 value 347.201994
iter  60 value 347.172735
iter  70 value 347.168375
final  value 347.168242 
converged
# weights:  210
initial  value 434.643749 
iter  10 value 362.333655
iter  20 value 360.576374
iter  30 value 360.556573
iter  40 value 360.523017
iter  50 value 349.638556
iter  60 value 344.099982
iter  70 value 343.513640
iter  80 value 343.363038
iter  90 value 343.296180
iter 100 value 343.217099
final  value 343.217099 
stopped after 100 iterations
# weights:  177
initial  value 905.580473 
iter  10 value 364.482496
iter  20 value 361.006915
iter  30 value 360.665634
iter  40 value 348.774384
iter  50 value 343.769739
iter  60 value 343.392499
iter  70 value 343.338937
iter  80 value 343.315479
iter  90 value 343.305202
iter 100 value 343.298254
final  value 343.298254 
stopped after 100 iterations
# weights:  78
initial  value 757.874284 
iter  10 value 363.942977
iter  20 value 360.818936
iter  30 value 346.669536
iter  40 value 343.951788
iter  50 value 343.734154
iter  60 value 343.672077
iter  70 value 343.649867
iter  80 value 343.642253
iter  90 value 343.636372
iter 100 value 343.632593
final  value 343.632593 
stopped after 100 iterations
# weights:  221
initial  value 471.373397 
iter  10 value 361.484440
iter  20 value 360.564667
iter  30 value 360.550535
iter  40 value 346.237385
iter  50 value 343.601917
iter  60 value 343.146458
iter  70 value 343.108630
iter  80 value 343.096783
iter  90 value 343.085279
iter 100 value 343.076038
final  value 343.076038 
stopped after 100 iterations
# weights:  144
initial  value 567.907671 
iter  10 value 365.409436
iter  20 value 363.659642
final  value 363.657007 
converged
# weights:  56
initial  value 527.460285 
iter  10 value 361.573273
iter  20 value 360.565891
iter  30 value 360.544833
iter  40 value 351.304151
iter  50 value 343.404419
iter  60 value 343.150244
iter  70 value 343.091797
iter  80 value 343.080438
iter  90 value 343.075320
iter 100 value 343.073509
final  value 343.073509 
stopped after 100 iterations
# weights:  67
initial  value 777.636517 
iter  10 value 361.650952
iter  20 value 360.567476
iter  30 value 360.561569
iter  40 value 360.561006
final  value 360.558460 
converged
# weights:  78
initial  value 886.929810 
iter  10 value 367.096309
iter  20 value 363.704927
iter  30 value 358.731442
iter  40 value 357.104676
final  value 357.089032 
converged
# weights:  67
initial  value 696.030893 
iter  10 value 360.731000
iter  20 value 360.555980
final  value 360.554858 
converged
# weights:  188
initial  value 466.391417 
iter  10 value 362.420227
iter  20 value 360.575456
iter  30 value 360.546149
iter  40 value 359.914583
iter  50 value 345.191072
iter  60 value 343.341026
iter  70 value 343.135920
iter  80 value 343.111379
iter  90 value 343.099888
iter 100 value 343.089235
final  value 343.089235 
stopped after 100 iterations
# weights:  89
initial  value 862.975657 
iter  10 value 367.231749
iter  20 value 361.733928
iter  30 value 346.934338
iter  40 value 344.206192
iter  50 value 343.930286
iter  60 value 343.898763
iter  70 value 343.883496
iter  80 value 343.878335
iter  90 value 343.876199
iter 100 value 343.874144
final  value 343.874144 
stopped after 100 iterations
# weights:  199
initial  value 740.176179 
iter  10 value 367.571259
iter  20 value 352.363731
iter  30 value 345.684988
iter  40 value 344.440535
iter  50 value 343.764654
iter  60 value 343.642563
iter  70 value 343.621578
iter  80 value 343.612915
iter  90 value 343.610224
iter 100 value 343.608261
final  value 343.608261 
stopped after 100 iterations
# weights:  144
initial  value 709.367294 
iter  10 value 362.710681
iter  20 value 361.069176
iter  30 value 345.356759
iter  40 value 343.717991
iter  50 value 343.530431
iter  60 value 343.441375
iter  70 value 343.399046
iter  80 value 343.383108
iter  90 value 343.370369
iter 100 value 343.365175
final  value 343.365175 
stopped after 100 iterations
# weights:  56
initial  value 608.469877 
iter  10 value 361.102050
iter  20 value 355.961616
iter  30 value 346.211516
iter  40 value 344.958915
iter  50 value 344.818949
iter  60 value 344.770309
iter  70 value 344.752323
iter  80 value 344.738980
iter  90 value 344.736975
iter 100 value 344.736395
final  value 344.736395 
stopped after 100 iterations
# weights:  188
initial  value 830.789090 
iter  10 value 377.504472
iter  20 value 360.749365
iter  30 value 360.578300
iter  40 value 360.559513
iter  50 value 346.175502
iter  60 value 343.649776
iter  70 value 343.286605
iter  80 value 343.226495
iter  90 value 343.191377
iter 100 value 343.175314
final  value 343.175314 
stopped after 100 iterations
# weights:  12
initial  value 942.306416 
iter  10 value 407.993280
final  value 407.819275 
converged
# weights:  188
initial  value 1176.946298 
iter  10 value 426.453648
iter  20 value 396.000095
iter  30 value 395.834937
iter  40 value 395.831668
final  value 395.831630 
converged
# weights:  188
initial  value 1083.653724 
iter  10 value 373.222881
iter  20 value 370.822997
iter  30 value 355.599216
iter  40 value 352.718002
iter  50 value 352.412744
iter  60 value 352.306175
iter  70 value 352.277070
iter  80 value 352.261110
iter  90 value 352.250326
iter 100 value 352.243811
final  value 352.243811 
stopped after 100 iterations
# weights:  199
initial  value 930.356840 
iter  10 value 377.306241
iter  20 value 370.416933
iter  30 value 370.346962
iter  40 value 370.344218
iter  50 value 370.339217
iter  60 value 370.338847
iter  70 value 370.338369
iter  80 value 370.337713
iter  90 value 370.336743
iter 100 value 370.335126
final  value 370.335126 
stopped after 100 iterations
# weights:  133
initial  value 922.188121 
iter  10 value 371.725845
iter  20 value 369.251954
iter  30 value 358.387522
iter  40 value 356.470566
iter  50 value 356.355272
iter  60 value 356.191573
iter  70 value 356.058042
iter  80 value 356.028866
iter  90 value 356.027722
final  value 356.027610 
converged
# weights:  210
initial  value 1010.702846 
iter  10 value 370.468411
iter  20 value 370.328102
iter  30 value 362.948296
iter  40 value 353.796666
iter  50 value 352.276371
iter  60 value 352.140545
iter  70 value 352.110033
iter  80 value 352.075699
iter  90 value 352.050624
iter 100 value 352.037082
final  value 352.037082 
stopped after 100 iterations
# weights:  177
initial  value 752.178684 
iter  10 value 373.157695
iter  20 value 370.690638
iter  30 value 370.381165
iter  40 value 370.087030
iter  50 value 353.512232
iter  60 value 352.401382
iter  70 value 352.290580
iter  80 value 352.243940
iter  90 value 352.215666
iter 100 value 352.197121
final  value 352.197121 
stopped after 100 iterations
# weights:  78
initial  value 821.224065 
iter  10 value 373.731612
iter  20 value 370.749150
iter  30 value 370.485744
iter  40 value 358.700668
iter  50 value 353.712375
iter  60 value 352.680439
iter  70 value 352.550927
iter  80 value 352.514559
iter  90 value 352.500455
iter 100 value 352.495379
final  value 352.495379 
stopped after 100 iterations
# weights:  221
initial  value 542.734149 
iter  10 value 371.791843
iter  20 value 370.353356
iter  30 value 370.338303
iter  40 value 355.043462
iter  50 value 352.413062
iter  60 value 352.057953
iter  70 value 351.967248
iter  80 value 351.940789
iter  90 value 351.922063
iter 100 value 351.915529
final  value 351.915529 
stopped after 100 iterations
# weights:  144
initial  value 641.318624 
iter  10 value 376.031057
iter  20 value 373.418074
final  value 373.413947 
converged
# weights:  56
initial  value 956.987933 
iter  10 value 374.125226
iter  20 value 370.380478
iter  30 value 370.341964
iter  40 value 370.188919
iter  50 value 356.429331
iter  60 value 352.688888
iter  70 value 351.988177
iter  80 value 351.933907
iter  90 value 351.922038
iter 100 value 351.915947
final  value 351.915947 
stopped after 100 iterations
# weights:  67
initial  value 769.383839 
iter  10 value 371.433385
iter  20 value 370.349223
iter  30 value 370.340147
iter  40 value 370.332894
iter  50 value 370.317343
iter  60 value 367.728052
iter  70 value 353.910220
iter  80 value 352.079459
iter  90 value 351.927955
iter 100 value 351.910347
final  value 351.910347 
stopped after 100 iterations
# weights:  78
initial  value 765.052662 
iter  10 value 376.022423
iter  20 value 371.035314
iter  30 value 366.393922
iter  40 value 365.489776
final  value 365.389456 
converged
# weights:  67
initial  value 1124.503881 
iter  10 value 370.438512
iter  20 value 370.340190
iter  30 value 370.337904
final  value 370.337513 
converged
# weights:  188
initial  value 429.315714 
iter  10 value 370.961772
iter  20 value 370.347281
iter  30 value 370.321348
iter  40 value 367.319635
iter  50 value 353.487709
iter  60 value 352.234855
iter  70 value 352.088790
iter  80 value 352.042341
iter  90 value 352.001269
iter 100 value 351.982231
final  value 351.982231 
stopped after 100 iterations
# weights:  89
initial  value 940.303762 
iter  10 value 376.373863
iter  20 value 371.021604
iter  30 value 359.500791
iter  40 value 353.444838
iter  50 value 352.943976
iter  60 value 352.804464
iter  70 value 352.761577
iter  80 value 352.748219
iter  90 value 352.740482
iter 100 value 352.735813
final  value 352.735813 
stopped after 100 iterations
# weights:  199
initial  value 858.705373 
iter  10 value 379.787896
iter  20 value 364.899895
iter  30 value 354.540376
iter  40 value 353.105940
iter  50 value 352.676558
iter  60 value 352.548842
iter  70 value 352.515787
iter  80 value 352.497938
iter  90 value 352.489868
iter 100 value 352.485262
final  value 352.485262 
stopped after 100 iterations
# weights:  144
initial  value 998.685569 
iter  10 value 373.317003
iter  20 value 371.007646
iter  30 value 370.511255
iter  40 value 370.387794
iter  50 value 364.337557
iter  60 value 352.992981
iter  70 value 352.471011
iter  80 value 352.325687
iter  90 value 352.269170
iter 100 value 352.242551
final  value 352.242551 
stopped after 100 iterations
# weights:  56
initial  value 677.293190 
iter  10 value 377.290889
iter  20 value 370.874720
iter  30 value 355.513387
iter  40 value 353.685152
iter  50 value 353.619711
iter  60 value 353.608905
iter  70 value 353.604089
iter  80 value 353.601632
iter  90 value 353.601339
iter 100 value 353.601177
final  value 353.601177 
stopped after 100 iterations
# weights:  188
initial  value 786.111134 
iter  10 value 371.543170
iter  20 value 370.716303
iter  30 value 370.418315
iter  40 value 370.363199
iter  50 value 370.354070
iter  60 value 362.318567
iter  70 value 353.132449
iter  80 value 352.238197
iter  90 value 352.117746
iter 100 value 352.049292
final  value 352.049292 
stopped after 100 iterations
# weights:  12
initial  value 942.708098 
iter  10 value 418.939711
final  value 417.588482 
converged
# weights:  56
initial  value 1682.369496 
iter  10 value 551.536893
iter  20 value 547.354264
iter  30 value 547.306777
iter  40 value 547.292223
iter  50 value 547.290591
iter  60 value 547.289743
iter  70 value 547.288498
iter  80 value 547.286454
iter  90 value 547.282442
iter 100 value 547.271118
final  value 547.271118 
stopped after 100 iterations
Neural Network 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  size  decay         RMSE       Rsquared   MAE        Selected
   1    4.828817e+00  0.8728696  0.5837344  0.7204586          
   5    2.001656e-04  0.8330040  0.4505621  0.6368553  *       
   5    1.800074e-02  0.8336546  0.4409073  0.6441047          
   6    1.021320e-05  0.8535910  0.4467239  0.7045550          
   6    6.637047e-05  0.8397237  0.3602021  0.6608000          
   7    4.343504e-03  0.8332483  0.4474928  0.6406371          
   7    3.787248e-01  0.8393724  0.4555762  0.6674022          
   8    7.006547e-03  0.8333334  0.4459619  0.6413635          
  12    7.197236e-02  0.8345899  0.4329270  0.6494326          
  13    1.969011e-03  0.8331214  0.4482219  0.6390483          
  13    6.081004e-01  0.8541118  0.5655437  0.7046228          
  16    1.553060e-03  0.8331088  0.4500801  0.6386276          
  17    2.583383e-04  0.8330382  0.4501582  0.6378194          
  17    6.085167e-04  0.8330706  0.4521699  0.6383446          
  17    2.138257e-03  0.8331876  0.4491824  0.6399372          
  17    9.728275e+00  0.8610326  0.7495350  0.7103875          
  18    2.092909e-04  0.8391843  0.3944507  0.6592696          
  18    4.458840e-03  0.8332480  0.4468988  0.6405725          
  19    5.247859e-04  0.8330650  0.4488852  0.6383901          
  20    1.238569e-04  0.8345172  0.4301965  0.6459867          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 5 and decay = 0.0002001656.
[1] "Mon Mar 12 04:55:13 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: package 'mxnet' is not available (for R version 3.4.2) 
2: package 'mxnet' is not available (for R version 3.4.2) 
3: predictions failed for Fold1: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments
 
4: predictions failed for Fold2: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments
 
5: predictions failed for Fold3: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments
 
6: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: predictions failed for Fold1: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments
 
2: predictions failed for Fold2: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments
 
3: predictions failed for Fold3: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 04:55:20 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "nnls"                           
Non-Informative Model 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results:

  RMSE       Rsquared  MAE      
  0.6406086  NaN       0.5285982

[1] "Mon Mar 12 04:55:26 2018"
Error : The tuning parameter grid should have columns alpha, criteria, link
In addition: Warning message:
In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Error : The tuning parameter grid should have columns alpha, criteria, link
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :12    NA's   :12    NA's   :12   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 04:55:35 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "ordinalNet"                     
Parallel Random Forest 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE         Selected
  1     0.05342568  0.9945070  0.03882446          
  2     0.03921428  0.9966512  0.02859726          
  3     0.03635671  0.9970137  0.02655389          
  4     0.03593982  0.9970384  0.02651020  *       
  6     0.03710397  0.9967403  0.02739522          
  7     0.03815958  0.9965306  0.02835361          
  8     0.03898762  0.9963462  0.02909559          
  9     0.04017521  0.9960988  0.03001828          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 4.
[1] "Mon Mar 12 04:56:24 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
# weights:  103
initial  value 1002.993232 
iter  10 value 405.104227
iter  20 value 399.098324
iter  30 value 399.042807
final  value 399.042407 
converged
# weights:  103
initial  value 1148.248447 
iter  10 value 365.103073
iter  20 value 363.675531
iter  30 value 347.839853
iter  40 value 347.798534
iter  50 value 347.775710
iter  60 value 347.765919
iter  70 value 347.758839
iter  80 value 347.754541
iter  90 value 347.751107
iter 100 value 347.747028
final  value 347.747028 
stopped after 100 iterations
# weights:  109
initial  value 682.510260 
iter  10 value 366.222341
iter  20 value 363.692368
iter  30 value 363.663626
iter  40 value 358.386754
iter  50 value 347.715262
iter  60 value 347.673126
iter  70 value 347.604513
iter  80 value 347.583227
iter  90 value 347.572202
iter 100 value 347.559590
final  value 347.559590 
stopped after 100 iterations
# weights:  73
initial  value 600.520332 
iter  10 value 361.731848
iter  20 value 351.314415
iter  30 value 350.507315
iter  40 value 350.209025
iter  50 value 350.173143
iter  60 value 350.171284
final  value 350.171265 
converged
# weights:  115
initial  value 825.780200 
iter  10 value 374.642223
iter  20 value 363.789442
iter  30 value 363.662520
iter  40 value 350.682871
iter  50 value 347.708997
iter  60 value 347.664135
iter  70 value 347.619325
iter  80 value 347.609189
iter  90 value 347.603335
iter 100 value 347.599017
final  value 347.599017 
stopped after 100 iterations
# weights:  97
initial  value 662.702047 
iter  10 value 364.995438
iter  20 value 364.047513
iter  30 value 348.340829
iter  40 value 347.869545
iter  50 value 347.782365
iter  60 value 347.725598
iter  70 value 347.708063
iter  80 value 347.698166
iter  90 value 347.690898
iter 100 value 347.684360
final  value 347.684360 
stopped after 100 iterations
# weights:  43
initial  value 814.454306 
iter  10 value 365.570795
iter  20 value 363.828375
iter  30 value 354.041982
iter  40 value 349.500589
iter  50 value 348.350090
iter  60 value 348.054467
iter  70 value 347.983150
iter  80 value 347.954056
iter  90 value 347.939072
iter 100 value 347.933028
final  value 347.933028 
stopped after 100 iterations
# weights:  121
initial  value 638.444931 
iter  10 value 365.324910
iter  20 value 363.682021
iter  30 value 362.552201
iter  40 value 347.697859
iter  50 value 347.677639
iter  60 value 347.622561
iter  70 value 347.569671
iter  80 value 347.546533
iter  90 value 347.536377
iter 100 value 347.528195
final  value 347.528195 
stopped after 100 iterations
# weights:  79
initial  value 704.488445 
iter  10 value 368.719217
iter  20 value 360.533213
iter  30 value 360.402134
final  value 360.401427 
converged
# weights:  31
initial  value 691.927421 
iter  10 value 364.904414
iter  20 value 363.680539
iter  30 value 363.582422
iter  40 value 348.289834
iter  50 value 347.680542
iter  60 value 347.599587
iter  70 value 347.554533
iter  80 value 347.538958
iter  90 value 347.530170
iter 100 value 347.525030
final  value 347.525030 
stopped after 100 iterations
# weights:  37
initial  value 1059.505878 
iter  10 value 364.141977
iter  20 value 363.677680
iter  30 value 363.657080
iter  40 value 354.091000
iter  50 value 347.736123
iter  60 value 347.608509
iter  70 value 347.568509
iter  80 value 347.542512
iter  90 value 347.516615
iter 100 value 347.505327
final  value 347.505327 
stopped after 100 iterations
# weights:  43
initial  value 866.378199 
iter  10 value 364.452777
iter  20 value 358.537005
iter  30 value 358.362570
final  value 358.362111 
converged
# weights:  37
initial  value 612.152614 
iter  10 value 363.709485
iter  20 value 363.664052
iter  20 value 363.664050
iter  20 value 363.664050
final  value 363.664050 
converged
# weights:  103
initial  value 475.641259 
iter  10 value 364.463175
iter  20 value 363.672056
iter  30 value 363.668250
iter  40 value 363.520790
iter  50 value 347.719094
iter  60 value 347.666999
iter  70 value 347.630645
iter  80 value 347.596133
iter  90 value 347.578045
iter 100 value 347.571750
final  value 347.571750 
stopped after 100 iterations
# weights:  49
initial  value 1011.950593 
iter  10 value 366.660839
iter  20 value 364.062621
iter  30 value 351.962361
iter  40 value 348.798650
iter  50 value 348.343918
iter  60 value 348.192405
iter  70 value 348.151414
iter  80 value 348.116592
iter  90 value 348.095635
iter 100 value 348.077663
final  value 348.077663 
stopped after 100 iterations
# weights:  109
initial  value 423.557761 
iter  10 value 356.321823
iter  20 value 347.904611
iter  30 value 347.896218
iter  40 value 347.892396
iter  50 value 347.891477
iter  60 value 347.891212
iter  70 value 347.891134
iter  70 value 347.891132
iter  70 value 347.891132
final  value 347.891132 
converged
# weights:  79
initial  value 519.800206 
iter  10 value 364.367516
iter  20 value 363.707787
iter  30 value 348.476701
iter  40 value 347.847205
iter  50 value 347.759054
iter  60 value 347.737907
iter  70 value 347.723861
iter  80 value 347.718143
iter  90 value 347.715346
iter 100 value 347.714289
final  value 347.714289 
stopped after 100 iterations
# weights:  31
initial  value 609.168949 
iter  10 value 367.828302
iter  20 value 354.144602
iter  30 value 349.241400
iter  40 value 348.821218
iter  50 value 348.726147
iter  60 value 348.700079
iter  70 value 348.698068
iter  80 value 348.697909
iter  80 value 348.697908
final  value 348.697908 
converged
# weights:  103
initial  value 661.753701 
iter  10 value 367.721821
iter  20 value 363.709656
iter  30 value 363.673057
iter  40 value 354.719544
iter  50 value 347.775709
iter  60 value 347.699670
iter  70 value 347.629493
iter  80 value 347.610224
iter  90 value 347.598905
iter 100 value 347.593073
final  value 347.593073 
stopped after 100 iterations
# weights:  7
initial  value 881.085856 
iter  10 value 417.445179
final  value 416.930699 
converged
# weights:  103
initial  value 727.525149 
iter  10 value 398.316174
iter  20 value 395.843306
final  value 395.836155 
converged
# weights:  103
initial  value 1008.658782 
iter  10 value 346.346274
iter  20 value 343.681062
iter  30 value 343.458525
iter  40 value 343.345093
iter  50 value 343.293236
iter  60 value 343.273729
iter  70 value 343.262924
iter  80 value 343.258363
iter  90 value 343.254642
iter 100 value 343.251954
final  value 343.251954 
stopped after 100 iterations
# weights:  109
initial  value 791.563953 
iter  10 value 364.362423
iter  20 value 360.597848
iter  30 value 360.561693
iter  40 value 360.561581
iter  50 value 360.561441
iter  60 value 360.561265
iter  70 value 360.561016
iter  80 value 360.560625
iter  90 value 360.559844
iter 100 value 360.557205
final  value 360.557205 
stopped after 100 iterations
# weights:  73
initial  value 735.111015 
iter  10 value 358.227050
iter  20 value 347.191574
iter  30 value 345.920987
iter  40 value 345.701348
iter  50 value 345.682838
iter  60 value 345.673727
iter  70 value 345.672689
final  value 345.672677 
converged
# weights:  115
initial  value 576.886759 
iter  10 value 365.127929
iter  20 value 360.606674
iter  30 value 357.550063
iter  40 value 343.575556
iter  50 value 343.239013
iter  60 value 343.155876
iter  70 value 343.139777
iter  80 value 343.128966
iter  90 value 343.125421
iter 100 value 343.123442
final  value 343.123442 
stopped after 100 iterations
# weights:  97
initial  value 1007.843714 
iter  10 value 366.301617
iter  20 value 360.493016
iter  30 value 343.425727
iter  40 value 343.309337
iter  50 value 343.252670
iter  60 value 343.225092
iter  70 value 343.214800
iter  80 value 343.209355
iter  90 value 343.206935
iter 100 value 343.204600
final  value 343.204600 
stopped after 100 iterations
# weights:  43
initial  value 644.291466 
iter  10 value 362.260570
iter  20 value 356.548739
iter  30 value 343.684205
iter  40 value 343.507750
iter  50 value 343.466126
iter  60 value 343.453746
iter  70 value 343.448120
iter  80 value 343.443856
iter  90 value 343.441507
iter 100 value 343.440394
final  value 343.440394 
stopped after 100 iterations
# weights:  121
initial  value 1046.092330 
iter  10 value 363.105662
iter  20 value 360.583358
iter  30 value 360.558919
iter  40 value 360.558110
iter  50 value 360.557999
iter  60 value 360.557846
iter  70 value 360.557610
iter  80 value 360.557190
iter  90 value 360.556200
iter 100 value 360.551458
final  value 360.551458 
stopped after 100 iterations
# weights:  79
initial  value 885.198326 
iter  10 value 361.046274
iter  20 value 355.863331
iter  30 value 355.810216
iter  40 value 355.806671
iter  40 value 355.806668
iter  40 value 355.806668
final  value 355.806668 
converged
# weights:  31
initial  value 538.633061 
iter  10 value 361.122926
iter  20 value 360.565397
iter  30 value 360.462704
iter  40 value 344.584676
iter  50 value 343.224434
iter  60 value 343.119003
iter  70 value 343.085246
iter  80 value 343.065659
iter  90 value 343.060672
iter 100 value 343.055016
final  value 343.055016 
stopped after 100 iterations
# weights:  37
initial  value 1120.532272 
iter  10 value 360.901940
iter  20 value 360.565792
iter  30 value 360.556905
iter  40 value 360.539966
iter  50 value 344.751049
iter  60 value 343.279568
iter  70 value 343.140965
iter  80 value 343.085277
iter  90 value 343.063935
iter 100 value 343.043083
final  value 343.043083 
stopped after 100 iterations
# weights:  43
initial  value 1059.645331 
iter  10 value 358.970586
iter  20 value 353.698087
iter  30 value 353.636303
final  value 353.636281 
converged
# weights:  37
initial  value 874.864041 
iter  10 value 360.643894
iter  20 value 360.555122
final  value 360.555056 
converged
# weights:  103
initial  value 907.604769 
iter  10 value 365.228874
iter  20 value 360.607837
iter  30 value 360.563609
iter  40 value 360.563388
iter  50 value 360.563081
iter  60 value 360.562579
iter  70 value 360.561471
iter  80 value 360.556480
iter  90 value 350.866151
iter 100 value 343.291309
final  value 343.291309 
stopped after 100 iterations
# weights:  49
initial  value 1179.486751 
iter  10 value 363.842061
iter  20 value 354.220921
iter  30 value 343.661182
iter  40 value 343.630658
iter  50 value 343.618195
iter  60 value 343.615619
iter  70 value 343.614748
iter  80 value 343.614412
final  value 343.614304 
converged
# weights:  109
initial  value 470.121778 
iter  10 value 361.478760
iter  20 value 343.813445
iter  30 value 343.425436
iter  40 value 343.400665
iter  50 value 343.393316
iter  60 value 343.389137
iter  70 value 343.386922
iter  80 value 343.385727
iter  90 value 343.385209
iter 100 value 343.384831
final  value 343.384831 
stopped after 100 iterations
# weights:  79
initial  value 1079.282886 
iter  10 value 362.552801
iter  20 value 360.697835
iter  30 value 344.924336
iter  40 value 343.393539
iter  50 value 343.304455
iter  60 value 343.281201
iter  70 value 343.259875
iter  80 value 343.250169
iter  90 value 343.245396
final  value 343.242487 
converged
# weights:  31
initial  value 772.531816 
iter  10 value 367.114053
iter  20 value 347.290756
iter  30 value 344.479487
iter  40 value 344.302690
iter  50 value 344.265560
iter  60 value 344.261719
iter  70 value 344.261148
iter  80 value 344.260788
final  value 344.260771 
converged
# weights:  103
initial  value 817.501919 
iter  10 value 371.548232
iter  20 value 360.680695
iter  30 value 360.496077
iter  40 value 343.293995
iter  50 value 343.260385
iter  60 value 343.194589
iter  70 value 343.149504
iter  80 value 343.141397
iter  90 value 343.134795
iter 100 value 343.131257
final  value 343.131257 
stopped after 100 iterations
# weights:  7
initial  value 713.883131 
iter  10 value 413.853572
final  value 413.638590 
converged
# weights:  103
initial  value 858.732916 
iter  10 value 419.718066
iter  20 value 405.790320
iter  30 value 405.482246
final  value 405.478736 
converged
# weights:  103
initial  value 1149.028450 
iter  10 value 372.197032
iter  20 value 353.111331
iter  30 value 352.301699
iter  40 value 352.178739
iter  50 value 352.135848
iter  60 value 352.118540
iter  70 value 352.113017
iter  80 value 352.108945
iter  90 value 352.106041
iter 100 value 352.103678
final  value 352.103678 
stopped after 100 iterations
# weights:  109
initial  value 995.300777 
iter  10 value 373.445777
iter  20 value 370.372425
iter  30 value 370.344330
iter  40 value 370.341948
iter  50 value 370.336033
iter  60 value 358.243724
iter  70 value 352.088841
iter  80 value 352.063494
iter  90 value 351.975361
iter 100 value 351.949732
final  value 351.949732 
stopped after 100 iterations
# weights:  73
initial  value 1157.078303 
iter  10 value 377.329312
iter  20 value 356.427243
iter  30 value 354.875660
iter  40 value 354.842725
final  value 354.842487 
converged
# weights:  115
initial  value 833.124308 
iter  10 value 380.073610
iter  20 value 370.448838
iter  30 value 370.346157
iter  40 value 365.024296
iter  50 value 352.199428
iter  60 value 352.096742
iter  70 value 352.013290
iter  80 value 351.993085
iter  90 value 351.983129
iter 100 value 351.975625
final  value 351.975625 
stopped after 100 iterations
# weights:  97
initial  value 632.905107 
iter  10 value 371.319246
iter  20 value 363.357663
iter  30 value 352.654678
iter  40 value 352.179319
iter  50 value 352.123580
iter  60 value 352.083423
iter  70 value 352.069434
iter  80 value 352.062862
iter  90 value 352.059985
iter 100 value 352.057132
final  value 352.057132 
stopped after 100 iterations
# weights:  43
initial  value 716.327157 
iter  10 value 372.141084
iter  20 value 369.972260
iter  30 value 352.634869
iter  40 value 352.344968
iter  50 value 352.307909
iter  60 value 352.295170
iter  70 value 352.290185
iter  80 value 352.286761
iter  90 value 352.285116
iter 100 value 352.283771
final  value 352.283771 
stopped after 100 iterations
# weights:  121
initial  value 834.337825 
iter  10 value 372.689924
iter  20 value 370.363710
iter  30 value 370.341630
iter  40 value 370.338547
iter  50 value 370.331818
iter  60 value 356.862426
iter  70 value 352.091674
iter  80 value 352.035454
iter  90 value 351.976405
iter 100 value 351.928177
final  value 351.928177 
stopped after 100 iterations
# weights:  79
initial  value 964.437987 
iter  10 value 369.890538
iter  20 value 364.731389
iter  30 value 364.701456
final  value 364.699317 
converged
# weights:  31
initial  value 766.472793 
iter  10 value 371.788925
iter  20 value 370.354682
iter  30 value 356.925404
iter  40 value 352.114388
iter  50 value 351.966895
iter  60 value 351.928191
iter  70 value 351.906726
iter  80 value 351.899877
iter  90 value 351.894729
iter 100 value 351.892648
final  value 351.892648 
stopped after 100 iterations
# weights:  37
initial  value 874.446530 
iter  10 value 370.881010
iter  20 value 370.343436
iter  30 value 356.995162
iter  40 value 352.109214
iter  50 value 351.912684
iter  60 value 351.906603
iter  70 value 351.900227
iter  80 value 351.892373
iter  90 value 351.887639
iter 100 value 351.883417
final  value 351.883417 
stopped after 100 iterations
# weights:  43
initial  value 1149.717519 
iter  10 value 370.840092
iter  20 value 362.887838
iter  30 value 362.621055
final  value 362.620974 
converged
# weights:  37
initial  value 568.634169 
iter  10 value 370.368032
iter  20 value 365.555555
iter  30 value 352.304019
iter  40 value 352.095019
iter  50 value 351.976476
iter  60 value 351.903894
iter  70 value 351.861003
iter  80 value 351.847269
iter  90 value 351.843246
iter 100 value 351.842232
final  value 351.842232 
stopped after 100 iterations
# weights:  103
initial  value 513.268989 
iter  10 value 371.555013
iter  20 value 370.350546
iter  30 value 352.749327
iter  40 value 352.081860
iter  50 value 352.028629
iter  60 value 351.984221
iter  70 value 351.960068
iter  80 value 351.947838
iter  90 value 351.935025
iter 100 value 351.927446
final  value 351.927446 
stopped after 100 iterations
# weights:  49
initial  value 908.197195 
iter  10 value 373.604975
iter  20 value 370.780550
iter  30 value 353.477139
iter  40 value 352.538806
iter  50 value 352.467952
iter  60 value 352.454668
iter  70 value 352.445523
iter  80 value 352.437886
iter  90 value 352.435254
iter 100 value 352.434373
final  value 352.434373 
stopped after 100 iterations
# weights:  109
initial  value 900.158135 
iter  10 value 375.411628
iter  20 value 371.315532
iter  30 value 353.102100
iter  40 value 352.462903
iter  50 value 352.309082
iter  60 value 352.278662
iter  70 value 352.263515
iter  80 value 352.256712
iter  90 value 352.252046
iter 100 value 352.250332
final  value 352.250332 
stopped after 100 iterations
# weights:  79
initial  value 1038.768120 
iter  10 value 360.357419
iter  20 value 353.622553
iter  30 value 353.107961
iter  40 value 352.393596
iter  50 value 352.213138
iter  60 value 352.155690
iter  70 value 352.124478
iter  80 value 352.110956
iter  90 value 352.104419
iter 100 value 352.099374
final  value 352.099374 
stopped after 100 iterations
# weights:  31
initial  value 561.699931 
iter  10 value 373.607465
iter  20 value 355.508623
iter  30 value 353.463972
iter  40 value 353.185918
iter  50 value 353.109777
iter  60 value 353.103965
iter  70 value 353.103467
final  value 353.103381 
converged
# weights:  103
initial  value 861.001031 
iter  10 value 381.782491
iter  20 value 370.468540
iter  30 value 363.243992
iter  40 value 352.142233
iter  50 value 352.078847
iter  60 value 352.031580
iter  70 value 352.005710
iter  80 value 351.993939
iter  90 value 351.986786
iter 100 value 351.980811
final  value 351.980811 
stopped after 100 iterations
# weights:  7
initial  value 844.310017 
iter  10 value 423.586155
final  value 423.251050 
converged
# weights:  31
initial  value 1110.657724 
iter  10 value 550.754707
iter  20 value 547.316787
iter  30 value 547.285830
iter  40 value 547.269622
iter  50 value 545.057331
iter  60 value 521.718773
iter  70 value 521.325192
iter  80 value 521.245521
iter  90 value 521.229285
iter 100 value 521.216139
final  value 521.216139 
stopped after 100 iterations
Neural Networks with Feature Extraction 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  size  decay         RMSE       Rsquared   MAE        Selected
   1    4.828817e+00  0.8731928  0.8612771  0.7202374          
   5    2.001656e-04  0.8330094  0.4510224  0.6370025  *       
   5    1.800074e-02  0.8334378  0.4613136  0.6424702          
   6    1.021320e-05  0.8474741  0.1945558  0.6836820          
   6    6.637047e-05  0.8331205  0.4547866  0.6383921          
   7    4.343504e-03  0.8331986  0.4534257  0.6403700          
   7    3.787248e-01  0.8367830  0.5747209  0.6588017          
   8    7.006547e-03  0.8332600  0.4530914  0.6411994          
  12    7.197236e-02  0.8337733  0.4891328  0.6434949          
  13    1.969011e-03  0.8331040  0.4529951  0.6390133          
  13    6.081004e-01  0.8383564  0.5864476  0.6658182          
  16    1.553060e-03  0.8330981  0.4522658  0.6389590          
  17    2.583383e-04  0.8331156  0.4485250  0.6397654          
  17    6.085167e-04  0.8330722  0.4516998  0.6386935          
  17    2.138257e-03  0.8331254  0.4527498  0.6394660          
  17    9.728275e+00  0.8652841  0.8283401  0.7140307          
  18    2.092909e-04  0.8397133  0.4529003  0.6608859          
  18    4.458840e-03  0.8332145  0.4515555  0.6407621          
  19    5.247859e-04  0.8330821  0.4521778  0.6388356          
  20    1.238569e-04  0.8396933  0.4315714  0.6603639          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 5 and decay = 0.0002001656.
[1] "Mon Mar 12 04:56:45 2018"
Principal Component Analysis 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  ncomp  RMSE         Rsquared   MAE          Selected
  1      0.268112429  0.8251338  0.206802171          
  2      0.128480892  0.9600727  0.104041200          
  3      0.053958870  0.9929124  0.043256798          
  4      0.035747779  0.9969107  0.027146057          
  5      0.018893888  0.9991449  0.014928607          
  6      0.008294213  0.9998346  0.006282741          
  7      0.008273073  0.9998353  0.006252607          
  8      0.008190933  0.9998383  0.006279708  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 8.
[1] "Mon Mar 12 04:56:51 2018"
Installing package into 'C:/Users/John/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
No more significant predictors (<0.0573327379766852) found
Warning only 8 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
No more significant predictors (<0.0364306780975312) found
Warning only 8 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
No more significant predictors (<0.0573327379766852) found
Warning only 8 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
No more significant predictors (<0.0364306780975312) found
Warning only 8 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
No more significant predictors (<0.0573327379766852) found
Warning only 8 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
No more significant predictors (<0.0364306780975312) found
Warning only 8 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

Partial Least Squares Generalized Linear Models  

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  nt  alpha.pvals.expli  RMSE         Rsquared   MAE          Selected
  1   0.1894613570       0.218161057  0.8841297  0.173238453          
  2   0.0433796509       0.044823579  0.9951276  0.033150144          
  3   0.0003053938       0.026181607  0.9983423  0.018778110          
  3   0.0273991627       0.026181607  0.9983423  0.018778110          
  3   0.1085096810       0.026181607  0.9983423  0.018778110          
  4   0.0879280068       0.018263452  0.9991978  0.014542354          
  4   0.0948501341       0.018263452  0.9991978  0.014542354          
  4   0.1526107906       0.018208612  0.9992018  0.014500409          
  6   0.0764749352       0.008773268  0.9998166  0.006772857          
  6   0.1285721912       0.008760018  0.9998172  0.006767041          
  6   0.1594658426       0.008760018  0.9998172  0.006767041          
  7   0.0730396079       0.008138401  0.9998405  0.006240917          
  8   0.0440250138       0.007980272  0.9998471  0.006080954          
  8   0.0470729623       0.007980272  0.9998471  0.006080954          
  8   0.0594757489       0.007971998  0.9998474  0.006078384          
  8   0.0776686625       0.007971998  0.9998474  0.006078384          
  8   0.0883073962       0.007971998  0.9998474  0.006078384  *       
  8   0.1996011952       0.007977078  0.9998472  0.006083402          
  9   0.0364306781       0.007980568  0.9998471  0.006081346          
  9   0.0573327380       0.007971998  0.9998474  0.006078384          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nt = 8 and alpha.pvals.expli
 = 0.0883074.
[1] "Mon Mar 12 04:57:32 2018"
Projection Pursuit Regression 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  nterms  RMSE         Rsquared   MAE          Selected
   1      0.005881951  0.9999162  0.004514619          
   2      0.005615615  0.9999240  0.004215136          
   3      0.004538589  0.9999487  0.003490178          
   4      0.004511687  0.9999502  0.003426650          
   5      0.004186843  0.9999582  0.003204918          
   6      0.004047369  0.9999608  0.003040251          
   7      0.004049851  0.9999608  0.003072664          
   8      0.004151277  0.9999592  0.003162765          
   9      0.004023511  0.9999616  0.003052023  *       
  10      0.004038873  0.9999612  0.003047295          
  11      0.004060053  0.9999608  0.003076958          
  12      0.004244843  0.9999569  0.003206397          
  13      0.004145168  0.9999589  0.003158017          
  14      0.004106009  0.9999603  0.003150101          
  15      0.004216576  0.9999580  0.003211934          
  16      0.004201676  0.9999584  0.003219452          
  17      0.004226163  0.9999582  0.003267776          
  18      0.004261964  0.9999576  0.003265427          
  19      0.004407281  0.9999546  0.003352636          
  20      0.004377096  0.9999551  0.003368736          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nterms = 9.
[1] "Mon Mar 12 04:57:48 2018"
Installing package into 'C:/Users/John/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Quantile Random Forest 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE         Selected
  1     0.18687146  0.9887720  0.17241894          
  2     0.13927301  0.9925474  0.12664521          
  3     0.12376701  0.9930133  0.11059293          
  4     0.11406944  0.9935516  0.10096616          
  6     0.10220958  0.9941304  0.08967159          
  7     0.09818259  0.9940828  0.08594647          
  8     0.09526654  0.9939843  0.08303893          
  9     0.09241461  0.9939127  0.08031603  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 9.
[1] "Mon Mar 12 04:58:57 2018"
Random Forest 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE        Rsquared   MAE         Selected
  1     maxstat     0.07165509  0.9916998  0.05186596          
  2     variance    0.03967890  0.9966091  0.02880130          
  3     extratrees  0.03073072  0.9979783  0.02189008          
  3     variance    0.03681870  0.9969408  0.02690829          
  4     extratrees  0.02916480  0.9981305  0.02099021          
  4     maxstat     0.03586445  0.9971940  0.02531737          
  6     extratrees  0.02766053  0.9982778  0.01970353          
  6     maxstat     0.03692417  0.9969304  0.02617864          
  7     extratrees  0.02747112  0.9982998  0.01939910          
  8     extratrees  0.02704600  0.9983373  0.01909417  *       
  8     maxstat     0.03818729  0.9966578  0.02733623          
  8     variance    0.03918265  0.9963179  0.02915518          
  9     variance    0.04033082  0.9960696  0.03007590          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mtry = 8 and splitrule = extratrees.
[1] "Mon Mar 12 04:59:31 2018"
Error in code$varImp(object$finalModel, ...) : 
  No importance values available
In addition: Warning messages:
1: package 'gpls' is not available (for R version 3.4.2) 
2: package 'rPython' is not available (for R version 3.4.2) 
Random Forest 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  predFixed  RMSE        Rsquared   MAE         Selected
  1          0.05657013  0.9940479  0.04040226          
  2          0.04137777  0.9963655  0.02982243          
  3          0.03821202  0.9967617  0.02776356          
  4          0.03767494  0.9967591  0.02744250  *       
  6          0.03914926  0.9964111  0.02861713          
  7          0.03990374  0.9962346  0.02931338          
  8          0.04090883  0.9960221  0.03034564          
  9          0.04196247  0.9957889  0.03120369          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was predFixed = 4.
[1] "Mon Mar 12 05:01:59 2018"
Relaxed Lasso 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  phi          lambda     RMSE        Rsquared   MAE        Selected
  0.001526969   301.0869  0.82820115  0.9967288  0.8156442          
  0.136995813   319.1181  0.75019835  0.9971065  0.7399873          
  0.182153390  3635.0775  1.69020426        NaN  1.5640596          
  0.216898255   244.4606  0.70423632  0.9973144  0.6953632          
  0.220125069  2709.2426  1.69020426        NaN  1.5640596          
  0.235364811  2153.0422  1.69020426        NaN  1.5640596          
  0.286663690  3081.2943  1.69020426        NaN  1.5640596          
  0.297378745  2169.1870  1.69020426        NaN  1.5640596          
  0.365198040  1777.3890  0.41085003  0.9714814  0.3854800          
  0.382374676  1125.4767  0.42315791  0.9714814  0.3972838          
  0.388343312  2140.5535  1.69020426        NaN  1.5640596          
  0.439640034   380.3662  0.57636653  0.9978205  0.5709660          
  0.441536981  2665.1627  1.69020426        NaN  1.5640596          
  0.474250670   457.3275  0.55654335  0.9978877  0.5516365          
  0.542548405   255.5672  0.51747513  0.9980097  0.5134935          
  0.642860956   935.6991  0.30798397  0.9913905  0.2694568          
  0.763053953   378.8393  0.39199575  0.9982910  0.3903451          
  0.797329213   974.5569  0.37891967  0.9904003  0.3382170          
  0.947306785   114.0730  0.09430683  0.9996801  0.0935971  *       
  0.998005976  2306.6777  1.69020426        NaN  1.5640596          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 114.073 and phi = 0.9473068.
[1] "Mon Mar 12 05:02:06 2018"
Random Forest 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE         Selected
  1     0.05281158  0.9946868  0.03775805          
  2     0.04016202  0.9964832  0.02938745          
  3     0.03670247  0.9969692  0.02689231          
  4     0.03572930  0.9970752  0.02635765  *       
  6     0.03684241  0.9967859  0.02725759          
  7     0.03817196  0.9965234  0.02834262          
  8     0.03901179  0.9963501  0.02904106          
  9     0.04024220  0.9960961  0.03010660          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 4.
[1] "Mon Mar 12 05:02:55 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Ridge Regression 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  lambda        RMSE         Rsquared   MAE          Selected
  1.146087e-05  0.007977730  0.9998472  0.006080027  *       
  2.122499e-04  0.008381655  0.9998317  0.006375158          
  2.516173e-04  0.008473644  0.9998281  0.006448146          
  4.713494e-04  0.009005744  0.9998062  0.006857479          
  5.889412e-04  0.009284540  0.9997941  0.007074642          
  1.135993e-03  0.010425962  0.9997407  0.008004697          
  1.153627e-03  0.010458507  0.9997391  0.008030838          
  2.336223e-03  0.012204990  0.9996458  0.009413970          
  3.623328e-02  0.027308188  0.9988969  0.021074552          
  4.234253e-02  0.029816768  0.9988072  0.023071904          
  7.348968e-02  0.043196771  0.9983756  0.034456475          
  4.228193e-01  0.199010279  0.9926955  0.162518392          
  8.616960e-01  0.377333577  0.9835801  0.307431514          
  8.811075e-01  0.384739779  0.9831704  0.313463435          
  9.066781e-01  0.394437144  0.9826319  0.321361348          
  1.147254e+00  0.482521195  0.9776668  0.393108586          
  1.994819e+00  0.753370712  0.9624736  0.614001239          
  2.124149e+00  0.790056108  0.9604946  0.643932922          
  3.476897e+00  1.117284980  0.9441434  0.911041686          
  6.547384e+00  1.610239853  0.9240528  1.314053034          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was lambda = 1.146087e-05.
[1] "Mon Mar 12 05:03:02 2018"
Robust Linear Model 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  intercept  psi           RMSE         Rsquared   MAE          Selected
  FALSE      psi.huber     0.009798621  0.9997783  0.007249195          
  FALSE      psi.hampel    0.009758753  0.9997783  0.007245594          
  FALSE      psi.bisquare  0.010034466  0.9997734  0.007260893          
   TRUE      psi.huber     0.008216984  0.9998387  0.006064361          
   TRUE      psi.hampel    0.008091760  0.9998431  0.006053331  *       
   TRUE      psi.bisquare  0.008810601  0.9998156  0.006142220          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were intercept = TRUE and psi = psi.hampel.
[1] "Mon Mar 12 05:03:08 2018"
CART 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  cp            RMSE        Rsquared   MAE         Selected
  0.000000e+00  0.08298728  0.9833120  0.06519829          
  3.986750e-05  0.08298728  0.9833120  0.06519829  *       
  5.115541e-05  0.08331173  0.9831789  0.06550053          
  5.949006e-05  0.08334438  0.9831675  0.06553729          
  7.880249e-05  0.08334438  0.9831675  0.06553729          
  1.264515e-04  0.08365091  0.9830469  0.06576023          
  1.401098e-04  0.08445179  0.9827246  0.06642450          
  1.450167e-04  0.08445179  0.9827246  0.06642450          
  1.784702e-04  0.08548444  0.9822928  0.06787483          
  2.503336e-04  0.08792369  0.9812961  0.06980258          
  4.224123e-04  0.09492438  0.9782036  0.07533257          
  4.646118e-04  0.09626173  0.9776010  0.07616062          
  5.129015e-04  0.09626173  0.9776010  0.07616062          
  9.561535e-04  0.10746546  0.9721945  0.08627711          
  9.904114e-04  0.10746546  0.9721945  0.08627711          
  1.639484e-03  0.11325073  0.9692362  0.09154490          
  2.325087e-02  0.20852106  0.8946047  0.16713137          
  6.723642e-01  0.45921378  0.6630375  0.38196574          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was cp = 3.98675e-05.
[1] "Mon Mar 12 05:03:15 2018"
CART 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results:

  RMSE       Rsquared   MAE      
  0.1571443  0.9406786  0.1272914

[1] "Mon Mar 12 05:03:21 2018"
CART 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  maxdepth  RMSE       Rsquared   MAE        Selected
   1        0.3677881  0.6707241  0.3061006          
   2        0.3065748  0.7715681  0.2446085          
   8        0.1571443  0.9406786  0.1272914  *       
   9        0.1571443  0.9406786  0.1272914          
  10        0.1571443  0.9406786  0.1272914          
  14        0.1571443  0.9406786  0.1272914          
  15        0.1571443  0.9406786  0.1272914          
  17        0.1571443  0.9406786  0.1272914          
  19        0.1571443  0.9406786  0.1272914          
  22        0.1571443  0.9406786  0.1272914          
  23        0.1571443  0.9406786  0.1272914          
  24        0.1571443  0.9406786  0.1272914          
  27        0.1571443  0.9406786  0.1272914          
  28        0.1571443  0.9406786  0.1272914          
  29        0.1571443  0.9406786  0.1272914          
  30        0.1571443  0.9406786  0.1272914          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was maxdepth = 8.
[1] "Mon Mar 12 05:03:28 2018"
Quantile Regression with LASSO penalty 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  lambda        RMSE         Rsquared   MAE          Selected
  1.146087e-05  0.008325146  0.9998359  0.006055505          
  2.122499e-04  0.008254939  0.9998385  0.006064016  *       
  2.516173e-04  0.008271968  0.9998376  0.006088804          
  4.713494e-04  0.008379976  0.9998336  0.006200743          
  5.889412e-04  0.008349961  0.9998349  0.006191185          
  1.135993e-03  0.008297731  0.9998375  0.006187271          
  1.153627e-03  0.008296929  0.9998375  0.006189023          
  2.336223e-03  0.008467280  0.9998317  0.006332613          
  3.623328e-02  0.015093385  0.9994713  0.010980392          
  4.234253e-02  0.021669778  0.9989052  0.015917655          
  7.348968e-02  0.070002187  0.9888912  0.054038836          
  4.228193e-01  0.198691786  0.9714814  0.162243340          
  8.616960e-01  0.641206513        NaN  0.528342079          
  8.811075e-01  0.641204616        NaN  0.528342271          
  9.066781e-01  0.641202530        NaN  0.528342485          
  1.147254e+00  0.641193884        NaN  0.528343385          
  1.994819e+00  0.641190950        NaN  0.528343696          
  2.124149e+00  0.641190949        NaN  0.528343696          
  3.476897e+00  0.641214747        NaN  0.528341254          
  6.547384e+00  0.641227331        NaN  0.528340031          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was lambda = 0.0002122499.
[1] "Mon Mar 12 05:03:35 2018"
Non-Convex Penalized Quantile Regression 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  lambda        penalty  RMSE         Rsquared   MAE          Selected
  1.146087e-05  SCAD     0.008335791  0.9998357  0.006052063          
  2.122499e-04  MCP      0.008335791  0.9998357  0.006052063          
  2.516173e-04  SCAD     0.008335791  0.9998357  0.006052063          
  4.713494e-04  MCP      0.008335791  0.9998357  0.006052063          
  5.889412e-04  MCP      0.008335791  0.9998357  0.006052063  *       
  1.135993e-03  SCAD     0.008447159  0.9998303  0.006192800          
  1.153627e-03  MCP      0.008447159  0.9998303  0.006192800          
  2.336223e-03  MCP      0.008490832  0.9998283  0.006255942          
  3.623328e-02  SCAD     0.013820541  0.9995488  0.010648237          
  4.234253e-02  SCAD     0.017859672  0.9992660  0.012181958          
  7.348968e-02  MCP      0.066774228  0.9895311  0.050888255          
  4.228193e-01  MCP      0.134562123  0.9714814  0.108236115          
  8.616960e-01  MCP      0.641206513        NaN  0.528342079          
  8.811075e-01  MCP      0.641204616        NaN  0.528342271          
  9.066781e-01  MCP      0.641202530        NaN  0.528342485          
  1.147254e+00  SCAD     0.641193884        NaN  0.528343385          
  1.994819e+00  MCP      0.641190950        NaN  0.528343696          
  2.124149e+00  MCP      0.641190949        NaN  0.528343696          
  3.476897e+00  MCP      0.641214747        NaN  0.528341254          
  6.547384e+00  MCP      0.641227331        NaN  0.528340031          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 0.0005889412 and penalty
 = MCP.
[1] "Mon Mar 12 05:03:43 2018"
Regularized Random Forest 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  mtry  coefReg      coefImp     RMSE        Rsquared   MAE         Selected
  1     0.947306785  0.26976855  0.05305563  0.9946197  0.03808100          
  2     0.216898255  0.19256424  0.03915083  0.9966856  0.02857804          
  3     0.001526969  0.14498439  0.03550478  0.9971834  0.02624598          
  3     0.136995813  0.03103932  0.03679701  0.9969460  0.02699461          
  3     0.542548405  0.65755735  0.03626053  0.9970529  0.02658761          
  4     0.439640034  0.94142463  0.03545445  0.9970970  0.02635536  *       
  4     0.474250670  0.34611686  0.03595651  0.9970265  0.02661528          
  4     0.763053953  0.74436394  0.03570868  0.9970624  0.02640265          
  6     0.382374676  0.36664330  0.03702599  0.9967607  0.02742325          
  6     0.642860956  0.55462914  0.03711681  0.9967383  0.02747379          
  6     0.797329213  0.43026772  0.03679517  0.9968057  0.02742929          
  7     0.365198040  0.74504368  0.03847194  0.9964741  0.02843537          
  8     0.220125069  0.35439635  0.03892963  0.9963594  0.02903658          
  8     0.235364811  0.49584545  0.03901984  0.9963372  0.02901930          
  8     0.297378745  0.70333525  0.03895362  0.9963517  0.02904855          
  8     0.388343312  0.88301748  0.03890683  0.9963817  0.02895925          
  8     0.441536981  0.81211547  0.03919761  0.9963059  0.02913767          
  8     0.998005976  0.87309525  0.03906194  0.9963302  0.02899185          
  9     0.182153390  0.98027775  0.03919265  0.9962982  0.02931172          
  9     0.286663690  0.97057208  0.03916261  0.9963054  0.02928349          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mtry = 4, coefReg = 0.43964
 and coefImp = 0.9414246.
[1] "Mon Mar 12 05:07:55 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: Warning messages:
1: In rlm.default(x, y, weights, method = method, wt.method = wt.method,  :
  'rlm' failed to converge in 20 steps
2: In rlm.default(x, y, weights, method = method, wt.method = wt.method,  :
  'rlm' failed to converge in 20 steps
3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
5: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Regularized Random Forest 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  mtry  coefReg      RMSE        Rsquared   MAE         Selected
  1     0.947306785  0.05256832  0.9948140  0.03823367          
  2     0.216898255  0.03939873  0.9966141  0.02856863          
  3     0.001526969  0.03572142  0.9971521  0.02616152          
  3     0.136995813  0.03650320  0.9970055  0.02669696          
  3     0.542548405  0.03633176  0.9970147  0.02664848          
  4     0.439640034  0.03561278  0.9970890  0.02645685  *       
  4     0.474250670  0.03574686  0.9970549  0.02634562          
  4     0.763053953  0.03579310  0.9970402  0.02633407          
  6     0.382374676  0.03717054  0.9967278  0.02758104          
  6     0.642860956  0.03709328  0.9967441  0.02758752          
  6     0.797329213  0.03695854  0.9967752  0.02742685          
  7     0.365198040  0.03815295  0.9965193  0.02821034          
  8     0.220125069  0.03888602  0.9963646  0.02891998          
  8     0.235364811  0.03871224  0.9963979  0.02896096          
  8     0.297378745  0.03908109  0.9963237  0.02908923          
  8     0.388343312  0.03896124  0.9963545  0.02903181          
  8     0.441536981  0.03898927  0.9963486  0.02904344          
  8     0.998005976  0.03921952  0.9963058  0.02914299          
  9     0.182153390  0.03957970  0.9962172  0.02952196          
  9     0.286663690  0.03976282  0.9961867  0.02968185          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mtry = 4 and coefReg = 0.43964.
[1] "Mon Mar 12 05:09:53 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
In addition: Warning messages:
1: model fit failed for Resample09: parameter=none Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
2: model fit failed for Resample18: parameter=none Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) : 
  the leading minor of order 504 is not positive definite
 
3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 05:16:30 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "rvmLinear"                      
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
In addition: There were 46 warnings (use warnings() to see them)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 06:11:11 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "rvmPoly"                        
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 06:32:15 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "rvmRadial"                      
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |===                                                                   |   5%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |===                                                                   |   5%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%
Subtractive Clustering and Fuzzy c-Means Rules 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  r.a  eps.high   eps.low      RMSE       Rsquared   MAE         Selected
   1   0.8231573  0.099664698  0.1171106  0.9764796  0.08639958  *       
   1   0.8544348  0.268525645  0.1171106  0.9764796  0.08639958          
   3   0.9172582  0.327368467        NaN        NaN         NaN          
   5   0.5586216  0.447599182        NaN        NaN         NaN          
   5   0.8398758  0.708438625        NaN        NaN         NaN          
   5   0.8695538  0.727089552        NaN        NaN         NaN          
   6   0.3842208  0.186653432        NaN        NaN         NaN          
   6   0.9788516  0.612386382        NaN        NaN         NaN          
   7   0.9789608  0.005010948        NaN        NaN         NaN          
   8   0.2388115  0.175293773        NaN        NaN         NaN          
   8   0.2890333  0.225535774        NaN        NaN         NaN          
   8   0.8899366  0.273556882        NaN        NaN         NaN          
   9   0.8387063  0.094950396        NaN        NaN         NaN          
  13   0.2628139  0.057226161        NaN        NaN         NaN          
  13   0.4401107  0.214645969        NaN        NaN         NaN          
  16   0.7353590  0.154471486        NaN        NaN         NaN          
  17   0.4641205  0.433973492        NaN        NaN         NaN          
  17   0.8295477  0.414300181        NaN        NaN         NaN          
  17   0.8644114  0.217183894        NaN        NaN         NaN          
  20   0.3147245  0.296233668        NaN        NaN         NaN          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were r.a = 1, eps.high = 0.8231573
 and eps.low = 0.0996647.
[1] "Mon Mar 12 07:07:09 2018"
Partial Least Squares 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  ncomp  RMSE         Rsquared   MAE          Selected
  1      0.220586693  0.8816199  0.168898378          
  2      0.110245877  0.9705719  0.088226323          
  3      0.048460227  0.9942869  0.038532560          
  4      0.025462455  0.9984341  0.019718348          
  5      0.014000037  0.9995275  0.010876852          
  6      0.008225863  0.9998372  0.006247250          
  7      0.008094245  0.9998423  0.006193119          
  8      0.008029662  0.9998448  0.006198711  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 8.
[1] "Mon Mar 12 07:07:16 2018"
Spike and Slab Regression 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  vars  RMSE         Rsquared   MAE          Selected
  1     0.311139319  0.9714814  0.254175383          
  2     0.201183284  0.9886457  0.164764218          
  3     0.028602406  0.9983458  0.022242129          
  4     0.012026389  0.9996555  0.009288641          
  6     0.010178943  0.9997514  0.007976661          
  7     0.009194539  0.9997970  0.007179226          
  8     0.008770200  0.9998159  0.006691715          
  9     0.008617615  0.9998218  0.006570391  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was vars = 9.
[1] "Mon Mar 12 07:07:33 2018"
Sparse Partial Least Squares 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  kappa        eta          K  RMSE         Rsquared   MAE          Selected
  0.004934815  0.947306785  3  0.031434720  0.9976071  0.025190162          
  0.110570630  0.216898255  2  0.042770818  0.9955516  0.031104693          
  0.116728370  0.542548405  6  0.010570201  0.9997356  0.008071933          
  0.139445250  0.001526969  2  0.042770818  0.9955516  0.031104693          
  0.147505993  0.136995813  1  0.218161057  0.8841297  0.173238453          
  0.171281312  0.763053953  7  0.008805959  0.9998131  0.006494807          
  0.171838771  0.439640034  9  0.007977303  0.9998473  0.006072960  *       
  0.197376190  0.474250670  4  0.018598774  0.9991720  0.014720616          
  0.296592303  0.642860956  5  0.016472736  0.9993487  0.012553544          
  0.302231400  0.797329213  4  0.018096489  0.9992172  0.014265090          
  0.322185529  0.382374676  4  0.018598774  0.9991720  0.014720616          
  0.385512900  0.365198040  7  0.008805959  0.9998131  0.006494807          
  0.411279505  0.388343312  8  0.008082125  0.9998428  0.006194063          
  0.412085743  0.235364811  5  0.016472736  0.9993487  0.012553544          
  0.413121096  0.297378745  7  0.008805959  0.9998131  0.006494807          
  0.421638289  0.998005976  8  0.008122059  0.9998412  0.006217726          
  0.441658633  0.441536981  8  0.008082125  0.9998428  0.006194063          
  0.443932076  0.220125069  4  0.018598774  0.9991720  0.014720616          
  0.461765986  0.286663690  9  0.007977303  0.9998473  0.006072960          
  0.484672318  0.182153390  9  0.007977303  0.9998473  0.006072960          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were K = 9, eta = 0.43964 and kappa
 = 0.1718388.
[1] "Mon Mar 12 07:07:42 2018"
Supervised Principal Component Analysis 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  threshold    n.components  RMSE      Rsquared   MAE       Selected
  0.009869629  3             1.565224  0.9929124  1.564292  *       
  0.221141259  1             1.586744  0.8251338  1.563927          
  0.233456740  2             1.569258  0.9600727  1.564019          
  0.278890499  1             1.586744  0.8251338  1.563927          
  0.295011987  1             1.586744  0.8251338  1.563927          
  0.342562624  3             1.565224  0.9929124  1.564292          
  0.343677541  2             1.569258  0.9600727  1.564019          
  0.394752379  2             1.569258  0.9600727  1.564019          
  0.593184605  2             1.569258  0.9600727  1.564019          
  0.604462799  3             1.565224  0.9929124  1.564292          
  0.644371057  2             1.569258  0.9600727  1.564019          
  0.771025799  2             1.569258  0.9600727  1.564019          
  0.822559009  2             1.569258  0.9600727  1.564019          
  0.824171485  1             1.586744  0.8251338  1.563927          
  0.826242191  1             1.586744  0.8251338  1.563927          
  0.843276577  3             1.565224  0.9929124  1.564292          
  0.883317267  2             1.569258  0.9600727  1.564019          
  0.887864152  1             1.586744  0.8251338  1.563927          
  0.923531971  1             1.586744  0.8251338  1.563927          
  0.969344636  1             1.586744  0.8251338  1.563927          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were threshold = 0.009869629
 and n.components = 3.
[1] "Mon Mar 12 07:07:48 2018"
Error : 'x' should be a character matrix with a single column for string kernel methods
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 07:07:54 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "svmBoundrangeString"            
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 07:08:00 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "svmExpoString"                  
Support Vector Machines with Linear Kernel 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  C             RMSE        Rsquared   MAE         Selected
    0.03462708  0.02733963  0.9987587  0.02159698          
    0.31146002  0.02520837  0.9990047  0.02051378  *       
    0.35400732  0.02520837  0.9990047  0.02051378          
    0.56776172  0.02520837  0.9990047  0.02051378          
    0.67136992  0.02520837  0.9990047  0.02051378          
    1.10071377  0.02520837  0.9990047  0.02051378          
    1.11354752  0.02520837  0.9990047  0.02051378          
    1.89380224  0.02520837  0.9990047  0.02051378          
   14.90546084  0.02520837  0.9990047  0.02051378          
   16.75990412  0.02520837  0.9990047  0.02051378          
   25.37904465  0.02520837  0.9990047  0.02051378          
   94.70620139  0.02520837  0.9990047  0.02051378          
  161.83559421  0.02520837  0.9990047  0.02051378          
  164.57167928  0.02520837  0.9990047  0.02051378          
  168.15325146  0.02520837  0.9990047  0.02051378          
  200.73503951  0.02520837  0.9990047  0.02051378          
  304.38616971  0.02520837  0.9990047  0.02051378          
  319.12156270  0.02520837  0.9990047  0.02051378          
  462.39422442  0.02520837  0.9990047  0.02051378          
  744.52142033  0.02520837  0.9990047  0.02051378          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was C = 0.31146.
[1] "Mon Mar 12 07:08:08 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Linear Kernel 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  cost          RMSE        Rsquared   MAE         Selected
    0.03462708  0.02733963  0.9987587  0.02159698          
    0.31146002  0.02520837  0.9990047  0.02051378  *       
    0.35400732  0.02520837  0.9990047  0.02051378          
    0.56776172  0.02520837  0.9990047  0.02051378          
    0.67136992  0.02520837  0.9990047  0.02051378          
    1.10071377  0.02520837  0.9990047  0.02051378          
    1.11354752  0.02520837  0.9990047  0.02051378          
    1.89380224  0.02520837  0.9990047  0.02051378          
   14.90546084  0.02520837  0.9990047  0.02051378          
   16.75990412  0.02520837  0.9990047  0.02051378          
   25.37904465  0.02520837  0.9990047  0.02051378          
   94.70620139  0.02520837  0.9990047  0.02051378          
  161.83559421  0.02520837  0.9990047  0.02051378          
  164.57167928  0.02520837  0.9990047  0.02051378          
  168.15325146  0.02520837  0.9990047  0.02051378          
  200.73503951  0.02520837  0.9990047  0.02051378          
  304.38616971  0.02520837  0.9990047  0.02051378          
  319.12156270  0.02520837  0.9990047  0.02051378          
  462.39422442  0.02520837  0.9990047  0.02051378          
  744.52142033  0.02520837  0.9990047  0.02051378          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was cost = 0.31146.
[1] "Mon Mar 12 07:08:15 2018"
L2 Regularized Support Vector Machine (dual) with Linear Kernel 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  cost          Loss  RMSE        Rsquared   MAE         Selected
    0.00111975  L2    0.09394898  0.9803733  0.07487329          
    0.02094610  L1    0.09787789  0.9775214  0.07875001          
    0.02484561  L2    0.05118134  0.9945520  0.04303627          
    0.04664318  L1    0.09532843  0.9787673  0.07729263          
    0.05832425  L1    0.09491406  0.9789821  0.07704203          
    0.11275415  L2    0.04413096  0.9956877  0.03552122          
    0.11451042  L1    0.09415431  0.9794081  0.07663873          
    0.23245890  L1    0.09378500  0.9796120  0.07638049          
    3.63937850  L2    0.04344167  0.9957211  0.03485484          
    4.25528567  L2    0.04464214  0.9957444  0.03507767          
    7.39946592  L1    0.09345925  0.9798033  0.07617305          
   42.82899707  L1    0.09345096  0.9798082  0.07616678          
   87.49812436  L1    0.09345008  0.9798087  0.07616611          
   89.47605244  L1    0.09345006  0.9798087  0.07616609          
   92.08178022  L1    0.09345004  0.9798088  0.07616607          
  116.60869405  L2    0.04329452  0.9957301  0.03448024  *       
  203.14208330  L1    0.09344960  0.9798090  0.07616574          
  216.35895797  L1    0.09344958  0.9798090  0.07616572          
  354.74521725  L1    0.09344944  0.9798091  0.07616562          
  669.47794961  L1    0.09344934  0.9798092  0.07616555          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were cost = 116.6087 and Loss = L2.
[1] "Mon Mar 12 07:08:23 2018"
Support Vector Machines with Polynomial Kernel 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  degree  scale         C             RMSE        Rsquared   MAE       
  1       1.018813e-05    0.14109764  0.63910309  0.8963570  0.52655070
  1       5.323765e-05    0.04315256  0.63785866  0.8963570  0.52553681
  1       1.411836e-04    0.23140046  0.59431709  0.8987126  0.49014273
  1       7.517360e-03   29.10833024  0.02520437  0.9990053  0.02051233
  1       1.051240e+00    0.51638830  0.02521240  0.9990045  0.02051905
  2       1.064093e-03    1.41386997  0.04467503  0.9972454  0.03578918
  2       2.140647e-03  556.93666337  0.03335389  0.9990465  0.03032237
  2       3.266006e-03    1.14215056  0.03435074  0.9984357  0.02679508
  2       2.557556e-02    9.98274847  0.03291847  0.9988092  0.02782618
  2       1.109103e-01   71.77746464  0.03424896  0.9981167  0.02943481
  2       1.685257e-01    2.73969603  0.03387356  0.9982200  0.02901453
  3       9.238481e-05  834.15189697  0.02632925  0.9990371  0.02193913
  3       1.468553e-04    1.24482704  0.14584330  0.9778574  0.11987757
  3       1.768789e-04    5.41770479  0.04542077  0.9971115  0.03639817
  3       3.308360e-04  754.08386205  0.02912971  0.9990599  0.02532404
  3       3.770628e-04   46.85167730  0.02940767  0.9988601  0.02462574
  3       8.628310e-04   72.28654694  0.03267403  0.9990499  0.02954390
  3       1.144510e-03  303.43887952  0.03353969  0.9990466  0.03050615
  3       2.190790e-03  145.18333055  0.03576023  0.9990109  0.03272744
  3       1.951909e+00  273.69564846  0.03953938  0.9965922  0.03366079
  Selected
          
          
          
  *       
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were degree = 1, scale = 0.00751736 and C
 = 29.10833.
[1] "Mon Mar 12 07:08:32 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Support Vector Machines with Radial Basis Function Kernel 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  sigma        C             RMSE        Rsquared   MAE         Selected
  0.003466281    0.86762337  0.09453285  0.9821329  0.05788207          
  0.004721804    0.36883258  0.12405513  0.9713203  0.07069687          
  0.004892829    0.07206762  0.30754546  0.8558799  0.18685420          
  0.022414828    0.28254218  0.13977595  0.9644961  0.07917903          
  0.025588694    1.23776720  0.08866348  0.9840933  0.05580024          
  0.053370858    0.36552374  0.12463139  0.9711045  0.07104757          
  0.059247718  772.15379624  0.08216239  0.9864610  0.05396332          
  0.072080732  144.96016570  0.08216239  0.9864610  0.05396332          
  0.118481710    0.10786873  0.24807767  0.9020652  0.14299701          
  0.179359516   30.36860909  0.08216239  0.9864610  0.05396332          
  0.295448830    3.27345320  0.08216239  0.9864610  0.05396332  *       
  0.380737573  519.73256080  0.08216239  0.9864610  0.05396332          
  0.432840229   56.34753631  0.08216239  0.9864610  0.05396332          
  0.881296710   16.95850895  0.08216239  0.9864610  0.05396332          
  1.015227986    0.07309629  0.30539459  0.8576924  0.18515281          
  1.316383211   17.45822598  0.08216239  0.9864610  0.05396332          
  1.328532740    3.71160163  0.08216239  0.9864610  0.05396332          
  1.623093658   63.86156931  0.08216239  0.9864610  0.05396332          
  1.670170124    0.35547539  0.12631366  0.9704181  0.07196643          
  1.774855266    0.06703277  0.31861827  0.8465383  0.19593242          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were sigma = 0.2954488 and C = 3.273453.
[1] "Mon Mar 12 07:08:42 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  C             RMSE        Rsquared   MAE         Selected
    0.03462708  0.42709228  0.7610689  0.29484312          
    0.31146002  0.13408717  0.9671973  0.07609309          
    0.35400732  0.12654232  0.9703237  0.07209735          
    0.56776172  0.10584398  0.9780088  0.06145830          
    0.67136992  0.10115063  0.9797403  0.06008957          
    1.10071377  0.09036587  0.9835379  0.05628881          
    1.11354752  0.09020861  0.9835943  0.05623361          
    1.89380224  0.08340016  0.9860099  0.05437380          
   14.90546084  0.08216239  0.9864610  0.05396332  *       
   16.75990412  0.08216239  0.9864610  0.05396332          
   25.37904465  0.08216239  0.9864610  0.05396332          
   94.70620139  0.08216239  0.9864610  0.05396332          
  161.83559421  0.08216239  0.9864610  0.05396332          
  164.57167928  0.08216239  0.9864610  0.05396332          
  168.15325146  0.08216239  0.9864610  0.05396332          
  200.73503951  0.08216239  0.9864610  0.05396332          
  304.38616971  0.08216239  0.9864610  0.05396332          
  319.12156270  0.08216239  0.9864610  0.05396332          
  462.39422442  0.08216239  0.9864610  0.05396332          
  744.52142033  0.08216239  0.9864610  0.05396332          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was C = 14.90546.
[1] "Mon Mar 12 07:08:52 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  sigma        C             RMSE        Rsquared   MAE         Selected
  0.003466281    0.86762337  0.09453285  0.9821329  0.05788207          
  0.004721804    0.36883258  0.12405513  0.9713203  0.07069687          
  0.004892829    0.07206762  0.30754546  0.8558799  0.18685420          
  0.022414828    0.28254218  0.13977595  0.9644961  0.07917903          
  0.025588694    1.23776720  0.08866348  0.9840933  0.05580024          
  0.053370858    0.36552374  0.12463139  0.9711045  0.07104757          
  0.059247718  772.15379624  0.08216239  0.9864610  0.05396332          
  0.072080732  144.96016570  0.08216239  0.9864610  0.05396332          
  0.118481710    0.10786873  0.24807767  0.9020652  0.14299701          
  0.179359516   30.36860909  0.08216239  0.9864610  0.05396332          
  0.295448830    3.27345320  0.08216239  0.9864610  0.05396332  *       
  0.380737573  519.73256080  0.08216239  0.9864610  0.05396332          
  0.432840229   56.34753631  0.08216239  0.9864610  0.05396332          
  0.881296710   16.95850895  0.08216239  0.9864610  0.05396332          
  1.015227986    0.07309629  0.30539459  0.8576924  0.18515281          
  1.316383211   17.45822598  0.08216239  0.9864610  0.05396332          
  1.328532740    3.71160163  0.08216239  0.9864610  0.05396332          
  1.623093658   63.86156931  0.08216239  0.9864610  0.05396332          
  1.670170124    0.35547539  0.12631366  0.9704181  0.07196643          
  1.774855266    0.06703277  0.31861827  0.8465383  0.19593242          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were sigma = 0.2954488 and C = 3.273453.
[1] "Mon Mar 12 07:09:02 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 07:09:08 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "expoTrans"                       "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "svmSpectrumString"              
Bagged CART 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results:

  RMSE       Rsquared   MAE      
  0.1309839  0.9588191  0.1043317

[1] "Mon Mar 12 07:09:15 2018"
Partial Least Squares 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  ncomp  RMSE         Rsquared   MAE          Selected
  1      0.220586693  0.8816199  0.168898378          
  2      0.110245877  0.9705719  0.088226323          
  3      0.048460227  0.9942869  0.038532560          
  4      0.025462455  0.9984341  0.019718348          
  5      0.014000037  0.9995275  0.010876852          
  6      0.008225863  0.9998372  0.006247250          
  7      0.008094245  0.9998423  0.006193119          
  8      0.008029662  0.9998448  0.006198711  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 8.
[1] "Mon Mar 12 07:10:46 2018"
Wang and Mendel Fuzzy Rules 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  num.labels  type.mf    RMSE        Rsquared   MAE         Selected
   2          TRIANGLE   0.34449651  0.8629273  0.30260250          
   6          GAUSSIAN   0.10694530  0.9731315  0.08107148          
   6          TRAPEZOID  0.13951999  0.9726043  0.11321781          
   7          GAUSSIAN   0.09245000  0.9793832  0.07057086          
   8          TRAPEZOID  0.13745685  0.9674128  0.10174129          
   8          TRIANGLE   0.09255325  0.9796797  0.07121179          
   9          TRAPEZOID  0.13431040  0.9640783  0.09477347          
  13          TRAPEZOID  0.16036585  0.9416664  0.09628104          
  13          TRIANGLE   0.15898781  0.9341339  0.08676045          
  14          TRAPEZOID  0.19853860  0.9090424  0.10683696          
  16          TRAPEZOID  0.21914443  0.8812895  0.11688793          
  17          GAUSSIAN   0.08073266  0.9842730  0.06350567          
  17          TRAPEZOID  0.27267795  0.8121166  0.14074805          
  18          GAUSSIAN   0.08065698  0.9845631  0.06275465  *       
  18          TRAPEZOID  0.29465865  0.7879447  0.15236732          
  18          TRIANGLE   0.31116731  0.7641583  0.16557247          
  19          GAUSSIAN   0.08075865  0.9841455  0.06394425          
  20          GAUSSIAN   0.08110507  0.9841224  0.06328584          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were num.labels = 18 and type.mf = GAUSSIAN.
[1] "Mon Mar 12 08:04:22 2018"
eXtreme Gradient Boosting 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  lambda        alpha         nrounds  eta         RMSE        Rsquared 
  1.120336e-05  5.451725e-01  27       1.94586334  0.04810407  0.9945276
  1.275576e-04  1.214762e-04  20       0.06532910  0.03810190  0.9964675
  1.469886e-04  5.161097e-03  66       0.26433579  0.03894978  0.9963256
  2.480005e-04  1.017735e-05  15       0.60196771  0.03876925  0.9964094
  2.985795e-04  4.841490e-05   4       0.07287340  0.30204233  0.9932239
  5.161942e-04  6.535364e-02  75       1.12804754  0.03757408  0.9966368
  5.228628e-04  1.578339e-03  95       2.24892584  0.03758286  0.9965577
  9.413733e-04  2.351004e-03  35       1.88337411  0.03831088  0.9964168
  9.245343e-03  1.637966e-02  56       1.15562856  0.03762133  0.9965692
  1.052723e-02  9.697194e-02  44       1.46946249  0.03947611  0.9962622
  1.666692e-02  8.163443e-04  37       0.67506978  0.03658938  0.9967412
  7.163562e-02  6.698695e-04  75       0.08823399  0.03786889  0.9965630
  1.296567e-01  8.744129e-04  89       2.96851654  0.03811148  0.9965137
  1.320862e-01  1.502533e-04  50       1.31459476  0.03737545  0.9966323
  1.352729e-01  3.068271e-04  71       1.39267632  0.03747353  0.9966179
  1.645822e-01  9.773045e-01  88       1.08999537  0.05442995  0.9928834
  2.609674e-01  1.613188e-03  82       0.78113555  0.03738315  0.9966851
  2.749924e-01  1.260739e-04  36       0.49170642  0.03716358  0.9967153
  4.146292e-01  2.712180e-04  98       1.64998581  0.03826404  0.9964792
  7.026243e-01  8.142672e-05  99       0.52448704  0.03550311  0.9969943
  MAE         Selected
  0.03487436          
  0.02930909          
  0.02938469          
  0.02999388          
  0.26115007          
  0.02873724          
  0.02851467          
  0.02891810          
  0.02863382          
  0.03006022          
  0.02803272          
  0.02873325          
  0.02862986          
  0.02798792          
  0.02843880          
  0.04052078          
  0.02864334          
  0.02844833          
  0.02910507          
  0.02703312  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 99, lambda =
 0.7026243, alpha = 8.142672e-05 and eta = 0.524487.
[1] "Mon Mar 12 08:04:48 2018"
eXtreme Gradient Boosting 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  eta         max_depth  gamma      colsample_bytree  min_child_weight
  0.01959256   2         0.2429113  0.5018684          8              
  0.08784565   1         2.0065590  0.3572496          7              
  0.11634598   3         0.2177637  0.3053284         12              
  0.16259136  10         6.4862111  0.3688311          0              
  0.20832400   5         6.2779137  0.4478056         14              
  0.21328342   3         1.6390214  0.5098434          4              
  0.22061933   4         2.2502326  0.4524542         17              
  0.25873037   8         4.8982083  0.5064035         11              
  0.29801143   3         4.3819825  0.3144399         10              
  0.33322285   7         3.8520952  0.3876097          4              
  0.39487685   6         0.8811193  0.5798571         17              
  0.42229782   3         4.6422544  0.5607523         16              
  0.44687400   8         3.7601585  0.4729278         14              
  0.44728117   4         0.2941133  0.3299326          4              
  0.48745716   5         2.6037852  0.4348456         11              
  0.52398405  10         3.6333179  0.3540813         18              
  0.52992747   4         9.8950551  0.5672055         20              
  0.56491335   5         7.4964195  0.3407815          8              
  0.58237267   3         5.4999527  0.5419262          1              
  0.58818637   2         1.7482901  0.4620434         13              
  subsample  nrounds  RMSE        Rsquared   MAE         Selected
  0.5452989  296      0.06295955  0.9933477  0.04551070  *       
  0.8833055  279      0.12173831  0.9778636  0.08340996          
  0.3043524  222      0.09266916  0.9795930  0.06985805          
  0.9272171   10      0.29097973  0.9528608  0.23262699          
  0.4864249  395      0.21409238  0.9371808  0.15695811          
  0.4894500  888      0.11804333  0.9793322  0.08260689          
  0.8755377  645      0.13244367  0.9688692  0.09575165          
  0.5342285  605      0.18516331  0.9505264  0.13100511          
  0.6523322  825      0.18177988  0.9394601  0.13516290          
  0.2863834  594      0.18682782  0.9331788  0.13485501          
  0.7114505  234      0.10576926  0.9760298  0.07885676          
  0.3753015  827      0.18892226  0.9354949  0.13323956          
  0.7514984  343      0.16507335  0.9433531  0.12317674          
  0.4402663  772      0.09724749  0.9770918  0.07705280          
  0.7063715  884      0.15864274  0.9465875  0.12101900          
  0.5542166  844      0.17934348  0.9253463  0.13933557          
  0.3878976  823      0.26594391  0.8614705  0.20373098          
  0.5463008  344      0.22588183  0.8987890  0.17000161          
  0.5444015  924      0.19452633  0.9180674  0.14542486          
  0.5936141  970      0.14893720  0.9477988  0.11673926          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 296, max_depth = 2, eta
 = 0.01959256, gamma = 0.2429113, colsample_bytree =
 0.5018684, min_child_weight = 8 and subsample = 0.5452989.
[1] "Mon Mar 12 08:07:14 2018"
Self-Organizing Maps 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 500, 500, 502 
Resampling results across tuning parameters:

  xdim  ydim  topo         user.weights  RMSE        Rsquared   MAE       
   1    17    rectangular  0.8586873     0.17145195  0.9293598  0.13872228
   1    18    rectangular  0.4027285     0.17409675  0.9264344  0.14054337
   3    19    rectangular  0.1121535     0.13643457  0.9548582  0.11014382
   5    12    rectangular  0.8771629     0.12531670  0.9619303  0.09992135
   5    17    hexagonal    0.7640245     0.12453252  0.9625778  0.09898183
   5    18    hexagonal    0.4432731     0.11565999  0.9673674  0.09324904
   6     8    rectangular  0.6335037     0.13598583  0.9555392  0.11000862
   6    13    hexagonal    0.2731866     0.11570551  0.9679039  0.09288799
   6    14    hexagonal    0.4215378     0.11901635  0.9660655  0.09582314
   6    20    hexagonal    0.1719211     0.11110309  0.9701847  0.08874580
   7    11    hexagonal    0.2082922     0.12366664  0.9634334  0.09908795
   7    20    rectangular  0.9670036     0.11507022  0.9681261  0.08986461
   8    18    rectangular  0.3282911     0.10951368  0.9710416  0.08736575
   9    12    rectangular  0.8989440     0.11892415  0.9660213  0.09472592
   9    17    rectangular  0.1585203     0.10873110  0.9715451  0.08535282
  10    10    rectangular  0.4018718     0.11619287  0.9675383  0.09240812
  11    15    hexagonal    0.4214241     0.10165521  0.9756151  0.08064470
  12    14    hexagonal    0.2097340     0.09883250  0.9761657  0.07911037
  17    17    rectangular  0.2417003     0.09400462  0.9804968  0.07388048
  17    18    rectangular  0.4705243     0.09196101  0.9810254  0.07218616
  Selected
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were xdim = 17, ydim = 18, user.weights
 = 0.4705243 and topo = rectangular.
[1] "Mon Mar 12 08:07:46 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  No variation for for: V11, V12
Fitting Repeat 1 

# weights:  199
initial  value 69.930198 
iter  10 value 4.755824
iter  20 value 4.334764
iter  30 value 4.297950
iter  40 value 4.252851
iter  50 value 4.241351
iter  60 value 4.240931
iter  70 value 4.240756
final  value 4.240749 
converged
Fitting Repeat 2 

# weights:  199
initial  value 77.950585 
iter  10 value 5.427218
iter  20 value 4.444703
iter  30 value 4.440930
iter  40 value 4.440542
iter  50 value 4.440525
final  value 4.440523 
converged
Fitting Repeat 3 

# weights:  199
initial  value 28.696438 
iter  10 value 4.957444
iter  20 value 4.434101
iter  30 value 4.368932
iter  40 value 4.366400
iter  50 value 4.361919
iter  60 value 4.361801
iter  70 value 4.361799
iter  70 value 4.361799
iter  70 value 4.361799
final  value 4.361799 
converged
Fitting Repeat 4 

# weights:  199
initial  value 57.870615 
iter  10 value 4.706163
iter  20 value 4.450475
iter  30 value 4.304449
iter  40 value 4.255144
iter  50 value 4.251420
iter  60 value 4.249802
final  value 4.249791 
converged
Fitting Repeat 5 

# weights:  199
initial  value 51.849997 
iter  10 value 4.614932
iter  20 value 4.248989
iter  30 value 4.240474
iter  40 value 4.238151
iter  50 value 4.237293
final  value 4.237259 
converged
Fitting Repeat 1 

# weights:  89
initial  value 42.761882 
iter  10 value 0.454520
iter  20 value 0.155693
iter  30 value 0.098860
iter  40 value 0.080739
iter  50 value 0.067979
iter  60 value 0.061131
iter  70 value 0.056252
iter  80 value 0.053951
iter  90 value 0.053009
iter 100 value 0.051878
final  value 0.051878 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 22.561924 
iter  10 value 0.239282
iter  20 value 0.111837
iter  30 value 0.082068
iter  40 value 0.070067
iter  50 value 0.061426
iter  60 value 0.058818
iter  70 value 0.056884
iter  80 value 0.055228
iter  90 value 0.054137
iter 100 value 0.053769
final  value 0.053769 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  89
initial  value 41.654730 
iter  10 value 0.493102
iter  20 value 0.237244
iter  30 value 0.154866
iter  40 value 0.088948
iter  50 value 0.066439
iter  60 value 0.061558
iter  70 value 0.056428
iter  80 value 0.054046
iter  90 value 0.052216
iter 100 value 0.050576
final  value 0.050576 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 24.886128 
iter  10 value 0.352851
iter  20 value 0.178132
iter  30 value 0.107899
iter  40 value 0.091718
iter  50 value 0.073757
iter  60 value 0.070705
iter  70 value 0.067859
iter  80 value 0.066245
iter  90 value 0.064738
iter 100 value 0.063125
final  value 0.063125 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 23.651726 
iter  10 value 0.375327
iter  20 value 0.207273
iter  30 value 0.166022
iter  40 value 0.132664
iter  50 value 0.090978
iter  60 value 0.077285
iter  70 value 0.066740
iter  80 value 0.061073
iter  90 value 0.058106
iter 100 value 0.056193
final  value 0.056193 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  133
initial  value 24.462642 
iter  10 value 0.511619
iter  20 value 0.336934
iter  30 value 0.293763
iter  40 value 0.284255
iter  50 value 0.280227
iter  60 value 0.278984
iter  70 value 0.277977
iter  80 value 0.277194
iter  90 value 0.274590
iter 100 value 0.272657
final  value 0.272657 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  133
initial  value 19.474933 
iter  10 value 0.383137
iter  20 value 0.298646
iter  30 value 0.274849
iter  40 value 0.272351
iter  50 value 0.271084
iter  60 value 0.270611
iter  70 value 0.270299
iter  80 value 0.269849
iter  90 value 0.269486
iter 100 value 0.269042
final  value 0.269042 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 37.170221 
iter  10 value 0.420597
iter  20 value 0.318900
iter  30 value 0.288019
iter  40 value 0.280537
iter  50 value 0.278677
iter  60 value 0.276505
iter  70 value 0.275218
iter  80 value 0.274196
iter  90 value 0.273347
iter 100 value 0.272404
final  value 0.272404 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  133
initial  value 25.443283 
iter  10 value 0.394235
iter  20 value 0.309098
iter  30 value 0.289170
iter  40 value 0.286258
iter  50 value 0.282792
iter  60 value 0.280395
iter  70 value 0.278807
iter  80 value 0.277955
iter  90 value 0.277447
iter 100 value 0.276345
final  value 0.276345 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  133
initial  value 20.292904 
iter  10 value 0.442418
iter  20 value 0.313204
iter  30 value 0.279926
iter  40 value 0.273375
iter  50 value 0.271873
iter  60 value 0.270890
iter  70 value 0.270436
iter  80 value 0.270170
iter  90 value 0.269739
iter 100 value 0.269397
final  value 0.269397 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  34
initial  value 52.022032 
iter  10 value 4.931371
iter  20 value 4.098324
iter  30 value 3.715429
iter  40 value 3.683739
iter  50 value 3.683601
final  value 3.683597 
converged
Fitting Repeat 2 

# weights:  34
initial  value 32.900511 
iter  10 value 4.380099
iter  20 value 3.807061
iter  30 value 3.800016
iter  40 value 3.799240
final  value 3.799238 
converged
Fitting Repeat 3 

# weights:  34
initial  value 20.963080 
iter  10 value 4.161080
iter  20 value 3.528957
iter  30 value 3.504372
iter  40 value 3.499947
final  value 3.499947 
converged
Fitting Repeat 4 

# weights:  34
initial  value 36.867188 
iter  10 value 4.270275
iter  20 value 3.577874
iter  30 value 3.539758
iter  40 value 3.536376
final  value 3.536361 
converged
Fitting Repeat 5 

# weights:  34
initial  value 30.723924 
iter  10 value 4.626515
iter  20 value 4.228591
iter  30 value 4.206085
iter  40 value 4.203991
final  value 4.203967 
converged
Fitting Repeat 1 

# weights:  188
initial  value 24.814570 
iter  10 value 0.230147
iter  20 value 0.197086
iter  30 value 0.178594
iter  40 value 0.169167
iter  50 value 0.160337
iter  60 value 0.143563
iter  70 value 0.133031
iter  80 value 0.122298
iter  90 value 0.115169
iter 100 value 0.110040
final  value 0.110040 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 32.634951 
iter  10 value 0.322138
iter  20 value 0.206387
iter  30 value 0.184893
iter  40 value 0.170162
iter  50 value 0.157675
iter  60 value 0.139923
iter  70 value 0.127402
iter  80 value 0.119893
iter  90 value 0.114026
iter 100 value 0.110336
final  value 0.110336 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 51.333115 
iter  10 value 0.272976
iter  20 value 0.211468
iter  30 value 0.188712
iter  40 value 0.176443
iter  50 value 0.169743
iter  60 value 0.164206
iter  70 value 0.157731
iter  80 value 0.147493
iter  90 value 0.135630
iter 100 value 0.128714
final  value 0.128714 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 21.721529 
iter  10 value 0.308661
iter  20 value 0.210907
iter  30 value 0.191187
iter  40 value 0.179179
iter  50 value 0.168530
iter  60 value 0.154466
iter  70 value 0.143522
iter  80 value 0.134057
iter  90 value 0.129211
iter 100 value 0.123821
final  value 0.123821 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 26.440192 
iter  10 value 0.563325
iter  20 value 0.233895
iter  30 value 0.187723
iter  40 value 0.160610
iter  50 value 0.141113
iter  60 value 0.129034
iter  70 value 0.124077
iter  80 value 0.118492
iter  90 value 0.115505
iter 100 value 0.112423
final  value 0.112423 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  56
initial  value 29.075585 
iter  10 value 1.412332
iter  20 value 0.566756
iter  30 value 0.501765
iter  40 value 0.481185
iter  50 value 0.446340
iter  60 value 0.431491
iter  70 value 0.428370
iter  80 value 0.427793
iter  90 value 0.427646
iter 100 value 0.427403
final  value 0.427403 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 21.965308 
iter  10 value 1.061957
iter  20 value 0.548988
iter  30 value 0.440819
iter  40 value 0.419239
iter  50 value 0.415550
iter  60 value 0.414247
iter  70 value 0.413180
iter  80 value 0.412784
iter  90 value 0.412645
iter 100 value 0.412574
final  value 0.412574 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 22.651793 
iter  10 value 0.900509
iter  20 value 0.468381
iter  30 value 0.431467
iter  40 value 0.423612
iter  50 value 0.421251
iter  60 value 0.419883
iter  70 value 0.418792
iter  80 value 0.418444
iter  90 value 0.418336
iter 100 value 0.418317
final  value 0.418317 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 31.878358 
iter  10 value 0.808553
iter  20 value 0.441090
iter  30 value 0.412798
iter  40 value 0.410904
iter  50 value 0.410325
iter  60 value 0.410232
iter  70 value 0.410159
iter  80 value 0.410119
iter  90 value 0.410029
iter 100 value 0.410003
final  value 0.410003 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 46.837322 
iter  10 value 0.612391
iter  20 value 0.476410
iter  30 value 0.462588
iter  40 value 0.452433
iter  50 value 0.440301
iter  60 value 0.431346
iter  70 value 0.425785
iter  80 value 0.422867
iter  90 value 0.421442
iter 100 value 0.420361
final  value 0.420361 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  78
initial  value 45.906544 
iter  10 value 5.819454
iter  20 value 4.306388
iter  30 value 4.048465
iter  40 value 4.014087
iter  50 value 3.992620
iter  60 value 3.985008
iter  70 value 3.983781
iter  80 value 3.979631
iter  90 value 3.979343
iter  90 value 3.979343
iter  90 value 3.979343
final  value 3.979343 
converged
Fitting Repeat 2 

# weights:  78
initial  value 19.928879 
iter  10 value 4.253147
iter  20 value 4.024115
iter  30 value 3.969980
iter  40 value 3.922454
iter  50 value 3.859271
iter  60 value 3.852488
iter  70 value 3.851985
final  value 3.851980 
converged
Fitting Repeat 3 

# weights:  78
initial  value 31.426113 
iter  10 value 5.092084
iter  20 value 4.210175
iter  30 value 3.926835
iter  40 value 3.893006
iter  50 value 3.872160
iter  60 value 3.870526
iter  70 value 3.869428
iter  80 value 3.869416
iter  80 value 3.869416
iter  80 value 3.869416
final  value 3.869416 
converged
Fitting Repeat 4 

# weights:  78
initial  value 59.979635 
iter  10 value 4.527693
iter  20 value 4.001741
iter  30 value 3.891570
iter  40 value 3.821529
iter  50 value 3.783440
iter  60 value 3.769532
iter  70 value 3.769139
final  value 3.769126 
converged
Fitting Repeat 5 

# weights:  78
initial  value 29.110786 
iter  10 value 4.599435
iter  20 value 4.187388
iter  30 value 4.077161
iter  40 value 4.027020
iter  50 value 4.016673
iter  60 value 4.016349
final  value 4.016306 
converged
Fitting Repeat 1 

# weights:  177
initial  value 30.824906 
iter  10 value 4.014267
iter  20 value 3.611081
iter  30 value 3.600123
iter  40 value 3.590106
iter  50 value 3.587599
iter  60 value 3.587136
final  value 3.587134 
converged
Fitting Repeat 2 

# weights:  177
initial  value 22.427397 
iter  10 value 3.638873
iter  20 value 3.538772
iter  30 value 3.534350
iter  40 value 3.530894
iter  50 value 3.530750
iter  50 value 3.530750
iter  50 value 3.530750
final  value 3.530750 
converged
Fitting Repeat 3 

# weights:  177
initial  value 22.938015 
iter  10 value 3.738285
iter  20 value 3.642774
iter  30 value 3.621783
iter  40 value 3.621554
final  value 3.621554 
converged
Fitting Repeat 4 

# weights:  177
initial  value 19.570762 
iter  10 value 3.697108
iter  20 value 3.642302
iter  30 value 3.590827
iter  40 value 3.567588
iter  50 value 3.563292
iter  60 value 3.562375
final  value 3.562371 
converged
Fitting Repeat 5 

# weights:  177
initial  value 46.609937 
iter  10 value 4.138115
iter  20 value 3.615745
iter  30 value 3.572471
iter  40 value 3.550163
iter  50 value 3.522030
iter  60 value 3.512200
iter  70 value 3.510230
iter  80 value 3.509728
final  value 3.509727 
converged
Fitting Repeat 1 

# weights:  221
initial  value 30.788374 
iter  10 value 0.192675
iter  20 value 0.155188
iter  30 value 0.125356
iter  40 value 0.098956
iter  50 value 0.063143
iter  60 value 0.044159
iter  70 value 0.032013
iter  80 value 0.020322
iter  90 value 0.014779
iter 100 value 0.012761
final  value 0.012761 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  221
initial  value 57.582481 
iter  10 value 0.194034
iter  20 value 0.160673
iter  30 value 0.113803
iter  40 value 0.086563
iter  50 value 0.043853
iter  60 value 0.030303
iter  70 value 0.022210
iter  80 value 0.018949
iter  90 value 0.014666
iter 100 value 0.012464
final  value 0.012464 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  221
initial  value 20.111193 
iter  10 value 0.159275
iter  20 value 0.122796
iter  30 value 0.097595
iter  40 value 0.073399
iter  50 value 0.050547
iter  60 value 0.036349
iter  70 value 0.025348
iter  80 value 0.019532
iter  90 value 0.014559
iter 100 value 0.011477
final  value 0.011477 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  221
initial  value 141.035842 
iter  10 value 11.271592
iter  20 value 0.227573
iter  30 value 0.151446
iter  40 value 0.139644
iter  50 value 0.123196
iter  60 value 0.108360
iter  70 value 0.087148
iter  80 value 0.055755
iter  90 value 0.043672
iter 100 value 0.024016
final  value 0.024016 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  221
initial  value 35.710570 
iter  10 value 0.190147
iter  20 value 0.160040
iter  30 value 0.124438
iter  40 value 0.079531
iter  50 value 0.046936
iter  60 value 0.038120
iter  70 value 0.030671
iter  80 value 0.026730
iter  90 value 0.021517
iter 100 value 0.018345
final  value 0.018345 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  155
initial  value 33.612058 
iter  10 value 0.263931
iter  20 value 0.158021
iter  30 value 0.129109
iter  40 value 0.088018
iter  50 value 0.050548
iter  60 value 0.037058
iter  70 value 0.033859
iter  80 value 0.031154
iter  90 value 0.029824
iter 100 value 0.027740
final  value 0.027740 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  155
initial  value 28.042699 
iter  10 value 0.210027
iter  20 value 0.163495
iter  30 value 0.138029
iter  40 value 0.107767
iter  50 value 0.065566
iter  60 value 0.047268
iter  70 value 0.035794
iter  80 value 0.032475
iter  90 value 0.028896
iter 100 value 0.026488
final  value 0.026488 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  155
initial  value 20.017413 
iter  10 value 0.929725
iter  20 value 0.159884
iter  30 value 0.086936
iter  40 value 0.047362
iter  50 value 0.035681
iter  60 value 0.032919
iter  70 value 0.031221
iter  80 value 0.028811
iter  90 value 0.027819
iter 100 value 0.027227
final  value 0.027227 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  155
initial  value 43.435326 
iter  10 value 0.439733
iter  20 value 0.143185
iter  30 value 0.090602
iter  40 value 0.050135
iter  50 value 0.038712
iter  60 value 0.031937
iter  70 value 0.029684
iter  80 value 0.027463
iter  90 value 0.026116
iter 100 value 0.025067
final  value 0.025067 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  155
initial  value 72.428361 
iter  10 value 0.380310
iter  20 value 0.125318
iter  30 value 0.073349
iter  40 value 0.038642
iter  50 value 0.031994
iter  60 value 0.029887
iter  70 value 0.029075
iter  80 value 0.027037
iter  90 value 0.026646
iter 100 value 0.026048
final  value 0.026048 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 25.174206 
iter  10 value 6.831066
iter  20 value 6.380138
iter  30 value 6.213993
iter  40 value 6.201388
iter  50 value 6.200983
final  value 6.200982 
converged
Fitting Repeat 2 

# weights:  89
initial  value 32.058975 
iter  10 value 6.635694
iter  20 value 6.354216
iter  30 value 6.286874
iter  40 value 6.286185
final  value 6.286184 
converged
Fitting Repeat 3 

# weights:  89
initial  value 34.384393 
iter  10 value 7.244455
iter  20 value 6.494375
iter  30 value 6.277227
iter  40 value 6.207077
iter  50 value 6.201993
iter  60 value 6.200984
final  value 6.200982 
converged
Fitting Repeat 4 

# weights:  89
initial  value 32.546855 
iter  10 value 6.933506
iter  20 value 6.508049
iter  30 value 6.474140
iter  40 value 6.468267
iter  50 value 6.465094
iter  60 value 6.464474
final  value 6.464473 
converged
Fitting Repeat 5 

# weights:  89
initial  value 43.012164 
iter  10 value 7.206316
iter  20 value 6.649813
iter  30 value 6.369110
iter  40 value 6.288706
iter  50 value 6.286390
iter  60 value 6.286185
iter  60 value 6.286184
iter  60 value 6.286184
final  value 6.286184 
converged
Fitting Repeat 1 

# weights:  155
initial  value 33.890809 
iter  10 value 0.292086
iter  20 value 0.207674
iter  30 value 0.183807
iter  40 value 0.174868
iter  50 value 0.171505
iter  60 value 0.170486
iter  70 value 0.169187
iter  80 value 0.166707
iter  90 value 0.160237
iter 100 value 0.154680
final  value 0.154680 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  155
initial  value 68.682206 
iter  10 value 0.305552
iter  20 value 0.225912
iter  30 value 0.192808
iter  40 value 0.175276
iter  50 value 0.167242
iter  60 value 0.160886
iter  70 value 0.157622
iter  80 value 0.154153
iter  90 value 0.149256
iter 100 value 0.146671
final  value 0.146671 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  155
initial  value 30.477908 
iter  10 value 0.465005
iter  20 value 0.264973
iter  30 value 0.227019
iter  40 value 0.197911
iter  50 value 0.187948
iter  60 value 0.168003
iter  70 value 0.156177
iter  80 value 0.151988
iter  90 value 0.149627
iter 100 value 0.148207
final  value 0.148207 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  155
initial  value 52.379072 
iter  10 value 0.718223
iter  20 value 0.389718
iter  30 value 0.234243
iter  40 value 0.198764
iter  50 value 0.182261
iter  60 value 0.175722
iter  70 value 0.168118
iter  80 value 0.160450
iter  90 value 0.155362
iter 100 value 0.150749
final  value 0.150749 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  155
initial  value 23.899205 
iter  10 value 0.564038
iter  20 value 0.275890
iter  30 value 0.219011
iter  40 value 0.188048
iter  50 value 0.171261
iter  60 value 0.163657
iter  70 value 0.158780
iter  80 value 0.154855
iter  90 value 0.153163
iter 100 value 0.152102
final  value 0.152102 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  188
initial  value 33.216863 
iter  10 value 0.210814
iter  20 value 0.150789
iter  30 value 0.126793
iter  40 value 0.107171
iter  50 value 0.076939
iter  60 value 0.055787
iter  70 value 0.048469
iter  80 value 0.043773
iter  90 value 0.039720
iter 100 value 0.038219
final  value 0.038219 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 27.782990 
iter  10 value 0.376605
iter  20 value 0.164036
iter  30 value 0.132773
iter  40 value 0.097276
iter  50 value 0.058369
iter  60 value 0.045569
iter  70 value 0.040524
iter  80 value 0.038422
iter  90 value 0.037251
iter 100 value 0.035353
final  value 0.035353 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 17.075696 
iter  10 value 0.282167
iter  20 value 0.164845
iter  30 value 0.146323
iter  40 value 0.121602
iter  50 value 0.098780
iter  60 value 0.078526
iter  70 value 0.061794
iter  80 value 0.051127
iter  90 value 0.046264
iter 100 value 0.042267
final  value 0.042267 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 28.133848 
iter  10 value 0.321817
iter  20 value 0.147348
iter  30 value 0.130114
iter  40 value 0.110320
iter  50 value 0.064148
iter  60 value 0.051350
iter  70 value 0.041658
iter  80 value 0.038908
iter  90 value 0.036551
iter 100 value 0.036183
final  value 0.036183 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 68.790346 
iter  10 value 0.212869
iter  20 value 0.113031
iter  30 value 0.075807
iter  40 value 0.069615
iter  50 value 0.061262
iter  60 value 0.054604
iter  70 value 0.051501
iter  80 value 0.048673
iter  90 value 0.046462
iter 100 value 0.045194
final  value 0.045194 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 50.195662 
iter  10 value 5.252985
iter  20 value 4.918283
iter  30 value 4.873382
iter  40 value 4.859895
iter  50 value 4.858899
final  value 4.858899 
converged
Fitting Repeat 2 

# weights:  89
initial  value 40.849887 
iter  10 value 5.656786
iter  20 value 4.941391
iter  30 value 4.878384
iter  40 value 4.860160
iter  50 value 4.858949
iter  60 value 4.858905
final  value 4.858899 
converged
Fitting Repeat 3 

# weights:  89
initial  value 26.654214 
iter  10 value 5.118518
iter  20 value 4.959536
iter  30 value 4.931385
iter  40 value 4.927840
iter  50 value 4.926277
iter  50 value 4.926277
iter  50 value 4.926277
final  value 4.926277 
converged
Fitting Repeat 4 

# weights:  89
initial  value 27.348908 
iter  10 value 5.164932
iter  20 value 4.909485
iter  30 value 4.883527
iter  40 value 4.882194
final  value 4.882188 
converged
Fitting Repeat 5 

# weights:  89
initial  value 53.422556 
iter  10 value 5.126393
iter  20 value 4.884772
iter  30 value 4.864614
iter  40 value 4.858919
final  value 4.858899 
converged
Fitting Repeat 1 

# weights:  210
initial  value 137.110812 
iter  10 value 1.244367
iter  20 value 0.294555
iter  30 value 0.211734
iter  40 value 0.162905
iter  50 value 0.138796
iter  60 value 0.128447
iter  70 value 0.124827
iter  80 value 0.122836
iter  90 value 0.121758
iter 100 value 0.121281
final  value 0.121281 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  210
initial  value 29.643306 
iter  10 value 0.221837
iter  20 value 0.187584
iter  30 value 0.171378
iter  40 value 0.156931
iter  50 value 0.140824
iter  60 value 0.128530
iter  70 value 0.121290
iter  80 value 0.114750
iter  90 value 0.110943
iter 100 value 0.108917
final  value 0.108917 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  210
initial  value 65.673210 
iter  10 value 0.264633
iter  20 value 0.212914
iter  30 value 0.190742
iter  40 value 0.176100
iter  50 value 0.168898
iter  60 value 0.164424
iter  70 value 0.151943
iter  80 value 0.138803
iter  90 value 0.130817
iter 100 value 0.122690
final  value 0.122690 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  210
initial  value 18.064381 
iter  10 value 0.245260
iter  20 value 0.201206
iter  30 value 0.175832
iter  40 value 0.163415
iter  50 value 0.145691
iter  60 value 0.136815
iter  70 value 0.130462
iter  80 value 0.125342
iter  90 value 0.121266
iter 100 value 0.117831
final  value 0.117831 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  210
initial  value 20.249844 
iter  10 value 0.291140
iter  20 value 0.196118
iter  30 value 0.161940
iter  40 value 0.140667
iter  50 value 0.129570
iter  60 value 0.124831
iter  70 value 0.117533
iter  80 value 0.113203
iter  90 value 0.109461
iter 100 value 0.107235
final  value 0.107235 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 22.999855 
iter  10 value 1.445761
iter  20 value 0.927927
iter  30 value 0.915862
iter  40 value 0.906878
iter  50 value 0.897569
iter  60 value 0.894021
iter  70 value 0.892786
iter  80 value 0.892201
iter  90 value 0.892088
iter 100 value 0.892018
final  value 0.892018 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 58.361922 
iter  10 value 1.688149
iter  20 value 1.013406
iter  30 value 0.974043
iter  40 value 0.964713
iter  50 value 0.960344
iter  60 value 0.958621
iter  70 value 0.958122
iter  80 value 0.957633
iter  90 value 0.957418
iter 100 value 0.957272
final  value 0.957272 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  89
initial  value 32.146481 
iter  10 value 1.886570
iter  20 value 1.272940
iter  30 value 1.040049
iter  40 value 0.971660
iter  50 value 0.951308
iter  60 value 0.942787
iter  70 value 0.936214
iter  80 value 0.934256
iter  90 value 0.934044
iter 100 value 0.933953
final  value 0.933953 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 29.536755 
iter  10 value 1.280653
iter  20 value 0.947676
iter  30 value 0.933651
iter  40 value 0.929990
iter  50 value 0.928457
iter  60 value 0.928133
iter  70 value 0.927840
iter  80 value 0.927786
iter  90 value 0.927783
final  value 0.927782 
converged
Fitting Repeat 5 

# weights:  89
initial  value 24.046193 
iter  10 value 1.188744
iter  20 value 0.916929
iter  30 value 0.893905
iter  40 value 0.890480
iter  50 value 0.887261
iter  60 value 0.886484
iter  70 value 0.886416
iter  80 value 0.886352
iter  90 value 0.886295
iter 100 value 0.886273
final  value 0.886273 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 116.184296 
iter  10 value 21.199325
iter  20 value 19.295813
iter  30 value 19.248777
iter  40 value 19.241510
iter  50 value 19.240439
final  value 19.240394 
converged
Fitting Repeat 2 

# weights:  177
initial  value 82.986366 
iter  10 value 20.004184
iter  20 value 18.760699
iter  30 value 18.737156
iter  40 value 18.729668
iter  50 value 18.729476
final  value 18.729458 
converged
Fitting Repeat 3 

# weights:  177
initial  value 82.775171 
iter  10 value 19.790729
iter  20 value 18.969424
iter  30 value 18.911613
iter  40 value 18.901182
iter  50 value 18.890437
iter  60 value 18.885771
final  value 18.885726 
converged
Fitting Repeat 4 

# weights:  177
initial  value 174.156440 
iter  10 value 19.137148
iter  20 value 18.970317
iter  30 value 18.954195
iter  40 value 18.953800
iter  40 value 18.953800
iter  40 value 18.953800
final  value 18.953800 
converged
Fitting Repeat 5 

# weights:  177
initial  value 84.023212 
iter  10 value 18.429819
iter  20 value 18.026122
iter  30 value 17.999794
iter  40 value 17.991492
iter  50 value 17.986923
iter  60 value 17.986820
final  value 17.986819 
converged
Fitting Repeat 1 

# weights:  144
initial  value 158.946709 
iter  10 value 21.014125
iter  20 value 20.126453
iter  30 value 20.103834
final  value 20.103830 
converged
Fitting Repeat 2 

# weights:  144
initial  value 165.450863 
iter  10 value 20.719603
iter  20 value 20.107784
final  value 20.103830 
converged
Fitting Repeat 3 

# weights:  144
initial  value 129.541821 
iter  10 value 20.275733
iter  20 value 20.104979
iter  30 value 20.103830
iter  30 value 20.103830
iter  30 value 20.103830
final  value 20.103830 
converged
Fitting Repeat 4 

# weights:  144
initial  value 143.121333 
iter  10 value 20.687173
iter  20 value 20.108301
iter  30 value 20.103832
final  value 20.103830 
converged
Fitting Repeat 5 

# weights:  144
initial  value 152.486263 
iter  10 value 21.507618
iter  20 value 20.118571
iter  30 value 20.103834
final  value 20.103830 
converged
Fitting Repeat 1 

# weights:  89
initial  value 32.297077 
iter  10 value 10.667825
iter  20 value 10.298694
iter  30 value 10.129632
iter  40 value 10.126679
iter  40 value 10.126679
iter  40 value 10.126679
final  value 10.126679 
converged
Fitting Repeat 2 

# weights:  89
initial  value 37.696571 
iter  10 value 10.391919
iter  20 value 10.257104
iter  30 value 10.256647
final  value 10.256624 
converged
Fitting Repeat 3 

# weights:  89
initial  value 32.862000 
iter  10 value 10.637040
iter  20 value 10.241650
iter  30 value 10.159173
final  value 10.158796 
converged
Fitting Repeat 4 

# weights:  89
initial  value 39.410214 
iter  10 value 10.754868
iter  20 value 10.287188
iter  30 value 10.159038
iter  40 value 10.158796
iter  40 value 10.158796
iter  40 value 10.158796
final  value 10.158796 
converged
Fitting Repeat 5 

# weights:  89
initial  value 42.000928 
iter  10 value 11.066750
iter  20 value 10.558316
iter  30 value 10.521433
iter  40 value 10.521151
final  value 10.521122 
converged
Fitting Repeat 1 

# weights:  155
initial  value 27.018205 
iter  10 value 0.364959
iter  20 value 0.151158
iter  30 value 0.101880
iter  40 value 0.087867
iter  50 value 0.075153
iter  60 value 0.064422
iter  70 value 0.060646
iter  80 value 0.056968
iter  90 value 0.055378
iter 100 value 0.054021
final  value 0.054021 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  155
initial  value 23.628509 
iter  10 value 0.238267
iter  20 value 0.179712
iter  30 value 0.136231
iter  40 value 0.101476
iter  50 value 0.077497
iter  60 value 0.067815
iter  70 value 0.058540
iter  80 value 0.054230
iter  90 value 0.052263
iter 100 value 0.050895
final  value 0.050895 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  155
initial  value 18.708009 
iter  10 value 0.385608
iter  20 value 0.139642
iter  30 value 0.094273
iter  40 value 0.071615
iter  50 value 0.062077
iter  60 value 0.059366
iter  70 value 0.056604
iter  80 value 0.053894
iter  90 value 0.052994
iter 100 value 0.052034
final  value 0.052034 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  155
initial  value 20.931805 
iter  10 value 0.210224
iter  20 value 0.167631
iter  30 value 0.139398
iter  40 value 0.122568
iter  50 value 0.094210
iter  60 value 0.074931
iter  70 value 0.062077
iter  80 value 0.054790
iter  90 value 0.051922
iter 100 value 0.049933
final  value 0.049933 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  155
initial  value 22.478015 
iter  10 value 0.224170
iter  20 value 0.164887
iter  30 value 0.119700
iter  40 value 0.099437
iter  50 value 0.076684
iter  60 value 0.067206
iter  70 value 0.057834
iter  80 value 0.053923
iter  90 value 0.052244
iter 100 value 0.050741
final  value 0.050741 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  199
initial  value 34.495433 
iter  10 value 4.704207
iter  20 value 4.371259
iter  30 value 4.332354
iter  40 value 4.314844
iter  50 value 4.302016
iter  60 value 4.301564
final  value 4.301564 
converged
Fitting Repeat 2 

# weights:  199
initial  value 57.797549 
iter  10 value 5.329008
iter  20 value 4.422021
iter  30 value 4.360103
iter  40 value 4.343060
iter  50 value 4.337129
iter  60 value 4.337118
final  value 4.337113 
converged
Fitting Repeat 3 

# weights:  199
initial  value 50.065316 
iter  10 value 4.623214
iter  20 value 4.307886
iter  30 value 4.296481
iter  40 value 4.291236
iter  50 value 4.290199
final  value 4.290183 
converged
Fitting Repeat 4 

# weights:  199
initial  value 72.627653 
iter  10 value 6.353311
iter  20 value 4.493511
iter  30 value 4.440732
iter  40 value 4.407358
iter  50 value 4.402065
iter  60 value 4.401290
iter  70 value 4.401000
final  value 4.400999 
converged
Fitting Repeat 5 

# weights:  199
initial  value 32.629799 
iter  10 value 5.107179
iter  20 value 4.557240
iter  30 value 4.415110
iter  40 value 4.402278
iter  50 value 4.396457
iter  60 value 4.384091
iter  70 value 4.377505
iter  80 value 4.377050
final  value 4.377049 
converged
Fitting Repeat 1 

# weights:  89
initial  value 66.206558 
iter  10 value 0.283853
iter  20 value 0.178159
iter  30 value 0.113660
iter  40 value 0.064848
iter  50 value 0.057177
iter  60 value 0.053626
iter  70 value 0.052003
iter  80 value 0.051076
iter  90 value 0.050692
iter 100 value 0.049968
final  value 0.049968 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 25.962771 
iter  10 value 0.495088
iter  20 value 0.134689
iter  30 value 0.078643
iter  40 value 0.062853
iter  50 value 0.057088
iter  60 value 0.053240
iter  70 value 0.051314
iter  80 value 0.050030
iter  90 value 0.048089
iter 100 value 0.046782
final  value 0.046782 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  89
initial  value 25.954188 
iter  10 value 0.343284
iter  20 value 0.142396
iter  30 value 0.079771
iter  40 value 0.064657
iter  50 value 0.060909
iter  60 value 0.057227
iter  70 value 0.055920
iter  80 value 0.054650
iter  90 value 0.053769
iter 100 value 0.051923
final  value 0.051923 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 26.785818 
iter  10 value 0.348932
iter  20 value 0.202623
iter  30 value 0.157596
iter  40 value 0.099474
iter  50 value 0.066036
iter  60 value 0.056559
iter  70 value 0.054358
iter  80 value 0.052621
iter  90 value 0.051759
iter 100 value 0.051113
final  value 0.051113 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 36.411850 
iter  10 value 0.510852
iter  20 value 0.177634
iter  30 value 0.099559
iter  40 value 0.076918
iter  50 value 0.068832
iter  60 value 0.065060
iter  70 value 0.061844
iter  80 value 0.057633
iter  90 value 0.054832
iter 100 value 0.054063
final  value 0.054063 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  133
initial  value 27.631214 
iter  10 value 0.687545
iter  20 value 0.328324
iter  30 value 0.290717
iter  40 value 0.284424
iter  50 value 0.280887
iter  60 value 0.278724
iter  70 value 0.277792
iter  80 value 0.277340
iter  90 value 0.277067
iter 100 value 0.276771
final  value 0.276771 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  133
initial  value 68.299666 
iter  10 value 0.467362
iter  20 value 0.332224
iter  30 value 0.294849
iter  40 value 0.286726
iter  50 value 0.284084
iter  60 value 0.282835
iter  70 value 0.281774
iter  80 value 0.280925
iter  90 value 0.279484
iter 100 value 0.277805
final  value 0.277805 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 23.168572 
iter  10 value 0.437385
iter  20 value 0.322301
iter  30 value 0.296599
iter  40 value 0.291908
iter  50 value 0.286250
iter  60 value 0.283137
iter  70 value 0.282043
iter  80 value 0.281644
iter  90 value 0.281488
iter 100 value 0.281386
final  value 0.281386 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  133
initial  value 31.539148 
iter  10 value 1.115859
iter  20 value 0.547898
iter  30 value 0.374503
iter  40 value 0.337695
iter  50 value 0.312526
iter  60 value 0.301531
iter  70 value 0.295795
iter  80 value 0.288908
iter  90 value 0.283340
iter 100 value 0.280161
final  value 0.280161 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  133
initial  value 43.342848 
iter  10 value 0.491772
iter  20 value 0.343928
iter  30 value 0.293046
iter  40 value 0.284719
iter  50 value 0.281014
iter  60 value 0.279857
iter  70 value 0.279031
iter  80 value 0.278424
iter  90 value 0.278189
iter 100 value 0.277990
final  value 0.277990 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  34
initial  value 48.045526 
iter  10 value 5.429798
iter  20 value 4.188456
iter  30 value 3.940123
iter  40 value 3.909809
iter  50 value 3.882742
iter  60 value 3.880883
final  value 3.880883 
converged
Fitting Repeat 2 

# weights:  34
initial  value 38.558706 
iter  10 value 4.280439
iter  20 value 3.630495
iter  30 value 3.518862
iter  40 value 3.514594
final  value 3.514592 
converged
Fitting Repeat 3 

# weights:  34
initial  value 47.885336 
iter  10 value 5.069517
iter  20 value 3.913738
iter  30 value 3.878667
iter  40 value 3.821813
iter  50 value 3.818898
final  value 3.818896 
converged
Fitting Repeat 4 

# weights:  34
initial  value 30.896155 
iter  10 value 5.601405
iter  20 value 4.212005
iter  30 value 3.781379
iter  40 value 3.743314
iter  50 value 3.741416
final  value 3.741393 
converged
Fitting Repeat 5 

# weights:  34
initial  value 60.017056 
iter  10 value 4.459419
iter  20 value 3.833732
iter  30 value 3.819990
final  value 3.819953 
converged
Fitting Repeat 1 

# weights:  188
initial  value 16.794428 
iter  10 value 0.228932
iter  20 value 0.198582
iter  30 value 0.180356
iter  40 value 0.160783
iter  50 value 0.139537
iter  60 value 0.123673
iter  70 value 0.115166
iter  80 value 0.110163
iter  90 value 0.107339
iter 100 value 0.105668
final  value 0.105668 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 91.771120 
iter  10 value 0.287068
iter  20 value 0.214080
iter  30 value 0.191882
iter  40 value 0.180985
iter  50 value 0.171191
iter  60 value 0.157739
iter  70 value 0.137917
iter  80 value 0.126530
iter  90 value 0.118852
iter 100 value 0.113892
final  value 0.113892 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 58.357850 
iter  10 value 0.271124
iter  20 value 0.216865
iter  30 value 0.190335
iter  40 value 0.159342
iter  50 value 0.138193
iter  60 value 0.125771
iter  70 value 0.117935
iter  80 value 0.114138
iter  90 value 0.112516
iter 100 value 0.110993
final  value 0.110993 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 42.568104 
iter  10 value 0.325562
iter  20 value 0.208517
iter  30 value 0.180483
iter  40 value 0.167277
iter  50 value 0.144631
iter  60 value 0.127367
iter  70 value 0.116589
iter  80 value 0.111254
iter  90 value 0.108074
iter 100 value 0.106360
final  value 0.106360 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 30.974460 
iter  10 value 0.244259
iter  20 value 0.206562
iter  30 value 0.183397
iter  40 value 0.163028
iter  50 value 0.140068
iter  60 value 0.125131
iter  70 value 0.115089
iter  80 value 0.110971
iter  90 value 0.108095
iter 100 value 0.105745
final  value 0.105745 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  56
initial  value 17.899084 
iter  10 value 0.901396
iter  20 value 0.595696
iter  30 value 0.497404
iter  40 value 0.449763
iter  50 value 0.433016
iter  60 value 0.430057
iter  70 value 0.425779
iter  80 value 0.420542
iter  90 value 0.417010
iter 100 value 0.415155
final  value 0.415155 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 20.916171 
iter  10 value 0.603298
iter  20 value 0.497024
iter  30 value 0.481118
iter  40 value 0.479334
iter  50 value 0.479053
iter  60 value 0.478840
iter  70 value 0.478731
iter  80 value 0.478630
iter  90 value 0.478590
iter 100 value 0.478575
final  value 0.478575 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 68.609220 
iter  10 value 0.739055
iter  20 value 0.517277
iter  30 value 0.464156
iter  40 value 0.458090
iter  50 value 0.455663
iter  60 value 0.453542
iter  70 value 0.452404
iter  80 value 0.450813
iter  90 value 0.449402
iter 100 value 0.447986
final  value 0.447986 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 19.355741 
iter  10 value 1.292205
iter  20 value 0.559548
iter  30 value 0.472360
iter  40 value 0.450422
iter  50 value 0.444092
iter  60 value 0.441315
iter  70 value 0.439033
iter  80 value 0.437755
iter  90 value 0.435364
iter 100 value 0.433811
final  value 0.433811 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 43.463391 
iter  10 value 0.965407
iter  20 value 0.512443
iter  30 value 0.426707
iter  40 value 0.420583
iter  50 value 0.418079
iter  60 value 0.416437
iter  70 value 0.415866
iter  80 value 0.415660
iter  90 value 0.415543
iter 100 value 0.415441
final  value 0.415441 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  78
initial  value 43.214968 
iter  10 value 4.670472
iter  20 value 3.963671
iter  30 value 3.911036
iter  40 value 3.868747
iter  50 value 3.862447
iter  60 value 3.861895
final  value 3.861890 
converged
Fitting Repeat 2 

# weights:  78
initial  value 23.354578 
iter  10 value 5.030926
iter  20 value 4.054013
iter  30 value 3.992189
iter  40 value 3.982326
iter  50 value 3.979387
iter  60 value 3.978389
iter  70 value 3.978344
iter  70 value 3.978344
iter  70 value 3.978344
final  value 3.978344 
converged
Fitting Repeat 3 

# weights:  78
initial  value 39.417376 
iter  10 value 4.118096
iter  20 value 3.926610
iter  30 value 3.906030
iter  40 value 3.905072
iter  50 value 3.904664
final  value 3.904663 
converged
Fitting Repeat 4 

# weights:  78
initial  value 28.854496 
iter  10 value 4.401135
iter  20 value 3.999228
iter  30 value 3.957113
iter  40 value 3.949877
iter  50 value 3.947338
iter  60 value 3.945889
final  value 3.945845 
converged
Fitting Repeat 5 

# weights:  78
initial  value 32.303636 
iter  10 value 4.062059
iter  20 value 3.884925
iter  30 value 3.876549
iter  40 value 3.876040
final  value 3.876031 
converged
Fitting Repeat 1 

# weights:  177
initial  value 26.608387 
iter  10 value 3.664805
iter  20 value 3.460601
iter  30 value 3.440675
iter  40 value 3.436593
iter  50 value 3.430794
iter  60 value 3.430283
final  value 3.430276 
converged
Fitting Repeat 2 

# weights:  177
initial  value 43.427038 
iter  10 value 3.895277
iter  20 value 3.533988
iter  30 value 3.479176
iter  40 value 3.467510
iter  50 value 3.457163
iter  60 value 3.456943
iter  70 value 3.456938
final  value 3.456938 
converged
Fitting Repeat 3 

# weights:  177
initial  value 44.798382 
iter  10 value 4.131237
iter  20 value 3.660040
iter  30 value 3.631060
iter  40 value 3.616052
iter  50 value 3.609123
iter  60 value 3.598431
iter  70 value 3.598246
final  value 3.598245 
converged
Fitting Repeat 4 

# weights:  177
initial  value 21.074798 
iter  10 value 3.640857
iter  20 value 3.528075
iter  30 value 3.526716
iter  40 value 3.526344
iter  50 value 3.526039
final  value 3.526030 
converged
Fitting Repeat 5 

# weights:  177
initial  value 29.463078 
iter  10 value 3.756490
iter  20 value 3.610001
iter  30 value 3.578890
iter  40 value 3.558088
iter  50 value 3.542397
iter  60 value 3.542186
final  value 3.542186 
converged
Fitting Repeat 1 

# weights:  221
initial  value 30.561078 
iter  10 value 0.202796
iter  20 value 0.140417
iter  30 value 0.116251
iter  40 value 0.092509
iter  50 value 0.046345
iter  60 value 0.025252
iter  70 value 0.013854
iter  80 value 0.010216
iter  90 value 0.008740
iter 100 value 0.007887
final  value 0.007887 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  221
initial  value 67.146331 
iter  10 value 0.144087
iter  20 value 0.097702
iter  30 value 0.090223
iter  40 value 0.086199
iter  50 value 0.082882
iter  60 value 0.073400
iter  70 value 0.046162
iter  80 value 0.029610
iter  90 value 0.015723
iter 100 value 0.012801
final  value 0.012801 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  221
initial  value 57.885111 
iter  10 value 0.185522
iter  20 value 0.133445
iter  30 value 0.109096
iter  40 value 0.064011
iter  50 value 0.025513
iter  60 value 0.014842
iter  70 value 0.012052
iter  80 value 0.010186
iter  90 value 0.009601
iter 100 value 0.009017
final  value 0.009017 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  221
initial  value 49.548600 
iter  10 value 0.186800
iter  20 value 0.161230
iter  30 value 0.135971
iter  40 value 0.110596
iter  50 value 0.049362
iter  60 value 0.029738
iter  70 value 0.022362
iter  80 value 0.015709
iter  90 value 0.013223
iter 100 value 0.010837
final  value 0.010837 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  221
initial  value 62.991447 
iter  10 value 0.223774
iter  20 value 0.156761
iter  30 value 0.136424
iter  40 value 0.110258
iter  50 value 0.066897
iter  60 value 0.046380
iter  70 value 0.036030
iter  80 value 0.027693
iter  90 value 0.020408
iter 100 value 0.015375
final  value 0.015375 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  155
initial  value 17.046719 
iter  10 value 0.199979
iter  20 value 0.164456
iter  30 value 0.116861
iter  40 value 0.087077
iter  50 value 0.055710
iter  60 value 0.039041
iter  70 value 0.031489
iter  80 value 0.028022
iter  90 value 0.026076
iter 100 value 0.024851
final  value 0.024851 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  155
initial  value 18.271725 
iter  10 value 0.224169
iter  20 value 0.169299
iter  30 value 0.149869
iter  40 value 0.131246
iter  50 value 0.109380
iter  60 value 0.047098
iter  70 value 0.036762
iter  80 value 0.029986
iter  90 value 0.027684
iter 100 value 0.026516
final  value 0.026516 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  155
initial  value 67.915528 
iter  10 value 0.204847
iter  20 value 0.171448
iter  30 value 0.133795
iter  40 value 0.085187
iter  50 value 0.064626
iter  60 value 0.051655
iter  70 value 0.042559
iter  80 value 0.034533
iter  90 value 0.032108
iter 100 value 0.029529
final  value 0.029529 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  155
initial  value 43.702433 
iter  10 value 0.378604
iter  20 value 0.175219
iter  30 value 0.135718
iter  40 value 0.069653
iter  50 value 0.046787
iter  60 value 0.036255
iter  70 value 0.031104
iter  80 value 0.028919
iter  90 value 0.027523
iter 100 value 0.026210
final  value 0.026210 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  155
initial  value 25.373018 
iter  10 value 0.192471
iter  20 value 0.162182
iter  30 value 0.132497
iter  40 value 0.078460
iter  50 value 0.059950
iter  60 value 0.045416
iter  70 value 0.037223
iter  80 value 0.033082
iter  90 value 0.030750
iter 100 value 0.027941
final  value 0.027941 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 32.408790 
iter  10 value 6.922395
iter  20 value 6.478116
iter  30 value 6.283904
iter  40 value 6.273763
iter  50 value 6.273661
final  value 6.273659 
converged
Fitting Repeat 2 

# weights:  89
initial  value 29.685173 
iter  10 value 7.237899
iter  20 value 6.428411
iter  30 value 6.377881
iter  40 value 6.360566
iter  50 value 6.358311
final  value 6.358297 
converged
Fitting Repeat 3 

# weights:  89
initial  value 49.130848 
iter  10 value 6.704479
iter  20 value 6.383521
iter  30 value 6.281117
iter  40 value 6.273781
final  value 6.273659 
converged
Fitting Repeat 4 

# weights:  89
initial  value 32.925883 
iter  10 value 6.716157
iter  20 value 6.398064
iter  30 value 6.360212
iter  40 value 6.358378
final  value 6.358297 
converged
Fitting Repeat 5 

# weights:  89
initial  value 81.405573 
iter  10 value 7.785516
iter  20 value 6.468543
iter  30 value 6.344091
iter  40 value 6.287070
iter  50 value 6.273831
iter  60 value 6.273660
final  value 6.273659 
converged
Fitting Repeat 1 

# weights:  155
initial  value 20.999068 
iter  10 value 0.304632
iter  20 value 0.229586
iter  30 value 0.201357
iter  40 value 0.181777
iter  50 value 0.165951
iter  60 value 0.159946
iter  70 value 0.154751
iter  80 value 0.150735
iter  90 value 0.148328
iter 100 value 0.146875
final  value 0.146875 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  155
initial  value 77.030831 
iter  10 value 0.315911
iter  20 value 0.231228
iter  30 value 0.208837
iter  40 value 0.195779
iter  50 value 0.183801
iter  60 value 0.173651
iter  70 value 0.160561
iter  80 value 0.156635
iter  90 value 0.153674
iter 100 value 0.151631
final  value 0.151631 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  155
initial  value 27.542013 
iter  10 value 0.288878
iter  20 value 0.243008
iter  30 value 0.227492
iter  40 value 0.216346
iter  50 value 0.206636
iter  60 value 0.187355
iter  70 value 0.176015
iter  80 value 0.172102
iter  90 value 0.168275
iter 100 value 0.163886
final  value 0.163886 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  155
initial  value 25.033272 
iter  10 value 0.668782
iter  20 value 0.374860
iter  30 value 0.239098
iter  40 value 0.200928
iter  50 value 0.181970
iter  60 value 0.169074
iter  70 value 0.161195
iter  80 value 0.156252
iter  90 value 0.154303
iter 100 value 0.152665
final  value 0.152665 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  155
initial  value 31.858019 
iter  10 value 0.283915
iter  20 value 0.237163
iter  30 value 0.218125
iter  40 value 0.210620
iter  50 value 0.204575
iter  60 value 0.196504
iter  70 value 0.182837
iter  80 value 0.174986
iter  90 value 0.170950
iter 100 value 0.165801
final  value 0.165801 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  188
initial  value 43.986786 
iter  10 value 0.214311
iter  20 value 0.167907
iter  30 value 0.138097
iter  40 value 0.102002
iter  50 value 0.067288
iter  60 value 0.054438
iter  70 value 0.047704
iter  80 value 0.043742
iter  90 value 0.040885
iter 100 value 0.038220
final  value 0.038220 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 50.814225 
iter  10 value 0.195029
iter  20 value 0.153521
iter  30 value 0.121437
iter  40 value 0.098958
iter  50 value 0.061591
iter  60 value 0.047701
iter  70 value 0.043779
iter  80 value 0.038786
iter  90 value 0.037491
iter 100 value 0.035739
final  value 0.035739 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 31.547345 
iter  10 value 0.261586
iter  20 value 0.187347
iter  30 value 0.173187
iter  40 value 0.143003
iter  50 value 0.105996
iter  60 value 0.073225
iter  70 value 0.062069
iter  80 value 0.048503
iter  90 value 0.043170
iter 100 value 0.041698
final  value 0.041698 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 51.854205 
iter  10 value 0.291978
iter  20 value 0.175735
iter  30 value 0.145739
iter  40 value 0.120080
iter  50 value 0.090183
iter  60 value 0.062564
iter  70 value 0.049929
iter  80 value 0.043387
iter  90 value 0.039207
iter 100 value 0.038024
final  value 0.038024 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 45.834885 
iter  10 value 0.353022
iter  20 value 0.198688
iter  30 value 0.158536
iter  40 value 0.134882
iter  50 value 0.082251
iter  60 value 0.062309
iter  70 value 0.049812
iter  80 value 0.044914
iter  90 value 0.041754
iter 100 value 0.040110
final  value 0.040110 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 40.912198 
iter  10 value 5.692465
iter  20 value 5.032397
iter  30 value 4.973731
iter  40 value 4.930512
iter  50 value 4.912409
iter  60 value 4.910922
final  value 4.910920 
converged
Fitting Repeat 2 

# weights:  89
initial  value 40.511521 
iter  10 value 5.296866
iter  20 value 5.023719
iter  30 value 4.933190
iter  40 value 4.911005
final  value 4.910920 
converged
Fitting Repeat 3 

# weights:  89
initial  value 29.156928 
iter  10 value 5.271038
iter  20 value 4.996352
iter  30 value 4.980895
iter  40 value 4.977849
final  value 4.977603 
converged
Fitting Repeat 4 

# weights:  89
initial  value 22.775106 
iter  10 value 5.016785
iter  20 value 4.922877
iter  30 value 4.910959
final  value 4.910920 
converged
Fitting Repeat 5 

# weights:  89
initial  value 30.918411 
iter  10 value 5.177830
iter  20 value 4.981477
iter  30 value 4.977858
iter  40 value 4.977613
final  value 4.977603 
converged
Fitting Repeat 1 

# weights:  210
initial  value 26.890832 
iter  10 value 0.255129
iter  20 value 0.220319
iter  30 value 0.192246
iter  40 value 0.163286
iter  50 value 0.151292
iter  60 value 0.139476
iter  70 value 0.129695
iter  80 value 0.122758
iter  90 value 0.119384
iter 100 value 0.117527
final  value 0.117527 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  210
initial  value 31.438626 
iter  10 value 0.236634
iter  20 value 0.203267
iter  30 value 0.183958
iter  40 value 0.160503
iter  50 value 0.143266
iter  60 value 0.131690
iter  70 value 0.124790
iter  80 value 0.119378
iter  90 value 0.114834
iter 100 value 0.112112
final  value 0.112112 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  210
initial  value 66.079552 
iter  10 value 0.233346
iter  20 value 0.185695
iter  30 value 0.167185
iter  40 value 0.158622
iter  50 value 0.155107
iter  60 value 0.153864
iter  70 value 0.150010
iter  80 value 0.139761
iter  90 value 0.134487
iter 100 value 0.131002
final  value 0.131002 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  210
initial  value 42.517799 
iter  10 value 0.282274
iter  20 value 0.217632
iter  30 value 0.188879
iter  40 value 0.170886
iter  50 value 0.153879
iter  60 value 0.145162
iter  70 value 0.140192
iter  80 value 0.135677
iter  90 value 0.130314
iter 100 value 0.124984
final  value 0.124984 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  210
initial  value 53.658917 
iter  10 value 0.309096
iter  20 value 0.216241
iter  30 value 0.196256
iter  40 value 0.178262
iter  50 value 0.158008
iter  60 value 0.147672
iter  70 value 0.141147
iter  80 value 0.135518
iter  90 value 0.129878
iter 100 value 0.125329
final  value 0.125329 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 18.784695 
iter  10 value 1.368107
iter  20 value 0.946846
iter  30 value 0.919136
iter  40 value 0.913058
iter  50 value 0.910137
iter  60 value 0.909033
iter  70 value 0.908638
iter  80 value 0.908618
iter  90 value 0.908615
iter 100 value 0.908608
final  value 0.908608 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 21.076119 
iter  10 value 1.376862
iter  20 value 0.994147
iter  30 value 0.971193
iter  40 value 0.954150
iter  50 value 0.950406
iter  60 value 0.949587
iter  70 value 0.949172
iter  80 value 0.948617
iter  90 value 0.948550
final  value 0.948546 
converged
Fitting Repeat 3 

# weights:  89
initial  value 44.837143 
iter  10 value 1.760102
iter  20 value 1.121440
iter  30 value 0.990036
iter  40 value 0.940141
iter  50 value 0.934764
iter  60 value 0.934378
iter  70 value 0.934301
iter  80 value 0.934272
iter  90 value 0.934254
iter 100 value 0.934221
final  value 0.934221 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 23.431566 
iter  10 value 2.575425
iter  20 value 1.011360
iter  30 value 0.958998
iter  40 value 0.945605
iter  50 value 0.924614
iter  60 value 0.909262
iter  70 value 0.905313
iter  80 value 0.903105
iter  90 value 0.901826
iter 100 value 0.900972
final  value 0.900972 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 30.524986 
iter  10 value 1.388054
iter  20 value 1.009150
iter  30 value 0.989853
iter  40 value 0.983717
iter  50 value 0.976175
iter  60 value 0.971818
iter  70 value 0.970212
iter  80 value 0.969529
iter  90 value 0.969255
iter 100 value 0.968853
final  value 0.968853 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 92.776756 
iter  10 value 19.766809
iter  20 value 19.174126
iter  30 value 19.144530
iter  40 value 19.142812
iter  50 value 19.142787
iter  50 value 19.142787
iter  50 value 19.142787
final  value 19.142787 
converged
Fitting Repeat 2 

# weights:  177
initial  value 201.303665 
iter  10 value 21.452401
iter  20 value 19.844745
iter  30 value 19.666192
iter  40 value 19.638513
final  value 19.638139 
converged
Fitting Repeat 3 

# weights:  177
initial  value 80.856277 
iter  10 value 20.680388
iter  20 value 20.401664
iter  30 value 20.383846
iter  40 value 20.382073
final  value 20.381642 
converged
Fitting Repeat 4 

# weights:  177
initial  value 83.951424 
iter  10 value 18.539116
iter  20 value 18.232300
iter  30 value 18.214598
iter  40 value 18.213829
final  value 18.213822 
converged
Fitting Repeat 5 

# weights:  177
initial  value 80.445726 
iter  10 value 19.681274
iter  20 value 19.537058
iter  30 value 19.520203
iter  40 value 19.515849
final  value 19.515760 
converged
Fitting Repeat 1 

# weights:  144
initial  value 176.611583 
iter  10 value 22.945703
iter  20 value 20.902610
iter  30 value 20.867961
final  value 20.867945 
converged
Fitting Repeat 2 

# weights:  144
initial  value 158.996045 
iter  10 value 21.940264
iter  20 value 20.873005
iter  30 value 20.867948
final  value 20.867945 
converged
Fitting Repeat 3 

# weights:  144
initial  value 152.356532 
iter  10 value 21.083797
iter  20 value 20.867950
final  value 20.867945 
converged
Fitting Repeat 4 

# weights:  144
initial  value 143.072215 
iter  10 value 21.811187
iter  20 value 20.871325
iter  30 value 20.867950
final  value 20.867945 
converged
Fitting Repeat 5 

# weights:  144
initial  value 200.172793 
iter  10 value 21.336327
iter  20 value 20.879579
iter  30 value 20.867954
final  value 20.867945 
converged
Fitting Repeat 1 

# weights:  89
initial  value 78.556275 
iter  10 value 10.745342
iter  20 value 10.370148
iter  30 value 10.277744
final  value 10.273972 
converged
Fitting Repeat 2 

# weights:  89
initial  value 70.852652 
iter  10 value 10.735115
iter  20 value 10.398832
iter  30 value 10.311634
final  value 10.310511 
converged
Fitting Repeat 3 

# weights:  89
initial  value 29.016290 
iter  10 value 10.522086
iter  20 value 10.331402
iter  30 value 10.310519
final  value 10.310511 
converged
Fitting Repeat 4 

# weights:  89
initial  value 66.690699 
iter  10 value 10.499651
iter  20 value 10.279671
iter  30 value 10.273982
final  value 10.273972 
converged
Fitting Repeat 5 

# weights:  89
initial  value 32.998726 
iter  10 value 10.502489
iter  20 value 10.332031
iter  30 value 10.310537
final  value 10.310511 
converged
Fitting Repeat 1 

# weights:  155
initial  value 18.964649 
iter  10 value 0.437582
iter  20 value 0.138225
iter  30 value 0.099870
iter  40 value 0.085651
iter  50 value 0.073005
iter  60 value 0.064122
iter  70 value 0.060784
iter  80 value 0.058053
iter  90 value 0.055753
iter 100 value 0.053410
final  value 0.053410 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  155
initial  value 21.599888 
iter  10 value 0.228217
iter  20 value 0.173461
iter  30 value 0.140352
iter  40 value 0.091052
iter  50 value 0.068393
iter  60 value 0.057968
iter  70 value 0.054958
iter  80 value 0.052764
iter  90 value 0.051439
iter 100 value 0.049935
final  value 0.049935 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  155
initial  value 25.988073 
iter  10 value 0.223504
iter  20 value 0.173671
iter  30 value 0.149605
iter  40 value 0.136039
iter  50 value 0.110248
iter  60 value 0.091431
iter  70 value 0.067505
iter  80 value 0.057708
iter  90 value 0.053034
iter 100 value 0.050721
final  value 0.050721 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  155
initial  value 32.965611 
iter  10 value 0.226838
iter  20 value 0.125971
iter  30 value 0.098040
iter  40 value 0.081932
iter  50 value 0.071600
iter  60 value 0.066744
iter  70 value 0.063685
iter  80 value 0.061166
iter  90 value 0.057736
iter 100 value 0.056125
final  value 0.056125 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  155
initial  value 26.296548 
iter  10 value 0.231831
iter  20 value 0.181832
iter  30 value 0.156591
iter  40 value 0.131898
iter  50 value 0.095971
iter  60 value 0.074429
iter  70 value 0.061778
iter  80 value 0.055831
iter  90 value 0.053666
iter 100 value 0.052358
final  value 0.052358 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  199
initial  value 71.729042 
iter  10 value 5.010589
iter  20 value 4.471688
iter  30 value 4.440332
iter  40 value 4.378523
iter  50 value 4.342932
iter  60 value 4.333315
iter  70 value 4.331668
iter  80 value 4.330951
final  value 4.330938 
converged
Fitting Repeat 2 

# weights:  199
initial  value 29.207670 
iter  10 value 5.024088
iter  20 value 4.434327
iter  30 value 4.306321
iter  40 value 4.279961
iter  50 value 4.276541
iter  60 value 4.272365
iter  70 value 4.272202
iter  80 value 4.272145
final  value 4.272144 
converged
Fitting Repeat 3 

# weights:  199
initial  value 52.975528 
iter  10 value 4.781860
iter  20 value 4.369187
iter  30 value 4.332933
iter  40 value 4.309152
iter  50 value 4.287925
iter  60 value 4.287134
final  value 4.287106 
converged
Fitting Repeat 4 

# weights:  199
initial  value 68.271652 
iter  10 value 7.206309
iter  20 value 4.491683
iter  30 value 4.450279
iter  40 value 4.437813
iter  50 value 4.424256
iter  60 value 4.422613
iter  70 value 4.421837
iter  80 value 4.421678
final  value 4.421677 
converged
Fitting Repeat 5 

# weights:  199
initial  value 34.365138 
iter  10 value 4.657512
iter  20 value 4.354759
iter  30 value 4.332718
iter  40 value 4.321180
iter  50 value 4.314160
iter  60 value 4.313460
final  value 4.313459 
converged
Fitting Repeat 1 

# weights:  89
initial  value 103.851645 
iter  10 value 0.204449
iter  20 value 0.148166
iter  30 value 0.102539
iter  40 value 0.068440
iter  50 value 0.055538
iter  60 value 0.049918
iter  70 value 0.046493
iter  80 value 0.045671
iter  90 value 0.044759
iter 100 value 0.044229
final  value 0.044229 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 21.558507 
iter  10 value 0.207574
iter  20 value 0.125271
iter  30 value 0.095710
iter  40 value 0.060327
iter  50 value 0.050453
iter  60 value 0.044040
iter  70 value 0.041999
iter  80 value 0.041047
iter  90 value 0.040664
iter 100 value 0.040183
final  value 0.040183 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  89
initial  value 23.487354 
iter  10 value 0.294559
iter  20 value 0.157843
iter  30 value 0.126226
iter  40 value 0.081012
iter  50 value 0.063055
iter  60 value 0.052705
iter  70 value 0.049560
iter  80 value 0.048013
iter  90 value 0.046650
iter 100 value 0.045910
final  value 0.045910 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 21.428769 
iter  10 value 0.245875
iter  20 value 0.156900
iter  30 value 0.133853
iter  40 value 0.111385
iter  50 value 0.082799
iter  60 value 0.068980
iter  70 value 0.060007
iter  80 value 0.053058
iter  90 value 0.050341
iter 100 value 0.048936
final  value 0.048936 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 22.833435 
iter  10 value 0.415183
iter  20 value 0.148287
iter  30 value 0.090862
iter  40 value 0.077297
iter  50 value 0.070764
iter  60 value 0.060974
iter  70 value 0.057082
iter  80 value 0.055708
iter  90 value 0.053438
iter 100 value 0.051832
final  value 0.051832 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  133
initial  value 23.386724 
iter  10 value 0.984269
iter  20 value 0.488844
iter  30 value 0.302966
iter  40 value 0.278077
iter  50 value 0.271753
iter  60 value 0.267091
iter  70 value 0.264453
iter  80 value 0.263460
iter  90 value 0.262868
iter 100 value 0.262495
final  value 0.262495 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  133
initial  value 40.719287 
iter  10 value 1.493970
iter  20 value 0.504746
iter  30 value 0.374130
iter  40 value 0.319473
iter  50 value 0.300700
iter  60 value 0.288916
iter  70 value 0.280448
iter  80 value 0.274675
iter  90 value 0.270024
iter 100 value 0.267708
final  value 0.267708 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 29.129862 
iter  10 value 0.446441
iter  20 value 0.311244
iter  30 value 0.277933
iter  40 value 0.268969
iter  50 value 0.266459
iter  60 value 0.265190
iter  70 value 0.264214
iter  80 value 0.263587
iter  90 value 0.262993
iter 100 value 0.262401
final  value 0.262401 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  133
initial  value 18.784036 
iter  10 value 0.378251
iter  20 value 0.293167
iter  30 value 0.271822
iter  40 value 0.267653
iter  50 value 0.265036
iter  60 value 0.263941
iter  70 value 0.263535
iter  80 value 0.262892
iter  90 value 0.261194
iter 100 value 0.259838
final  value 0.259838 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  133
initial  value 17.505442 
iter  10 value 0.497327
iter  20 value 0.311961
iter  30 value 0.274008
iter  40 value 0.265790
iter  50 value 0.264329
iter  60 value 0.262854
iter  70 value 0.260017
iter  80 value 0.258619
iter  90 value 0.258133
iter 100 value 0.258011
final  value 0.258011 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  34
initial  value 26.789674 
iter  10 value 3.653711
iter  20 value 3.457605
iter  30 value 3.452668
iter  40 value 3.452435
final  value 3.452435 
converged
Fitting Repeat 2 

# weights:  34
initial  value 34.518532 
iter  10 value 5.384233
iter  20 value 4.633437
iter  30 value 4.100106
iter  40 value 3.726346
iter  50 value 3.675601
iter  60 value 3.671791
iter  70 value 3.671370
iter  70 value 3.671370
final  value 3.671370 
converged
Fitting Repeat 3 

# weights:  34
initial  value 22.192287 
iter  10 value 4.954720
iter  20 value 3.803107
iter  30 value 3.692742
iter  40 value 3.686083
final  value 3.685980 
converged
Fitting Repeat 4 

# weights:  34
initial  value 24.138138 
iter  10 value 4.988445
iter  20 value 3.886014
iter  30 value 3.523555
iter  40 value 3.478090
iter  50 value 3.477756
final  value 3.477756 
converged
Fitting Repeat 5 

# weights:  34
initial  value 23.471596 
iter  10 value 4.077597
iter  20 value 3.560680
iter  30 value 3.543119
iter  40 value 3.540806
final  value 3.540805 
converged
Fitting Repeat 1 

# weights:  188
initial  value 19.054812 
iter  10 value 0.248983
iter  20 value 0.192814
iter  30 value 0.175031
iter  40 value 0.156742
iter  50 value 0.135188
iter  60 value 0.127061
iter  70 value 0.123340
iter  80 value 0.119210
iter  90 value 0.115578
iter 100 value 0.113455
final  value 0.113455 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 30.917750 
iter  10 value 0.253402
iter  20 value 0.193432
iter  30 value 0.175091
iter  40 value 0.163504
iter  50 value 0.146961
iter  60 value 0.132319
iter  70 value 0.122321
iter  80 value 0.116711
iter  90 value 0.113497
iter 100 value 0.111101
final  value 0.111101 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 25.624587 
iter  10 value 0.237877
iter  20 value 0.190359
iter  30 value 0.166973
iter  40 value 0.157589
iter  50 value 0.147979
iter  60 value 0.135089
iter  70 value 0.119803
iter  80 value 0.112251
iter  90 value 0.106780
iter 100 value 0.103123
final  value 0.103123 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 44.711168 
iter  10 value 0.281840
iter  20 value 0.197761
iter  30 value 0.177184
iter  40 value 0.164833
iter  50 value 0.153025
iter  60 value 0.136156
iter  70 value 0.123778
iter  80 value 0.115905
iter  90 value 0.109833
iter 100 value 0.106316
final  value 0.106316 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 19.660693 
iter  10 value 0.242049
iter  20 value 0.194584
iter  30 value 0.173974
iter  40 value 0.163375
iter  50 value 0.156923
iter  60 value 0.147930
iter  70 value 0.134949
iter  80 value 0.120683
iter  90 value 0.112904
iter 100 value 0.106501
final  value 0.106501 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  56
initial  value 19.997546 
iter  10 value 0.604967
iter  20 value 0.491833
iter  30 value 0.463489
iter  40 value 0.449498
iter  50 value 0.446843
iter  60 value 0.445075
iter  70 value 0.443305
iter  80 value 0.442290
iter  90 value 0.440921
iter 100 value 0.440139
final  value 0.440139 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 47.528389 
iter  10 value 0.596746
iter  20 value 0.471060
iter  30 value 0.452866
iter  40 value 0.450939
iter  50 value 0.449817
iter  60 value 0.447525
iter  70 value 0.445190
iter  80 value 0.444538
iter  90 value 0.444240
iter 100 value 0.444186
final  value 0.444186 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 49.163710 
iter  10 value 0.577422
iter  20 value 0.465081
iter  30 value 0.443785
iter  40 value 0.439621
iter  50 value 0.438930
iter  60 value 0.438630
iter  70 value 0.438332
iter  80 value 0.438050
iter  90 value 0.437970
iter 100 value 0.437910
final  value 0.437910 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 20.146577 
iter  10 value 1.083446
iter  20 value 0.529772
iter  30 value 0.485581
iter  40 value 0.454632
iter  50 value 0.442640
iter  60 value 0.436176
iter  70 value 0.432744
iter  80 value 0.430621
iter  90 value 0.430177
iter 100 value 0.429717
final  value 0.429717 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 23.647668 
iter  10 value 0.599172
iter  20 value 0.448906
iter  30 value 0.414720
iter  40 value 0.408052
iter  50 value 0.406785
iter  60 value 0.406441
iter  70 value 0.406341
iter  80 value 0.406176
iter  90 value 0.406087
iter 100 value 0.406041
final  value 0.406041 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  78
initial  value 18.076422 
iter  10 value 4.171425
iter  20 value 3.865874
iter  30 value 3.849399
iter  40 value 3.811554
iter  50 value 3.806440
iter  60 value 3.806397
final  value 3.806396 
converged
Fitting Repeat 2 

# weights:  78
initial  value 28.284719 
iter  10 value 4.843635
iter  20 value 4.133044
iter  30 value 4.021519
iter  40 value 3.970345
iter  50 value 3.953845
iter  60 value 3.939942
iter  70 value 3.938776
final  value 3.938774 
converged
Fitting Repeat 3 

# weights:  78
initial  value 23.278370 
iter  10 value 4.568760
iter  20 value 4.102923
iter  30 value 4.035696
iter  40 value 4.005920
iter  50 value 3.988564
iter  60 value 3.988178
final  value 3.988178 
converged
Fitting Repeat 4 

# weights:  78
initial  value 53.168223 
iter  10 value 4.564463
iter  20 value 3.925838
iter  30 value 3.872878
iter  40 value 3.854657
iter  50 value 3.852805
iter  60 value 3.852321
final  value 3.852321 
converged
Fitting Repeat 5 

# weights:  78
initial  value 22.261035 
iter  10 value 4.438397
iter  20 value 3.813111
iter  30 value 3.748355
iter  40 value 3.741786
iter  50 value 3.736712
iter  60 value 3.735285
final  value 3.735268 
converged
Fitting Repeat 1 

# weights:  177
initial  value 29.052674 
iter  10 value 4.971352
iter  20 value 3.803489
iter  30 value 3.569354
iter  40 value 3.512347
iter  50 value 3.507634
iter  60 value 3.505096
iter  70 value 3.502207
iter  80 value 3.499012
iter  90 value 3.498117
final  value 3.498110 
converged
Fitting Repeat 2 

# weights:  177
initial  value 28.916575 
iter  10 value 3.845917
iter  20 value 3.441178
iter  30 value 3.398754
iter  40 value 3.390173
iter  50 value 3.381628
iter  60 value 3.372568
iter  70 value 3.370623
final  value 3.370615 
converged
Fitting Repeat 3 

# weights:  177
initial  value 43.759641 
iter  10 value 3.789114
iter  20 value 3.473398
iter  30 value 3.426265
iter  40 value 3.411017
iter  50 value 3.408222
iter  60 value 3.405581
iter  70 value 3.405401
final  value 3.405400 
converged
Fitting Repeat 4 

# weights:  177
initial  value 27.866108 
iter  10 value 4.016923
iter  20 value 3.544969
iter  30 value 3.499530
iter  40 value 3.488003
iter  50 value 3.481179
iter  60 value 3.477221
iter  70 value 3.477122
iter  80 value 3.477108
final  value 3.477108 
converged
Fitting Repeat 5 

# weights:  177
initial  value 36.303558 
iter  10 value 4.171303
iter  20 value 3.588839
iter  30 value 3.576010
iter  40 value 3.568469
iter  50 value 3.565427
iter  60 value 3.562740
iter  70 value 3.560950
final  value 3.560929 
converged
Fitting Repeat 1 

# weights:  221
initial  value 22.491018 
iter  10 value 0.148628
iter  20 value 0.109908
iter  30 value 0.093494
iter  40 value 0.079931
iter  50 value 0.057593
iter  60 value 0.035011
iter  70 value 0.020267
iter  80 value 0.015432
iter  90 value 0.010733
iter 100 value 0.009684
final  value 0.009684 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  221
initial  value 29.207908 
iter  10 value 0.651711
iter  20 value 0.122986
iter  30 value 0.049441
iter  40 value 0.026713
iter  50 value 0.021032
iter  60 value 0.017791
iter  70 value 0.015224
iter  80 value 0.013181
iter  90 value 0.012318
iter 100 value 0.011361
final  value 0.011361 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  221
initial  value 41.146118 
iter  10 value 0.139883
iter  20 value 0.113390
iter  30 value 0.095225
iter  40 value 0.083800
iter  50 value 0.067469
iter  60 value 0.039667
iter  70 value 0.025583
iter  80 value 0.016787
iter  90 value 0.013878
iter 100 value 0.011014
final  value 0.011014 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  221
initial  value 123.607636 
iter  10 value 4.957146
iter  20 value 0.233772
iter  30 value 0.073175
iter  40 value 0.035119
iter  50 value 0.021480
iter  60 value 0.018194
iter  70 value 0.016349
iter  80 value 0.014632
iter  90 value 0.013326
iter 100 value 0.012462
final  value 0.012462 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  221
initial  value 27.475548 
iter  10 value 0.196085
iter  20 value 0.135229
iter  30 value 0.111524
iter  40 value 0.096912
iter  50 value 0.067885
iter  60 value 0.029824
iter  70 value 0.014914
iter  80 value 0.009734
iter  90 value 0.008043
iter 100 value 0.007417
final  value 0.007417 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  155
initial  value 30.113087 
iter  10 value 0.203192
iter  20 value 0.156625
iter  30 value 0.135091
iter  40 value 0.098729
iter  50 value 0.058998
iter  60 value 0.044826
iter  70 value 0.036841
iter  80 value 0.030541
iter  90 value 0.026725
iter 100 value 0.025585
final  value 0.025585 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  155
initial  value 28.526156 
iter  10 value 0.711605
iter  20 value 0.114552
iter  30 value 0.065231
iter  40 value 0.036435
iter  50 value 0.031970
iter  60 value 0.028033
iter  70 value 0.026157
iter  80 value 0.025410
iter  90 value 0.024652
iter 100 value 0.024283
final  value 0.024283 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  155
initial  value 28.276345 
iter  10 value 0.399729
iter  20 value 0.080800
iter  30 value 0.053053
iter  40 value 0.041285
iter  50 value 0.037699
iter  60 value 0.035825
iter  70 value 0.033194
iter  80 value 0.031351
iter  90 value 0.029636
iter 100 value 0.028766
final  value 0.028766 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  155
initial  value 17.924357 
iter  10 value 0.406437
iter  20 value 0.095857
iter  30 value 0.054712
iter  40 value 0.040550
iter  50 value 0.032010
iter  60 value 0.029830
iter  70 value 0.028467
iter  80 value 0.027186
iter  90 value 0.025916
iter 100 value 0.024722
final  value 0.024722 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  155
initial  value 46.242301 
iter  10 value 0.435622
iter  20 value 0.092820
iter  30 value 0.054720
iter  40 value 0.036101
iter  50 value 0.032144
iter  60 value 0.029547
iter  70 value 0.028631
iter  80 value 0.027521
iter  90 value 0.026823
iter 100 value 0.026095
final  value 0.026095 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 27.389342 
iter  10 value 6.558766
iter  20 value 6.324258
iter  30 value 6.229916
iter  40 value 6.221678
final  value 6.221631 
converged
Fitting Repeat 2 

# weights:  89
initial  value 28.403352 
iter  10 value 6.468636
iter  20 value 6.140004
iter  30 value 6.136819
iter  40 value 6.136722
final  value 6.136722 
converged
Fitting Repeat 3 

# weights:  89
initial  value 38.632493 
iter  10 value 6.765943
iter  20 value 6.228261
iter  30 value 6.221944
iter  40 value 6.221687
final  value 6.221631 
converged
Fitting Repeat 4 

# weights:  89
initial  value 26.621100 
iter  10 value 6.383833
iter  20 value 6.229132
iter  30 value 6.221854
iter  40 value 6.221634
final  value 6.221631 
converged
Fitting Repeat 5 

# weights:  89
initial  value 81.526422 
iter  10 value 7.417673
iter  20 value 6.477998
iter  30 value 6.408478
iter  40 value 6.398940
iter  50 value 6.397981
iter  60 value 6.397852
iter  60 value 6.397852
iter  60 value 6.397852
final  value 6.397852 
converged
Fitting Repeat 1 

# weights:  155
initial  value 27.644782 
iter  10 value 0.471982
iter  20 value 0.293239
iter  30 value 0.221976
iter  40 value 0.187038
iter  50 value 0.164413
iter  60 value 0.152527
iter  70 value 0.147735
iter  80 value 0.145821
iter  90 value 0.144425
iter 100 value 0.143896
final  value 0.143896 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  155
initial  value 21.631505 
iter  10 value 0.225950
iter  20 value 0.185404
iter  30 value 0.169469
iter  40 value 0.162249
iter  50 value 0.157996
iter  60 value 0.151361
iter  70 value 0.147141
iter  80 value 0.144229
iter  90 value 0.142632
iter 100 value 0.141679
final  value 0.141679 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  155
initial  value 22.343309 
iter  10 value 0.244228
iter  20 value 0.207248
iter  30 value 0.189696
iter  40 value 0.182643
iter  50 value 0.170695
iter  60 value 0.157709
iter  70 value 0.151009
iter  80 value 0.148341
iter  90 value 0.146541
iter 100 value 0.145269
final  value 0.145269 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  155
initial  value 43.888674 
iter  10 value 0.344012
iter  20 value 0.237831
iter  30 value 0.204258
iter  40 value 0.179381
iter  50 value 0.172350
iter  60 value 0.165112
iter  70 value 0.162381
iter  80 value 0.161190
iter  90 value 0.159424
iter 100 value 0.158158
final  value 0.158158 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  155
initial  value 17.083957 
iter  10 value 0.260815
iter  20 value 0.207693
iter  30 value 0.181406
iter  40 value 0.164746
iter  50 value 0.154767
iter  60 value 0.148378
iter  70 value 0.144708
iter  80 value 0.141657
iter  90 value 0.139320
iter 100 value 0.138359
final  value 0.138359 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  188
initial  value 88.663419 
iter  10 value 0.592683
iter  20 value 0.102419
iter  30 value 0.065695
iter  40 value 0.054800
iter  50 value 0.047385
iter  60 value 0.043491
iter  70 value 0.039633
iter  80 value 0.036775
iter  90 value 0.034531
iter 100 value 0.032838
final  value 0.032838 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 25.480800 
iter  10 value 0.363371
iter  20 value 0.176065
iter  30 value 0.136758
iter  40 value 0.091355
iter  50 value 0.071900
iter  60 value 0.061575
iter  70 value 0.052824
iter  80 value 0.045388
iter  90 value 0.043557
iter 100 value 0.042157
final  value 0.042157 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 112.383299 
iter  10 value 0.338646
iter  20 value 0.141783
iter  30 value 0.118604
iter  40 value 0.101481
iter  50 value 0.078066
iter  60 value 0.062262
iter  70 value 0.055476
iter  80 value 0.047747
iter  90 value 0.043864
iter 100 value 0.041078
final  value 0.041078 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 50.762343 
iter  10 value 0.207441
iter  20 value 0.147550
iter  30 value 0.112562
iter  40 value 0.069098
iter  50 value 0.053839
iter  60 value 0.046718
iter  70 value 0.042742
iter  80 value 0.040259
iter  90 value 0.038225
iter 100 value 0.036940
final  value 0.036940 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 17.521278 
iter  10 value 0.552382
iter  20 value 0.139126
iter  30 value 0.079758
iter  40 value 0.066636
iter  50 value 0.059046
iter  60 value 0.053574
iter  70 value 0.050819
iter  80 value 0.048540
iter  90 value 0.047383
iter 100 value 0.045558
final  value 0.045558 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 22.056886 
iter  10 value 5.227941
iter  20 value 4.928022
iter  30 value 4.884533
iter  40 value 4.875761
iter  50 value 4.875647
iter  50 value 4.875647
iter  50 value 4.875647
final  value 4.875647 
converged
Fitting Repeat 2 

# weights:  89
initial  value 27.381216 
iter  10 value 5.262952
iter  20 value 5.025665
iter  30 value 5.019266
iter  40 value 5.017360
iter  50 value 5.017075
final  value 5.017072 
converged
Fitting Repeat 3 

# weights:  89
initial  value 46.503651 
iter  10 value 5.083131
iter  20 value 4.855722
iter  30 value 4.812196
iter  40 value 4.808560
final  value 4.808558 
converged
Fitting Repeat 4 

# weights:  89
initial  value 24.180726 
iter  10 value 6.232192
iter  20 value 5.058178
iter  30 value 4.987486
iter  40 value 4.893361
iter  50 value 4.876218
iter  60 value 4.875648
final  value 4.875647 
converged
Fitting Repeat 5 

# weights:  89
initial  value 26.510710 
iter  10 value 6.264927
iter  20 value 4.993834
iter  30 value 4.868241
iter  40 value 4.820575
iter  50 value 4.812873
iter  60 value 4.808574
final  value 4.808558 
converged
Fitting Repeat 1 

# weights:  210
initial  value 28.064055 
iter  10 value 0.525088
iter  20 value 0.251912
iter  30 value 0.186676
iter  40 value 0.148786
iter  50 value 0.128807
iter  60 value 0.117096
iter  70 value 0.112626
iter  80 value 0.109412
iter  90 value 0.107807
iter 100 value 0.106694
final  value 0.106694 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  210
initial  value 20.492034 
iter  10 value 0.213305
iter  20 value 0.186716
iter  30 value 0.171059
iter  40 value 0.147410
iter  50 value 0.131582
iter  60 value 0.123867
iter  70 value 0.117296
iter  80 value 0.113222
iter  90 value 0.110847
iter 100 value 0.109065
final  value 0.109065 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  210
initial  value 23.440981 
iter  10 value 0.232632
iter  20 value 0.191750
iter  30 value 0.158733
iter  40 value 0.132356
iter  50 value 0.122483
iter  60 value 0.115286
iter  70 value 0.111126
iter  80 value 0.109127
iter  90 value 0.107801
iter 100 value 0.107057
final  value 0.107057 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  210
initial  value 16.235680 
iter  10 value 0.448793
iter  20 value 0.215564
iter  30 value 0.192183
iter  40 value 0.159073
iter  50 value 0.141965
iter  60 value 0.135153
iter  70 value 0.131543
iter  80 value 0.126402
iter  90 value 0.121049
iter 100 value 0.116705
final  value 0.116705 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  210
initial  value 22.636810 
iter  10 value 0.270860
iter  20 value 0.228120
iter  30 value 0.201384
iter  40 value 0.182315
iter  50 value 0.158163
iter  60 value 0.145986
iter  70 value 0.136906
iter  80 value 0.129627
iter  90 value 0.125678
iter 100 value 0.123703
final  value 0.123703 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 28.176430 
iter  10 value 1.256108
iter  20 value 0.946025
iter  30 value 0.930398
iter  40 value 0.924718
iter  50 value 0.920526
iter  60 value 0.914905
iter  70 value 0.905524
iter  80 value 0.893840
iter  90 value 0.891188
iter 100 value 0.890943
final  value 0.890943 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 26.839033 
iter  10 value 2.227145
iter  20 value 1.015031
iter  30 value 0.938222
iter  40 value 0.931535
iter  50 value 0.927827
iter  60 value 0.920095
iter  70 value 0.916261
iter  80 value 0.912276
iter  90 value 0.909944
iter 100 value 0.909123
final  value 0.909123 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  89
initial  value 18.641491 
iter  10 value 1.381412
iter  20 value 0.900641
iter  30 value 0.880287
iter  40 value 0.877433
iter  50 value 0.875908
iter  60 value 0.874558
iter  70 value 0.873126
iter  80 value 0.872942
iter  90 value 0.872856
iter 100 value 0.872769
final  value 0.872769 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 22.087366 
iter  10 value 1.245221
iter  20 value 0.948269
iter  30 value 0.925965
iter  40 value 0.908507
iter  50 value 0.905358
iter  60 value 0.904935
iter  70 value 0.904565
iter  80 value 0.904305
iter  90 value 0.904137
iter 100 value 0.904099
final  value 0.904099 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 30.157567 
iter  10 value 1.558509
iter  20 value 0.896889
iter  30 value 0.875939
iter  40 value 0.870072
iter  50 value 0.865521
iter  60 value 0.862756
iter  70 value 0.859175
iter  80 value 0.855011
iter  90 value 0.854107
iter 100 value 0.853927
final  value 0.853927 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 79.915861 
iter  10 value 18.665487
iter  20 value 18.386702
iter  30 value 18.362868
iter  40 value 18.359122
iter  50 value 18.358774
final  value 18.358746 
converged
Fitting Repeat 2 

# weights:  177
initial  value 140.170573 
iter  10 value 21.218121
iter  20 value 18.678099
iter  30 value 18.554327
iter  40 value 18.548143
iter  50 value 18.546708
final  value 18.546682 
converged
Fitting Repeat 3 

# weights:  177
initial  value 127.093772 
iter  10 value 18.834528
iter  20 value 18.564909
iter  30 value 18.556505
iter  40 value 18.548955
iter  50 value 18.547284
final  value 18.547280 
converged
Fitting Repeat 4 

# weights:  177
initial  value 86.431749 
iter  10 value 21.215998
iter  20 value 20.305281
iter  30 value 20.275379
iter  40 value 20.260698
iter  50 value 20.259943
final  value 20.259933 
converged
Fitting Repeat 5 

# weights:  177
initial  value 83.544357 
iter  10 value 19.959006
iter  20 value 18.370565
iter  30 value 18.340472
iter  40 value 18.339004
iter  50 value 18.338981
final  value 18.338979 
converged
Fitting Repeat 1 

# weights:  144
initial  value 145.865165 
iter  10 value 21.593096
iter  20 value 19.615384
iter  30 value 19.585162
final  value 19.585136 
converged
Fitting Repeat 2 

# weights:  144
initial  value 208.916265 
iter  10 value 20.671006
iter  20 value 19.601066
iter  30 value 19.585213
final  value 19.585136 
converged
Fitting Repeat 3 

# weights:  144
initial  value 129.410069 
iter  10 value 21.347020
iter  20 value 19.596889
iter  30 value 19.585150
final  value 19.585136 
converged
Fitting Repeat 4 

# weights:  144
initial  value 128.792125 
iter  10 value 27.882075
iter  20 value 19.670285
iter  30 value 19.586293
final  value 19.585136 
converged
Fitting Repeat 5 

# weights:  144
initial  value 157.028990 
iter  10 value 21.708679
iter  20 value 19.605675
iter  30 value 19.585541
final  value 19.585136 
converged
Fitting Repeat 1 

# weights:  89
initial  value 41.996446 
iter  10 value 10.238853
iter  20 value 10.128850
iter  30 value 10.128249
iter  30 value 10.128249
iter  30 value 10.128249
final  value 10.128249 
converged
Fitting Repeat 2 

# weights:  89
initial  value 63.387775 
iter  10 value 11.558167
iter  20 value 10.311630
iter  30 value 10.128441
iter  40 value 10.128249
iter  40 value 10.128249
iter  40 value 10.128249
final  value 10.128249 
converged
Fitting Repeat 3 

# weights:  89
initial  value 35.528355 
iter  10 value 10.549534
iter  20 value 10.171410
iter  30 value 10.137357
iter  40 value 10.128250
final  value 10.128249 
converged
Fitting Repeat 4 

# weights:  89
initial  value 33.638278 
iter  10 value 10.471690
iter  20 value 10.182746
iter  30 value 10.098308
iter  40 value 9.998643
final  value 9.998609 
converged
Fitting Repeat 5 

# weights:  89
initial  value 26.994414 
iter  10 value 11.115454
iter  20 value 10.145993
iter  30 value 10.008835
iter  40 value 9.999071
final  value 9.998609 
converged
Fitting Repeat 1 

# weights:  155
initial  value 20.141422 
iter  10 value 0.298263
iter  20 value 0.116421
iter  30 value 0.087517
iter  40 value 0.072495
iter  50 value 0.063200
iter  60 value 0.059267
iter  70 value 0.056770
iter  80 value 0.053827
iter  90 value 0.051720
iter 100 value 0.050195
final  value 0.050195 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  155
initial  value 30.863738 
iter  10 value 0.602221
iter  20 value 0.162697
iter  30 value 0.115496
iter  40 value 0.094352
iter  50 value 0.079825
iter  60 value 0.071058
iter  70 value 0.066425
iter  80 value 0.062360
iter  90 value 0.058067
iter 100 value 0.055668
final  value 0.055668 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  155
initial  value 17.979285 
iter  10 value 0.715357
iter  20 value 0.124992
iter  30 value 0.094106
iter  40 value 0.077592
iter  50 value 0.068917
iter  60 value 0.062168
iter  70 value 0.059387
iter  80 value 0.057377
iter  90 value 0.055186
iter 100 value 0.053542
final  value 0.053542 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  155
initial  value 22.792020 
iter  10 value 0.204533
iter  20 value 0.144268
iter  30 value 0.126639
iter  40 value 0.110085
iter  50 value 0.068392
iter  60 value 0.052892
iter  70 value 0.049384
iter  80 value 0.047316
iter  90 value 0.045663
iter 100 value 0.044477
final  value 0.044477 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  155
initial  value 27.867499 
iter  10 value 0.250849
iter  20 value 0.158843
iter  30 value 0.143969
iter  40 value 0.130616
iter  50 value 0.118679
iter  60 value 0.103691
iter  70 value 0.091784
iter  80 value 0.069475
iter  90 value 0.062699
iter 100 value 0.055487
final  value 0.055487 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  221
initial  value 25.384008 
iter  10 value 0.251648
iter  20 value 0.198796
iter  30 value 0.170182
iter  40 value 0.132431
iter  50 value 0.087131
iter  60 value 0.056291
iter  70 value 0.041069
iter  80 value 0.028513
iter  90 value 0.023696
iter 100 value 0.019740
final  value 0.019740 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  221
initial  value 54.106737 
iter  10 value 0.311571
iter  20 value 0.189553
iter  30 value 0.160658
iter  40 value 0.128816
iter  50 value 0.079219
iter  60 value 0.037279
iter  70 value 0.019697
iter  80 value 0.013464
iter  90 value 0.010030
iter 100 value 0.009098
final  value 0.009098 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  221
initial  value 46.282436 
iter  10 value 0.339595
iter  20 value 0.213689
iter  30 value 0.162220
iter  40 value 0.098807
iter  50 value 0.043341
iter  60 value 0.022955
iter  70 value 0.014996
iter  80 value 0.011878
iter  90 value 0.009841
iter 100 value 0.008346
final  value 0.008346 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  221
initial  value 33.741096 
iter  10 value 0.322281
iter  20 value 0.193393
iter  30 value 0.158540
iter  40 value 0.140297
iter  50 value 0.109723
iter  60 value 0.069113
iter  70 value 0.042071
iter  80 value 0.030657
iter  90 value 0.020073
iter 100 value 0.017491
final  value 0.017491 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  221
initial  value 40.823433 
iter  10 value 0.280062
iter  20 value 0.186130
iter  30 value 0.152936
iter  40 value 0.119583
iter  50 value 0.064070
iter  60 value 0.041846
iter  70 value 0.031247
iter  80 value 0.019891
iter  90 value 0.016530
iter 100 value 0.014289
final  value 0.014289 
stopped after 100 iterations
Model Averaged Neural Network 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  size  decay         bag    RMSE         Rsquared   MAE          Selected
   3    0.1505363399   TRUE  0.034195531  0.9894106  0.022179308          
   5    0.0094575873   TRUE  0.019706973  0.9909297  0.013607755          
   7    0.1860506215   TRUE  0.034108518  0.9914052  0.023460227          
   8    0.0003513622   TRUE  0.006245181  0.9990590  0.003349802          
   8    0.0338702219   TRUE  0.020048882  0.9915211  0.012883919          
   8    0.2481023680  FALSE  0.039881603  0.9909695  0.028937240          
   8    0.3335624159  FALSE  0.048858380  0.9896158  0.037142389          
   8    0.6328665552  FALSE  0.076926917  0.9808832  0.061946662          
  12    0.0047074044  FALSE  0.018000556  0.9923105  0.012589002          
  13    5.1154613290  FALSE  0.200440350  0.9159259  0.165428887          
  14    0.0001059793  FALSE  0.004414454  0.9995277  0.002470166          
  14    0.0003038330  FALSE  0.006079125  0.9991134  0.003369688          
  14    0.0016379824   TRUE  0.009852554  0.9977352  0.005654566          
  16    0.1830050068   TRUE  0.030656366  0.9924000  0.020967668          
  16    2.0940486010   TRUE  0.161185100  0.9320834  0.133228747          
  17    0.0002032754   TRUE  0.005528343  0.9992590  0.003045981          
  17    0.0010235172  FALSE  0.008133668  0.9984290  0.004377933          
  18    0.2353431183   TRUE  0.035793667  0.9912167  0.025936968          
  19    0.0011270169   TRUE  0.008344537  0.9983360  0.004626132          
  20    0.0000136168   TRUE  0.004335268  0.9995400  0.002408577  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 20, decay = 1.36168e-05 and
 bag = TRUE.
[1] "Mon Mar 12 08:09:08 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 08:09:15 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "bag"                            
Bagged MARS 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  degree  nprune  RMSE         Rsquared   MAE          Selected
  1       3       0.026565382  0.9825252  0.021440324          
  1       4       0.016305716  0.9934647  0.013471685          
  1       5       0.009930157  0.9975247  0.008109573          
  1       6       0.007018047  0.9987776  0.005298709          
  1       7       0.006155447  0.9990539  0.004661259          
  2       2       0.039594991  0.9624874  0.031709042          
  2       4       0.016406832  0.9933769  0.013617231          
  2       5       0.009675216  0.9976906  0.007860321          
  2       6       0.006886324  0.9988148  0.005172918          
  2       7       0.005736770  0.9991550  0.004327648  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nprune = 7 and degree = 2.
[1] "Mon Mar 12 08:09:56 2018"
Bagged MARS using gCV Pruning 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results:

  RMSE         Rsquared   MAE        
  0.006295609  0.9990062  0.004748545

Tuning parameter 'degree' was held constant at a value of 1
[1] "Mon Mar 12 08:10:30 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :6     NA's   :6     NA's   :6    
Error : Stopping
In addition: There were 19 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 13 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 08:11:06 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "bam"                            
bartMachine initializing with 87 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 131.6/477.6MB
Iteration 200/1250  mem: 66.6/477.6MB
Iteration 300/1250  mem: 93.3/477.6MB
Iteration 400/1250  mem: 115.5/477.6MB
Iteration 500/1250  mem: 135.6/477.6MB
Iteration 600/1250  mem: 144/477.6MB
Iteration 700/1250  mem: 141.9/477.6MB
Iteration 800/1250  mem: 122.5/477.6MB
Iteration 900/1250  mem: 98.5/477.6MB
Iteration 1000/1250  mem: 187.1/477.6MB
Iteration 1100/1250  mem: 163.6/477.6MB
Iteration 1200/1250  mem: 136.4/477.6MB
done building BART in 4.375 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 44 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 136.2/477.6MB
Iteration 200/1250  mem: 179.4/477.6MB
Iteration 300/1250  mem: 120/477.6MB
Iteration 400/1250  mem: 171.9/477.6MB
Iteration 500/1250  mem: 119.3/477.6MB
Iteration 600/1250  mem: 170.8/477.6MB
Iteration 700/1250  mem: 121.7/477.6MB
Iteration 800/1250  mem: 173.6/477.6MB
Iteration 900/1250  mem: 123.9/477.6MB
Iteration 1000/1250  mem: 178.4/477.6MB
Iteration 1100/1250  mem: 126.3/477.6MB
Iteration 1200/1250  mem: 177.3/477.6MB
done building BART in 2.141 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 62 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 199.9/477.6MB
Iteration 200/1250  mem: 223.7/477.6MB
Iteration 300/1250  mem: 145.9/477.6MB
Iteration 400/1250  mem: 166.1/477.6MB
Iteration 500/1250  mem: 184.4/477.6MB
Iteration 600/1250  mem: 198.4/477.6MB
Iteration 700/1250  mem: 212/477.6MB
Iteration 800/1250  mem: 225.8/477.6MB
Iteration 900/1250  mem: 235.2/477.6MB
Iteration 1000/1250  mem: 244.6/477.6MB
Iteration 1100/1250  mem: 252.9/477.6MB
Iteration 1200/1250  mem: 259.8/477.6MB
done building BART in 3.047 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 22 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 249.2/477.6MB
Iteration 200/1250  mem: 223/477.6MB
Iteration 300/1250  mem: 192.3/477.6MB
Iteration 400/1250  mem: 273.1/477.6MB
Iteration 500/1250  mem: 242.6/477.6MB
Iteration 600/1250  mem: 214.6/477.6MB
Iteration 700/1250  mem: 184.7/477.6MB
Iteration 800/1250  mem: 263.8/477.6MB
Iteration 900/1250  mem: 239.6/477.6MB
Iteration 1000/1250  mem: 213.8/477.6MB
Iteration 1100/1250  mem: 197.6/477.6MB
Iteration 1200/1250  mem: 278.5/477.6MB
done building BART in 1.032 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 85 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 255.4/477.6MB
Iteration 200/1250  mem: 257.7/477.6MB
Iteration 300/1250  mem: 241.1/477.6MB
Iteration 400/1250  mem: 267.6/477.6MB
Iteration 500/1250  mem: 242.7/477.6MB
Iteration 600/1250  mem: 269.6/477.6MB
Iteration 700/1250  mem: 272.6/477.6MB
Iteration 800/1250  mem: 257.4/477.6MB
Iteration 900/1250  mem: 307.7/477.6MB
Iteration 1000/1250  mem: 341.9/477.6MB
Iteration 1100/1250  mem: 262.3/477.6MB
Iteration 1200/1250  mem: 265.3/477.6MB
done building BART in 4.328 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 28 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 330.7/477.6MB
Iteration 200/1250  mem: 324/477.6MB
Iteration 300/1250  mem: 315.7/477.6MB
Iteration 400/1250  mem: 304.1/477.6MB
Iteration 500/1250  mem: 295.6/477.6MB
Iteration 600/1250  mem: 283.4/477.6MB
Iteration 700/1250  mem: 381.7/477.6MB
Iteration 800/1250  mem: 367.6/477.6MB
Iteration 900/1250  mem: 357.6/477.6MB
Iteration 1000/1250  mem: 345.6/477.6MB
Iteration 1100/1250  mem: 341.7/477.6MB
Iteration 1200/1250  mem: 334.8/477.6MB
done building BART in 1.265 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 40 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 358.2/477.6MB
Iteration 200/1250  mem: 322.5/477.6MB
Iteration 300/1250  mem: 303.6/477.6MB
Iteration 400/1250  mem: 368.8/477.6MB
Iteration 500/1250  mem: 348.2/477.6MB
Iteration 600/1250  mem: 323.1/477.6MB
Iteration 700/1250  mem: 382.5/477.6MB
Iteration 800/1250  mem: 88.4/477.6MB
Iteration 900/1250  mem: 57.3/477.6MB
Iteration 1000/1250  mem: 121.5/477.6MB
Iteration 1100/1250  mem: 106.6/477.6MB
Iteration 1200/1250  mem: 88.7/477.6MB
done building BART in 2.031 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 81 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 165.1/477.6MB
Iteration 200/1250  mem: 142.4/477.6MB
Iteration 300/1250  mem: 138.5/477.6MB
Iteration 400/1250  mem: 114.5/477.6MB
Iteration 500/1250  mem: 150.4/477.6MB
Iteration 600/1250  mem: 169.3/477.6MB
Iteration 700/1250  mem: 177.6/477.6MB
Iteration 800/1250  mem: 166.3/477.6MB
Iteration 900/1250  mem: 138.6/477.6MB
Iteration 1000/1250  mem: 204.1/477.6MB
Iteration 1100/1250  mem: 141/477.6MB
Iteration 1200/1250  mem: 193/477.6MB
done building BART in 3.968 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 100 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 190.6/477.6MB
Iteration 200/1250  mem: 153.2/477.6MB
Iteration 300/1250  mem: 220.8/477.6MB
Iteration 400/1250  mem: 182.8/477.6MB
Iteration 500/1250  mem: 236.2/477.6MB
Iteration 600/1250  mem: 202.8/477.6MB
Iteration 700/1250  mem: 168.3/477.6MB
Iteration 800/1250  mem: 216.8/477.6MB
Iteration 900/1250  mem: 249.4/477.6MB
Iteration 1000/1250  mem: 265.2/477.6MB
Iteration 1100/1250  mem: 264.6/477.6MB
Iteration 1200/1250  mem: 253.9/477.6MB
done building BART in 5.093 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 70 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 214.7/477.6MB
Iteration 200/1250  mem: 226.4/477.6MB
Iteration 300/1250  mem: 246.2/477.6MB
Iteration 400/1250  mem: 274.7/477.6MB
Iteration 500/1250  mem: 213.8/477.6MB
Iteration 600/1250  mem: 270.3/477.6MB
Iteration 700/1250  mem: 237.7/477.6MB
Iteration 800/1250  mem: 297.3/477.6MB
Iteration 900/1250  mem: 254.8/477.6MB
Iteration 1000/1250  mem: 303.9/477.6MB
Iteration 1100/1250  mem: 237.1/477.6MB
Iteration 1200/1250  mem: 272.3/477.6MB
done building BART in 3.406 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 43 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 247.8/477.6MB
Iteration 200/1250  mem: 296.8/477.6MB
Iteration 300/1250  mem: 346.3/477.6MB
Iteration 400/1250  mem: 294.9/477.6MB
Iteration 500/1250  mem: 354.5/477.6MB
Iteration 600/1250  mem: 321.2/477.6MB
Iteration 700/1250  mem: 299.6/477.6MB
Iteration 800/1250  mem: 289.9/477.6MB
Iteration 900/1250  mem: 286.7/477.6MB
Iteration 1000/1250  mem: 285.6/477.6MB
Iteration 1100/1250  mem: 287.2/477.6MB
Iteration 1200/1250  mem: 365.1/477.6MB
done building BART in 2.219 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 69 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 310.7/477.6MB
Iteration 200/1250  mem: 313.1/477.6MB
Iteration 300/1250  mem: 312.1/477.6MB
Iteration 400/1250  mem: 394.8/477.6MB
Iteration 500/1250  mem: 382.9/477.6MB
Iteration 600/1250  mem: 133.2/477.6MB
Iteration 700/1250  mem: 147.9/477.6MB
Iteration 800/1250  mem: 107.2/477.6MB
Iteration 900/1250  mem: 126.2/477.6MB
Iteration 1000/1250  mem: 130.8/477.6MB
Iteration 1100/1250  mem: 116.1/477.6MB
Iteration 1200/1250  mem: 178.4/477.6MB
done building BART in 3.578 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 83 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 149.2/477.6MB
Iteration 200/1250  mem: 174.9/477.6MB
Iteration 300/1250  mem: 142.1/477.6MB
Iteration 400/1250  mem: 197.9/477.6MB
Iteration 500/1250  mem: 158.2/477.6MB
Iteration 600/1250  mem: 191.7/477.6MB
Iteration 700/1250  mem: 218.5/477.6MB
Iteration 800/1250  mem: 228.4/477.6MB
Iteration 900/1250  mem: 220/477.6MB
Iteration 1000/1250  mem: 190/477.6MB
Iteration 1100/1250  mem: 265.7/477.6MB
Iteration 1200/1250  mem: 219.1/477.6MB
done building BART in 4.156 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 44 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 218.3/477.6MB
Iteration 200/1250  mem: 270.6/477.6MB
Iteration 300/1250  mem: 223.5/477.6MB
Iteration 400/1250  mem: 281.5/477.6MB
Iteration 500/1250  mem: 241.6/477.6MB
Iteration 600/1250  mem: 203/477.6MB
Iteration 700/1250  mem: 263.3/477.6MB
Iteration 800/1250  mem: 222/477.6MB
Iteration 900/1250  mem: 283/477.6MB
Iteration 1000/1250  mem: 238/477.6MB
Iteration 1100/1250  mem: 294.6/477.6MB
Iteration 1200/1250  mem: 247.5/477.6MB
done building BART in 2.188 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 92 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 307.6/477.6MB
Iteration 200/1250  mem: 306.3/477.6MB
Iteration 300/1250  mem: 316.2/477.6MB
Iteration 400/1250  mem: 305.8/477.6MB
Iteration 500/1250  mem: 272.7/477.6MB
Iteration 600/1250  mem: 337.1/477.6MB
Iteration 700/1250  mem: 303.4/477.6MB
Iteration 800/1250  mem: 341.1/477.6MB
Iteration 900/1250  mem: 358.9/477.6MB
Iteration 1000/1250  mem: 364.8/477.6MB
Iteration 1100/1250  mem: 357.9/477.6MB
Iteration 1200/1250  mem: 342.9/477.6MB
done building BART in 4.594 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 42 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 342.7/477.6MB
Iteration 200/1250  mem: 382.2/477.6MB
Iteration 300/1250  mem: 315.2/477.6MB
Iteration 400/1250  mem: 359/477.6MB
Iteration 500/1250  mem: 401.3/477.6MB
Iteration 600/1250  mem: 345/477.6MB
Iteration 700/1250  mem: 393.7/477.6MB
Iteration 800/1250  mem: 345.7/477.6MB
Iteration 900/1250  mem: 395.9/477.6MB
Iteration 1000/1250  mem: 346.5/477.6MB
Iteration 1100/1250  mem: 395.9/477.6MB
Iteration 1200/1250  mem: 344.7/477.6MB
done building BART in 2.078 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 82 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 354.6/477.6MB
Iteration 200/1250  mem: 385.3/477.6MB
Iteration 300/1250  mem: 426.9/477.6MB
Iteration 400/1250  mem: 384.4/477.6MB
Iteration 500/1250  mem: 140.6/477.6MB
Iteration 600/1250  mem: 139.7/477.6MB
Iteration 700/1250  mem: 102.7/477.6MB
Iteration 800/1250  mem: 182.5/477.6MB
Iteration 900/1250  mem: 145.2/477.6MB
Iteration 1000/1250  mem: 181.2/477.6MB
Iteration 1100/1250  mem: 137.5/477.6MB
Iteration 1200/1250  mem: 176.1/477.6MB
done building BART in 4.469 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 67 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 189.7/477.6MB
Iteration 200/1250  mem: 221/477.6MB
Iteration 300/1250  mem: 186.3/477.6MB
Iteration 400/1250  mem: 206.7/477.6MB
Iteration 500/1250  mem: 208.8/477.6MB
Iteration 600/1250  mem: 203.8/477.6MB
Iteration 700/1250  mem: 199.1/477.6MB
Iteration 800/1250  mem: 272.9/477.6MB
Iteration 900/1250  mem: 242.6/477.6MB
Iteration 1000/1250  mem: 195.8/477.6MB
Iteration 1100/1250  mem: 233.7/477.6MB
Iteration 1200/1250  mem: 269.5/477.6MB
done building BART in 3.532 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 42 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 253/477.6MB
Iteration 200/1250  mem: 300.2/477.6MB
Iteration 300/1250  mem: 257.4/477.6MB
Iteration 400/1250  mem: 314.8/477.6MB
Iteration 500/1250  mem: 282.1/477.6MB
Iteration 600/1250  mem: 252.8/477.6MB
Iteration 700/1250  mem: 314.6/477.6MB
Iteration 800/1250  mem: 288/477.6MB
Iteration 900/1250  mem: 259.4/477.6MB
Iteration 1000/1250  mem: 321.9/477.6MB
Iteration 1100/1250  mem: 291.1/477.6MB
Iteration 1200/1250  mem: 257.3/477.6MB
done building BART in 2.187 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 69 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 344.4/477.6MB
Iteration 200/1250  mem: 330.2/477.6MB
Iteration 300/1250  mem: 311.9/477.6MB
Iteration 400/1250  mem: 285.3/477.6MB
Iteration 500/1250  mem: 343.8/477.6MB
Iteration 600/1250  mem: 305.9/477.6MB
Iteration 700/1250  mem: 365.4/477.6MB
Iteration 800/1250  mem: 339.3/477.6MB
Iteration 900/1250  mem: 306/477.6MB
Iteration 1000/1250  mem: 363.7/477.6MB
Iteration 1100/1250  mem: 312/477.6MB
Iteration 1200/1250  mem: 355.6/477.6MB
done building BART in 3.453 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 87 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 421.7/477.6MB
Iteration 200/1250  mem: 359.7/477.6MB
Iteration 300/1250  mem: 407.4/477.6MB
Iteration 400/1250  mem: 355.7/477.6MB
Iteration 500/1250  mem: 383.3/477.6MB
Iteration 600/1250  mem: 407/477.6MB
Iteration 700/1250  mem: 429.6/477.6MB
Iteration 800/1250  mem: 91.5/477.6MB
Iteration 900/1250  mem: 109.1/477.6MB
Iteration 1000/1250  mem: 128/477.6MB
Iteration 1100/1250  mem: 130.4/477.6MB
Iteration 1200/1250  mem: 221.2/482.3MB
done building BART in 4.422 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 44 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 218.6/478.7MB
Iteration 200/1250  mem: 161.5/477.6MB
Iteration 300/1250  mem: 218.9/477.6MB
Iteration 400/1250  mem: 184.2/477.6MB
Iteration 500/1250  mem: 152.3/477.6MB
Iteration 600/1250  mem: 215.8/477.6MB
Iteration 700/1250  mem: 183.9/477.6MB
Iteration 800/1250  mem: 151.6/477.6MB
Iteration 900/1250  mem: 216.1/477.6MB
Iteration 1000/1250  mem: 183/477.6MB
Iteration 1100/1250  mem: 141/477.6MB
Iteration 1200/1250  mem: 198.8/477.6MB
done building BART in 2.156 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 62 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 228.2/477.6MB
Iteration 200/1250  mem: 253.5/477.6MB
Iteration 300/1250  mem: 185/477.6MB
Iteration 400/1250  mem: 212.1/477.6MB
Iteration 500/1250  mem: 236.7/477.6MB
Iteration 600/1250  mem: 257.5/477.6MB
Iteration 700/1250  mem: 172.2/477.6MB
Iteration 800/1250  mem: 183.6/477.6MB
Iteration 900/1250  mem: 197.4/477.6MB
Iteration 1000/1250  mem: 206.9/477.6MB
Iteration 1100/1250  mem: 214.2/477.6MB
Iteration 1200/1250  mem: 220.1/478.7MB
done building BART in 3.078 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 22 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 210.8/478.7MB
Iteration 200/1250  mem: 293.7/478.7MB
Iteration 300/1250  mem: 263.7/479.2MB
Iteration 400/1250  mem: 234.9/478.2MB
Iteration 500/1250  mem: 208/477.6MB
Iteration 600/1250  mem: 286.7/477.6MB
Iteration 700/1250  mem: 258.1/477.6MB
Iteration 800/1250  mem: 233.1/477.6MB
Iteration 900/1250  mem: 312.1/477.6MB
Iteration 1000/1250  mem: 288.7/477.6MB
Iteration 1100/1250  mem: 272.4/477.6MB
Iteration 1200/1250  mem: 257.5/477.6MB
done building BART in 1.016 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 85 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 285.3/477.6MB
Iteration 200/1250  mem: 298/477.6MB
Iteration 300/1250  mem: 289/477.6MB
Iteration 400/1250  mem: 289.8/477.6MB
Iteration 500/1250  mem: 320.3/477.6MB
Iteration 600/1250  mem: 307.7/477.6MB
Iteration 700/1250  mem: 343/477.6MB
Iteration 800/1250  mem: 344.4/477.6MB
Iteration 900/1250  mem: 313.8/481.3MB
Iteration 1000/1250  mem: 383.8/477.6MB
Iteration 1100/1250  mem: 336.8/480.2MB
Iteration 1200/1250  mem: 298.9/480.2MB
done building BART in 4.265 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 28 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 351.1/477.6MB
Iteration 200/1250  mem: 343.9/477.6MB
Iteration 300/1250  mem: 339.7/477.6MB
Iteration 400/1250  mem: 335.4/477.6MB
Iteration 500/1250  mem: 336.9/477.6MB
Iteration 600/1250  mem: 336.3/477.6MB
Iteration 700/1250  mem: 338.7/477.6MB
Iteration 800/1250  mem: 339.8/477.6MB
Iteration 900/1250  mem: 344.1/477.6MB
Iteration 1000/1250  mem: 349/477.6MB
Iteration 1100/1250  mem: 352.9/477.6MB
Iteration 1200/1250  mem: 356.7/477.6MB
done building BART in 1.343 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 40 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 386.6/477.6MB
Iteration 200/1250  mem: 350.8/477.6MB
Iteration 300/1250  mem: 405.6/477.6MB
Iteration 400/1250  mem: 371.3/477.6MB
Iteration 500/1250  mem: 335.4/477.6MB
Iteration 600/1250  mem: 386.8/477.6MB
Iteration 700/1250  mem: 340.8/477.6MB
Iteration 800/1250  mem: 384.4/477.6MB
Iteration 900/1250  mem: 424.9/477.6MB
Iteration 1000/1250  mem: 365.1/477.6MB
Iteration 1100/1250  mem: 403.2/477.6MB
Iteration 1200/1250  mem: 441.6/477.6MB
done building BART in 1.937 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 81 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 444.6/477.6MB
Iteration 200/1250  mem: 97.6/477.6MB
Iteration 300/1250  mem: 112.7/477.6MB
Iteration 400/1250  mem: 117.6/477.6MB
Iteration 500/1250  mem: 86.3/477.6MB
Iteration 600/1250  mem: 107.2/477.6MB
Iteration 700/1250  mem: 98.6/477.6MB
Iteration 800/1250  mem: 167.7/477.6MB
Iteration 900/1250  mem: 118/477.6MB
Iteration 1000/1250  mem: 187.2/477.6MB
Iteration 1100/1250  mem: 144.8/477.6MB
Iteration 1200/1250  mem: 103.5/477.6MB
done building BART in 4.047 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 100 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 138.1/477.6MB
Iteration 200/1250  mem: 149.9/477.6MB
Iteration 300/1250  mem: 163.5/477.6MB
Iteration 400/1250  mem: 152.6/477.6MB
Iteration 500/1250  mem: 138/477.6MB
Iteration 600/1250  mem: 133.8/477.6MB
Iteration 700/1250  mem: 206.5/477.6MB
Iteration 800/1250  mem: 146/477.6MB
Iteration 900/1250  mem: 170.4/477.6MB
Iteration 1000/1250  mem: 172.1/477.6MB
Iteration 1100/1250  mem: 163.1/477.6MB
Iteration 1200/1250  mem: 276.2/477.6MB
done building BART in 5.062 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 70 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 235.3/477.6MB
Iteration 200/1250  mem: 257.1/477.6MB
Iteration 300/1250  mem: 178/477.6MB
Iteration 400/1250  mem: 219.4/477.6MB
Iteration 500/1250  mem: 268/477.6MB
Iteration 600/1250  mem: 215.6/477.6MB
Iteration 700/1250  mem: 265.8/477.6MB
Iteration 800/1250  mem: 214.9/477.6MB
Iteration 900/1250  mem: 258.9/477.6MB
Iteration 1000/1250  mem: 296.9/477.6MB
Iteration 1100/1250  mem: 216.2/477.6MB
Iteration 1200/1250  mem: 238.1/477.6MB
done building BART in 3.422 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 43 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 318.2/477.6MB
Iteration 200/1250  mem: 250.8/477.6MB
Iteration 300/1250  mem: 299.9/477.6MB
Iteration 400/1250  mem: 252.5/477.6MB
Iteration 500/1250  mem: 309.8/477.6MB
Iteration 600/1250  mem: 275/477.6MB
Iteration 700/1250  mem: 255.8/477.6MB
Iteration 800/1250  mem: 248.6/477.6MB
Iteration 900/1250  mem: 327.5/477.6MB
Iteration 1000/1250  mem: 319.1/477.6MB
Iteration 1100/1250  mem: 307.7/477.6MB
Iteration 1200/1250  mem: 296.7/477.6MB
done building BART in 2.219 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 69 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 323.6/477.6MB
Iteration 200/1250  mem: 314.1/477.6MB
Iteration 300/1250  mem: 293.5/477.6MB
Iteration 400/1250  mem: 368.7/477.6MB
Iteration 500/1250  mem: 346.2/477.6MB
Iteration 600/1250  mem: 306.1/477.6MB
Iteration 700/1250  mem: 361.8/477.6MB
Iteration 800/1250  mem: 317.9/477.6MB
Iteration 900/1250  mem: 366/477.6MB
Iteration 1000/1250  mem: 406.2/477.6MB
Iteration 1100/1250  mem: 328.1/477.6MB
Iteration 1200/1250  mem: 351.8/477.6MB
done building BART in 3.422 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 83 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 372.4/477.6MB
Iteration 200/1250  mem: 375.5/477.6MB
Iteration 300/1250  mem: 143.6/477.6MB
Iteration 400/1250  mem: 126.5/477.6MB
Iteration 500/1250  mem: 102.9/477.6MB
Iteration 600/1250  mem: 123.4/477.6MB
Iteration 700/1250  mem: 111.4/477.6MB
Iteration 800/1250  mem: 202.1/477.6MB
Iteration 900/1250  mem: 188.9/477.6MB
Iteration 1000/1250  mem: 177.7/477.6MB
Iteration 1100/1250  mem: 164.4/477.6MB
Iteration 1200/1250  mem: 143.7/477.6MB
done building BART in 4.141 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 44 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 156.3/477.6MB
Iteration 200/1250  mem: 209.8/477.6MB
Iteration 300/1250  mem: 151/477.6MB
Iteration 400/1250  mem: 204.7/477.6MB
Iteration 500/1250  mem: 161.8/477.6MB
Iteration 600/1250  mem: 223.4/477.6MB
Iteration 700/1250  mem: 184/477.6MB
Iteration 800/1250  mem: 147.7/477.6MB
Iteration 900/1250  mem: 209.9/477.6MB
Iteration 1000/1250  mem: 171.1/477.6MB
Iteration 1100/1250  mem: 231.4/477.6MB
Iteration 1200/1250  mem: 188.8/477.6MB
done building BART in 2.125 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 92 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 244.2/477.6MB
Iteration 200/1250  mem: 224.5/477.6MB
Iteration 300/1250  mem: 220.3/477.6MB
Iteration 400/1250  mem: 206/477.6MB
Iteration 500/1250  mem: 269.9/477.6MB
Iteration 600/1250  mem: 202.8/477.6MB
Iteration 700/1250  mem: 211.5/477.6MB
Iteration 800/1250  mem: 214/477.6MB
Iteration 900/1250  mem: 324.6/477.6MB
Iteration 1000/1250  mem: 322.1/477.6MB
Iteration 1100/1250  mem: 312/477.6MB
Iteration 1200/1250  mem: 303.8/477.6MB
done building BART in 4.515 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 42 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 310.1/477.6MB
Iteration 200/1250  mem: 244.9/477.6MB
Iteration 300/1250  mem: 294/477.6MB
Iteration 400/1250  mem: 341.2/477.6MB
Iteration 500/1250  mem: 285.1/477.6MB
Iteration 600/1250  mem: 333.4/477.6MB
Iteration 700/1250  mem: 278.1/477.6MB
Iteration 800/1250  mem: 324.4/477.6MB
Iteration 900/1250  mem: 268.8/477.6MB
Iteration 1000/1250  mem: 320.3/477.6MB
Iteration 1100/1250  mem: 267.9/477.6MB
Iteration 1200/1250  mem: 316.8/477.6MB
done building BART in 1.985 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 82 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 320.4/477.6MB
Iteration 200/1250  mem: 334.4/477.6MB
Iteration 300/1250  mem: 364.1/477.6MB
Iteration 400/1250  mem: 393/477.6MB
Iteration 500/1250  mem: 327.5/477.6MB
Iteration 600/1250  mem: 338.4/477.6MB
Iteration 700/1250  mem: 432.4/477.6MB
Iteration 800/1250  mem: 406.4/477.6MB
Iteration 900/1250  mem: 127.8/490.2MB
Iteration 1000/1250  mem: 202.1/490.2MB
Iteration 1100/1250  mem: 164.3/490.7MB
Iteration 1200/1250  mem: 235.3/488.1MB
done building BART in 4.14 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 67 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 224.9/483.9MB
Iteration 200/1250  mem: 254.1/477.6MB
Iteration 300/1250  mem: 176.3/487.1MB
Iteration 400/1250  mem: 211/486.5MB
Iteration 500/1250  mem: 247.6/486.5MB
Iteration 600/1250  mem: 277.4/491.3MB
Iteration 700/1250  mem: 183.6/483.9MB
Iteration 800/1250  mem: 206.4/489.7MB
Iteration 900/1250  mem: 233.6/488.1MB
Iteration 1000/1250  mem: 254.8/491.3MB
Iteration 1100/1250  mem: 273.3/492.8MB
Iteration 1200/1250  mem: 288.4/477.6MB
done building BART in 3.14 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 42 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 247/492.8MB
Iteration 200/1250  mem: 284.8/493.4MB
Iteration 300/1250  mem: 320.1/491.3MB
Iteration 400/1250  mem: 248.5/489.2MB
Iteration 500/1250  mem: 294.5/481.3MB
Iteration 600/1250  mem: 231.2/477.6MB
Iteration 700/1250  mem: 275.4/488.6MB
Iteration 800/1250  mem: 319.4/488.1MB
Iteration 900/1250  mem: 252/491.8MB
Iteration 1000/1250  mem: 293.8/491.3MB
Iteration 1100/1250  mem: 335.5/491.8MB
Iteration 1200/1250  mem: 262/491.8MB
done building BART in 1.906 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 69 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 281.3/484.4MB
Iteration 200/1250  mem: 326.2/480.8MB
Iteration 300/1250  mem: 291.7/481.3MB
Iteration 400/1250  mem: 353.4/484.4MB
Iteration 500/1250  mem: 305.9/482.9MB
Iteration 600/1250  mem: 352.8/484.4MB
Iteration 700/1250  mem: 385.6/489.2MB
Iteration 800/1250  mem: 291/493.9MB
Iteration 900/1250  mem: 306.7/494.4MB
Iteration 1000/1250  mem: 317.2/494.9MB
Iteration 1100/1250  mem: 325.7/496MB
Iteration 1200/1250  mem: 329.3/496MB
done building BART in 3.25 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 87 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 329.3/485.5MB
Iteration 200/1250  mem: 427.7/492.3MB
Iteration 300/1250  mem: 408.4/491.3MB
Iteration 400/1250  mem: 394.9/477.6MB
Iteration 500/1250  mem: 380.8/492.8MB
Iteration 600/1250  mem: 348/493.9MB
Iteration 700/1250  mem: 429.8/496.5MB
Iteration 800/1250  mem: 385.5/493.4MB
Iteration 900/1250  mem: 354/483.9MB
Iteration 1000/1250  mem: 438.1/493.4MB
Iteration 1100/1250  mem: 406.2/496MB
Iteration 1200/1250  mem: 108/477.6MB
done building BART in 4.375 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 44 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 224.4/479.7MB
Iteration 200/1250  mem: 149.7/493.4MB
Iteration 300/1250  mem: 189.7/496MB
Iteration 400/1250  mem: 229/496MB
Iteration 500/1250  mem: 147.7/490.7MB
Iteration 600/1250  mem: 185.1/491.8MB
Iteration 700/1250  mem: 231.2/486.5MB
Iteration 800/1250  mem: 166.6/477.6MB
Iteration 900/1250  mem: 214.7/492.3MB
Iteration 1000/1250  mem: 146.5/492.3MB
Iteration 1100/1250  mem: 190.1/492.3MB
Iteration 1200/1250  mem: 235.4/494.4MB
done building BART in 2.032 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 62 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 226.4/493.4MB
Iteration 200/1250  mem: 216.4/495.5MB
Iteration 300/1250  mem: 205.6/484.4MB
Iteration 400/1250  mem: 194/494.9MB
Iteration 500/1250  mem: 176.2/495.5MB
Iteration 600/1250  mem: 278.5/495.5MB
Iteration 700/1250  mem: 254.5/498.6MB
Iteration 800/1250  mem: 219.5/499.6MB
Iteration 900/1250  mem: 187/499.1MB
Iteration 1000/1250  mem: 280/499.6MB
Iteration 1100/1250  mem: 243.4/500.2MB
Iteration 1200/1250  mem: 201.8/486MB
done building BART in 2.828 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 22 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 278.5/501.2MB
Iteration 200/1250  mem: 232/489.2MB
Iteration 300/1250  mem: 314.1/489.2MB
Iteration 400/1250  mem: 264/501.7MB
Iteration 500/1250  mem: 216.1/500.7MB
Iteration 600/1250  mem: 297.9/500.7MB
Iteration 700/1250  mem: 248.8/498.6MB
Iteration 800/1250  mem: 330.2/498.6MB
Iteration 900/1250  mem: 284.3/497.5MB
Iteration 1000/1250  mem: 238.1/498.1MB
Iteration 1100/1250  mem: 321.3/498.1MB
Iteration 1200/1250  mem: 277.4/485MB
done building BART in 0.891 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 85 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 297.6/498.6MB
Iteration 200/1250  mem: 354.8/499.1MB
Iteration 300/1250  mem: 282.4/498.6MB
Iteration 400/1250  mem: 335/498.6MB
Iteration 500/1250  mem: 376.1/496.5MB
Iteration 600/1250  mem: 290.4/494.9MB
Iteration 700/1250  mem: 354.1/492.3MB
Iteration 800/1250  mem: 305.6/492.8MB
Iteration 900/1250  mem: 368.4/493.9MB
Iteration 1000/1250  mem: 304.1/497MB
Iteration 1100/1250  mem: 359.7/497MB
Iteration 1200/1250  mem: 291.1/480.2MB
done building BART in 3.781 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 28 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 317.7/496MB
Iteration 200/1250  mem: 294.2/497MB
Iteration 300/1250  mem: 392.9/497MB
Iteration 400/1250  mem: 369.1/492.8MB
Iteration 500/1250  mem: 344.3/492.8MB
Iteration 600/1250  mem: 326.6/488.6MB
Iteration 700/1250  mem: 309.7/491.8MB
Iteration 800/1250  mem: 410.9/491.8MB
Iteration 900/1250  mem: 400.4/492.3MB
Iteration 1000/1250  mem: 387/490.2MB
Iteration 1100/1250  mem: 375.7/491.3MB
Iteration 1200/1250  mem: 363.5/492.3MB
done building BART in 1.188 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 40 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 355.6/492.8MB
Iteration 200/1250  mem: 382.6/493.9MB
Iteration 300/1250  mem: 406.3/496MB
Iteration 400/1250  mem: 429.5/496.5MB
Iteration 500/1250  mem: 450.6/496MB
Iteration 600/1250  mem: 347.9/497MB
Iteration 700/1250  mem: 363.8/497MB
Iteration 800/1250  mem: 379.2/499.1MB
Iteration 900/1250  mem: 392.6/499.6MB
Iteration 1000/1250  mem: 407.6/498.6MB
Iteration 1100/1250  mem: 420.7/499.1MB
Iteration 1200/1250  mem: 149.1/477.6MB
done building BART in 1.812 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 81 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 170.4/477.6MB
Iteration 200/1250  mem: 209.1/477.6MB
Iteration 300/1250  mem: 112.9/477.6MB
Iteration 400/1250  mem: 143.2/477.6MB
Iteration 500/1250  mem: 161.6/477.6MB
Iteration 600/1250  mem: 186.4/477.6MB
Iteration 700/1250  mem: 224.6/477.6MB
Iteration 800/1250  mem: 134.9/477.6MB
Iteration 900/1250  mem: 168.7/477.6MB
Iteration 1000/1250  mem: 196.2/477.6MB
Iteration 1100/1250  mem: 224/477.6MB
Iteration 1200/1250  mem: 252.5/477.6MB
done building BART in 3.593 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 100 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 162.7/477.6MB
Iteration 200/1250  mem: 187.7/477.6MB
Iteration 300/1250  mem: 211.2/477.6MB
Iteration 400/1250  mem: 233.7/477.6MB
Iteration 500/1250  mem: 239.3/477.6MB
Iteration 600/1250  mem: 223.2/477.6MB
Iteration 700/1250  mem: 204.5/477.6MB
Iteration 800/1250  mem: 189.8/477.6MB
Iteration 900/1250  mem: 297.6/477.6MB
Iteration 1000/1250  mem: 263.8/477.6MB
Iteration 1100/1250  mem: 223.3/477.6MB
Iteration 1200/1250  mem: 183.4/477.6MB
done building BART in 4.766 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 70 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 253.5/477.6MB
Iteration 200/1250  mem: 251.3/477.6MB
Iteration 300/1250  mem: 261.7/477.6MB
Iteration 400/1250  mem: 274.5/477.6MB
Iteration 500/1250  mem: 281.8/477.6MB
Iteration 600/1250  mem: 284.6/477.6MB
Iteration 700/1250  mem: 285.2/477.6MB
Iteration 800/1250  mem: 275/477.6MB
Iteration 900/1250  mem: 258.3/477.6MB
Iteration 1000/1250  mem: 238.3/477.6MB
Iteration 1100/1250  mem: 349.5/477.6MB
Iteration 1200/1250  mem: 318.1/477.6MB
done building BART in 3.125 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 43 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 340.9/477.6MB
Iteration 200/1250  mem: 357.7/477.6MB
Iteration 300/1250  mem: 369.7/477.6MB
Iteration 400/1250  mem: 391.4/477.6MB
Iteration 500/1250  mem: 277.5/477.6MB
Iteration 600/1250  mem: 299.1/477.6MB
Iteration 700/1250  mem: 318.3/477.6MB
Iteration 800/1250  mem: 337.2/477.6MB
Iteration 900/1250  mem: 355.8/477.6MB
Iteration 1000/1250  mem: 374.9/477.6MB
Iteration 1100/1250  mem: 393.6/477.6MB
Iteration 1200/1250  mem: 412.8/477.6MB
done building BART in 1.906 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 69 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 359.3/477.6MB
Iteration 200/1250  mem: 91.6/485.5MB
Iteration 300/1250  mem: 208/485.5MB
Iteration 400/1250  mem: 192.3/485.5MB
Iteration 500/1250  mem: 175.8/486MB
Iteration 600/1250  mem: 156.6/486MB
Iteration 700/1250  mem: 130.9/489.2MB
Iteration 800/1250  mem: 102.3/489.2MB
Iteration 900/1250  mem: 207.4/489.2MB
Iteration 1000/1250  mem: 170.6/490.2MB
Iteration 1100/1250  mem: 131.4/490.7MB
Iteration 1200/1250  mem: 231/491.3MB
done building BART in 3.14 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 83 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 266.3/491.8MB
Iteration 200/1250  mem: 147/486MB
Iteration 300/1250  mem: 173.5/488.6MB
Iteration 400/1250  mem: 192.7/488.1MB
Iteration 500/1250  mem: 215.6/485.5MB
Iteration 600/1250  mem: 243.9/486.5MB
Iteration 700/1250  mem: 278/485MB
Iteration 800/1250  mem: 172.9/487.1MB
Iteration 900/1250  mem: 200.3/487.6MB
Iteration 1000/1250  mem: 219.3/488.6MB
Iteration 1100/1250  mem: 236.2/489.7MB
Iteration 1200/1250  mem: 247.2/490.7MB
done building BART in 3.703 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 44 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 295/491.8MB
Iteration 200/1250  mem: 309/487.6MB
Iteration 300/1250  mem: 325.4/487.6MB
Iteration 400/1250  mem: 209.3/486MB
Iteration 500/1250  mem: 235.8/483.4MB
Iteration 600/1250  mem: 260.1/485MB
Iteration 700/1250  mem: 283.2/486MB
Iteration 800/1250  mem: 305.3/487.1MB
Iteration 900/1250  mem: 326.2/487.1MB
Iteration 1000/1250  mem: 342.8/488.1MB
Iteration 1100/1250  mem: 361.4/488.1MB
Iteration 1200/1250  mem: 237.7/488.1MB
done building BART in 1.874 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 92 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 274.9/487.1MB
Iteration 200/1250  mem: 336.3/486.5MB
Iteration 300/1250  mem: 391.1/487.6MB
Iteration 400/1250  mem: 300.5/489.7MB
Iteration 500/1250  mem: 348.1/489.7MB
Iteration 600/1250  mem: 396.8/488.1MB
Iteration 700/1250  mem: 301.9/490.2MB
Iteration 800/1250  mem: 340/491.3MB
Iteration 900/1250  mem: 378.5/491.3MB
Iteration 1000/1250  mem: 412.5/493.9MB
Iteration 1100/1250  mem: 439.6/493.9MB
Iteration 1200/1250  mem: 314.9/496.5MB
done building BART in 4.016 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 42 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 351.8/495.5MB
Iteration 200/1250  mem: 354.8/496.5MB
Iteration 300/1250  mem: 353.4/492.3MB
Iteration 400/1250  mem: 107.8/497MB
Iteration 500/1250  mem: 111.2/495.5MB
Iteration 600/1250  mem: 110.9/496.5MB
Iteration 700/1250  mem: 110.5/497.5MB
Iteration 800/1250  mem: 107.4/498.1MB
Iteration 900/1250  mem: 103.9/497.5MB
Iteration 1000/1250  mem: 99.7/498.1MB
Iteration 1100/1250  mem: 249.5/498.1MB
Iteration 1200/1250  mem: 247.8/498.6MB
done building BART in 1.875 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 82 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 202.2/496.5MB
Iteration 200/1250  mem: 201.6/496MB
Iteration 300/1250  mem: 197.1/496.5MB
Iteration 400/1250  mem: 192.5/497.5MB
Iteration 500/1250  mem: 180.9/498.6MB
Iteration 600/1250  mem: 168.9/499.1MB
Iteration 700/1250  mem: 309.8/500.2MB
Iteration 800/1250  mem: 292.8/499.6MB
Iteration 900/1250  mem: 273.4/500.2MB
Iteration 1000/1250  mem: 256.4/500.7MB
Iteration 1100/1250  mem: 233.1/501.2MB
Iteration 1200/1250  mem: 213.3/502.3MB
done building BART in 3.562 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 67 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 323.5/492.8MB
Iteration 200/1250  mem: 268.4/497.5MB
Iteration 300/1250  mem: 358.9/497.5MB
Iteration 400/1250  mem: 301.8/497MB
Iteration 500/1250  mem: 391.9/497MB
Iteration 600/1250  mem: 330.4/499.1MB
Iteration 700/1250  mem: 412.5/499.1MB
Iteration 800/1250  mem: 342.7/499.1MB
Iteration 900/1250  mem: 429.4/499.1MB
Iteration 1000/1250  mem: 361.7/499.6MB
Iteration 1100/1250  mem: 445.8/500.2MB
Iteration 1200/1250  mem: 374.1/497.5MB
done building BART in 3.125 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 42 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 360.8/500.7MB
Iteration 200/1250  mem: 358.1/496.5MB
Iteration 300/1250  mem: 356.6/498.6MB
Iteration 400/1250  mem: 358.1/497.5MB
Iteration 500/1250  mem: 356.5/498.1MB
Iteration 600/1250  mem: 358.2/498.1MB
Iteration 700/1250  mem: 356.2/498.6MB
Iteration 800/1250  mem: 120.8/517.5MB
Iteration 900/1250  mem: 118.8/517.5MB
Iteration 1000/1250  mem: 112.6/517.5MB
Iteration 1100/1250  mem: 264.7/517.5MB
Iteration 1200/1250  mem: 258.6/517.5MB
done building BART in 1.922 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 69 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 156.1/512.8MB
Iteration 200/1250  mem: 257.6/515.4MB
Iteration 300/1250  mem: 206.4/515.4MB
Iteration 400/1250  mem: 153.4/515.4MB
Iteration 500/1250  mem: 252.2/515.4MB
Iteration 600/1250  mem: 193.8/516.4MB
Iteration 700/1250  mem: 291/516.9MB
Iteration 800/1250  mem: 230.5/518MB
Iteration 900/1250  mem: 165.4/518MB
Iteration 1000/1250  mem: 254.7/518.5MB
Iteration 1100/1250  mem: 188.9/519MB
Iteration 1200/1250  mem: 279.2/519MB
done building BART in 3.016 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 81 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 280.7/513.3MB
Iteration 200/1250  mem: 285.6/512.8MB
Iteration 300/1250  mem: 271.8/514.9MB
Iteration 400/1250  mem: 251.6/515.9MB
Iteration 500/1250  mem: 378.5/516.4MB
Iteration 600/1250  mem: 344.8/517.5MB
Iteration 700/1250  mem: 307.3/518.5MB
Iteration 800/1250  mem: 257.8/519.6MB
Iteration 900/1250  mem: 370.9/518MB
Iteration 1000/1250  mem: 314.9/521.1MB
Iteration 1100/1250  mem: 261/520.6MB
Iteration 1200/1250  mem: 368/521.7MB
done building BART in 5.094 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
Bayesian Additive Regression Trees 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  num_trees  k          alpha      beta        nu         RMSE       
   22        3.4813678  0.9214478  1.10892284  0.2026322  0.011695226
   28        2.4798170  0.9200893  3.46645078  0.7360418  0.012598522
   40        3.5580259  0.9078867  2.35800209  4.4102201  0.010486788
   42        2.9415150  0.9115110  3.39307996  4.8454580  0.010096485
   42        4.0010935  0.9962236  2.00397418  3.8955309  0.009008888
   43        3.7693143  0.9708941  0.28576733  2.4568919  0.009803453
   44        1.2881292  0.9337671  3.54380219  1.5962272  0.010032026
   44        3.6621924  0.9826701  3.59701570  2.6849798  0.010646880
   62        2.2273179  0.9559256  3.29647936  2.3224788  0.008592256
   67        4.7574040  0.9681602  0.19679126  2.2862146  0.008937259
   69        1.2355291  0.9999570  2.27229754  4.0374196  0.008688499
   69        1.8452577  0.9302290  2.13551725  0.5361209  0.009837064
   70        0.8543510  0.9950277  1.89718182  0.6354402  0.009867030
   81        3.5520525  0.9091787  3.28695455  1.5394923  0.008214331
   82        4.4341556  0.9086684  0.48008649  1.9097800  0.009341099
   83        1.0900708  0.9425635  0.49833165  1.4608206  0.009520934
   85        1.6750793  0.9848128  2.43375067  4.6519415  0.010069171
   87        3.6430846  0.9140229  0.02357525  0.3726360  0.008658388
   92        1.7099420  0.9282080  1.55325157  1.3728867  0.008507213
  100        0.1117293  0.9125186  1.61762561  3.9168591  0.011531369
  Rsquared   MAE          Selected
  0.9966357  0.009022217          
  0.9961232  0.009798349          
  0.9973181  0.008090853          
  0.9975016  0.007854928          
  0.9980190  0.006998172          
  0.9976541  0.007724245          
  0.9975522  0.007935393          
  0.9972431  0.008261391          
  0.9982048  0.006773612          
  0.9980458  0.007053912          
  0.9981399  0.006689531          
  0.9976001  0.007248141          
  0.9975828  0.007440607          
  0.9983329  0.006407048  *       
  0.9978559  0.007138870          
  0.9977458  0.007295869          
  0.9975237  0.007379461          
  0.9981265  0.006770113          
  0.9982065  0.006615065          
  0.9967505  0.007601581          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were num_trees = 81, k = 3.552052, alpha
 = 0.9091787, beta = 3.286955 and nu = 1.539492.
[1] "Mon Mar 12 08:22:07 2018"
.....
Bayesian Generalized Linear Model 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results:

  RMSE          Rsquared   MAE         
  0.0009635385  0.9999769  0.0007574329

[1] "Mon Mar 12 08:22:24 2018"
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
The Bayesian lasso 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  sparsity   RMSE         Rsquared   MAE          Selected
  0.1344562  0.001089911  0.9999768  0.000870194          
  0.2080064  0.001089911  0.9999768  0.000870194          
  0.3400364  0.001089911  0.9999768  0.000870194          
  0.3528485  0.001089911  0.9999768  0.000870194          
  0.3529606  0.001089911  0.9999768  0.000870194          
  0.3634999  0.001089911  0.9999768  0.000870194          
  0.3784709  0.001089911  0.9999768  0.000870194          
  0.3809253  0.001089911  0.9999768  0.000870194          
  0.5749706  0.001089911  0.9999768  0.000870194          
  0.6317869  0.001089911  0.9999768  0.000870194          
  0.6531948  0.001089911  0.9999768  0.000870194          
  0.6537831  0.001089911  0.9999768  0.000870194          
  0.6674360  0.001089911  0.9999768  0.000870194          
  0.7810396  0.001089911  0.9999768  0.000870194          
  0.7966071  0.001089911  0.9999768  0.000870194          
  0.8123921  0.001089911  0.9999768  0.000870194          
  0.8315435  0.001089911  0.9999768  0.000870194          
  0.8562675  0.001089911  0.9999768  0.000870194          
  0.9108732  0.001089911  0.9999768  0.000870194          
  0.9965235  0.001089911  0.9999768  0.000870194  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was sparsity = 0.9965235.
[1] "Mon Mar 12 08:22:38 2018"
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
Bayesian Ridge Regression (Model Averaged) 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results:

  RMSE         Rsquared   MAE         
  0.001088969  0.9999769  0.0008701218

[1] "Mon Mar 12 08:22:51 2018"
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
Bayesian Ridge Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results:

  RMSE         Rsquared   MAE         
  0.001088469  0.9999769  0.0008693357

[1] "Mon Mar 12 08:23:05 2018"
Boosted Linear Model 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  nu          mstop  RMSE       Rsquared   MAE         Selected
  0.01438517  498    0.1741544  0.9269008  0.14072280          
  0.10335125  334    0.1416554  0.7761292  0.11322234          
  0.13159048  406    0.1286637  0.7705632  0.10111729          
  0.14901639  327    0.1311077  0.7691791  0.10351585          
  0.15531788  191    0.1450713  0.7846662  0.11634139          
  0.20167450  415    0.1152729  0.7617900  0.08778276          
  0.20585105  455    0.1126785  0.7604388  0.08489352          
  0.22206187  326    0.1186374  0.7637849  0.09126129          
  0.26783269  287    0.1163940  0.7625068  0.08897893          
  0.29808207  104    0.1416486  0.7732737  0.11325056          
  0.35339350  177    0.1204695  0.7644916  0.09318243          
  0.41806786   68    0.1420538  0.7756678  0.11368981          
  0.42653589  390    0.1041967  0.7473644  0.07324032          
  0.42725151  170    0.1154254  0.7605373  0.08798435          
  0.43744153  428    0.1038000  0.7462469  0.07237030          
  0.43973065  189    0.1121759  0.7615437  0.08427923          
  0.45256385  182    0.1121967  0.7583362  0.08445560          
  0.48033100  177    0.1111711  0.7607047  0.08305152          
  0.53221184  398    0.1033176  0.7461548  0.07137615  *       
  0.57093700  316    0.1036589  0.7466831  0.07194509          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mstop = 398 and nu = 0.5322118.
[1] "Mon Mar 12 08:24:25 2018"
Conditional Inference Random Forest 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE         Selected
  2     0.02647371  0.9884762  0.01994343          
  4     0.01589748  0.9943577  0.01171897  *       
  6     0.01669221  0.9934075  0.01232799          
  7     0.01723848  0.9929001  0.01276429          
  8     0.01796252  0.9922633  0.01341649          
  9     0.01850922  0.9917535  0.01382604          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 4.
[1] "Mon Mar 12 08:25:38 2018"
Error in UseMethod("varimp") : 
  no applicable method for 'varimp' applied to an object of class "RandomForest"
Conditional Inference Tree 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  mincriterion  RMSE        Rsquared   MAE         Selected
  0.1344562     0.02743890  0.9812246  0.02114532          
  0.2080064     0.02743890  0.9812246  0.02114532          
  0.3400364     0.02743890  0.9812246  0.02114532          
  0.3528485     0.02743890  0.9812246  0.02114532          
  0.3529606     0.02743890  0.9812246  0.02114532          
  0.3634999     0.02743890  0.9812246  0.02114532          
  0.3784709     0.02743890  0.9812246  0.02114532          
  0.3809253     0.02743890  0.9812246  0.02114532          
  0.5749706     0.02743890  0.9812246  0.02114532  *       
  0.6317869     0.02749215  0.9811606  0.02118078          
  0.6531948     0.02749215  0.9811606  0.02118078          
  0.6537831     0.02749215  0.9811606  0.02118078          
  0.6674360     0.02749215  0.9811606  0.02118078          
  0.7810396     0.02749215  0.9811606  0.02118078          
  0.7966071     0.02749215  0.9811606  0.02118078          
  0.8123921     0.02749215  0.9811606  0.02118078          
  0.8315435     0.02749215  0.9811606  0.02118078          
  0.8562675     0.02752004  0.9811269  0.02121297          
  0.9108732     0.02763465  0.9809821  0.02131980          
  0.9965235     0.02940643  0.9784874  0.02305546          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mincriterion = 0.5749706.
[1] "Mon Mar 12 08:25:46 2018"
Conditional Inference Tree 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  maxdepth  mincriterion  RMSE        Rsquared   MAE         Selected
   3        0.69627356    0.04610597  0.9473105  0.03755431          
   4        0.49596339    0.03632996  0.9673211  0.02891366          
   6        0.25762584    0.02783351  0.9806525  0.02154955          
   6        0.58830301    0.02783351  0.9806525  0.02154955          
   6        0.71160519    0.02783351  0.9806525  0.02154955          
   6        0.73243848    0.02783351  0.9806525  0.02154955          
   6        0.75386285    0.02783351  0.9806525  0.02154955          
   6        0.80021869    0.02783351  0.9806525  0.02154955          
   9        0.44546359    0.02743890  0.9812246  0.02114532  *       
  10        0.24710582    0.02743890  0.9812246  0.02114532          
  10        0.36905154    0.02743890  0.9812246  0.02114532          
  10        0.95148080    0.02768118  0.9809261  0.02132830          
  11        0.17087020    0.02743890  0.9812246  0.02114532          
  12        0.71041050    0.02749215  0.9811606  0.02118078          
  12        0.88683113    0.02763465  0.9809821  0.02131980          
  13        0.21801415    0.02743890  0.9812246  0.02114532          
  13        0.33501586    0.02743890  0.9812246  0.02114532          
  13        0.72861692    0.02749215  0.9811606  0.02118078          
  14        0.34198841    0.02743890  0.9812246  0.02114532          
  15        0.02234585    0.02743890  0.9812246  0.02114532          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were maxdepth = 9 and mincriterion
 = 0.4454636.
[1] "Mon Mar 12 08:25:54 2018"
Cubist 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  committees  neighbors  RMSE         Rsquared   MAE          Selected
   3          9          0.002441749  0.9998512  0.001724622          
  18          6          0.001905760  0.9999104  0.001393631          
  22          8          0.001918404  0.9999089  0.001392772          
  25          6          0.001848797  0.9999155  0.001343407          
  26          3          0.001774308  0.9999223  0.001308190          
  34          8          0.001817614  0.9999182  0.001328527          
  35          9          0.001829579  0.9999171  0.001331262          
  37          6          0.001781512  0.9999215  0.001301841          
  45          5          0.001728728  0.9999259  0.001262676          
  50          2          0.001694098  0.9999282  0.001244111          
  59          3          0.001657884  0.9999314  0.001215531          
  70          1          0.001779306  0.9999207  0.001306934          
  72          3          0.001669825  0.9999306  0.001223502          
  72          7          0.001768423  0.9999223  0.001284272          
  73          8          0.001785684  0.9999208  0.001297074          
  74          3          0.001666249  0.9999309  0.001222268          
  76          3          0.001668834  0.9999308  0.001221997          
  81          3          0.001654458  0.9999320  0.001209699  *       
  89          7          0.001753224  0.9999236  0.001273913          
  96          6          0.001734265  0.9999252  0.001258952          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were committees = 81 and neighbors = 3.
[1] "Mon Mar 12 08:28:11 2018"
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
Stacked AutoEncoder Deep Neural Network 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  layer1  layer2  layer3  hidden_dropout  visible_dropout  RMSE       Rsquared 
   4      15       6      0.19406150      0.02836851       0.2271613  0.8992320
   5      11       5      0.60662889      0.10304585       0.2120160  0.8837816
   8      13       4      0.59378899      0.67836412       0.2235824  0.6737213
   8      15       3      0.41265037      0.61743081       0.2358109  0.9009511
   8      16      15      0.05000928      0.34396486       0.2099758  0.8688685
   8      17      20      0.35069548      0.54537432       6.1279366  0.8799749
   9       6       8      0.62016538      0.22347181       0.2833112  0.8681195
   9      15      17      0.62947775      0.37589717       0.2339203  0.7710552
  12      10      12      0.57688389      0.32514703       0.2563182  0.9085503
  14       5      20      0.33200682      0.08896162       0.2247978  0.8136203
  14       6      20      0.39765207      0.56523874       3.2513848  0.7457628
  14       9       7      0.37371552      0.07505693       0.2515905  0.7481589
  14      20      14      0.03443847      0.32007004       0.2102310  0.7394126
  16      15       3      0.57521705      0.21552892       0.2008357  0.5563360
  17       6      10      0.08720804      0.20451488       0.3386296  0.7734600
  17       8      18      0.42590637      0.65127181       0.3071045  0.5895187
  17      18       3      0.08401514      0.26736919       0.2032529  0.6706126
  18      15       4      0.00412567      0.05216903       0.2143761  0.4020497
  19       8       7      0.27181902      0.19220414       0.3032870  0.4760203
  20       2       4      0.28308448      0.54836027       0.2534642  0.4399125
  MAE        Selected
  0.1863869          
  0.1729430          
  0.1842412          
  0.1936468          
  0.1735131          
  6.0948821          
  0.2387020          
  0.1917804          
  0.2117557          
  0.1821392          
  3.2202330          
  0.2077805          
  0.1740018          
  0.1653678  *       
  0.2964999          
  0.2571875          
  0.1669304          
  0.1745824          
  0.2565175          
  0.2100103          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were layer1 = 16, layer2 = 15, layer3 =
 3, hidden_dropout = 0.575217 and visible_dropout = 0.2155289.
[1] "Mon Mar 12 08:28:23 2018"
Multivariate Adaptive Regression Spline 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  degree  nprune  RMSE         Rsquared   MAE          Selected
  1       3       0.032640144  0.9735565  0.024360774          
  1       4       0.021302673  0.9886120  0.016920832          
  1       5       0.015300606  0.9942791  0.012171064          
  1       6       0.009150573  0.9978156  0.007033124          
  1       7       0.007435549  0.9986170  0.005687262  *       
  2       2       0.047271765  0.9431602  0.036103843          
  2       4       0.021302673  0.9886120  0.016920832          
  2       5       0.015300606  0.9942791  0.012171064          
  2       6       0.009150573  0.9978156  0.007033124          
  2       7       0.007435549  0.9986170  0.005687262          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nprune = 7 and degree = 1.
[1] "Mon Mar 12 08:28:31 2018"
Extreme Learning Machine 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  nhid  actfun   RMSE          Rsquared   MAE          Selected
   3    purelin  0.1083090525  0.7359735  0.079768822          
   4    radbas   0.1130859630  0.6811875  0.088354241          
   7    purelin  0.0048637448  0.9994134  0.003649094          
   7    tansig   0.0274646677  0.9793266  0.021331789          
   8    purelin  0.0039300533  0.9996161  0.002875934          
   8    radbas   0.0464594402  0.9365634  0.033952826          
  11    radbas   0.0289310897  0.9790359  0.022025104          
  13    radbas   0.0167544477  0.9898523  0.012768596          
  13    sin      0.0049206582  0.9994029  0.003570971          
  13    tansig   0.0080860142  0.9981793  0.006065583          
  15    purelin  0.0009634527  0.9999769  0.000757459          
  16    radbas   0.0166679819  0.9929558  0.012313679          
  16    sin      0.0034039064  0.9997102  0.002447490          
  16    tansig   0.0059980576  0.9989192  0.004369907          
  17    purelin  0.0009634527  0.9999769  0.000757459  *       
  18    radbas   0.0067758323  0.9988121  0.005212969          
  19    sin      0.0032085740  0.9996572  0.002377356          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nhid = 17 and actfun = purelin.
[1] "Mon Mar 12 08:28:37 2018"
Elasticnet 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  lambda        fraction    RMSE        Rsquared   MAE          Selected
  6.408213e-05  0.69627356  0.00236384  0.9998608  0.001657019  *       
  1.770265e-04  0.49596339  0.01057936  0.9979025  0.008171595          
  1.097029e-03  0.71160519  0.03142257  0.9961325  0.025748551          
  1.309457e-03  0.80021869  0.01473754  0.9975384  0.011984345          
  1.311486e-03  0.58830301  0.06058715  0.9890008  0.049701385          
  1.517048e-03  0.75386285  0.02521179  0.9968800  0.020655684          
  1.865629e-03  0.73243848  0.03123779  0.9962959  0.025570011          
  1.929976e-03  0.25762584  0.13907646  0.9711931  0.114322937          
  2.817240e-02  0.44546359  0.09274776  0.9814134  0.076013712          
  6.176199e-02  0.95148080  0.01077477  0.9987472  0.008072884          
  8.301732e-02  0.36905154  0.10837157  0.9755331  0.088770554          
  8.369486e-02  0.24710582  0.13795870  0.9695289  0.113347357          
  1.010686e-01  0.17087020  0.15653669  0.9709082  0.128876493          
  4.855539e-01  0.71041050  0.02085455  0.9931930  0.016343231          
  6.020641e-01  0.88683113  0.07373637  0.9905661  0.059590528          
  7.487771e-01  0.21801415  0.12641657  0.9677551  0.103591482          
  9.755753e-01  0.33501586  0.07937611  0.9854757  0.064680193          
  1.372793e+00  0.72861692  0.08801077  0.9895308  0.071236015          
  2.919036e+00  0.34198841  0.02609964  0.9899823  0.020924435          
  9.531051e+00  0.02234585  0.18129739  0.9624131  0.149458379          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were fraction = 0.6962736 and lambda
 = 6.408213e-05.
[1] "Mon Mar 12 08:28:44 2018"
Tree Models from Genetic Algorithms 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  alpha     RMSE        Rsquared   MAE         Selected
  1.537825  0.03524207  0.9693307  0.02797819  *       
  1.832025  0.04007283  0.9597817  0.03204309          
  2.360145  0.04603042  0.9472764  0.03693074          
  2.411394  0.04630972  0.9465865  0.03711767          
  2.411842  0.04603042  0.9472764  0.03693074          
  2.453999  0.04630972  0.9465865  0.03711767          
  2.513883  0.04630972  0.9465865  0.03711767          
  2.523701  0.04630972  0.9465865  0.03711767          
  3.299883  0.04630972  0.9465865  0.03711767          
  3.527148  0.05782093  0.9170805  0.04697698          
  3.612779  0.05080325  0.9347794  0.04170351          
  3.615133  0.05080325  0.9347794  0.04170351          
  3.669744  0.05383915  0.9283699  0.04332885          
  4.124158  0.05813257  0.9162556  0.04748971          
  4.186428  0.05813257  0.9162556  0.04748971          
  4.249568  0.05813257  0.9162556  0.04748971          
  4.326174  0.06019311  0.9096701  0.04938396          
  4.425070  0.05813257  0.9162556  0.04748971          
  4.643493  0.05813257  0.9162556  0.04748971          
  4.986094  0.06530832  0.8942762  0.05316429          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was alpha = 1.537825.
[1] "Mon Mar 12 08:55:00 2018"
Random Forest by Randomization 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  mtry  numRandomCuts  RMSE         Rsquared   MAE          Selected
  2     13             0.009237245  0.9979520  0.006850340          
  2     18             0.009375131  0.9979036  0.007025498          
  4      7             0.007650218  0.9985492  0.005734183          
  4     15             0.008361397  0.9983217  0.006116848          
  4     18             0.008299672  0.9982789  0.006237065          
  4     19             0.008411857  0.9982280  0.006307244          
  4     21             0.008701965  0.9981750  0.006256585          
  6      7             0.007974230  0.9984484  0.005807286          
  6     10             0.008313758  0.9982517  0.006175286          
  6     12             0.008597788  0.9981304  0.006403592          
  6     24             0.009403411  0.9978489  0.006815789          
  7      5             0.007760502  0.9984731  0.005714876          
  8      6             0.008349676  0.9982306  0.006148340          
  8      9             0.008964803  0.9979598  0.006632794          
  8     18             0.009944308  0.9974936  0.007381641          
  8     19             0.010023602  0.9974573  0.007419245          
  8     23             0.010188362  0.9974647  0.007485125          
  9      1             0.006703846  0.9988730  0.004942572  *       
  9      9             0.009332866  0.9978653  0.006775767          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mtry = 9 and numRandomCuts = 1.
[1] "Mon Mar 12 09:05:28 2018"
Ridge Regression with Variable Selection 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  lambda        k  RMSE         Rsquared   MAE          Selected
  6.408213e-05  7  0.001652842  0.9999319  0.001181991  *       
  1.770265e-04  5  0.003847418  0.9996325  0.002735222          
  1.097029e-03  7  0.003826872  0.9996365  0.002726957          
  1.309457e-03  8  0.003985748  0.9996057  0.002846283          
  1.311486e-03  6  0.003947423  0.9996131  0.002796111          
  1.517048e-03  7  0.004656856  0.9994576  0.003336668          
  1.865629e-03  7  0.004978863  0.9993869  0.003604079          
  1.929976e-03  3  0.009889909  0.9975836  0.007239463          
  2.817240e-02  5  0.007127051  0.9988376  0.005492558          
  6.176199e-02  9  0.008396146  0.9987102  0.006603931          
  8.301732e-02  4  0.009424723  0.9986118  0.007550113          
  8.369486e-02  3  0.012772545  0.9969207  0.009534963          
  1.010686e-01  2  0.022820819  0.9906403  0.018745517          
  4.855539e-01  7  0.033894670  0.9963057  0.027737861          
  6.020641e-01  8  0.039875459  0.9962655  0.032635932          
  7.487771e-01  2  0.070846718  0.9786327  0.059403188          
  9.755753e-01  4          NaN        NaN          NaN          
  1.372793e+00  7          NaN        NaN          NaN          
  2.919036e+00  4          NaN        NaN          NaN          
  9.531051e+00  1          NaN        NaN          NaN          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were k = 7 and lambda = 6.408213e-05.
[1] "Mon Mar 12 09:05:34 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: select= TRUE, method=GCV.Cp Error in as.matrix(x) : object 'V7' not found
 
2: model fit failed for Fold1: select=FALSE, method=ML Error in as.matrix(x) : object 'V7' not found
 
3: model fit failed for Fold1: select= TRUE, method=ML Error in as.matrix(x) : object 'V7' not found
 
4: model fit failed for Fold2: select= TRUE, method=GCV.Cp Error in as.matrix(x) : object 'V6' not found
 
5: model fit failed for Fold2: select=FALSE, method=ML Error in as.matrix(x) : object 'V6' not found
 
6: model fit failed for Fold2: select= TRUE, method=ML Error in as.matrix(x) : object 'V6' not found
 
7: model fit failed for Fold3: select= TRUE, method=GCV.Cp Error in as.matrix(x) : object 'V6' not found
 
8: model fit failed for Fold3: select=FALSE, method=ML Error in as.matrix(x) : object 'V6' not found
 
9: model fit failed for Fold3: select= TRUE, method=ML Error in as.matrix(x) : object 'V6' not found
 
10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 09:05:44 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "gam"                            
Boosted Generalized Additive Model 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  mstop  prune  RMSE         Rsquared   MAE          Selected
  135    no     0.004866896  0.9994121  0.003479469          
  209    yes    0.004587750  0.9994766  0.003301586          
  341    no     0.004374891  0.9995240  0.003142271          
  353    no     0.004361517  0.9995270  0.003131875          
  364    no     0.004349349  0.9995297  0.003121863          
  379    no     0.004333413  0.9995331  0.003109308          
  381    yes    0.004331160  0.9995336  0.003107026          
  575    yes    0.004167604  0.9995682  0.002983602          
  632    no     0.004128086  0.9995764  0.002954884          
  654    yes    0.004113264  0.9995794  0.002943916          
  668    yes    0.004103803  0.9995814  0.002937224          
  782    no     0.004033178  0.9995957  0.002888672          
  797    no     0.004024511  0.9995974  0.002883139          
  813    yes    0.004015418  0.9995992  0.002876494          
  832    yes    0.004004358  0.9996014  0.002868905          
  857    no     0.003990068  0.9996043  0.002859307          
  911    yes    0.003959984  0.9996102  0.002839918          
  997    yes    0.003913909  0.9996192  0.002808982  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mstop = 997 and prune = yes.
[1] "Mon Mar 12 09:29:51 2018"
Generalized Additive Model using Splines 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  df         RMSE          Rsquared   MAE           Selected
  0.6722808  0.0009634519  0.9999769  0.0007574583          
  1.0400319  0.0009622059  0.9999769  0.0007563508          
  1.7001819  0.0009339821  0.9999783  0.0007323414          
  1.7642427  0.0009304670  0.9999784  0.0007293496          
  1.7648030  0.0009304357  0.9999784  0.0007293230          
  1.8174993  0.0009274535  0.9999786  0.0007267735          
  1.8923544  0.0009230951  0.9999788  0.0007230095          
  1.9046266  0.0009223647  0.9999788  0.0007223724          
  2.8748532  0.0008595621  0.9999816  0.0006703815          
  3.1589344  0.0008417160  0.9999823  0.0006565237          
  3.2659739  0.0008352357  0.9999826  0.0006513237          
  3.2689157  0.0008350596  0.9999826  0.0006511839          
  3.3371802  0.0008310143  0.9999828  0.0006479808          
  3.9051978  0.0007998178  0.9999841  0.0006236607          
  3.9830356  0.0007958950  0.9999842  0.0006206058          
  4.0619605  0.0007920024  0.9999844  0.0006176100          
  4.1577173  0.0007873971  0.9999846  0.0006141792          
  4.2813376  0.0007816109  0.9999848  0.0006097999          
  4.5543662  0.0007695055  0.9999853  0.0006005203          
  4.9826173  0.0007521407  0.9999859  0.0005874234  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was df = 4.982617.
[1] "Mon Mar 12 09:30:20 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: parameter=none Error in `kernlab::vanilladot`() : 
  could not find function "kernlab::vanilladot"
 
2: model fit failed for Fold2: parameter=none Error in `kernlab::vanilladot`() : 
  could not find function "kernlab::vanilladot"
 
3: model fit failed for Fold3: parameter=none Error in `kernlab::vanilladot`() : 
  could not find function "kernlab::vanilladot"
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 09:30:27 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "gaussprLinear"                  
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 09:36:46 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "gaussprPoly"                    
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 09:36:54 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "gaussprRadial"                  
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0365             nan     0.0482    0.0035
     2        0.0332             nan     0.0482    0.0036
     3        0.0303             nan     0.0482    0.0028
     4        0.0276             nan     0.0482    0.0029
     5        0.0251             nan     0.0482    0.0023
     6        0.0229             nan     0.0482    0.0023
     7        0.0209             nan     0.0482    0.0020
     8        0.0191             nan     0.0482    0.0018
     9        0.0175             nan     0.0482    0.0016
    10        0.0159             nan     0.0482    0.0015
    20        0.0065             nan     0.0482    0.0006
    40        0.0012             nan     0.0482    0.0001
    60        0.0004             nan     0.0482    0.0000
    80        0.0002             nan     0.0482    0.0000
   100        0.0001             nan     0.0482    0.0000
   120        0.0001             nan     0.0482    0.0000
   140        0.0001             nan     0.0482   -0.0000
   160        0.0001             nan     0.0482    0.0000
   180        0.0001             nan     0.0482   -0.0000
   200        0.0001             nan     0.0482    0.0000
   220        0.0001             nan     0.0482   -0.0000
   240        0.0001             nan     0.0482   -0.0000
   260        0.0001             nan     0.0482   -0.0000
   280        0.0001             nan     0.0482   -0.0000
   300        0.0001             nan     0.0482   -0.0000
   320        0.0001             nan     0.0482   -0.0000
   340        0.0001             nan     0.0482   -0.0000
   360        0.0001             nan     0.0482   -0.0000
   380        0.0001             nan     0.0482   -0.0000
   400        0.0000             nan     0.0482   -0.0000
   420        0.0000             nan     0.0482   -0.0000
   440        0.0000             nan     0.0482   -0.0000
   460        0.0000             nan     0.0482   -0.0000
   480        0.0000             nan     0.0482   -0.0000
   500        0.0000             nan     0.0482   -0.0000
   520        0.0000             nan     0.0482   -0.0000
   540        0.0000             nan     0.0482   -0.0000
   560        0.0000             nan     0.0482   -0.0000
   580        0.0000             nan     0.0482   -0.0000
   600        0.0000             nan     0.0482   -0.0000
   620        0.0000             nan     0.0482   -0.0000
   640        0.0000             nan     0.0482   -0.0000
   660        0.0000             nan     0.0482   -0.0000
   680        0.0000             nan     0.0482   -0.0000
   700        0.0000             nan     0.0482   -0.0000
   720        0.0000             nan     0.0482   -0.0000
   740        0.0000             nan     0.0482   -0.0000
   760        0.0000             nan     0.0482   -0.0000
   780        0.0000             nan     0.0482   -0.0000
   800        0.0000             nan     0.0482   -0.0000
   820        0.0000             nan     0.0482   -0.0000
   840        0.0000             nan     0.0482   -0.0000
   860        0.0000             nan     0.0482   -0.0000
   880        0.0000             nan     0.0482   -0.0000
   900        0.0000             nan     0.0482   -0.0000
   920        0.0000             nan     0.0482   -0.0000
   940        0.0000             nan     0.0482   -0.0000
   960        0.0000             nan     0.0482   -0.0000
   980        0.0000             nan     0.0482   -0.0000
  1000        0.0000             nan     0.0482   -0.0000
  1020        0.0000             nan     0.0482   -0.0000
  1040        0.0000             nan     0.0482   -0.0000
  1060        0.0000             nan     0.0482   -0.0000
  1080        0.0000             nan     0.0482   -0.0000
  1100        0.0000             nan     0.0482   -0.0000
  1120        0.0000             nan     0.0482   -0.0000
  1140        0.0000             nan     0.0482   -0.0000
  1160        0.0000             nan     0.0482   -0.0000
  1180        0.0000             nan     0.0482   -0.0000
  1200        0.0000             nan     0.0482   -0.0000
  1220        0.0000             nan     0.0482   -0.0000
  1240        0.0000             nan     0.0482   -0.0000
  1260        0.0000             nan     0.0482   -0.0000
  1280        0.0000             nan     0.0482   -0.0000
  1300        0.0000             nan     0.0482   -0.0000
  1320        0.0000             nan     0.0482   -0.0000
  1340        0.0000             nan     0.0482   -0.0000
  1360        0.0000             nan     0.0482   -0.0000
  1380        0.0000             nan     0.0482   -0.0000
  1400        0.0000             nan     0.0482   -0.0000
  1420        0.0000             nan     0.0482   -0.0000
  1440        0.0000             nan     0.0482   -0.0000
  1460        0.0000             nan     0.0482   -0.0000
  1480        0.0000             nan     0.0482   -0.0000
  1500        0.0000             nan     0.0482   -0.0000
  1520        0.0000             nan     0.0482   -0.0000
  1540        0.0000             nan     0.0482   -0.0000
  1560        0.0000             nan     0.0482   -0.0000
  1580        0.0000             nan     0.0482   -0.0000
  1600        0.0000             nan     0.0482   -0.0000
  1620        0.0000             nan     0.0482   -0.0000
  1640        0.0000             nan     0.0482   -0.0000
  1660        0.0000             nan     0.0482   -0.0000
  1680        0.0000             nan     0.0482   -0.0000
  1700        0.0000             nan     0.0482   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0361             nan     0.0529    0.0039
     2        0.0325             nan     0.0529    0.0032
     3        0.0293             nan     0.0529    0.0030
     4        0.0264             nan     0.0529    0.0028
     5        0.0239             nan     0.0529    0.0025
     6        0.0215             nan     0.0529    0.0023
     7        0.0194             nan     0.0529    0.0021
     8        0.0175             nan     0.0529    0.0018
     9        0.0158             nan     0.0529    0.0017
    10        0.0143             nan     0.0529    0.0015
    20        0.0052             nan     0.0529    0.0005
    40        0.0008             nan     0.0529    0.0001
    60        0.0002             nan     0.0529    0.0000
    80        0.0001             nan     0.0529    0.0000
   100        0.0000             nan     0.0529    0.0000
   120        0.0000             nan     0.0529   -0.0000
   140        0.0000             nan     0.0529   -0.0000
   160        0.0000             nan     0.0529   -0.0000
   180        0.0000             nan     0.0529   -0.0000
   200        0.0000             nan     0.0529   -0.0000
   220        0.0000             nan     0.0529   -0.0000
   240        0.0000             nan     0.0529   -0.0000
   260        0.0000             nan     0.0529   -0.0000
   280        0.0000             nan     0.0529   -0.0000
   300        0.0000             nan     0.0529   -0.0000
   320        0.0000             nan     0.0529   -0.0000
   340        0.0000             nan     0.0529   -0.0000
   360        0.0000             nan     0.0529   -0.0000
   380        0.0000             nan     0.0529   -0.0000
   400        0.0000             nan     0.0529   -0.0000
   420        0.0000             nan     0.0529   -0.0000
   440        0.0000             nan     0.0529   -0.0000
   460        0.0000             nan     0.0529   -0.0000
   480        0.0000             nan     0.0529   -0.0000
   500        0.0000             nan     0.0529   -0.0000
   520        0.0000             nan     0.0529   -0.0000
   540        0.0000             nan     0.0529   -0.0000
   560        0.0000             nan     0.0529   -0.0000
   580        0.0000             nan     0.0529   -0.0000
   600        0.0000             nan     0.0529   -0.0000
   620        0.0000             nan     0.0529   -0.0000
   640        0.0000             nan     0.0529   -0.0000
   660        0.0000             nan     0.0529   -0.0000
   680        0.0000             nan     0.0529   -0.0000
   700        0.0000             nan     0.0529   -0.0000
   720        0.0000             nan     0.0529   -0.0000
   740        0.0000             nan     0.0529   -0.0000
   760        0.0000             nan     0.0529   -0.0000
   780        0.0000             nan     0.0529   -0.0000
   800        0.0000             nan     0.0529   -0.0000
   820        0.0000             nan     0.0529   -0.0000
   840        0.0000             nan     0.0529   -0.0000
   860        0.0000             nan     0.0529   -0.0000
   880        0.0000             nan     0.0529   -0.0000
   900        0.0000             nan     0.0529   -0.0000
   920        0.0000             nan     0.0529   -0.0000
   940        0.0000             nan     0.0529   -0.0000
   960        0.0000             nan     0.0529   -0.0000
   980        0.0000             nan     0.0529   -0.0000
  1000        0.0000             nan     0.0529   -0.0000
  1020        0.0000             nan     0.0529   -0.0000
  1040        0.0000             nan     0.0529   -0.0000
  1060        0.0000             nan     0.0529   -0.0000
  1080        0.0000             nan     0.0529   -0.0000
  1100        0.0000             nan     0.0529   -0.0000
  1120        0.0000             nan     0.0529   -0.0000
  1140        0.0000             nan     0.0529   -0.0000
  1160        0.0000             nan     0.0529   -0.0000
  1180        0.0000             nan     0.0529   -0.0000
  1200        0.0000             nan     0.0529   -0.0000
  1220        0.0000             nan     0.0529   -0.0000
  1240        0.0000             nan     0.0529   -0.0000
  1260        0.0000             nan     0.0529   -0.0000
  1280        0.0000             nan     0.0529   -0.0000
  1300        0.0000             nan     0.0529   -0.0000
  1320        0.0000             nan     0.0529   -0.0000
  1340        0.0000             nan     0.0529   -0.0000
  1360        0.0000             nan     0.0529   -0.0000
  1380        0.0000             nan     0.0529   -0.0000
  1400        0.0000             nan     0.0529   -0.0000
  1420        0.0000             nan     0.0529   -0.0000
  1440        0.0000             nan     0.0529   -0.0000
  1460        0.0000             nan     0.0529   -0.0000
  1480        0.0000             nan     0.0529   -0.0000
  1500        0.0000             nan     0.0529   -0.0000
  1520        0.0000             nan     0.0529   -0.0000
  1540        0.0000             nan     0.0529   -0.0000
  1560        0.0000             nan     0.0529   -0.0000
  1580        0.0000             nan     0.0529   -0.0000
  1600        0.0000             nan     0.0529   -0.0000
  1620        0.0000             nan     0.0529   -0.0000
  1640        0.0000             nan     0.0529   -0.0000
  1660        0.0000             nan     0.0529   -0.0000
  1680        0.0000             nan     0.0529   -0.0000
  1700        0.0000             nan     0.0529   -0.0000
  1720        0.0000             nan     0.0529   -0.0000
  1740        0.0000             nan     0.0529   -0.0000
  1760        0.0000             nan     0.0529   -0.0000
  1780        0.0000             nan     0.0529   -0.0000
  1800        0.0000             nan     0.0529   -0.0000
  1820        0.0000             nan     0.0529   -0.0000
  1840        0.0000             nan     0.0529   -0.0000
  1860        0.0000             nan     0.0529   -0.0000
  1880        0.0000             nan     0.0529   -0.0000
  1900        0.0000             nan     0.0529   -0.0000
  1920        0.0000             nan     0.0529   -0.0000
  1940        0.0000             nan     0.0529   -0.0000
  1960        0.0000             nan     0.0529   -0.0000
  1980        0.0000             nan     0.0529   -0.0000
  2000        0.0000             nan     0.0529   -0.0000
  2020        0.0000             nan     0.0529   -0.0000
  2040        0.0000             nan     0.0529   -0.0000
  2060        0.0000             nan     0.0529   -0.0000
  2080        0.0000             nan     0.0529   -0.0000
  2100        0.0000             nan     0.0529   -0.0000
  2120        0.0000             nan     0.0529   -0.0000
  2140        0.0000             nan     0.0529   -0.0000
  2160        0.0000             nan     0.0529   -0.0000
  2180        0.0000             nan     0.0529   -0.0000
  2200        0.0000             nan     0.0529   -0.0000
  2220        0.0000             nan     0.0529   -0.0000
  2240        0.0000             nan     0.0529   -0.0000
  2260        0.0000             nan     0.0529   -0.0000
  2280        0.0000             nan     0.0529   -0.0000
  2300        0.0000             nan     0.0529   -0.0000
  2320        0.0000             nan     0.0529   -0.0000
  2340        0.0000             nan     0.0529   -0.0000
  2360        0.0000             nan     0.0529   -0.0000
  2380        0.0000             nan     0.0529   -0.0000
  2400        0.0000             nan     0.0529   -0.0000
  2420        0.0000             nan     0.0529   -0.0000
  2440        0.0000             nan     0.0529   -0.0000
  2460        0.0000             nan     0.0529   -0.0000
  2480        0.0000             nan     0.0529   -0.0000
  2500        0.0000             nan     0.0529   -0.0000
  2520        0.0000             nan     0.0529   -0.0000
  2540        0.0000             nan     0.0529   -0.0000
  2560        0.0000             nan     0.0529   -0.0000
  2580        0.0000             nan     0.0529   -0.0000
  2600        0.0000             nan     0.0529   -0.0000
  2620        0.0000             nan     0.0529   -0.0000
  2640        0.0000             nan     0.0529   -0.0000
  2660        0.0000             nan     0.0529   -0.0000
  2680        0.0000             nan     0.0529   -0.0000
  2700        0.0000             nan     0.0529   -0.0000
  2720        0.0000             nan     0.0529   -0.0000
  2740        0.0000             nan     0.0529   -0.0000
  2760        0.0000             nan     0.0529    0.0000
  2780        0.0000             nan     0.0529   -0.0000
  2800        0.0000             nan     0.0529   -0.0000
  2820        0.0000             nan     0.0529   -0.0000
  2840        0.0000             nan     0.0529   -0.0000
  2860        0.0000             nan     0.0529   -0.0000
  2880        0.0000             nan     0.0529   -0.0000
  2900        0.0000             nan     0.0529   -0.0000
  2920        0.0000             nan     0.0529   -0.0000
  2940        0.0000             nan     0.0529   -0.0000
  2960        0.0000             nan     0.0529   -0.0000
  2980        0.0000             nan     0.0529    0.0000
  3000        0.0000             nan     0.0529   -0.0000
  3020        0.0000             nan     0.0529   -0.0000
  3040        0.0000             nan     0.0529   -0.0000
  3060        0.0000             nan     0.0529    0.0000
  3080        0.0000             nan     0.0529   -0.0000
  3100        0.0000             nan     0.0529   -0.0000
  3120        0.0000             nan     0.0529   -0.0000
  3140        0.0000             nan     0.0529   -0.0000
  3160        0.0000             nan     0.0529   -0.0000
  3180        0.0000             nan     0.0529    0.0000
  3200        0.0000             nan     0.0529   -0.0000
  3220        0.0000             nan     0.0529   -0.0000
  3240        0.0000             nan     0.0529    0.0000
  3260        0.0000             nan     0.0529   -0.0000
  3280        0.0000             nan     0.0529   -0.0000
  3300        0.0000             nan     0.0529   -0.0000
  3320        0.0000             nan     0.0529   -0.0000
  3340        0.0000             nan     0.0529   -0.0000
  3360        0.0000             nan     0.0529   -0.0000
  3380        0.0000             nan     0.0529   -0.0000
  3400        0.0000             nan     0.0529   -0.0000
  3420        0.0000             nan     0.0529    0.0000
  3440        0.0000             nan     0.0529   -0.0000
  3460        0.0000             nan     0.0529   -0.0000
  3480        0.0000             nan     0.0529   -0.0000
  3500        0.0000             nan     0.0529    0.0000
  3520        0.0000             nan     0.0529   -0.0000
  3540        0.0000             nan     0.0529   -0.0000
  3560        0.0000             nan     0.0529    0.0000
  3580        0.0000             nan     0.0529   -0.0000
  3600        0.0000             nan     0.0529   -0.0000
  3620        0.0000             nan     0.0529    0.0000
  3640        0.0000             nan     0.0529   -0.0000
  3660        0.0000             nan     0.0529   -0.0000
  3680        0.0000             nan     0.0529   -0.0000
  3700        0.0000             nan     0.0529   -0.0000
  3720        0.0000             nan     0.0529   -0.0000
  3740        0.0000             nan     0.0529    0.0000
  3760        0.0000             nan     0.0529   -0.0000
  3780        0.0000             nan     0.0529   -0.0000
  3800        0.0000             nan     0.0529    0.0000
  3820        0.0000             nan     0.0529    0.0000
  3840        0.0000             nan     0.0529    0.0000
  3860        0.0000             nan     0.0529    0.0000
  3880        0.0000             nan     0.0529    0.0000
  3900        0.0000             nan     0.0529    0.0000
  3920        0.0000             nan     0.0529    0.0000
  3940        0.0000             nan     0.0529    0.0000
  3960        0.0000             nan     0.0529   -0.0000
  3980        0.0000             nan     0.0529    0.0000
  3983        0.0000             nan     0.0529   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0360             nan     0.0560    0.0042
     2        0.0323             nan     0.0560    0.0038
     3        0.0291             nan     0.0560    0.0031
     4        0.0261             nan     0.0560    0.0028
     5        0.0235             nan     0.0560    0.0027
     6        0.0212             nan     0.0560    0.0025
     7        0.0190             nan     0.0560    0.0020
     8        0.0170             nan     0.0560    0.0019
     9        0.0153             nan     0.0560    0.0017
    10        0.0138             nan     0.0560    0.0015
    20        0.0050             nan     0.0560    0.0005
    40        0.0009             nan     0.0560    0.0001
    60        0.0004             nan     0.0560    0.0000
    80        0.0003             nan     0.0560    0.0000
   100        0.0002             nan     0.0560    0.0000
   120        0.0002             nan     0.0560   -0.0000
   140        0.0002             nan     0.0560    0.0000
   160        0.0002             nan     0.0560   -0.0000
   180        0.0001             nan     0.0560   -0.0000
   200        0.0001             nan     0.0560   -0.0000
   220        0.0001             nan     0.0560    0.0000
   240        0.0001             nan     0.0560    0.0000
   260        0.0001             nan     0.0560    0.0000
   280        0.0001             nan     0.0560   -0.0000
   300        0.0001             nan     0.0560   -0.0000
   320        0.0001             nan     0.0560   -0.0000
   340        0.0001             nan     0.0560   -0.0000
   360        0.0001             nan     0.0560   -0.0000
   380        0.0001             nan     0.0560   -0.0000
   400        0.0001             nan     0.0560   -0.0000
   420        0.0001             nan     0.0560   -0.0000
   440        0.0001             nan     0.0560   -0.0000
   460        0.0001             nan     0.0560   -0.0000
   480        0.0001             nan     0.0560   -0.0000
   500        0.0001             nan     0.0560   -0.0000
   520        0.0001             nan     0.0560   -0.0000
   540        0.0001             nan     0.0560   -0.0000
   560        0.0001             nan     0.0560   -0.0000
   580        0.0001             nan     0.0560   -0.0000
   600        0.0001             nan     0.0560   -0.0000
   620        0.0001             nan     0.0560   -0.0000
   640        0.0001             nan     0.0560   -0.0000
   660        0.0001             nan     0.0560   -0.0000
   680        0.0001             nan     0.0560   -0.0000
   700        0.0001             nan     0.0560   -0.0000
   720        0.0001             nan     0.0560   -0.0000
   740        0.0000             nan     0.0560   -0.0000
   760        0.0000             nan     0.0560   -0.0000
   780        0.0000             nan     0.0560   -0.0000
   800        0.0000             nan     0.0560   -0.0000
   820        0.0000             nan     0.0560   -0.0000
   840        0.0000             nan     0.0560   -0.0000
   860        0.0000             nan     0.0560   -0.0000
   880        0.0000             nan     0.0560   -0.0000
   900        0.0000             nan     0.0560   -0.0000
   920        0.0000             nan     0.0560   -0.0000
   940        0.0000             nan     0.0560   -0.0000
   960        0.0000             nan     0.0560   -0.0000
   980        0.0000             nan     0.0560   -0.0000
  1000        0.0000             nan     0.0560   -0.0000
  1020        0.0000             nan     0.0560   -0.0000
  1040        0.0000             nan     0.0560   -0.0000
  1060        0.0000             nan     0.0560   -0.0000
  1080        0.0000             nan     0.0560   -0.0000
  1100        0.0000             nan     0.0560   -0.0000
  1120        0.0000             nan     0.0560   -0.0000
  1140        0.0000             nan     0.0560   -0.0000
  1160        0.0000             nan     0.0560   -0.0000
  1180        0.0000             nan     0.0560   -0.0000
  1200        0.0000             nan     0.0560   -0.0000
  1220        0.0000             nan     0.0560   -0.0000
  1240        0.0000             nan     0.0560   -0.0000
  1260        0.0000             nan     0.0560   -0.0000
  1280        0.0000             nan     0.0560   -0.0000
  1300        0.0000             nan     0.0560   -0.0000
  1320        0.0000             nan     0.0560   -0.0000
  1340        0.0000             nan     0.0560   -0.0000
  1360        0.0000             nan     0.0560   -0.0000
  1380        0.0000             nan     0.0560   -0.0000
  1400        0.0000             nan     0.0560   -0.0000
  1420        0.0000             nan     0.0560   -0.0000
  1440        0.0000             nan     0.0560   -0.0000
  1460        0.0000             nan     0.0560   -0.0000
  1480        0.0000             nan     0.0560   -0.0000
  1500        0.0000             nan     0.0560   -0.0000
  1520        0.0000             nan     0.0560   -0.0000
  1540        0.0000             nan     0.0560   -0.0000
  1560        0.0000             nan     0.0560   -0.0000
  1580        0.0000             nan     0.0560   -0.0000
  1600        0.0000             nan     0.0560   -0.0000
  1620        0.0000             nan     0.0560   -0.0000
  1640        0.0000             nan     0.0560   -0.0000
  1660        0.0000             nan     0.0560   -0.0000
  1680        0.0000             nan     0.0560   -0.0000
  1700        0.0000             nan     0.0560   -0.0000
  1720        0.0000             nan     0.0560   -0.0000
  1740        0.0000             nan     0.0560   -0.0000
  1760        0.0000             nan     0.0560   -0.0000
  1780        0.0000             nan     0.0560   -0.0000
  1800        0.0000             nan     0.0560   -0.0000
  1820        0.0000             nan     0.0560   -0.0000
  1840        0.0000             nan     0.0560   -0.0000
  1860        0.0000             nan     0.0560   -0.0000
  1880        0.0000             nan     0.0560   -0.0000
  1900        0.0000             nan     0.0560   -0.0000
  1920        0.0000             nan     0.0560   -0.0000
  1940        0.0000             nan     0.0560   -0.0000
  1960        0.0000             nan     0.0560   -0.0000
  1980        0.0000             nan     0.0560   -0.0000
  2000        0.0000             nan     0.0560   -0.0000
  2020        0.0000             nan     0.0560   -0.0000
  2040        0.0000             nan     0.0560   -0.0000
  2060        0.0000             nan     0.0560   -0.0000
  2080        0.0000             nan     0.0560   -0.0000
  2100        0.0000             nan     0.0560   -0.0000
  2120        0.0000             nan     0.0560   -0.0000
  2140        0.0000             nan     0.0560   -0.0000
  2160        0.0000             nan     0.0560   -0.0000
  2180        0.0000             nan     0.0560   -0.0000
  2200        0.0000             nan     0.0560   -0.0000
  2220        0.0000             nan     0.0560   -0.0000
  2240        0.0000             nan     0.0560   -0.0000
  2260        0.0000             nan     0.0560   -0.0000
  2280        0.0000             nan     0.0560   -0.0000
  2300        0.0000             nan     0.0560   -0.0000
  2320        0.0000             nan     0.0560   -0.0000
  2340        0.0000             nan     0.0560   -0.0000
  2360        0.0000             nan     0.0560   -0.0000
  2380        0.0000             nan     0.0560   -0.0000
  2400        0.0000             nan     0.0560   -0.0000
  2420        0.0000             nan     0.0560   -0.0000
  2440        0.0000             nan     0.0560   -0.0000
  2460        0.0000             nan     0.0560   -0.0000
  2480        0.0000             nan     0.0560   -0.0000
  2500        0.0000             nan     0.0560   -0.0000
  2520        0.0000             nan     0.0560   -0.0000
  2540        0.0000             nan     0.0560   -0.0000
  2560        0.0000             nan     0.0560   -0.0000
  2580        0.0000             nan     0.0560   -0.0000
  2600        0.0000             nan     0.0560   -0.0000
  2620        0.0000             nan     0.0560   -0.0000
  2640        0.0000             nan     0.0560   -0.0000
  2660        0.0000             nan     0.0560   -0.0000
  2680        0.0000             nan     0.0560   -0.0000
  2700        0.0000             nan     0.0560   -0.0000
  2720        0.0000             nan     0.0560   -0.0000
  2740        0.0000             nan     0.0560   -0.0000
  2760        0.0000             nan     0.0560   -0.0000
  2780        0.0000             nan     0.0560   -0.0000
  2800        0.0000             nan     0.0560   -0.0000
  2820        0.0000             nan     0.0560   -0.0000
  2840        0.0000             nan     0.0560   -0.0000
  2860        0.0000             nan     0.0560   -0.0000
  2880        0.0000             nan     0.0560   -0.0000
  2900        0.0000             nan     0.0560   -0.0000
  2920        0.0000             nan     0.0560   -0.0000
  2940        0.0000             nan     0.0560   -0.0000
  2960        0.0000             nan     0.0560   -0.0000
  2980        0.0000             nan     0.0560   -0.0000
  3000        0.0000             nan     0.0560   -0.0000
  3020        0.0000             nan     0.0560   -0.0000
  3040        0.0000             nan     0.0560   -0.0000
  3060        0.0000             nan     0.0560   -0.0000
  3080        0.0000             nan     0.0560   -0.0000
  3100        0.0000             nan     0.0560   -0.0000
  3120        0.0000             nan     0.0560   -0.0000
  3140        0.0000             nan     0.0560   -0.0000
  3160        0.0000             nan     0.0560   -0.0000
  3180        0.0000             nan     0.0560   -0.0000
  3200        0.0000             nan     0.0560   -0.0000
  3220        0.0000             nan     0.0560   -0.0000
  3240        0.0000             nan     0.0560   -0.0000
  3260        0.0000             nan     0.0560   -0.0000
  3280        0.0000             nan     0.0560   -0.0000
  3300        0.0000             nan     0.0560   -0.0000
  3320        0.0000             nan     0.0560   -0.0000
  3340        0.0000             nan     0.0560   -0.0000
  3360        0.0000             nan     0.0560   -0.0000
  3380        0.0000             nan     0.0560   -0.0000
  3400        0.0000             nan     0.0560   -0.0000
  3420        0.0000             nan     0.0560   -0.0000
  3440        0.0000             nan     0.0560   -0.0000
  3460        0.0000             nan     0.0560   -0.0000
  3480        0.0000             nan     0.0560   -0.0000
  3500        0.0000             nan     0.0560   -0.0000
  3520        0.0000             nan     0.0560   -0.0000
  3540        0.0000             nan     0.0560   -0.0000
  3560        0.0000             nan     0.0560   -0.0000
  3580        0.0000             nan     0.0560   -0.0000
  3600        0.0000             nan     0.0560   -0.0000
  3620        0.0000             nan     0.0560   -0.0000
  3640        0.0000             nan     0.0560   -0.0000
  3660        0.0000             nan     0.0560   -0.0000
  3680        0.0000             nan     0.0560   -0.0000
  3700        0.0000             nan     0.0560   -0.0000
  3720        0.0000             nan     0.0560   -0.0000
  3740        0.0000             nan     0.0560   -0.0000
  3760        0.0000             nan     0.0560   -0.0000
  3780        0.0000             nan     0.0560   -0.0000
  3800        0.0000             nan     0.0560   -0.0000
  3820        0.0000             nan     0.0560   -0.0000
  3840        0.0000             nan     0.0560   -0.0000
  3860        0.0000             nan     0.0560   -0.0000
  3880        0.0000             nan     0.0560   -0.0000
  3900        0.0000             nan     0.0560   -0.0000
  3905        0.0000             nan     0.0560   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0350             nan     0.0700    0.0051
     2        0.0307             nan     0.0700    0.0043
     3        0.0267             nan     0.0700    0.0040
     4        0.0234             nan     0.0700    0.0035
     5        0.0205             nan     0.0700    0.0029
     6        0.0180             nan     0.0700    0.0024
     7        0.0159             nan     0.0700    0.0021
     8        0.0139             nan     0.0700    0.0018
     9        0.0123             nan     0.0700    0.0017
    10        0.0108             nan     0.0700    0.0014
    20        0.0032             nan     0.0700    0.0004
    40        0.0006             nan     0.0700    0.0000
    60        0.0003             nan     0.0700    0.0000
    80        0.0002             nan     0.0700   -0.0000
   100        0.0002             nan     0.0700   -0.0000
   120        0.0002             nan     0.0700   -0.0000
   140        0.0002             nan     0.0700   -0.0000
   160        0.0001             nan     0.0700    0.0000
   180        0.0001             nan     0.0700   -0.0000
   200        0.0001             nan     0.0700   -0.0000
   220        0.0001             nan     0.0700   -0.0000
   240        0.0001             nan     0.0700   -0.0000
   260        0.0001             nan     0.0700   -0.0000
   280        0.0001             nan     0.0700    0.0000
   300        0.0001             nan     0.0700   -0.0000
   320        0.0001             nan     0.0700   -0.0000
   340        0.0001             nan     0.0700   -0.0000
   360        0.0001             nan     0.0700   -0.0000
   380        0.0001             nan     0.0700   -0.0000
   400        0.0001             nan     0.0700   -0.0000
   420        0.0001             nan     0.0700   -0.0000
   440        0.0001             nan     0.0700   -0.0000
   460        0.0001             nan     0.0700   -0.0000
   480        0.0001             nan     0.0700   -0.0000
   500        0.0001             nan     0.0700   -0.0000
   520        0.0001             nan     0.0700   -0.0000
   540        0.0001             nan     0.0700   -0.0000
   560        0.0001             nan     0.0700   -0.0000
   580        0.0001             nan     0.0700   -0.0000
   600        0.0001             nan     0.0700   -0.0000
   620        0.0001             nan     0.0700   -0.0000
   640        0.0001             nan     0.0700   -0.0000
   660        0.0000             nan     0.0700   -0.0000
   680        0.0000             nan     0.0700   -0.0000
   700        0.0000             nan     0.0700   -0.0000
   720        0.0000             nan     0.0700   -0.0000
   740        0.0000             nan     0.0700   -0.0000
   760        0.0000             nan     0.0700   -0.0000
   780        0.0000             nan     0.0700   -0.0000
   800        0.0000             nan     0.0700   -0.0000
   820        0.0000             nan     0.0700   -0.0000
   840        0.0000             nan     0.0700   -0.0000
   860        0.0000             nan     0.0700   -0.0000
   880        0.0000             nan     0.0700   -0.0000
   900        0.0000             nan     0.0700   -0.0000
   920        0.0000             nan     0.0700   -0.0000
   940        0.0000             nan     0.0700   -0.0000
   960        0.0000             nan     0.0700   -0.0000
   980        0.0000             nan     0.0700   -0.0000
  1000        0.0000             nan     0.0700   -0.0000
  1020        0.0000             nan     0.0700   -0.0000
  1040        0.0000             nan     0.0700   -0.0000
  1060        0.0000             nan     0.0700   -0.0000
  1080        0.0000             nan     0.0700   -0.0000
  1100        0.0000             nan     0.0700   -0.0000
  1120        0.0000             nan     0.0700   -0.0000
  1140        0.0000             nan     0.0700   -0.0000
  1160        0.0000             nan     0.0700   -0.0000
  1180        0.0000             nan     0.0700   -0.0000
  1200        0.0000             nan     0.0700   -0.0000
  1220        0.0000             nan     0.0700   -0.0000
  1240        0.0000             nan     0.0700   -0.0000
  1260        0.0000             nan     0.0700   -0.0000
  1280        0.0000             nan     0.0700   -0.0000
  1300        0.0000             nan     0.0700   -0.0000
  1320        0.0000             nan     0.0700   -0.0000
  1340        0.0000             nan     0.0700   -0.0000
  1360        0.0000             nan     0.0700   -0.0000
  1380        0.0000             nan     0.0700   -0.0000
  1400        0.0000             nan     0.0700   -0.0000
  1420        0.0000             nan     0.0700   -0.0000
  1440        0.0000             nan     0.0700   -0.0000
  1460        0.0000             nan     0.0700   -0.0000
  1480        0.0000             nan     0.0700   -0.0000
  1500        0.0000             nan     0.0700   -0.0000
  1520        0.0000             nan     0.0700   -0.0000
  1540        0.0000             nan     0.0700   -0.0000
  1560        0.0000             nan     0.0700   -0.0000
  1580        0.0000             nan     0.0700   -0.0000
  1600        0.0000             nan     0.0700   -0.0000
  1620        0.0000             nan     0.0700   -0.0000
  1640        0.0000             nan     0.0700   -0.0000
  1660        0.0000             nan     0.0700   -0.0000
  1680        0.0000             nan     0.0700   -0.0000
  1700        0.0000             nan     0.0700   -0.0000
  1720        0.0000             nan     0.0700   -0.0000
  1740        0.0000             nan     0.0700   -0.0000
  1760        0.0000             nan     0.0700   -0.0000
  1765        0.0000             nan     0.0700   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0361             nan     0.0760    0.0039
     2        0.0327             nan     0.0760    0.0034
     3        0.0298             nan     0.0760    0.0030
     4        0.0272             nan     0.0760    0.0026
     5        0.0250             nan     0.0760    0.0020
     6        0.0228             nan     0.0760    0.0022
     7        0.0210             nan     0.0760    0.0018
     8        0.0194             nan     0.0760    0.0016
     9        0.0178             nan     0.0760    0.0015
    10        0.0165             nan     0.0760    0.0014
    20        0.0081             nan     0.0760    0.0005
    40        0.0024             nan     0.0760    0.0001
    60        0.0010             nan     0.0760    0.0000
    80        0.0005             nan     0.0760    0.0000
   100        0.0003             nan     0.0760    0.0000
   120        0.0002             nan     0.0760    0.0000
   140        0.0002             nan     0.0760    0.0000
   160        0.0002             nan     0.0760    0.0000
   180        0.0002             nan     0.0760   -0.0000
   200        0.0002             nan     0.0760   -0.0000
   220        0.0002             nan     0.0760   -0.0000
   240        0.0002             nan     0.0760    0.0000
   260        0.0002             nan     0.0760   -0.0000
   280        0.0002             nan     0.0760   -0.0000
   300        0.0002             nan     0.0760   -0.0000
   320        0.0001             nan     0.0760    0.0000
   340        0.0001             nan     0.0760   -0.0000
   360        0.0001             nan     0.0760   -0.0000
   380        0.0001             nan     0.0760   -0.0000
   400        0.0001             nan     0.0760   -0.0000
   420        0.0001             nan     0.0760   -0.0000
   440        0.0001             nan     0.0760   -0.0000
   460        0.0001             nan     0.0760   -0.0000
   480        0.0001             nan     0.0760   -0.0000
   500        0.0001             nan     0.0760   -0.0000
   520        0.0001             nan     0.0760    0.0000
   540        0.0001             nan     0.0760   -0.0000
   560        0.0001             nan     0.0760   -0.0000
   580        0.0001             nan     0.0760   -0.0000
   600        0.0001             nan     0.0760   -0.0000
   620        0.0001             nan     0.0760   -0.0000
   640        0.0001             nan     0.0760   -0.0000
   660        0.0001             nan     0.0760   -0.0000
   680        0.0001             nan     0.0760   -0.0000
   700        0.0001             nan     0.0760   -0.0000
   720        0.0001             nan     0.0760   -0.0000
   740        0.0001             nan     0.0760   -0.0000
   760        0.0001             nan     0.0760   -0.0000
   780        0.0001             nan     0.0760   -0.0000
   800        0.0001             nan     0.0760   -0.0000
   820        0.0001             nan     0.0760   -0.0000
   840        0.0001             nan     0.0760   -0.0000
   860        0.0001             nan     0.0760   -0.0000
   880        0.0001             nan     0.0760   -0.0000
   900        0.0001             nan     0.0760   -0.0000
   920        0.0001             nan     0.0760   -0.0000
   940        0.0001             nan     0.0760   -0.0000
   960        0.0001             nan     0.0760   -0.0000
   980        0.0001             nan     0.0760   -0.0000
  1000        0.0001             nan     0.0760   -0.0000
  1020        0.0001             nan     0.0760   -0.0000
  1040        0.0001             nan     0.0760   -0.0000
  1060        0.0001             nan     0.0760   -0.0000
  1080        0.0001             nan     0.0760   -0.0000
  1100        0.0001             nan     0.0760   -0.0000
  1120        0.0001             nan     0.0760   -0.0000
  1140        0.0001             nan     0.0760   -0.0000
  1160        0.0001             nan     0.0760   -0.0000
  1180        0.0001             nan     0.0760   -0.0000
  1200        0.0001             nan     0.0760   -0.0000
  1220        0.0001             nan     0.0760   -0.0000
  1240        0.0001             nan     0.0760   -0.0000
  1260        0.0001             nan     0.0760   -0.0000
  1280        0.0001             nan     0.0760   -0.0000
  1300        0.0001             nan     0.0760   -0.0000
  1320        0.0001             nan     0.0760   -0.0000
  1340        0.0001             nan     0.0760   -0.0000
  1360        0.0001             nan     0.0760   -0.0000
  1380        0.0001             nan     0.0760   -0.0000
  1400        0.0001             nan     0.0760   -0.0000
  1420        0.0001             nan     0.0760   -0.0000
  1440        0.0001             nan     0.0760   -0.0000
  1460        0.0001             nan     0.0760   -0.0000
  1480        0.0001             nan     0.0760   -0.0000
  1500        0.0001             nan     0.0760   -0.0000
  1520        0.0001             nan     0.0760   -0.0000
  1540        0.0001             nan     0.0760   -0.0000
  1560        0.0001             nan     0.0760   -0.0000
  1580        0.0001             nan     0.0760   -0.0000
  1600        0.0001             nan     0.0760   -0.0000
  1620        0.0001             nan     0.0760   -0.0000
  1640        0.0001             nan     0.0760   -0.0000
  1660        0.0001             nan     0.0760   -0.0000
  1680        0.0001             nan     0.0760   -0.0000
  1700        0.0001             nan     0.0760   -0.0000
  1720        0.0001             nan     0.0760   -0.0000
  1740        0.0001             nan     0.0760   -0.0000
  1760        0.0001             nan     0.0760   -0.0000
  1780        0.0001             nan     0.0760   -0.0000
  1800        0.0001             nan     0.0760   -0.0000
  1820        0.0001             nan     0.0760   -0.0000
  1840        0.0001             nan     0.0760   -0.0000
  1860        0.0001             nan     0.0760   -0.0000
  1880        0.0001             nan     0.0760   -0.0000
  1900        0.0001             nan     0.0760   -0.0000
  1920        0.0001             nan     0.0760   -0.0000
  1940        0.0001             nan     0.0760   -0.0000
  1960        0.0001             nan     0.0760   -0.0000
  1980        0.0001             nan     0.0760   -0.0000
  2000        0.0001             nan     0.0760   -0.0000
  2020        0.0001             nan     0.0760   -0.0000
  2040        0.0001             nan     0.0760   -0.0000
  2060        0.0001             nan     0.0760   -0.0000
  2080        0.0001             nan     0.0760   -0.0000
  2100        0.0001             nan     0.0760   -0.0000
  2120        0.0001             nan     0.0760   -0.0000
  2140        0.0001             nan     0.0760   -0.0000
  2160        0.0001             nan     0.0760   -0.0000
  2180        0.0001             nan     0.0760   -0.0000
  2200        0.0001             nan     0.0760   -0.0000
  2220        0.0001             nan     0.0760   -0.0000
  2240        0.0001             nan     0.0760   -0.0000
  2260        0.0001             nan     0.0760   -0.0000
  2280        0.0001             nan     0.0760   -0.0000
  2300        0.0001             nan     0.0760   -0.0000
  2320        0.0001             nan     0.0760   -0.0000
  2340        0.0001             nan     0.0760   -0.0000
  2360        0.0001             nan     0.0760   -0.0000
  2380        0.0001             nan     0.0760   -0.0000
  2400        0.0001             nan     0.0760   -0.0000
  2420        0.0001             nan     0.0760   -0.0000
  2440        0.0001             nan     0.0760   -0.0000
  2460        0.0001             nan     0.0760   -0.0000
  2480        0.0001             nan     0.0760   -0.0000
  2500        0.0001             nan     0.0760   -0.0000
  2520        0.0001             nan     0.0760   -0.0000
  2540        0.0001             nan     0.0760   -0.0000
  2560        0.0001             nan     0.0760   -0.0000
  2580        0.0001             nan     0.0760   -0.0000
  2600        0.0001             nan     0.0760   -0.0000
  2620        0.0001             nan     0.0760   -0.0000
  2640        0.0001             nan     0.0760   -0.0000
  2660        0.0001             nan     0.0760   -0.0000
  2680        0.0001             nan     0.0760   -0.0000
  2700        0.0001             nan     0.0760   -0.0000
  2720        0.0001             nan     0.0760   -0.0000
  2740        0.0001             nan     0.0760   -0.0000
  2760        0.0001             nan     0.0760   -0.0000
  2780        0.0001             nan     0.0760   -0.0000
  2800        0.0001             nan     0.0760   -0.0000
  2820        0.0001             nan     0.0760   -0.0000
  2840        0.0001             nan     0.0760   -0.0000
  2860        0.0001             nan     0.0760   -0.0000
  2880        0.0001             nan     0.0760   -0.0000
  2900        0.0001             nan     0.0760   -0.0000
  2920        0.0001             nan     0.0760   -0.0000
  2940        0.0001             nan     0.0760   -0.0000
  2960        0.0001             nan     0.0760   -0.0000
  2980        0.0001             nan     0.0760   -0.0000
  3000        0.0001             nan     0.0760   -0.0000
  3020        0.0001             nan     0.0760   -0.0000
  3040        0.0001             nan     0.0760   -0.0000
  3060        0.0001             nan     0.0760   -0.0000
  3080        0.0001             nan     0.0760   -0.0000
  3100        0.0001             nan     0.0760   -0.0000
  3120        0.0001             nan     0.0760   -0.0000
  3140        0.0001             nan     0.0760   -0.0000
  3160        0.0001             nan     0.0760   -0.0000
  3180        0.0001             nan     0.0760   -0.0000
  3200        0.0001             nan     0.0760   -0.0000
  3220        0.0001             nan     0.0760   -0.0000
  3240        0.0001             nan     0.0760   -0.0000
  3260        0.0001             nan     0.0760   -0.0000
  3280        0.0001             nan     0.0760   -0.0000
  3300        0.0001             nan     0.0760   -0.0000
  3320        0.0001             nan     0.0760   -0.0000
  3340        0.0001             nan     0.0760   -0.0000
  3360        0.0001             nan     0.0760   -0.0000
  3380        0.0001             nan     0.0760   -0.0000
  3400        0.0001             nan     0.0760   -0.0000
  3420        0.0001             nan     0.0760   -0.0000
  3440        0.0001             nan     0.0760   -0.0000
  3460        0.0000             nan     0.0760   -0.0000
  3480        0.0000             nan     0.0760   -0.0000
  3500        0.0000             nan     0.0760   -0.0000
  3520        0.0000             nan     0.0760   -0.0000
  3540        0.0000             nan     0.0760   -0.0000
  3560        0.0000             nan     0.0760   -0.0000
  3580        0.0000             nan     0.0760   -0.0000
  3600        0.0000             nan     0.0760   -0.0000
  3620        0.0000             nan     0.0760   -0.0000
  3640        0.0000             nan     0.0760   -0.0000
  3660        0.0000             nan     0.0760   -0.0000
  3680        0.0000             nan     0.0760   -0.0000
  3700        0.0000             nan     0.0760   -0.0000
  3720        0.0000             nan     0.0760   -0.0000
  3740        0.0000             nan     0.0760   -0.0000
  3760        0.0000             nan     0.0760   -0.0000
  3780        0.0000             nan     0.0760   -0.0000
  3800        0.0000             nan     0.0760   -0.0000
  3820        0.0000             nan     0.0760   -0.0000
  3840        0.0000             nan     0.0760   -0.0000
  3860        0.0000             nan     0.0760   -0.0000
  3880        0.0000             nan     0.0760   -0.0000
  3900        0.0000             nan     0.0760   -0.0000
  3920        0.0000             nan     0.0760   -0.0000
  3940        0.0000             nan     0.0760   -0.0000
  3960        0.0000             nan     0.0760   -0.0000
  3980        0.0000             nan     0.0760   -0.0000
  4000        0.0000             nan     0.0760   -0.0000
  4020        0.0000             nan     0.0760   -0.0000
  4040        0.0000             nan     0.0760   -0.0000
  4060        0.0000             nan     0.0760   -0.0000
  4080        0.0000             nan     0.0760   -0.0000
  4100        0.0000             nan     0.0760   -0.0000
  4120        0.0000             nan     0.0760   -0.0000
  4140        0.0000             nan     0.0760   -0.0000
  4160        0.0000             nan     0.0760   -0.0000
  4180        0.0000             nan     0.0760   -0.0000
  4200        0.0000             nan     0.0760   -0.0000
  4220        0.0000             nan     0.0760   -0.0000
  4240        0.0000             nan     0.0760   -0.0000
  4260        0.0000             nan     0.0760   -0.0000
  4280        0.0000             nan     0.0760   -0.0000
  4300        0.0000             nan     0.0760   -0.0000
  4320        0.0000             nan     0.0760   -0.0000
  4340        0.0000             nan     0.0760   -0.0000
  4360        0.0000             nan     0.0760   -0.0000
  4380        0.0000             nan     0.0760   -0.0000
  4400        0.0000             nan     0.0760   -0.0000
  4420        0.0000             nan     0.0760   -0.0000
  4440        0.0000             nan     0.0760   -0.0000
  4460        0.0000             nan     0.0760   -0.0000
  4480        0.0000             nan     0.0760   -0.0000
  4500        0.0000             nan     0.0760   -0.0000
  4520        0.0000             nan     0.0760   -0.0000
  4540        0.0000             nan     0.0760   -0.0000
  4560        0.0000             nan     0.0760   -0.0000
  4580        0.0000             nan     0.0760   -0.0000
  4600        0.0000             nan     0.0760   -0.0000
  4620        0.0000             nan     0.0760   -0.0000
  4640        0.0000             nan     0.0760   -0.0000
  4660        0.0000             nan     0.0760   -0.0000
  4680        0.0000             nan     0.0760   -0.0000
  4700        0.0000             nan     0.0760   -0.0000
  4720        0.0000             nan     0.0760   -0.0000
  4740        0.0000             nan     0.0760   -0.0000
  4760        0.0000             nan     0.0760   -0.0000
  4780        0.0000             nan     0.0760   -0.0000
  4800        0.0000             nan     0.0760   -0.0000
  4820        0.0000             nan     0.0760   -0.0000
  4840        0.0000             nan     0.0760   -0.0000
  4860        0.0000             nan     0.0760   -0.0000
  4880        0.0000             nan     0.0760   -0.0000
  4900        0.0000             nan     0.0760   -0.0000
  4920        0.0000             nan     0.0760   -0.0000
  4940        0.0000             nan     0.0760   -0.0000
  4960        0.0000             nan     0.0760   -0.0000
  4980        0.0000             nan     0.0760   -0.0000
  4982        0.0000             nan     0.0760   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0338             nan     0.0850    0.0066
     2        0.0286             nan     0.0850    0.0055
     3        0.0242             nan     0.0850    0.0043
     4        0.0205             nan     0.0850    0.0037
     5        0.0174             nan     0.0850    0.0028
     6        0.0147             nan     0.0850    0.0027
     7        0.0124             nan     0.0850    0.0022
     8        0.0105             nan     0.0850    0.0018
     9        0.0090             nan     0.0850    0.0015
    10        0.0076             nan     0.0850    0.0011
    20        0.0016             nan     0.0850    0.0003
    40        0.0001             nan     0.0850    0.0000
    60        0.0000             nan     0.0850    0.0000
    80        0.0000             nan     0.0850   -0.0000
   100        0.0000             nan     0.0850   -0.0000
   120        0.0000             nan     0.0850   -0.0000
   140        0.0000             nan     0.0850   -0.0000
   160        0.0000             nan     0.0850   -0.0000
   180        0.0000             nan     0.0850   -0.0000
   200        0.0000             nan     0.0850   -0.0000
   220        0.0000             nan     0.0850   -0.0000
   240        0.0000             nan     0.0850   -0.0000
   260        0.0000             nan     0.0850   -0.0000
   280        0.0000             nan     0.0850   -0.0000
   300        0.0000             nan     0.0850   -0.0000
   320        0.0000             nan     0.0850   -0.0000
   340        0.0000             nan     0.0850   -0.0000
   360        0.0000             nan     0.0850   -0.0000
   380        0.0000             nan     0.0850   -0.0000
   400        0.0000             nan     0.0850   -0.0000
   420        0.0000             nan     0.0850   -0.0000
   440        0.0000             nan     0.0850   -0.0000
   460        0.0000             nan     0.0850   -0.0000
   480        0.0000             nan     0.0850   -0.0000
   500        0.0000             nan     0.0850   -0.0000
   520        0.0000             nan     0.0850   -0.0000
   540        0.0000             nan     0.0850   -0.0000
   560        0.0000             nan     0.0850   -0.0000
   580        0.0000             nan     0.0850   -0.0000
   600        0.0000             nan     0.0850   -0.0000
   620        0.0000             nan     0.0850   -0.0000
   640        0.0000             nan     0.0850   -0.0000
   660        0.0000             nan     0.0850   -0.0000
   680        0.0000             nan     0.0850   -0.0000
   700        0.0000             nan     0.0850   -0.0000
   720        0.0000             nan     0.0850   -0.0000
   740        0.0000             nan     0.0850   -0.0000
   760        0.0000             nan     0.0850   -0.0000
   780        0.0000             nan     0.0850   -0.0000
   800        0.0000             nan     0.0850   -0.0000
   820        0.0000             nan     0.0850   -0.0000
   840        0.0000             nan     0.0850   -0.0000
   860        0.0000             nan     0.0850   -0.0000
   880        0.0000             nan     0.0850   -0.0000
   900        0.0000             nan     0.0850   -0.0000
   920        0.0000             nan     0.0850   -0.0000
   940        0.0000             nan     0.0850   -0.0000
   960        0.0000             nan     0.0850   -0.0000
   980        0.0000             nan     0.0850   -0.0000
  1000        0.0000             nan     0.0850   -0.0000
  1020        0.0000             nan     0.0850   -0.0000
  1040        0.0000             nan     0.0850   -0.0000
  1060        0.0000             nan     0.0850   -0.0000
  1080        0.0000             nan     0.0850   -0.0000
  1100        0.0000             nan     0.0850   -0.0000
  1120        0.0000             nan     0.0850   -0.0000
  1140        0.0000             nan     0.0850   -0.0000
  1160        0.0000             nan     0.0850   -0.0000
  1180        0.0000             nan     0.0850   -0.0000
  1200        0.0000             nan     0.0850   -0.0000
  1220        0.0000             nan     0.0850   -0.0000
  1240        0.0000             nan     0.0850   -0.0000
  1260        0.0000             nan     0.0850   -0.0000
  1280        0.0000             nan     0.0850   -0.0000
  1300        0.0000             nan     0.0850   -0.0000
  1320        0.0000             nan     0.0850   -0.0000
  1340        0.0000             nan     0.0850   -0.0000
  1360        0.0000             nan     0.0850   -0.0000
  1380        0.0000             nan     0.0850   -0.0000
  1400        0.0000             nan     0.0850   -0.0000
  1420        0.0000             nan     0.0850   -0.0000
  1440        0.0000             nan     0.0850   -0.0000
  1460        0.0000             nan     0.0850   -0.0000
  1480        0.0000             nan     0.0850   -0.0000
  1500        0.0000             nan     0.0850   -0.0000
  1520        0.0000             nan     0.0850    0.0000
  1540        0.0000             nan     0.0850   -0.0000
  1560        0.0000             nan     0.0850   -0.0000
  1580        0.0000             nan     0.0850   -0.0000
  1600        0.0000             nan     0.0850   -0.0000
  1620        0.0000             nan     0.0850    0.0000
  1640        0.0000             nan     0.0850   -0.0000
  1660        0.0000             nan     0.0850   -0.0000
  1680        0.0000             nan     0.0850   -0.0000
  1700        0.0000             nan     0.0850    0.0000
  1720        0.0000             nan     0.0850    0.0000
  1740        0.0000             nan     0.0850   -0.0000
  1760        0.0000             nan     0.0850    0.0000
  1780        0.0000             nan     0.0850   -0.0000
  1800        0.0000             nan     0.0850   -0.0000
  1820        0.0000             nan     0.0850    0.0000
  1840        0.0000             nan     0.0850   -0.0000
  1860        0.0000             nan     0.0850   -0.0000
  1880        0.0000             nan     0.0850    0.0000
  1900        0.0000             nan     0.0850   -0.0000
  1920        0.0000             nan     0.0850   -0.0000
  1940        0.0000             nan     0.0850   -0.0000
  1960        0.0000             nan     0.0850   -0.0000
  1980        0.0000             nan     0.0850   -0.0000
  2000        0.0000             nan     0.0850   -0.0000
  2020        0.0000             nan     0.0850    0.0000
  2040        0.0000             nan     0.0850   -0.0000
  2060        0.0000             nan     0.0850   -0.0000
  2080        0.0000             nan     0.0850    0.0000
  2100        0.0000             nan     0.0850   -0.0000
  2120        0.0000             nan     0.0850   -0.0000
  2140        0.0000             nan     0.0850   -0.0000
  2160        0.0000             nan     0.0850   -0.0000
  2180        0.0000             nan     0.0850    0.0000
  2200        0.0000             nan     0.0850   -0.0000
  2220        0.0000             nan     0.0850   -0.0000
  2240        0.0000             nan     0.0850   -0.0000
  2260        0.0000             nan     0.0850    0.0000
  2280        0.0000             nan     0.0850   -0.0000
  2300        0.0000             nan     0.0850   -0.0000
  2320        0.0000             nan     0.0850    0.0000
  2340        0.0000             nan     0.0850   -0.0000
  2360        0.0000             nan     0.0850    0.0000
  2380        0.0000             nan     0.0850    0.0000
  2400        0.0000             nan     0.0850   -0.0000
  2420        0.0000             nan     0.0850   -0.0000
  2440        0.0000             nan     0.0850    0.0000
  2460        0.0000             nan     0.0850    0.0000
  2480        0.0000             nan     0.0850    0.0000
  2500        0.0000             nan     0.0850   -0.0000
  2520        0.0000             nan     0.0850    0.0000
  2540        0.0000             nan     0.0850   -0.0000
  2560        0.0000             nan     0.0850   -0.0000
  2580        0.0000             nan     0.0850   -0.0000
  2600        0.0000             nan     0.0850    0.0000
  2620        0.0000             nan     0.0850   -0.0000
  2640        0.0000             nan     0.0850   -0.0000
  2660        0.0000             nan     0.0850   -0.0000
  2680        0.0000             nan     0.0850   -0.0000
  2700        0.0000             nan     0.0850   -0.0000
  2720        0.0000             nan     0.0850   -0.0000
  2740        0.0000             nan     0.0850   -0.0000
  2760        0.0000             nan     0.0850   -0.0000
  2780        0.0000             nan     0.0850   -0.0000
  2800        0.0000             nan     0.0850    0.0000
  2820        0.0000             nan     0.0850   -0.0000
  2840        0.0000             nan     0.0850    0.0000
  2860        0.0000             nan     0.0850    0.0000
  2880        0.0000             nan     0.0850    0.0000
  2900        0.0000             nan     0.0850   -0.0000
  2920        0.0000             nan     0.0850    0.0000
  2940        0.0000             nan     0.0850   -0.0000
  2960        0.0000             nan     0.0850    0.0000
  2980        0.0000             nan     0.0850    0.0000
  3000        0.0000             nan     0.0850    0.0000
  3020        0.0000             nan     0.0850   -0.0000
  3040        0.0000             nan     0.0850   -0.0000
  3060        0.0000             nan     0.0850    0.0000
  3080        0.0000             nan     0.0850    0.0000
  3100        0.0000             nan     0.0850   -0.0000
  3120        0.0000             nan     0.0850   -0.0000
  3140        0.0000             nan     0.0850   -0.0000
  3160        0.0000             nan     0.0850    0.0000
  3180        0.0000             nan     0.0850   -0.0000
  3200        0.0000             nan     0.0850   -0.0000
  3220        0.0000             nan     0.0850    0.0000
  3240        0.0000             nan     0.0850   -0.0000
  3260        0.0000             nan     0.0850   -0.0000
  3280        0.0000             nan     0.0850   -0.0000
  3300        0.0000             nan     0.0850   -0.0000
  3320        0.0000             nan     0.0850   -0.0000
  3340        0.0000             nan     0.0850   -0.0000
  3360        0.0000             nan     0.0850   -0.0000
  3380        0.0000             nan     0.0850    0.0000
  3400        0.0000             nan     0.0850    0.0000
  3420        0.0000             nan     0.0850    0.0000
  3440        0.0000             nan     0.0850   -0.0000
  3460        0.0000             nan     0.0850   -0.0000
  3480        0.0000             nan     0.0850   -0.0000
  3500        0.0000             nan     0.0850   -0.0000
  3520        0.0000             nan     0.0850   -0.0000
  3540        0.0000             nan     0.0850    0.0000
  3560        0.0000             nan     0.0850    0.0000
  3580        0.0000             nan     0.0850    0.0000
  3600        0.0000             nan     0.0850    0.0000
  3620        0.0000             nan     0.0850   -0.0000
  3640        0.0000             nan     0.0850    0.0000
  3660        0.0000             nan     0.0850    0.0000
  3680        0.0000             nan     0.0850   -0.0000
  3700        0.0000             nan     0.0850   -0.0000
  3720        0.0000             nan     0.0850   -0.0000
  3740        0.0000             nan     0.0850    0.0000
  3760        0.0000             nan     0.0850   -0.0000
  3780        0.0000             nan     0.0850    0.0000
  3800        0.0000             nan     0.0850    0.0000
  3820        0.0000             nan     0.0850   -0.0000
  3840        0.0000             nan     0.0850   -0.0000
  3860        0.0000             nan     0.0850   -0.0000
  3880        0.0000             nan     0.0850   -0.0000
  3900        0.0000             nan     0.0850    0.0000
  3920        0.0000             nan     0.0850   -0.0000
  3940        0.0000             nan     0.0850    0.0000
  3960        0.0000             nan     0.0850    0.0000
  3980        0.0000             nan     0.0850    0.0000
  4000        0.0000             nan     0.0850    0.0000
  4020        0.0000             nan     0.0850   -0.0000
  4040        0.0000             nan     0.0850   -0.0000
  4060        0.0000             nan     0.0850    0.0000
  4080        0.0000             nan     0.0850   -0.0000
  4100        0.0000             nan     0.0850   -0.0000
  4120        0.0000             nan     0.0850    0.0000
  4140        0.0000             nan     0.0850   -0.0000
  4160        0.0000             nan     0.0850    0.0000
  4180        0.0000             nan     0.0850   -0.0000
  4200        0.0000             nan     0.0850    0.0000
  4220        0.0000             nan     0.0850    0.0000
  4240        0.0000             nan     0.0850   -0.0000
  4260        0.0000             nan     0.0850   -0.0000
  4280        0.0000             nan     0.0850   -0.0000
  4281        0.0000             nan     0.0850    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0316             nan     0.1213    0.0084
     2        0.0250             nan     0.1213    0.0064
     3        0.0198             nan     0.1213    0.0050
     4        0.0159             nan     0.1213    0.0039
     5        0.0126             nan     0.1213    0.0030
     6        0.0101             nan     0.1213    0.0026
     7        0.0082             nan     0.1213    0.0019
     8        0.0066             nan     0.1213    0.0015
     9        0.0053             nan     0.1213    0.0012
    10        0.0043             nan     0.1213    0.0009
    20        0.0008             nan     0.1213    0.0001
    40        0.0003             nan     0.1213    0.0000
    60        0.0002             nan     0.1213   -0.0000
    80        0.0002             nan     0.1213   -0.0000
   100        0.0002             nan     0.1213   -0.0000
   120        0.0001             nan     0.1213   -0.0000
   140        0.0001             nan     0.1213   -0.0000
   160        0.0001             nan     0.1213   -0.0000
   180        0.0001             nan     0.1213    0.0000
   200        0.0001             nan     0.1213   -0.0000
   220        0.0001             nan     0.1213   -0.0000
   240        0.0001             nan     0.1213   -0.0000
   260        0.0001             nan     0.1213   -0.0000
   280        0.0001             nan     0.1213   -0.0000
   300        0.0001             nan     0.1213   -0.0000
   320        0.0001             nan     0.1213   -0.0000
   340        0.0001             nan     0.1213   -0.0000
   360        0.0001             nan     0.1213   -0.0000
   380        0.0001             nan     0.1213   -0.0000
   400        0.0001             nan     0.1213   -0.0000
   420        0.0001             nan     0.1213   -0.0000
   440        0.0001             nan     0.1213   -0.0000
   460        0.0001             nan     0.1213   -0.0000
   480        0.0001             nan     0.1213   -0.0000
   500        0.0000             nan     0.1213   -0.0000
   520        0.0000             nan     0.1213   -0.0000
   540        0.0000             nan     0.1213   -0.0000
   560        0.0000             nan     0.1213   -0.0000
   580        0.0000             nan     0.1213   -0.0000
   600        0.0000             nan     0.1213   -0.0000
   620        0.0000             nan     0.1213   -0.0000
   640        0.0000             nan     0.1213   -0.0000
   660        0.0000             nan     0.1213   -0.0000
   680        0.0000             nan     0.1213   -0.0000
   700        0.0000             nan     0.1213   -0.0000
   720        0.0000             nan     0.1213   -0.0000
   740        0.0000             nan     0.1213   -0.0000
   760        0.0000             nan     0.1213   -0.0000
   780        0.0000             nan     0.1213   -0.0000
   800        0.0000             nan     0.1213   -0.0000
   820        0.0000             nan     0.1213   -0.0000
   840        0.0000             nan     0.1213   -0.0000
   860        0.0000             nan     0.1213   -0.0000
   880        0.0000             nan     0.1213   -0.0000
   900        0.0000             nan     0.1213   -0.0000
   920        0.0000             nan     0.1213   -0.0000
   940        0.0000             nan     0.1213   -0.0000
   960        0.0000             nan     0.1213   -0.0000
   980        0.0000             nan     0.1213   -0.0000
  1000        0.0000             nan     0.1213   -0.0000
  1020        0.0000             nan     0.1213   -0.0000
  1040        0.0000             nan     0.1213   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0308             nan     0.1295    0.0093
     2        0.0239             nan     0.1295    0.0068
     3        0.0184             nan     0.1295    0.0054
     4        0.0142             nan     0.1295    0.0040
     5        0.0110             nan     0.1295    0.0028
     6        0.0086             nan     0.1295    0.0025
     7        0.0067             nan     0.1295    0.0018
     8        0.0052             nan     0.1295    0.0016
     9        0.0041             nan     0.1295    0.0011
    10        0.0032             nan     0.1295    0.0008
    20        0.0004             nan     0.1295    0.0001
    40        0.0001             nan     0.1295   -0.0000
    60        0.0001             nan     0.1295    0.0000
    80        0.0000             nan     0.1295   -0.0000
   100        0.0000             nan     0.1295   -0.0000
   120        0.0000             nan     0.1295   -0.0000
   140        0.0000             nan     0.1295   -0.0000
   160        0.0000             nan     0.1295    0.0000
   180        0.0000             nan     0.1295   -0.0000
   200        0.0000             nan     0.1295   -0.0000
   220        0.0000             nan     0.1295   -0.0000
   240        0.0000             nan     0.1295   -0.0000
   260        0.0000             nan     0.1295   -0.0000
   280        0.0000             nan     0.1295   -0.0000
   300        0.0000             nan     0.1295   -0.0000
   320        0.0000             nan     0.1295   -0.0000
   340        0.0000             nan     0.1295   -0.0000
   360        0.0000             nan     0.1295   -0.0000
   380        0.0000             nan     0.1295   -0.0000
   400        0.0000             nan     0.1295   -0.0000
   420        0.0000             nan     0.1295   -0.0000
   440        0.0000             nan     0.1295   -0.0000
   460        0.0000             nan     0.1295   -0.0000
   480        0.0000             nan     0.1295   -0.0000
   500        0.0000             nan     0.1295   -0.0000
   520        0.0000             nan     0.1295   -0.0000
   540        0.0000             nan     0.1295   -0.0000
   560        0.0000             nan     0.1295   -0.0000
   580        0.0000             nan     0.1295   -0.0000
   600        0.0000             nan     0.1295   -0.0000
   620        0.0000             nan     0.1295   -0.0000
   640        0.0000             nan     0.1295   -0.0000
   660        0.0000             nan     0.1295   -0.0000
   673        0.0000             nan     0.1295   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0287             nan     0.1700    0.0115
     2        0.0206             nan     0.1700    0.0081
     3        0.0150             nan     0.1700    0.0055
     4        0.0111             nan     0.1700    0.0042
     5        0.0081             nan     0.1700    0.0028
     6        0.0061             nan     0.1700    0.0020
     7        0.0046             nan     0.1700    0.0015
     8        0.0034             nan     0.1700    0.0011
     9        0.0026             nan     0.1700    0.0008
    10        0.0020             nan     0.1700    0.0006
    20        0.0003             nan     0.1700    0.0000
    40        0.0001             nan     0.1700   -0.0000
    60        0.0001             nan     0.1700   -0.0000
    80        0.0001             nan     0.1700   -0.0000
   100        0.0001             nan     0.1700   -0.0000
   120        0.0001             nan     0.1700   -0.0000
   140        0.0001             nan     0.1700    0.0000
   160        0.0001             nan     0.1700   -0.0000
   180        0.0000             nan     0.1700   -0.0000
   200        0.0000             nan     0.1700   -0.0000
   220        0.0000             nan     0.1700   -0.0000
   240        0.0000             nan     0.1700   -0.0000
   260        0.0000             nan     0.1700   -0.0000
   280        0.0000             nan     0.1700   -0.0000
   300        0.0000             nan     0.1700   -0.0000
   320        0.0000             nan     0.1700   -0.0000
   340        0.0000             nan     0.1700   -0.0000
   360        0.0000             nan     0.1700   -0.0000
   380        0.0000             nan     0.1700   -0.0000
   400        0.0000             nan     0.1700   -0.0000
   420        0.0000             nan     0.1700   -0.0000
   440        0.0000             nan     0.1700   -0.0000
   460        0.0000             nan     0.1700   -0.0000
   480        0.0000             nan     0.1700   -0.0000
   500        0.0000             nan     0.1700   -0.0000
   520        0.0000             nan     0.1700   -0.0000
   540        0.0000             nan     0.1700   -0.0000
   560        0.0000             nan     0.1700   -0.0000
   580        0.0000             nan     0.1700   -0.0000
   600        0.0000             nan     0.1700   -0.0000
   620        0.0000             nan     0.1700   -0.0000
   640        0.0000             nan     0.1700   -0.0000
   660        0.0000             nan     0.1700   -0.0000
   680        0.0000             nan     0.1700   -0.0000
   700        0.0000             nan     0.1700   -0.0000
   720        0.0000             nan     0.1700   -0.0000
   740        0.0000             nan     0.1700   -0.0000
   760        0.0000             nan     0.1700   -0.0000
   780        0.0000             nan     0.1700   -0.0000
   800        0.0000             nan     0.1700   -0.0000
   820        0.0000             nan     0.1700   -0.0000
   840        0.0000             nan     0.1700   -0.0000
   860        0.0000             nan     0.1700   -0.0000
   880        0.0000             nan     0.1700   -0.0000
   900        0.0000             nan     0.1700   -0.0000
   920        0.0000             nan     0.1700   -0.0000
   940        0.0000             nan     0.1700   -0.0000
   960        0.0000             nan     0.1700   -0.0000
   980        0.0000             nan     0.1700   -0.0000
  1000        0.0000             nan     0.1700   -0.0000
  1020        0.0000             nan     0.1700   -0.0000
  1040        0.0000             nan     0.1700   -0.0000
  1060        0.0000             nan     0.1700   -0.0000
  1080        0.0000             nan     0.1700   -0.0000
  1100        0.0000             nan     0.1700   -0.0000
  1120        0.0000             nan     0.1700   -0.0000
  1140        0.0000             nan     0.1700   -0.0000
  1160        0.0000             nan     0.1700   -0.0000
  1180        0.0000             nan     0.1700   -0.0000
  1200        0.0000             nan     0.1700   -0.0000
  1220        0.0000             nan     0.1700   -0.0000
  1240        0.0000             nan     0.1700   -0.0000
  1260        0.0000             nan     0.1700   -0.0000
  1280        0.0000             nan     0.1700   -0.0000
  1300        0.0000             nan     0.1700   -0.0000
  1320        0.0000             nan     0.1700   -0.0000
  1340        0.0000             nan     0.1700   -0.0000
  1360        0.0000             nan     0.1700   -0.0000
  1380        0.0000             nan     0.1700   -0.0000
  1400        0.0000             nan     0.1700   -0.0000
  1420        0.0000             nan     0.1700   -0.0000
  1440        0.0000             nan     0.1700   -0.0000
  1460        0.0000             nan     0.1700   -0.0000
  1480        0.0000             nan     0.1700   -0.0000
  1500        0.0000             nan     0.1700   -0.0000
  1520        0.0000             nan     0.1700   -0.0000
  1540        0.0000             nan     0.1700   -0.0000
  1560        0.0000             nan     0.1700   -0.0000
  1580        0.0000             nan     0.1700   -0.0000
  1600        0.0000             nan     0.1700   -0.0000
  1620        0.0000             nan     0.1700   -0.0000
  1640        0.0000             nan     0.1700   -0.0000
  1660        0.0000             nan     0.1700   -0.0000
  1680        0.0000             nan     0.1700   -0.0000
  1700        0.0000             nan     0.1700   -0.0000
  1720        0.0000             nan     0.1700   -0.0000
  1740        0.0000             nan     0.1700   -0.0000
  1760        0.0000             nan     0.1700   -0.0000
  1780        0.0000             nan     0.1700   -0.0000
  1800        0.0000             nan     0.1700   -0.0000
  1820        0.0000             nan     0.1700   -0.0000
  1840        0.0000             nan     0.1700   -0.0000
  1860        0.0000             nan     0.1700   -0.0000
  1880        0.0000             nan     0.1700   -0.0000
  1900        0.0000             nan     0.1700   -0.0000
  1920        0.0000             nan     0.1700   -0.0000
  1940        0.0000             nan     0.1700   -0.0000
  1960        0.0000             nan     0.1700   -0.0000
  1980        0.0000             nan     0.1700   -0.0000
  2000        0.0000             nan     0.1700   -0.0000
  2020        0.0000             nan     0.1700   -0.0000
  2040        0.0000             nan     0.1700   -0.0000
  2060        0.0000             nan     0.1700   -0.0000
  2080        0.0000             nan     0.1700   -0.0000
  2100        0.0000             nan     0.1700   -0.0000
  2120        0.0000             nan     0.1700   -0.0000
  2140        0.0000             nan     0.1700   -0.0000
  2160        0.0000             nan     0.1700   -0.0000
  2180        0.0000             nan     0.1700   -0.0000
  2200        0.0000             nan     0.1700   -0.0000
  2220        0.0000             nan     0.1700   -0.0000
  2240        0.0000             nan     0.1700   -0.0000
  2260        0.0000             nan     0.1700   -0.0000
  2280        0.0000             nan     0.1700   -0.0000
  2300        0.0000             nan     0.1700   -0.0000
  2320        0.0000             nan     0.1700   -0.0000
  2340        0.0000             nan     0.1700   -0.0000
  2360        0.0000             nan     0.1700   -0.0000
  2380        0.0000             nan     0.1700   -0.0000
  2400        0.0000             nan     0.1700   -0.0000
  2420        0.0000             nan     0.1700   -0.0000
  2440        0.0000             nan     0.1700   -0.0000
  2460        0.0000             nan     0.1700   -0.0000
  2480        0.0000             nan     0.1700   -0.0000
  2500        0.0000             nan     0.1700   -0.0000
  2520        0.0000             nan     0.1700   -0.0000
  2540        0.0000             nan     0.1700   -0.0000
  2560        0.0000             nan     0.1700   -0.0000
  2580        0.0000             nan     0.1700   -0.0000
  2600        0.0000             nan     0.1700   -0.0000
  2620        0.0000             nan     0.1700    0.0000
  2640        0.0000             nan     0.1700    0.0000
  2660        0.0000             nan     0.1700   -0.0000
  2680        0.0000             nan     0.1700   -0.0000
  2700        0.0000             nan     0.1700   -0.0000
  2720        0.0000             nan     0.1700   -0.0000
  2740        0.0000             nan     0.1700   -0.0000
  2760        0.0000             nan     0.1700   -0.0000
  2780        0.0000             nan     0.1700   -0.0000
  2800        0.0000             nan     0.1700   -0.0000
  2820        0.0000             nan     0.1700   -0.0000
  2840        0.0000             nan     0.1700   -0.0000
  2860        0.0000             nan     0.1700   -0.0000
  2880        0.0000             nan     0.1700   -0.0000
  2900        0.0000             nan     0.1700   -0.0000
  2920        0.0000             nan     0.1700   -0.0000
  2940        0.0000             nan     0.1700   -0.0000
  2960        0.0000             nan     0.1700   -0.0000
  2980        0.0000             nan     0.1700   -0.0000
  3000        0.0000             nan     0.1700   -0.0000
  3020        0.0000             nan     0.1700   -0.0000
  3040        0.0000             nan     0.1700   -0.0000
  3060        0.0000             nan     0.1700    0.0000
  3080        0.0000             nan     0.1700   -0.0000
  3100        0.0000             nan     0.1700   -0.0000
  3120        0.0000             nan     0.1700   -0.0000
  3140        0.0000             nan     0.1700   -0.0000
  3160        0.0000             nan     0.1700   -0.0000
  3180        0.0000             nan     0.1700   -0.0000
  3200        0.0000             nan     0.1700   -0.0000
  3220        0.0000             nan     0.1700   -0.0000
  3240        0.0000             nan     0.1700   -0.0000
  3260        0.0000             nan     0.1700   -0.0000
  3280        0.0000             nan     0.1700   -0.0000
  3300        0.0000             nan     0.1700   -0.0000
  3320        0.0000             nan     0.1700   -0.0000
  3340        0.0000             nan     0.1700   -0.0000
  3360        0.0000             nan     0.1700   -0.0000
  3380        0.0000             nan     0.1700   -0.0000
  3400        0.0000             nan     0.1700   -0.0000
  3420        0.0000             nan     0.1700   -0.0000
  3440        0.0000             nan     0.1700   -0.0000
  3460        0.0000             nan     0.1700   -0.0000
  3480        0.0000             nan     0.1700   -0.0000
  3500        0.0000             nan     0.1700   -0.0000
  3520        0.0000             nan     0.1700   -0.0000
  3540        0.0000             nan     0.1700   -0.0000
  3560        0.0000             nan     0.1700   -0.0000
  3580        0.0000             nan     0.1700   -0.0000
  3600        0.0000             nan     0.1700   -0.0000
  3620        0.0000             nan     0.1700    0.0000
  3640        0.0000             nan     0.1700   -0.0000
  3660        0.0000             nan     0.1700   -0.0000
  3680        0.0000             nan     0.1700   -0.0000
  3700        0.0000             nan     0.1700   -0.0000
  3720        0.0000             nan     0.1700   -0.0000
  3740        0.0000             nan     0.1700   -0.0000
  3760        0.0000             nan     0.1700   -0.0000
  3780        0.0000             nan     0.1700   -0.0000
  3800        0.0000             nan     0.1700   -0.0000
  3820        0.0000             nan     0.1700    0.0000
  3840        0.0000             nan     0.1700   -0.0000
  3860        0.0000             nan     0.1700   -0.0000
  3880        0.0000             nan     0.1700   -0.0000
  3900        0.0000             nan     0.1700   -0.0000
  3920        0.0000             nan     0.1700   -0.0000
  3940        0.0000             nan     0.1700   -0.0000
  3960        0.0000             nan     0.1700   -0.0000
  3980        0.0000             nan     0.1700   -0.0000
  4000        0.0000             nan     0.1700   -0.0000
  4020        0.0000             nan     0.1700    0.0000
  4040        0.0000             nan     0.1700   -0.0000
  4060        0.0000             nan     0.1700   -0.0000
  4080        0.0000             nan     0.1700    0.0000
  4100        0.0000             nan     0.1700   -0.0000
  4120        0.0000             nan     0.1700   -0.0000
  4140        0.0000             nan     0.1700   -0.0000
  4160        0.0000             nan     0.1700   -0.0000
  4180        0.0000             nan     0.1700   -0.0000
  4200        0.0000             nan     0.1700   -0.0000
  4220        0.0000             nan     0.1700    0.0000
  4240        0.0000             nan     0.1700   -0.0000
  4260        0.0000             nan     0.1700   -0.0000
  4280        0.0000             nan     0.1700   -0.0000
  4300        0.0000             nan     0.1700   -0.0000
  4320        0.0000             nan     0.1700   -0.0000
  4340        0.0000             nan     0.1700   -0.0000
  4360        0.0000             nan     0.1700    0.0000
  4380        0.0000             nan     0.1700    0.0000
  4400        0.0000             nan     0.1700   -0.0000
  4420        0.0000             nan     0.1700   -0.0000
  4440        0.0000             nan     0.1700   -0.0000
  4460        0.0000             nan     0.1700   -0.0000
  4480        0.0000             nan     0.1700   -0.0000
  4500        0.0000             nan     0.1700   -0.0000
  4520        0.0000             nan     0.1700   -0.0000
  4540        0.0000             nan     0.1700   -0.0000
  4554        0.0000             nan     0.1700   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0282             nan     0.1821    0.0132
     2        0.0196             nan     0.1821    0.0084
     3        0.0139             nan     0.1821    0.0059
     4        0.0099             nan     0.1821    0.0038
     5        0.0071             nan     0.1821    0.0028
     6        0.0052             nan     0.1821    0.0019
     7        0.0037             nan     0.1821    0.0013
     8        0.0028             nan     0.1821    0.0010
     9        0.0021             nan     0.1821    0.0006
    10        0.0016             nan     0.1821    0.0005
    20        0.0003             nan     0.1821    0.0000
    40        0.0002             nan     0.1821   -0.0000
    60        0.0001             nan     0.1821   -0.0000
    80        0.0001             nan     0.1821   -0.0000
   100        0.0001             nan     0.1821   -0.0000
   120        0.0001             nan     0.1821   -0.0000
   140        0.0001             nan     0.1821   -0.0000
   160        0.0001             nan     0.1821   -0.0000
   180        0.0001             nan     0.1821   -0.0000
   200        0.0001             nan     0.1821   -0.0000
   220        0.0001             nan     0.1821   -0.0000
   240        0.0000             nan     0.1821   -0.0000
   260        0.0000             nan     0.1821   -0.0000
   280        0.0000             nan     0.1821   -0.0000
   300        0.0000             nan     0.1821   -0.0000
   320        0.0000             nan     0.1821   -0.0000
   340        0.0000             nan     0.1821   -0.0000
   360        0.0000             nan     0.1821   -0.0000
   380        0.0000             nan     0.1821   -0.0000
   400        0.0000             nan     0.1821   -0.0000
   420        0.0000             nan     0.1821   -0.0000
   440        0.0000             nan     0.1821   -0.0000
   460        0.0000             nan     0.1821   -0.0000
   480        0.0000             nan     0.1821   -0.0000
   500        0.0000             nan     0.1821   -0.0000
   520        0.0000             nan     0.1821   -0.0000
   540        0.0000             nan     0.1821   -0.0000
   560        0.0000             nan     0.1821   -0.0000
   580        0.0000             nan     0.1821   -0.0000
   600        0.0000             nan     0.1821   -0.0000
   620        0.0000             nan     0.1821   -0.0000
   640        0.0000             nan     0.1821   -0.0000
   660        0.0000             nan     0.1821   -0.0000
   680        0.0000             nan     0.1821   -0.0000
   700        0.0000             nan     0.1821   -0.0000
   720        0.0000             nan     0.1821   -0.0000
   740        0.0000             nan     0.1821   -0.0000
   760        0.0000             nan     0.1821   -0.0000
   780        0.0000             nan     0.1821   -0.0000
   800        0.0000             nan     0.1821   -0.0000
   820        0.0000             nan     0.1821   -0.0000
   840        0.0000             nan     0.1821   -0.0000
   860        0.0000             nan     0.1821   -0.0000
   880        0.0000             nan     0.1821   -0.0000
   900        0.0000             nan     0.1821   -0.0000
   920        0.0000             nan     0.1821   -0.0000
   940        0.0000             nan     0.1821   -0.0000
   960        0.0000             nan     0.1821   -0.0000
   980        0.0000             nan     0.1821   -0.0000
  1000        0.0000             nan     0.1821   -0.0000
  1020        0.0000             nan     0.1821   -0.0000
  1040        0.0000             nan     0.1821   -0.0000
  1060        0.0000             nan     0.1821   -0.0000
  1080        0.0000             nan     0.1821   -0.0000
  1100        0.0000             nan     0.1821   -0.0000
  1120        0.0000             nan     0.1821   -0.0000
  1140        0.0000             nan     0.1821   -0.0000
  1160        0.0000             nan     0.1821   -0.0000
  1180        0.0000             nan     0.1821   -0.0000
  1200        0.0000             nan     0.1821   -0.0000
  1220        0.0000             nan     0.1821   -0.0000
  1240        0.0000             nan     0.1821   -0.0000
  1260        0.0000             nan     0.1821   -0.0000
  1280        0.0000             nan     0.1821   -0.0000
  1300        0.0000             nan     0.1821   -0.0000
  1320        0.0000             nan     0.1821   -0.0000
  1340        0.0000             nan     0.1821   -0.0000
  1360        0.0000             nan     0.1821   -0.0000
  1380        0.0000             nan     0.1821   -0.0000
  1400        0.0000             nan     0.1821   -0.0000
  1420        0.0000             nan     0.1821   -0.0000
  1440        0.0000             nan     0.1821   -0.0000
  1460        0.0000             nan     0.1821   -0.0000
  1480        0.0000             nan     0.1821   -0.0000
  1500        0.0000             nan     0.1821   -0.0000
  1520        0.0000             nan     0.1821   -0.0000
  1540        0.0000             nan     0.1821   -0.0000
  1560        0.0000             nan     0.1821   -0.0000
  1580        0.0000             nan     0.1821   -0.0000
  1600        0.0000             nan     0.1821   -0.0000
  1620        0.0000             nan     0.1821   -0.0000
  1640        0.0000             nan     0.1821   -0.0000
  1660        0.0000             nan     0.1821   -0.0000
  1680        0.0000             nan     0.1821   -0.0000
  1700        0.0000             nan     0.1821    0.0000
  1720        0.0000             nan     0.1821   -0.0000
  1740        0.0000             nan     0.1821   -0.0000
  1760        0.0000             nan     0.1821   -0.0000
  1780        0.0000             nan     0.1821   -0.0000
  1800        0.0000             nan     0.1821   -0.0000
  1820        0.0000             nan     0.1821   -0.0000
  1840        0.0000             nan     0.1821   -0.0000
  1860        0.0000             nan     0.1821   -0.0000
  1880        0.0000             nan     0.1821   -0.0000
  1900        0.0000             nan     0.1821   -0.0000
  1920        0.0000             nan     0.1821   -0.0000
  1940        0.0000             nan     0.1821   -0.0000
  1960        0.0000             nan     0.1821   -0.0000
  1980        0.0000             nan     0.1821   -0.0000
  2000        0.0000             nan     0.1821   -0.0000
  2020        0.0000             nan     0.1821   -0.0000
  2040        0.0000             nan     0.1821   -0.0000
  2060        0.0000             nan     0.1821   -0.0000
  2080        0.0000             nan     0.1821   -0.0000
  2100        0.0000             nan     0.1821   -0.0000
  2120        0.0000             nan     0.1821   -0.0000
  2140        0.0000             nan     0.1821   -0.0000
  2160        0.0000             nan     0.1821   -0.0000
  2180        0.0000             nan     0.1821   -0.0000
  2200        0.0000             nan     0.1821   -0.0000
  2220        0.0000             nan     0.1821   -0.0000
  2240        0.0000             nan     0.1821   -0.0000
  2260        0.0000             nan     0.1821   -0.0000
  2280        0.0000             nan     0.1821   -0.0000
  2300        0.0000             nan     0.1821   -0.0000
  2320        0.0000             nan     0.1821   -0.0000
  2340        0.0000             nan     0.1821   -0.0000
  2360        0.0000             nan     0.1821   -0.0000
  2380        0.0000             nan     0.1821   -0.0000
  2400        0.0000             nan     0.1821   -0.0000
  2420        0.0000             nan     0.1821   -0.0000
  2440        0.0000             nan     0.1821   -0.0000
  2460        0.0000             nan     0.1821   -0.0000
  2480        0.0000             nan     0.1821   -0.0000
  2500        0.0000             nan     0.1821   -0.0000
  2520        0.0000             nan     0.1821   -0.0000
  2540        0.0000             nan     0.1821    0.0000
  2560        0.0000             nan     0.1821   -0.0000
  2580        0.0000             nan     0.1821    0.0000
  2600        0.0000             nan     0.1821   -0.0000
  2620        0.0000             nan     0.1821    0.0000
  2640        0.0000             nan     0.1821   -0.0000
  2660        0.0000             nan     0.1821   -0.0000
  2680        0.0000             nan     0.1821   -0.0000
  2700        0.0000             nan     0.1821   -0.0000
  2720        0.0000             nan     0.1821   -0.0000
  2740        0.0000             nan     0.1821   -0.0000
  2760        0.0000             nan     0.1821   -0.0000
  2780        0.0000             nan     0.1821    0.0000
  2800        0.0000             nan     0.1821    0.0000
  2820        0.0000             nan     0.1821   -0.0000
  2840        0.0000             nan     0.1821   -0.0000
  2860        0.0000             nan     0.1821   -0.0000
  2880        0.0000             nan     0.1821   -0.0000
  2900        0.0000             nan     0.1821   -0.0000
  2920        0.0000             nan     0.1821   -0.0000
  2940        0.0000             nan     0.1821    0.0000
  2960        0.0000             nan     0.1821   -0.0000
  2980        0.0000             nan     0.1821   -0.0000
  3000        0.0000             nan     0.1821    0.0000
  3020        0.0000             nan     0.1821   -0.0000
  3040        0.0000             nan     0.1821   -0.0000
  3060        0.0000             nan     0.1821   -0.0000
  3080        0.0000             nan     0.1821   -0.0000
  3100        0.0000             nan     0.1821   -0.0000
  3120        0.0000             nan     0.1821   -0.0000
  3140        0.0000             nan     0.1821    0.0000
  3160        0.0000             nan     0.1821   -0.0000
  3180        0.0000             nan     0.1821   -0.0000
  3200        0.0000             nan     0.1821   -0.0000
  3220        0.0000             nan     0.1821   -0.0000
  3240        0.0000             nan     0.1821   -0.0000
  3260        0.0000             nan     0.1821   -0.0000
  3266        0.0000             nan     0.1821   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0271             nan     0.2033    0.0126
     2        0.0187             nan     0.2033    0.0085
     3        0.0130             nan     0.2033    0.0064
     4        0.0091             nan     0.2033    0.0042
     5        0.0065             nan     0.2033    0.0027
     6        0.0048             nan     0.2033    0.0019
     7        0.0036             nan     0.2033    0.0013
     8        0.0027             nan     0.2033    0.0008
     9        0.0020             nan     0.2033    0.0006
    10        0.0016             nan     0.2033    0.0004
    20        0.0005             nan     0.2033    0.0000
    40        0.0003             nan     0.2033   -0.0000
    60        0.0003             nan     0.2033   -0.0000
    80        0.0002             nan     0.2033   -0.0000
   100        0.0002             nan     0.2033   -0.0000
   120        0.0002             nan     0.2033   -0.0000
   140        0.0002             nan     0.2033   -0.0000
   160        0.0001             nan     0.2033   -0.0000
   180        0.0001             nan     0.2033   -0.0000
   200        0.0001             nan     0.2033   -0.0000
   220        0.0001             nan     0.2033   -0.0000
   240        0.0001             nan     0.2033   -0.0000
   260        0.0001             nan     0.2033   -0.0000
   280        0.0001             nan     0.2033   -0.0000
   300        0.0001             nan     0.2033   -0.0000
   320        0.0001             nan     0.2033   -0.0000
   340        0.0001             nan     0.2033   -0.0000
   360        0.0001             nan     0.2033   -0.0000
   380        0.0001             nan     0.2033   -0.0000
   400        0.0001             nan     0.2033   -0.0000
   420        0.0001             nan     0.2033   -0.0000
   440        0.0001             nan     0.2033   -0.0000
   460        0.0001             nan     0.2033   -0.0000
   480        0.0001             nan     0.2033   -0.0000
   500        0.0001             nan     0.2033   -0.0000
   520        0.0001             nan     0.2033   -0.0000
   540        0.0001             nan     0.2033   -0.0000
   560        0.0001             nan     0.2033   -0.0000
   580        0.0001             nan     0.2033   -0.0000
   600        0.0001             nan     0.2033   -0.0000
   620        0.0000             nan     0.2033   -0.0000
   640        0.0000             nan     0.2033   -0.0000
   660        0.0000             nan     0.2033   -0.0000
   680        0.0000             nan     0.2033   -0.0000
   700        0.0000             nan     0.2033   -0.0000
   720        0.0000             nan     0.2033   -0.0000
   740        0.0000             nan     0.2033   -0.0000
   760        0.0000             nan     0.2033   -0.0000
   780        0.0000             nan     0.2033   -0.0000
   800        0.0000             nan     0.2033   -0.0000
   820        0.0000             nan     0.2033   -0.0000
   840        0.0000             nan     0.2033   -0.0000
   860        0.0000             nan     0.2033   -0.0000
   880        0.0000             nan     0.2033   -0.0000
   900        0.0000             nan     0.2033   -0.0000
   920        0.0000             nan     0.2033   -0.0000
   940        0.0000             nan     0.2033   -0.0000
   960        0.0000             nan     0.2033   -0.0000
   980        0.0000             nan     0.2033   -0.0000
  1000        0.0000             nan     0.2033   -0.0000
  1020        0.0000             nan     0.2033   -0.0000
  1040        0.0000             nan     0.2033   -0.0000
  1060        0.0000             nan     0.2033   -0.0000
  1080        0.0000             nan     0.2033   -0.0000
  1100        0.0000             nan     0.2033   -0.0000
  1120        0.0000             nan     0.2033   -0.0000
  1140        0.0000             nan     0.2033   -0.0000
  1160        0.0000             nan     0.2033   -0.0000
  1180        0.0000             nan     0.2033   -0.0000
  1200        0.0000             nan     0.2033   -0.0000
  1220        0.0000             nan     0.2033   -0.0000
  1240        0.0000             nan     0.2033   -0.0000
  1260        0.0000             nan     0.2033   -0.0000
  1280        0.0000             nan     0.2033   -0.0000
  1300        0.0000             nan     0.2033   -0.0000
  1320        0.0000             nan     0.2033   -0.0000
  1340        0.0000             nan     0.2033   -0.0000
  1360        0.0000             nan     0.2033   -0.0000
  1380        0.0000             nan     0.2033   -0.0000
  1400        0.0000             nan     0.2033   -0.0000
  1420        0.0000             nan     0.2033   -0.0000
  1440        0.0000             nan     0.2033   -0.0000
  1460        0.0000             nan     0.2033   -0.0000
  1480        0.0000             nan     0.2033   -0.0000
  1500        0.0000             nan     0.2033   -0.0000
  1520        0.0000             nan     0.2033   -0.0000
  1540        0.0000             nan     0.2033   -0.0000
  1560        0.0000             nan     0.2033   -0.0000
  1580        0.0000             nan     0.2033   -0.0000
  1600        0.0000             nan     0.2033   -0.0000
  1620        0.0000             nan     0.2033   -0.0000
  1640        0.0000             nan     0.2033   -0.0000
  1660        0.0000             nan     0.2033   -0.0000
  1680        0.0000             nan     0.2033   -0.0000
  1700        0.0000             nan     0.2033   -0.0000
  1720        0.0000             nan     0.2033   -0.0000
  1740        0.0000             nan     0.2033   -0.0000
  1760        0.0000             nan     0.2033   -0.0000
  1780        0.0000             nan     0.2033   -0.0000
  1800        0.0000             nan     0.2033   -0.0000
  1820        0.0000             nan     0.2033   -0.0000
  1840        0.0000             nan     0.2033   -0.0000
  1860        0.0000             nan     0.2033   -0.0000
  1880        0.0000             nan     0.2033   -0.0000
  1900        0.0000             nan     0.2033   -0.0000
  1905        0.0000             nan     0.2033   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0239             nan     0.2560    0.0151
     2        0.0149             nan     0.2560    0.0084
     3        0.0092             nan     0.2560    0.0057
     4        0.0059             nan     0.2560    0.0034
     5        0.0039             nan     0.2560    0.0019
     6        0.0027             nan     0.2560    0.0014
     7        0.0018             nan     0.2560    0.0008
     8        0.0013             nan     0.2560    0.0005
     9        0.0009             nan     0.2560    0.0003
    10        0.0007             nan     0.2560    0.0002
    20        0.0003             nan     0.2560   -0.0000
    40        0.0001             nan     0.2560   -0.0000
    60        0.0001             nan     0.2560   -0.0000
    80        0.0001             nan     0.2560   -0.0000
   100        0.0001             nan     0.2560   -0.0000
   120        0.0001             nan     0.2560   -0.0000
   140        0.0000             nan     0.2560   -0.0000
   160        0.0000             nan     0.2560   -0.0000
   180        0.0000             nan     0.2560   -0.0000
   200        0.0000             nan     0.2560   -0.0000
   220        0.0000             nan     0.2560   -0.0000
   240        0.0000             nan     0.2560   -0.0000
   260        0.0000             nan     0.2560   -0.0000
   280        0.0000             nan     0.2560   -0.0000
   300        0.0000             nan     0.2560   -0.0000
   320        0.0000             nan     0.2560   -0.0000
   340        0.0000             nan     0.2560   -0.0000
   360        0.0000             nan     0.2560   -0.0000
   380        0.0000             nan     0.2560   -0.0000
   400        0.0000             nan     0.2560   -0.0000
   420        0.0000             nan     0.2560   -0.0000
   440        0.0000             nan     0.2560   -0.0000
   460        0.0000             nan     0.2560   -0.0000
   480        0.0000             nan     0.2560   -0.0000
   500        0.0000             nan     0.2560   -0.0000
   520        0.0000             nan     0.2560   -0.0000
   540        0.0000             nan     0.2560   -0.0000
   560        0.0000             nan     0.2560   -0.0000
   580        0.0000             nan     0.2560   -0.0000
   600        0.0000             nan     0.2560   -0.0000
   620        0.0000             nan     0.2560   -0.0000
   640        0.0000             nan     0.2560   -0.0000
   660        0.0000             nan     0.2560   -0.0000
   680        0.0000             nan     0.2560   -0.0000
   700        0.0000             nan     0.2560   -0.0000
   720        0.0000             nan     0.2560   -0.0000
   740        0.0000             nan     0.2560   -0.0000
   760        0.0000             nan     0.2560   -0.0000
   780        0.0000             nan     0.2560   -0.0000
   800        0.0000             nan     0.2560   -0.0000
   820        0.0000             nan     0.2560   -0.0000
   840        0.0000             nan     0.2560   -0.0000
   860        0.0000             nan     0.2560   -0.0000
   880        0.0000             nan     0.2560   -0.0000
   900        0.0000             nan     0.2560   -0.0000
   920        0.0000             nan     0.2560   -0.0000
   940        0.0000             nan     0.2560   -0.0000
   960        0.0000             nan     0.2560   -0.0000
   980        0.0000             nan     0.2560   -0.0000
  1000        0.0000             nan     0.2560   -0.0000
  1020        0.0000             nan     0.2560   -0.0000
  1040        0.0000             nan     0.2560   -0.0000
  1060        0.0000             nan     0.2560   -0.0000
  1080        0.0000             nan     0.2560   -0.0000
  1100        0.0000             nan     0.2560   -0.0000
  1120        0.0000             nan     0.2560   -0.0000
  1140        0.0000             nan     0.2560   -0.0000
  1160        0.0000             nan     0.2560   -0.0000
  1180        0.0000             nan     0.2560   -0.0000
  1200        0.0000             nan     0.2560   -0.0000
  1220        0.0000             nan     0.2560   -0.0000
  1240        0.0000             nan     0.2560   -0.0000
  1260        0.0000             nan     0.2560   -0.0000
  1280        0.0000             nan     0.2560   -0.0000
  1300        0.0000             nan     0.2560   -0.0000
  1320        0.0000             nan     0.2560   -0.0000
  1340        0.0000             nan     0.2560   -0.0000
  1360        0.0000             nan     0.2560   -0.0000
  1380        0.0000             nan     0.2560   -0.0000
  1400        0.0000             nan     0.2560   -0.0000
  1420        0.0000             nan     0.2560   -0.0000
  1440        0.0000             nan     0.2560   -0.0000
  1460        0.0000             nan     0.2560   -0.0000
  1480        0.0000             nan     0.2560   -0.0000
  1500        0.0000             nan     0.2560   -0.0000
  1520        0.0000             nan     0.2560   -0.0000
  1540        0.0000             nan     0.2560   -0.0000
  1560        0.0000             nan     0.2560   -0.0000
  1580        0.0000             nan     0.2560   -0.0000
  1600        0.0000             nan     0.2560   -0.0000
  1620        0.0000             nan     0.2560   -0.0000
  1640        0.0000             nan     0.2560   -0.0000
  1660        0.0000             nan     0.2560   -0.0000
  1680        0.0000             nan     0.2560   -0.0000
  1700        0.0000             nan     0.2560   -0.0000
  1720        0.0000             nan     0.2560    0.0000
  1740        0.0000             nan     0.2560   -0.0000
  1760        0.0000             nan     0.2560   -0.0000
  1780        0.0000             nan     0.2560   -0.0000
  1800        0.0000             nan     0.2560   -0.0000
  1820        0.0000             nan     0.2560   -0.0000
  1840        0.0000             nan     0.2560   -0.0000
  1860        0.0000             nan     0.2560   -0.0000
  1880        0.0000             nan     0.2560   -0.0000
  1900        0.0000             nan     0.2560   -0.0000
  1920        0.0000             nan     0.2560   -0.0000
  1940        0.0000             nan     0.2560   -0.0000
  1960        0.0000             nan     0.2560   -0.0000
  1980        0.0000             nan     0.2560   -0.0000
  2000        0.0000             nan     0.2560   -0.0000
  2020        0.0000             nan     0.2560   -0.0000
  2040        0.0000             nan     0.2560    0.0000
  2060        0.0000             nan     0.2560   -0.0000
  2080        0.0000             nan     0.2560   -0.0000
  2100        0.0000             nan     0.2560   -0.0000
  2120        0.0000             nan     0.2560    0.0000
  2140        0.0000             nan     0.2560   -0.0000
  2160        0.0000             nan     0.2560    0.0000
  2180        0.0000             nan     0.2560   -0.0000
  2200        0.0000             nan     0.2560    0.0000
  2220        0.0000             nan     0.2560   -0.0000
  2240        0.0000             nan     0.2560   -0.0000
  2260        0.0000             nan     0.2560   -0.0000
  2280        0.0000             nan     0.2560   -0.0000
  2300        0.0000             nan     0.2560   -0.0000
  2320        0.0000             nan     0.2560   -0.0000
  2340        0.0000             nan     0.2560    0.0000
  2360        0.0000             nan     0.2560   -0.0000
  2380        0.0000             nan     0.2560   -0.0000
  2400        0.0000             nan     0.2560   -0.0000
  2420        0.0000             nan     0.2560    0.0000
  2440        0.0000             nan     0.2560   -0.0000
  2460        0.0000             nan     0.2560   -0.0000
  2480        0.0000             nan     0.2560   -0.0000
  2500        0.0000             nan     0.2560   -0.0000
  2520        0.0000             nan     0.2560   -0.0000
  2540        0.0000             nan     0.2560   -0.0000
  2560        0.0000             nan     0.2560   -0.0000
  2580        0.0000             nan     0.2560    0.0000
  2600        0.0000             nan     0.2560    0.0000
  2620        0.0000             nan     0.2560   -0.0000
  2640        0.0000             nan     0.2560   -0.0000
  2660        0.0000             nan     0.2560   -0.0000
  2680        0.0000             nan     0.2560   -0.0000
  2700        0.0000             nan     0.2560   -0.0000
  2720        0.0000             nan     0.2560   -0.0000
  2740        0.0000             nan     0.2560   -0.0000
  2760        0.0000             nan     0.2560   -0.0000
  2780        0.0000             nan     0.2560   -0.0000
  2800        0.0000             nan     0.2560   -0.0000
  2820        0.0000             nan     0.2560    0.0000
  2840        0.0000             nan     0.2560   -0.0000
  2860        0.0000             nan     0.2560   -0.0000
  2880        0.0000             nan     0.2560   -0.0000
  2900        0.0000             nan     0.2560   -0.0000
  2920        0.0000             nan     0.2560   -0.0000
  2940        0.0000             nan     0.2560   -0.0000
  2960        0.0000             nan     0.2560    0.0000
  2980        0.0000             nan     0.2560    0.0000
  3000        0.0000             nan     0.2560   -0.0000
  3020        0.0000             nan     0.2560    0.0000
  3040        0.0000             nan     0.2560   -0.0000
  3060        0.0000             nan     0.2560   -0.0000
  3080        0.0000             nan     0.2560   -0.0000
  3100        0.0000             nan     0.2560    0.0000
  3120        0.0000             nan     0.2560   -0.0000
  3140        0.0000             nan     0.2560    0.0000
  3160        0.0000             nan     0.2560   -0.0000
  3180        0.0000             nan     0.2560   -0.0000
  3200        0.0000             nan     0.2560   -0.0000
  3220        0.0000             nan     0.2560    0.0000
  3240        0.0000             nan     0.2560   -0.0000
  3260        0.0000             nan     0.2560    0.0000
  3280        0.0000             nan     0.2560   -0.0000
  3300        0.0000             nan     0.2560   -0.0000
  3320        0.0000             nan     0.2560   -0.0000
  3340        0.0000             nan     0.2560   -0.0000
  3360        0.0000             nan     0.2560   -0.0000
  3380        0.0000             nan     0.2560   -0.0000
  3400        0.0000             nan     0.2560   -0.0000
  3420        0.0000             nan     0.2560   -0.0000
  3440        0.0000             nan     0.2560   -0.0000
  3460        0.0000             nan     0.2560   -0.0000
  3480        0.0000             nan     0.2560   -0.0000
  3500        0.0000             nan     0.2560   -0.0000
  3520        0.0000             nan     0.2560   -0.0000
  3540        0.0000             nan     0.2560   -0.0000
  3560        0.0000             nan     0.2560    0.0000
  3580        0.0000             nan     0.2560    0.0000
  3600        0.0000             nan     0.2560   -0.0000
  3620        0.0000             nan     0.2560    0.0000
  3640        0.0000             nan     0.2560    0.0000
  3660        0.0000             nan     0.2560   -0.0000
  3680        0.0000             nan     0.2560    0.0000
  3700        0.0000             nan     0.2560   -0.0000
  3720        0.0000             nan     0.2560   -0.0000
  3740        0.0000             nan     0.2560   -0.0000
  3760        0.0000             nan     0.2560   -0.0000
  3780        0.0000             nan     0.2560   -0.0000
  3800        0.0000             nan     0.2560    0.0000
  3820        0.0000             nan     0.2560   -0.0000
  3840        0.0000             nan     0.2560   -0.0000
  3860        0.0000             nan     0.2560   -0.0000
  3880        0.0000             nan     0.2560    0.0000
  3900        0.0000             nan     0.2560    0.0000
  3920        0.0000             nan     0.2560    0.0000
  3940        0.0000             nan     0.2560   -0.0000
  3960        0.0000             nan     0.2560   -0.0000
  3980        0.0000             nan     0.2560   -0.0000
  4000        0.0000             nan     0.2560    0.0000
  4020        0.0000             nan     0.2560    0.0000
  4040        0.0000             nan     0.2560   -0.0000
  4060        0.0000             nan     0.2560    0.0000
  4062        0.0000             nan     0.2560   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0196             nan     0.3360    0.0213
     2        0.0099             nan     0.3360    0.0095
     3        0.0053             nan     0.3360    0.0047
     4        0.0029             nan     0.3360    0.0023
     5        0.0017             nan     0.3360    0.0011
     6        0.0012             nan     0.3360    0.0006
     7        0.0008             nan     0.3360    0.0002
     8        0.0007             nan     0.3360    0.0001
     9        0.0006             nan     0.3360    0.0001
    10        0.0005             nan     0.3360    0.0000
    20        0.0003             nan     0.3360    0.0000
    40        0.0002             nan     0.3360   -0.0000
    60        0.0001             nan     0.3360   -0.0000
    80        0.0001             nan     0.3360   -0.0000
   100        0.0001             nan     0.3360   -0.0000
   120        0.0001             nan     0.3360   -0.0000
   140        0.0001             nan     0.3360   -0.0000
   160        0.0001             nan     0.3360   -0.0000
   180        0.0001             nan     0.3360   -0.0000
   200        0.0001             nan     0.3360   -0.0000
   220        0.0001             nan     0.3360   -0.0000
   240        0.0000             nan     0.3360   -0.0000
   260        0.0000             nan     0.3360   -0.0000
   280        0.0000             nan     0.3360   -0.0000
   300        0.0000             nan     0.3360   -0.0000
   320        0.0000             nan     0.3360   -0.0000
   340        0.0000             nan     0.3360   -0.0000
   360        0.0000             nan     0.3360   -0.0000
   380        0.0000             nan     0.3360   -0.0000
   400        0.0000             nan     0.3360   -0.0000
   420        0.0000             nan     0.3360   -0.0000
   440        0.0000             nan     0.3360   -0.0000
   460        0.0000             nan     0.3360   -0.0000
   480        0.0000             nan     0.3360   -0.0000
   500        0.0000             nan     0.3360   -0.0000
   520        0.0000             nan     0.3360   -0.0000
   540        0.0000             nan     0.3360   -0.0000
   560        0.0000             nan     0.3360   -0.0000
   580        0.0000             nan     0.3360   -0.0000
   600        0.0000             nan     0.3360   -0.0000
   620        0.0000             nan     0.3360   -0.0000
   640        0.0000             nan     0.3360   -0.0000
   660        0.0000             nan     0.3360   -0.0000
   680        0.0000             nan     0.3360   -0.0000
   700        0.0000             nan     0.3360    0.0000
   720        0.0000             nan     0.3360   -0.0000
   740        0.0000             nan     0.3360   -0.0000
   760        0.0000             nan     0.3360   -0.0000
   780        0.0000             nan     0.3360    0.0000
   800        0.0000             nan     0.3360   -0.0000
   820        0.0000             nan     0.3360   -0.0000
   840        0.0000             nan     0.3360   -0.0000
   860        0.0000             nan     0.3360   -0.0000
   880        0.0000             nan     0.3360   -0.0000
   900        0.0000             nan     0.3360   -0.0000
   920        0.0000             nan     0.3360   -0.0000
   940        0.0000             nan     0.3360   -0.0000
   960        0.0000             nan     0.3360   -0.0000
   980        0.0000             nan     0.3360   -0.0000
  1000        0.0000             nan     0.3360   -0.0000
  1020        0.0000             nan     0.3360   -0.0000
  1040        0.0000             nan     0.3360    0.0000
  1060        0.0000             nan     0.3360    0.0000
  1080        0.0000             nan     0.3360   -0.0000
  1100        0.0000             nan     0.3360   -0.0000
  1120        0.0000             nan     0.3360   -0.0000
  1140        0.0000             nan     0.3360   -0.0000
  1160        0.0000             nan     0.3360   -0.0000
  1180        0.0000             nan     0.3360   -0.0000
  1200        0.0000             nan     0.3360   -0.0000
  1220        0.0000             nan     0.3360   -0.0000
  1240        0.0000             nan     0.3360   -0.0000
  1260        0.0000             nan     0.3360   -0.0000
  1280        0.0000             nan     0.3360   -0.0000
  1300        0.0000             nan     0.3360   -0.0000
  1320        0.0000             nan     0.3360   -0.0000
  1340        0.0000             nan     0.3360   -0.0000
  1360        0.0000             nan     0.3360   -0.0000
  1380        0.0000             nan     0.3360    0.0000
  1400        0.0000             nan     0.3360   -0.0000
  1420        0.0000             nan     0.3360   -0.0000
  1440        0.0000             nan     0.3360   -0.0000
  1460        0.0000             nan     0.3360   -0.0000
  1480        0.0000             nan     0.3360   -0.0000
  1500        0.0000             nan     0.3360   -0.0000
  1520        0.0000             nan     0.3360   -0.0000
  1540        0.0000             nan     0.3360   -0.0000
  1560        0.0000             nan     0.3360    0.0000
  1580        0.0000             nan     0.3360   -0.0000
  1600        0.0000             nan     0.3360   -0.0000
  1620        0.0000             nan     0.3360   -0.0000
  1640        0.0000             nan     0.3360   -0.0000
  1660        0.0000             nan     0.3360   -0.0000
  1680        0.0000             nan     0.3360   -0.0000
  1700        0.0000             nan     0.3360   -0.0000
  1720        0.0000             nan     0.3360   -0.0000
  1740        0.0000             nan     0.3360   -0.0000
  1760        0.0000             nan     0.3360    0.0000
  1780        0.0000             nan     0.3360   -0.0000
  1800        0.0000             nan     0.3360   -0.0000
  1820        0.0000             nan     0.3360   -0.0000
  1840        0.0000             nan     0.3360   -0.0000
  1860        0.0000             nan     0.3360   -0.0000
  1880        0.0000             nan     0.3360    0.0000
  1900        0.0000             nan     0.3360   -0.0000
  1920        0.0000             nan     0.3360    0.0000
  1940        0.0000             nan     0.3360   -0.0000
  1960        0.0000             nan     0.3360    0.0000
  1980        0.0000             nan     0.3360   -0.0000
  2000        0.0000             nan     0.3360    0.0000
  2020        0.0000             nan     0.3360   -0.0000
  2040        0.0000             nan     0.3360   -0.0000
  2060        0.0000             nan     0.3360    0.0000
  2080        0.0000             nan     0.3360   -0.0000
  2100        0.0000             nan     0.3360    0.0000
  2120        0.0000             nan     0.3360   -0.0000
  2140        0.0000             nan     0.3360   -0.0000
  2160        0.0000             nan     0.3360   -0.0000
  2180        0.0000             nan     0.3360   -0.0000
  2200        0.0000             nan     0.3360   -0.0000
  2220        0.0000             nan     0.3360    0.0000
  2240        0.0000             nan     0.3360   -0.0000
  2260        0.0000             nan     0.3360   -0.0000
  2280        0.0000             nan     0.3360   -0.0000
  2300        0.0000             nan     0.3360   -0.0000
  2320        0.0000             nan     0.3360    0.0000
  2340        0.0000             nan     0.3360   -0.0000
  2360        0.0000             nan     0.3360   -0.0000
  2380        0.0000             nan     0.3360   -0.0000
  2400        0.0000             nan     0.3360   -0.0000
  2420        0.0000             nan     0.3360   -0.0000
  2440        0.0000             nan     0.3360   -0.0000
  2460        0.0000             nan     0.3360    0.0000
  2480        0.0000             nan     0.3360    0.0000
  2500        0.0000             nan     0.3360   -0.0000
  2520        0.0000             nan     0.3360   -0.0000
  2540        0.0000             nan     0.3360   -0.0000
  2560        0.0000             nan     0.3360    0.0000
  2580        0.0000             nan     0.3360    0.0000
  2600        0.0000             nan     0.3360   -0.0000
  2620        0.0000             nan     0.3360    0.0000
  2640        0.0000             nan     0.3360   -0.0000
  2660        0.0000             nan     0.3360   -0.0000
  2680        0.0000             nan     0.3360   -0.0000
  2700        0.0000             nan     0.3360   -0.0000
  2720        0.0000             nan     0.3360   -0.0000
  2740        0.0000             nan     0.3360   -0.0000
  2760        0.0000             nan     0.3360   -0.0000
  2780        0.0000             nan     0.3360   -0.0000
  2800        0.0000             nan     0.3360   -0.0000
  2820        0.0000             nan     0.3360   -0.0000
  2840        0.0000             nan     0.3360    0.0000
  2860        0.0000             nan     0.3360   -0.0000
  2875        0.0000             nan     0.3360   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0150             nan     0.4093    0.0239
     2        0.0058             nan     0.4093    0.0099
     3        0.0024             nan     0.4093    0.0033
     4        0.0010             nan     0.4093    0.0012
     5        0.0005             nan     0.4093    0.0005
     6        0.0003             nan     0.4093    0.0002
     7        0.0002             nan     0.4093    0.0001
     8        0.0002             nan     0.4093    0.0000
     9        0.0001             nan     0.4093    0.0000
    10        0.0001             nan     0.4093    0.0000
    20        0.0001             nan     0.4093   -0.0000
    40        0.0000             nan     0.4093   -0.0000
    60        0.0000             nan     0.4093   -0.0000
    80        0.0000             nan     0.4093   -0.0000
   100        0.0000             nan     0.4093   -0.0000
   120        0.0000             nan     0.4093   -0.0000
   140        0.0000             nan     0.4093   -0.0000
   160        0.0000             nan     0.4093   -0.0000
   180        0.0000             nan     0.4093   -0.0000
   200        0.0000             nan     0.4093   -0.0000
   220        0.0000             nan     0.4093   -0.0000
   240        0.0000             nan     0.4093   -0.0000
   260        0.0000             nan     0.4093   -0.0000
   280        0.0000             nan     0.4093   -0.0000
   300        0.0000             nan     0.4093   -0.0000
   320        0.0000             nan     0.4093   -0.0000
   340        0.0000             nan     0.4093   -0.0000
   360        0.0000             nan     0.4093   -0.0000
   380        0.0000             nan     0.4093   -0.0000
   400        0.0000             nan     0.4093   -0.0000
   420        0.0000             nan     0.4093   -0.0000
   440        0.0000             nan     0.4093   -0.0000
   460        0.0000             nan     0.4093   -0.0000
   480        0.0000             nan     0.4093   -0.0000
   500        0.0000             nan     0.4093   -0.0000
   520        0.0000             nan     0.4093    0.0000
   540        0.0000             nan     0.4093   -0.0000
   560        0.0000             nan     0.4093   -0.0000
   580        0.0000             nan     0.4093   -0.0000
   600        0.0000             nan     0.4093   -0.0000
   620        0.0000             nan     0.4093   -0.0000
   640        0.0000             nan     0.4093   -0.0000
   660        0.0000             nan     0.4093   -0.0000
   680        0.0000             nan     0.4093   -0.0000
   700        0.0000             nan     0.4093   -0.0000
   720        0.0000             nan     0.4093   -0.0000
   740        0.0000             nan     0.4093    0.0000
   760        0.0000             nan     0.4093    0.0000
   780        0.0000             nan     0.4093    0.0000
   800        0.0000             nan     0.4093   -0.0000
   820        0.0000             nan     0.4093   -0.0000
   840        0.0000             nan     0.4093   -0.0000
   860        0.0000             nan     0.4093   -0.0000
   880        0.0000             nan     0.4093   -0.0000
   900        0.0000             nan     0.4093    0.0000
   920        0.0000             nan     0.4093    0.0000
   940        0.0000             nan     0.4093    0.0000
   960        0.0000             nan     0.4093   -0.0000
   980        0.0000             nan     0.4093   -0.0000
  1000        0.0000             nan     0.4093   -0.0000
  1020        0.0000             nan     0.4093   -0.0000
  1040        0.0000             nan     0.4093   -0.0000
  1060        0.0000             nan     0.4093   -0.0000
  1080        0.0000             nan     0.4093   -0.0000
  1100        0.0000             nan     0.4093    0.0000
  1120        0.0000             nan     0.4093    0.0000
  1140        0.0000             nan     0.4093    0.0000
  1160        0.0000             nan     0.4093    0.0000
  1180        0.0000             nan     0.4093   -0.0000
  1200        0.0000             nan     0.4093    0.0000
  1220        0.0000             nan     0.4093   -0.0000
  1240        0.0000             nan     0.4093   -0.0000
  1260        0.0000             nan     0.4093   -0.0000
  1280        0.0000             nan     0.4093   -0.0000
  1300        0.0000             nan     0.4093   -0.0000
  1320        0.0000             nan     0.4093   -0.0000
  1340        0.0000             nan     0.4093   -0.0000
  1360        0.0000             nan     0.4093   -0.0000
  1380        0.0000             nan     0.4093    0.0000
  1400        0.0000             nan     0.4093   -0.0000
  1420        0.0000             nan     0.4093   -0.0000
  1440        0.0000             nan     0.4093    0.0000
  1460        0.0000             nan     0.4093   -0.0000
  1480        0.0000             nan     0.4093    0.0000
  1500        0.0000             nan     0.4093   -0.0000
  1520        0.0000             nan     0.4093   -0.0000
  1540        0.0000             nan     0.4093    0.0000
  1560        0.0000             nan     0.4093   -0.0000
  1580        0.0000             nan     0.4093   -0.0000
  1600        0.0000             nan     0.4093   -0.0000
  1620        0.0000             nan     0.4093   -0.0000
  1640        0.0000             nan     0.4093   -0.0000
  1660        0.0000             nan     0.4093    0.0000
  1680        0.0000             nan     0.4093    0.0000
  1700        0.0000             nan     0.4093   -0.0000
  1720        0.0000             nan     0.4093   -0.0000
  1740        0.0000             nan     0.4093    0.0000
  1760        0.0000             nan     0.4093   -0.0000
  1780        0.0000             nan     0.4093   -0.0000
  1800        0.0000             nan     0.4093   -0.0000
  1820        0.0000             nan     0.4093   -0.0000
  1840        0.0000             nan     0.4093   -0.0000
  1860        0.0000             nan     0.4093   -0.0000
  1880        0.0000             nan     0.4093    0.0000
  1900        0.0000             nan     0.4093   -0.0000
  1920        0.0000             nan     0.4093   -0.0000
  1940        0.0000             nan     0.4093   -0.0000
  1960        0.0000             nan     0.4093   -0.0000
  1980        0.0000             nan     0.4093   -0.0000
  2000        0.0000             nan     0.4093   -0.0000
  2020        0.0000             nan     0.4093   -0.0000
  2040        0.0000             nan     0.4093   -0.0000
  2060        0.0000             nan     0.4093   -0.0000
  2080        0.0000             nan     0.4093    0.0000
  2100        0.0000             nan     0.4093    0.0000
  2120        0.0000             nan     0.4093   -0.0000
  2140        0.0000             nan     0.4093   -0.0000
  2160        0.0000             nan     0.4093    0.0000
  2180        0.0000             nan     0.4093   -0.0000
  2200        0.0000             nan     0.4093   -0.0000
  2220        0.0000             nan     0.4093   -0.0000
  2240        0.0000             nan     0.4093    0.0000
  2260        0.0000             nan     0.4093    0.0000
  2280        0.0000             nan     0.4093    0.0000
  2300        0.0000             nan     0.4093   -0.0000
  2320        0.0000             nan     0.4093   -0.0000
  2340        0.0000             nan     0.4093    0.0000
  2360        0.0000             nan     0.4093   -0.0000
  2380        0.0000             nan     0.4093   -0.0000
  2400        0.0000             nan     0.4093   -0.0000
  2420        0.0000             nan     0.4093    0.0000
  2440        0.0000             nan     0.4093   -0.0000
  2460        0.0000             nan     0.4093   -0.0000
  2480        0.0000             nan     0.4093    0.0000
  2500        0.0000             nan     0.4093   -0.0000
  2520        0.0000             nan     0.4093    0.0000
  2540        0.0000             nan     0.4093   -0.0000
  2560        0.0000             nan     0.4093   -0.0000
  2580        0.0000             nan     0.4093   -0.0000
  2600        0.0000             nan     0.4093   -0.0000
  2620        0.0000             nan     0.4093   -0.0000
  2640        0.0000             nan     0.4093    0.0000
  2660        0.0000             nan     0.4093    0.0000
  2680        0.0000             nan     0.4093    0.0000
  2700        0.0000             nan     0.4093   -0.0000
  2720        0.0000             nan     0.4093    0.0000
  2740        0.0000             nan     0.4093   -0.0000
  2760        0.0000             nan     0.4093    0.0000
  2780        0.0000             nan     0.4093    0.0000
  2800        0.0000             nan     0.4093   -0.0000
  2820        0.0000             nan     0.4093   -0.0000
  2840        0.0000             nan     0.4093   -0.0000
  2860        0.0000             nan     0.4093   -0.0000
  2880        0.0000             nan     0.4093   -0.0000
  2900        0.0000             nan     0.4093    0.0000
  2920        0.0000             nan     0.4093   -0.0000
  2940        0.0000             nan     0.4093    0.0000
  2960        0.0000             nan     0.4093   -0.0000
  2980        0.0000             nan     0.4093    0.0000
  3000        0.0000             nan     0.4093   -0.0000
  3020        0.0000             nan     0.4093    0.0000
  3040        0.0000             nan     0.4093    0.0000
  3060        0.0000             nan     0.4093   -0.0000
  3080        0.0000             nan     0.4093   -0.0000
  3100        0.0000             nan     0.4093   -0.0000
  3120        0.0000             nan     0.4093    0.0000
  3140        0.0000             nan     0.4093    0.0000
  3159        0.0000             nan     0.4093   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0143             nan     0.4257    0.0255
     2        0.0053             nan     0.4257    0.0098
     3        0.0021             nan     0.4257    0.0029
     4        0.0010             nan     0.4257    0.0011
     5        0.0005             nan     0.4257    0.0004
     6        0.0004             nan     0.4257    0.0001
     7        0.0003             nan     0.4257    0.0001
     8        0.0002             nan     0.4257    0.0000
     9        0.0002             nan     0.4257    0.0000
    10        0.0002             nan     0.4257    0.0000
    20        0.0001             nan     0.4257   -0.0000
    40        0.0001             nan     0.4257   -0.0000
    60        0.0000             nan     0.4257   -0.0000
    80        0.0000             nan     0.4257   -0.0000
   100        0.0000             nan     0.4257   -0.0000
   120        0.0000             nan     0.4257   -0.0000
   140        0.0000             nan     0.4257   -0.0000
   160        0.0000             nan     0.4257   -0.0000
   180        0.0000             nan     0.4257   -0.0000
   200        0.0000             nan     0.4257   -0.0000
   220        0.0000             nan     0.4257   -0.0000
   240        0.0000             nan     0.4257   -0.0000
   260        0.0000             nan     0.4257   -0.0000
   280        0.0000             nan     0.4257   -0.0000
   300        0.0000             nan     0.4257   -0.0000
   320        0.0000             nan     0.4257   -0.0000
   340        0.0000             nan     0.4257   -0.0000
   360        0.0000             nan     0.4257   -0.0000
   380        0.0000             nan     0.4257    0.0000
   400        0.0000             nan     0.4257    0.0000
   420        0.0000             nan     0.4257   -0.0000
   440        0.0000             nan     0.4257    0.0000
   460        0.0000             nan     0.4257    0.0000
   480        0.0000             nan     0.4257   -0.0000
   500        0.0000             nan     0.4257   -0.0000
   520        0.0000             nan     0.4257   -0.0000
   540        0.0000             nan     0.4257   -0.0000
   560        0.0000             nan     0.4257   -0.0000
   580        0.0000             nan     0.4257   -0.0000
   600        0.0000             nan     0.4257   -0.0000
   620        0.0000             nan     0.4257   -0.0000
   640        0.0000             nan     0.4257    0.0000
   660        0.0000             nan     0.4257   -0.0000
   680        0.0000             nan     0.4257   -0.0000
   700        0.0000             nan     0.4257    0.0000
   720        0.0000             nan     0.4257    0.0000
   740        0.0000             nan     0.4257    0.0000
   760        0.0000             nan     0.4257    0.0000
   780        0.0000             nan     0.4257   -0.0000
   800        0.0000             nan     0.4257   -0.0000
   820        0.0000             nan     0.4257   -0.0000
   840        0.0000             nan     0.4257   -0.0000
   860        0.0000             nan     0.4257   -0.0000
   880        0.0000             nan     0.4257   -0.0000
   900        0.0000             nan     0.4257   -0.0000
   920        0.0000             nan     0.4257    0.0000
   940        0.0000             nan     0.4257   -0.0000
   960        0.0000             nan     0.4257   -0.0000
   980        0.0000             nan     0.4257   -0.0000
  1000        0.0000             nan     0.4257   -0.0000
  1020        0.0000             nan     0.4257   -0.0000
  1040        0.0000             nan     0.4257    0.0000
  1060        0.0000             nan     0.4257   -0.0000
  1080        0.0000             nan     0.4257   -0.0000
  1100        0.0000             nan     0.4257    0.0000
  1120        0.0000             nan     0.4257   -0.0000
  1140        0.0000             nan     0.4257   -0.0000
  1160        0.0000             nan     0.4257    0.0000
  1180        0.0000             nan     0.4257   -0.0000
  1200        0.0000             nan     0.4257    0.0000
  1220        0.0000             nan     0.4257   -0.0000
  1240        0.0000             nan     0.4257    0.0000
  1260        0.0000             nan     0.4257   -0.0000
  1280        0.0000             nan     0.4257   -0.0000
  1300        0.0000             nan     0.4257   -0.0000
  1320        0.0000             nan     0.4257   -0.0000
  1340        0.0000             nan     0.4257   -0.0000
  1360        0.0000             nan     0.4257   -0.0000
  1380        0.0000             nan     0.4257   -0.0000
  1400        0.0000             nan     0.4257   -0.0000
  1420        0.0000             nan     0.4257   -0.0000
  1440        0.0000             nan     0.4257    0.0000
  1460        0.0000             nan     0.4257   -0.0000
  1480        0.0000             nan     0.4257   -0.0000
  1500        0.0000             nan     0.4257    0.0000
  1520        0.0000             nan     0.4257    0.0000
  1540        0.0000             nan     0.4257   -0.0000
  1560        0.0000             nan     0.4257   -0.0000
  1580        0.0000             nan     0.4257   -0.0000
  1600        0.0000             nan     0.4257    0.0000
  1620        0.0000             nan     0.4257    0.0000
  1640        0.0000             nan     0.4257   -0.0000
  1660        0.0000             nan     0.4257   -0.0000
  1680        0.0000             nan     0.4257    0.0000
  1700        0.0000             nan     0.4257   -0.0000
  1720        0.0000             nan     0.4257   -0.0000
  1740        0.0000             nan     0.4257   -0.0000
  1760        0.0000             nan     0.4257   -0.0000
  1780        0.0000             nan     0.4257   -0.0000
  1800        0.0000             nan     0.4257   -0.0000
  1818        0.0000             nan     0.4257    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0116             nan     0.4962    0.0289
     2        0.0037             nan     0.4962    0.0077
     3        0.0015             nan     0.4962    0.0021
     4        0.0009             nan     0.4962    0.0006
     5        0.0007             nan     0.4962    0.0001
     6        0.0007             nan     0.4962    0.0001
     7        0.0006             nan     0.4962    0.0001
     8        0.0005             nan     0.4962    0.0000
     9        0.0004             nan     0.4962    0.0000
    10        0.0004             nan     0.4962    0.0000
    20        0.0003             nan     0.4962   -0.0000
    40        0.0002             nan     0.4962   -0.0000
    60        0.0001             nan     0.4962   -0.0000
    80        0.0001             nan     0.4962   -0.0000
   100        0.0001             nan     0.4962   -0.0000
   120        0.0001             nan     0.4962   -0.0000
   140        0.0001             nan     0.4962   -0.0000
   160        0.0000             nan     0.4962   -0.0000
   180        0.0000             nan     0.4962   -0.0000
   200        0.0000             nan     0.4962    0.0000
   220        0.0000             nan     0.4962   -0.0000
   240        0.0000             nan     0.4962   -0.0000
   260        0.0000             nan     0.4962   -0.0000
   280        0.0000             nan     0.4962   -0.0000
   300        0.0000             nan     0.4962   -0.0000
   320        0.0000             nan     0.4962   -0.0000
   340        0.0000             nan     0.4962   -0.0000
   360        0.0000             nan     0.4962   -0.0000
   380        0.0000             nan     0.4962   -0.0000
   400        0.0000             nan     0.4962   -0.0000
   420        0.0000             nan     0.4962   -0.0000
   440        0.0000             nan     0.4962   -0.0000
   460        0.0000             nan     0.4962   -0.0000
   480        0.0000             nan     0.4962   -0.0000
   500        0.0000             nan     0.4962   -0.0000
   520        0.0000             nan     0.4962   -0.0000
   540        0.0000             nan     0.4962   -0.0000
   560        0.0000             nan     0.4962   -0.0000
   580        0.0000             nan     0.4962   -0.0000
   600        0.0000             nan     0.4962   -0.0000
   620        0.0000             nan     0.4962   -0.0000
   640        0.0000             nan     0.4962   -0.0000
   660        0.0000             nan     0.4962   -0.0000
   680        0.0000             nan     0.4962   -0.0000
   700        0.0000             nan     0.4962   -0.0000
   720        0.0000             nan     0.4962    0.0000
   740        0.0000             nan     0.4962   -0.0000
   760        0.0000             nan     0.4962   -0.0000
   780        0.0000             nan     0.4962   -0.0000
   800        0.0000             nan     0.4962   -0.0000
   820        0.0000             nan     0.4962   -0.0000
   840        0.0000             nan     0.4962   -0.0000
   860        0.0000             nan     0.4962   -0.0000
   880        0.0000             nan     0.4962   -0.0000
   900        0.0000             nan     0.4962   -0.0000
   920        0.0000             nan     0.4962   -0.0000
   940        0.0000             nan     0.4962    0.0000
   960        0.0000             nan     0.4962   -0.0000
   980        0.0000             nan     0.4962    0.0000
  1000        0.0000             nan     0.4962   -0.0000
  1020        0.0000             nan     0.4962   -0.0000
  1040        0.0000             nan     0.4962   -0.0000
  1060        0.0000             nan     0.4962   -0.0000
  1080        0.0000             nan     0.4962   -0.0000
  1100        0.0000             nan     0.4962   -0.0000
  1120        0.0000             nan     0.4962    0.0000
  1140        0.0000             nan     0.4962   -0.0000
  1160        0.0000             nan     0.4962   -0.0000
  1180        0.0000             nan     0.4962   -0.0000
  1200        0.0000             nan     0.4962   -0.0000
  1220        0.0000             nan     0.4962   -0.0000
  1240        0.0000             nan     0.4962   -0.0000
  1260        0.0000             nan     0.4962   -0.0000
  1280        0.0000             nan     0.4962   -0.0000
  1300        0.0000             nan     0.4962   -0.0000
  1320        0.0000             nan     0.4962   -0.0000
  1340        0.0000             nan     0.4962   -0.0000
  1360        0.0000             nan     0.4962    0.0000
  1380        0.0000             nan     0.4962   -0.0000
  1400        0.0000             nan     0.4962   -0.0000
  1420        0.0000             nan     0.4962   -0.0000
  1440        0.0000             nan     0.4962   -0.0000
  1460        0.0000             nan     0.4962   -0.0000
  1480        0.0000             nan     0.4962   -0.0000
  1500        0.0000             nan     0.4962   -0.0000
  1520        0.0000             nan     0.4962   -0.0000
  1540        0.0000             nan     0.4962   -0.0000
  1560        0.0000             nan     0.4962   -0.0000
  1580        0.0000             nan     0.4962   -0.0000
  1600        0.0000             nan     0.4962   -0.0000
  1620        0.0000             nan     0.4962   -0.0000
  1640        0.0000             nan     0.4962   -0.0000
  1660        0.0000             nan     0.4962    0.0000
  1680        0.0000             nan     0.4962   -0.0000
  1700        0.0000             nan     0.4962   -0.0000
  1720        0.0000             nan     0.4962   -0.0000
  1740        0.0000             nan     0.4962   -0.0000
  1760        0.0000             nan     0.4962   -0.0000
  1780        0.0000             nan     0.4962   -0.0000
  1800        0.0000             nan     0.4962   -0.0000
  1820        0.0000             nan     0.4962   -0.0000
  1840        0.0000             nan     0.4962   -0.0000
  1860        0.0000             nan     0.4962    0.0000
  1880        0.0000             nan     0.4962   -0.0000
  1892        0.0000             nan     0.4962   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0124             nan     0.5090    0.0243
     2        0.0044             nan     0.5090    0.0070
     3        0.0020             nan     0.5090    0.0023
     4        0.0012             nan     0.5090    0.0009
     5        0.0009             nan     0.5090    0.0003
     6        0.0007             nan     0.5090    0.0001
     7        0.0006             nan     0.5090    0.0001
     8        0.0006             nan     0.5090    0.0000
     9        0.0005             nan     0.5090    0.0001
    10        0.0005             nan     0.5090   -0.0000
    20        0.0003             nan     0.5090   -0.0000
    40        0.0002             nan     0.5090   -0.0000
    60        0.0001             nan     0.5090   -0.0000
    80        0.0001             nan     0.5090   -0.0000
   100        0.0001             nan     0.5090   -0.0000
   120        0.0001             nan     0.5090   -0.0000
   140        0.0001             nan     0.5090   -0.0000
   160        0.0000             nan     0.5090   -0.0000
   180        0.0000             nan     0.5090   -0.0000
   200        0.0000             nan     0.5090   -0.0000
   220        0.0000             nan     0.5090   -0.0000
   240        0.0000             nan     0.5090   -0.0000
   260        0.0000             nan     0.5090   -0.0000
   280        0.0000             nan     0.5090   -0.0000
   300        0.0000             nan     0.5090   -0.0000
   320        0.0000             nan     0.5090   -0.0000
   340        0.0000             nan     0.5090   -0.0000
   360        0.0000             nan     0.5090   -0.0000
   380        0.0000             nan     0.5090   -0.0000
   400        0.0000             nan     0.5090   -0.0000
   420        0.0000             nan     0.5090   -0.0000
   440        0.0000             nan     0.5090   -0.0000
   460        0.0000             nan     0.5090   -0.0000
   480        0.0000             nan     0.5090   -0.0000
   500        0.0000             nan     0.5090   -0.0000
   520        0.0000             nan     0.5090   -0.0000
   540        0.0000             nan     0.5090   -0.0000
   560        0.0000             nan     0.5090   -0.0000
   580        0.0000             nan     0.5090   -0.0000
   600        0.0000             nan     0.5090   -0.0000
   620        0.0000             nan     0.5090   -0.0000
   640        0.0000             nan     0.5090   -0.0000
   660        0.0000             nan     0.5090   -0.0000
   680        0.0000             nan     0.5090   -0.0000
   700        0.0000             nan     0.5090   -0.0000
   720        0.0000             nan     0.5090   -0.0000
   740        0.0000             nan     0.5090   -0.0000
   760        0.0000             nan     0.5090   -0.0000
   780        0.0000             nan     0.5090   -0.0000
   800        0.0000             nan     0.5090   -0.0000
   820        0.0000             nan     0.5090   -0.0000
   840        0.0000             nan     0.5090   -0.0000
   860        0.0000             nan     0.5090   -0.0000
   880        0.0000             nan     0.5090   -0.0000
   900        0.0000             nan     0.5090   -0.0000
   920        0.0000             nan     0.5090   -0.0000
   940        0.0000             nan     0.5090    0.0000
   960        0.0000             nan     0.5090   -0.0000
   980        0.0000             nan     0.5090    0.0000
  1000        0.0000             nan     0.5090   -0.0000
  1020        0.0000             nan     0.5090   -0.0000
  1040        0.0000             nan     0.5090   -0.0000
  1060        0.0000             nan     0.5090   -0.0000
  1080        0.0000             nan     0.5090   -0.0000
  1100        0.0000             nan     0.5090   -0.0000
  1120        0.0000             nan     0.5090   -0.0000
  1140        0.0000             nan     0.5090    0.0000
  1160        0.0000             nan     0.5090    0.0000
  1180        0.0000             nan     0.5090    0.0000
  1200        0.0000             nan     0.5090    0.0000
  1220        0.0000             nan     0.5090   -0.0000
  1240        0.0000             nan     0.5090   -0.0000
  1260        0.0000             nan     0.5090   -0.0000
  1280        0.0000             nan     0.5090   -0.0000
  1300        0.0000             nan     0.5090   -0.0000
  1320        0.0000             nan     0.5090   -0.0000
  1340        0.0000             nan     0.5090    0.0000
  1360        0.0000             nan     0.5090   -0.0000
  1380        0.0000             nan     0.5090    0.0000
  1400        0.0000             nan     0.5090   -0.0000
  1420        0.0000             nan     0.5090   -0.0000
  1440        0.0000             nan     0.5090   -0.0000
  1460        0.0000             nan     0.5090   -0.0000
  1480        0.0000             nan     0.5090   -0.0000
  1500        0.0000             nan     0.5090   -0.0000
  1520        0.0000             nan     0.5090   -0.0000
  1540        0.0000             nan     0.5090    0.0000
  1560        0.0000             nan     0.5090   -0.0000
  1580        0.0000             nan     0.5090    0.0000
  1600        0.0000             nan     0.5090   -0.0000
  1620        0.0000             nan     0.5090   -0.0000
  1640        0.0000             nan     0.5090    0.0000
  1660        0.0000             nan     0.5090   -0.0000
  1680        0.0000             nan     0.5090   -0.0000
  1700        0.0000             nan     0.5090   -0.0000
  1720        0.0000             nan     0.5090   -0.0000
  1740        0.0000             nan     0.5090   -0.0000
  1760        0.0000             nan     0.5090   -0.0000
  1780        0.0000             nan     0.5090   -0.0000
  1800        0.0000             nan     0.5090   -0.0000
  1820        0.0000             nan     0.5090   -0.0000
  1840        0.0000             nan     0.5090   -0.0000
  1860        0.0000             nan     0.5090   -0.0000
  1880        0.0000             nan     0.5090   -0.0000
  1900        0.0000             nan     0.5090   -0.0000
  1920        0.0000             nan     0.5090   -0.0000
  1940        0.0000             nan     0.5090   -0.0000
  1960        0.0000             nan     0.5090   -0.0000
  1980        0.0000             nan     0.5090   -0.0000
  2000        0.0000             nan     0.5090   -0.0000
  2020        0.0000             nan     0.5090   -0.0000
  2040        0.0000             nan     0.5090   -0.0000
  2060        0.0000             nan     0.5090   -0.0000
  2080        0.0000             nan     0.5090    0.0000
  2100        0.0000             nan     0.5090   -0.0000
  2120        0.0000             nan     0.5090    0.0000
  2140        0.0000             nan     0.5090   -0.0000
  2160        0.0000             nan     0.5090   -0.0000
  2180        0.0000             nan     0.5090   -0.0000
  2200        0.0000             nan     0.5090   -0.0000
  2220        0.0000             nan     0.5090   -0.0000
  2240        0.0000             nan     0.5090   -0.0000
  2260        0.0000             nan     0.5090   -0.0000
  2280        0.0000             nan     0.5090   -0.0000
  2300        0.0000             nan     0.5090   -0.0000
  2320        0.0000             nan     0.5090   -0.0000
  2340        0.0000             nan     0.5090   -0.0000
  2360        0.0000             nan     0.5090   -0.0000
  2380        0.0000             nan     0.5090   -0.0000
  2400        0.0000             nan     0.5090   -0.0000
  2420        0.0000             nan     0.5090   -0.0000
  2440        0.0000             nan     0.5090   -0.0000
  2460        0.0000             nan     0.5090    0.0000
  2480        0.0000             nan     0.5090   -0.0000
  2500        0.0000             nan     0.5090   -0.0000
  2520        0.0000             nan     0.5090   -0.0000
  2540        0.0000             nan     0.5090   -0.0000
  2560        0.0000             nan     0.5090   -0.0000
  2580        0.0000             nan     0.5090    0.0000
  2600        0.0000             nan     0.5090    0.0000
  2620        0.0000             nan     0.5090   -0.0000
  2640        0.0000             nan     0.5090   -0.0000
  2660        0.0000             nan     0.5090   -0.0000
  2680        0.0000             nan     0.5090   -0.0000
  2700        0.0000             nan     0.5090   -0.0000
  2720        0.0000             nan     0.5090   -0.0000
  2740        0.0000             nan     0.5090   -0.0000
  2760        0.0000             nan     0.5090   -0.0000
  2780        0.0000             nan     0.5090    0.0000
  2800        0.0000             nan     0.5090    0.0000
  2820        0.0000             nan     0.5090   -0.0000
  2840        0.0000             nan     0.5090    0.0000
  2860        0.0000             nan     0.5090   -0.0000
  2880        0.0000             nan     0.5090   -0.0000
  2900        0.0000             nan     0.5090   -0.0000
  2920        0.0000             nan     0.5090    0.0000
  2940        0.0000             nan     0.5090   -0.0000
  2960        0.0000             nan     0.5090   -0.0000
  2980        0.0000             nan     0.5090   -0.0000
  3000        0.0000             nan     0.5090   -0.0000
  3020        0.0000             nan     0.5090    0.0000
  3040        0.0000             nan     0.5090   -0.0000
  3060        0.0000             nan     0.5090    0.0000
  3080        0.0000             nan     0.5090    0.0000
  3100        0.0000             nan     0.5090   -0.0000
  3120        0.0000             nan     0.5090   -0.0000
  3140        0.0000             nan     0.5090   -0.0000
  3160        0.0000             nan     0.5090   -0.0000
  3180        0.0000             nan     0.5090   -0.0000
  3200        0.0000             nan     0.5090   -0.0000
  3220        0.0000             nan     0.5090   -0.0000
  3240        0.0000             nan     0.5090   -0.0000
  3260        0.0000             nan     0.5090   -0.0000
  3280        0.0000             nan     0.5090   -0.0000
  3300        0.0000             nan     0.5090   -0.0000
  3320        0.0000             nan     0.5090   -0.0000
  3340        0.0000             nan     0.5090   -0.0000
  3360        0.0000             nan     0.5090   -0.0000
  3380        0.0000             nan     0.5090   -0.0000
  3400        0.0000             nan     0.5090   -0.0000
  3420        0.0000             nan     0.5090   -0.0000
  3440        0.0000             nan     0.5090    0.0000
  3460        0.0000             nan     0.5090   -0.0000
  3480        0.0000             nan     0.5090   -0.0000
  3500        0.0000             nan     0.5090   -0.0000
  3520        0.0000             nan     0.5090   -0.0000
  3540        0.0000             nan     0.5090   -0.0000
  3560        0.0000             nan     0.5090   -0.0000
  3580        0.0000             nan     0.5090   -0.0000
  3600        0.0000             nan     0.5090    0.0000
  3620        0.0000             nan     0.5090   -0.0000
  3640        0.0000             nan     0.5090   -0.0000
  3660        0.0000             nan     0.5090    0.0000
  3680        0.0000             nan     0.5090   -0.0000
  3700        0.0000             nan     0.5090    0.0000
  3720        0.0000             nan     0.5090   -0.0000
  3740        0.0000             nan     0.5090   -0.0000
  3760        0.0000             nan     0.5090    0.0000
  3780        0.0000             nan     0.5090    0.0000
  3800        0.0000             nan     0.5090   -0.0000
  3820        0.0000             nan     0.5090   -0.0000
  3840        0.0000             nan     0.5090    0.0000
  3860        0.0000             nan     0.5090   -0.0000
  3880        0.0000             nan     0.5090    0.0000
  3900        0.0000             nan     0.5090   -0.0000
  3920        0.0000             nan     0.5090    0.0000
  3940        0.0000             nan     0.5090   -0.0000
  3960        0.0000             nan     0.5090   -0.0000
  3980        0.0000             nan     0.5090    0.0000
  4000        0.0000             nan     0.5090   -0.0000
  4020        0.0000             nan     0.5090    0.0000
  4040        0.0000             nan     0.5090   -0.0000
  4060        0.0000             nan     0.5090   -0.0000
  4080        0.0000             nan     0.5090    0.0000
  4100        0.0000             nan     0.5090   -0.0000
  4120        0.0000             nan     0.5090    0.0000
  4140        0.0000             nan     0.5090   -0.0000
  4157        0.0000             nan     0.5090    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0141             nan     0.5702    0.0265
     2        0.0059             nan     0.5702    0.0094
     3        0.0029             nan     0.5702    0.0026
     4        0.0021             nan     0.5702    0.0009
     5        0.0016             nan     0.5702    0.0003
     6        0.0014             nan     0.5702    0.0001
     7        0.0012             nan     0.5702    0.0001
     8        0.0012             nan     0.5702    0.0001
     9        0.0011             nan     0.5702    0.0001
    10        0.0010             nan     0.5702    0.0001
    20        0.0005             nan     0.5702   -0.0000
    40        0.0003             nan     0.5702   -0.0000
    60        0.0002             nan     0.5702   -0.0000
    80        0.0002             nan     0.5702   -0.0000
   100        0.0001             nan     0.5702   -0.0000
   120        0.0001             nan     0.5702   -0.0000
   140        0.0001             nan     0.5702   -0.0000
   160        0.0001             nan     0.5702   -0.0000
   180        0.0001             nan     0.5702   -0.0000
   200        0.0001             nan     0.5702   -0.0000
   220        0.0001             nan     0.5702   -0.0000
   240        0.0001             nan     0.5702   -0.0000
   260        0.0001             nan     0.5702   -0.0000
   280        0.0001             nan     0.5702   -0.0000
   300        0.0000             nan     0.5702   -0.0000
   320        0.0000             nan     0.5702   -0.0000
   340        0.0000             nan     0.5702    0.0000
   360        0.0000             nan     0.5702   -0.0000
   380        0.0000             nan     0.5702   -0.0000
   400        0.0000             nan     0.5702   -0.0000
   420        0.0000             nan     0.5702   -0.0000
   440        0.0000             nan     0.5702    0.0000
   460        0.0000             nan     0.5702   -0.0000
   480        0.0000             nan     0.5702   -0.0000
   500        0.0000             nan     0.5702   -0.0000
   520        0.0000             nan     0.5702   -0.0000
   540        0.0000             nan     0.5702   -0.0000
   560        0.0000             nan     0.5702   -0.0000
   580        0.0000             nan     0.5702   -0.0000
   600        0.0000             nan     0.5702   -0.0000
   620        0.0000             nan     0.5702   -0.0000
   640        0.0000             nan     0.5702   -0.0000
   660        0.0000             nan     0.5702   -0.0000
   680        0.0000             nan     0.5702   -0.0000
   700        0.0000             nan     0.5702   -0.0000
   720        0.0000             nan     0.5702   -0.0000
   740        0.0000             nan     0.5702   -0.0000
   760        0.0000             nan     0.5702   -0.0000
   780        0.0000             nan     0.5702   -0.0000
   800        0.0000             nan     0.5702   -0.0000
   820        0.0000             nan     0.5702   -0.0000
   840        0.0000             nan     0.5702   -0.0000
   860        0.0000             nan     0.5702   -0.0000
   880        0.0000             nan     0.5702   -0.0000
   900        0.0000             nan     0.5702   -0.0000
   920        0.0000             nan     0.5702   -0.0000
   940        0.0000             nan     0.5702   -0.0000
   960        0.0000             nan     0.5702   -0.0000
   980        0.0000             nan     0.5702   -0.0000
  1000        0.0000             nan     0.5702   -0.0000
  1020        0.0000             nan     0.5702   -0.0000
  1040        0.0000             nan     0.5702   -0.0000
  1060        0.0000             nan     0.5702   -0.0000
  1080        0.0000             nan     0.5702   -0.0000
  1100        0.0000             nan     0.5702   -0.0000
  1120        0.0000             nan     0.5702   -0.0000
  1140        0.0000             nan     0.5702    0.0000
  1160        0.0000             nan     0.5702   -0.0000
  1180        0.0000             nan     0.5702   -0.0000
  1200        0.0000             nan     0.5702   -0.0000
  1220        0.0000             nan     0.5702   -0.0000
  1240        0.0000             nan     0.5702   -0.0000
  1260        0.0000             nan     0.5702   -0.0000
  1280        0.0000             nan     0.5702   -0.0000
  1300        0.0000             nan     0.5702   -0.0000
  1320        0.0000             nan     0.5702   -0.0000
  1340        0.0000             nan     0.5702   -0.0000
  1360        0.0000             nan     0.5702   -0.0000
  1380        0.0000             nan     0.5702   -0.0000
  1400        0.0000             nan     0.5702   -0.0000
  1420        0.0000             nan     0.5702    0.0000
  1440        0.0000             nan     0.5702   -0.0000
  1460        0.0000             nan     0.5702    0.0000
  1480        0.0000             nan     0.5702   -0.0000
  1500        0.0000             nan     0.5702   -0.0000
  1520        0.0000             nan     0.5702   -0.0000
  1540        0.0000             nan     0.5702   -0.0000
  1560        0.0000             nan     0.5702   -0.0000
  1580        0.0000             nan     0.5702   -0.0000
  1600        0.0000             nan     0.5702   -0.0000
  1620        0.0000             nan     0.5702   -0.0000
  1640        0.0000             nan     0.5702   -0.0000
  1660        0.0000             nan     0.5702    0.0000
  1680        0.0000             nan     0.5702   -0.0000
  1700        0.0000             nan     0.5702   -0.0000
  1720        0.0000             nan     0.5702   -0.0000
  1740        0.0000             nan     0.5702   -0.0000
  1760        0.0000             nan     0.5702   -0.0000
  1780        0.0000             nan     0.5702   -0.0000
  1800        0.0000             nan     0.5702   -0.0000
  1820        0.0000             nan     0.5702   -0.0000
  1840        0.0000             nan     0.5702   -0.0000
  1860        0.0000             nan     0.5702    0.0000
  1880        0.0000             nan     0.5702   -0.0000
  1900        0.0000             nan     0.5702   -0.0000
  1920        0.0000             nan     0.5702   -0.0000
  1940        0.0000             nan     0.5702   -0.0000
  1960        0.0000             nan     0.5702   -0.0000
  1980        0.0000             nan     0.5702   -0.0000
  2000        0.0000             nan     0.5702   -0.0000
  2020        0.0000             nan     0.5702    0.0000
  2040        0.0000             nan     0.5702   -0.0000
  2060        0.0000             nan     0.5702   -0.0000
  2080        0.0000             nan     0.5702   -0.0000
  2100        0.0000             nan     0.5702   -0.0000
  2120        0.0000             nan     0.5702   -0.0000
  2140        0.0000             nan     0.5702   -0.0000
  2160        0.0000             nan     0.5702   -0.0000
  2180        0.0000             nan     0.5702   -0.0000
  2200        0.0000             nan     0.5702   -0.0000
  2220        0.0000             nan     0.5702    0.0000
  2240        0.0000             nan     0.5702   -0.0000
  2260        0.0000             nan     0.5702   -0.0000
  2280        0.0000             nan     0.5702   -0.0000
  2300        0.0000             nan     0.5702   -0.0000
  2320        0.0000             nan     0.5702   -0.0000
  2340        0.0000             nan     0.5702   -0.0000
  2360        0.0000             nan     0.5702   -0.0000
  2380        0.0000             nan     0.5702   -0.0000
  2400        0.0000             nan     0.5702   -0.0000
  2420        0.0000             nan     0.5702   -0.0000
  2440        0.0000             nan     0.5702   -0.0000
  2460        0.0000             nan     0.5702   -0.0000
  2480        0.0000             nan     0.5702   -0.0000
  2500        0.0000             nan     0.5702   -0.0000
  2520        0.0000             nan     0.5702   -0.0000
  2540        0.0000             nan     0.5702   -0.0000
  2560        0.0000             nan     0.5702   -0.0000
  2580        0.0000             nan     0.5702   -0.0000
  2600        0.0000             nan     0.5702   -0.0000
  2620        0.0000             nan     0.5702   -0.0000
  2640        0.0000             nan     0.5702   -0.0000
  2660        0.0000             nan     0.5702   -0.0000
  2680        0.0000             nan     0.5702   -0.0000
  2700        0.0000             nan     0.5702    0.0000
  2720        0.0000             nan     0.5702   -0.0000
  2740        0.0000             nan     0.5702   -0.0000
  2760        0.0000             nan     0.5702   -0.0000
  2780        0.0000             nan     0.5702   -0.0000
  2800        0.0000             nan     0.5702   -0.0000
  2820        0.0000             nan     0.5702   -0.0000
  2840        0.0000             nan     0.5702   -0.0000
  2860        0.0000             nan     0.5702   -0.0000
  2880        0.0000             nan     0.5702   -0.0000
  2900        0.0000             nan     0.5702   -0.0000
  2920        0.0000             nan     0.5702   -0.0000
  2940        0.0000             nan     0.5702   -0.0000
  2960        0.0000             nan     0.5702   -0.0000
  2980        0.0000             nan     0.5702   -0.0000
  3000        0.0000             nan     0.5702   -0.0000
  3020        0.0000             nan     0.5702   -0.0000
  3040        0.0000             nan     0.5702   -0.0000
  3060        0.0000             nan     0.5702   -0.0000
  3080        0.0000             nan     0.5702   -0.0000
  3100        0.0000             nan     0.5702   -0.0000
  3120        0.0000             nan     0.5702   -0.0000
  3140        0.0000             nan     0.5702    0.0000
  3160        0.0000             nan     0.5702    0.0000
  3180        0.0000             nan     0.5702   -0.0000
  3200        0.0000             nan     0.5702   -0.0000
  3220        0.0000             nan     0.5702   -0.0000
  3240        0.0000             nan     0.5702   -0.0000
  3260        0.0000             nan     0.5702   -0.0000
  3280        0.0000             nan     0.5702   -0.0000
  3300        0.0000             nan     0.5702    0.0000
  3320        0.0000             nan     0.5702   -0.0000
  3337        0.0000             nan     0.5702    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0085             nan     0.5774    0.0291
     2        0.0021             nan     0.5774    0.0061
     3        0.0008             nan     0.5774    0.0012
     4        0.0006             nan     0.5774    0.0002
     5        0.0004             nan     0.5774    0.0001
     6        0.0004             nan     0.5774   -0.0000
     7        0.0004             nan     0.5774    0.0000
     8        0.0003             nan     0.5774   -0.0000
     9        0.0003             nan     0.5774   -0.0000
    10        0.0003             nan     0.5774   -0.0000
    20        0.0002             nan     0.5774   -0.0000
    40        0.0001             nan     0.5774   -0.0000
    60        0.0001             nan     0.5774   -0.0000
    80        0.0000             nan     0.5774   -0.0000
   100        0.0000             nan     0.5774   -0.0000
   120        0.0000             nan     0.5774   -0.0000
   140        0.0000             nan     0.5774   -0.0000
   160        0.0000             nan     0.5774   -0.0000
   180        0.0000             nan     0.5774   -0.0000
   200        0.0000             nan     0.5774   -0.0000
   220        0.0000             nan     0.5774   -0.0000
   240        0.0000             nan     0.5774   -0.0000
   260        0.0000             nan     0.5774   -0.0000
   280        0.0000             nan     0.5774   -0.0000
   300        0.0000             nan     0.5774    0.0000
   320        0.0000             nan     0.5774   -0.0000
   340        0.0000             nan     0.5774   -0.0000
   360        0.0000             nan     0.5774   -0.0000
   380        0.0000             nan     0.5774    0.0000
   400        0.0000             nan     0.5774   -0.0000
   420        0.0000             nan     0.5774    0.0000
   440        0.0000             nan     0.5774   -0.0000
   460        0.0000             nan     0.5774   -0.0000
   480        0.0000             nan     0.5774   -0.0000
   500        0.0000             nan     0.5774   -0.0000
   520        0.0000             nan     0.5774   -0.0000
   540        0.0000             nan     0.5774   -0.0000
   560        0.0000             nan     0.5774   -0.0000
   580        0.0000             nan     0.5774   -0.0000
   600        0.0000             nan     0.5774   -0.0000
   620        0.0000             nan     0.5774    0.0000
   640        0.0000             nan     0.5774   -0.0000
   660        0.0000             nan     0.5774   -0.0000
   680        0.0000             nan     0.5774   -0.0000
   700        0.0000             nan     0.5774   -0.0000
   720        0.0000             nan     0.5774   -0.0000
   740        0.0000             nan     0.5774   -0.0000
   760        0.0000             nan     0.5774    0.0000
   780        0.0000             nan     0.5774   -0.0000
   800        0.0000             nan     0.5774   -0.0000
   820        0.0000             nan     0.5774    0.0000
   840        0.0000             nan     0.5774   -0.0000
   860        0.0000             nan     0.5774    0.0000
   880        0.0000             nan     0.5774   -0.0000
   900        0.0000             nan     0.5774   -0.0000
   920        0.0000             nan     0.5774    0.0000
   940        0.0000             nan     0.5774   -0.0000
   960        0.0000             nan     0.5774   -0.0000
   980        0.0000             nan     0.5774   -0.0000
  1000        0.0000             nan     0.5774   -0.0000
  1020        0.0000             nan     0.5774    0.0000
  1040        0.0000             nan     0.5774    0.0000
  1060        0.0000             nan     0.5774    0.0000
  1080        0.0000             nan     0.5774   -0.0000
  1100        0.0000             nan     0.5774   -0.0000
  1120        0.0000             nan     0.5774    0.0000
  1140        0.0000             nan     0.5774   -0.0000
  1160        0.0000             nan     0.5774   -0.0000
  1180        0.0000             nan     0.5774   -0.0000
  1200        0.0000             nan     0.5774   -0.0000
  1220        0.0000             nan     0.5774   -0.0000
  1240        0.0000             nan     0.5774   -0.0000
  1260        0.0000             nan     0.5774   -0.0000
  1280        0.0000             nan     0.5774    0.0000
  1300        0.0000             nan     0.5774   -0.0000
  1320        0.0000             nan     0.5774   -0.0000
  1340        0.0000             nan     0.5774   -0.0000
  1360        0.0000             nan     0.5774   -0.0000
  1380        0.0000             nan     0.5774    0.0000
  1400        0.0000             nan     0.5774    0.0000
  1420        0.0000             nan     0.5774   -0.0000
  1440        0.0000             nan     0.5774    0.0000
  1460        0.0000             nan     0.5774    0.0000
  1480        0.0000             nan     0.5774    0.0000
  1500        0.0000             nan     0.5774   -0.0000
  1520        0.0000             nan     0.5774    0.0000
  1540        0.0000             nan     0.5774    0.0000
  1560        0.0000             nan     0.5774    0.0000
  1580        0.0000             nan     0.5774    0.0000
  1600        0.0000             nan     0.5774    0.0000
  1620        0.0000             nan     0.5774   -0.0000
  1640        0.0000             nan     0.5774    0.0000
  1660        0.0000             nan     0.5774    0.0000
  1680        0.0000             nan     0.5774   -0.0000
  1700        0.0000             nan     0.5774    0.0000
  1720        0.0000             nan     0.5774   -0.0000
  1740        0.0000             nan     0.5774    0.0000
  1760        0.0000             nan     0.5774    0.0000
  1764        0.0000             nan     0.5774    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0100             nan     0.5997    0.0284
     2        0.0035             nan     0.5997    0.0061
     3        0.0019             nan     0.5997    0.0016
     4        0.0013             nan     0.5997    0.0005
     5        0.0011             nan     0.5997    0.0002
     6        0.0009             nan     0.5997    0.0001
     7        0.0008             nan     0.5997    0.0001
     8        0.0008             nan     0.5997    0.0000
     9        0.0006             nan     0.5997    0.0001
    10        0.0006             nan     0.5997   -0.0000
    20        0.0004             nan     0.5997   -0.0000
    40        0.0002             nan     0.5997   -0.0000
    60        0.0001             nan     0.5997   -0.0000
    80        0.0001             nan     0.5997   -0.0000
   100        0.0001             nan     0.5997   -0.0000
   120        0.0001             nan     0.5997   -0.0000
   140        0.0001             nan     0.5997   -0.0000
   160        0.0001             nan     0.5997   -0.0000
   180        0.0001             nan     0.5997   -0.0000
   200        0.0000             nan     0.5997   -0.0000
   220        0.0000             nan     0.5997   -0.0000
   240        0.0000             nan     0.5997   -0.0000
   260        0.0000             nan     0.5997   -0.0000
   280        0.0000             nan     0.5997   -0.0000
   300        0.0000             nan     0.5997   -0.0000
   320        0.0000             nan     0.5997   -0.0000
   340        0.0000             nan     0.5997   -0.0000
   360        0.0000             nan     0.5997   -0.0000
   380        0.0000             nan     0.5997   -0.0000
   400        0.0000             nan     0.5997   -0.0000
   420        0.0000             nan     0.5997   -0.0000
   440        0.0000             nan     0.5997   -0.0000
   460        0.0000             nan     0.5997   -0.0000
   480        0.0000             nan     0.5997   -0.0000
   500        0.0000             nan     0.5997   -0.0000
   520        0.0000             nan     0.5997   -0.0000
   540        0.0000             nan     0.5997   -0.0000
   560        0.0000             nan     0.5997   -0.0000
   580        0.0000             nan     0.5997   -0.0000
   600        0.0000             nan     0.5997   -0.0000
   620        0.0000             nan     0.5997   -0.0000
   640        0.0000             nan     0.5997   -0.0000
   660        0.0000             nan     0.5997   -0.0000
   680        0.0000             nan     0.5997   -0.0000
   700        0.0000             nan     0.5997    0.0000
   720        0.0000             nan     0.5997   -0.0000
   740        0.0000             nan     0.5997    0.0000
   760        0.0000             nan     0.5997   -0.0000
   780        0.0000             nan     0.5997   -0.0000
   800        0.0000             nan     0.5997   -0.0000
   820        0.0000             nan     0.5997   -0.0000
   840        0.0000             nan     0.5997   -0.0000
   860        0.0000             nan     0.5997   -0.0000
   880        0.0000             nan     0.5997   -0.0000
   900        0.0000             nan     0.5997    0.0000
   920        0.0000             nan     0.5997   -0.0000
   940        0.0000             nan     0.5997   -0.0000
   960        0.0000             nan     0.5997   -0.0000
   980        0.0000             nan     0.5997    0.0000
  1000        0.0000             nan     0.5997   -0.0000
  1020        0.0000             nan     0.5997   -0.0000
  1040        0.0000             nan     0.5997   -0.0000
  1060        0.0000             nan     0.5997   -0.0000
  1080        0.0000             nan     0.5997   -0.0000
  1100        0.0000             nan     0.5997   -0.0000
  1120        0.0000             nan     0.5997   -0.0000
  1140        0.0000             nan     0.5997   -0.0000
  1160        0.0000             nan     0.5997   -0.0000
  1180        0.0000             nan     0.5997    0.0000
  1200        0.0000             nan     0.5997   -0.0000
  1220        0.0000             nan     0.5997   -0.0000
  1240        0.0000             nan     0.5997   -0.0000
  1260        0.0000             nan     0.5997   -0.0000
  1280        0.0000             nan     0.5997   -0.0000
  1300        0.0000             nan     0.5997   -0.0000
  1320        0.0000             nan     0.5997   -0.0000
  1340        0.0000             nan     0.5997   -0.0000
  1360        0.0000             nan     0.5997   -0.0000
  1380        0.0000             nan     0.5997    0.0000
  1400        0.0000             nan     0.5997   -0.0000
  1420        0.0000             nan     0.5997   -0.0000
  1440        0.0000             nan     0.5997   -0.0000
  1460        0.0000             nan     0.5997   -0.0000
  1480        0.0000             nan     0.5997   -0.0000
  1500        0.0000             nan     0.5997   -0.0000
  1520        0.0000             nan     0.5997    0.0000
  1540        0.0000             nan     0.5997   -0.0000
  1560        0.0000             nan     0.5997   -0.0000
  1580        0.0000             nan     0.5997   -0.0000
  1600        0.0000             nan     0.5997   -0.0000
  1620        0.0000             nan     0.5997   -0.0000
  1640        0.0000             nan     0.5997    0.0000
  1660        0.0000             nan     0.5997    0.0000
  1680        0.0000             nan     0.5997   -0.0000
  1700        0.0000             nan     0.5997   -0.0000
  1720        0.0000             nan     0.5997    0.0000
  1740        0.0000             nan     0.5997   -0.0000
  1760        0.0000             nan     0.5997   -0.0000
  1780        0.0000             nan     0.5997   -0.0000
  1800        0.0000             nan     0.5997   -0.0000
  1820        0.0000             nan     0.5997   -0.0000
  1840        0.0000             nan     0.5997   -0.0000
  1860        0.0000             nan     0.5997   -0.0000
  1880        0.0000             nan     0.5997   -0.0000
  1900        0.0000             nan     0.5997   -0.0000
  1920        0.0000             nan     0.5997   -0.0000
  1940        0.0000             nan     0.5997   -0.0000
  1960        0.0000             nan     0.5997   -0.0000
  1980        0.0000             nan     0.5997   -0.0000
  2000        0.0000             nan     0.5997   -0.0000
  2020        0.0000             nan     0.5997   -0.0000
  2040        0.0000             nan     0.5997   -0.0000
  2060        0.0000             nan     0.5997   -0.0000
  2080        0.0000             nan     0.5997   -0.0000
  2100        0.0000             nan     0.5997   -0.0000
  2120        0.0000             nan     0.5997   -0.0000
  2140        0.0000             nan     0.5997   -0.0000
  2160        0.0000             nan     0.5997    0.0000
  2180        0.0000             nan     0.5997   -0.0000
  2200        0.0000             nan     0.5997   -0.0000
  2220        0.0000             nan     0.5997   -0.0000
  2240        0.0000             nan     0.5997   -0.0000
  2260        0.0000             nan     0.5997   -0.0000
  2280        0.0000             nan     0.5997   -0.0000
  2300        0.0000             nan     0.5997   -0.0000
  2320        0.0000             nan     0.5997    0.0000
  2340        0.0000             nan     0.5997    0.0000
  2360        0.0000             nan     0.5997   -0.0000
  2380        0.0000             nan     0.5997   -0.0000
  2400        0.0000             nan     0.5997   -0.0000
  2420        0.0000             nan     0.5997    0.0000
  2440        0.0000             nan     0.5997   -0.0000
  2460        0.0000             nan     0.5997   -0.0000
  2480        0.0000             nan     0.5997   -0.0000
  2500        0.0000             nan     0.5997   -0.0000
  2520        0.0000             nan     0.5997   -0.0000
  2540        0.0000             nan     0.5997   -0.0000
  2560        0.0000             nan     0.5997   -0.0000
  2580        0.0000             nan     0.5997   -0.0000
  2600        0.0000             nan     0.5997   -0.0000
  2620        0.0000             nan     0.5997    0.0000
  2640        0.0000             nan     0.5997   -0.0000
  2660        0.0000             nan     0.5997   -0.0000
  2680        0.0000             nan     0.5997   -0.0000
  2700        0.0000             nan     0.5997   -0.0000
  2720        0.0000             nan     0.5997    0.0000
  2740        0.0000             nan     0.5997   -0.0000
  2760        0.0000             nan     0.5997   -0.0000
  2780        0.0000             nan     0.5997   -0.0000
  2800        0.0000             nan     0.5997    0.0000
  2820        0.0000             nan     0.5997   -0.0000
  2840        0.0000             nan     0.5997   -0.0000
  2860        0.0000             nan     0.5997   -0.0000
  2880        0.0000             nan     0.5997   -0.0000
  2900        0.0000             nan     0.5997   -0.0000
  2920        0.0000             nan     0.5997   -0.0000
  2940        0.0000             nan     0.5997   -0.0000
  2960        0.0000             nan     0.5997   -0.0000
  2980        0.0000             nan     0.5997   -0.0000
  3000        0.0000             nan     0.5997   -0.0000
  3020        0.0000             nan     0.5997   -0.0000
  3040        0.0000             nan     0.5997    0.0000
  3060        0.0000             nan     0.5997   -0.0000
  3080        0.0000             nan     0.5997   -0.0000
  3100        0.0000             nan     0.5997   -0.0000
  3120        0.0000             nan     0.5997   -0.0000
  3140        0.0000             nan     0.5997   -0.0000
  3160        0.0000             nan     0.5997   -0.0000
  3180        0.0000             nan     0.5997   -0.0000
  3200        0.0000             nan     0.5997   -0.0000
  3220        0.0000             nan     0.5997   -0.0000
  3240        0.0000             nan     0.5997   -0.0000
  3260        0.0000             nan     0.5997    0.0000
  3269        0.0000             nan     0.5997   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0377             nan     0.0482    0.0036
     2        0.0343             nan     0.0482    0.0032
     3        0.0312             nan     0.0482    0.0031
     4        0.0284             nan     0.0482    0.0026
     5        0.0259             nan     0.0482    0.0026
     6        0.0236             nan     0.0482    0.0023
     7        0.0215             nan     0.0482    0.0019
     8        0.0197             nan     0.0482    0.0018
     9        0.0179             nan     0.0482    0.0017
    10        0.0163             nan     0.0482    0.0016
    20        0.0066             nan     0.0482    0.0006
    40        0.0013             nan     0.0482    0.0001
    60        0.0003             nan     0.0482    0.0000
    80        0.0002             nan     0.0482    0.0000
   100        0.0001             nan     0.0482    0.0000
   120        0.0001             nan     0.0482    0.0000
   140        0.0001             nan     0.0482    0.0000
   160        0.0001             nan     0.0482    0.0000
   180        0.0001             nan     0.0482   -0.0000
   200        0.0001             nan     0.0482   -0.0000
   220        0.0001             nan     0.0482    0.0000
   240        0.0001             nan     0.0482   -0.0000
   260        0.0001             nan     0.0482   -0.0000
   280        0.0001             nan     0.0482   -0.0000
   300        0.0001             nan     0.0482   -0.0000
   320        0.0001             nan     0.0482   -0.0000
   340        0.0000             nan     0.0482   -0.0000
   360        0.0000             nan     0.0482   -0.0000
   380        0.0000             nan     0.0482   -0.0000
   400        0.0000             nan     0.0482   -0.0000
   420        0.0000             nan     0.0482   -0.0000
   440        0.0000             nan     0.0482   -0.0000
   460        0.0000             nan     0.0482   -0.0000
   480        0.0000             nan     0.0482   -0.0000
   500        0.0000             nan     0.0482   -0.0000
   520        0.0000             nan     0.0482   -0.0000
   540        0.0000             nan     0.0482   -0.0000
   560        0.0000             nan     0.0482   -0.0000
   580        0.0000             nan     0.0482   -0.0000
   600        0.0000             nan     0.0482   -0.0000
   620        0.0000             nan     0.0482   -0.0000
   640        0.0000             nan     0.0482   -0.0000
   660        0.0000             nan     0.0482   -0.0000
   680        0.0000             nan     0.0482   -0.0000
   700        0.0000             nan     0.0482   -0.0000
   720        0.0000             nan     0.0482   -0.0000
   740        0.0000             nan     0.0482   -0.0000
   760        0.0000             nan     0.0482   -0.0000
   780        0.0000             nan     0.0482   -0.0000
   800        0.0000             nan     0.0482   -0.0000
   820        0.0000             nan     0.0482   -0.0000
   840        0.0000             nan     0.0482   -0.0000
   860        0.0000             nan     0.0482   -0.0000
   880        0.0000             nan     0.0482   -0.0000
   900        0.0000             nan     0.0482   -0.0000
   920        0.0000             nan     0.0482   -0.0000
   940        0.0000             nan     0.0482   -0.0000
   960        0.0000             nan     0.0482   -0.0000
   980        0.0000             nan     0.0482   -0.0000
  1000        0.0000             nan     0.0482   -0.0000
  1020        0.0000             nan     0.0482   -0.0000
  1040        0.0000             nan     0.0482   -0.0000
  1060        0.0000             nan     0.0482   -0.0000
  1080        0.0000             nan     0.0482   -0.0000
  1100        0.0000             nan     0.0482   -0.0000
  1120        0.0000             nan     0.0482   -0.0000
  1140        0.0000             nan     0.0482   -0.0000
  1160        0.0000             nan     0.0482   -0.0000
  1180        0.0000             nan     0.0482   -0.0000
  1200        0.0000             nan     0.0482   -0.0000
  1220        0.0000             nan     0.0482   -0.0000
  1240        0.0000             nan     0.0482   -0.0000
  1260        0.0000             nan     0.0482   -0.0000
  1280        0.0000             nan     0.0482   -0.0000
  1300        0.0000             nan     0.0482   -0.0000
  1320        0.0000             nan     0.0482   -0.0000
  1340        0.0000             nan     0.0482   -0.0000
  1360        0.0000             nan     0.0482   -0.0000
  1380        0.0000             nan     0.0482   -0.0000
  1400        0.0000             nan     0.0482   -0.0000
  1420        0.0000             nan     0.0482   -0.0000
  1440        0.0000             nan     0.0482   -0.0000
  1460        0.0000             nan     0.0482   -0.0000
  1480        0.0000             nan     0.0482   -0.0000
  1500        0.0000             nan     0.0482   -0.0000
  1520        0.0000             nan     0.0482   -0.0000
  1540        0.0000             nan     0.0482   -0.0000
  1560        0.0000             nan     0.0482   -0.0000
  1580        0.0000             nan     0.0482   -0.0000
  1600        0.0000             nan     0.0482   -0.0000
  1620        0.0000             nan     0.0482   -0.0000
  1640        0.0000             nan     0.0482   -0.0000
  1660        0.0000             nan     0.0482   -0.0000
  1680        0.0000             nan     0.0482   -0.0000
  1700        0.0000             nan     0.0482   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0374             nan     0.0529    0.0040
     2        0.0337             nan     0.0529    0.0036
     3        0.0304             nan     0.0529    0.0034
     4        0.0274             nan     0.0529    0.0030
     5        0.0247             nan     0.0529    0.0025
     6        0.0222             nan     0.0529    0.0025
     7        0.0201             nan     0.0529    0.0019
     8        0.0181             nan     0.0529    0.0019
     9        0.0164             nan     0.0529    0.0017
    10        0.0148             nan     0.0529    0.0015
    20        0.0054             nan     0.0529    0.0006
    40        0.0008             nan     0.0529    0.0001
    60        0.0002             nan     0.0529    0.0000
    80        0.0001             nan     0.0529    0.0000
   100        0.0000             nan     0.0529    0.0000
   120        0.0000             nan     0.0529    0.0000
   140        0.0000             nan     0.0529   -0.0000
   160        0.0000             nan     0.0529   -0.0000
   180        0.0000             nan     0.0529    0.0000
   200        0.0000             nan     0.0529   -0.0000
   220        0.0000             nan     0.0529   -0.0000
   240        0.0000             nan     0.0529    0.0000
   260        0.0000             nan     0.0529   -0.0000
   280        0.0000             nan     0.0529   -0.0000
   300        0.0000             nan     0.0529   -0.0000
   320        0.0000             nan     0.0529   -0.0000
   340        0.0000             nan     0.0529   -0.0000
   360        0.0000             nan     0.0529   -0.0000
   380        0.0000             nan     0.0529   -0.0000
   400        0.0000             nan     0.0529   -0.0000
   420        0.0000             nan     0.0529   -0.0000
   440        0.0000             nan     0.0529   -0.0000
   460        0.0000             nan     0.0529   -0.0000
   480        0.0000             nan     0.0529   -0.0000
   500        0.0000             nan     0.0529   -0.0000
   520        0.0000             nan     0.0529   -0.0000
   540        0.0000             nan     0.0529   -0.0000
   560        0.0000             nan     0.0529   -0.0000
   580        0.0000             nan     0.0529   -0.0000
   600        0.0000             nan     0.0529   -0.0000
   620        0.0000             nan     0.0529   -0.0000
   640        0.0000             nan     0.0529   -0.0000
   660        0.0000             nan     0.0529   -0.0000
   680        0.0000             nan     0.0529   -0.0000
   700        0.0000             nan     0.0529   -0.0000
   720        0.0000             nan     0.0529   -0.0000
   740        0.0000             nan     0.0529   -0.0000
   760        0.0000             nan     0.0529   -0.0000
   780        0.0000             nan     0.0529   -0.0000
   800        0.0000             nan     0.0529   -0.0000
   820        0.0000             nan     0.0529   -0.0000
   840        0.0000             nan     0.0529   -0.0000
   860        0.0000             nan     0.0529   -0.0000
   880        0.0000             nan     0.0529   -0.0000
   900        0.0000             nan     0.0529   -0.0000
   920        0.0000             nan     0.0529   -0.0000
   940        0.0000             nan     0.0529   -0.0000
   960        0.0000             nan     0.0529   -0.0000
   980        0.0000             nan     0.0529   -0.0000
  1000        0.0000             nan     0.0529   -0.0000
  1020        0.0000             nan     0.0529   -0.0000
  1040        0.0000             nan     0.0529   -0.0000
  1060        0.0000             nan     0.0529   -0.0000
  1080        0.0000             nan     0.0529   -0.0000
  1100        0.0000             nan     0.0529   -0.0000
  1120        0.0000             nan     0.0529   -0.0000
  1140        0.0000             nan     0.0529   -0.0000
  1160        0.0000             nan     0.0529   -0.0000
  1180        0.0000             nan     0.0529   -0.0000
  1200        0.0000             nan     0.0529   -0.0000
  1220        0.0000             nan     0.0529   -0.0000
  1240        0.0000             nan     0.0529   -0.0000
  1260        0.0000             nan     0.0529   -0.0000
  1280        0.0000             nan     0.0529   -0.0000
  1300        0.0000             nan     0.0529   -0.0000
  1320        0.0000             nan     0.0529   -0.0000
  1340        0.0000             nan     0.0529   -0.0000
  1360        0.0000             nan     0.0529   -0.0000
  1380        0.0000             nan     0.0529   -0.0000
  1400        0.0000             nan     0.0529   -0.0000
  1420        0.0000             nan     0.0529   -0.0000
  1440        0.0000             nan     0.0529   -0.0000
  1460        0.0000             nan     0.0529   -0.0000
  1480        0.0000             nan     0.0529   -0.0000
  1500        0.0000             nan     0.0529   -0.0000
  1520        0.0000             nan     0.0529   -0.0000
  1540        0.0000             nan     0.0529   -0.0000
  1560        0.0000             nan     0.0529   -0.0000
  1580        0.0000             nan     0.0529   -0.0000
  1600        0.0000             nan     0.0529   -0.0000
  1620        0.0000             nan     0.0529   -0.0000
  1640        0.0000             nan     0.0529   -0.0000
  1660        0.0000             nan     0.0529   -0.0000
  1680        0.0000             nan     0.0529   -0.0000
  1700        0.0000             nan     0.0529   -0.0000
  1720        0.0000             nan     0.0529   -0.0000
  1740        0.0000             nan     0.0529   -0.0000
  1760        0.0000             nan     0.0529   -0.0000
  1780        0.0000             nan     0.0529   -0.0000
  1800        0.0000             nan     0.0529   -0.0000
  1820        0.0000             nan     0.0529   -0.0000
  1840        0.0000             nan     0.0529   -0.0000
  1860        0.0000             nan     0.0529   -0.0000
  1880        0.0000             nan     0.0529   -0.0000
  1900        0.0000             nan     0.0529   -0.0000
  1920        0.0000             nan     0.0529   -0.0000
  1940        0.0000             nan     0.0529   -0.0000
  1960        0.0000             nan     0.0529   -0.0000
  1980        0.0000             nan     0.0529   -0.0000
  2000        0.0000             nan     0.0529   -0.0000
  2020        0.0000             nan     0.0529   -0.0000
  2040        0.0000             nan     0.0529   -0.0000
  2060        0.0000             nan     0.0529   -0.0000
  2080        0.0000             nan     0.0529   -0.0000
  2100        0.0000             nan     0.0529   -0.0000
  2120        0.0000             nan     0.0529   -0.0000
  2140        0.0000             nan     0.0529   -0.0000
  2160        0.0000             nan     0.0529   -0.0000
  2180        0.0000             nan     0.0529   -0.0000
  2200        0.0000             nan     0.0529   -0.0000
  2220        0.0000             nan     0.0529   -0.0000
  2240        0.0000             nan     0.0529   -0.0000
  2260        0.0000             nan     0.0529   -0.0000
  2280        0.0000             nan     0.0529   -0.0000
  2300        0.0000             nan     0.0529   -0.0000
  2320        0.0000             nan     0.0529   -0.0000
  2340        0.0000             nan     0.0529   -0.0000
  2360        0.0000             nan     0.0529   -0.0000
  2380        0.0000             nan     0.0529   -0.0000
  2400        0.0000             nan     0.0529   -0.0000
  2420        0.0000             nan     0.0529   -0.0000
  2440        0.0000             nan     0.0529   -0.0000
  2460        0.0000             nan     0.0529   -0.0000
  2480        0.0000             nan     0.0529   -0.0000
  2500        0.0000             nan     0.0529   -0.0000
  2520        0.0000             nan     0.0529   -0.0000
  2540        0.0000             nan     0.0529   -0.0000
  2560        0.0000             nan     0.0529   -0.0000
  2580        0.0000             nan     0.0529   -0.0000
  2600        0.0000             nan     0.0529   -0.0000
  2620        0.0000             nan     0.0529   -0.0000
  2640        0.0000             nan     0.0529   -0.0000
  2660        0.0000             nan     0.0529   -0.0000
  2680        0.0000             nan     0.0529   -0.0000
  2700        0.0000             nan     0.0529   -0.0000
  2720        0.0000             nan     0.0529   -0.0000
  2740        0.0000             nan     0.0529   -0.0000
  2760        0.0000             nan     0.0529   -0.0000
  2780        0.0000             nan     0.0529   -0.0000
  2800        0.0000             nan     0.0529   -0.0000
  2820        0.0000             nan     0.0529   -0.0000
  2840        0.0000             nan     0.0529   -0.0000
  2860        0.0000             nan     0.0529   -0.0000
  2880        0.0000             nan     0.0529   -0.0000
  2900        0.0000             nan     0.0529   -0.0000
  2920        0.0000             nan     0.0529   -0.0000
  2940        0.0000             nan     0.0529   -0.0000
  2960        0.0000             nan     0.0529   -0.0000
  2980        0.0000             nan     0.0529   -0.0000
  3000        0.0000             nan     0.0529    0.0000
  3020        0.0000             nan     0.0529   -0.0000
  3040        0.0000             nan     0.0529   -0.0000
  3060        0.0000             nan     0.0529   -0.0000
  3080        0.0000             nan     0.0529   -0.0000
  3100        0.0000             nan     0.0529    0.0000
  3120        0.0000             nan     0.0529   -0.0000
  3140        0.0000             nan     0.0529   -0.0000
  3160        0.0000             nan     0.0529   -0.0000
  3180        0.0000             nan     0.0529   -0.0000
  3200        0.0000             nan     0.0529   -0.0000
  3220        0.0000             nan     0.0529   -0.0000
  3240        0.0000             nan     0.0529   -0.0000
  3260        0.0000             nan     0.0529   -0.0000
  3280        0.0000             nan     0.0529   -0.0000
  3300        0.0000             nan     0.0529   -0.0000
  3320        0.0000             nan     0.0529   -0.0000
  3340        0.0000             nan     0.0529   -0.0000
  3360        0.0000             nan     0.0529   -0.0000
  3380        0.0000             nan     0.0529    0.0000
  3400        0.0000             nan     0.0529   -0.0000
  3420        0.0000             nan     0.0529   -0.0000
  3440        0.0000             nan     0.0529   -0.0000
  3460        0.0000             nan     0.0529   -0.0000
  3480        0.0000             nan     0.0529   -0.0000
  3500        0.0000             nan     0.0529   -0.0000
  3520        0.0000             nan     0.0529   -0.0000
  3540        0.0000             nan     0.0529   -0.0000
  3560        0.0000             nan     0.0529   -0.0000
  3580        0.0000             nan     0.0529   -0.0000
  3600        0.0000             nan     0.0529   -0.0000
  3620        0.0000             nan     0.0529   -0.0000
  3640        0.0000             nan     0.0529   -0.0000
  3660        0.0000             nan     0.0529   -0.0000
  3680        0.0000             nan     0.0529   -0.0000
  3700        0.0000             nan     0.0529   -0.0000
  3720        0.0000             nan     0.0529   -0.0000
  3740        0.0000             nan     0.0529   -0.0000
  3760        0.0000             nan     0.0529   -0.0000
  3780        0.0000             nan     0.0529   -0.0000
  3800        0.0000             nan     0.0529   -0.0000
  3820        0.0000             nan     0.0529   -0.0000
  3840        0.0000             nan     0.0529   -0.0000
  3860        0.0000             nan     0.0529   -0.0000
  3880        0.0000             nan     0.0529   -0.0000
  3900        0.0000             nan     0.0529    0.0000
  3920        0.0000             nan     0.0529   -0.0000
  3940        0.0000             nan     0.0529   -0.0000
  3960        0.0000             nan     0.0529   -0.0000
  3980        0.0000             nan     0.0529   -0.0000
  3983        0.0000             nan     0.0529    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0371             nan     0.0560    0.0041
     2        0.0332             nan     0.0560    0.0039
     3        0.0298             nan     0.0560    0.0034
     4        0.0268             nan     0.0560    0.0031
     5        0.0240             nan     0.0560    0.0028
     6        0.0216             nan     0.0560    0.0025
     7        0.0195             nan     0.0560    0.0023
     8        0.0175             nan     0.0560    0.0018
     9        0.0157             nan     0.0560    0.0017
    10        0.0142             nan     0.0560    0.0014
    20        0.0051             nan     0.0560    0.0005
    40        0.0009             nan     0.0560    0.0001
    60        0.0004             nan     0.0560    0.0000
    80        0.0002             nan     0.0560    0.0000
   100        0.0002             nan     0.0560   -0.0000
   120        0.0002             nan     0.0560   -0.0000
   140        0.0002             nan     0.0560   -0.0000
   160        0.0001             nan     0.0560   -0.0000
   180        0.0001             nan     0.0560   -0.0000
   200        0.0001             nan     0.0560   -0.0000
   220        0.0001             nan     0.0560   -0.0000
   240        0.0001             nan     0.0560   -0.0000
   260        0.0001             nan     0.0560   -0.0000
   280        0.0001             nan     0.0560   -0.0000
   300        0.0001             nan     0.0560   -0.0000
   320        0.0001             nan     0.0560   -0.0000
   340        0.0001             nan     0.0560   -0.0000
   360        0.0001             nan     0.0560   -0.0000
   380        0.0001             nan     0.0560   -0.0000
   400        0.0001             nan     0.0560   -0.0000
   420        0.0001             nan     0.0560   -0.0000
   440        0.0001             nan     0.0560   -0.0000
   460        0.0001             nan     0.0560   -0.0000
   480        0.0001             nan     0.0560   -0.0000
   500        0.0001             nan     0.0560   -0.0000
   520        0.0001             nan     0.0560   -0.0000
   540        0.0001             nan     0.0560   -0.0000
   560        0.0001             nan     0.0560   -0.0000
   580        0.0001             nan     0.0560   -0.0000
   600        0.0001             nan     0.0560   -0.0000
   620        0.0001             nan     0.0560   -0.0000
   640        0.0001             nan     0.0560   -0.0000
   660        0.0000             nan     0.0560   -0.0000
   680        0.0000             nan     0.0560   -0.0000
   700        0.0000             nan     0.0560   -0.0000
   720        0.0000             nan     0.0560   -0.0000
   740        0.0000             nan     0.0560   -0.0000
   760        0.0000             nan     0.0560   -0.0000
   780        0.0000             nan     0.0560   -0.0000
   800        0.0000             nan     0.0560   -0.0000
   820        0.0000             nan     0.0560   -0.0000
   840        0.0000             nan     0.0560   -0.0000
   860        0.0000             nan     0.0560   -0.0000
   880        0.0000             nan     0.0560   -0.0000
   900        0.0000             nan     0.0560   -0.0000
   920        0.0000             nan     0.0560   -0.0000
   940        0.0000             nan     0.0560   -0.0000
   960        0.0000             nan     0.0560   -0.0000
   980        0.0000             nan     0.0560   -0.0000
  1000        0.0000             nan     0.0560   -0.0000
  1020        0.0000             nan     0.0560   -0.0000
  1040        0.0000             nan     0.0560   -0.0000
  1060        0.0000             nan     0.0560   -0.0000
  1080        0.0000             nan     0.0560   -0.0000
  1100        0.0000             nan     0.0560   -0.0000
  1120        0.0000             nan     0.0560   -0.0000
  1140        0.0000             nan     0.0560   -0.0000
  1160        0.0000             nan     0.0560   -0.0000
  1180        0.0000             nan     0.0560   -0.0000
  1200        0.0000             nan     0.0560   -0.0000
  1220        0.0000             nan     0.0560   -0.0000
  1240        0.0000             nan     0.0560   -0.0000
  1260        0.0000             nan     0.0560   -0.0000
  1280        0.0000             nan     0.0560   -0.0000
  1300        0.0000             nan     0.0560   -0.0000
  1320        0.0000             nan     0.0560   -0.0000
  1340        0.0000             nan     0.0560   -0.0000
  1360        0.0000             nan     0.0560   -0.0000
  1380        0.0000             nan     0.0560   -0.0000
  1400        0.0000             nan     0.0560   -0.0000
  1420        0.0000             nan     0.0560   -0.0000
  1440        0.0000             nan     0.0560   -0.0000
  1460        0.0000             nan     0.0560   -0.0000
  1480        0.0000             nan     0.0560   -0.0000
  1500        0.0000             nan     0.0560   -0.0000
  1520        0.0000             nan     0.0560   -0.0000
  1540        0.0000             nan     0.0560   -0.0000
  1560        0.0000             nan     0.0560   -0.0000
  1580        0.0000             nan     0.0560   -0.0000
  1600        0.0000             nan     0.0560   -0.0000
  1620        0.0000             nan     0.0560   -0.0000
  1640        0.0000             nan     0.0560   -0.0000
  1660        0.0000             nan     0.0560   -0.0000
  1680        0.0000             nan     0.0560   -0.0000
  1700        0.0000             nan     0.0560   -0.0000
  1720        0.0000             nan     0.0560   -0.0000
  1740        0.0000             nan     0.0560   -0.0000
  1760        0.0000             nan     0.0560   -0.0000
  1780        0.0000             nan     0.0560   -0.0000
  1800        0.0000             nan     0.0560   -0.0000
  1820        0.0000             nan     0.0560   -0.0000
  1840        0.0000             nan     0.0560   -0.0000
  1860        0.0000             nan     0.0560   -0.0000
  1880        0.0000             nan     0.0560   -0.0000
  1900        0.0000             nan     0.0560   -0.0000
  1920        0.0000             nan     0.0560   -0.0000
  1940        0.0000             nan     0.0560   -0.0000
  1960        0.0000             nan     0.0560   -0.0000
  1980        0.0000             nan     0.0560   -0.0000
  2000        0.0000             nan     0.0560   -0.0000
  2020        0.0000             nan     0.0560   -0.0000
  2040        0.0000             nan     0.0560   -0.0000
  2060        0.0000             nan     0.0560   -0.0000
  2080        0.0000             nan     0.0560   -0.0000
  2100        0.0000             nan     0.0560   -0.0000
  2120        0.0000             nan     0.0560   -0.0000
  2140        0.0000             nan     0.0560   -0.0000
  2160        0.0000             nan     0.0560   -0.0000
  2180        0.0000             nan     0.0560   -0.0000
  2200        0.0000             nan     0.0560   -0.0000
  2220        0.0000             nan     0.0560   -0.0000
  2240        0.0000             nan     0.0560   -0.0000
  2260        0.0000             nan     0.0560   -0.0000
  2280        0.0000             nan     0.0560   -0.0000
  2300        0.0000             nan     0.0560   -0.0000
  2320        0.0000             nan     0.0560   -0.0000
  2340        0.0000             nan     0.0560   -0.0000
  2360        0.0000             nan     0.0560   -0.0000
  2380        0.0000             nan     0.0560   -0.0000
  2400        0.0000             nan     0.0560   -0.0000
  2420        0.0000             nan     0.0560   -0.0000
  2440        0.0000             nan     0.0560   -0.0000
  2460        0.0000             nan     0.0560   -0.0000
  2480        0.0000             nan     0.0560   -0.0000
  2500        0.0000             nan     0.0560   -0.0000
  2520        0.0000             nan     0.0560   -0.0000
  2540        0.0000             nan     0.0560   -0.0000
  2560        0.0000             nan     0.0560   -0.0000
  2580        0.0000             nan     0.0560   -0.0000
  2600        0.0000             nan     0.0560   -0.0000
  2620        0.0000             nan     0.0560   -0.0000
  2640        0.0000             nan     0.0560   -0.0000
  2660        0.0000             nan     0.0560   -0.0000
  2680        0.0000             nan     0.0560   -0.0000
  2700        0.0000             nan     0.0560   -0.0000
  2720        0.0000             nan     0.0560   -0.0000
  2740        0.0000             nan     0.0560   -0.0000
  2760        0.0000             nan     0.0560   -0.0000
  2780        0.0000             nan     0.0560   -0.0000
  2800        0.0000             nan     0.0560   -0.0000
  2820        0.0000             nan     0.0560   -0.0000
  2840        0.0000             nan     0.0560   -0.0000
  2860        0.0000             nan     0.0560   -0.0000
  2880        0.0000             nan     0.0560   -0.0000
  2900        0.0000             nan     0.0560   -0.0000
  2920        0.0000             nan     0.0560   -0.0000
  2940        0.0000             nan     0.0560   -0.0000
  2960        0.0000             nan     0.0560   -0.0000
  2980        0.0000             nan     0.0560   -0.0000
  3000        0.0000             nan     0.0560   -0.0000
  3020        0.0000             nan     0.0560   -0.0000
  3040        0.0000             nan     0.0560   -0.0000
  3060        0.0000             nan     0.0560   -0.0000
  3080        0.0000             nan     0.0560   -0.0000
  3100        0.0000             nan     0.0560   -0.0000
  3120        0.0000             nan     0.0560   -0.0000
  3140        0.0000             nan     0.0560   -0.0000
  3160        0.0000             nan     0.0560   -0.0000
  3180        0.0000             nan     0.0560   -0.0000
  3200        0.0000             nan     0.0560   -0.0000
  3220        0.0000             nan     0.0560   -0.0000
  3240        0.0000             nan     0.0560   -0.0000
  3260        0.0000             nan     0.0560   -0.0000
  3280        0.0000             nan     0.0560   -0.0000
  3300        0.0000             nan     0.0560   -0.0000
  3320        0.0000             nan     0.0560   -0.0000
  3340        0.0000             nan     0.0560   -0.0000
  3360        0.0000             nan     0.0560   -0.0000
  3380        0.0000             nan     0.0560   -0.0000
  3400        0.0000             nan     0.0560   -0.0000
  3420        0.0000             nan     0.0560   -0.0000
  3440        0.0000             nan     0.0560   -0.0000
  3460        0.0000             nan     0.0560   -0.0000
  3480        0.0000             nan     0.0560   -0.0000
  3500        0.0000             nan     0.0560   -0.0000
  3520        0.0000             nan     0.0560   -0.0000
  3540        0.0000             nan     0.0560   -0.0000
  3560        0.0000             nan     0.0560   -0.0000
  3580        0.0000             nan     0.0560   -0.0000
  3600        0.0000             nan     0.0560   -0.0000
  3620        0.0000             nan     0.0560   -0.0000
  3640        0.0000             nan     0.0560   -0.0000
  3660        0.0000             nan     0.0560   -0.0000
  3680        0.0000             nan     0.0560   -0.0000
  3700        0.0000             nan     0.0560   -0.0000
  3720        0.0000             nan     0.0560   -0.0000
  3740        0.0000             nan     0.0560   -0.0000
  3760        0.0000             nan     0.0560   -0.0000
  3780        0.0000             nan     0.0560   -0.0000
  3800        0.0000             nan     0.0560   -0.0000
  3820        0.0000             nan     0.0560   -0.0000
  3840        0.0000             nan     0.0560   -0.0000
  3860        0.0000             nan     0.0560   -0.0000
  3880        0.0000             nan     0.0560   -0.0000
  3900        0.0000             nan     0.0560   -0.0000
  3905        0.0000             nan     0.0560   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0361             nan     0.0700    0.0055
     2        0.0316             nan     0.0700    0.0048
     3        0.0275             nan     0.0700    0.0043
     4        0.0241             nan     0.0700    0.0035
     5        0.0212             nan     0.0700    0.0028
     6        0.0186             nan     0.0700    0.0025
     7        0.0163             nan     0.0700    0.0023
     8        0.0143             nan     0.0700    0.0017
     9        0.0126             nan     0.0700    0.0017
    10        0.0110             nan     0.0700    0.0015
    20        0.0033             nan     0.0700    0.0004
    40        0.0005             nan     0.0700    0.0000
    60        0.0003             nan     0.0700    0.0000
    80        0.0002             nan     0.0700    0.0000
   100        0.0002             nan     0.0700    0.0000
   120        0.0001             nan     0.0700   -0.0000
   140        0.0001             nan     0.0700   -0.0000
   160        0.0001             nan     0.0700   -0.0000
   180        0.0001             nan     0.0700   -0.0000
   200        0.0001             nan     0.0700   -0.0000
   220        0.0001             nan     0.0700   -0.0000
   240        0.0001             nan     0.0700   -0.0000
   260        0.0001             nan     0.0700   -0.0000
   280        0.0001             nan     0.0700   -0.0000
   300        0.0001             nan     0.0700   -0.0000
   320        0.0001             nan     0.0700   -0.0000
   340        0.0001             nan     0.0700   -0.0000
   360        0.0001             nan     0.0700   -0.0000
   380        0.0001             nan     0.0700   -0.0000
   400        0.0001             nan     0.0700   -0.0000
   420        0.0001             nan     0.0700   -0.0000
   440        0.0001             nan     0.0700   -0.0000
   460        0.0001             nan     0.0700   -0.0000
   480        0.0001             nan     0.0700   -0.0000
   500        0.0001             nan     0.0700   -0.0000
   520        0.0001             nan     0.0700   -0.0000
   540        0.0000             nan     0.0700   -0.0000
   560        0.0000             nan     0.0700   -0.0000
   580        0.0000             nan     0.0700   -0.0000
   600        0.0000             nan     0.0700   -0.0000
   620        0.0000             nan     0.0700   -0.0000
   640        0.0000             nan     0.0700   -0.0000
   660        0.0000             nan     0.0700   -0.0000
   680        0.0000             nan     0.0700   -0.0000
   700        0.0000             nan     0.0700   -0.0000
   720        0.0000             nan     0.0700   -0.0000
   740        0.0000             nan     0.0700   -0.0000
   760        0.0000             nan     0.0700   -0.0000
   780        0.0000             nan     0.0700   -0.0000
   800        0.0000             nan     0.0700   -0.0000
   820        0.0000             nan     0.0700   -0.0000
   840        0.0000             nan     0.0700   -0.0000
   860        0.0000             nan     0.0700   -0.0000
   880        0.0000             nan     0.0700   -0.0000
   900        0.0000             nan     0.0700   -0.0000
   920        0.0000             nan     0.0700   -0.0000
   940        0.0000             nan     0.0700   -0.0000
   960        0.0000             nan     0.0700   -0.0000
   980        0.0000             nan     0.0700   -0.0000
  1000        0.0000             nan     0.0700   -0.0000
  1020        0.0000             nan     0.0700   -0.0000
  1040        0.0000             nan     0.0700   -0.0000
  1060        0.0000             nan     0.0700   -0.0000
  1080        0.0000             nan     0.0700   -0.0000
  1100        0.0000             nan     0.0700   -0.0000
  1120        0.0000             nan     0.0700   -0.0000
  1140        0.0000             nan     0.0700   -0.0000
  1160        0.0000             nan     0.0700   -0.0000
  1180        0.0000             nan     0.0700   -0.0000
  1200        0.0000             nan     0.0700   -0.0000
  1220        0.0000             nan     0.0700   -0.0000
  1240        0.0000             nan     0.0700   -0.0000
  1260        0.0000             nan     0.0700   -0.0000
  1280        0.0000             nan     0.0700   -0.0000
  1300        0.0000             nan     0.0700   -0.0000
  1320        0.0000             nan     0.0700   -0.0000
  1340        0.0000             nan     0.0700   -0.0000
  1360        0.0000             nan     0.0700   -0.0000
  1380        0.0000             nan     0.0700   -0.0000
  1400        0.0000             nan     0.0700   -0.0000
  1420        0.0000             nan     0.0700   -0.0000
  1440        0.0000             nan     0.0700   -0.0000
  1460        0.0000             nan     0.0700   -0.0000
  1480        0.0000             nan     0.0700   -0.0000
  1500        0.0000             nan     0.0700   -0.0000
  1520        0.0000             nan     0.0700   -0.0000
  1540        0.0000             nan     0.0700   -0.0000
  1560        0.0000             nan     0.0700   -0.0000
  1580        0.0000             nan     0.0700   -0.0000
  1600        0.0000             nan     0.0700   -0.0000
  1620        0.0000             nan     0.0700   -0.0000
  1640        0.0000             nan     0.0700   -0.0000
  1660        0.0000             nan     0.0700   -0.0000
  1680        0.0000             nan     0.0700   -0.0000
  1700        0.0000             nan     0.0700   -0.0000
  1720        0.0000             nan     0.0700   -0.0000
  1740        0.0000             nan     0.0700   -0.0000
  1760        0.0000             nan     0.0700   -0.0000
  1765        0.0000             nan     0.0700   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0375             nan     0.0760    0.0041
     2        0.0340             nan     0.0760    0.0035
     3        0.0308             nan     0.0760    0.0030
     4        0.0282             nan     0.0760    0.0026
     5        0.0258             nan     0.0760    0.0024
     6        0.0236             nan     0.0760    0.0021
     7        0.0217             nan     0.0760    0.0020
     8        0.0200             nan     0.0760    0.0017
     9        0.0184             nan     0.0760    0.0014
    10        0.0169             nan     0.0760    0.0013
    20        0.0083             nan     0.0760    0.0006
    40        0.0025             nan     0.0760    0.0001
    60        0.0010             nan     0.0760    0.0000
    80        0.0005             nan     0.0760    0.0000
   100        0.0003             nan     0.0760    0.0000
   120        0.0002             nan     0.0760    0.0000
   140        0.0002             nan     0.0760    0.0000
   160        0.0002             nan     0.0760    0.0000
   180        0.0002             nan     0.0760   -0.0000
   200        0.0001             nan     0.0760    0.0000
   220        0.0001             nan     0.0760    0.0000
   240        0.0001             nan     0.0760   -0.0000
   260        0.0001             nan     0.0760    0.0000
   280        0.0001             nan     0.0760   -0.0000
   300        0.0001             nan     0.0760    0.0000
   320        0.0001             nan     0.0760   -0.0000
   340        0.0001             nan     0.0760   -0.0000
   360        0.0001             nan     0.0760   -0.0000
   380        0.0001             nan     0.0760   -0.0000
   400        0.0001             nan     0.0760   -0.0000
   420        0.0001             nan     0.0760   -0.0000
   440        0.0001             nan     0.0760   -0.0000
   460        0.0001             nan     0.0760   -0.0000
   480        0.0001             nan     0.0760   -0.0000
   500        0.0001             nan     0.0760   -0.0000
   520        0.0001             nan     0.0760   -0.0000
   540        0.0001             nan     0.0760   -0.0000
   560        0.0001             nan     0.0760   -0.0000
   580        0.0001             nan     0.0760   -0.0000
   600        0.0001             nan     0.0760   -0.0000
   620        0.0001             nan     0.0760   -0.0000
   640        0.0001             nan     0.0760   -0.0000
   660        0.0001             nan     0.0760   -0.0000
   680        0.0001             nan     0.0760   -0.0000
   700        0.0001             nan     0.0760   -0.0000
   720        0.0001             nan     0.0760   -0.0000
   740        0.0001             nan     0.0760   -0.0000
   760        0.0001             nan     0.0760    0.0000
   780        0.0001             nan     0.0760   -0.0000
   800        0.0001             nan     0.0760   -0.0000
   820        0.0001             nan     0.0760   -0.0000
   840        0.0001             nan     0.0760   -0.0000
   860        0.0001             nan     0.0760   -0.0000
   880        0.0001             nan     0.0760   -0.0000
   900        0.0001             nan     0.0760   -0.0000
   920        0.0001             nan     0.0760   -0.0000
   940        0.0001             nan     0.0760   -0.0000
   960        0.0001             nan     0.0760   -0.0000
   980        0.0001             nan     0.0760   -0.0000
  1000        0.0001             nan     0.0760   -0.0000
  1020        0.0001             nan     0.0760   -0.0000
  1040        0.0001             nan     0.0760   -0.0000
  1060        0.0001             nan     0.0760   -0.0000
  1080        0.0001             nan     0.0760   -0.0000
  1100        0.0001             nan     0.0760   -0.0000
  1120        0.0001             nan     0.0760   -0.0000
  1140        0.0001             nan     0.0760   -0.0000
  1160        0.0001             nan     0.0760   -0.0000
  1180        0.0001             nan     0.0760   -0.0000
  1200        0.0001             nan     0.0760   -0.0000
  1220        0.0001             nan     0.0760   -0.0000
  1240        0.0001             nan     0.0760   -0.0000
  1260        0.0001             nan     0.0760   -0.0000
  1280        0.0001             nan     0.0760   -0.0000
  1300        0.0001             nan     0.0760   -0.0000
  1320        0.0001             nan     0.0760   -0.0000
  1340        0.0001             nan     0.0760   -0.0000
  1360        0.0001             nan     0.0760   -0.0000
  1380        0.0001             nan     0.0760   -0.0000
  1400        0.0001             nan     0.0760   -0.0000
  1420        0.0001             nan     0.0760   -0.0000
  1440        0.0001             nan     0.0760   -0.0000
  1460        0.0001             nan     0.0760   -0.0000
  1480        0.0001             nan     0.0760   -0.0000
  1500        0.0001             nan     0.0760   -0.0000
  1520        0.0001             nan     0.0760   -0.0000
  1540        0.0001             nan     0.0760   -0.0000
  1560        0.0001             nan     0.0760   -0.0000
  1580        0.0001             nan     0.0760   -0.0000
  1600        0.0001             nan     0.0760   -0.0000
  1620        0.0001             nan     0.0760   -0.0000
  1640        0.0001             nan     0.0760   -0.0000
  1660        0.0001             nan     0.0760   -0.0000
  1680        0.0001             nan     0.0760   -0.0000
  1700        0.0001             nan     0.0760   -0.0000
  1720        0.0001             nan     0.0760   -0.0000
  1740        0.0001             nan     0.0760   -0.0000
  1760        0.0001             nan     0.0760   -0.0000
  1780        0.0001             nan     0.0760   -0.0000
  1800        0.0001             nan     0.0760   -0.0000
  1820        0.0001             nan     0.0760   -0.0000
  1840        0.0001             nan     0.0760   -0.0000
  1860        0.0001             nan     0.0760   -0.0000
  1880        0.0001             nan     0.0760   -0.0000
  1900        0.0001             nan     0.0760   -0.0000
  1920        0.0001             nan     0.0760   -0.0000
  1940        0.0001             nan     0.0760   -0.0000
  1960        0.0001             nan     0.0760   -0.0000
  1980        0.0001             nan     0.0760   -0.0000
  2000        0.0001             nan     0.0760   -0.0000
  2020        0.0000             nan     0.0760   -0.0000
  2040        0.0000             nan     0.0760   -0.0000
  2060        0.0000             nan     0.0760   -0.0000
  2080        0.0000             nan     0.0760   -0.0000
  2100        0.0000             nan     0.0760   -0.0000
  2120        0.0000             nan     0.0760   -0.0000
  2140        0.0000             nan     0.0760   -0.0000
  2160        0.0000             nan     0.0760   -0.0000
  2180        0.0000             nan     0.0760   -0.0000
  2200        0.0000             nan     0.0760   -0.0000
  2220        0.0000             nan     0.0760   -0.0000
  2240        0.0000             nan     0.0760   -0.0000
  2260        0.0000             nan     0.0760   -0.0000
  2280        0.0000             nan     0.0760   -0.0000
  2300        0.0000             nan     0.0760   -0.0000
  2320        0.0000             nan     0.0760   -0.0000
  2340        0.0000             nan     0.0760   -0.0000
  2360        0.0000             nan     0.0760   -0.0000
  2380        0.0000             nan     0.0760   -0.0000
  2400        0.0000             nan     0.0760   -0.0000
  2420        0.0000             nan     0.0760   -0.0000
  2440        0.0000             nan     0.0760   -0.0000
  2460        0.0000             nan     0.0760   -0.0000
  2480        0.0000             nan     0.0760   -0.0000
  2500        0.0000             nan     0.0760   -0.0000
  2520        0.0000             nan     0.0760   -0.0000
  2540        0.0000             nan     0.0760   -0.0000
  2560        0.0000             nan     0.0760   -0.0000
  2580        0.0000             nan     0.0760   -0.0000
  2600        0.0000             nan     0.0760   -0.0000
  2620        0.0000             nan     0.0760   -0.0000
  2640        0.0000             nan     0.0760   -0.0000
  2660        0.0000             nan     0.0760   -0.0000
  2680        0.0000             nan     0.0760   -0.0000
  2700        0.0000             nan     0.0760   -0.0000
  2720        0.0000             nan     0.0760   -0.0000
  2740        0.0000             nan     0.0760   -0.0000
  2760        0.0000             nan     0.0760   -0.0000
  2780        0.0000             nan     0.0760   -0.0000
  2800        0.0000             nan     0.0760   -0.0000
  2820        0.0000             nan     0.0760   -0.0000
  2840        0.0000             nan     0.0760   -0.0000
  2860        0.0000             nan     0.0760   -0.0000
  2880        0.0000             nan     0.0760   -0.0000
  2900        0.0000             nan     0.0760   -0.0000
  2920        0.0000             nan     0.0760   -0.0000
  2940        0.0000             nan     0.0760   -0.0000
  2960        0.0000             nan     0.0760   -0.0000
  2980        0.0000             nan     0.0760   -0.0000
  3000        0.0000             nan     0.0760   -0.0000
  3020        0.0000             nan     0.0760   -0.0000
  3040        0.0000             nan     0.0760   -0.0000
  3060        0.0000             nan     0.0760   -0.0000
  3080        0.0000             nan     0.0760   -0.0000
  3100        0.0000             nan     0.0760   -0.0000
  3120        0.0000             nan     0.0760   -0.0000
  3140        0.0000             nan     0.0760   -0.0000
  3160        0.0000             nan     0.0760   -0.0000
  3180        0.0000             nan     0.0760   -0.0000
  3200        0.0000             nan     0.0760   -0.0000
  3220        0.0000             nan     0.0760   -0.0000
  3240        0.0000             nan     0.0760   -0.0000
  3260        0.0000             nan     0.0760   -0.0000
  3280        0.0000             nan     0.0760   -0.0000
  3300        0.0000             nan     0.0760   -0.0000
  3320        0.0000             nan     0.0760   -0.0000
  3340        0.0000             nan     0.0760   -0.0000
  3360        0.0000             nan     0.0760   -0.0000
  3380        0.0000             nan     0.0760    0.0000
  3400        0.0000             nan     0.0760   -0.0000
  3420        0.0000             nan     0.0760   -0.0000
  3440        0.0000             nan     0.0760   -0.0000
  3460        0.0000             nan     0.0760   -0.0000
  3480        0.0000             nan     0.0760   -0.0000
  3500        0.0000             nan     0.0760   -0.0000
  3520        0.0000             nan     0.0760   -0.0000
  3540        0.0000             nan     0.0760   -0.0000
  3560        0.0000             nan     0.0760   -0.0000
  3580        0.0000             nan     0.0760   -0.0000
  3600        0.0000             nan     0.0760   -0.0000
  3620        0.0000             nan     0.0760   -0.0000
  3640        0.0000             nan     0.0760   -0.0000
  3660        0.0000             nan     0.0760   -0.0000
  3680        0.0000             nan     0.0760   -0.0000
  3700        0.0000             nan     0.0760   -0.0000
  3720        0.0000             nan     0.0760   -0.0000
  3740        0.0000             nan     0.0760   -0.0000
  3760        0.0000             nan     0.0760   -0.0000
  3780        0.0000             nan     0.0760   -0.0000
  3800        0.0000             nan     0.0760   -0.0000
  3820        0.0000             nan     0.0760   -0.0000
  3840        0.0000             nan     0.0760   -0.0000
  3860        0.0000             nan     0.0760   -0.0000
  3880        0.0000             nan     0.0760   -0.0000
  3900        0.0000             nan     0.0760   -0.0000
  3920        0.0000             nan     0.0760   -0.0000
  3940        0.0000             nan     0.0760   -0.0000
  3960        0.0000             nan     0.0760   -0.0000
  3980        0.0000             nan     0.0760   -0.0000
  4000        0.0000             nan     0.0760   -0.0000
  4020        0.0000             nan     0.0760   -0.0000
  4040        0.0000             nan     0.0760   -0.0000
  4060        0.0000             nan     0.0760   -0.0000
  4080        0.0000             nan     0.0760   -0.0000
  4100        0.0000             nan     0.0760   -0.0000
  4120        0.0000             nan     0.0760   -0.0000
  4140        0.0000             nan     0.0760   -0.0000
  4160        0.0000             nan     0.0760   -0.0000
  4180        0.0000             nan     0.0760   -0.0000
  4200        0.0000             nan     0.0760   -0.0000
  4220        0.0000             nan     0.0760   -0.0000
  4240        0.0000             nan     0.0760   -0.0000
  4260        0.0000             nan     0.0760   -0.0000
  4280        0.0000             nan     0.0760   -0.0000
  4300        0.0000             nan     0.0760   -0.0000
  4320        0.0000             nan     0.0760   -0.0000
  4340        0.0000             nan     0.0760   -0.0000
  4360        0.0000             nan     0.0760   -0.0000
  4380        0.0000             nan     0.0760   -0.0000
  4400        0.0000             nan     0.0760   -0.0000
  4420        0.0000             nan     0.0760   -0.0000
  4440        0.0000             nan     0.0760   -0.0000
  4460        0.0000             nan     0.0760   -0.0000
  4480        0.0000             nan     0.0760   -0.0000
  4500        0.0000             nan     0.0760   -0.0000
  4520        0.0000             nan     0.0760   -0.0000
  4540        0.0000             nan     0.0760   -0.0000
  4560        0.0000             nan     0.0760   -0.0000
  4580        0.0000             nan     0.0760   -0.0000
  4600        0.0000             nan     0.0760   -0.0000
  4620        0.0000             nan     0.0760   -0.0000
  4640        0.0000             nan     0.0760   -0.0000
  4660        0.0000             nan     0.0760   -0.0000
  4680        0.0000             nan     0.0760   -0.0000
  4700        0.0000             nan     0.0760   -0.0000
  4720        0.0000             nan     0.0760   -0.0000
  4740        0.0000             nan     0.0760   -0.0000
  4760        0.0000             nan     0.0760   -0.0000
  4780        0.0000             nan     0.0760   -0.0000
  4800        0.0000             nan     0.0760    0.0000
  4820        0.0000             nan     0.0760   -0.0000
  4840        0.0000             nan     0.0760   -0.0000
  4860        0.0000             nan     0.0760   -0.0000
  4880        0.0000             nan     0.0760   -0.0000
  4900        0.0000             nan     0.0760   -0.0000
  4920        0.0000             nan     0.0760   -0.0000
  4940        0.0000             nan     0.0760   -0.0000
  4960        0.0000             nan     0.0760   -0.0000
  4980        0.0000             nan     0.0760   -0.0000
  4982        0.0000             nan     0.0760   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0350             nan     0.0850    0.0067
     2        0.0296             nan     0.0850    0.0051
     3        0.0250             nan     0.0850    0.0050
     4        0.0211             nan     0.0850    0.0041
     5        0.0179             nan     0.0850    0.0034
     6        0.0152             nan     0.0850    0.0026
     7        0.0129             nan     0.0850    0.0026
     8        0.0110             nan     0.0850    0.0018
     9        0.0093             nan     0.0850    0.0017
    10        0.0079             nan     0.0850    0.0014
    20        0.0017             nan     0.0850    0.0003
    40        0.0001             nan     0.0850    0.0000
    60        0.0000             nan     0.0850    0.0000
    80        0.0000             nan     0.0850    0.0000
   100        0.0000             nan     0.0850   -0.0000
   120        0.0000             nan     0.0850   -0.0000
   140        0.0000             nan     0.0850   -0.0000
   160        0.0000             nan     0.0850   -0.0000
   180        0.0000             nan     0.0850   -0.0000
   200        0.0000             nan     0.0850   -0.0000
   220        0.0000             nan     0.0850   -0.0000
   240        0.0000             nan     0.0850   -0.0000
   260        0.0000             nan     0.0850   -0.0000
   280        0.0000             nan     0.0850   -0.0000
   300        0.0000             nan     0.0850   -0.0000
   320        0.0000             nan     0.0850   -0.0000
   340        0.0000             nan     0.0850   -0.0000
   360        0.0000             nan     0.0850   -0.0000
   380        0.0000             nan     0.0850   -0.0000
   400        0.0000             nan     0.0850   -0.0000
   420        0.0000             nan     0.0850   -0.0000
   440        0.0000             nan     0.0850   -0.0000
   460        0.0000             nan     0.0850   -0.0000
   480        0.0000             nan     0.0850   -0.0000
   500        0.0000             nan     0.0850   -0.0000
   520        0.0000             nan     0.0850   -0.0000
   540        0.0000             nan     0.0850   -0.0000
   560        0.0000             nan     0.0850   -0.0000
   580        0.0000             nan     0.0850   -0.0000
   600        0.0000             nan     0.0850   -0.0000
   620        0.0000             nan     0.0850   -0.0000
   640        0.0000             nan     0.0850   -0.0000
   660        0.0000             nan     0.0850   -0.0000
   680        0.0000             nan     0.0850   -0.0000
   700        0.0000             nan     0.0850   -0.0000
   720        0.0000             nan     0.0850   -0.0000
   740        0.0000             nan     0.0850   -0.0000
   760        0.0000             nan     0.0850   -0.0000
   780        0.0000             nan     0.0850   -0.0000
   800        0.0000             nan     0.0850   -0.0000
   820        0.0000             nan     0.0850   -0.0000
   840        0.0000             nan     0.0850   -0.0000
   860        0.0000             nan     0.0850   -0.0000
   880        0.0000             nan     0.0850   -0.0000
   900        0.0000             nan     0.0850   -0.0000
   920        0.0000             nan     0.0850   -0.0000
   940        0.0000             nan     0.0850   -0.0000
   960        0.0000             nan     0.0850   -0.0000
   980        0.0000             nan     0.0850   -0.0000
  1000        0.0000             nan     0.0850   -0.0000
  1020        0.0000             nan     0.0850   -0.0000
  1040        0.0000             nan     0.0850   -0.0000
  1060        0.0000             nan     0.0850   -0.0000
  1080        0.0000             nan     0.0850   -0.0000
  1100        0.0000             nan     0.0850   -0.0000
  1120        0.0000             nan     0.0850   -0.0000
  1140        0.0000             nan     0.0850   -0.0000
  1160        0.0000             nan     0.0850   -0.0000
  1180        0.0000             nan     0.0850   -0.0000
  1200        0.0000             nan     0.0850   -0.0000
  1220        0.0000             nan     0.0850   -0.0000
  1240        0.0000             nan     0.0850   -0.0000
  1260        0.0000             nan     0.0850   -0.0000
  1280        0.0000             nan     0.0850   -0.0000
  1300        0.0000             nan     0.0850   -0.0000
  1320        0.0000             nan     0.0850   -0.0000
  1340        0.0000             nan     0.0850   -0.0000
  1360        0.0000             nan     0.0850   -0.0000
  1380        0.0000             nan     0.0850   -0.0000
  1400        0.0000             nan     0.0850   -0.0000
  1420        0.0000             nan     0.0850   -0.0000
  1440        0.0000             nan     0.0850   -0.0000
  1460        0.0000             nan     0.0850   -0.0000
  1480        0.0000             nan     0.0850   -0.0000
  1500        0.0000             nan     0.0850   -0.0000
  1520        0.0000             nan     0.0850   -0.0000
  1540        0.0000             nan     0.0850   -0.0000
  1560        0.0000             nan     0.0850   -0.0000
  1580        0.0000             nan     0.0850   -0.0000
  1600        0.0000             nan     0.0850   -0.0000
  1620        0.0000             nan     0.0850   -0.0000
  1640        0.0000             nan     0.0850   -0.0000
  1660        0.0000             nan     0.0850   -0.0000
  1680        0.0000             nan     0.0850   -0.0000
  1700        0.0000             nan     0.0850   -0.0000
  1720        0.0000             nan     0.0850   -0.0000
  1740        0.0000             nan     0.0850   -0.0000
  1760        0.0000             nan     0.0850   -0.0000
  1780        0.0000             nan     0.0850   -0.0000
  1800        0.0000             nan     0.0850   -0.0000
  1820        0.0000             nan     0.0850   -0.0000
  1840        0.0000             nan     0.0850   -0.0000
  1860        0.0000             nan     0.0850   -0.0000
  1880        0.0000             nan     0.0850   -0.0000
  1900        0.0000             nan     0.0850   -0.0000
  1920        0.0000             nan     0.0850   -0.0000
  1940        0.0000             nan     0.0850   -0.0000
  1960        0.0000             nan     0.0850   -0.0000
  1980        0.0000             nan     0.0850   -0.0000
  2000        0.0000             nan     0.0850   -0.0000
  2020        0.0000             nan     0.0850   -0.0000
  2040        0.0000             nan     0.0850   -0.0000
  2060        0.0000             nan     0.0850   -0.0000
  2080        0.0000             nan     0.0850   -0.0000
  2100        0.0000             nan     0.0850   -0.0000
  2120        0.0000             nan     0.0850   -0.0000
  2140        0.0000             nan     0.0850   -0.0000
  2160        0.0000             nan     0.0850   -0.0000
  2180        0.0000             nan     0.0850   -0.0000
  2200        0.0000             nan     0.0850   -0.0000
  2220        0.0000             nan     0.0850   -0.0000
  2240        0.0000             nan     0.0850   -0.0000
  2260        0.0000             nan     0.0850   -0.0000
  2280        0.0000             nan     0.0850   -0.0000
  2300        0.0000             nan     0.0850   -0.0000
  2320        0.0000             nan     0.0850   -0.0000
  2340        0.0000             nan     0.0850   -0.0000
  2360        0.0000             nan     0.0850   -0.0000
  2380        0.0000             nan     0.0850   -0.0000
  2400        0.0000             nan     0.0850   -0.0000
  2420        0.0000             nan     0.0850   -0.0000
  2440        0.0000             nan     0.0850   -0.0000
  2460        0.0000             nan     0.0850   -0.0000
  2480        0.0000             nan     0.0850   -0.0000
  2500        0.0000             nan     0.0850   -0.0000
  2520        0.0000             nan     0.0850   -0.0000
  2540        0.0000             nan     0.0850   -0.0000
  2560        0.0000             nan     0.0850   -0.0000
  2580        0.0000             nan     0.0850   -0.0000
  2600        0.0000             nan     0.0850   -0.0000
  2620        0.0000             nan     0.0850   -0.0000
  2640        0.0000             nan     0.0850   -0.0000
  2660        0.0000             nan     0.0850   -0.0000
  2680        0.0000             nan     0.0850   -0.0000
  2700        0.0000             nan     0.0850   -0.0000
  2720        0.0000             nan     0.0850   -0.0000
  2740        0.0000             nan     0.0850   -0.0000
  2760        0.0000             nan     0.0850   -0.0000
  2780        0.0000             nan     0.0850   -0.0000
  2800        0.0000             nan     0.0850   -0.0000
  2820        0.0000             nan     0.0850   -0.0000
  2840        0.0000             nan     0.0850   -0.0000
  2860        0.0000             nan     0.0850   -0.0000
  2880        0.0000             nan     0.0850   -0.0000
  2900        0.0000             nan     0.0850   -0.0000
  2920        0.0000             nan     0.0850   -0.0000
  2940        0.0000             nan     0.0850   -0.0000
  2960        0.0000             nan     0.0850   -0.0000
  2980        0.0000             nan     0.0850   -0.0000
  3000        0.0000             nan     0.0850   -0.0000
  3020        0.0000             nan     0.0850   -0.0000
  3040        0.0000             nan     0.0850   -0.0000
  3060        0.0000             nan     0.0850   -0.0000
  3080        0.0000             nan     0.0850   -0.0000
  3100        0.0000             nan     0.0850   -0.0000
  3120        0.0000             nan     0.0850   -0.0000
  3140        0.0000             nan     0.0850   -0.0000
  3160        0.0000             nan     0.0850   -0.0000
  3180        0.0000             nan     0.0850   -0.0000
  3200        0.0000             nan     0.0850   -0.0000
  3220        0.0000             nan     0.0850   -0.0000
  3240        0.0000             nan     0.0850   -0.0000
  3260        0.0000             nan     0.0850   -0.0000
  3280        0.0000             nan     0.0850   -0.0000
  3300        0.0000             nan     0.0850   -0.0000
  3320        0.0000             nan     0.0850   -0.0000
  3340        0.0000             nan     0.0850   -0.0000
  3360        0.0000             nan     0.0850   -0.0000
  3380        0.0000             nan     0.0850   -0.0000
  3400        0.0000             nan     0.0850   -0.0000
  3420        0.0000             nan     0.0850   -0.0000
  3440        0.0000             nan     0.0850   -0.0000
  3460        0.0000             nan     0.0850   -0.0000
  3480        0.0000             nan     0.0850   -0.0000
  3500        0.0000             nan     0.0850   -0.0000
  3520        0.0000             nan     0.0850   -0.0000
  3540        0.0000             nan     0.0850   -0.0000
  3560        0.0000             nan     0.0850   -0.0000
  3580        0.0000             nan     0.0850   -0.0000
  3600        0.0000             nan     0.0850   -0.0000
  3620        0.0000             nan     0.0850   -0.0000
  3640        0.0000             nan     0.0850   -0.0000
  3660        0.0000             nan     0.0850    0.0000
  3680        0.0000             nan     0.0850    0.0000
  3700        0.0000             nan     0.0850   -0.0000
  3720        0.0000             nan     0.0850   -0.0000
  3740        0.0000             nan     0.0850   -0.0000
  3760        0.0000             nan     0.0850   -0.0000
  3780        0.0000             nan     0.0850   -0.0000
  3800        0.0000             nan     0.0850   -0.0000
  3820        0.0000             nan     0.0850   -0.0000
  3840        0.0000             nan     0.0850   -0.0000
  3860        0.0000             nan     0.0850   -0.0000
  3880        0.0000             nan     0.0850   -0.0000
  3900        0.0000             nan     0.0850   -0.0000
  3920        0.0000             nan     0.0850   -0.0000
  3940        0.0000             nan     0.0850   -0.0000
  3960        0.0000             nan     0.0850   -0.0000
  3980        0.0000             nan     0.0850   -0.0000
  4000        0.0000             nan     0.0850   -0.0000
  4020        0.0000             nan     0.0850   -0.0000
  4040        0.0000             nan     0.0850   -0.0000
  4060        0.0000             nan     0.0850   -0.0000
  4080        0.0000             nan     0.0850   -0.0000
  4100        0.0000             nan     0.0850   -0.0000
  4120        0.0000             nan     0.0850   -0.0000
  4140        0.0000             nan     0.0850   -0.0000
  4160        0.0000             nan     0.0850   -0.0000
  4180        0.0000             nan     0.0850   -0.0000
  4200        0.0000             nan     0.0850   -0.0000
  4220        0.0000             nan     0.0850   -0.0000
  4240        0.0000             nan     0.0850   -0.0000
  4260        0.0000             nan     0.0850   -0.0000
  4280        0.0000             nan     0.0850   -0.0000
  4281        0.0000             nan     0.0850   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0326             nan     0.1213    0.0090
     2        0.0257             nan     0.1213    0.0066
     3        0.0204             nan     0.1213    0.0056
     4        0.0163             nan     0.1213    0.0041
     5        0.0130             nan     0.1213    0.0034
     6        0.0104             nan     0.1213    0.0026
     7        0.0084             nan     0.1213    0.0022
     8        0.0068             nan     0.1213    0.0016
     9        0.0055             nan     0.1213    0.0013
    10        0.0045             nan     0.1213    0.0010
    20        0.0008             nan     0.1213    0.0001
    40        0.0003             nan     0.1213    0.0000
    60        0.0002             nan     0.1213    0.0000
    80        0.0002             nan     0.1213   -0.0000
   100        0.0002             nan     0.1213   -0.0000
   120        0.0001             nan     0.1213   -0.0000
   140        0.0001             nan     0.1213   -0.0000
   160        0.0001             nan     0.1213   -0.0000
   180        0.0001             nan     0.1213    0.0000
   200        0.0001             nan     0.1213   -0.0000
   220        0.0001             nan     0.1213   -0.0000
   240        0.0001             nan     0.1213   -0.0000
   260        0.0001             nan     0.1213   -0.0000
   280        0.0001             nan     0.1213   -0.0000
   300        0.0001             nan     0.1213   -0.0000
   320        0.0001             nan     0.1213   -0.0000
   340        0.0001             nan     0.1213   -0.0000
   360        0.0001             nan     0.1213   -0.0000
   380        0.0001             nan     0.1213   -0.0000
   400        0.0001             nan     0.1213   -0.0000
   420        0.0001             nan     0.1213   -0.0000
   440        0.0001             nan     0.1213   -0.0000
   460        0.0000             nan     0.1213   -0.0000
   480        0.0000             nan     0.1213   -0.0000
   500        0.0000             nan     0.1213   -0.0000
   520        0.0000             nan     0.1213   -0.0000
   540        0.0000             nan     0.1213   -0.0000
   560        0.0000             nan     0.1213   -0.0000
   580        0.0000             nan     0.1213   -0.0000
   600        0.0000             nan     0.1213   -0.0000
   620        0.0000             nan     0.1213   -0.0000
   640        0.0000             nan     0.1213   -0.0000
   660        0.0000             nan     0.1213   -0.0000
   680        0.0000             nan     0.1213   -0.0000
   700        0.0000             nan     0.1213   -0.0000
   720        0.0000             nan     0.1213   -0.0000
   740        0.0000             nan     0.1213   -0.0000
   760        0.0000             nan     0.1213   -0.0000
   780        0.0000             nan     0.1213   -0.0000
   800        0.0000             nan     0.1213   -0.0000
   820        0.0000             nan     0.1213   -0.0000
   840        0.0000             nan     0.1213   -0.0000
   860        0.0000             nan     0.1213   -0.0000
   880        0.0000             nan     0.1213   -0.0000
   900        0.0000             nan     0.1213   -0.0000
   920        0.0000             nan     0.1213   -0.0000
   940        0.0000             nan     0.1213   -0.0000
   960        0.0000             nan     0.1213   -0.0000
   980        0.0000             nan     0.1213   -0.0000
  1000        0.0000             nan     0.1213   -0.0000
  1020        0.0000             nan     0.1213   -0.0000
  1040        0.0000             nan     0.1213   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0318             nan     0.1295    0.0097
     2        0.0245             nan     0.1295    0.0072
     3        0.0189             nan     0.1295    0.0058
     4        0.0147             nan     0.1295    0.0044
     5        0.0114             nan     0.1295    0.0034
     6        0.0088             nan     0.1295    0.0027
     7        0.0069             nan     0.1295    0.0018
     8        0.0054             nan     0.1295    0.0014
     9        0.0043             nan     0.1295    0.0010
    10        0.0033             nan     0.1295    0.0009
    20        0.0004             nan     0.1295    0.0001
    40        0.0001             nan     0.1295   -0.0000
    60        0.0001             nan     0.1295   -0.0000
    80        0.0000             nan     0.1295   -0.0000
   100        0.0000             nan     0.1295   -0.0000
   120        0.0000             nan     0.1295   -0.0000
   140        0.0000             nan     0.1295   -0.0000
   160        0.0000             nan     0.1295   -0.0000
   180        0.0000             nan     0.1295   -0.0000
   200        0.0000             nan     0.1295   -0.0000
   220        0.0000             nan     0.1295   -0.0000
   240        0.0000             nan     0.1295   -0.0000
   260        0.0000             nan     0.1295   -0.0000
   280        0.0000             nan     0.1295   -0.0000
   300        0.0000             nan     0.1295   -0.0000
   320        0.0000             nan     0.1295   -0.0000
   340        0.0000             nan     0.1295   -0.0000
   360        0.0000             nan     0.1295   -0.0000
   380        0.0000             nan     0.1295   -0.0000
   400        0.0000             nan     0.1295   -0.0000
   420        0.0000             nan     0.1295   -0.0000
   440        0.0000             nan     0.1295   -0.0000
   460        0.0000             nan     0.1295   -0.0000
   480        0.0000             nan     0.1295   -0.0000
   500        0.0000             nan     0.1295   -0.0000
   520        0.0000             nan     0.1295   -0.0000
   540        0.0000             nan     0.1295   -0.0000
   560        0.0000             nan     0.1295   -0.0000
   580        0.0000             nan     0.1295   -0.0000
   600        0.0000             nan     0.1295   -0.0000
   620        0.0000             nan     0.1295   -0.0000
   640        0.0000             nan     0.1295   -0.0000
   660        0.0000             nan     0.1295   -0.0000
   673        0.0000             nan     0.1295   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0296             nan     0.1700    0.0118
     2        0.0213             nan     0.1700    0.0081
     3        0.0153             nan     0.1700    0.0060
     4        0.0110             nan     0.1700    0.0044
     5        0.0082             nan     0.1700    0.0029
     6        0.0062             nan     0.1700    0.0020
     7        0.0046             nan     0.1700    0.0016
     8        0.0034             nan     0.1700    0.0011
     9        0.0026             nan     0.1700    0.0007
    10        0.0020             nan     0.1700    0.0005
    20        0.0003             nan     0.1700    0.0000
    40        0.0001             nan     0.1700    0.0000
    60        0.0001             nan     0.1700    0.0000
    80        0.0001             nan     0.1700   -0.0000
   100        0.0001             nan     0.1700   -0.0000
   120        0.0001             nan     0.1700   -0.0000
   140        0.0001             nan     0.1700   -0.0000
   160        0.0000             nan     0.1700   -0.0000
   180        0.0000             nan     0.1700   -0.0000
   200        0.0000             nan     0.1700   -0.0000
   220        0.0000             nan     0.1700   -0.0000
   240        0.0000             nan     0.1700   -0.0000
   260        0.0000             nan     0.1700   -0.0000
   280        0.0000             nan     0.1700   -0.0000
   300        0.0000             nan     0.1700   -0.0000
   320        0.0000             nan     0.1700   -0.0000
   340        0.0000             nan     0.1700   -0.0000
   360        0.0000             nan     0.1700   -0.0000
   380        0.0000             nan     0.1700   -0.0000
   400        0.0000             nan     0.1700   -0.0000
   420        0.0000             nan     0.1700   -0.0000
   440        0.0000             nan     0.1700   -0.0000
   460        0.0000             nan     0.1700   -0.0000
   480        0.0000             nan     0.1700   -0.0000
   500        0.0000             nan     0.1700   -0.0000
   520        0.0000             nan     0.1700   -0.0000
   540        0.0000             nan     0.1700   -0.0000
   560        0.0000             nan     0.1700   -0.0000
   580        0.0000             nan     0.1700   -0.0000
   600        0.0000             nan     0.1700   -0.0000
   620        0.0000             nan     0.1700   -0.0000
   640        0.0000             nan     0.1700   -0.0000
   660        0.0000             nan     0.1700   -0.0000
   680        0.0000             nan     0.1700   -0.0000
   700        0.0000             nan     0.1700   -0.0000
   720        0.0000             nan     0.1700   -0.0000
   740        0.0000             nan     0.1700   -0.0000
   760        0.0000             nan     0.1700   -0.0000
   780        0.0000             nan     0.1700   -0.0000
   800        0.0000             nan     0.1700   -0.0000
   820        0.0000             nan     0.1700   -0.0000
   840        0.0000             nan     0.1700   -0.0000
   860        0.0000             nan     0.1700   -0.0000
   880        0.0000             nan     0.1700   -0.0000
   900        0.0000             nan     0.1700   -0.0000
   920        0.0000             nan     0.1700   -0.0000
   940        0.0000             nan     0.1700   -0.0000
   960        0.0000             nan     0.1700   -0.0000
   980        0.0000             nan     0.1700   -0.0000
  1000        0.0000             nan     0.1700   -0.0000
  1020        0.0000             nan     0.1700   -0.0000
  1040        0.0000             nan     0.1700   -0.0000
  1060        0.0000             nan     0.1700   -0.0000
  1080        0.0000             nan     0.1700   -0.0000
  1100        0.0000             nan     0.1700   -0.0000
  1120        0.0000             nan     0.1700   -0.0000
  1140        0.0000             nan     0.1700   -0.0000
  1160        0.0000             nan     0.1700   -0.0000
  1180        0.0000             nan     0.1700   -0.0000
  1200        0.0000             nan     0.1700   -0.0000
  1220        0.0000             nan     0.1700   -0.0000
  1240        0.0000             nan     0.1700   -0.0000
  1260        0.0000             nan     0.1700   -0.0000
  1280        0.0000             nan     0.1700   -0.0000
  1300        0.0000             nan     0.1700   -0.0000
  1320        0.0000             nan     0.1700   -0.0000
  1340        0.0000             nan     0.1700   -0.0000
  1360        0.0000             nan     0.1700   -0.0000
  1380        0.0000             nan     0.1700   -0.0000
  1400        0.0000             nan     0.1700   -0.0000
  1420        0.0000             nan     0.1700   -0.0000
  1440        0.0000             nan     0.1700   -0.0000
  1460        0.0000             nan     0.1700   -0.0000
  1480        0.0000             nan     0.1700   -0.0000
  1500        0.0000             nan     0.1700   -0.0000
  1520        0.0000             nan     0.1700   -0.0000
  1540        0.0000             nan     0.1700   -0.0000
  1560        0.0000             nan     0.1700   -0.0000
  1580        0.0000             nan     0.1700   -0.0000
  1600        0.0000             nan     0.1700   -0.0000
  1620        0.0000             nan     0.1700   -0.0000
  1640        0.0000             nan     0.1700   -0.0000
  1660        0.0000             nan     0.1700   -0.0000
  1680        0.0000             nan     0.1700   -0.0000
  1700        0.0000             nan     0.1700   -0.0000
  1720        0.0000             nan     0.1700   -0.0000
  1740        0.0000             nan     0.1700   -0.0000
  1760        0.0000             nan     0.1700   -0.0000
  1780        0.0000             nan     0.1700   -0.0000
  1800        0.0000             nan     0.1700   -0.0000
  1820        0.0000             nan     0.1700   -0.0000
  1840        0.0000             nan     0.1700   -0.0000
  1860        0.0000             nan     0.1700   -0.0000
  1880        0.0000             nan     0.1700   -0.0000
  1900        0.0000             nan     0.1700   -0.0000
  1920        0.0000             nan     0.1700   -0.0000
  1940        0.0000             nan     0.1700   -0.0000
  1960        0.0000             nan     0.1700   -0.0000
  1980        0.0000             nan     0.1700   -0.0000
  2000        0.0000             nan     0.1700   -0.0000
  2020        0.0000             nan     0.1700   -0.0000
  2040        0.0000             nan     0.1700   -0.0000
  2060        0.0000             nan     0.1700   -0.0000
  2080        0.0000             nan     0.1700   -0.0000
  2100        0.0000             nan     0.1700   -0.0000
  2120        0.0000             nan     0.1700   -0.0000
  2140        0.0000             nan     0.1700   -0.0000
  2160        0.0000             nan     0.1700   -0.0000
  2180        0.0000             nan     0.1700   -0.0000
  2200        0.0000             nan     0.1700   -0.0000
  2220        0.0000             nan     0.1700   -0.0000
  2240        0.0000             nan     0.1700   -0.0000
  2260        0.0000             nan     0.1700   -0.0000
  2280        0.0000             nan     0.1700   -0.0000
  2300        0.0000             nan     0.1700    0.0000
  2320        0.0000             nan     0.1700   -0.0000
  2340        0.0000             nan     0.1700   -0.0000
  2360        0.0000             nan     0.1700   -0.0000
  2380        0.0000             nan     0.1700   -0.0000
  2400        0.0000             nan     0.1700   -0.0000
  2420        0.0000             nan     0.1700   -0.0000
  2440        0.0000             nan     0.1700   -0.0000
  2460        0.0000             nan     0.1700   -0.0000
  2480        0.0000             nan     0.1700   -0.0000
  2500        0.0000             nan     0.1700   -0.0000
  2520        0.0000             nan     0.1700   -0.0000
  2540        0.0000             nan     0.1700    0.0000
  2560        0.0000             nan     0.1700    0.0000
  2580        0.0000             nan     0.1700   -0.0000
  2600        0.0000             nan     0.1700   -0.0000
  2620        0.0000             nan     0.1700   -0.0000
  2640        0.0000             nan     0.1700   -0.0000
  2660        0.0000             nan     0.1700   -0.0000
  2680        0.0000             nan     0.1700    0.0000
  2700        0.0000             nan     0.1700   -0.0000
  2720        0.0000             nan     0.1700    0.0000
  2740        0.0000             nan     0.1700    0.0000
  2760        0.0000             nan     0.1700   -0.0000
  2780        0.0000             nan     0.1700   -0.0000
  2800        0.0000             nan     0.1700    0.0000
  2820        0.0000             nan     0.1700   -0.0000
  2840        0.0000             nan     0.1700    0.0000
  2860        0.0000             nan     0.1700   -0.0000
  2880        0.0000             nan     0.1700   -0.0000
  2900        0.0000             nan     0.1700    0.0000
  2920        0.0000             nan     0.1700   -0.0000
  2940        0.0000             nan     0.1700   -0.0000
  2960        0.0000             nan     0.1700   -0.0000
  2980        0.0000             nan     0.1700   -0.0000
  3000        0.0000             nan     0.1700   -0.0000
  3020        0.0000             nan     0.1700   -0.0000
  3040        0.0000             nan     0.1700   -0.0000
  3060        0.0000             nan     0.1700   -0.0000
  3080        0.0000             nan     0.1700    0.0000
  3100        0.0000             nan     0.1700   -0.0000
  3120        0.0000             nan     0.1700   -0.0000
  3140        0.0000             nan     0.1700   -0.0000
  3160        0.0000             nan     0.1700    0.0000
  3180        0.0000             nan     0.1700   -0.0000
  3200        0.0000             nan     0.1700   -0.0000
  3220        0.0000             nan     0.1700   -0.0000
  3240        0.0000             nan     0.1700   -0.0000
  3260        0.0000             nan     0.1700   -0.0000
  3280        0.0000             nan     0.1700   -0.0000
  3300        0.0000             nan     0.1700   -0.0000
  3320        0.0000             nan     0.1700    0.0000
  3340        0.0000             nan     0.1700    0.0000
  3360        0.0000             nan     0.1700   -0.0000
  3380        0.0000             nan     0.1700   -0.0000
  3400        0.0000             nan     0.1700   -0.0000
  3420        0.0000             nan     0.1700   -0.0000
  3440        0.0000             nan     0.1700   -0.0000
  3460        0.0000             nan     0.1700   -0.0000
  3480        0.0000             nan     0.1700    0.0000
  3500        0.0000             nan     0.1700   -0.0000
  3520        0.0000             nan     0.1700    0.0000
  3540        0.0000             nan     0.1700   -0.0000
  3560        0.0000             nan     0.1700   -0.0000
  3580        0.0000             nan     0.1700   -0.0000
  3600        0.0000             nan     0.1700    0.0000
  3620        0.0000             nan     0.1700   -0.0000
  3640        0.0000             nan     0.1700   -0.0000
  3660        0.0000             nan     0.1700   -0.0000
  3680        0.0000             nan     0.1700    0.0000
  3700        0.0000             nan     0.1700   -0.0000
  3720        0.0000             nan     0.1700    0.0000
  3740        0.0000             nan     0.1700   -0.0000
  3760        0.0000             nan     0.1700   -0.0000
  3780        0.0000             nan     0.1700   -0.0000
  3800        0.0000             nan     0.1700    0.0000
  3820        0.0000             nan     0.1700   -0.0000
  3840        0.0000             nan     0.1700    0.0000
  3860        0.0000             nan     0.1700   -0.0000
  3880        0.0000             nan     0.1700   -0.0000
  3900        0.0000             nan     0.1700   -0.0000
  3920        0.0000             nan     0.1700   -0.0000
  3940        0.0000             nan     0.1700   -0.0000
  3960        0.0000             nan     0.1700   -0.0000
  3980        0.0000             nan     0.1700    0.0000
  4000        0.0000             nan     0.1700   -0.0000
  4020        0.0000             nan     0.1700   -0.0000
  4040        0.0000             nan     0.1700   -0.0000
  4060        0.0000             nan     0.1700   -0.0000
  4080        0.0000             nan     0.1700   -0.0000
  4100        0.0000             nan     0.1700    0.0000
  4120        0.0000             nan     0.1700   -0.0000
  4140        0.0000             nan     0.1700   -0.0000
  4160        0.0000             nan     0.1700   -0.0000
  4180        0.0000             nan     0.1700   -0.0000
  4200        0.0000             nan     0.1700   -0.0000
  4220        0.0000             nan     0.1700    0.0000
  4240        0.0000             nan     0.1700   -0.0000
  4260        0.0000             nan     0.1700    0.0000
  4280        0.0000             nan     0.1700   -0.0000
  4300        0.0000             nan     0.1700   -0.0000
  4320        0.0000             nan     0.1700   -0.0000
  4340        0.0000             nan     0.1700   -0.0000
  4360        0.0000             nan     0.1700   -0.0000
  4380        0.0000             nan     0.1700   -0.0000
  4400        0.0000             nan     0.1700    0.0000
  4420        0.0000             nan     0.1700   -0.0000
  4440        0.0000             nan     0.1700   -0.0000
  4460        0.0000             nan     0.1700    0.0000
  4480        0.0000             nan     0.1700   -0.0000
  4500        0.0000             nan     0.1700   -0.0000
  4520        0.0000             nan     0.1700   -0.0000
  4540        0.0000             nan     0.1700   -0.0000
  4554        0.0000             nan     0.1700   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0289             nan     0.1821    0.0118
     2        0.0203             nan     0.1821    0.0081
     3        0.0143             nan     0.1821    0.0058
     4        0.0101             nan     0.1821    0.0041
     5        0.0073             nan     0.1821    0.0027
     6        0.0053             nan     0.1821    0.0021
     7        0.0039             nan     0.1821    0.0013
     8        0.0029             nan     0.1821    0.0010
     9        0.0021             nan     0.1821    0.0007
    10        0.0017             nan     0.1821    0.0005
    20        0.0003             nan     0.1821    0.0000
    40        0.0002             nan     0.1821   -0.0000
    60        0.0001             nan     0.1821    0.0000
    80        0.0001             nan     0.1821   -0.0000
   100        0.0001             nan     0.1821   -0.0000
   120        0.0001             nan     0.1821   -0.0000
   140        0.0001             nan     0.1821   -0.0000
   160        0.0001             nan     0.1821   -0.0000
   180        0.0000             nan     0.1821   -0.0000
   200        0.0000             nan     0.1821   -0.0000
   220        0.0000             nan     0.1821   -0.0000
   240        0.0000             nan     0.1821   -0.0000
   260        0.0000             nan     0.1821   -0.0000
   280        0.0000             nan     0.1821   -0.0000
   300        0.0000             nan     0.1821   -0.0000
   320        0.0000             nan     0.1821   -0.0000
   340        0.0000             nan     0.1821   -0.0000
   360        0.0000             nan     0.1821   -0.0000
   380        0.0000             nan     0.1821   -0.0000
   400        0.0000             nan     0.1821   -0.0000
   420        0.0000             nan     0.1821   -0.0000
   440        0.0000             nan     0.1821   -0.0000
   460        0.0000             nan     0.1821   -0.0000
   480        0.0000             nan     0.1821   -0.0000
   500        0.0000             nan     0.1821   -0.0000
   520        0.0000             nan     0.1821   -0.0000
   540        0.0000             nan     0.1821   -0.0000
   560        0.0000             nan     0.1821   -0.0000
   580        0.0000             nan     0.1821   -0.0000
   600        0.0000             nan     0.1821   -0.0000
   620        0.0000             nan     0.1821   -0.0000
   640        0.0000             nan     0.1821   -0.0000
   660        0.0000             nan     0.1821   -0.0000
   680        0.0000             nan     0.1821   -0.0000
   700        0.0000             nan     0.1821   -0.0000
   720        0.0000             nan     0.1821   -0.0000
   740        0.0000             nan     0.1821   -0.0000
   760        0.0000             nan     0.1821   -0.0000
   780        0.0000             nan     0.1821   -0.0000
   800        0.0000             nan     0.1821   -0.0000
   820        0.0000             nan     0.1821   -0.0000
   840        0.0000             nan     0.1821   -0.0000
   860        0.0000             nan     0.1821   -0.0000
   880        0.0000             nan     0.1821   -0.0000
   900        0.0000             nan     0.1821   -0.0000
   920        0.0000             nan     0.1821   -0.0000
   940        0.0000             nan     0.1821   -0.0000
   960        0.0000             nan     0.1821   -0.0000
   980        0.0000             nan     0.1821   -0.0000
  1000        0.0000             nan     0.1821   -0.0000
  1020        0.0000             nan     0.1821   -0.0000
  1040        0.0000             nan     0.1821   -0.0000
  1060        0.0000             nan     0.1821   -0.0000
  1080        0.0000             nan     0.1821   -0.0000
  1100        0.0000             nan     0.1821   -0.0000
  1120        0.0000             nan     0.1821   -0.0000
  1140        0.0000             nan     0.1821   -0.0000
  1160        0.0000             nan     0.1821   -0.0000
  1180        0.0000             nan     0.1821   -0.0000
  1200        0.0000             nan     0.1821   -0.0000
  1220        0.0000             nan     0.1821   -0.0000
  1240        0.0000             nan     0.1821   -0.0000
  1260        0.0000             nan     0.1821   -0.0000
  1280        0.0000             nan     0.1821   -0.0000
  1300        0.0000             nan     0.1821   -0.0000
  1320        0.0000             nan     0.1821   -0.0000
  1340        0.0000             nan     0.1821   -0.0000
  1360        0.0000             nan     0.1821   -0.0000
  1380        0.0000             nan     0.1821   -0.0000
  1400        0.0000             nan     0.1821   -0.0000
  1420        0.0000             nan     0.1821   -0.0000
  1440        0.0000             nan     0.1821   -0.0000
  1460        0.0000             nan     0.1821   -0.0000
  1480        0.0000             nan     0.1821   -0.0000
  1500        0.0000             nan     0.1821   -0.0000
  1520        0.0000             nan     0.1821   -0.0000
  1540        0.0000             nan     0.1821   -0.0000
  1560        0.0000             nan     0.1821   -0.0000
  1580        0.0000             nan     0.1821   -0.0000
  1600        0.0000             nan     0.1821   -0.0000
  1620        0.0000             nan     0.1821   -0.0000
  1640        0.0000             nan     0.1821   -0.0000
  1660        0.0000             nan     0.1821   -0.0000
  1680        0.0000             nan     0.1821   -0.0000
  1700        0.0000             nan     0.1821   -0.0000
  1720        0.0000             nan     0.1821   -0.0000
  1740        0.0000             nan     0.1821   -0.0000
  1760        0.0000             nan     0.1821   -0.0000
  1780        0.0000             nan     0.1821   -0.0000
  1800        0.0000             nan     0.1821   -0.0000
  1820        0.0000             nan     0.1821   -0.0000
  1840        0.0000             nan     0.1821   -0.0000
  1860        0.0000             nan     0.1821   -0.0000
  1880        0.0000             nan     0.1821   -0.0000
  1900        0.0000             nan     0.1821   -0.0000
  1920        0.0000             nan     0.1821   -0.0000
  1940        0.0000             nan     0.1821   -0.0000
  1960        0.0000             nan     0.1821   -0.0000
  1980        0.0000             nan     0.1821   -0.0000
  2000        0.0000             nan     0.1821   -0.0000
  2020        0.0000             nan     0.1821   -0.0000
  2040        0.0000             nan     0.1821   -0.0000
  2060        0.0000             nan     0.1821   -0.0000
  2080        0.0000             nan     0.1821   -0.0000
  2100        0.0000             nan     0.1821   -0.0000
  2120        0.0000             nan     0.1821   -0.0000
  2140        0.0000             nan     0.1821   -0.0000
  2160        0.0000             nan     0.1821   -0.0000
  2180        0.0000             nan     0.1821   -0.0000
  2200        0.0000             nan     0.1821   -0.0000
  2220        0.0000             nan     0.1821   -0.0000
  2240        0.0000             nan     0.1821   -0.0000
  2260        0.0000             nan     0.1821   -0.0000
  2280        0.0000             nan     0.1821   -0.0000
  2300        0.0000             nan     0.1821   -0.0000
  2320        0.0000             nan     0.1821   -0.0000
  2340        0.0000             nan     0.1821   -0.0000
  2360        0.0000             nan     0.1821   -0.0000
  2380        0.0000             nan     0.1821   -0.0000
  2400        0.0000             nan     0.1821   -0.0000
  2420        0.0000             nan     0.1821   -0.0000
  2440        0.0000             nan     0.1821   -0.0000
  2460        0.0000             nan     0.1821   -0.0000
  2480        0.0000             nan     0.1821   -0.0000
  2500        0.0000             nan     0.1821   -0.0000
  2520        0.0000             nan     0.1821   -0.0000
  2540        0.0000             nan     0.1821   -0.0000
  2560        0.0000             nan     0.1821   -0.0000
  2580        0.0000             nan     0.1821   -0.0000
  2600        0.0000             nan     0.1821   -0.0000
  2620        0.0000             nan     0.1821   -0.0000
  2640        0.0000             nan     0.1821   -0.0000
  2660        0.0000             nan     0.1821   -0.0000
  2680        0.0000             nan     0.1821   -0.0000
  2700        0.0000             nan     0.1821   -0.0000
  2720        0.0000             nan     0.1821   -0.0000
  2740        0.0000             nan     0.1821    0.0000
  2760        0.0000             nan     0.1821   -0.0000
  2780        0.0000             nan     0.1821   -0.0000
  2800        0.0000             nan     0.1821   -0.0000
  2820        0.0000             nan     0.1821   -0.0000
  2840        0.0000             nan     0.1821   -0.0000
  2860        0.0000             nan     0.1821   -0.0000
  2880        0.0000             nan     0.1821   -0.0000
  2900        0.0000             nan     0.1821   -0.0000
  2920        0.0000             nan     0.1821   -0.0000
  2940        0.0000             nan     0.1821   -0.0000
  2960        0.0000             nan     0.1821   -0.0000
  2980        0.0000             nan     0.1821   -0.0000
  3000        0.0000             nan     0.1821   -0.0000
  3020        0.0000             nan     0.1821   -0.0000
  3040        0.0000             nan     0.1821   -0.0000
  3060        0.0000             nan     0.1821   -0.0000
  3080        0.0000             nan     0.1821   -0.0000
  3100        0.0000             nan     0.1821   -0.0000
  3120        0.0000             nan     0.1821   -0.0000
  3140        0.0000             nan     0.1821   -0.0000
  3160        0.0000             nan     0.1821   -0.0000
  3180        0.0000             nan     0.1821   -0.0000
  3200        0.0000             nan     0.1821   -0.0000
  3220        0.0000             nan     0.1821    0.0000
  3240        0.0000             nan     0.1821   -0.0000
  3260        0.0000             nan     0.1821   -0.0000
  3266        0.0000             nan     0.1821   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0281             nan     0.2033    0.0134
     2        0.0192             nan     0.2033    0.0089
     3        0.0135             nan     0.2033    0.0060
     4        0.0094             nan     0.2033    0.0037
     5        0.0068             nan     0.2033    0.0027
     6        0.0049             nan     0.2033    0.0021
     7        0.0036             nan     0.2033    0.0012
     8        0.0026             nan     0.2033    0.0009
     9        0.0020             nan     0.2033    0.0006
    10        0.0015             nan     0.2033    0.0004
    20        0.0005             nan     0.2033    0.0000
    40        0.0003             nan     0.2033    0.0000
    60        0.0002             nan     0.2033   -0.0000
    80        0.0002             nan     0.2033    0.0000
   100        0.0002             nan     0.2033   -0.0000
   120        0.0002             nan     0.2033   -0.0000
   140        0.0001             nan     0.2033   -0.0000
   160        0.0001             nan     0.2033   -0.0000
   180        0.0001             nan     0.2033   -0.0000
   200        0.0001             nan     0.2033   -0.0000
   220        0.0001             nan     0.2033   -0.0000
   240        0.0001             nan     0.2033   -0.0000
   260        0.0001             nan     0.2033   -0.0000
   280        0.0001             nan     0.2033   -0.0000
   300        0.0001             nan     0.2033   -0.0000
   320        0.0001             nan     0.2033   -0.0000
   340        0.0001             nan     0.2033   -0.0000
   360        0.0001             nan     0.2033   -0.0000
   380        0.0001             nan     0.2033   -0.0000
   400        0.0001             nan     0.2033   -0.0000
   420        0.0001             nan     0.2033   -0.0000
   440        0.0001             nan     0.2033   -0.0000
   460        0.0001             nan     0.2033   -0.0000
   480        0.0001             nan     0.2033   -0.0000
   500        0.0001             nan     0.2033   -0.0000
   520        0.0001             nan     0.2033   -0.0000
   540        0.0001             nan     0.2033   -0.0000
   560        0.0001             nan     0.2033   -0.0000
   580        0.0000             nan     0.2033   -0.0000
   600        0.0000             nan     0.2033   -0.0000
   620        0.0000             nan     0.2033   -0.0000
   640        0.0000             nan     0.2033   -0.0000
   660        0.0000             nan     0.2033   -0.0000
   680        0.0000             nan     0.2033   -0.0000
   700        0.0000             nan     0.2033   -0.0000
   720        0.0000             nan     0.2033   -0.0000
   740        0.0000             nan     0.2033   -0.0000
   760        0.0000             nan     0.2033   -0.0000
   780        0.0000             nan     0.2033   -0.0000
   800        0.0000             nan     0.2033   -0.0000
   820        0.0000             nan     0.2033   -0.0000
   840        0.0000             nan     0.2033   -0.0000
   860        0.0000             nan     0.2033   -0.0000
   880        0.0000             nan     0.2033   -0.0000
   900        0.0000             nan     0.2033   -0.0000
   920        0.0000             nan     0.2033   -0.0000
   940        0.0000             nan     0.2033   -0.0000
   960        0.0000             nan     0.2033   -0.0000
   980        0.0000             nan     0.2033   -0.0000
  1000        0.0000             nan     0.2033   -0.0000
  1020        0.0000             nan     0.2033   -0.0000
  1040        0.0000             nan     0.2033   -0.0000
  1060        0.0000             nan     0.2033   -0.0000
  1080        0.0000             nan     0.2033   -0.0000
  1100        0.0000             nan     0.2033   -0.0000
  1120        0.0000             nan     0.2033   -0.0000
  1140        0.0000             nan     0.2033   -0.0000
  1160        0.0000             nan     0.2033   -0.0000
  1180        0.0000             nan     0.2033   -0.0000
  1200        0.0000             nan     0.2033   -0.0000
  1220        0.0000             nan     0.2033   -0.0000
  1240        0.0000             nan     0.2033   -0.0000
  1260        0.0000             nan     0.2033   -0.0000
  1280        0.0000             nan     0.2033   -0.0000
  1300        0.0000             nan     0.2033   -0.0000
  1320        0.0000             nan     0.2033   -0.0000
  1340        0.0000             nan     0.2033   -0.0000
  1360        0.0000             nan     0.2033   -0.0000
  1380        0.0000             nan     0.2033   -0.0000
  1400        0.0000             nan     0.2033   -0.0000
  1420        0.0000             nan     0.2033   -0.0000
  1440        0.0000             nan     0.2033   -0.0000
  1460        0.0000             nan     0.2033   -0.0000
  1480        0.0000             nan     0.2033   -0.0000
  1500        0.0000             nan     0.2033   -0.0000
  1520        0.0000             nan     0.2033   -0.0000
  1540        0.0000             nan     0.2033   -0.0000
  1560        0.0000             nan     0.2033   -0.0000
  1580        0.0000             nan     0.2033   -0.0000
  1600        0.0000             nan     0.2033   -0.0000
  1620        0.0000             nan     0.2033   -0.0000
  1640        0.0000             nan     0.2033   -0.0000
  1660        0.0000             nan     0.2033   -0.0000
  1680        0.0000             nan     0.2033   -0.0000
  1700        0.0000             nan     0.2033   -0.0000
  1720        0.0000             nan     0.2033   -0.0000
  1740        0.0000             nan     0.2033   -0.0000
  1760        0.0000             nan     0.2033   -0.0000
  1780        0.0000             nan     0.2033    0.0000
  1800        0.0000             nan     0.2033   -0.0000
  1820        0.0000             nan     0.2033   -0.0000
  1840        0.0000             nan     0.2033   -0.0000
  1860        0.0000             nan     0.2033   -0.0000
  1880        0.0000             nan     0.2033   -0.0000
  1900        0.0000             nan     0.2033   -0.0000
  1905        0.0000             nan     0.2033   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0241             nan     0.2560    0.0167
     2        0.0149             nan     0.2560    0.0090
     3        0.0094             nan     0.2560    0.0053
     4        0.0061             nan     0.2560    0.0032
     5        0.0040             nan     0.2560    0.0019
     6        0.0028             nan     0.2560    0.0009
     7        0.0019             nan     0.2560    0.0009
     8        0.0014             nan     0.2560    0.0005
     9        0.0010             nan     0.2560    0.0003
    10        0.0008             nan     0.2560    0.0002
    20        0.0003             nan     0.2560    0.0000
    40        0.0002             nan     0.2560   -0.0000
    60        0.0001             nan     0.2560   -0.0000
    80        0.0001             nan     0.2560   -0.0000
   100        0.0001             nan     0.2560   -0.0000
   120        0.0001             nan     0.2560   -0.0000
   140        0.0000             nan     0.2560   -0.0000
   160        0.0000             nan     0.2560   -0.0000
   180        0.0000             nan     0.2560   -0.0000
   200        0.0000             nan     0.2560    0.0000
   220        0.0000             nan     0.2560   -0.0000
   240        0.0000             nan     0.2560   -0.0000
   260        0.0000             nan     0.2560   -0.0000
   280        0.0000             nan     0.2560   -0.0000
   300        0.0000             nan     0.2560   -0.0000
   320        0.0000             nan     0.2560   -0.0000
   340        0.0000             nan     0.2560   -0.0000
   360        0.0000             nan     0.2560   -0.0000
   380        0.0000             nan     0.2560   -0.0000
   400        0.0000             nan     0.2560   -0.0000
   420        0.0000             nan     0.2560   -0.0000
   440        0.0000             nan     0.2560   -0.0000
   460        0.0000             nan     0.2560   -0.0000
   480        0.0000             nan     0.2560   -0.0000
   500        0.0000             nan     0.2560   -0.0000
   520        0.0000             nan     0.2560   -0.0000
   540        0.0000             nan     0.2560   -0.0000
   560        0.0000             nan     0.2560   -0.0000
   580        0.0000             nan     0.2560   -0.0000
   600        0.0000             nan     0.2560   -0.0000
   620        0.0000             nan     0.2560   -0.0000
   640        0.0000             nan     0.2560   -0.0000
   660        0.0000             nan     0.2560   -0.0000
   680        0.0000             nan     0.2560   -0.0000
   700        0.0000             nan     0.2560   -0.0000
   720        0.0000             nan     0.2560   -0.0000
   740        0.0000             nan     0.2560   -0.0000
   760        0.0000             nan     0.2560   -0.0000
   780        0.0000             nan     0.2560   -0.0000
   800        0.0000             nan     0.2560   -0.0000
   820        0.0000             nan     0.2560   -0.0000
   840        0.0000             nan     0.2560   -0.0000
   860        0.0000             nan     0.2560   -0.0000
   880        0.0000             nan     0.2560   -0.0000
   900        0.0000             nan     0.2560   -0.0000
   920        0.0000             nan     0.2560   -0.0000
   940        0.0000             nan     0.2560   -0.0000
   960        0.0000             nan     0.2560   -0.0000
   980        0.0000             nan     0.2560   -0.0000
  1000        0.0000             nan     0.2560   -0.0000
  1020        0.0000             nan     0.2560   -0.0000
  1040        0.0000             nan     0.2560   -0.0000
  1060        0.0000             nan     0.2560   -0.0000
  1080        0.0000             nan     0.2560   -0.0000
  1100        0.0000             nan     0.2560   -0.0000
  1120        0.0000             nan     0.2560   -0.0000
  1140        0.0000             nan     0.2560   -0.0000
  1160        0.0000             nan     0.2560   -0.0000
  1180        0.0000             nan     0.2560   -0.0000
  1200        0.0000             nan     0.2560   -0.0000
  1220        0.0000             nan     0.2560   -0.0000
  1240        0.0000             nan     0.2560   -0.0000
  1260        0.0000             nan     0.2560   -0.0000
  1280        0.0000             nan     0.2560   -0.0000
  1300        0.0000             nan     0.2560   -0.0000
  1320        0.0000             nan     0.2560   -0.0000
  1340        0.0000             nan     0.2560   -0.0000
  1360        0.0000             nan     0.2560   -0.0000
  1380        0.0000             nan     0.2560   -0.0000
  1400        0.0000             nan     0.2560   -0.0000
  1420        0.0000             nan     0.2560   -0.0000
  1440        0.0000             nan     0.2560   -0.0000
  1460        0.0000             nan     0.2560   -0.0000
  1480        0.0000             nan     0.2560   -0.0000
  1500        0.0000             nan     0.2560   -0.0000
  1520        0.0000             nan     0.2560   -0.0000
  1540        0.0000             nan     0.2560   -0.0000
  1560        0.0000             nan     0.2560   -0.0000
  1580        0.0000             nan     0.2560   -0.0000
  1600        0.0000             nan     0.2560   -0.0000
  1620        0.0000             nan     0.2560   -0.0000
  1640        0.0000             nan     0.2560    0.0000
  1660        0.0000             nan     0.2560   -0.0000
  1680        0.0000             nan     0.2560   -0.0000
  1700        0.0000             nan     0.2560   -0.0000
  1720        0.0000             nan     0.2560   -0.0000
  1740        0.0000             nan     0.2560   -0.0000
  1760        0.0000             nan     0.2560   -0.0000
  1780        0.0000             nan     0.2560   -0.0000
  1800        0.0000             nan     0.2560   -0.0000
  1820        0.0000             nan     0.2560   -0.0000
  1840        0.0000             nan     0.2560   -0.0000
  1860        0.0000             nan     0.2560   -0.0000
  1880        0.0000             nan     0.2560   -0.0000
  1900        0.0000             nan     0.2560   -0.0000
  1920        0.0000             nan     0.2560   -0.0000
  1940        0.0000             nan     0.2560   -0.0000
  1960        0.0000             nan     0.2560   -0.0000
  1980        0.0000             nan     0.2560   -0.0000
  2000        0.0000             nan     0.2560   -0.0000
  2020        0.0000             nan     0.2560   -0.0000
  2040        0.0000             nan     0.2560   -0.0000
  2060        0.0000             nan     0.2560   -0.0000
  2080        0.0000             nan     0.2560   -0.0000
  2100        0.0000             nan     0.2560   -0.0000
  2120        0.0000             nan     0.2560   -0.0000
  2140        0.0000             nan     0.2560   -0.0000
  2160        0.0000             nan     0.2560   -0.0000
  2180        0.0000             nan     0.2560   -0.0000
  2200        0.0000             nan     0.2560   -0.0000
  2220        0.0000             nan     0.2560   -0.0000
  2240        0.0000             nan     0.2560   -0.0000
  2260        0.0000             nan     0.2560   -0.0000
  2280        0.0000             nan     0.2560   -0.0000
  2300        0.0000             nan     0.2560   -0.0000
  2320        0.0000             nan     0.2560   -0.0000
  2340        0.0000             nan     0.2560   -0.0000
  2360        0.0000             nan     0.2560   -0.0000
  2380        0.0000             nan     0.2560   -0.0000
  2400        0.0000             nan     0.2560    0.0000
  2420        0.0000             nan     0.2560   -0.0000
  2440        0.0000             nan     0.2560   -0.0000
  2460        0.0000             nan     0.2560   -0.0000
  2480        0.0000             nan     0.2560   -0.0000
  2500        0.0000             nan     0.2560   -0.0000
  2520        0.0000             nan     0.2560   -0.0000
  2540        0.0000             nan     0.2560    0.0000
  2560        0.0000             nan     0.2560   -0.0000
  2580        0.0000             nan     0.2560   -0.0000
  2600        0.0000             nan     0.2560   -0.0000
  2620        0.0000             nan     0.2560   -0.0000
  2640        0.0000             nan     0.2560    0.0000
  2660        0.0000             nan     0.2560   -0.0000
  2680        0.0000             nan     0.2560   -0.0000
  2700        0.0000             nan     0.2560   -0.0000
  2720        0.0000             nan     0.2560   -0.0000
  2740        0.0000             nan     0.2560   -0.0000
  2760        0.0000             nan     0.2560   -0.0000
  2780        0.0000             nan     0.2560   -0.0000
  2800        0.0000             nan     0.2560   -0.0000
  2820        0.0000             nan     0.2560   -0.0000
  2840        0.0000             nan     0.2560   -0.0000
  2860        0.0000             nan     0.2560   -0.0000
  2880        0.0000             nan     0.2560   -0.0000
  2900        0.0000             nan     0.2560   -0.0000
  2920        0.0000             nan     0.2560    0.0000
  2940        0.0000             nan     0.2560   -0.0000
  2960        0.0000             nan     0.2560   -0.0000
  2980        0.0000             nan     0.2560    0.0000
  3000        0.0000             nan     0.2560   -0.0000
  3020        0.0000             nan     0.2560   -0.0000
  3040        0.0000             nan     0.2560   -0.0000
  3060        0.0000             nan     0.2560    0.0000
  3080        0.0000             nan     0.2560   -0.0000
  3100        0.0000             nan     0.2560   -0.0000
  3120        0.0000             nan     0.2560   -0.0000
  3140        0.0000             nan     0.2560   -0.0000
  3160        0.0000             nan     0.2560   -0.0000
  3180        0.0000             nan     0.2560   -0.0000
  3200        0.0000             nan     0.2560   -0.0000
  3220        0.0000             nan     0.2560   -0.0000
  3240        0.0000             nan     0.2560   -0.0000
  3260        0.0000             nan     0.2560   -0.0000
  3280        0.0000             nan     0.2560   -0.0000
  3300        0.0000             nan     0.2560   -0.0000
  3320        0.0000             nan     0.2560   -0.0000
  3340        0.0000             nan     0.2560   -0.0000
  3360        0.0000             nan     0.2560   -0.0000
  3380        0.0000             nan     0.2560    0.0000
  3400        0.0000             nan     0.2560   -0.0000
  3420        0.0000             nan     0.2560    0.0000
  3440        0.0000             nan     0.2560   -0.0000
  3460        0.0000             nan     0.2560   -0.0000
  3480        0.0000             nan     0.2560   -0.0000
  3500        0.0000             nan     0.2560   -0.0000
  3520        0.0000             nan     0.2560   -0.0000
  3540        0.0000             nan     0.2560   -0.0000
  3560        0.0000             nan     0.2560   -0.0000
  3580        0.0000             nan     0.2560   -0.0000
  3600        0.0000             nan     0.2560    0.0000
  3620        0.0000             nan     0.2560    0.0000
  3640        0.0000             nan     0.2560   -0.0000
  3660        0.0000             nan     0.2560    0.0000
  3680        0.0000             nan     0.2560   -0.0000
  3700        0.0000             nan     0.2560    0.0000
  3720        0.0000             nan     0.2560   -0.0000
  3740        0.0000             nan     0.2560   -0.0000
  3760        0.0000             nan     0.2560   -0.0000
  3780        0.0000             nan     0.2560   -0.0000
  3800        0.0000             nan     0.2560   -0.0000
  3820        0.0000             nan     0.2560   -0.0000
  3840        0.0000             nan     0.2560   -0.0000
  3860        0.0000             nan     0.2560   -0.0000
  3880        0.0000             nan     0.2560   -0.0000
  3900        0.0000             nan     0.2560   -0.0000
  3920        0.0000             nan     0.2560   -0.0000
  3940        0.0000             nan     0.2560   -0.0000
  3960        0.0000             nan     0.2560    0.0000
  3980        0.0000             nan     0.2560   -0.0000
  4000        0.0000             nan     0.2560   -0.0000
  4020        0.0000             nan     0.2560   -0.0000
  4040        0.0000             nan     0.2560   -0.0000
  4060        0.0000             nan     0.2560   -0.0000
  4062        0.0000             nan     0.2560   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0198             nan     0.3360    0.0229
     2        0.0099             nan     0.3360    0.0102
     3        0.0050             nan     0.3360    0.0048
     4        0.0027             nan     0.3360    0.0021
     5        0.0015             nan     0.3360    0.0012
     6        0.0010             nan     0.3360    0.0005
     7        0.0007             nan     0.3360    0.0002
     8        0.0005             nan     0.3360    0.0001
     9        0.0005             nan     0.3360    0.0001
    10        0.0004             nan     0.3360    0.0000
    20        0.0003             nan     0.3360   -0.0000
    40        0.0002             nan     0.3360   -0.0000
    60        0.0001             nan     0.3360    0.0000
    80        0.0001             nan     0.3360   -0.0000
   100        0.0001             nan     0.3360   -0.0000
   120        0.0001             nan     0.3360   -0.0000
   140        0.0001             nan     0.3360   -0.0000
   160        0.0001             nan     0.3360   -0.0000
   180        0.0001             nan     0.3360   -0.0000
   200        0.0001             nan     0.3360   -0.0000
   220        0.0000             nan     0.3360   -0.0000
   240        0.0000             nan     0.3360   -0.0000
   260        0.0000             nan     0.3360   -0.0000
   280        0.0000             nan     0.3360   -0.0000
   300        0.0000             nan     0.3360   -0.0000
   320        0.0000             nan     0.3360   -0.0000
   340        0.0000             nan     0.3360   -0.0000
   360        0.0000             nan     0.3360   -0.0000
   380        0.0000             nan     0.3360   -0.0000
   400        0.0000             nan     0.3360   -0.0000
   420        0.0000             nan     0.3360   -0.0000
   440        0.0000             nan     0.3360   -0.0000
   460        0.0000             nan     0.3360   -0.0000
   480        0.0000             nan     0.3360   -0.0000
   500        0.0000             nan     0.3360   -0.0000
   520        0.0000             nan     0.3360   -0.0000
   540        0.0000             nan     0.3360   -0.0000
   560        0.0000             nan     0.3360   -0.0000
   580        0.0000             nan     0.3360   -0.0000
   600        0.0000             nan     0.3360   -0.0000
   620        0.0000             nan     0.3360   -0.0000
   640        0.0000             nan     0.3360   -0.0000
   660        0.0000             nan     0.3360   -0.0000
   680        0.0000             nan     0.3360   -0.0000
   700        0.0000             nan     0.3360   -0.0000
   720        0.0000             nan     0.3360   -0.0000
   740        0.0000             nan     0.3360   -0.0000
   760        0.0000             nan     0.3360   -0.0000
   780        0.0000             nan     0.3360   -0.0000
   800        0.0000             nan     0.3360   -0.0000
   820        0.0000             nan     0.3360   -0.0000
   840        0.0000             nan     0.3360   -0.0000
   860        0.0000             nan     0.3360   -0.0000
   880        0.0000             nan     0.3360   -0.0000
   900        0.0000             nan     0.3360   -0.0000
   920        0.0000             nan     0.3360    0.0000
   940        0.0000             nan     0.3360   -0.0000
   960        0.0000             nan     0.3360   -0.0000
   980        0.0000             nan     0.3360   -0.0000
  1000        0.0000             nan     0.3360   -0.0000
  1020        0.0000             nan     0.3360   -0.0000
  1040        0.0000             nan     0.3360   -0.0000
  1060        0.0000             nan     0.3360   -0.0000
  1080        0.0000             nan     0.3360   -0.0000
  1100        0.0000             nan     0.3360   -0.0000
  1120        0.0000             nan     0.3360   -0.0000
  1140        0.0000             nan     0.3360   -0.0000
  1160        0.0000             nan     0.3360   -0.0000
  1180        0.0000             nan     0.3360   -0.0000
  1200        0.0000             nan     0.3360   -0.0000
  1220        0.0000             nan     0.3360   -0.0000
  1240        0.0000             nan     0.3360   -0.0000
  1260        0.0000             nan     0.3360   -0.0000
  1280        0.0000             nan     0.3360   -0.0000
  1300        0.0000             nan     0.3360   -0.0000
  1320        0.0000             nan     0.3360   -0.0000
  1340        0.0000             nan     0.3360   -0.0000
  1360        0.0000             nan     0.3360   -0.0000
  1380        0.0000             nan     0.3360   -0.0000
  1400        0.0000             nan     0.3360   -0.0000
  1420        0.0000             nan     0.3360   -0.0000
  1440        0.0000             nan     0.3360   -0.0000
  1460        0.0000             nan     0.3360   -0.0000
  1480        0.0000             nan     0.3360   -0.0000
  1500        0.0000             nan     0.3360    0.0000
  1520        0.0000             nan     0.3360   -0.0000
  1540        0.0000             nan     0.3360   -0.0000
  1560        0.0000             nan     0.3360   -0.0000
  1580        0.0000             nan     0.3360   -0.0000
  1600        0.0000             nan     0.3360   -0.0000
  1620        0.0000             nan     0.3360   -0.0000
  1640        0.0000             nan     0.3360   -0.0000
  1660        0.0000             nan     0.3360   -0.0000
  1680        0.0000             nan     0.3360    0.0000
  1700        0.0000             nan     0.3360   -0.0000
  1720        0.0000             nan     0.3360   -0.0000
  1740        0.0000             nan     0.3360    0.0000
  1760        0.0000             nan     0.3360   -0.0000
  1780        0.0000             nan     0.3360   -0.0000
  1800        0.0000             nan     0.3360   -0.0000
  1820        0.0000             nan     0.3360   -0.0000
  1840        0.0000             nan     0.3360   -0.0000
  1860        0.0000             nan     0.3360   -0.0000
  1880        0.0000             nan     0.3360   -0.0000
  1900        0.0000             nan     0.3360    0.0000
  1920        0.0000             nan     0.3360   -0.0000
  1940        0.0000             nan     0.3360   -0.0000
  1960        0.0000             nan     0.3360    0.0000
  1980        0.0000             nan     0.3360   -0.0000
  2000        0.0000             nan     0.3360   -0.0000
  2020        0.0000             nan     0.3360   -0.0000
  2040        0.0000             nan     0.3360   -0.0000
  2060        0.0000             nan     0.3360   -0.0000
  2080        0.0000             nan     0.3360   -0.0000
  2100        0.0000             nan     0.3360   -0.0000
  2120        0.0000             nan     0.3360    0.0000
  2140        0.0000             nan     0.3360   -0.0000
  2160        0.0000             nan     0.3360   -0.0000
  2180        0.0000             nan     0.3360   -0.0000
  2200        0.0000             nan     0.3360   -0.0000
  2220        0.0000             nan     0.3360   -0.0000
  2240        0.0000             nan     0.3360   -0.0000
  2260        0.0000             nan     0.3360   -0.0000
  2280        0.0000             nan     0.3360   -0.0000
  2300        0.0000             nan     0.3360   -0.0000
  2320        0.0000             nan     0.3360    0.0000
  2340        0.0000             nan     0.3360   -0.0000
  2360        0.0000             nan     0.3360    0.0000
  2380        0.0000             nan     0.3360   -0.0000
  2400        0.0000             nan     0.3360   -0.0000
  2420        0.0000             nan     0.3360   -0.0000
  2440        0.0000             nan     0.3360    0.0000
  2460        0.0000             nan     0.3360   -0.0000
  2480        0.0000             nan     0.3360   -0.0000
  2500        0.0000             nan     0.3360   -0.0000
  2520        0.0000             nan     0.3360   -0.0000
  2540        0.0000             nan     0.3360   -0.0000
  2560        0.0000             nan     0.3360   -0.0000
  2580        0.0000             nan     0.3360   -0.0000
  2600        0.0000             nan     0.3360   -0.0000
  2620        0.0000             nan     0.3360   -0.0000
  2640        0.0000             nan     0.3360   -0.0000
  2660        0.0000             nan     0.3360   -0.0000
  2680        0.0000             nan     0.3360   -0.0000
  2700        0.0000             nan     0.3360   -0.0000
  2720        0.0000             nan     0.3360    0.0000
  2740        0.0000             nan     0.3360   -0.0000
  2760        0.0000             nan     0.3360   -0.0000
  2780        0.0000             nan     0.3360   -0.0000
  2800        0.0000             nan     0.3360   -0.0000
  2820        0.0000             nan     0.3360   -0.0000
  2840        0.0000             nan     0.3360    0.0000
  2860        0.0000             nan     0.3360    0.0000
  2875        0.0000             nan     0.3360    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0158             nan     0.4093    0.0250
     2        0.0063             nan     0.4093    0.0097
     3        0.0026             nan     0.4093    0.0036
     4        0.0011             nan     0.4093    0.0014
     5        0.0005             nan     0.4093    0.0005
     6        0.0003             nan     0.4093    0.0002
     7        0.0002             nan     0.4093    0.0001
     8        0.0002             nan     0.4093    0.0000
     9        0.0002             nan     0.4093    0.0000
    10        0.0001             nan     0.4093   -0.0000
    20        0.0001             nan     0.4093   -0.0000
    40        0.0000             nan     0.4093   -0.0000
    60        0.0000             nan     0.4093   -0.0000
    80        0.0000             nan     0.4093   -0.0000
   100        0.0000             nan     0.4093   -0.0000
   120        0.0000             nan     0.4093   -0.0000
   140        0.0000             nan     0.4093   -0.0000
   160        0.0000             nan     0.4093   -0.0000
   180        0.0000             nan     0.4093   -0.0000
   200        0.0000             nan     0.4093   -0.0000
   220        0.0000             nan     0.4093   -0.0000
   240        0.0000             nan     0.4093   -0.0000
   260        0.0000             nan     0.4093   -0.0000
   280        0.0000             nan     0.4093   -0.0000
   300        0.0000             nan     0.4093   -0.0000
   320        0.0000             nan     0.4093   -0.0000
   340        0.0000             nan     0.4093   -0.0000
   360        0.0000             nan     0.4093    0.0000
   380        0.0000             nan     0.4093   -0.0000
   400        0.0000             nan     0.4093    0.0000
   420        0.0000             nan     0.4093   -0.0000
   440        0.0000             nan     0.4093    0.0000
   460        0.0000             nan     0.4093    0.0000
   480        0.0000             nan     0.4093   -0.0000
   500        0.0000             nan     0.4093   -0.0000
   520        0.0000             nan     0.4093   -0.0000
   540        0.0000             nan     0.4093   -0.0000
   560        0.0000             nan     0.4093   -0.0000
   580        0.0000             nan     0.4093   -0.0000
   600        0.0000             nan     0.4093    0.0000
   620        0.0000             nan     0.4093    0.0000
   640        0.0000             nan     0.4093    0.0000
   660        0.0000             nan     0.4093   -0.0000
   680        0.0000             nan     0.4093   -0.0000
   700        0.0000             nan     0.4093   -0.0000
   720        0.0000             nan     0.4093   -0.0000
   740        0.0000             nan     0.4093   -0.0000
   760        0.0000             nan     0.4093    0.0000
   780        0.0000             nan     0.4093   -0.0000
   800        0.0000             nan     0.4093   -0.0000
   820        0.0000             nan     0.4093   -0.0000
   840        0.0000             nan     0.4093   -0.0000
   860        0.0000             nan     0.4093    0.0000
   880        0.0000             nan     0.4093   -0.0000
   900        0.0000             nan     0.4093   -0.0000
   920        0.0000             nan     0.4093   -0.0000
   940        0.0000             nan     0.4093   -0.0000
   960        0.0000             nan     0.4093    0.0000
   980        0.0000             nan     0.4093   -0.0000
  1000        0.0000             nan     0.4093   -0.0000
  1020        0.0000             nan     0.4093   -0.0000
  1040        0.0000             nan     0.4093   -0.0000
  1060        0.0000             nan     0.4093   -0.0000
  1080        0.0000             nan     0.4093   -0.0000
  1100        0.0000             nan     0.4093   -0.0000
  1120        0.0000             nan     0.4093   -0.0000
  1140        0.0000             nan     0.4093   -0.0000
  1160        0.0000             nan     0.4093   -0.0000
  1180        0.0000             nan     0.4093   -0.0000
  1200        0.0000             nan     0.4093   -0.0000
  1220        0.0000             nan     0.4093   -0.0000
  1240        0.0000             nan     0.4093   -0.0000
  1260        0.0000             nan     0.4093   -0.0000
  1280        0.0000             nan     0.4093   -0.0000
  1300        0.0000             nan     0.4093   -0.0000
  1320        0.0000             nan     0.4093   -0.0000
  1340        0.0000             nan     0.4093   -0.0000
  1360        0.0000             nan     0.4093   -0.0000
  1380        0.0000             nan     0.4093   -0.0000
  1400        0.0000             nan     0.4093    0.0000
  1420        0.0000             nan     0.4093    0.0000
  1440        0.0000             nan     0.4093   -0.0000
  1460        0.0000             nan     0.4093    0.0000
  1480        0.0000             nan     0.4093   -0.0000
  1500        0.0000             nan     0.4093   -0.0000
  1520        0.0000             nan     0.4093   -0.0000
  1540        0.0000             nan     0.4093   -0.0000
  1560        0.0000             nan     0.4093   -0.0000
  1580        0.0000             nan     0.4093   -0.0000
  1600        0.0000             nan     0.4093   -0.0000
  1620        0.0000             nan     0.4093   -0.0000
  1640        0.0000             nan     0.4093   -0.0000
  1660        0.0000             nan     0.4093   -0.0000
  1680        0.0000             nan     0.4093   -0.0000
  1700        0.0000             nan     0.4093   -0.0000
  1720        0.0000             nan     0.4093   -0.0000
  1740        0.0000             nan     0.4093   -0.0000
  1760        0.0000             nan     0.4093   -0.0000
  1780        0.0000             nan     0.4093   -0.0000
  1800        0.0000             nan     0.4093   -0.0000
  1820        0.0000             nan     0.4093   -0.0000
  1840        0.0000             nan     0.4093    0.0000
  1860        0.0000             nan     0.4093   -0.0000
  1880        0.0000             nan     0.4093   -0.0000
  1900        0.0000             nan     0.4093   -0.0000
  1920        0.0000             nan     0.4093   -0.0000
  1940        0.0000             nan     0.4093   -0.0000
  1960        0.0000             nan     0.4093   -0.0000
  1980        0.0000             nan     0.4093   -0.0000
  2000        0.0000             nan     0.4093   -0.0000
  2020        0.0000             nan     0.4093   -0.0000
  2040        0.0000             nan     0.4093   -0.0000
  2060        0.0000             nan     0.4093   -0.0000
  2080        0.0000             nan     0.4093    0.0000
  2100        0.0000             nan     0.4093   -0.0000
  2120        0.0000             nan     0.4093   -0.0000
  2140        0.0000             nan     0.4093    0.0000
  2160        0.0000             nan     0.4093    0.0000
  2180        0.0000             nan     0.4093   -0.0000
  2200        0.0000             nan     0.4093   -0.0000
  2220        0.0000             nan     0.4093   -0.0000
  2240        0.0000             nan     0.4093   -0.0000
  2260        0.0000             nan     0.4093   -0.0000
  2280        0.0000             nan     0.4093   -0.0000
  2300        0.0000             nan     0.4093   -0.0000
  2320        0.0000             nan     0.4093   -0.0000
  2340        0.0000             nan     0.4093   -0.0000
  2360        0.0000             nan     0.4093    0.0000
  2380        0.0000             nan     0.4093    0.0000
  2400        0.0000             nan     0.4093    0.0000
  2420        0.0000             nan     0.4093   -0.0000
  2440        0.0000             nan     0.4093   -0.0000
  2460        0.0000             nan     0.4093   -0.0000
  2480        0.0000             nan     0.4093   -0.0000
  2500        0.0000             nan     0.4093   -0.0000
  2520        0.0000             nan     0.4093   -0.0000
  2540        0.0000             nan     0.4093   -0.0000
  2560        0.0000             nan     0.4093   -0.0000
  2580        0.0000             nan     0.4093    0.0000
  2600        0.0000             nan     0.4093   -0.0000
  2620        0.0000             nan     0.4093   -0.0000
  2640        0.0000             nan     0.4093   -0.0000
  2660        0.0000             nan     0.4093    0.0000
  2680        0.0000             nan     0.4093    0.0000
  2700        0.0000             nan     0.4093    0.0000
  2720        0.0000             nan     0.4093   -0.0000
  2740        0.0000             nan     0.4093    0.0000
  2760        0.0000             nan     0.4093   -0.0000
  2780        0.0000             nan     0.4093   -0.0000
  2800        0.0000             nan     0.4093   -0.0000
  2820        0.0000             nan     0.4093   -0.0000
  2840        0.0000             nan     0.4093   -0.0000
  2860        0.0000             nan     0.4093   -0.0000
  2880        0.0000             nan     0.4093   -0.0000
  2900        0.0000             nan     0.4093    0.0000
  2920        0.0000             nan     0.4093   -0.0000
  2940        0.0000             nan     0.4093   -0.0000
  2960        0.0000             nan     0.4093   -0.0000
  2980        0.0000             nan     0.4093   -0.0000
  3000        0.0000             nan     0.4093   -0.0000
  3020        0.0000             nan     0.4093    0.0000
  3040        0.0000             nan     0.4093   -0.0000
  3060        0.0000             nan     0.4093    0.0000
  3080        0.0000             nan     0.4093   -0.0000
  3100        0.0000             nan     0.4093   -0.0000
  3120        0.0000             nan     0.4093    0.0000
  3140        0.0000             nan     0.4093   -0.0000
  3159        0.0000             nan     0.4093   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0147             nan     0.4257    0.0257
     2        0.0056             nan     0.4257    0.0091
     3        0.0022             nan     0.4257    0.0034
     4        0.0010             nan     0.4257    0.0012
     5        0.0005             nan     0.4257    0.0004
     6        0.0003             nan     0.4257    0.0002
     7        0.0003             nan     0.4257    0.0001
     8        0.0002             nan     0.4257    0.0000
     9        0.0002             nan     0.4257   -0.0000
    10        0.0002             nan     0.4257   -0.0000
    20        0.0001             nan     0.4257   -0.0000
    40        0.0001             nan     0.4257   -0.0000
    60        0.0000             nan     0.4257   -0.0000
    80        0.0000             nan     0.4257   -0.0000
   100        0.0000             nan     0.4257   -0.0000
   120        0.0000             nan     0.4257   -0.0000
   140        0.0000             nan     0.4257   -0.0000
   160        0.0000             nan     0.4257   -0.0000
   180        0.0000             nan     0.4257   -0.0000
   200        0.0000             nan     0.4257   -0.0000
   220        0.0000             nan     0.4257   -0.0000
   240        0.0000             nan     0.4257   -0.0000
   260        0.0000             nan     0.4257   -0.0000
   280        0.0000             nan     0.4257   -0.0000
   300        0.0000             nan     0.4257   -0.0000
   320        0.0000             nan     0.4257   -0.0000
   340        0.0000             nan     0.4257   -0.0000
   360        0.0000             nan     0.4257   -0.0000
   380        0.0000             nan     0.4257   -0.0000
   400        0.0000             nan     0.4257   -0.0000
   420        0.0000             nan     0.4257   -0.0000
   440        0.0000             nan     0.4257   -0.0000
   460        0.0000             nan     0.4257   -0.0000
   480        0.0000             nan     0.4257   -0.0000
   500        0.0000             nan     0.4257   -0.0000
   520        0.0000             nan     0.4257   -0.0000
   540        0.0000             nan     0.4257   -0.0000
   560        0.0000             nan     0.4257   -0.0000
   580        0.0000             nan     0.4257   -0.0000
   600        0.0000             nan     0.4257   -0.0000
   620        0.0000             nan     0.4257   -0.0000
   640        0.0000             nan     0.4257   -0.0000
   660        0.0000             nan     0.4257   -0.0000
   680        0.0000             nan     0.4257   -0.0000
   700        0.0000             nan     0.4257    0.0000
   720        0.0000             nan     0.4257    0.0000
   740        0.0000             nan     0.4257    0.0000
   760        0.0000             nan     0.4257   -0.0000
   780        0.0000             nan     0.4257   -0.0000
   800        0.0000             nan     0.4257   -0.0000
   820        0.0000             nan     0.4257   -0.0000
   840        0.0000             nan     0.4257   -0.0000
   860        0.0000             nan     0.4257   -0.0000
   880        0.0000             nan     0.4257   -0.0000
   900        0.0000             nan     0.4257   -0.0000
   920        0.0000             nan     0.4257   -0.0000
   940        0.0000             nan     0.4257   -0.0000
   960        0.0000             nan     0.4257   -0.0000
   980        0.0000             nan     0.4257   -0.0000
  1000        0.0000             nan     0.4257   -0.0000
  1020        0.0000             nan     0.4257   -0.0000
  1040        0.0000             nan     0.4257   -0.0000
  1060        0.0000             nan     0.4257   -0.0000
  1080        0.0000             nan     0.4257   -0.0000
  1100        0.0000             nan     0.4257   -0.0000
  1120        0.0000             nan     0.4257   -0.0000
  1140        0.0000             nan     0.4257   -0.0000
  1160        0.0000             nan     0.4257   -0.0000
  1180        0.0000             nan     0.4257    0.0000
  1200        0.0000             nan     0.4257   -0.0000
  1220        0.0000             nan     0.4257   -0.0000
  1240        0.0000             nan     0.4257   -0.0000
  1260        0.0000             nan     0.4257   -0.0000
  1280        0.0000             nan     0.4257   -0.0000
  1300        0.0000             nan     0.4257   -0.0000
  1320        0.0000             nan     0.4257   -0.0000
  1340        0.0000             nan     0.4257    0.0000
  1360        0.0000             nan     0.4257   -0.0000
  1380        0.0000             nan     0.4257   -0.0000
  1400        0.0000             nan     0.4257    0.0000
  1420        0.0000             nan     0.4257   -0.0000
  1440        0.0000             nan     0.4257   -0.0000
  1460        0.0000             nan     0.4257   -0.0000
  1480        0.0000             nan     0.4257   -0.0000
  1500        0.0000             nan     0.4257   -0.0000
  1520        0.0000             nan     0.4257   -0.0000
  1540        0.0000             nan     0.4257   -0.0000
  1560        0.0000             nan     0.4257   -0.0000
  1580        0.0000             nan     0.4257    0.0000
  1600        0.0000             nan     0.4257    0.0000
  1620        0.0000             nan     0.4257    0.0000
  1640        0.0000             nan     0.4257   -0.0000
  1660        0.0000             nan     0.4257    0.0000
  1680        0.0000             nan     0.4257   -0.0000
  1700        0.0000             nan     0.4257   -0.0000
  1720        0.0000             nan     0.4257   -0.0000
  1740        0.0000             nan     0.4257   -0.0000
  1760        0.0000             nan     0.4257   -0.0000
  1780        0.0000             nan     0.4257   -0.0000
  1800        0.0000             nan     0.4257   -0.0000
  1818        0.0000             nan     0.4257   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0124             nan     0.4962    0.0298
     2        0.0045             nan     0.4962    0.0079
     3        0.0018             nan     0.4962    0.0028
     4        0.0010             nan     0.4962    0.0009
     5        0.0007             nan     0.4962    0.0003
     6        0.0006             nan     0.4962    0.0001
     7        0.0006             nan     0.4962    0.0001
     8        0.0005             nan     0.4962    0.0000
     9        0.0004             nan     0.4962   -0.0000
    10        0.0004             nan     0.4962    0.0000
    20        0.0003             nan     0.4962   -0.0000
    40        0.0002             nan     0.4962   -0.0000
    60        0.0001             nan     0.4962   -0.0000
    80        0.0001             nan     0.4962   -0.0000
   100        0.0001             nan     0.4962   -0.0000
   120        0.0001             nan     0.4962   -0.0000
   140        0.0000             nan     0.4962   -0.0000
   160        0.0000             nan     0.4962   -0.0000
   180        0.0000             nan     0.4962   -0.0000
   200        0.0000             nan     0.4962   -0.0000
   220        0.0000             nan     0.4962   -0.0000
   240        0.0000             nan     0.4962   -0.0000
   260        0.0000             nan     0.4962   -0.0000
   280        0.0000             nan     0.4962   -0.0000
   300        0.0000             nan     0.4962   -0.0000
   320        0.0000             nan     0.4962   -0.0000
   340        0.0000             nan     0.4962   -0.0000
   360        0.0000             nan     0.4962   -0.0000
   380        0.0000             nan     0.4962   -0.0000
   400        0.0000             nan     0.4962   -0.0000
   420        0.0000             nan     0.4962   -0.0000
   440        0.0000             nan     0.4962   -0.0000
   460        0.0000             nan     0.4962   -0.0000
   480        0.0000             nan     0.4962   -0.0000
   500        0.0000             nan     0.4962   -0.0000
   520        0.0000             nan     0.4962   -0.0000
   540        0.0000             nan     0.4962   -0.0000
   560        0.0000             nan     0.4962   -0.0000
   580        0.0000             nan     0.4962   -0.0000
   600        0.0000             nan     0.4962   -0.0000
   620        0.0000             nan     0.4962   -0.0000
   640        0.0000             nan     0.4962   -0.0000
   660        0.0000             nan     0.4962   -0.0000
   680        0.0000             nan     0.4962   -0.0000
   700        0.0000             nan     0.4962    0.0000
   720        0.0000             nan     0.4962   -0.0000
   740        0.0000             nan     0.4962    0.0000
   760        0.0000             nan     0.4962    0.0000
   780        0.0000             nan     0.4962   -0.0000
   800        0.0000             nan     0.4962   -0.0000
   820        0.0000             nan     0.4962   -0.0000
   840        0.0000             nan     0.4962   -0.0000
   860        0.0000             nan     0.4962   -0.0000
   880        0.0000             nan     0.4962   -0.0000
   900        0.0000             nan     0.4962   -0.0000
   920        0.0000             nan     0.4962   -0.0000
   940        0.0000             nan     0.4962   -0.0000
   960        0.0000             nan     0.4962   -0.0000
   980        0.0000             nan     0.4962   -0.0000
  1000        0.0000             nan     0.4962   -0.0000
  1020        0.0000             nan     0.4962   -0.0000
  1040        0.0000             nan     0.4962   -0.0000
  1060        0.0000             nan     0.4962   -0.0000
  1080        0.0000             nan     0.4962   -0.0000
  1100        0.0000             nan     0.4962   -0.0000
  1120        0.0000             nan     0.4962   -0.0000
  1140        0.0000             nan     0.4962    0.0000
  1160        0.0000             nan     0.4962   -0.0000
  1180        0.0000             nan     0.4962   -0.0000
  1200        0.0000             nan     0.4962   -0.0000
  1220        0.0000             nan     0.4962   -0.0000
  1240        0.0000             nan     0.4962   -0.0000
  1260        0.0000             nan     0.4962   -0.0000
  1280        0.0000             nan     0.4962   -0.0000
  1300        0.0000             nan     0.4962   -0.0000
  1320        0.0000             nan     0.4962   -0.0000
  1340        0.0000             nan     0.4962    0.0000
  1360        0.0000             nan     0.4962   -0.0000
  1380        0.0000             nan     0.4962   -0.0000
  1400        0.0000             nan     0.4962   -0.0000
  1420        0.0000             nan     0.4962   -0.0000
  1440        0.0000             nan     0.4962    0.0000
  1460        0.0000             nan     0.4962   -0.0000
  1480        0.0000             nan     0.4962    0.0000
  1500        0.0000             nan     0.4962   -0.0000
  1520        0.0000             nan     0.4962   -0.0000
  1540        0.0000             nan     0.4962   -0.0000
  1560        0.0000             nan     0.4962    0.0000
  1580        0.0000             nan     0.4962   -0.0000
  1600        0.0000             nan     0.4962   -0.0000
  1620        0.0000             nan     0.4962   -0.0000
  1640        0.0000             nan     0.4962   -0.0000
  1660        0.0000             nan     0.4962    0.0000
  1680        0.0000             nan     0.4962   -0.0000
  1700        0.0000             nan     0.4962   -0.0000
  1720        0.0000             nan     0.4962   -0.0000
  1740        0.0000             nan     0.4962   -0.0000
  1760        0.0000             nan     0.4962   -0.0000
  1780        0.0000             nan     0.4962   -0.0000
  1800        0.0000             nan     0.4962   -0.0000
  1820        0.0000             nan     0.4962    0.0000
  1840        0.0000             nan     0.4962    0.0000
  1860        0.0000             nan     0.4962   -0.0000
  1880        0.0000             nan     0.4962    0.0000
  1892        0.0000             nan     0.4962    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0126             nan     0.5090    0.0310
     2        0.0044             nan     0.5090    0.0082
     3        0.0018             nan     0.5090    0.0025
     4        0.0010             nan     0.5090    0.0007
     5        0.0008             nan     0.5090    0.0002
     6        0.0006             nan     0.5090    0.0001
     7        0.0006             nan     0.5090    0.0000
     8        0.0005             nan     0.5090    0.0000
     9        0.0005             nan     0.5090    0.0000
    10        0.0004             nan     0.5090    0.0000
    20        0.0003             nan     0.5090   -0.0000
    40        0.0001             nan     0.5090   -0.0000
    60        0.0001             nan     0.5090   -0.0000
    80        0.0001             nan     0.5090   -0.0000
   100        0.0001             nan     0.5090   -0.0000
   120        0.0001             nan     0.5090   -0.0000
   140        0.0001             nan     0.5090   -0.0000
   160        0.0000             nan     0.5090   -0.0000
   180        0.0000             nan     0.5090   -0.0000
   200        0.0000             nan     0.5090   -0.0000
   220        0.0000             nan     0.5090   -0.0000
   240        0.0000             nan     0.5090   -0.0000
   260        0.0000             nan     0.5090   -0.0000
   280        0.0000             nan     0.5090   -0.0000
   300        0.0000             nan     0.5090   -0.0000
   320        0.0000             nan     0.5090   -0.0000
   340        0.0000             nan     0.5090   -0.0000
   360        0.0000             nan     0.5090   -0.0000
   380        0.0000             nan     0.5090   -0.0000
   400        0.0000             nan     0.5090   -0.0000
   420        0.0000             nan     0.5090   -0.0000
   440        0.0000             nan     0.5090   -0.0000
   460        0.0000             nan     0.5090   -0.0000
   480        0.0000             nan     0.5090   -0.0000
   500        0.0000             nan     0.5090   -0.0000
   520        0.0000             nan     0.5090   -0.0000
   540        0.0000             nan     0.5090   -0.0000
   560        0.0000             nan     0.5090   -0.0000
   580        0.0000             nan     0.5090   -0.0000
   600        0.0000             nan     0.5090   -0.0000
   620        0.0000             nan     0.5090   -0.0000
   640        0.0000             nan     0.5090   -0.0000
   660        0.0000             nan     0.5090   -0.0000
   680        0.0000             nan     0.5090   -0.0000
   700        0.0000             nan     0.5090   -0.0000
   720        0.0000             nan     0.5090   -0.0000
   740        0.0000             nan     0.5090   -0.0000
   760        0.0000             nan     0.5090   -0.0000
   780        0.0000             nan     0.5090   -0.0000
   800        0.0000             nan     0.5090   -0.0000
   820        0.0000             nan     0.5090   -0.0000
   840        0.0000             nan     0.5090   -0.0000
   860        0.0000             nan     0.5090   -0.0000
   880        0.0000             nan     0.5090   -0.0000
   900        0.0000             nan     0.5090   -0.0000
   920        0.0000             nan     0.5090   -0.0000
   940        0.0000             nan     0.5090   -0.0000
   960        0.0000             nan     0.5090   -0.0000
   980        0.0000             nan     0.5090   -0.0000
  1000        0.0000             nan     0.5090   -0.0000
  1020        0.0000             nan     0.5090    0.0000
  1040        0.0000             nan     0.5090   -0.0000
  1060        0.0000             nan     0.5090   -0.0000
  1080        0.0000             nan     0.5090   -0.0000
  1100        0.0000             nan     0.5090   -0.0000
  1120        0.0000             nan     0.5090   -0.0000
  1140        0.0000             nan     0.5090   -0.0000
  1160        0.0000             nan     0.5090   -0.0000
  1180        0.0000             nan     0.5090   -0.0000
  1200        0.0000             nan     0.5090   -0.0000
  1220        0.0000             nan     0.5090   -0.0000
  1240        0.0000             nan     0.5090   -0.0000
  1260        0.0000             nan     0.5090   -0.0000
  1280        0.0000             nan     0.5090   -0.0000
  1300        0.0000             nan     0.5090   -0.0000
  1320        0.0000             nan     0.5090   -0.0000
  1340        0.0000             nan     0.5090   -0.0000
  1360        0.0000             nan     0.5090   -0.0000
  1380        0.0000             nan     0.5090   -0.0000
  1400        0.0000             nan     0.5090   -0.0000
  1420        0.0000             nan     0.5090   -0.0000
  1440        0.0000             nan     0.5090    0.0000
  1460        0.0000             nan     0.5090    0.0000
  1480        0.0000             nan     0.5090   -0.0000
  1500        0.0000             nan     0.5090   -0.0000
  1520        0.0000             nan     0.5090   -0.0000
  1540        0.0000             nan     0.5090    0.0000
  1560        0.0000             nan     0.5090   -0.0000
  1580        0.0000             nan     0.5090    0.0000
  1600        0.0000             nan     0.5090   -0.0000
  1620        0.0000             nan     0.5090   -0.0000
  1640        0.0000             nan     0.5090   -0.0000
  1660        0.0000             nan     0.5090   -0.0000
  1680        0.0000             nan     0.5090   -0.0000
  1700        0.0000             nan     0.5090    0.0000
  1720        0.0000             nan     0.5090   -0.0000
  1740        0.0000             nan     0.5090   -0.0000
  1760        0.0000             nan     0.5090   -0.0000
  1780        0.0000             nan     0.5090   -0.0000
  1800        0.0000             nan     0.5090   -0.0000
  1820        0.0000             nan     0.5090   -0.0000
  1840        0.0000             nan     0.5090   -0.0000
  1860        0.0000             nan     0.5090   -0.0000
  1880        0.0000             nan     0.5090    0.0000
  1900        0.0000             nan     0.5090   -0.0000
  1920        0.0000             nan     0.5090   -0.0000
  1940        0.0000             nan     0.5090    0.0000
  1960        0.0000             nan     0.5090    0.0000
  1980        0.0000             nan     0.5090   -0.0000
  2000        0.0000             nan     0.5090   -0.0000
  2020        0.0000             nan     0.5090   -0.0000
  2040        0.0000             nan     0.5090   -0.0000
  2060        0.0000             nan     0.5090    0.0000
  2080        0.0000             nan     0.5090    0.0000
  2100        0.0000             nan     0.5090   -0.0000
  2120        0.0000             nan     0.5090   -0.0000
  2140        0.0000             nan     0.5090   -0.0000
  2160        0.0000             nan     0.5090   -0.0000
  2180        0.0000             nan     0.5090   -0.0000
  2200        0.0000             nan     0.5090   -0.0000
  2220        0.0000             nan     0.5090    0.0000
  2240        0.0000             nan     0.5090   -0.0000
  2260        0.0000             nan     0.5090   -0.0000
  2280        0.0000             nan     0.5090   -0.0000
  2300        0.0000             nan     0.5090   -0.0000
  2320        0.0000             nan     0.5090   -0.0000
  2340        0.0000             nan     0.5090   -0.0000
  2360        0.0000             nan     0.5090   -0.0000
  2380        0.0000             nan     0.5090   -0.0000
  2400        0.0000             nan     0.5090   -0.0000
  2420        0.0000             nan     0.5090   -0.0000
  2440        0.0000             nan     0.5090   -0.0000
  2460        0.0000             nan     0.5090   -0.0000
  2480        0.0000             nan     0.5090   -0.0000
  2500        0.0000             nan     0.5090    0.0000
  2520        0.0000             nan     0.5090   -0.0000
  2540        0.0000             nan     0.5090   -0.0000
  2560        0.0000             nan     0.5090    0.0000
  2580        0.0000             nan     0.5090   -0.0000
  2600        0.0000             nan     0.5090    0.0000
  2620        0.0000             nan     0.5090   -0.0000
  2640        0.0000             nan     0.5090    0.0000
  2660        0.0000             nan     0.5090    0.0000
  2680        0.0000             nan     0.5090   -0.0000
  2700        0.0000             nan     0.5090   -0.0000
  2720        0.0000             nan     0.5090   -0.0000
  2740        0.0000             nan     0.5090   -0.0000
  2760        0.0000             nan     0.5090    0.0000
  2780        0.0000             nan     0.5090    0.0000
  2800        0.0000             nan     0.5090   -0.0000
  2820        0.0000             nan     0.5090    0.0000
  2840        0.0000             nan     0.5090   -0.0000
  2860        0.0000             nan     0.5090   -0.0000
  2880        0.0000             nan     0.5090   -0.0000
  2900        0.0000             nan     0.5090    0.0000
  2920        0.0000             nan     0.5090   -0.0000
  2940        0.0000             nan     0.5090   -0.0000
  2960        0.0000             nan     0.5090   -0.0000
  2980        0.0000             nan     0.5090   -0.0000
  3000        0.0000             nan     0.5090   -0.0000
  3020        0.0000             nan     0.5090   -0.0000
  3040        0.0000             nan     0.5090   -0.0000
  3060        0.0000             nan     0.5090   -0.0000
  3080        0.0000             nan     0.5090   -0.0000
  3100        0.0000             nan     0.5090   -0.0000
  3120        0.0000             nan     0.5090   -0.0000
  3140        0.0000             nan     0.5090   -0.0000
  3160        0.0000             nan     0.5090    0.0000
  3180        0.0000             nan     0.5090   -0.0000
  3200        0.0000             nan     0.5090   -0.0000
  3220        0.0000             nan     0.5090   -0.0000
  3240        0.0000             nan     0.5090   -0.0000
  3260        0.0000             nan     0.5090   -0.0000
  3280        0.0000             nan     0.5090    0.0000
  3300        0.0000             nan     0.5090    0.0000
  3320        0.0000             nan     0.5090   -0.0000
  3340        0.0000             nan     0.5090   -0.0000
  3360        0.0000             nan     0.5090   -0.0000
  3380        0.0000             nan     0.5090   -0.0000
  3400        0.0000             nan     0.5090   -0.0000
  3420        0.0000             nan     0.5090   -0.0000
  3440        0.0000             nan     0.5090   -0.0000
  3460        0.0000             nan     0.5090   -0.0000
  3480        0.0000             nan     0.5090   -0.0000
  3500        0.0000             nan     0.5090   -0.0000
  3520        0.0000             nan     0.5090    0.0000
  3540        0.0000             nan     0.5090   -0.0000
  3560        0.0000             nan     0.5090   -0.0000
  3580        0.0000             nan     0.5090   -0.0000
  3600        0.0000             nan     0.5090   -0.0000
  3620        0.0000             nan     0.5090   -0.0000
  3640        0.0000             nan     0.5090   -0.0000
  3660        0.0000             nan     0.5090   -0.0000
  3680        0.0000             nan     0.5090   -0.0000
  3700        0.0000             nan     0.5090    0.0000
  3720        0.0000             nan     0.5090   -0.0000
  3740        0.0000             nan     0.5090   -0.0000
  3760        0.0000             nan     0.5090   -0.0000
  3780        0.0000             nan     0.5090   -0.0000
  3800        0.0000             nan     0.5090   -0.0000
  3820        0.0000             nan     0.5090   -0.0000
  3840        0.0000             nan     0.5090    0.0000
  3860        0.0000             nan     0.5090   -0.0000
  3880        0.0000             nan     0.5090   -0.0000
  3900        0.0000             nan     0.5090    0.0000
  3920        0.0000             nan     0.5090   -0.0000
  3940        0.0000             nan     0.5090   -0.0000
  3960        0.0000             nan     0.5090   -0.0000
  3980        0.0000             nan     0.5090   -0.0000
  4000        0.0000             nan     0.5090    0.0000
  4020        0.0000             nan     0.5090    0.0000
  4040        0.0000             nan     0.5090    0.0000
  4060        0.0000             nan     0.5090    0.0000
  4080        0.0000             nan     0.5090    0.0000
  4100        0.0000             nan     0.5090   -0.0000
  4120        0.0000             nan     0.5090    0.0000
  4140        0.0000             nan     0.5090   -0.0000
  4157        0.0000             nan     0.5090   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0155             nan     0.5702    0.0266
     2        0.0060             nan     0.5702    0.0094
     3        0.0030             nan     0.5702    0.0031
     4        0.0021             nan     0.5702    0.0011
     5        0.0017             nan     0.5702    0.0003
     6        0.0014             nan     0.5702    0.0002
     7        0.0013             nan     0.5702    0.0001
     8        0.0012             nan     0.5702    0.0000
     9        0.0011             nan     0.5702    0.0002
    10        0.0010             nan     0.5702    0.0001
    20        0.0006             nan     0.5702    0.0000
    40        0.0003             nan     0.5702   -0.0000
    60        0.0002             nan     0.5702   -0.0000
    80        0.0002             nan     0.5702   -0.0000
   100        0.0001             nan     0.5702   -0.0000
   120        0.0001             nan     0.5702    0.0000
   140        0.0001             nan     0.5702   -0.0000
   160        0.0001             nan     0.5702   -0.0000
   180        0.0001             nan     0.5702   -0.0000
   200        0.0001             nan     0.5702    0.0000
   220        0.0001             nan     0.5702   -0.0000
   240        0.0001             nan     0.5702   -0.0000
   260        0.0000             nan     0.5702   -0.0000
   280        0.0000             nan     0.5702   -0.0000
   300        0.0000             nan     0.5702   -0.0000
   320        0.0000             nan     0.5702   -0.0000
   340        0.0000             nan     0.5702   -0.0000
   360        0.0000             nan     0.5702   -0.0000
   380        0.0000             nan     0.5702   -0.0000
   400        0.0000             nan     0.5702   -0.0000
   420        0.0000             nan     0.5702   -0.0000
   440        0.0000             nan     0.5702   -0.0000
   460        0.0000             nan     0.5702   -0.0000
   480        0.0000             nan     0.5702   -0.0000
   500        0.0000             nan     0.5702   -0.0000
   520        0.0000             nan     0.5702   -0.0000
   540        0.0000             nan     0.5702   -0.0000
   560        0.0000             nan     0.5702   -0.0000
   580        0.0000             nan     0.5702   -0.0000
   600        0.0000             nan     0.5702   -0.0000
   620        0.0000             nan     0.5702   -0.0000
   640        0.0000             nan     0.5702   -0.0000
   660        0.0000             nan     0.5702   -0.0000
   680        0.0000             nan     0.5702   -0.0000
   700        0.0000             nan     0.5702   -0.0000
   720        0.0000             nan     0.5702   -0.0000
   740        0.0000             nan     0.5702   -0.0000
   760        0.0000             nan     0.5702   -0.0000
   780        0.0000             nan     0.5702   -0.0000
   800        0.0000             nan     0.5702   -0.0000
   820        0.0000             nan     0.5702   -0.0000
   840        0.0000             nan     0.5702   -0.0000
   860        0.0000             nan     0.5702   -0.0000
   880        0.0000             nan     0.5702   -0.0000
   900        0.0000             nan     0.5702   -0.0000
   920        0.0000             nan     0.5702   -0.0000
   940        0.0000             nan     0.5702   -0.0000
   960        0.0000             nan     0.5702   -0.0000
   980        0.0000             nan     0.5702   -0.0000
  1000        0.0000             nan     0.5702   -0.0000
  1020        0.0000             nan     0.5702   -0.0000
  1040        0.0000             nan     0.5702   -0.0000
  1060        0.0000             nan     0.5702   -0.0000
  1080        0.0000             nan     0.5702   -0.0000
  1100        0.0000             nan     0.5702   -0.0000
  1120        0.0000             nan     0.5702   -0.0000
  1140        0.0000             nan     0.5702    0.0000
  1160        0.0000             nan     0.5702   -0.0000
  1180        0.0000             nan     0.5702   -0.0000
  1200        0.0000             nan     0.5702   -0.0000
  1220        0.0000             nan     0.5702   -0.0000
  1240        0.0000             nan     0.5702   -0.0000
  1260        0.0000             nan     0.5702   -0.0000
  1280        0.0000             nan     0.5702   -0.0000
  1300        0.0000             nan     0.5702   -0.0000
  1320        0.0000             nan     0.5702    0.0000
  1340        0.0000             nan     0.5702   -0.0000
  1360        0.0000             nan     0.5702   -0.0000
  1380        0.0000             nan     0.5702    0.0000
  1400        0.0000             nan     0.5702   -0.0000
  1420        0.0000             nan     0.5702   -0.0000
  1440        0.0000             nan     0.5702   -0.0000
  1460        0.0000             nan     0.5702    0.0000
  1480        0.0000             nan     0.5702   -0.0000
  1500        0.0000             nan     0.5702    0.0000
  1520        0.0000             nan     0.5702   -0.0000
  1540        0.0000             nan     0.5702   -0.0000
  1560        0.0000             nan     0.5702    0.0000
  1580        0.0000             nan     0.5702   -0.0000
  1600        0.0000             nan     0.5702   -0.0000
  1620        0.0000             nan     0.5702   -0.0000
  1640        0.0000             nan     0.5702   -0.0000
  1660        0.0000             nan     0.5702   -0.0000
  1680        0.0000             nan     0.5702   -0.0000
  1700        0.0000             nan     0.5702   -0.0000
  1720        0.0000             nan     0.5702   -0.0000
  1740        0.0000             nan     0.5702   -0.0000
  1760        0.0000             nan     0.5702   -0.0000
  1780        0.0000             nan     0.5702   -0.0000
  1800        0.0000             nan     0.5702    0.0000
  1820        0.0000             nan     0.5702   -0.0000
  1840        0.0000             nan     0.5702    0.0000
  1860        0.0000             nan     0.5702   -0.0000
  1880        0.0000             nan     0.5702   -0.0000
  1900        0.0000             nan     0.5702   -0.0000
  1920        0.0000             nan     0.5702   -0.0000
  1940        0.0000             nan     0.5702   -0.0000
  1960        0.0000             nan     0.5702   -0.0000
  1980        0.0000             nan     0.5702    0.0000
  2000        0.0000             nan     0.5702    0.0000
  2020        0.0000             nan     0.5702   -0.0000
  2040        0.0000             nan     0.5702   -0.0000
  2060        0.0000             nan     0.5702    0.0000
  2080        0.0000             nan     0.5702    0.0000
  2100        0.0000             nan     0.5702   -0.0000
  2120        0.0000             nan     0.5702   -0.0000
  2140        0.0000             nan     0.5702   -0.0000
  2160        0.0000             nan     0.5702   -0.0000
  2180        0.0000             nan     0.5702    0.0000
  2200        0.0000             nan     0.5702   -0.0000
  2220        0.0000             nan     0.5702   -0.0000
  2240        0.0000             nan     0.5702    0.0000
  2260        0.0000             nan     0.5702    0.0000
  2280        0.0000             nan     0.5702   -0.0000
  2300        0.0000             nan     0.5702   -0.0000
  2320        0.0000             nan     0.5702   -0.0000
  2340        0.0000             nan     0.5702    0.0000
  2360        0.0000             nan     0.5702    0.0000
  2380        0.0000             nan     0.5702   -0.0000
  2400        0.0000             nan     0.5702   -0.0000
  2420        0.0000             nan     0.5702   -0.0000
  2440        0.0000             nan     0.5702    0.0000
  2460        0.0000             nan     0.5702   -0.0000
  2480        0.0000             nan     0.5702    0.0000
  2500        0.0000             nan     0.5702   -0.0000
  2520        0.0000             nan     0.5702   -0.0000
  2540        0.0000             nan     0.5702   -0.0000
  2560        0.0000             nan     0.5702   -0.0000
  2580        0.0000             nan     0.5702   -0.0000
  2600        0.0000             nan     0.5702    0.0000
  2620        0.0000             nan     0.5702   -0.0000
  2640        0.0000             nan     0.5702   -0.0000
  2660        0.0000             nan     0.5702   -0.0000
  2680        0.0000             nan     0.5702   -0.0000
  2700        0.0000             nan     0.5702    0.0000
  2720        0.0000             nan     0.5702   -0.0000
  2740        0.0000             nan     0.5702   -0.0000
  2760        0.0000             nan     0.5702   -0.0000
  2780        0.0000             nan     0.5702   -0.0000
  2800        0.0000             nan     0.5702    0.0000
  2820        0.0000             nan     0.5702   -0.0000
  2840        0.0000             nan     0.5702    0.0000
  2860        0.0000             nan     0.5702   -0.0000
  2880        0.0000             nan     0.5702   -0.0000
  2900        0.0000             nan     0.5702   -0.0000
  2920        0.0000             nan     0.5702   -0.0000
  2940        0.0000             nan     0.5702   -0.0000
  2960        0.0000             nan     0.5702   -0.0000
  2980        0.0000             nan     0.5702   -0.0000
  3000        0.0000             nan     0.5702   -0.0000
  3020        0.0000             nan     0.5702   -0.0000
  3040        0.0000             nan     0.5702   -0.0000
  3060        0.0000             nan     0.5702    0.0000
  3080        0.0000             nan     0.5702   -0.0000
  3100        0.0000             nan     0.5702   -0.0000
  3120        0.0000             nan     0.5702    0.0000
  3140        0.0000             nan     0.5702   -0.0000
  3160        0.0000             nan     0.5702   -0.0000
  3180        0.0000             nan     0.5702   -0.0000
  3200        0.0000             nan     0.5702   -0.0000
  3220        0.0000             nan     0.5702   -0.0000
  3240        0.0000             nan     0.5702   -0.0000
  3260        0.0000             nan     0.5702   -0.0000
  3280        0.0000             nan     0.5702   -0.0000
  3300        0.0000             nan     0.5702   -0.0000
  3320        0.0000             nan     0.5702    0.0000
  3337        0.0000             nan     0.5702   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0091             nan     0.5774    0.0321
     2        0.0022             nan     0.5774    0.0065
     3        0.0008             nan     0.5774    0.0013
     4        0.0005             nan     0.5774    0.0003
     5        0.0004             nan     0.5774    0.0001
     6        0.0003             nan     0.5774    0.0000
     7        0.0003             nan     0.5774    0.0000
     8        0.0003             nan     0.5774   -0.0000
     9        0.0003             nan     0.5774   -0.0000
    10        0.0002             nan     0.5774   -0.0000
    20        0.0001             nan     0.5774   -0.0000
    40        0.0001             nan     0.5774   -0.0000
    60        0.0000             nan     0.5774   -0.0000
    80        0.0000             nan     0.5774   -0.0000
   100        0.0000             nan     0.5774   -0.0000
   120        0.0000             nan     0.5774   -0.0000
   140        0.0000             nan     0.5774   -0.0000
   160        0.0000             nan     0.5774   -0.0000
   180        0.0000             nan     0.5774   -0.0000
   200        0.0000             nan     0.5774   -0.0000
   220        0.0000             nan     0.5774    0.0000
   240        0.0000             nan     0.5774   -0.0000
   260        0.0000             nan     0.5774   -0.0000
   280        0.0000             nan     0.5774   -0.0000
   300        0.0000             nan     0.5774   -0.0000
   320        0.0000             nan     0.5774   -0.0000
   340        0.0000             nan     0.5774   -0.0000
   360        0.0000             nan     0.5774   -0.0000
   380        0.0000             nan     0.5774   -0.0000
   400        0.0000             nan     0.5774   -0.0000
   420        0.0000             nan     0.5774   -0.0000
   440        0.0000             nan     0.5774   -0.0000
   460        0.0000             nan     0.5774   -0.0000
   480        0.0000             nan     0.5774    0.0000
   500        0.0000             nan     0.5774   -0.0000
   520        0.0000             nan     0.5774   -0.0000
   540        0.0000             nan     0.5774   -0.0000
   560        0.0000             nan     0.5774   -0.0000
   580        0.0000             nan     0.5774    0.0000
   600        0.0000             nan     0.5774    0.0000
   620        0.0000             nan     0.5774   -0.0000
   640        0.0000             nan     0.5774   -0.0000
   660        0.0000             nan     0.5774   -0.0000
   680        0.0000             nan     0.5774   -0.0000
   700        0.0000             nan     0.5774   -0.0000
   720        0.0000             nan     0.5774   -0.0000
   740        0.0000             nan     0.5774    0.0000
   760        0.0000             nan     0.5774   -0.0000
   780        0.0000             nan     0.5774   -0.0000
   800        0.0000             nan     0.5774   -0.0000
   820        0.0000             nan     0.5774   -0.0000
   840        0.0000             nan     0.5774   -0.0000
   860        0.0000             nan     0.5774    0.0000
   880        0.0000             nan     0.5774   -0.0000
   900        0.0000             nan     0.5774   -0.0000
   920        0.0000             nan     0.5774   -0.0000
   940        0.0000             nan     0.5774   -0.0000
   960        0.0000             nan     0.5774   -0.0000
   980        0.0000             nan     0.5774   -0.0000
  1000        0.0000             nan     0.5774    0.0000
  1020        0.0000             nan     0.5774    0.0000
  1040        0.0000             nan     0.5774   -0.0000
  1060        0.0000             nan     0.5774   -0.0000
  1080        0.0000             nan     0.5774   -0.0000
  1100        0.0000             nan     0.5774   -0.0000
  1120        0.0000             nan     0.5774    0.0000
  1140        0.0000             nan     0.5774   -0.0000
  1160        0.0000             nan     0.5774   -0.0000
  1180        0.0000             nan     0.5774   -0.0000
  1200        0.0000             nan     0.5774   -0.0000
  1220        0.0000             nan     0.5774    0.0000
  1240        0.0000             nan     0.5774   -0.0000
  1260        0.0000             nan     0.5774   -0.0000
  1280        0.0000             nan     0.5774   -0.0000
  1300        0.0000             nan     0.5774   -0.0000
  1320        0.0000             nan     0.5774   -0.0000
  1340        0.0000             nan     0.5774    0.0000
  1360        0.0000             nan     0.5774    0.0000
  1380        0.0000             nan     0.5774    0.0000
  1400        0.0000             nan     0.5774   -0.0000
  1420        0.0000             nan     0.5774   -0.0000
  1440        0.0000             nan     0.5774   -0.0000
  1460        0.0000             nan     0.5774   -0.0000
  1480        0.0000             nan     0.5774    0.0000
  1500        0.0000             nan     0.5774    0.0000
  1520        0.0000             nan     0.5774    0.0000
  1540        0.0000             nan     0.5774    0.0000
  1560        0.0000             nan     0.5774    0.0000
  1580        0.0000             nan     0.5774   -0.0000
  1600        0.0000             nan     0.5774    0.0000
  1620        0.0000             nan     0.5774   -0.0000
  1640        0.0000             nan     0.5774    0.0000
  1660        0.0000             nan     0.5774   -0.0000
  1680        0.0000             nan     0.5774   -0.0000
  1700        0.0000             nan     0.5774    0.0000
  1720        0.0000             nan     0.5774   -0.0000
  1740        0.0000             nan     0.5774    0.0000
  1760        0.0000             nan     0.5774   -0.0000
  1764        0.0000             nan     0.5774    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0101             nan     0.5997    0.0313
     2        0.0037             nan     0.5997    0.0058
     3        0.0018             nan     0.5997    0.0018
     4        0.0013             nan     0.5997    0.0004
     5        0.0010             nan     0.5997    0.0002
     6        0.0008             nan     0.5997    0.0002
     7        0.0007             nan     0.5997    0.0001
     8        0.0006             nan     0.5997    0.0000
     9        0.0006             nan     0.5997   -0.0000
    10        0.0005             nan     0.5997    0.0000
    20        0.0003             nan     0.5997   -0.0000
    40        0.0002             nan     0.5997   -0.0000
    60        0.0001             nan     0.5997   -0.0000
    80        0.0001             nan     0.5997   -0.0000
   100        0.0001             nan     0.5997   -0.0000
   120        0.0001             nan     0.5997   -0.0000
   140        0.0001             nan     0.5997   -0.0000
   160        0.0001             nan     0.5997   -0.0000
   180        0.0000             nan     0.5997   -0.0000
   200        0.0000             nan     0.5997   -0.0000
   220        0.0000             nan     0.5997   -0.0000
   240        0.0000             nan     0.5997   -0.0000
   260        0.0000             nan     0.5997   -0.0000
   280        0.0000             nan     0.5997   -0.0000
   300        0.0000             nan     0.5997   -0.0000
   320        0.0000             nan     0.5997   -0.0000
   340        0.0000             nan     0.5997   -0.0000
   360        0.0000             nan     0.5997   -0.0000
   380        0.0000             nan     0.5997    0.0000
   400        0.0000             nan     0.5997   -0.0000
   420        0.0000             nan     0.5997   -0.0000
   440        0.0000             nan     0.5997   -0.0000
   460        0.0000             nan     0.5997   -0.0000
   480        0.0000             nan     0.5997   -0.0000
   500        0.0000             nan     0.5997   -0.0000
   520        0.0000             nan     0.5997   -0.0000
   540        0.0000             nan     0.5997   -0.0000
   560        0.0000             nan     0.5997   -0.0000
   580        0.0000             nan     0.5997   -0.0000
   600        0.0000             nan     0.5997   -0.0000
   620        0.0000             nan     0.5997   -0.0000
   640        0.0000             nan     0.5997   -0.0000
   660        0.0000             nan     0.5997   -0.0000
   680        0.0000             nan     0.5997   -0.0000
   700        0.0000             nan     0.5997   -0.0000
   720        0.0000             nan     0.5997   -0.0000
   740        0.0000             nan     0.5997   -0.0000
   760        0.0000             nan     0.5997   -0.0000
   780        0.0000             nan     0.5997   -0.0000
   800        0.0000             nan     0.5997   -0.0000
   820        0.0000             nan     0.5997   -0.0000
   840        0.0000             nan     0.5997   -0.0000
   860        0.0000             nan     0.5997   -0.0000
   880        0.0000             nan     0.5997    0.0000
   900        0.0000             nan     0.5997   -0.0000
   920        0.0000             nan     0.5997   -0.0000
   940        0.0000             nan     0.5997   -0.0000
   960        0.0000             nan     0.5997   -0.0000
   980        0.0000             nan     0.5997    0.0000
  1000        0.0000             nan     0.5997   -0.0000
  1020        0.0000             nan     0.5997   -0.0000
  1040        0.0000             nan     0.5997   -0.0000
  1060        0.0000             nan     0.5997   -0.0000
  1080        0.0000             nan     0.5997   -0.0000
  1100        0.0000             nan     0.5997    0.0000
  1120        0.0000             nan     0.5997   -0.0000
  1140        0.0000             nan     0.5997   -0.0000
  1160        0.0000             nan     0.5997   -0.0000
  1180        0.0000             nan     0.5997    0.0000
  1200        0.0000             nan     0.5997   -0.0000
  1220        0.0000             nan     0.5997   -0.0000
  1240        0.0000             nan     0.5997   -0.0000
  1260        0.0000             nan     0.5997   -0.0000
  1280        0.0000             nan     0.5997   -0.0000
  1300        0.0000             nan     0.5997   -0.0000
  1320        0.0000             nan     0.5997   -0.0000
  1340        0.0000             nan     0.5997   -0.0000
  1360        0.0000             nan     0.5997    0.0000
  1380        0.0000             nan     0.5997   -0.0000
  1400        0.0000             nan     0.5997   -0.0000
  1420        0.0000             nan     0.5997   -0.0000
  1440        0.0000             nan     0.5997   -0.0000
  1460        0.0000             nan     0.5997   -0.0000
  1480        0.0000             nan     0.5997   -0.0000
  1500        0.0000             nan     0.5997    0.0000
  1520        0.0000             nan     0.5997    0.0000
  1540        0.0000             nan     0.5997    0.0000
  1560        0.0000             nan     0.5997    0.0000
  1580        0.0000             nan     0.5997    0.0000
  1600        0.0000             nan     0.5997   -0.0000
  1620        0.0000             nan     0.5997   -0.0000
  1640        0.0000             nan     0.5997   -0.0000
  1660        0.0000             nan     0.5997   -0.0000
  1680        0.0000             nan     0.5997   -0.0000
  1700        0.0000             nan     0.5997    0.0000
  1720        0.0000             nan     0.5997   -0.0000
  1740        0.0000             nan     0.5997    0.0000
  1760        0.0000             nan     0.5997   -0.0000
  1780        0.0000             nan     0.5997    0.0000
  1800        0.0000             nan     0.5997   -0.0000
  1820        0.0000             nan     0.5997    0.0000
  1840        0.0000             nan     0.5997   -0.0000
  1860        0.0000             nan     0.5997   -0.0000
  1880        0.0000             nan     0.5997    0.0000
  1900        0.0000             nan     0.5997   -0.0000
  1920        0.0000             nan     0.5997    0.0000
  1940        0.0000             nan     0.5997   -0.0000
  1960        0.0000             nan     0.5997   -0.0000
  1980        0.0000             nan     0.5997    0.0000
  2000        0.0000             nan     0.5997    0.0000
  2020        0.0000             nan     0.5997   -0.0000
  2040        0.0000             nan     0.5997    0.0000
  2060        0.0000             nan     0.5997   -0.0000
  2080        0.0000             nan     0.5997   -0.0000
  2100        0.0000             nan     0.5997   -0.0000
  2120        0.0000             nan     0.5997   -0.0000
  2140        0.0000             nan     0.5997   -0.0000
  2160        0.0000             nan     0.5997   -0.0000
  2180        0.0000             nan     0.5997   -0.0000
  2200        0.0000             nan     0.5997   -0.0000
  2220        0.0000             nan     0.5997   -0.0000
  2240        0.0000             nan     0.5997   -0.0000
  2260        0.0000             nan     0.5997   -0.0000
  2280        0.0000             nan     0.5997   -0.0000
  2300        0.0000             nan     0.5997   -0.0000
  2320        0.0000             nan     0.5997   -0.0000
  2340        0.0000             nan     0.5997   -0.0000
  2360        0.0000             nan     0.5997    0.0000
  2380        0.0000             nan     0.5997    0.0000
  2400        0.0000             nan     0.5997   -0.0000
  2420        0.0000             nan     0.5997    0.0000
  2440        0.0000             nan     0.5997   -0.0000
  2460        0.0000             nan     0.5997    0.0000
  2480        0.0000             nan     0.5997   -0.0000
  2500        0.0000             nan     0.5997   -0.0000
  2520        0.0000             nan     0.5997   -0.0000
  2540        0.0000             nan     0.5997   -0.0000
  2560        0.0000             nan     0.5997   -0.0000
  2580        0.0000             nan     0.5997   -0.0000
  2600        0.0000             nan     0.5997    0.0000
  2620        0.0000             nan     0.5997   -0.0000
  2640        0.0000             nan     0.5997   -0.0000
  2660        0.0000             nan     0.5997   -0.0000
  2680        0.0000             nan     0.5997   -0.0000
  2700        0.0000             nan     0.5997   -0.0000
  2720        0.0000             nan     0.5997   -0.0000
  2740        0.0000             nan     0.5997    0.0000
  2760        0.0000             nan     0.5997   -0.0000
  2780        0.0000             nan     0.5997   -0.0000
  2800        0.0000             nan     0.5997   -0.0000
  2820        0.0000             nan     0.5997   -0.0000
  2840        0.0000             nan     0.5997   -0.0000
  2860        0.0000             nan     0.5997   -0.0000
  2880        0.0000             nan     0.5997   -0.0000
  2900        0.0000             nan     0.5997   -0.0000
  2920        0.0000             nan     0.5997    0.0000
  2940        0.0000             nan     0.5997    0.0000
  2960        0.0000             nan     0.5997    0.0000
  2980        0.0000             nan     0.5997   -0.0000
  3000        0.0000             nan     0.5997    0.0000
  3020        0.0000             nan     0.5997    0.0000
  3040        0.0000             nan     0.5997    0.0000
  3060        0.0000             nan     0.5997   -0.0000
  3080        0.0000             nan     0.5997   -0.0000
  3100        0.0000             nan     0.5997    0.0000
  3120        0.0000             nan     0.5997    0.0000
  3140        0.0000             nan     0.5997   -0.0000
  3160        0.0000             nan     0.5997   -0.0000
  3180        0.0000             nan     0.5997    0.0000
  3200        0.0000             nan     0.5997    0.0000
  3220        0.0000             nan     0.5997    0.0000
  3240        0.0000             nan     0.5997   -0.0000
  3260        0.0000             nan     0.5997   -0.0000
  3269        0.0000             nan     0.5997    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0356             nan     0.0482    0.0039
     2        0.0324             nan     0.0482    0.0031
     3        0.0295             nan     0.0482    0.0027
     4        0.0269             nan     0.0482    0.0024
     5        0.0245             nan     0.0482    0.0025
     6        0.0224             nan     0.0482    0.0022
     7        0.0204             nan     0.0482    0.0019
     8        0.0186             nan     0.0482    0.0018
     9        0.0169             nan     0.0482    0.0016
    10        0.0154             nan     0.0482    0.0015
    20        0.0063             nan     0.0482    0.0006
    40        0.0012             nan     0.0482    0.0001
    60        0.0004             nan     0.0482    0.0000
    80        0.0002             nan     0.0482    0.0000
   100        0.0001             nan     0.0482    0.0000
   120        0.0001             nan     0.0482   -0.0000
   140        0.0001             nan     0.0482   -0.0000
   160        0.0001             nan     0.0482   -0.0000
   180        0.0001             nan     0.0482    0.0000
   200        0.0001             nan     0.0482    0.0000
   220        0.0001             nan     0.0482   -0.0000
   240        0.0001             nan     0.0482   -0.0000
   260        0.0001             nan     0.0482   -0.0000
   280        0.0001             nan     0.0482   -0.0000
   300        0.0001             nan     0.0482   -0.0000
   320        0.0001             nan     0.0482   -0.0000
   340        0.0001             nan     0.0482   -0.0000
   360        0.0000             nan     0.0482   -0.0000
   380        0.0000             nan     0.0482   -0.0000
   400        0.0000             nan     0.0482   -0.0000
   420        0.0000             nan     0.0482   -0.0000
   440        0.0000             nan     0.0482   -0.0000
   460        0.0000             nan     0.0482   -0.0000
   480        0.0000             nan     0.0482   -0.0000
   500        0.0000             nan     0.0482   -0.0000
   520        0.0000             nan     0.0482   -0.0000
   540        0.0000             nan     0.0482   -0.0000
   560        0.0000             nan     0.0482   -0.0000
   580        0.0000             nan     0.0482   -0.0000
   600        0.0000             nan     0.0482   -0.0000
   620        0.0000             nan     0.0482   -0.0000
   640        0.0000             nan     0.0482   -0.0000
   660        0.0000             nan     0.0482   -0.0000
   680        0.0000             nan     0.0482   -0.0000
   700        0.0000             nan     0.0482   -0.0000
   720        0.0000             nan     0.0482   -0.0000
   740        0.0000             nan     0.0482   -0.0000
   760        0.0000             nan     0.0482   -0.0000
   780        0.0000             nan     0.0482   -0.0000
   800        0.0000             nan     0.0482   -0.0000
   820        0.0000             nan     0.0482   -0.0000
   840        0.0000             nan     0.0482   -0.0000
   860        0.0000             nan     0.0482   -0.0000
   880        0.0000             nan     0.0482   -0.0000
   900        0.0000             nan     0.0482   -0.0000
   920        0.0000             nan     0.0482   -0.0000
   940        0.0000             nan     0.0482   -0.0000
   960        0.0000             nan     0.0482   -0.0000
   980        0.0000             nan     0.0482   -0.0000
  1000        0.0000             nan     0.0482   -0.0000
  1020        0.0000             nan     0.0482   -0.0000
  1040        0.0000             nan     0.0482   -0.0000
  1060        0.0000             nan     0.0482   -0.0000
  1080        0.0000             nan     0.0482   -0.0000
  1100        0.0000             nan     0.0482   -0.0000
  1120        0.0000             nan     0.0482   -0.0000
  1140        0.0000             nan     0.0482   -0.0000
  1160        0.0000             nan     0.0482   -0.0000
  1180        0.0000             nan     0.0482   -0.0000
  1200        0.0000             nan     0.0482   -0.0000
  1220        0.0000             nan     0.0482   -0.0000
  1240        0.0000             nan     0.0482   -0.0000
  1260        0.0000             nan     0.0482   -0.0000
  1280        0.0000             nan     0.0482   -0.0000
  1300        0.0000             nan     0.0482   -0.0000
  1320        0.0000             nan     0.0482   -0.0000
  1340        0.0000             nan     0.0482   -0.0000
  1360        0.0000             nan     0.0482   -0.0000
  1380        0.0000             nan     0.0482   -0.0000
  1400        0.0000             nan     0.0482   -0.0000
  1420        0.0000             nan     0.0482   -0.0000
  1440        0.0000             nan     0.0482   -0.0000
  1460        0.0000             nan     0.0482   -0.0000
  1480        0.0000             nan     0.0482   -0.0000
  1500        0.0000             nan     0.0482   -0.0000
  1520        0.0000             nan     0.0482   -0.0000
  1540        0.0000             nan     0.0482   -0.0000
  1560        0.0000             nan     0.0482   -0.0000
  1580        0.0000             nan     0.0482   -0.0000
  1600        0.0000             nan     0.0482   -0.0000
  1620        0.0000             nan     0.0482   -0.0000
  1640        0.0000             nan     0.0482   -0.0000
  1660        0.0000             nan     0.0482   -0.0000
  1680        0.0000             nan     0.0482   -0.0000
  1700        0.0000             nan     0.0482   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0353             nan     0.0529    0.0038
     2        0.0318             nan     0.0529    0.0032
     3        0.0287             nan     0.0529    0.0030
     4        0.0258             nan     0.0529    0.0029
     5        0.0233             nan     0.0529    0.0025
     6        0.0210             nan     0.0529    0.0025
     7        0.0190             nan     0.0529    0.0021
     8        0.0172             nan     0.0529    0.0018
     9        0.0155             nan     0.0529    0.0016
    10        0.0140             nan     0.0529    0.0015
    20        0.0051             nan     0.0529    0.0005
    40        0.0008             nan     0.0529    0.0001
    60        0.0002             nan     0.0529    0.0000
    80        0.0001             nan     0.0529    0.0000
   100        0.0000             nan     0.0529   -0.0000
   120        0.0000             nan     0.0529   -0.0000
   140        0.0000             nan     0.0529   -0.0000
   160        0.0000             nan     0.0529   -0.0000
   180        0.0000             nan     0.0529   -0.0000
   200        0.0000             nan     0.0529   -0.0000
   220        0.0000             nan     0.0529   -0.0000
   240        0.0000             nan     0.0529   -0.0000
   260        0.0000             nan     0.0529   -0.0000
   280        0.0000             nan     0.0529   -0.0000
   300        0.0000             nan     0.0529   -0.0000
   320        0.0000             nan     0.0529   -0.0000
   340        0.0000             nan     0.0529   -0.0000
   360        0.0000             nan     0.0529   -0.0000
   380        0.0000             nan     0.0529   -0.0000
   400        0.0000             nan     0.0529   -0.0000
   420        0.0000             nan     0.0529   -0.0000
   440        0.0000             nan     0.0529   -0.0000
   460        0.0000             nan     0.0529   -0.0000
   480        0.0000             nan     0.0529   -0.0000
   500        0.0000             nan     0.0529   -0.0000
   520        0.0000             nan     0.0529   -0.0000
   540        0.0000             nan     0.0529   -0.0000
   560        0.0000             nan     0.0529   -0.0000
   580        0.0000             nan     0.0529   -0.0000
   600        0.0000             nan     0.0529   -0.0000
   620        0.0000             nan     0.0529   -0.0000
   640        0.0000             nan     0.0529   -0.0000
   660        0.0000             nan     0.0529   -0.0000
   680        0.0000             nan     0.0529   -0.0000
   700        0.0000             nan     0.0529   -0.0000
   720        0.0000             nan     0.0529   -0.0000
   740        0.0000             nan     0.0529   -0.0000
   760        0.0000             nan     0.0529   -0.0000
   780        0.0000             nan     0.0529   -0.0000
   800        0.0000             nan     0.0529   -0.0000
   820        0.0000             nan     0.0529   -0.0000
   840        0.0000             nan     0.0529   -0.0000
   860        0.0000             nan     0.0529   -0.0000
   880        0.0000             nan     0.0529   -0.0000
   900        0.0000             nan     0.0529   -0.0000
   920        0.0000             nan     0.0529   -0.0000
   940        0.0000             nan     0.0529   -0.0000
   960        0.0000             nan     0.0529   -0.0000
   980        0.0000             nan     0.0529   -0.0000
  1000        0.0000             nan     0.0529   -0.0000
  1020        0.0000             nan     0.0529   -0.0000
  1040        0.0000             nan     0.0529   -0.0000
  1060        0.0000             nan     0.0529   -0.0000
  1080        0.0000             nan     0.0529   -0.0000
  1100        0.0000             nan     0.0529   -0.0000
  1120        0.0000             nan     0.0529   -0.0000
  1140        0.0000             nan     0.0529   -0.0000
  1160        0.0000             nan     0.0529   -0.0000
  1180        0.0000             nan     0.0529   -0.0000
  1200        0.0000             nan     0.0529   -0.0000
  1220        0.0000             nan     0.0529   -0.0000
  1240        0.0000             nan     0.0529   -0.0000
  1260        0.0000             nan     0.0529   -0.0000
  1280        0.0000             nan     0.0529   -0.0000
  1300        0.0000             nan     0.0529   -0.0000
  1320        0.0000             nan     0.0529   -0.0000
  1340        0.0000             nan     0.0529   -0.0000
  1360        0.0000             nan     0.0529   -0.0000
  1380        0.0000             nan     0.0529   -0.0000
  1400        0.0000             nan     0.0529   -0.0000
  1420        0.0000             nan     0.0529   -0.0000
  1440        0.0000             nan     0.0529   -0.0000
  1460        0.0000             nan     0.0529   -0.0000
  1480        0.0000             nan     0.0529   -0.0000
  1500        0.0000             nan     0.0529   -0.0000
  1520        0.0000             nan     0.0529   -0.0000
  1540        0.0000             nan     0.0529   -0.0000
  1560        0.0000             nan     0.0529   -0.0000
  1580        0.0000             nan     0.0529   -0.0000
  1600        0.0000             nan     0.0529   -0.0000
  1620        0.0000             nan     0.0529   -0.0000
  1640        0.0000             nan     0.0529   -0.0000
  1660        0.0000             nan     0.0529   -0.0000
  1680        0.0000             nan     0.0529   -0.0000
  1700        0.0000             nan     0.0529   -0.0000
  1720        0.0000             nan     0.0529   -0.0000
  1740        0.0000             nan     0.0529   -0.0000
  1760        0.0000             nan     0.0529   -0.0000
  1780        0.0000             nan     0.0529   -0.0000
  1800        0.0000             nan     0.0529   -0.0000
  1820        0.0000             nan     0.0529   -0.0000
  1840        0.0000             nan     0.0529   -0.0000
  1860        0.0000             nan     0.0529   -0.0000
  1880        0.0000             nan     0.0529   -0.0000
  1900        0.0000             nan     0.0529   -0.0000
  1920        0.0000             nan     0.0529   -0.0000
  1940        0.0000             nan     0.0529   -0.0000
  1960        0.0000             nan     0.0529   -0.0000
  1980        0.0000             nan     0.0529   -0.0000
  2000        0.0000             nan     0.0529   -0.0000
  2020        0.0000             nan     0.0529   -0.0000
  2040        0.0000             nan     0.0529   -0.0000
  2060        0.0000             nan     0.0529   -0.0000
  2080        0.0000             nan     0.0529   -0.0000
  2100        0.0000             nan     0.0529   -0.0000
  2120        0.0000             nan     0.0529   -0.0000
  2140        0.0000             nan     0.0529   -0.0000
  2160        0.0000             nan     0.0529   -0.0000
  2180        0.0000             nan     0.0529   -0.0000
  2200        0.0000             nan     0.0529   -0.0000
  2220        0.0000             nan     0.0529   -0.0000
  2240        0.0000             nan     0.0529   -0.0000
  2260        0.0000             nan     0.0529   -0.0000
  2280        0.0000             nan     0.0529   -0.0000
  2300        0.0000             nan     0.0529   -0.0000
  2320        0.0000             nan     0.0529   -0.0000
  2340        0.0000             nan     0.0529   -0.0000
  2360        0.0000             nan     0.0529   -0.0000
  2380        0.0000             nan     0.0529   -0.0000
  2400        0.0000             nan     0.0529   -0.0000
  2420        0.0000             nan     0.0529   -0.0000
  2440        0.0000             nan     0.0529   -0.0000
  2460        0.0000             nan     0.0529   -0.0000
  2480        0.0000             nan     0.0529   -0.0000
  2500        0.0000             nan     0.0529   -0.0000
  2520        0.0000             nan     0.0529   -0.0000
  2540        0.0000             nan     0.0529   -0.0000
  2560        0.0000             nan     0.0529   -0.0000
  2580        0.0000             nan     0.0529   -0.0000
  2600        0.0000             nan     0.0529   -0.0000
  2620        0.0000             nan     0.0529   -0.0000
  2640        0.0000             nan     0.0529   -0.0000
  2660        0.0000             nan     0.0529   -0.0000
  2680        0.0000             nan     0.0529   -0.0000
  2700        0.0000             nan     0.0529   -0.0000
  2720        0.0000             nan     0.0529   -0.0000
  2740        0.0000             nan     0.0529   -0.0000
  2760        0.0000             nan     0.0529   -0.0000
  2780        0.0000             nan     0.0529   -0.0000
  2800        0.0000             nan     0.0529   -0.0000
  2820        0.0000             nan     0.0529   -0.0000
  2840        0.0000             nan     0.0529   -0.0000
  2860        0.0000             nan     0.0529   -0.0000
  2880        0.0000             nan     0.0529   -0.0000
  2900        0.0000             nan     0.0529   -0.0000
  2920        0.0000             nan     0.0529   -0.0000
  2940        0.0000             nan     0.0529   -0.0000
  2960        0.0000             nan     0.0529   -0.0000
  2980        0.0000             nan     0.0529   -0.0000
  3000        0.0000             nan     0.0529   -0.0000
  3020        0.0000             nan     0.0529   -0.0000
  3040        0.0000             nan     0.0529   -0.0000
  3060        0.0000             nan     0.0529   -0.0000
  3080        0.0000             nan     0.0529   -0.0000
  3100        0.0000             nan     0.0529   -0.0000
  3120        0.0000             nan     0.0529   -0.0000
  3140        0.0000             nan     0.0529   -0.0000
  3160        0.0000             nan     0.0529   -0.0000
  3180        0.0000             nan     0.0529   -0.0000
  3200        0.0000             nan     0.0529   -0.0000
  3220        0.0000             nan     0.0529   -0.0000
  3240        0.0000             nan     0.0529   -0.0000
  3260        0.0000             nan     0.0529   -0.0000
  3280        0.0000             nan     0.0529   -0.0000
  3300        0.0000             nan     0.0529   -0.0000
  3320        0.0000             nan     0.0529   -0.0000
  3340        0.0000             nan     0.0529   -0.0000
  3360        0.0000             nan     0.0529   -0.0000
  3380        0.0000             nan     0.0529   -0.0000
  3400        0.0000             nan     0.0529   -0.0000
  3420        0.0000             nan     0.0529   -0.0000
  3440        0.0000             nan     0.0529   -0.0000
  3460        0.0000             nan     0.0529   -0.0000
  3480        0.0000             nan     0.0529   -0.0000
  3500        0.0000             nan     0.0529   -0.0000
  3520        0.0000             nan     0.0529   -0.0000
  3540        0.0000             nan     0.0529   -0.0000
  3560        0.0000             nan     0.0529   -0.0000
  3580        0.0000             nan     0.0529   -0.0000
  3600        0.0000             nan     0.0529   -0.0000
  3620        0.0000             nan     0.0529   -0.0000
  3640        0.0000             nan     0.0529   -0.0000
  3660        0.0000             nan     0.0529   -0.0000
  3680        0.0000             nan     0.0529   -0.0000
  3700        0.0000             nan     0.0529   -0.0000
  3720        0.0000             nan     0.0529   -0.0000
  3740        0.0000             nan     0.0529   -0.0000
  3760        0.0000             nan     0.0529   -0.0000
  3780        0.0000             nan     0.0529   -0.0000
  3800        0.0000             nan     0.0529   -0.0000
  3820        0.0000             nan     0.0529   -0.0000
  3840        0.0000             nan     0.0529   -0.0000
  3860        0.0000             nan     0.0529   -0.0000
  3880        0.0000             nan     0.0529   -0.0000
  3900        0.0000             nan     0.0529   -0.0000
  3920        0.0000             nan     0.0529   -0.0000
  3940        0.0000             nan     0.0529   -0.0000
  3960        0.0000             nan     0.0529   -0.0000
  3980        0.0000             nan     0.0529   -0.0000
  3983        0.0000             nan     0.0529   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0352             nan     0.0560    0.0041
     2        0.0316             nan     0.0560    0.0035
     3        0.0283             nan     0.0560    0.0032
     4        0.0254             nan     0.0560    0.0030
     5        0.0229             nan     0.0560    0.0025
     6        0.0205             nan     0.0560    0.0022
     7        0.0185             nan     0.0560    0.0022
     8        0.0166             nan     0.0560    0.0018
     9        0.0150             nan     0.0560    0.0017
    10        0.0135             nan     0.0560    0.0014
    20        0.0049             nan     0.0560    0.0005
    40        0.0009             nan     0.0560    0.0001
    60        0.0004             nan     0.0560    0.0000
    80        0.0003             nan     0.0560    0.0000
   100        0.0002             nan     0.0560    0.0000
   120        0.0002             nan     0.0560    0.0000
   140        0.0002             nan     0.0560   -0.0000
   160        0.0001             nan     0.0560   -0.0000
   180        0.0001             nan     0.0560   -0.0000
   200        0.0001             nan     0.0560   -0.0000
   220        0.0001             nan     0.0560    0.0000
   240        0.0001             nan     0.0560   -0.0000
   260        0.0001             nan     0.0560   -0.0000
   280        0.0001             nan     0.0560   -0.0000
   300        0.0001             nan     0.0560   -0.0000
   320        0.0001             nan     0.0560   -0.0000
   340        0.0001             nan     0.0560   -0.0000
   360        0.0001             nan     0.0560   -0.0000
   380        0.0001             nan     0.0560   -0.0000
   400        0.0001             nan     0.0560   -0.0000
   420        0.0001             nan     0.0560   -0.0000
   440        0.0001             nan     0.0560   -0.0000
   460        0.0001             nan     0.0560   -0.0000
   480        0.0001             nan     0.0560   -0.0000
   500        0.0001             nan     0.0560   -0.0000
   520        0.0001             nan     0.0560   -0.0000
   540        0.0001             nan     0.0560   -0.0000
   560        0.0001             nan     0.0560   -0.0000
   580        0.0001             nan     0.0560   -0.0000
   600        0.0001             nan     0.0560   -0.0000
   620        0.0001             nan     0.0560   -0.0000
   640        0.0001             nan     0.0560   -0.0000
   660        0.0000             nan     0.0560   -0.0000
   680        0.0000             nan     0.0560   -0.0000
   700        0.0000             nan     0.0560   -0.0000
   720        0.0000             nan     0.0560   -0.0000
   740        0.0000             nan     0.0560   -0.0000
   760        0.0000             nan     0.0560   -0.0000
   780        0.0000             nan     0.0560   -0.0000
   800        0.0000             nan     0.0560   -0.0000
   820        0.0000             nan     0.0560   -0.0000
   840        0.0000             nan     0.0560   -0.0000
   860        0.0000             nan     0.0560   -0.0000
   880        0.0000             nan     0.0560   -0.0000
   900        0.0000             nan     0.0560   -0.0000
   920        0.0000             nan     0.0560   -0.0000
   940        0.0000             nan     0.0560   -0.0000
   960        0.0000             nan     0.0560   -0.0000
   980        0.0000             nan     0.0560   -0.0000
  1000        0.0000             nan     0.0560   -0.0000
  1020        0.0000             nan     0.0560   -0.0000
  1040        0.0000             nan     0.0560   -0.0000
  1060        0.0000             nan     0.0560   -0.0000
  1080        0.0000             nan     0.0560   -0.0000
  1100        0.0000             nan     0.0560   -0.0000
  1120        0.0000             nan     0.0560   -0.0000
  1140        0.0000             nan     0.0560   -0.0000
  1160        0.0000             nan     0.0560   -0.0000
  1180        0.0000             nan     0.0560   -0.0000
  1200        0.0000             nan     0.0560   -0.0000
  1220        0.0000             nan     0.0560   -0.0000
  1240        0.0000             nan     0.0560   -0.0000
  1260        0.0000             nan     0.0560   -0.0000
  1280        0.0000             nan     0.0560   -0.0000
  1300        0.0000             nan     0.0560   -0.0000
  1320        0.0000             nan     0.0560   -0.0000
  1340        0.0000             nan     0.0560   -0.0000
  1360        0.0000             nan     0.0560   -0.0000
  1380        0.0000             nan     0.0560   -0.0000
  1400        0.0000             nan     0.0560   -0.0000
  1420        0.0000             nan     0.0560   -0.0000
  1440        0.0000             nan     0.0560   -0.0000
  1460        0.0000             nan     0.0560   -0.0000
  1480        0.0000             nan     0.0560   -0.0000
  1500        0.0000             nan     0.0560   -0.0000
  1520        0.0000             nan     0.0560   -0.0000
  1540        0.0000             nan     0.0560   -0.0000
  1560        0.0000             nan     0.0560   -0.0000
  1580        0.0000             nan     0.0560   -0.0000
  1600        0.0000             nan     0.0560   -0.0000
  1620        0.0000             nan     0.0560   -0.0000
  1640        0.0000             nan     0.0560   -0.0000
  1660        0.0000             nan     0.0560   -0.0000
  1680        0.0000             nan     0.0560   -0.0000
  1700        0.0000             nan     0.0560   -0.0000
  1720        0.0000             nan     0.0560   -0.0000
  1740        0.0000             nan     0.0560   -0.0000
  1760        0.0000             nan     0.0560   -0.0000
  1780        0.0000             nan     0.0560   -0.0000
  1800        0.0000             nan     0.0560   -0.0000
  1820        0.0000             nan     0.0560   -0.0000
  1840        0.0000             nan     0.0560   -0.0000
  1860        0.0000             nan     0.0560   -0.0000
  1880        0.0000             nan     0.0560   -0.0000
  1900        0.0000             nan     0.0560   -0.0000
  1920        0.0000             nan     0.0560   -0.0000
  1940        0.0000             nan     0.0560   -0.0000
  1960        0.0000             nan     0.0560   -0.0000
  1980        0.0000             nan     0.0560   -0.0000
  2000        0.0000             nan     0.0560   -0.0000
  2020        0.0000             nan     0.0560   -0.0000
  2040        0.0000             nan     0.0560   -0.0000
  2060        0.0000             nan     0.0560   -0.0000
  2080        0.0000             nan     0.0560   -0.0000
  2100        0.0000             nan     0.0560   -0.0000
  2120        0.0000             nan     0.0560   -0.0000
  2140        0.0000             nan     0.0560   -0.0000
  2160        0.0000             nan     0.0560   -0.0000
  2180        0.0000             nan     0.0560   -0.0000
  2200        0.0000             nan     0.0560   -0.0000
  2220        0.0000             nan     0.0560   -0.0000
  2240        0.0000             nan     0.0560   -0.0000
  2260        0.0000             nan     0.0560   -0.0000
  2280        0.0000             nan     0.0560   -0.0000
  2300        0.0000             nan     0.0560   -0.0000
  2320        0.0000             nan     0.0560   -0.0000
  2340        0.0000             nan     0.0560   -0.0000
  2360        0.0000             nan     0.0560   -0.0000
  2380        0.0000             nan     0.0560   -0.0000
  2400        0.0000             nan     0.0560   -0.0000
  2420        0.0000             nan     0.0560   -0.0000
  2440        0.0000             nan     0.0560   -0.0000
  2460        0.0000             nan     0.0560   -0.0000
  2480        0.0000             nan     0.0560   -0.0000
  2500        0.0000             nan     0.0560   -0.0000
  2520        0.0000             nan     0.0560   -0.0000
  2540        0.0000             nan     0.0560   -0.0000
  2560        0.0000             nan     0.0560   -0.0000
  2580        0.0000             nan     0.0560   -0.0000
  2600        0.0000             nan     0.0560   -0.0000
  2620        0.0000             nan     0.0560   -0.0000
  2640        0.0000             nan     0.0560   -0.0000
  2660        0.0000             nan     0.0560   -0.0000
  2680        0.0000             nan     0.0560   -0.0000
  2700        0.0000             nan     0.0560   -0.0000
  2720        0.0000             nan     0.0560   -0.0000
  2740        0.0000             nan     0.0560   -0.0000
  2760        0.0000             nan     0.0560   -0.0000
  2780        0.0000             nan     0.0560   -0.0000
  2800        0.0000             nan     0.0560   -0.0000
  2820        0.0000             nan     0.0560   -0.0000
  2840        0.0000             nan     0.0560   -0.0000
  2860        0.0000             nan     0.0560   -0.0000
  2880        0.0000             nan     0.0560   -0.0000
  2900        0.0000             nan     0.0560   -0.0000
  2920        0.0000             nan     0.0560   -0.0000
  2940        0.0000             nan     0.0560   -0.0000
  2960        0.0000             nan     0.0560   -0.0000
  2980        0.0000             nan     0.0560   -0.0000
  3000        0.0000             nan     0.0560   -0.0000
  3020        0.0000             nan     0.0560   -0.0000
  3040        0.0000             nan     0.0560   -0.0000
  3060        0.0000             nan     0.0560   -0.0000
  3080        0.0000             nan     0.0560   -0.0000
  3100        0.0000             nan     0.0560   -0.0000
  3120        0.0000             nan     0.0560   -0.0000
  3140        0.0000             nan     0.0560   -0.0000
  3160        0.0000             nan     0.0560   -0.0000
  3180        0.0000             nan     0.0560   -0.0000
  3200        0.0000             nan     0.0560   -0.0000
  3220        0.0000             nan     0.0560   -0.0000
  3240        0.0000             nan     0.0560   -0.0000
  3260        0.0000             nan     0.0560   -0.0000
  3280        0.0000             nan     0.0560   -0.0000
  3300        0.0000             nan     0.0560   -0.0000
  3320        0.0000             nan     0.0560   -0.0000
  3340        0.0000             nan     0.0560   -0.0000
  3360        0.0000             nan     0.0560   -0.0000
  3380        0.0000             nan     0.0560   -0.0000
  3400        0.0000             nan     0.0560   -0.0000
  3420        0.0000             nan     0.0560   -0.0000
  3440        0.0000             nan     0.0560   -0.0000
  3460        0.0000             nan     0.0560   -0.0000
  3480        0.0000             nan     0.0560   -0.0000
  3500        0.0000             nan     0.0560   -0.0000
  3520        0.0000             nan     0.0560   -0.0000
  3540        0.0000             nan     0.0560   -0.0000
  3560        0.0000             nan     0.0560   -0.0000
  3580        0.0000             nan     0.0560   -0.0000
  3600        0.0000             nan     0.0560   -0.0000
  3620        0.0000             nan     0.0560   -0.0000
  3640        0.0000             nan     0.0560   -0.0000
  3660        0.0000             nan     0.0560   -0.0000
  3680        0.0000             nan     0.0560   -0.0000
  3700        0.0000             nan     0.0560   -0.0000
  3720        0.0000             nan     0.0560   -0.0000
  3740        0.0000             nan     0.0560   -0.0000
  3760        0.0000             nan     0.0560   -0.0000
  3780        0.0000             nan     0.0560   -0.0000
  3800        0.0000             nan     0.0560   -0.0000
  3820        0.0000             nan     0.0560   -0.0000
  3840        0.0000             nan     0.0560   -0.0000
  3860        0.0000             nan     0.0560   -0.0000
  3880        0.0000             nan     0.0560   -0.0000
  3900        0.0000             nan     0.0560   -0.0000
  3905        0.0000             nan     0.0560   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0342             nan     0.0700    0.0046
     2        0.0298             nan     0.0700    0.0038
     3        0.0261             nan     0.0700    0.0038
     4        0.0229             nan     0.0700    0.0034
     5        0.0201             nan     0.0700    0.0027
     6        0.0176             nan     0.0700    0.0026
     7        0.0155             nan     0.0700    0.0020
     8        0.0136             nan     0.0700    0.0018
     9        0.0120             nan     0.0700    0.0016
    10        0.0106             nan     0.0700    0.0014
    20        0.0032             nan     0.0700    0.0004
    40        0.0005             nan     0.0700    0.0000
    60        0.0003             nan     0.0700    0.0000
    80        0.0002             nan     0.0700    0.0000
   100        0.0002             nan     0.0700    0.0000
   120        0.0002             nan     0.0700    0.0000
   140        0.0001             nan     0.0700   -0.0000
   160        0.0001             nan     0.0700   -0.0000
   180        0.0001             nan     0.0700   -0.0000
   200        0.0001             nan     0.0700   -0.0000
   220        0.0001             nan     0.0700   -0.0000
   240        0.0001             nan     0.0700   -0.0000
   260        0.0001             nan     0.0700   -0.0000
   280        0.0001             nan     0.0700   -0.0000
   300        0.0001             nan     0.0700    0.0000
   320        0.0001             nan     0.0700   -0.0000
   340        0.0001             nan     0.0700   -0.0000
   360        0.0001             nan     0.0700   -0.0000
   380        0.0001             nan     0.0700   -0.0000
   400        0.0001             nan     0.0700   -0.0000
   420        0.0001             nan     0.0700   -0.0000
   440        0.0001             nan     0.0700   -0.0000
   460        0.0001             nan     0.0700   -0.0000
   480        0.0001             nan     0.0700   -0.0000
   500        0.0001             nan     0.0700   -0.0000
   520        0.0001             nan     0.0700   -0.0000
   540        0.0001             nan     0.0700   -0.0000
   560        0.0001             nan     0.0700   -0.0000
   580        0.0001             nan     0.0700   -0.0000
   600        0.0001             nan     0.0700   -0.0000
   620        0.0000             nan     0.0700   -0.0000
   640        0.0000             nan     0.0700   -0.0000
   660        0.0000             nan     0.0700   -0.0000
   680        0.0000             nan     0.0700   -0.0000
   700        0.0000             nan     0.0700   -0.0000
   720        0.0000             nan     0.0700   -0.0000
   740        0.0000             nan     0.0700   -0.0000
   760        0.0000             nan     0.0700   -0.0000
   780        0.0000             nan     0.0700   -0.0000
   800        0.0000             nan     0.0700   -0.0000
   820        0.0000             nan     0.0700   -0.0000
   840        0.0000             nan     0.0700   -0.0000
   860        0.0000             nan     0.0700   -0.0000
   880        0.0000             nan     0.0700   -0.0000
   900        0.0000             nan     0.0700   -0.0000
   920        0.0000             nan     0.0700   -0.0000
   940        0.0000             nan     0.0700   -0.0000
   960        0.0000             nan     0.0700   -0.0000
   980        0.0000             nan     0.0700   -0.0000
  1000        0.0000             nan     0.0700   -0.0000
  1020        0.0000             nan     0.0700   -0.0000
  1040        0.0000             nan     0.0700   -0.0000
  1060        0.0000             nan     0.0700   -0.0000
  1080        0.0000             nan     0.0700   -0.0000
  1100        0.0000             nan     0.0700   -0.0000
  1120        0.0000             nan     0.0700   -0.0000
  1140        0.0000             nan     0.0700   -0.0000
  1160        0.0000             nan     0.0700   -0.0000
  1180        0.0000             nan     0.0700   -0.0000
  1200        0.0000             nan     0.0700   -0.0000
  1220        0.0000             nan     0.0700   -0.0000
  1240        0.0000             nan     0.0700   -0.0000
  1260        0.0000             nan     0.0700   -0.0000
  1280        0.0000             nan     0.0700   -0.0000
  1300        0.0000             nan     0.0700   -0.0000
  1320        0.0000             nan     0.0700   -0.0000
  1340        0.0000             nan     0.0700   -0.0000
  1360        0.0000             nan     0.0700   -0.0000
  1380        0.0000             nan     0.0700   -0.0000
  1400        0.0000             nan     0.0700   -0.0000
  1420        0.0000             nan     0.0700   -0.0000
  1440        0.0000             nan     0.0700   -0.0000
  1460        0.0000             nan     0.0700   -0.0000
  1480        0.0000             nan     0.0700   -0.0000
  1500        0.0000             nan     0.0700   -0.0000
  1520        0.0000             nan     0.0700   -0.0000
  1540        0.0000             nan     0.0700   -0.0000
  1560        0.0000             nan     0.0700   -0.0000
  1580        0.0000             nan     0.0700   -0.0000
  1600        0.0000             nan     0.0700   -0.0000
  1620        0.0000             nan     0.0700   -0.0000
  1640        0.0000             nan     0.0700   -0.0000
  1660        0.0000             nan     0.0700   -0.0000
  1680        0.0000             nan     0.0700   -0.0000
  1700        0.0000             nan     0.0700   -0.0000
  1720        0.0000             nan     0.0700   -0.0000
  1740        0.0000             nan     0.0700   -0.0000
  1760        0.0000             nan     0.0700   -0.0000
  1765        0.0000             nan     0.0700   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0354             nan     0.0760    0.0038
     2        0.0321             nan     0.0760    0.0033
     3        0.0291             nan     0.0760    0.0028
     4        0.0266             nan     0.0760    0.0027
     5        0.0245             nan     0.0760    0.0022
     6        0.0226             nan     0.0760    0.0019
     7        0.0206             nan     0.0760    0.0017
     8        0.0190             nan     0.0760    0.0014
     9        0.0176             nan     0.0760    0.0014
    10        0.0163             nan     0.0760    0.0012
    20        0.0080             nan     0.0760    0.0004
    40        0.0024             nan     0.0760    0.0001
    60        0.0009             nan     0.0760    0.0000
    80        0.0004             nan     0.0760    0.0000
   100        0.0002             nan     0.0760    0.0000
   120        0.0002             nan     0.0760    0.0000
   140        0.0002             nan     0.0760    0.0000
   160        0.0001             nan     0.0760   -0.0000
   180        0.0001             nan     0.0760    0.0000
   200        0.0001             nan     0.0760    0.0000
   220        0.0001             nan     0.0760   -0.0000
   240        0.0001             nan     0.0760   -0.0000
   260        0.0001             nan     0.0760   -0.0000
   280        0.0001             nan     0.0760   -0.0000
   300        0.0001             nan     0.0760    0.0000
   320        0.0001             nan     0.0760   -0.0000
   340        0.0001             nan     0.0760   -0.0000
   360        0.0001             nan     0.0760   -0.0000
   380        0.0001             nan     0.0760   -0.0000
   400        0.0001             nan     0.0760   -0.0000
   420        0.0001             nan     0.0760   -0.0000
   440        0.0001             nan     0.0760   -0.0000
   460        0.0001             nan     0.0760   -0.0000
   480        0.0001             nan     0.0760   -0.0000
   500        0.0001             nan     0.0760   -0.0000
   520        0.0001             nan     0.0760   -0.0000
   540        0.0001             nan     0.0760   -0.0000
   560        0.0001             nan     0.0760   -0.0000
   580        0.0001             nan     0.0760   -0.0000
   600        0.0001             nan     0.0760   -0.0000
   620        0.0001             nan     0.0760    0.0000
   640        0.0001             nan     0.0760   -0.0000
   660        0.0001             nan     0.0760   -0.0000
   680        0.0001             nan     0.0760   -0.0000
   700        0.0001             nan     0.0760   -0.0000
   720        0.0001             nan     0.0760   -0.0000
   740        0.0001             nan     0.0760   -0.0000
   760        0.0001             nan     0.0760   -0.0000
   780        0.0001             nan     0.0760   -0.0000
   800        0.0001             nan     0.0760   -0.0000
   820        0.0001             nan     0.0760   -0.0000
   840        0.0001             nan     0.0760   -0.0000
   860        0.0001             nan     0.0760   -0.0000
   880        0.0001             nan     0.0760   -0.0000
   900        0.0001             nan     0.0760   -0.0000
   920        0.0001             nan     0.0760   -0.0000
   940        0.0001             nan     0.0760   -0.0000
   960        0.0001             nan     0.0760   -0.0000
   980        0.0001             nan     0.0760   -0.0000
  1000        0.0001             nan     0.0760   -0.0000
  1020        0.0001             nan     0.0760   -0.0000
  1040        0.0001             nan     0.0760   -0.0000
  1060        0.0001             nan     0.0760   -0.0000
  1080        0.0001             nan     0.0760   -0.0000
  1100        0.0001             nan     0.0760   -0.0000
  1120        0.0001             nan     0.0760   -0.0000
  1140        0.0001             nan     0.0760   -0.0000
  1160        0.0001             nan     0.0760   -0.0000
  1180        0.0001             nan     0.0760   -0.0000
  1200        0.0001             nan     0.0760   -0.0000
  1220        0.0001             nan     0.0760   -0.0000
  1240        0.0001             nan     0.0760   -0.0000
  1260        0.0001             nan     0.0760   -0.0000
  1280        0.0001             nan     0.0760   -0.0000
  1300        0.0001             nan     0.0760   -0.0000
  1320        0.0001             nan     0.0760   -0.0000
  1340        0.0001             nan     0.0760   -0.0000
  1360        0.0001             nan     0.0760   -0.0000
  1380        0.0001             nan     0.0760   -0.0000
  1400        0.0001             nan     0.0760   -0.0000
  1420        0.0001             nan     0.0760   -0.0000
  1440        0.0001             nan     0.0760   -0.0000
  1460        0.0001             nan     0.0760   -0.0000
  1480        0.0001             nan     0.0760   -0.0000
  1500        0.0001             nan     0.0760   -0.0000
  1520        0.0001             nan     0.0760   -0.0000
  1540        0.0001             nan     0.0760   -0.0000
  1560        0.0001             nan     0.0760   -0.0000
  1580        0.0001             nan     0.0760   -0.0000
  1600        0.0001             nan     0.0760   -0.0000
  1620        0.0001             nan     0.0760   -0.0000
  1640        0.0001             nan     0.0760   -0.0000
  1660        0.0001             nan     0.0760   -0.0000
  1680        0.0001             nan     0.0760   -0.0000
  1700        0.0001             nan     0.0760   -0.0000
  1720        0.0001             nan     0.0760   -0.0000
  1740        0.0001             nan     0.0760   -0.0000
  1760        0.0001             nan     0.0760   -0.0000
  1780        0.0001             nan     0.0760   -0.0000
  1800        0.0001             nan     0.0760   -0.0000
  1820        0.0001             nan     0.0760   -0.0000
  1840        0.0001             nan     0.0760   -0.0000
  1860        0.0001             nan     0.0760   -0.0000
  1880        0.0001             nan     0.0760   -0.0000
  1900        0.0001             nan     0.0760   -0.0000
  1920        0.0001             nan     0.0760   -0.0000
  1940        0.0001             nan     0.0760   -0.0000
  1960        0.0001             nan     0.0760   -0.0000
  1980        0.0001             nan     0.0760   -0.0000
  2000        0.0001             nan     0.0760   -0.0000
  2020        0.0001             nan     0.0760   -0.0000
  2040        0.0001             nan     0.0760   -0.0000
  2060        0.0001             nan     0.0760   -0.0000
  2080        0.0001             nan     0.0760   -0.0000
  2100        0.0001             nan     0.0760   -0.0000
  2120        0.0001             nan     0.0760   -0.0000
  2140        0.0001             nan     0.0760   -0.0000
  2160        0.0001             nan     0.0760   -0.0000
  2180        0.0001             nan     0.0760   -0.0000
  2200        0.0001             nan     0.0760   -0.0000
  2220        0.0001             nan     0.0760   -0.0000
  2240        0.0001             nan     0.0760   -0.0000
  2260        0.0001             nan     0.0760   -0.0000
  2280        0.0001             nan     0.0760   -0.0000
  2300        0.0000             nan     0.0760   -0.0000
  2320        0.0000             nan     0.0760   -0.0000
  2340        0.0000             nan     0.0760   -0.0000
  2360        0.0000             nan     0.0760   -0.0000
  2380        0.0000             nan     0.0760   -0.0000
  2400        0.0000             nan     0.0760   -0.0000
  2420        0.0000             nan     0.0760   -0.0000
  2440        0.0000             nan     0.0760   -0.0000
  2460        0.0000             nan     0.0760   -0.0000
  2480        0.0000             nan     0.0760   -0.0000
  2500        0.0000             nan     0.0760   -0.0000
  2520        0.0000             nan     0.0760   -0.0000
  2540        0.0000             nan     0.0760   -0.0000
  2560        0.0000             nan     0.0760   -0.0000
  2580        0.0000             nan     0.0760   -0.0000
  2600        0.0000             nan     0.0760   -0.0000
  2620        0.0000             nan     0.0760   -0.0000
  2640        0.0000             nan     0.0760   -0.0000
  2660        0.0000             nan     0.0760   -0.0000
  2680        0.0000             nan     0.0760   -0.0000
  2700        0.0000             nan     0.0760   -0.0000
  2720        0.0000             nan     0.0760   -0.0000
  2740        0.0000             nan     0.0760   -0.0000
  2760        0.0000             nan     0.0760   -0.0000
  2780        0.0000             nan     0.0760   -0.0000
  2800        0.0000             nan     0.0760   -0.0000
  2820        0.0000             nan     0.0760   -0.0000
  2840        0.0000             nan     0.0760   -0.0000
  2860        0.0000             nan     0.0760   -0.0000
  2880        0.0000             nan     0.0760   -0.0000
  2900        0.0000             nan     0.0760   -0.0000
  2920        0.0000             nan     0.0760   -0.0000
  2940        0.0000             nan     0.0760   -0.0000
  2960        0.0000             nan     0.0760   -0.0000
  2980        0.0000             nan     0.0760   -0.0000
  3000        0.0000             nan     0.0760   -0.0000
  3020        0.0000             nan     0.0760   -0.0000
  3040        0.0000             nan     0.0760   -0.0000
  3060        0.0000             nan     0.0760   -0.0000
  3080        0.0000             nan     0.0760   -0.0000
  3100        0.0000             nan     0.0760   -0.0000
  3120        0.0000             nan     0.0760   -0.0000
  3140        0.0000             nan     0.0760   -0.0000
  3160        0.0000             nan     0.0760   -0.0000
  3180        0.0000             nan     0.0760   -0.0000
  3200        0.0000             nan     0.0760   -0.0000
  3220        0.0000             nan     0.0760   -0.0000
  3240        0.0000             nan     0.0760   -0.0000
  3260        0.0000             nan     0.0760   -0.0000
  3280        0.0000             nan     0.0760   -0.0000
  3300        0.0000             nan     0.0760   -0.0000
  3320        0.0000             nan     0.0760   -0.0000
  3340        0.0000             nan     0.0760   -0.0000
  3360        0.0000             nan     0.0760   -0.0000
  3380        0.0000             nan     0.0760   -0.0000
  3400        0.0000             nan     0.0760   -0.0000
  3420        0.0000             nan     0.0760   -0.0000
  3440        0.0000             nan     0.0760   -0.0000
  3460        0.0000             nan     0.0760   -0.0000
  3480        0.0000             nan     0.0760   -0.0000
  3500        0.0000             nan     0.0760   -0.0000
  3520        0.0000             nan     0.0760   -0.0000
  3540        0.0000             nan     0.0760   -0.0000
  3560        0.0000             nan     0.0760   -0.0000
  3580        0.0000             nan     0.0760   -0.0000
  3600        0.0000             nan     0.0760   -0.0000
  3620        0.0000             nan     0.0760   -0.0000
  3640        0.0000             nan     0.0760   -0.0000
  3660        0.0000             nan     0.0760   -0.0000
  3680        0.0000             nan     0.0760   -0.0000
  3700        0.0000             nan     0.0760   -0.0000
  3720        0.0000             nan     0.0760   -0.0000
  3740        0.0000             nan     0.0760   -0.0000
  3760        0.0000             nan     0.0760   -0.0000
  3780        0.0000             nan     0.0760   -0.0000
  3800        0.0000             nan     0.0760   -0.0000
  3820        0.0000             nan     0.0760   -0.0000
  3840        0.0000             nan     0.0760   -0.0000
  3860        0.0000             nan     0.0760   -0.0000
  3880        0.0000             nan     0.0760   -0.0000
  3900        0.0000             nan     0.0760   -0.0000
  3920        0.0000             nan     0.0760   -0.0000
  3940        0.0000             nan     0.0760   -0.0000
  3960        0.0000             nan     0.0760   -0.0000
  3980        0.0000             nan     0.0760   -0.0000
  4000        0.0000             nan     0.0760   -0.0000
  4020        0.0000             nan     0.0760   -0.0000
  4040        0.0000             nan     0.0760   -0.0000
  4060        0.0000             nan     0.0760   -0.0000
  4080        0.0000             nan     0.0760   -0.0000
  4100        0.0000             nan     0.0760   -0.0000
  4120        0.0000             nan     0.0760   -0.0000
  4140        0.0000             nan     0.0760   -0.0000
  4160        0.0000             nan     0.0760   -0.0000
  4180        0.0000             nan     0.0760   -0.0000
  4200        0.0000             nan     0.0760   -0.0000
  4220        0.0000             nan     0.0760   -0.0000
  4240        0.0000             nan     0.0760   -0.0000
  4260        0.0000             nan     0.0760   -0.0000
  4280        0.0000             nan     0.0760   -0.0000
  4300        0.0000             nan     0.0760   -0.0000
  4320        0.0000             nan     0.0760   -0.0000
  4340        0.0000             nan     0.0760   -0.0000
  4360        0.0000             nan     0.0760   -0.0000
  4380        0.0000             nan     0.0760   -0.0000
  4400        0.0000             nan     0.0760   -0.0000
  4420        0.0000             nan     0.0760   -0.0000
  4440        0.0000             nan     0.0760   -0.0000
  4460        0.0000             nan     0.0760   -0.0000
  4480        0.0000             nan     0.0760   -0.0000
  4500        0.0000             nan     0.0760   -0.0000
  4520        0.0000             nan     0.0760   -0.0000
  4540        0.0000             nan     0.0760   -0.0000
  4560        0.0000             nan     0.0760   -0.0000
  4580        0.0000             nan     0.0760   -0.0000
  4600        0.0000             nan     0.0760   -0.0000
  4620        0.0000             nan     0.0760   -0.0000
  4640        0.0000             nan     0.0760   -0.0000
  4660        0.0000             nan     0.0760   -0.0000
  4680        0.0000             nan     0.0760   -0.0000
  4700        0.0000             nan     0.0760   -0.0000
  4720        0.0000             nan     0.0760   -0.0000
  4740        0.0000             nan     0.0760   -0.0000
  4760        0.0000             nan     0.0760   -0.0000
  4780        0.0000             nan     0.0760   -0.0000
  4800        0.0000             nan     0.0760   -0.0000
  4820        0.0000             nan     0.0760   -0.0000
  4840        0.0000             nan     0.0760   -0.0000
  4860        0.0000             nan     0.0760   -0.0000
  4880        0.0000             nan     0.0760   -0.0000
  4900        0.0000             nan     0.0760   -0.0000
  4920        0.0000             nan     0.0760   -0.0000
  4940        0.0000             nan     0.0760   -0.0000
  4960        0.0000             nan     0.0760   -0.0000
  4980        0.0000             nan     0.0760   -0.0000
  4982        0.0000             nan     0.0760   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0331             nan     0.0850    0.0056
     2        0.0280             nan     0.0850    0.0052
     3        0.0236             nan     0.0850    0.0044
     4        0.0200             nan     0.0850    0.0037
     5        0.0169             nan     0.0850    0.0031
     6        0.0144             nan     0.0850    0.0022
     7        0.0122             nan     0.0850    0.0023
     8        0.0103             nan     0.0850    0.0019
     9        0.0088             nan     0.0850    0.0015
    10        0.0075             nan     0.0850    0.0014
    20        0.0016             nan     0.0850    0.0002
    40        0.0001             nan     0.0850    0.0000
    60        0.0000             nan     0.0850    0.0000
    80        0.0000             nan     0.0850    0.0000
   100        0.0000             nan     0.0850   -0.0000
   120        0.0000             nan     0.0850   -0.0000
   140        0.0000             nan     0.0850   -0.0000
   160        0.0000             nan     0.0850   -0.0000
   180        0.0000             nan     0.0850   -0.0000
   200        0.0000             nan     0.0850   -0.0000
   220        0.0000             nan     0.0850   -0.0000
   240        0.0000             nan     0.0850   -0.0000
   260        0.0000             nan     0.0850   -0.0000
   280        0.0000             nan     0.0850   -0.0000
   300        0.0000             nan     0.0850   -0.0000
   320        0.0000             nan     0.0850   -0.0000
   340        0.0000             nan     0.0850   -0.0000
   360        0.0000             nan     0.0850   -0.0000
   380        0.0000             nan     0.0850   -0.0000
   400        0.0000             nan     0.0850   -0.0000
   420        0.0000             nan     0.0850   -0.0000
   440        0.0000             nan     0.0850   -0.0000
   460        0.0000             nan     0.0850   -0.0000
   480        0.0000             nan     0.0850   -0.0000
   500        0.0000             nan     0.0850   -0.0000
   520        0.0000             nan     0.0850   -0.0000
   540        0.0000             nan     0.0850   -0.0000
   560        0.0000             nan     0.0850   -0.0000
   580        0.0000             nan     0.0850   -0.0000
   600        0.0000             nan     0.0850   -0.0000
   620        0.0000             nan     0.0850   -0.0000
   640        0.0000             nan     0.0850   -0.0000
   660        0.0000             nan     0.0850   -0.0000
   680        0.0000             nan     0.0850   -0.0000
   700        0.0000             nan     0.0850   -0.0000
   720        0.0000             nan     0.0850   -0.0000
   740        0.0000             nan     0.0850   -0.0000
   760        0.0000             nan     0.0850   -0.0000
   780        0.0000             nan     0.0850   -0.0000
   800        0.0000             nan     0.0850   -0.0000
   820        0.0000             nan     0.0850   -0.0000
   840        0.0000             nan     0.0850   -0.0000
   860        0.0000             nan     0.0850   -0.0000
   880        0.0000             nan     0.0850   -0.0000
   900        0.0000             nan     0.0850   -0.0000
   920        0.0000             nan     0.0850   -0.0000
   940        0.0000             nan     0.0850   -0.0000
   960        0.0000             nan     0.0850   -0.0000
   980        0.0000             nan     0.0850   -0.0000
  1000        0.0000             nan     0.0850   -0.0000
  1020        0.0000             nan     0.0850   -0.0000
  1040        0.0000             nan     0.0850   -0.0000
  1060        0.0000             nan     0.0850   -0.0000
  1080        0.0000             nan     0.0850   -0.0000
  1100        0.0000             nan     0.0850   -0.0000
  1120        0.0000             nan     0.0850   -0.0000
  1140        0.0000             nan     0.0850   -0.0000
  1160        0.0000             nan     0.0850   -0.0000
  1180        0.0000             nan     0.0850   -0.0000
  1200        0.0000             nan     0.0850   -0.0000
  1220        0.0000             nan     0.0850   -0.0000
  1240        0.0000             nan     0.0850   -0.0000
  1260        0.0000             nan     0.0850   -0.0000
  1280        0.0000             nan     0.0850   -0.0000
  1300        0.0000             nan     0.0850   -0.0000
  1320        0.0000             nan     0.0850   -0.0000
  1340        0.0000             nan     0.0850   -0.0000
  1360        0.0000             nan     0.0850   -0.0000
  1380        0.0000             nan     0.0850   -0.0000
  1400        0.0000             nan     0.0850   -0.0000
  1420        0.0000             nan     0.0850   -0.0000
  1440        0.0000             nan     0.0850   -0.0000
  1460        0.0000             nan     0.0850   -0.0000
  1480        0.0000             nan     0.0850   -0.0000
  1500        0.0000             nan     0.0850   -0.0000
  1520        0.0000             nan     0.0850   -0.0000
  1540        0.0000             nan     0.0850   -0.0000
  1560        0.0000             nan     0.0850   -0.0000
  1580        0.0000             nan     0.0850   -0.0000
  1600        0.0000             nan     0.0850   -0.0000
  1620        0.0000             nan     0.0850   -0.0000
  1640        0.0000             nan     0.0850   -0.0000
  1660        0.0000             nan     0.0850   -0.0000
  1680        0.0000             nan     0.0850   -0.0000
  1700        0.0000             nan     0.0850   -0.0000
  1720        0.0000             nan     0.0850   -0.0000
  1740        0.0000             nan     0.0850   -0.0000
  1760        0.0000             nan     0.0850   -0.0000
  1780        0.0000             nan     0.0850   -0.0000
  1800        0.0000             nan     0.0850   -0.0000
  1820        0.0000             nan     0.0850   -0.0000
  1840        0.0000             nan     0.0850   -0.0000
  1860        0.0000             nan     0.0850   -0.0000
  1880        0.0000             nan     0.0850   -0.0000
  1900        0.0000             nan     0.0850   -0.0000
  1920        0.0000             nan     0.0850   -0.0000
  1940        0.0000             nan     0.0850   -0.0000
  1960        0.0000             nan     0.0850   -0.0000
  1980        0.0000             nan     0.0850   -0.0000
  2000        0.0000             nan     0.0850   -0.0000
  2020        0.0000             nan     0.0850   -0.0000
  2040        0.0000             nan     0.0850   -0.0000
  2060        0.0000             nan     0.0850   -0.0000
  2080        0.0000             nan     0.0850   -0.0000
  2100        0.0000             nan     0.0850   -0.0000
  2120        0.0000             nan     0.0850   -0.0000
  2140        0.0000             nan     0.0850   -0.0000
  2160        0.0000             nan     0.0850   -0.0000
  2180        0.0000             nan     0.0850   -0.0000
  2200        0.0000             nan     0.0850   -0.0000
  2220        0.0000             nan     0.0850   -0.0000
  2240        0.0000             nan     0.0850   -0.0000
  2260        0.0000             nan     0.0850   -0.0000
  2280        0.0000             nan     0.0850   -0.0000
  2300        0.0000             nan     0.0850   -0.0000
  2320        0.0000             nan     0.0850   -0.0000
  2340        0.0000             nan     0.0850   -0.0000
  2360        0.0000             nan     0.0850   -0.0000
  2380        0.0000             nan     0.0850   -0.0000
  2400        0.0000             nan     0.0850   -0.0000
  2420        0.0000             nan     0.0850   -0.0000
  2440        0.0000             nan     0.0850   -0.0000
  2460        0.0000             nan     0.0850   -0.0000
  2480        0.0000             nan     0.0850   -0.0000
  2500        0.0000             nan     0.0850   -0.0000
  2520        0.0000             nan     0.0850   -0.0000
  2540        0.0000             nan     0.0850   -0.0000
  2560        0.0000             nan     0.0850   -0.0000
  2580        0.0000             nan     0.0850   -0.0000
  2600        0.0000             nan     0.0850   -0.0000
  2620        0.0000             nan     0.0850   -0.0000
  2640        0.0000             nan     0.0850   -0.0000
  2660        0.0000             nan     0.0850   -0.0000
  2680        0.0000             nan     0.0850   -0.0000
  2700        0.0000             nan     0.0850   -0.0000
  2720        0.0000             nan     0.0850   -0.0000
  2740        0.0000             nan     0.0850   -0.0000
  2760        0.0000             nan     0.0850   -0.0000
  2780        0.0000             nan     0.0850   -0.0000
  2800        0.0000             nan     0.0850   -0.0000
  2820        0.0000             nan     0.0850   -0.0000
  2840        0.0000             nan     0.0850   -0.0000
  2860        0.0000             nan     0.0850   -0.0000
  2880        0.0000             nan     0.0850   -0.0000
  2900        0.0000             nan     0.0850   -0.0000
  2920        0.0000             nan     0.0850   -0.0000
  2940        0.0000             nan     0.0850   -0.0000
  2960        0.0000             nan     0.0850   -0.0000
  2980        0.0000             nan     0.0850   -0.0000
  3000        0.0000             nan     0.0850   -0.0000
  3020        0.0000             nan     0.0850   -0.0000
  3040        0.0000             nan     0.0850   -0.0000
  3060        0.0000             nan     0.0850   -0.0000
  3080        0.0000             nan     0.0850   -0.0000
  3100        0.0000             nan     0.0850   -0.0000
  3120        0.0000             nan     0.0850   -0.0000
  3140        0.0000             nan     0.0850   -0.0000
  3160        0.0000             nan     0.0850   -0.0000
  3180        0.0000             nan     0.0850   -0.0000
  3200        0.0000             nan     0.0850   -0.0000
  3220        0.0000             nan     0.0850   -0.0000
  3240        0.0000             nan     0.0850   -0.0000
  3260        0.0000             nan     0.0850   -0.0000
  3280        0.0000             nan     0.0850   -0.0000
  3300        0.0000             nan     0.0850   -0.0000
  3320        0.0000             nan     0.0850   -0.0000
  3340        0.0000             nan     0.0850   -0.0000
  3360        0.0000             nan     0.0850   -0.0000
  3380        0.0000             nan     0.0850   -0.0000
  3400        0.0000             nan     0.0850   -0.0000
  3420        0.0000             nan     0.0850   -0.0000
  3440        0.0000             nan     0.0850   -0.0000
  3460        0.0000             nan     0.0850   -0.0000
  3480        0.0000             nan     0.0850   -0.0000
  3500        0.0000             nan     0.0850   -0.0000
  3520        0.0000             nan     0.0850   -0.0000
  3540        0.0000             nan     0.0850   -0.0000
  3560        0.0000             nan     0.0850   -0.0000
  3580        0.0000             nan     0.0850   -0.0000
  3600        0.0000             nan     0.0850   -0.0000
  3620        0.0000             nan     0.0850   -0.0000
  3640        0.0000             nan     0.0850   -0.0000
  3660        0.0000             nan     0.0850   -0.0000
  3680        0.0000             nan     0.0850   -0.0000
  3700        0.0000             nan     0.0850   -0.0000
  3720        0.0000             nan     0.0850   -0.0000
  3740        0.0000             nan     0.0850   -0.0000
  3760        0.0000             nan     0.0850   -0.0000
  3780        0.0000             nan     0.0850   -0.0000
  3800        0.0000             nan     0.0850   -0.0000
  3820        0.0000             nan     0.0850   -0.0000
  3840        0.0000             nan     0.0850   -0.0000
  3860        0.0000             nan     0.0850   -0.0000
  3880        0.0000             nan     0.0850   -0.0000
  3900        0.0000             nan     0.0850   -0.0000
  3920        0.0000             nan     0.0850   -0.0000
  3940        0.0000             nan     0.0850   -0.0000
  3960        0.0000             nan     0.0850   -0.0000
  3980        0.0000             nan     0.0850   -0.0000
  4000        0.0000             nan     0.0850   -0.0000
  4020        0.0000             nan     0.0850   -0.0000
  4040        0.0000             nan     0.0850   -0.0000
  4060        0.0000             nan     0.0850   -0.0000
  4080        0.0000             nan     0.0850   -0.0000
  4100        0.0000             nan     0.0850   -0.0000
  4120        0.0000             nan     0.0850   -0.0000
  4140        0.0000             nan     0.0850   -0.0000
  4160        0.0000             nan     0.0850   -0.0000
  4180        0.0000             nan     0.0850   -0.0000
  4200        0.0000             nan     0.0850   -0.0000
  4220        0.0000             nan     0.0850   -0.0000
  4240        0.0000             nan     0.0850   -0.0000
  4260        0.0000             nan     0.0850   -0.0000
  4280        0.0000             nan     0.0850   -0.0000
  4281        0.0000             nan     0.0850   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0309             nan     0.1213    0.0085
     2        0.0244             nan     0.1213    0.0063
     3        0.0195             nan     0.1213    0.0056
     4        0.0153             nan     0.1213    0.0042
     5        0.0123             nan     0.1213    0.0030
     6        0.0099             nan     0.1213    0.0022
     7        0.0079             nan     0.1213    0.0018
     8        0.0064             nan     0.1213    0.0015
     9        0.0052             nan     0.1213    0.0011
    10        0.0042             nan     0.1213    0.0009
    20        0.0007             nan     0.1213    0.0001
    40        0.0003             nan     0.1213    0.0000
    60        0.0002             nan     0.1213   -0.0000
    80        0.0002             nan     0.1213    0.0000
   100        0.0002             nan     0.1213   -0.0000
   120        0.0001             nan     0.1213   -0.0000
   140        0.0001             nan     0.1213   -0.0000
   160        0.0001             nan     0.1213   -0.0000
   180        0.0001             nan     0.1213   -0.0000
   200        0.0001             nan     0.1213   -0.0000
   220        0.0001             nan     0.1213   -0.0000
   240        0.0001             nan     0.1213   -0.0000
   260        0.0001             nan     0.1213   -0.0000
   280        0.0001             nan     0.1213   -0.0000
   300        0.0001             nan     0.1213   -0.0000
   320        0.0001             nan     0.1213   -0.0000
   340        0.0001             nan     0.1213   -0.0000
   360        0.0001             nan     0.1213   -0.0000
   380        0.0001             nan     0.1213   -0.0000
   400        0.0001             nan     0.1213   -0.0000
   420        0.0001             nan     0.1213   -0.0000
   440        0.0001             nan     0.1213   -0.0000
   460        0.0001             nan     0.1213   -0.0000
   480        0.0000             nan     0.1213   -0.0000
   500        0.0000             nan     0.1213   -0.0000
   520        0.0000             nan     0.1213   -0.0000
   540        0.0000             nan     0.1213   -0.0000
   560        0.0000             nan     0.1213   -0.0000
   580        0.0000             nan     0.1213   -0.0000
   600        0.0000             nan     0.1213   -0.0000
   620        0.0000             nan     0.1213   -0.0000
   640        0.0000             nan     0.1213   -0.0000
   660        0.0000             nan     0.1213   -0.0000
   680        0.0000             nan     0.1213   -0.0000
   700        0.0000             nan     0.1213   -0.0000
   720        0.0000             nan     0.1213   -0.0000
   740        0.0000             nan     0.1213   -0.0000
   760        0.0000             nan     0.1213   -0.0000
   780        0.0000             nan     0.1213   -0.0000
   800        0.0000             nan     0.1213   -0.0000
   820        0.0000             nan     0.1213   -0.0000
   840        0.0000             nan     0.1213   -0.0000
   860        0.0000             nan     0.1213   -0.0000
   880        0.0000             nan     0.1213   -0.0000
   900        0.0000             nan     0.1213   -0.0000
   920        0.0000             nan     0.1213   -0.0000
   940        0.0000             nan     0.1213   -0.0000
   960        0.0000             nan     0.1213   -0.0000
   980        0.0000             nan     0.1213   -0.0000
  1000        0.0000             nan     0.1213   -0.0000
  1020        0.0000             nan     0.1213   -0.0000
  1040        0.0000             nan     0.1213   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0302             nan     0.1295    0.0097
     2        0.0232             nan     0.1295    0.0071
     3        0.0178             nan     0.1295    0.0055
     4        0.0138             nan     0.1295    0.0043
     5        0.0108             nan     0.1295    0.0030
     6        0.0084             nan     0.1295    0.0023
     7        0.0066             nan     0.1295    0.0016
     8        0.0052             nan     0.1295    0.0014
     9        0.0040             nan     0.1295    0.0011
    10        0.0031             nan     0.1295    0.0008
    20        0.0004             nan     0.1295    0.0001
    40        0.0001             nan     0.1295    0.0000
    60        0.0001             nan     0.1295   -0.0000
    80        0.0000             nan     0.1295   -0.0000
   100        0.0000             nan     0.1295   -0.0000
   120        0.0000             nan     0.1295   -0.0000
   140        0.0000             nan     0.1295   -0.0000
   160        0.0000             nan     0.1295   -0.0000
   180        0.0000             nan     0.1295   -0.0000
   200        0.0000             nan     0.1295   -0.0000
   220        0.0000             nan     0.1295   -0.0000
   240        0.0000             nan     0.1295   -0.0000
   260        0.0000             nan     0.1295   -0.0000
   280        0.0000             nan     0.1295   -0.0000
   300        0.0000             nan     0.1295   -0.0000
   320        0.0000             nan     0.1295   -0.0000
   340        0.0000             nan     0.1295   -0.0000
   360        0.0000             nan     0.1295   -0.0000
   380        0.0000             nan     0.1295   -0.0000
   400        0.0000             nan     0.1295   -0.0000
   420        0.0000             nan     0.1295   -0.0000
   440        0.0000             nan     0.1295   -0.0000
   460        0.0000             nan     0.1295   -0.0000
   480        0.0000             nan     0.1295   -0.0000
   500        0.0000             nan     0.1295   -0.0000
   520        0.0000             nan     0.1295   -0.0000
   540        0.0000             nan     0.1295   -0.0000
   560        0.0000             nan     0.1295   -0.0000
   580        0.0000             nan     0.1295   -0.0000
   600        0.0000             nan     0.1295   -0.0000
   620        0.0000             nan     0.1295   -0.0000
   640        0.0000             nan     0.1295   -0.0000
   660        0.0000             nan     0.1295   -0.0000
   673        0.0000             nan     0.1295   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0283             nan     0.1700    0.0111
     2        0.0205             nan     0.1700    0.0079
     3        0.0149             nan     0.1700    0.0055
     4        0.0109             nan     0.1700    0.0039
     5        0.0081             nan     0.1700    0.0030
     6        0.0059             nan     0.1700    0.0020
     7        0.0044             nan     0.1700    0.0015
     8        0.0033             nan     0.1700    0.0012
     9        0.0025             nan     0.1700    0.0008
    10        0.0019             nan     0.1700    0.0006
    20        0.0003             nan     0.1700    0.0000
    40        0.0001             nan     0.1700    0.0000
    60        0.0001             nan     0.1700   -0.0000
    80        0.0001             nan     0.1700   -0.0000
   100        0.0001             nan     0.1700   -0.0000
   120        0.0001             nan     0.1700   -0.0000
   140        0.0001             nan     0.1700   -0.0000
   160        0.0000             nan     0.1700   -0.0000
   180        0.0000             nan     0.1700   -0.0000
   200        0.0000             nan     0.1700   -0.0000
   220        0.0000             nan     0.1700   -0.0000
   240        0.0000             nan     0.1700   -0.0000
   260        0.0000             nan     0.1700   -0.0000
   280        0.0000             nan     0.1700   -0.0000
   300        0.0000             nan     0.1700   -0.0000
   320        0.0000             nan     0.1700   -0.0000
   340        0.0000             nan     0.1700   -0.0000
   360        0.0000             nan     0.1700   -0.0000
   380        0.0000             nan     0.1700   -0.0000
   400        0.0000             nan     0.1700   -0.0000
   420        0.0000             nan     0.1700   -0.0000
   440        0.0000             nan     0.1700   -0.0000
   460        0.0000             nan     0.1700   -0.0000
   480        0.0000             nan     0.1700   -0.0000
   500        0.0000             nan     0.1700   -0.0000
   520        0.0000             nan     0.1700   -0.0000
   540        0.0000             nan     0.1700   -0.0000
   560        0.0000             nan     0.1700   -0.0000
   580        0.0000             nan     0.1700   -0.0000
   600        0.0000             nan     0.1700   -0.0000
   620        0.0000             nan     0.1700   -0.0000
   640        0.0000             nan     0.1700   -0.0000
   660        0.0000             nan     0.1700   -0.0000
   680        0.0000             nan     0.1700   -0.0000
   700        0.0000             nan     0.1700   -0.0000
   720        0.0000             nan     0.1700   -0.0000
   740        0.0000             nan     0.1700   -0.0000
   760        0.0000             nan     0.1700   -0.0000
   780        0.0000             nan     0.1700   -0.0000
   800        0.0000             nan     0.1700   -0.0000
   820        0.0000             nan     0.1700   -0.0000
   840        0.0000             nan     0.1700   -0.0000
   860        0.0000             nan     0.1700   -0.0000
   880        0.0000             nan     0.1700   -0.0000
   900        0.0000             nan     0.1700   -0.0000
   920        0.0000             nan     0.1700   -0.0000
   940        0.0000             nan     0.1700   -0.0000
   960        0.0000             nan     0.1700   -0.0000
   980        0.0000             nan     0.1700   -0.0000
  1000        0.0000             nan     0.1700   -0.0000
  1020        0.0000             nan     0.1700   -0.0000
  1040        0.0000             nan     0.1700   -0.0000
  1060        0.0000             nan     0.1700   -0.0000
  1080        0.0000             nan     0.1700   -0.0000
  1100        0.0000             nan     0.1700   -0.0000
  1120        0.0000             nan     0.1700   -0.0000
  1140        0.0000             nan     0.1700   -0.0000
  1160        0.0000             nan     0.1700   -0.0000
  1180        0.0000             nan     0.1700   -0.0000
  1200        0.0000             nan     0.1700   -0.0000
  1220        0.0000             nan     0.1700   -0.0000
  1240        0.0000             nan     0.1700   -0.0000
  1260        0.0000             nan     0.1700   -0.0000
  1280        0.0000             nan     0.1700   -0.0000
  1300        0.0000             nan     0.1700   -0.0000
  1320        0.0000             nan     0.1700   -0.0000
  1340        0.0000             nan     0.1700   -0.0000
  1360        0.0000             nan     0.1700   -0.0000
  1380        0.0000             nan     0.1700   -0.0000
  1400        0.0000             nan     0.1700   -0.0000
  1420        0.0000             nan     0.1700   -0.0000
  1440        0.0000             nan     0.1700   -0.0000
  1460        0.0000             nan     0.1700   -0.0000
  1480        0.0000             nan     0.1700   -0.0000
  1500        0.0000             nan     0.1700   -0.0000
  1520        0.0000             nan     0.1700   -0.0000
  1540        0.0000             nan     0.1700   -0.0000
  1560        0.0000             nan     0.1700   -0.0000
  1580        0.0000             nan     0.1700   -0.0000
  1600        0.0000             nan     0.1700   -0.0000
  1620        0.0000             nan     0.1700   -0.0000
  1640        0.0000             nan     0.1700   -0.0000
  1660        0.0000             nan     0.1700   -0.0000
  1680        0.0000             nan     0.1700   -0.0000
  1700        0.0000             nan     0.1700   -0.0000
  1720        0.0000             nan     0.1700   -0.0000
  1740        0.0000             nan     0.1700   -0.0000
  1760        0.0000             nan     0.1700   -0.0000
  1780        0.0000             nan     0.1700   -0.0000
  1800        0.0000             nan     0.1700   -0.0000
  1820        0.0000             nan     0.1700   -0.0000
  1840        0.0000             nan     0.1700   -0.0000
  1860        0.0000             nan     0.1700    0.0000
  1880        0.0000             nan     0.1700   -0.0000
  1900        0.0000             nan     0.1700   -0.0000
  1920        0.0000             nan     0.1700    0.0000
  1940        0.0000             nan     0.1700   -0.0000
  1960        0.0000             nan     0.1700   -0.0000
  1980        0.0000             nan     0.1700   -0.0000
  2000        0.0000             nan     0.1700   -0.0000
  2020        0.0000             nan     0.1700   -0.0000
  2040        0.0000             nan     0.1700   -0.0000
  2060        0.0000             nan     0.1700   -0.0000
  2080        0.0000             nan     0.1700   -0.0000
  2100        0.0000             nan     0.1700   -0.0000
  2120        0.0000             nan     0.1700   -0.0000
  2140        0.0000             nan     0.1700   -0.0000
  2160        0.0000             nan     0.1700   -0.0000
  2180        0.0000             nan     0.1700   -0.0000
  2200        0.0000             nan     0.1700   -0.0000
  2220        0.0000             nan     0.1700   -0.0000
  2240        0.0000             nan     0.1700   -0.0000
  2260        0.0000             nan     0.1700    0.0000
  2280        0.0000             nan     0.1700   -0.0000
  2300        0.0000             nan     0.1700   -0.0000
  2320        0.0000             nan     0.1700   -0.0000
  2340        0.0000             nan     0.1700   -0.0000
  2360        0.0000             nan     0.1700   -0.0000
  2380        0.0000             nan     0.1700   -0.0000
  2400        0.0000             nan     0.1700   -0.0000
  2420        0.0000             nan     0.1700   -0.0000
  2440        0.0000             nan     0.1700   -0.0000
  2460        0.0000             nan     0.1700   -0.0000
  2480        0.0000             nan     0.1700   -0.0000
  2500        0.0000             nan     0.1700   -0.0000
  2520        0.0000             nan     0.1700   -0.0000
  2540        0.0000             nan     0.1700   -0.0000
  2560        0.0000             nan     0.1700   -0.0000
  2580        0.0000             nan     0.1700   -0.0000
  2600        0.0000             nan     0.1700   -0.0000
  2620        0.0000             nan     0.1700   -0.0000
  2640        0.0000             nan     0.1700   -0.0000
  2660        0.0000             nan     0.1700   -0.0000
  2680        0.0000             nan     0.1700   -0.0000
  2700        0.0000             nan     0.1700   -0.0000
  2720        0.0000             nan     0.1700   -0.0000
  2740        0.0000             nan     0.1700   -0.0000
  2760        0.0000             nan     0.1700   -0.0000
  2780        0.0000             nan     0.1700   -0.0000
  2800        0.0000             nan     0.1700   -0.0000
  2820        0.0000             nan     0.1700   -0.0000
  2840        0.0000             nan     0.1700   -0.0000
  2860        0.0000             nan     0.1700   -0.0000
  2880        0.0000             nan     0.1700   -0.0000
  2900        0.0000             nan     0.1700   -0.0000
  2920        0.0000             nan     0.1700   -0.0000
  2940        0.0000             nan     0.1700   -0.0000
  2960        0.0000             nan     0.1700   -0.0000
  2980        0.0000             nan     0.1700   -0.0000
  3000        0.0000             nan     0.1700   -0.0000
  3020        0.0000             nan     0.1700   -0.0000
  3040        0.0000             nan     0.1700   -0.0000
  3060        0.0000             nan     0.1700   -0.0000
  3080        0.0000             nan     0.1700   -0.0000
  3100        0.0000             nan     0.1700   -0.0000
  3120        0.0000             nan     0.1700   -0.0000
  3140        0.0000             nan     0.1700   -0.0000
  3160        0.0000             nan     0.1700   -0.0000
  3180        0.0000             nan     0.1700   -0.0000
  3200        0.0000             nan     0.1700   -0.0000
  3220        0.0000             nan     0.1700   -0.0000
  3240        0.0000             nan     0.1700   -0.0000
  3260        0.0000             nan     0.1700   -0.0000
  3280        0.0000             nan     0.1700   -0.0000
  3300        0.0000             nan     0.1700   -0.0000
  3320        0.0000             nan     0.1700   -0.0000
  3340        0.0000             nan     0.1700   -0.0000
  3360        0.0000             nan     0.1700   -0.0000
  3380        0.0000             nan     0.1700   -0.0000
  3400        0.0000             nan     0.1700   -0.0000
  3420        0.0000             nan     0.1700    0.0000
  3440        0.0000             nan     0.1700   -0.0000
  3460        0.0000             nan     0.1700   -0.0000
  3480        0.0000             nan     0.1700   -0.0000
  3500        0.0000             nan     0.1700   -0.0000
  3520        0.0000             nan     0.1700    0.0000
  3540        0.0000             nan     0.1700   -0.0000
  3560        0.0000             nan     0.1700   -0.0000
  3580        0.0000             nan     0.1700    0.0000
  3600        0.0000             nan     0.1700   -0.0000
  3620        0.0000             nan     0.1700   -0.0000
  3640        0.0000             nan     0.1700   -0.0000
  3660        0.0000             nan     0.1700   -0.0000
  3680        0.0000             nan     0.1700   -0.0000
  3700        0.0000             nan     0.1700   -0.0000
  3720        0.0000             nan     0.1700   -0.0000
  3740        0.0000             nan     0.1700    0.0000
  3760        0.0000             nan     0.1700   -0.0000
  3780        0.0000             nan     0.1700   -0.0000
  3800        0.0000             nan     0.1700   -0.0000
  3820        0.0000             nan     0.1700   -0.0000
  3840        0.0000             nan     0.1700   -0.0000
  3860        0.0000             nan     0.1700   -0.0000
  3880        0.0000             nan     0.1700   -0.0000
  3900        0.0000             nan     0.1700   -0.0000
  3920        0.0000             nan     0.1700   -0.0000
  3940        0.0000             nan     0.1700   -0.0000
  3960        0.0000             nan     0.1700    0.0000
  3980        0.0000             nan     0.1700    0.0000
  4000        0.0000             nan     0.1700   -0.0000
  4020        0.0000             nan     0.1700   -0.0000
  4040        0.0000             nan     0.1700   -0.0000
  4060        0.0000             nan     0.1700   -0.0000
  4080        0.0000             nan     0.1700   -0.0000
  4100        0.0000             nan     0.1700   -0.0000
  4120        0.0000             nan     0.1700   -0.0000
  4140        0.0000             nan     0.1700   -0.0000
  4160        0.0000             nan     0.1700   -0.0000
  4180        0.0000             nan     0.1700   -0.0000
  4200        0.0000             nan     0.1700   -0.0000
  4220        0.0000             nan     0.1700   -0.0000
  4240        0.0000             nan     0.1700   -0.0000
  4260        0.0000             nan     0.1700   -0.0000
  4280        0.0000             nan     0.1700   -0.0000
  4300        0.0000             nan     0.1700   -0.0000
  4320        0.0000             nan     0.1700   -0.0000
  4340        0.0000             nan     0.1700    0.0000
  4360        0.0000             nan     0.1700   -0.0000
  4380        0.0000             nan     0.1700   -0.0000
  4400        0.0000             nan     0.1700   -0.0000
  4420        0.0000             nan     0.1700   -0.0000
  4440        0.0000             nan     0.1700   -0.0000
  4460        0.0000             nan     0.1700   -0.0000
  4480        0.0000             nan     0.1700   -0.0000
  4500        0.0000             nan     0.1700   -0.0000
  4520        0.0000             nan     0.1700   -0.0000
  4540        0.0000             nan     0.1700   -0.0000
  4554        0.0000             nan     0.1700   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0276             nan     0.1821    0.0123
     2        0.0196             nan     0.1821    0.0082
     3        0.0139             nan     0.1821    0.0054
     4        0.0100             nan     0.1821    0.0040
     5        0.0072             nan     0.1821    0.0023
     6        0.0053             nan     0.1821    0.0018
     7        0.0039             nan     0.1821    0.0015
     8        0.0028             nan     0.1821    0.0011
     9        0.0021             nan     0.1821    0.0006
    10        0.0016             nan     0.1821    0.0005
    20        0.0003             nan     0.1821    0.0000
    40        0.0002             nan     0.1821    0.0000
    60        0.0001             nan     0.1821   -0.0000
    80        0.0001             nan     0.1821   -0.0000
   100        0.0001             nan     0.1821   -0.0000
   120        0.0001             nan     0.1821   -0.0000
   140        0.0001             nan     0.1821   -0.0000
   160        0.0001             nan     0.1821   -0.0000
   180        0.0001             nan     0.1821   -0.0000
   200        0.0001             nan     0.1821   -0.0000
   220        0.0000             nan     0.1821   -0.0000
   240        0.0000             nan     0.1821   -0.0000
   260        0.0000             nan     0.1821   -0.0000
   280        0.0000             nan     0.1821   -0.0000
   300        0.0000             nan     0.1821   -0.0000
   320        0.0000             nan     0.1821   -0.0000
   340        0.0000             nan     0.1821   -0.0000
   360        0.0000             nan     0.1821   -0.0000
   380        0.0000             nan     0.1821   -0.0000
   400        0.0000             nan     0.1821   -0.0000
   420        0.0000             nan     0.1821   -0.0000
   440        0.0000             nan     0.1821   -0.0000
   460        0.0000             nan     0.1821   -0.0000
   480        0.0000             nan     0.1821   -0.0000
   500        0.0000             nan     0.1821   -0.0000
   520        0.0000             nan     0.1821   -0.0000
   540        0.0000             nan     0.1821   -0.0000
   560        0.0000             nan     0.1821   -0.0000
   580        0.0000             nan     0.1821   -0.0000
   600        0.0000             nan     0.1821   -0.0000
   620        0.0000             nan     0.1821   -0.0000
   640        0.0000             nan     0.1821   -0.0000
   660        0.0000             nan     0.1821   -0.0000
   680        0.0000             nan     0.1821   -0.0000
   700        0.0000             nan     0.1821   -0.0000
   720        0.0000             nan     0.1821   -0.0000
   740        0.0000             nan     0.1821   -0.0000
   760        0.0000             nan     0.1821   -0.0000
   780        0.0000             nan     0.1821   -0.0000
   800        0.0000             nan     0.1821   -0.0000
   820        0.0000             nan     0.1821   -0.0000
   840        0.0000             nan     0.1821   -0.0000
   860        0.0000             nan     0.1821   -0.0000
   880        0.0000             nan     0.1821   -0.0000
   900        0.0000             nan     0.1821   -0.0000
   920        0.0000             nan     0.1821   -0.0000
   940        0.0000             nan     0.1821   -0.0000
   960        0.0000             nan     0.1821   -0.0000
   980        0.0000             nan     0.1821   -0.0000
  1000        0.0000             nan     0.1821   -0.0000
  1020        0.0000             nan     0.1821   -0.0000
  1040        0.0000             nan     0.1821   -0.0000
  1060        0.0000             nan     0.1821   -0.0000
  1080        0.0000             nan     0.1821   -0.0000
  1100        0.0000             nan     0.1821   -0.0000
  1120        0.0000             nan     0.1821   -0.0000
  1140        0.0000             nan     0.1821   -0.0000
  1160        0.0000             nan     0.1821   -0.0000
  1180        0.0000             nan     0.1821   -0.0000
  1200        0.0000             nan     0.1821   -0.0000
  1220        0.0000             nan     0.1821   -0.0000
  1240        0.0000             nan     0.1821   -0.0000
  1260        0.0000             nan     0.1821   -0.0000
  1280        0.0000             nan     0.1821   -0.0000
  1300        0.0000             nan     0.1821   -0.0000
  1320        0.0000             nan     0.1821   -0.0000
  1340        0.0000             nan     0.1821   -0.0000
  1360        0.0000             nan     0.1821   -0.0000
  1380        0.0000             nan     0.1821   -0.0000
  1400        0.0000             nan     0.1821   -0.0000
  1420        0.0000             nan     0.1821   -0.0000
  1440        0.0000             nan     0.1821   -0.0000
  1460        0.0000             nan     0.1821   -0.0000
  1480        0.0000             nan     0.1821   -0.0000
  1500        0.0000             nan     0.1821   -0.0000
  1520        0.0000             nan     0.1821   -0.0000
  1540        0.0000             nan     0.1821   -0.0000
  1560        0.0000             nan     0.1821   -0.0000
  1580        0.0000             nan     0.1821   -0.0000
  1600        0.0000             nan     0.1821   -0.0000
  1620        0.0000             nan     0.1821   -0.0000
  1640        0.0000             nan     0.1821   -0.0000
  1660        0.0000             nan     0.1821   -0.0000
  1680        0.0000             nan     0.1821   -0.0000
  1700        0.0000             nan     0.1821   -0.0000
  1720        0.0000             nan     0.1821   -0.0000
  1740        0.0000             nan     0.1821   -0.0000
  1760        0.0000             nan     0.1821   -0.0000
  1780        0.0000             nan     0.1821   -0.0000
  1800        0.0000             nan     0.1821   -0.0000
  1820        0.0000             nan     0.1821   -0.0000
  1840        0.0000             nan     0.1821   -0.0000
  1860        0.0000             nan     0.1821   -0.0000
  1880        0.0000             nan     0.1821   -0.0000
  1900        0.0000             nan     0.1821   -0.0000
  1920        0.0000             nan     0.1821   -0.0000
  1940        0.0000             nan     0.1821   -0.0000
  1960        0.0000             nan     0.1821   -0.0000
  1980        0.0000             nan     0.1821   -0.0000
  2000        0.0000             nan     0.1821   -0.0000
  2020        0.0000             nan     0.1821   -0.0000
  2040        0.0000             nan     0.1821   -0.0000
  2060        0.0000             nan     0.1821   -0.0000
  2080        0.0000             nan     0.1821   -0.0000
  2100        0.0000             nan     0.1821   -0.0000
  2120        0.0000             nan     0.1821   -0.0000
  2140        0.0000             nan     0.1821   -0.0000
  2160        0.0000             nan     0.1821   -0.0000
  2180        0.0000             nan     0.1821   -0.0000
  2200        0.0000             nan     0.1821   -0.0000
  2220        0.0000             nan     0.1821   -0.0000
  2240        0.0000             nan     0.1821    0.0000
  2260        0.0000             nan     0.1821   -0.0000
  2280        0.0000             nan     0.1821   -0.0000
  2300        0.0000             nan     0.1821   -0.0000
  2320        0.0000             nan     0.1821   -0.0000
  2340        0.0000             nan     0.1821   -0.0000
  2360        0.0000             nan     0.1821   -0.0000
  2380        0.0000             nan     0.1821   -0.0000
  2400        0.0000             nan     0.1821   -0.0000
  2420        0.0000             nan     0.1821   -0.0000
  2440        0.0000             nan     0.1821   -0.0000
  2460        0.0000             nan     0.1821   -0.0000
  2480        0.0000             nan     0.1821   -0.0000
  2500        0.0000             nan     0.1821   -0.0000
  2520        0.0000             nan     0.1821   -0.0000
  2540        0.0000             nan     0.1821   -0.0000
  2560        0.0000             nan     0.1821   -0.0000
  2580        0.0000             nan     0.1821   -0.0000
  2600        0.0000             nan     0.1821   -0.0000
  2620        0.0000             nan     0.1821   -0.0000
  2640        0.0000             nan     0.1821   -0.0000
  2660        0.0000             nan     0.1821   -0.0000
  2680        0.0000             nan     0.1821   -0.0000
  2700        0.0000             nan     0.1821   -0.0000
  2720        0.0000             nan     0.1821   -0.0000
  2740        0.0000             nan     0.1821   -0.0000
  2760        0.0000             nan     0.1821    0.0000
  2780        0.0000             nan     0.1821   -0.0000
  2800        0.0000             nan     0.1821    0.0000
  2820        0.0000             nan     0.1821   -0.0000
  2840        0.0000             nan     0.1821   -0.0000
  2860        0.0000             nan     0.1821   -0.0000
  2880        0.0000             nan     0.1821   -0.0000
  2900        0.0000             nan     0.1821   -0.0000
  2920        0.0000             nan     0.1821   -0.0000
  2940        0.0000             nan     0.1821    0.0000
  2960        0.0000             nan     0.1821   -0.0000
  2980        0.0000             nan     0.1821   -0.0000
  3000        0.0000             nan     0.1821   -0.0000
  3020        0.0000             nan     0.1821   -0.0000
  3040        0.0000             nan     0.1821   -0.0000
  3060        0.0000             nan     0.1821   -0.0000
  3080        0.0000             nan     0.1821   -0.0000
  3100        0.0000             nan     0.1821   -0.0000
  3120        0.0000             nan     0.1821   -0.0000
  3140        0.0000             nan     0.1821   -0.0000
  3160        0.0000             nan     0.1821   -0.0000
  3180        0.0000             nan     0.1821   -0.0000
  3200        0.0000             nan     0.1821   -0.0000
  3220        0.0000             nan     0.1821   -0.0000
  3240        0.0000             nan     0.1821   -0.0000
  3260        0.0000             nan     0.1821   -0.0000
  3266        0.0000             nan     0.1821    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0265             nan     0.2033    0.0123
     2        0.0181             nan     0.2033    0.0080
     3        0.0127             nan     0.2033    0.0053
     4        0.0089             nan     0.2033    0.0034
     5        0.0062             nan     0.2033    0.0024
     6        0.0045             nan     0.2033    0.0018
     7        0.0033             nan     0.2033    0.0012
     8        0.0025             nan     0.2033    0.0008
     9        0.0019             nan     0.2033    0.0006
    10        0.0015             nan     0.2033    0.0004
    20        0.0005             nan     0.2033    0.0000
    40        0.0003             nan     0.2033   -0.0000
    60        0.0002             nan     0.2033   -0.0000
    80        0.0002             nan     0.2033   -0.0000
   100        0.0002             nan     0.2033   -0.0000
   120        0.0002             nan     0.2033   -0.0000
   140        0.0001             nan     0.2033   -0.0000
   160        0.0001             nan     0.2033   -0.0000
   180        0.0001             nan     0.2033   -0.0000
   200        0.0001             nan     0.2033   -0.0000
   220        0.0001             nan     0.2033   -0.0000
   240        0.0001             nan     0.2033   -0.0000
   260        0.0001             nan     0.2033   -0.0000
   280        0.0001             nan     0.2033   -0.0000
   300        0.0001             nan     0.2033   -0.0000
   320        0.0001             nan     0.2033   -0.0000
   340        0.0001             nan     0.2033   -0.0000
   360        0.0001             nan     0.2033   -0.0000
   380        0.0001             nan     0.2033   -0.0000
   400        0.0001             nan     0.2033   -0.0000
   420        0.0001             nan     0.2033   -0.0000
   440        0.0001             nan     0.2033   -0.0000
   460        0.0001             nan     0.2033   -0.0000
   480        0.0001             nan     0.2033   -0.0000
   500        0.0001             nan     0.2033   -0.0000
   520        0.0001             nan     0.2033   -0.0000
   540        0.0001             nan     0.2033   -0.0000
   560        0.0001             nan     0.2033   -0.0000
   580        0.0000             nan     0.2033   -0.0000
   600        0.0000             nan     0.2033   -0.0000
   620        0.0000             nan     0.2033   -0.0000
   640        0.0000             nan     0.2033   -0.0000
   660        0.0000             nan     0.2033   -0.0000
   680        0.0000             nan     0.2033   -0.0000
   700        0.0000             nan     0.2033   -0.0000
   720        0.0000             nan     0.2033   -0.0000
   740        0.0000             nan     0.2033   -0.0000
   760        0.0000             nan     0.2033   -0.0000
   780        0.0000             nan     0.2033   -0.0000
   800        0.0000             nan     0.2033   -0.0000
   820        0.0000             nan     0.2033   -0.0000
   840        0.0000             nan     0.2033   -0.0000
   860        0.0000             nan     0.2033   -0.0000
   880        0.0000             nan     0.2033   -0.0000
   900        0.0000             nan     0.2033   -0.0000
   920        0.0000             nan     0.2033   -0.0000
   940        0.0000             nan     0.2033   -0.0000
   960        0.0000             nan     0.2033   -0.0000
   980        0.0000             nan     0.2033   -0.0000
  1000        0.0000             nan     0.2033   -0.0000
  1020        0.0000             nan     0.2033   -0.0000
  1040        0.0000             nan     0.2033   -0.0000
  1060        0.0000             nan     0.2033   -0.0000
  1080        0.0000             nan     0.2033   -0.0000
  1100        0.0000             nan     0.2033   -0.0000
  1120        0.0000             nan     0.2033   -0.0000
  1140        0.0000             nan     0.2033   -0.0000
  1160        0.0000             nan     0.2033   -0.0000
  1180        0.0000             nan     0.2033   -0.0000
  1200        0.0000             nan     0.2033   -0.0000
  1220        0.0000             nan     0.2033   -0.0000
  1240        0.0000             nan     0.2033   -0.0000
  1260        0.0000             nan     0.2033   -0.0000
  1280        0.0000             nan     0.2033   -0.0000
  1300        0.0000             nan     0.2033   -0.0000
  1320        0.0000             nan     0.2033   -0.0000
  1340        0.0000             nan     0.2033   -0.0000
  1360        0.0000             nan     0.2033   -0.0000
  1380        0.0000             nan     0.2033   -0.0000
  1400        0.0000             nan     0.2033   -0.0000
  1420        0.0000             nan     0.2033   -0.0000
  1440        0.0000             nan     0.2033   -0.0000
  1460        0.0000             nan     0.2033   -0.0000
  1480        0.0000             nan     0.2033   -0.0000
  1500        0.0000             nan     0.2033   -0.0000
  1520        0.0000             nan     0.2033   -0.0000
  1540        0.0000             nan     0.2033   -0.0000
  1560        0.0000             nan     0.2033   -0.0000
  1580        0.0000             nan     0.2033   -0.0000
  1600        0.0000             nan     0.2033   -0.0000
  1620        0.0000             nan     0.2033   -0.0000
  1640        0.0000             nan     0.2033   -0.0000
  1660        0.0000             nan     0.2033   -0.0000
  1680        0.0000             nan     0.2033   -0.0000
  1700        0.0000             nan     0.2033   -0.0000
  1720        0.0000             nan     0.2033   -0.0000
  1740        0.0000             nan     0.2033   -0.0000
  1760        0.0000             nan     0.2033   -0.0000
  1780        0.0000             nan     0.2033   -0.0000
  1800        0.0000             nan     0.2033   -0.0000
  1820        0.0000             nan     0.2033   -0.0000
  1840        0.0000             nan     0.2033   -0.0000
  1860        0.0000             nan     0.2033    0.0000
  1880        0.0000             nan     0.2033   -0.0000
  1900        0.0000             nan     0.2033   -0.0000
  1905        0.0000             nan     0.2033   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0243             nan     0.2560    0.0153
     2        0.0149             nan     0.2560    0.0095
     3        0.0095             nan     0.2560    0.0053
     4        0.0061             nan     0.2560    0.0034
     5        0.0040             nan     0.2560    0.0019
     6        0.0027             nan     0.2560    0.0012
     7        0.0018             nan     0.2560    0.0008
     8        0.0013             nan     0.2560    0.0005
     9        0.0010             nan     0.2560    0.0003
    10        0.0007             nan     0.2560    0.0002
    20        0.0002             nan     0.2560    0.0000
    40        0.0001             nan     0.2560   -0.0000
    60        0.0001             nan     0.2560   -0.0000
    80        0.0001             nan     0.2560   -0.0000
   100        0.0001             nan     0.2560   -0.0000
   120        0.0000             nan     0.2560   -0.0000
   140        0.0000             nan     0.2560   -0.0000
   160        0.0000             nan     0.2560   -0.0000
   180        0.0000             nan     0.2560   -0.0000
   200        0.0000             nan     0.2560   -0.0000
   220        0.0000             nan     0.2560   -0.0000
   240        0.0000             nan     0.2560   -0.0000
   260        0.0000             nan     0.2560   -0.0000
   280        0.0000             nan     0.2560   -0.0000
   300        0.0000             nan     0.2560   -0.0000
   320        0.0000             nan     0.2560   -0.0000
   340        0.0000             nan     0.2560   -0.0000
   360        0.0000             nan     0.2560   -0.0000
   380        0.0000             nan     0.2560   -0.0000
   400        0.0000             nan     0.2560   -0.0000
   420        0.0000             nan     0.2560   -0.0000
   440        0.0000             nan     0.2560   -0.0000
   460        0.0000             nan     0.2560   -0.0000
   480        0.0000             nan     0.2560   -0.0000
   500        0.0000             nan     0.2560   -0.0000
   520        0.0000             nan     0.2560   -0.0000
   540        0.0000             nan     0.2560   -0.0000
   560        0.0000             nan     0.2560   -0.0000
   580        0.0000             nan     0.2560   -0.0000
   600        0.0000             nan     0.2560   -0.0000
   620        0.0000             nan     0.2560   -0.0000
   640        0.0000             nan     0.2560   -0.0000
   660        0.0000             nan     0.2560   -0.0000
   680        0.0000             nan     0.2560   -0.0000
   700        0.0000             nan     0.2560   -0.0000
   720        0.0000             nan     0.2560   -0.0000
   740        0.0000             nan     0.2560   -0.0000
   760        0.0000             nan     0.2560   -0.0000
   780        0.0000             nan     0.2560   -0.0000
   800        0.0000             nan     0.2560   -0.0000
   820        0.0000             nan     0.2560   -0.0000
   840        0.0000             nan     0.2560   -0.0000
   860        0.0000             nan     0.2560   -0.0000
   880        0.0000             nan     0.2560   -0.0000
   900        0.0000             nan     0.2560   -0.0000
   920        0.0000             nan     0.2560   -0.0000
   940        0.0000             nan     0.2560   -0.0000
   960        0.0000             nan     0.2560   -0.0000
   980        0.0000             nan     0.2560   -0.0000
  1000        0.0000             nan     0.2560   -0.0000
  1020        0.0000             nan     0.2560   -0.0000
  1040        0.0000             nan     0.2560   -0.0000
  1060        0.0000             nan     0.2560   -0.0000
  1080        0.0000             nan     0.2560   -0.0000
  1100        0.0000             nan     0.2560   -0.0000
  1120        0.0000             nan     0.2560   -0.0000
  1140        0.0000             nan     0.2560   -0.0000
  1160        0.0000             nan     0.2560   -0.0000
  1180        0.0000             nan     0.2560   -0.0000
  1200        0.0000             nan     0.2560   -0.0000
  1220        0.0000             nan     0.2560   -0.0000
  1240        0.0000             nan     0.2560   -0.0000
  1260        0.0000             nan     0.2560   -0.0000
  1280        0.0000             nan     0.2560   -0.0000
  1300        0.0000             nan     0.2560   -0.0000
  1320        0.0000             nan     0.2560   -0.0000
  1340        0.0000             nan     0.2560   -0.0000
  1360        0.0000             nan     0.2560   -0.0000
  1380        0.0000             nan     0.2560   -0.0000
  1400        0.0000             nan     0.2560   -0.0000
  1420        0.0000             nan     0.2560   -0.0000
  1440        0.0000             nan     0.2560   -0.0000
  1460        0.0000             nan     0.2560   -0.0000
  1480        0.0000             nan     0.2560   -0.0000
  1500        0.0000             nan     0.2560   -0.0000
  1520        0.0000             nan     0.2560   -0.0000
  1540        0.0000             nan     0.2560   -0.0000
  1560        0.0000             nan     0.2560   -0.0000
  1580        0.0000             nan     0.2560   -0.0000
  1600        0.0000             nan     0.2560   -0.0000
  1620        0.0000             nan     0.2560   -0.0000
  1640        0.0000             nan     0.2560   -0.0000
  1660        0.0000             nan     0.2560   -0.0000
  1680        0.0000             nan     0.2560   -0.0000
  1700        0.0000             nan     0.2560   -0.0000
  1720        0.0000             nan     0.2560   -0.0000
  1740        0.0000             nan     0.2560   -0.0000
  1760        0.0000             nan     0.2560   -0.0000
  1780        0.0000             nan     0.2560   -0.0000
  1800        0.0000             nan     0.2560   -0.0000
  1820        0.0000             nan     0.2560   -0.0000
  1840        0.0000             nan     0.2560   -0.0000
  1860        0.0000             nan     0.2560   -0.0000
  1880        0.0000             nan     0.2560   -0.0000
  1900        0.0000             nan     0.2560   -0.0000
  1920        0.0000             nan     0.2560   -0.0000
  1940        0.0000             nan     0.2560   -0.0000
  1960        0.0000             nan     0.2560   -0.0000
  1980        0.0000             nan     0.2560   -0.0000
  2000        0.0000             nan     0.2560   -0.0000
  2020        0.0000             nan     0.2560   -0.0000
  2040        0.0000             nan     0.2560   -0.0000
  2060        0.0000             nan     0.2560   -0.0000
  2080        0.0000             nan     0.2560   -0.0000
  2100        0.0000             nan     0.2560   -0.0000
  2120        0.0000             nan     0.2560   -0.0000
  2140        0.0000             nan     0.2560   -0.0000
  2160        0.0000             nan     0.2560   -0.0000
  2180        0.0000             nan     0.2560   -0.0000
  2200        0.0000             nan     0.2560   -0.0000
  2220        0.0000             nan     0.2560   -0.0000
  2240        0.0000             nan     0.2560   -0.0000
  2260        0.0000             nan     0.2560   -0.0000
  2280        0.0000             nan     0.2560   -0.0000
  2300        0.0000             nan     0.2560   -0.0000
  2320        0.0000             nan     0.2560   -0.0000
  2340        0.0000             nan     0.2560   -0.0000
  2360        0.0000             nan     0.2560   -0.0000
  2380        0.0000             nan     0.2560   -0.0000
  2400        0.0000             nan     0.2560   -0.0000
  2420        0.0000             nan     0.2560   -0.0000
  2440        0.0000             nan     0.2560   -0.0000
  2460        0.0000             nan     0.2560   -0.0000
  2480        0.0000             nan     0.2560   -0.0000
  2500        0.0000             nan     0.2560   -0.0000
  2520        0.0000             nan     0.2560   -0.0000
  2540        0.0000             nan     0.2560   -0.0000
  2560        0.0000             nan     0.2560   -0.0000
  2580        0.0000             nan     0.2560   -0.0000
  2600        0.0000             nan     0.2560   -0.0000
  2620        0.0000             nan     0.2560   -0.0000
  2640        0.0000             nan     0.2560   -0.0000
  2660        0.0000             nan     0.2560   -0.0000
  2680        0.0000             nan     0.2560   -0.0000
  2700        0.0000             nan     0.2560   -0.0000
  2720        0.0000             nan     0.2560   -0.0000
  2740        0.0000             nan     0.2560   -0.0000
  2760        0.0000             nan     0.2560   -0.0000
  2780        0.0000             nan     0.2560    0.0000
  2800        0.0000             nan     0.2560   -0.0000
  2820        0.0000             nan     0.2560    0.0000
  2840        0.0000             nan     0.2560   -0.0000
  2860        0.0000             nan     0.2560    0.0000
  2880        0.0000             nan     0.2560    0.0000
  2900        0.0000             nan     0.2560   -0.0000
  2920        0.0000             nan     0.2560   -0.0000
  2940        0.0000             nan     0.2560   -0.0000
  2960        0.0000             nan     0.2560   -0.0000
  2980        0.0000             nan     0.2560   -0.0000
  3000        0.0000             nan     0.2560   -0.0000
  3020        0.0000             nan     0.2560   -0.0000
  3040        0.0000             nan     0.2560   -0.0000
  3060        0.0000             nan     0.2560   -0.0000
  3080        0.0000             nan     0.2560    0.0000
  3100        0.0000             nan     0.2560    0.0000
  3120        0.0000             nan     0.2560   -0.0000
  3140        0.0000             nan     0.2560   -0.0000
  3160        0.0000             nan     0.2560   -0.0000
  3180        0.0000             nan     0.2560   -0.0000
  3200        0.0000             nan     0.2560   -0.0000
  3220        0.0000             nan     0.2560   -0.0000
  3240        0.0000             nan     0.2560   -0.0000
  3260        0.0000             nan     0.2560    0.0000
  3280        0.0000             nan     0.2560   -0.0000
  3300        0.0000             nan     0.2560   -0.0000
  3320        0.0000             nan     0.2560   -0.0000
  3340        0.0000             nan     0.2560   -0.0000
  3360        0.0000             nan     0.2560   -0.0000
  3380        0.0000             nan     0.2560   -0.0000
  3400        0.0000             nan     0.2560   -0.0000
  3420        0.0000             nan     0.2560   -0.0000
  3440        0.0000             nan     0.2560   -0.0000
  3460        0.0000             nan     0.2560   -0.0000
  3480        0.0000             nan     0.2560    0.0000
  3500        0.0000             nan     0.2560   -0.0000
  3520        0.0000             nan     0.2560   -0.0000
  3540        0.0000             nan     0.2560    0.0000
  3560        0.0000             nan     0.2560   -0.0000
  3580        0.0000             nan     0.2560   -0.0000
  3600        0.0000             nan     0.2560   -0.0000
  3620        0.0000             nan     0.2560   -0.0000
  3640        0.0000             nan     0.2560   -0.0000
  3660        0.0000             nan     0.2560   -0.0000
  3680        0.0000             nan     0.2560   -0.0000
  3700        0.0000             nan     0.2560    0.0000
  3720        0.0000             nan     0.2560   -0.0000
  3740        0.0000             nan     0.2560   -0.0000
  3760        0.0000             nan     0.2560   -0.0000
  3780        0.0000             nan     0.2560   -0.0000
  3800        0.0000             nan     0.2560   -0.0000
  3820        0.0000             nan     0.2560   -0.0000
  3840        0.0000             nan     0.2560   -0.0000
  3860        0.0000             nan     0.2560   -0.0000
  3880        0.0000             nan     0.2560   -0.0000
  3900        0.0000             nan     0.2560    0.0000
  3920        0.0000             nan     0.2560   -0.0000
  3940        0.0000             nan     0.2560   -0.0000
  3960        0.0000             nan     0.2560    0.0000
  3980        0.0000             nan     0.2560    0.0000
  4000        0.0000             nan     0.2560    0.0000
  4020        0.0000             nan     0.2560   -0.0000
  4040        0.0000             nan     0.2560   -0.0000
  4060        0.0000             nan     0.2560   -0.0000
  4062        0.0000             nan     0.2560   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0186             nan     0.3360    0.0201
     2        0.0092             nan     0.3360    0.0084
     3        0.0047             nan     0.3360    0.0042
     4        0.0026             nan     0.3360    0.0022
     5        0.0016             nan     0.3360    0.0011
     6        0.0010             nan     0.3360    0.0005
     7        0.0007             nan     0.3360    0.0003
     8        0.0006             nan     0.3360    0.0001
     9        0.0005             nan     0.3360    0.0001
    10        0.0004             nan     0.3360    0.0000
    20        0.0003             nan     0.3360    0.0000
    40        0.0002             nan     0.3360   -0.0000
    60        0.0001             nan     0.3360   -0.0000
    80        0.0001             nan     0.3360    0.0000
   100        0.0001             nan     0.3360   -0.0000
   120        0.0001             nan     0.3360   -0.0000
   140        0.0001             nan     0.3360   -0.0000
   160        0.0001             nan     0.3360   -0.0000
   180        0.0001             nan     0.3360   -0.0000
   200        0.0001             nan     0.3360   -0.0000
   220        0.0000             nan     0.3360   -0.0000
   240        0.0000             nan     0.3360   -0.0000
   260        0.0000             nan     0.3360   -0.0000
   280        0.0000             nan     0.3360   -0.0000
   300        0.0000             nan     0.3360   -0.0000
   320        0.0000             nan     0.3360   -0.0000
   340        0.0000             nan     0.3360   -0.0000
   360        0.0000             nan     0.3360   -0.0000
   380        0.0000             nan     0.3360   -0.0000
   400        0.0000             nan     0.3360   -0.0000
   420        0.0000             nan     0.3360   -0.0000
   440        0.0000             nan     0.3360   -0.0000
   460        0.0000             nan     0.3360   -0.0000
   480        0.0000             nan     0.3360   -0.0000
   500        0.0000             nan     0.3360   -0.0000
   520        0.0000             nan     0.3360   -0.0000
   540        0.0000             nan     0.3360   -0.0000
   560        0.0000             nan     0.3360   -0.0000
   580        0.0000             nan     0.3360   -0.0000
   600        0.0000             nan     0.3360   -0.0000
   620        0.0000             nan     0.3360   -0.0000
   640        0.0000             nan     0.3360   -0.0000
   660        0.0000             nan     0.3360   -0.0000
   680        0.0000             nan     0.3360   -0.0000
   700        0.0000             nan     0.3360   -0.0000
   720        0.0000             nan     0.3360   -0.0000
   740        0.0000             nan     0.3360   -0.0000
   760        0.0000             nan     0.3360   -0.0000
   780        0.0000             nan     0.3360   -0.0000
   800        0.0000             nan     0.3360   -0.0000
   820        0.0000             nan     0.3360   -0.0000
   840        0.0000             nan     0.3360   -0.0000
   860        0.0000             nan     0.3360    0.0000
   880        0.0000             nan     0.3360   -0.0000
   900        0.0000             nan     0.3360   -0.0000
   920        0.0000             nan     0.3360   -0.0000
   940        0.0000             nan     0.3360   -0.0000
   960        0.0000             nan     0.3360   -0.0000
   980        0.0000             nan     0.3360   -0.0000
  1000        0.0000             nan     0.3360   -0.0000
  1020        0.0000             nan     0.3360    0.0000
  1040        0.0000             nan     0.3360   -0.0000
  1060        0.0000             nan     0.3360   -0.0000
  1080        0.0000             nan     0.3360   -0.0000
  1100        0.0000             nan     0.3360   -0.0000
  1120        0.0000             nan     0.3360   -0.0000
  1140        0.0000             nan     0.3360   -0.0000
  1160        0.0000             nan     0.3360   -0.0000
  1180        0.0000             nan     0.3360   -0.0000
  1200        0.0000             nan     0.3360   -0.0000
  1220        0.0000             nan     0.3360    0.0000
  1240        0.0000             nan     0.3360   -0.0000
  1260        0.0000             nan     0.3360    0.0000
  1280        0.0000             nan     0.3360   -0.0000
  1300        0.0000             nan     0.3360   -0.0000
  1320        0.0000             nan     0.3360   -0.0000
  1340        0.0000             nan     0.3360   -0.0000
  1360        0.0000             nan     0.3360   -0.0000
  1380        0.0000             nan     0.3360   -0.0000
  1400        0.0000             nan     0.3360   -0.0000
  1420        0.0000             nan     0.3360   -0.0000
  1440        0.0000             nan     0.3360   -0.0000
  1460        0.0000             nan     0.3360   -0.0000
  1480        0.0000             nan     0.3360   -0.0000
  1500        0.0000             nan     0.3360   -0.0000
  1520        0.0000             nan     0.3360   -0.0000
  1540        0.0000             nan     0.3360   -0.0000
  1560        0.0000             nan     0.3360   -0.0000
  1580        0.0000             nan     0.3360    0.0000
  1600        0.0000             nan     0.3360    0.0000
  1620        0.0000             nan     0.3360    0.0000
  1640        0.0000             nan     0.3360   -0.0000
  1660        0.0000             nan     0.3360   -0.0000
  1680        0.0000             nan     0.3360   -0.0000
  1700        0.0000             nan     0.3360   -0.0000
  1720        0.0000             nan     0.3360   -0.0000
  1740        0.0000             nan     0.3360    0.0000
  1760        0.0000             nan     0.3360   -0.0000
  1780        0.0000             nan     0.3360   -0.0000
  1800        0.0000             nan     0.3360   -0.0000
  1820        0.0000             nan     0.3360    0.0000
  1840        0.0000             nan     0.3360    0.0000
  1860        0.0000             nan     0.3360   -0.0000
  1880        0.0000             nan     0.3360   -0.0000
  1900        0.0000             nan     0.3360   -0.0000
  1920        0.0000             nan     0.3360   -0.0000
  1940        0.0000             nan     0.3360   -0.0000
  1960        0.0000             nan     0.3360   -0.0000
  1980        0.0000             nan     0.3360   -0.0000
  2000        0.0000             nan     0.3360   -0.0000
  2020        0.0000             nan     0.3360    0.0000
  2040        0.0000             nan     0.3360   -0.0000
  2060        0.0000             nan     0.3360   -0.0000
  2080        0.0000             nan     0.3360   -0.0000
  2100        0.0000             nan     0.3360   -0.0000
  2120        0.0000             nan     0.3360   -0.0000
  2140        0.0000             nan     0.3360   -0.0000
  2160        0.0000             nan     0.3360   -0.0000
  2180        0.0000             nan     0.3360   -0.0000
  2200        0.0000             nan     0.3360    0.0000
  2220        0.0000             nan     0.3360   -0.0000
  2240        0.0000             nan     0.3360   -0.0000
  2260        0.0000             nan     0.3360   -0.0000
  2280        0.0000             nan     0.3360   -0.0000
  2300        0.0000             nan     0.3360   -0.0000
  2320        0.0000             nan     0.3360   -0.0000
  2340        0.0000             nan     0.3360    0.0000
  2360        0.0000             nan     0.3360    0.0000
  2380        0.0000             nan     0.3360   -0.0000
  2400        0.0000             nan     0.3360   -0.0000
  2420        0.0000             nan     0.3360   -0.0000
  2440        0.0000             nan     0.3360   -0.0000
  2460        0.0000             nan     0.3360   -0.0000
  2480        0.0000             nan     0.3360   -0.0000
  2500        0.0000             nan     0.3360   -0.0000
  2520        0.0000             nan     0.3360   -0.0000
  2540        0.0000             nan     0.3360   -0.0000
  2560        0.0000             nan     0.3360   -0.0000
  2580        0.0000             nan     0.3360   -0.0000
  2600        0.0000             nan     0.3360   -0.0000
  2620        0.0000             nan     0.3360   -0.0000
  2640        0.0000             nan     0.3360    0.0000
  2660        0.0000             nan     0.3360   -0.0000
  2680        0.0000             nan     0.3360   -0.0000
  2700        0.0000             nan     0.3360    0.0000
  2720        0.0000             nan     0.3360   -0.0000
  2740        0.0000             nan     0.3360   -0.0000
  2760        0.0000             nan     0.3360   -0.0000
  2780        0.0000             nan     0.3360   -0.0000
  2800        0.0000             nan     0.3360    0.0000
  2820        0.0000             nan     0.3360    0.0000
  2840        0.0000             nan     0.3360   -0.0000
  2860        0.0000             nan     0.3360    0.0000
  2875        0.0000             nan     0.3360   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0144             nan     0.4093    0.0255
     2        0.0055             nan     0.4093    0.0083
     3        0.0022             nan     0.4093    0.0032
     4        0.0010             nan     0.4093    0.0012
     5        0.0005             nan     0.4093    0.0005
     6        0.0003             nan     0.4093    0.0002
     7        0.0002             nan     0.4093    0.0001
     8        0.0002             nan     0.4093    0.0000
     9        0.0002             nan     0.4093    0.0000
    10        0.0001             nan     0.4093   -0.0000
    20        0.0001             nan     0.4093   -0.0000
    40        0.0000             nan     0.4093   -0.0000
    60        0.0000             nan     0.4093   -0.0000
    80        0.0000             nan     0.4093   -0.0000
   100        0.0000             nan     0.4093   -0.0000
   120        0.0000             nan     0.4093   -0.0000
   140        0.0000             nan     0.4093   -0.0000
   160        0.0000             nan     0.4093   -0.0000
   180        0.0000             nan     0.4093   -0.0000
   200        0.0000             nan     0.4093   -0.0000
   220        0.0000             nan     0.4093   -0.0000
   240        0.0000             nan     0.4093   -0.0000
   260        0.0000             nan     0.4093   -0.0000
   280        0.0000             nan     0.4093   -0.0000
   300        0.0000             nan     0.4093   -0.0000
   320        0.0000             nan     0.4093   -0.0000
   340        0.0000             nan     0.4093   -0.0000
   360        0.0000             nan     0.4093   -0.0000
   380        0.0000             nan     0.4093   -0.0000
   400        0.0000             nan     0.4093   -0.0000
   420        0.0000             nan     0.4093   -0.0000
   440        0.0000             nan     0.4093   -0.0000
   460        0.0000             nan     0.4093   -0.0000
   480        0.0000             nan     0.4093   -0.0000
   500        0.0000             nan     0.4093   -0.0000
   520        0.0000             nan     0.4093   -0.0000
   540        0.0000             nan     0.4093   -0.0000
   560        0.0000             nan     0.4093   -0.0000
   580        0.0000             nan     0.4093   -0.0000
   600        0.0000             nan     0.4093   -0.0000
   620        0.0000             nan     0.4093   -0.0000
   640        0.0000             nan     0.4093   -0.0000
   660        0.0000             nan     0.4093   -0.0000
   680        0.0000             nan     0.4093   -0.0000
   700        0.0000             nan     0.4093   -0.0000
   720        0.0000             nan     0.4093   -0.0000
   740        0.0000             nan     0.4093   -0.0000
   760        0.0000             nan     0.4093   -0.0000
   780        0.0000             nan     0.4093   -0.0000
   800        0.0000             nan     0.4093   -0.0000
   820        0.0000             nan     0.4093   -0.0000
   840        0.0000             nan     0.4093   -0.0000
   860        0.0000             nan     0.4093   -0.0000
   880        0.0000             nan     0.4093   -0.0000
   900        0.0000             nan     0.4093   -0.0000
   920        0.0000             nan     0.4093    0.0000
   940        0.0000             nan     0.4093   -0.0000
   960        0.0000             nan     0.4093   -0.0000
   980        0.0000             nan     0.4093   -0.0000
  1000        0.0000             nan     0.4093    0.0000
  1020        0.0000             nan     0.4093   -0.0000
  1040        0.0000             nan     0.4093   -0.0000
  1060        0.0000             nan     0.4093    0.0000
  1080        0.0000             nan     0.4093    0.0000
  1100        0.0000             nan     0.4093    0.0000
  1120        0.0000             nan     0.4093   -0.0000
  1140        0.0000             nan     0.4093   -0.0000
  1160        0.0000             nan     0.4093   -0.0000
  1180        0.0000             nan     0.4093   -0.0000
  1200        0.0000             nan     0.4093   -0.0000
  1220        0.0000             nan     0.4093   -0.0000
  1240        0.0000             nan     0.4093   -0.0000
  1260        0.0000             nan     0.4093   -0.0000
  1280        0.0000             nan     0.4093    0.0000
  1300        0.0000             nan     0.4093   -0.0000
  1320        0.0000             nan     0.4093    0.0000
  1340        0.0000             nan     0.4093    0.0000
  1360        0.0000             nan     0.4093   -0.0000
  1380        0.0000             nan     0.4093   -0.0000
  1400        0.0000             nan     0.4093    0.0000
  1420        0.0000             nan     0.4093   -0.0000
  1440        0.0000             nan     0.4093    0.0000
  1460        0.0000             nan     0.4093   -0.0000
  1480        0.0000             nan     0.4093   -0.0000
  1500        0.0000             nan     0.4093   -0.0000
  1520        0.0000             nan     0.4093   -0.0000
  1540        0.0000             nan     0.4093   -0.0000
  1560        0.0000             nan     0.4093   -0.0000
  1580        0.0000             nan     0.4093   -0.0000
  1600        0.0000             nan     0.4093    0.0000
  1620        0.0000             nan     0.4093   -0.0000
  1640        0.0000             nan     0.4093   -0.0000
  1660        0.0000             nan     0.4093   -0.0000
  1680        0.0000             nan     0.4093   -0.0000
  1700        0.0000             nan     0.4093   -0.0000
  1720        0.0000             nan     0.4093   -0.0000
  1740        0.0000             nan     0.4093    0.0000
  1760        0.0000             nan     0.4093   -0.0000
  1780        0.0000             nan     0.4093   -0.0000
  1800        0.0000             nan     0.4093   -0.0000
  1820        0.0000             nan     0.4093    0.0000
  1840        0.0000             nan     0.4093   -0.0000
  1860        0.0000             nan     0.4093   -0.0000
  1880        0.0000             nan     0.4093   -0.0000
  1900        0.0000             nan     0.4093   -0.0000
  1920        0.0000             nan     0.4093   -0.0000
  1940        0.0000             nan     0.4093   -0.0000
  1960        0.0000             nan     0.4093   -0.0000
  1980        0.0000             nan     0.4093   -0.0000
  2000        0.0000             nan     0.4093   -0.0000
  2020        0.0000             nan     0.4093   -0.0000
  2040        0.0000             nan     0.4093   -0.0000
  2060        0.0000             nan     0.4093   -0.0000
  2080        0.0000             nan     0.4093   -0.0000
  2100        0.0000             nan     0.4093   -0.0000
  2120        0.0000             nan     0.4093    0.0000
  2140        0.0000             nan     0.4093   -0.0000
  2160        0.0000             nan     0.4093   -0.0000
  2180        0.0000             nan     0.4093   -0.0000
  2200        0.0000             nan     0.4093    0.0000
  2220        0.0000             nan     0.4093    0.0000
  2240        0.0000             nan     0.4093   -0.0000
  2260        0.0000             nan     0.4093    0.0000
  2280        0.0000             nan     0.4093   -0.0000
  2300        0.0000             nan     0.4093   -0.0000
  2320        0.0000             nan     0.4093   -0.0000
  2340        0.0000             nan     0.4093   -0.0000
  2360        0.0000             nan     0.4093   -0.0000
  2380        0.0000             nan     0.4093   -0.0000
  2400        0.0000             nan     0.4093   -0.0000
  2420        0.0000             nan     0.4093    0.0000
  2440        0.0000             nan     0.4093   -0.0000
  2460        0.0000             nan     0.4093   -0.0000
  2480        0.0000             nan     0.4093   -0.0000
  2500        0.0000             nan     0.4093    0.0000
  2520        0.0000             nan     0.4093   -0.0000
  2540        0.0000             nan     0.4093   -0.0000
  2560        0.0000             nan     0.4093   -0.0000
  2580        0.0000             nan     0.4093   -0.0000
  2600        0.0000             nan     0.4093   -0.0000
  2620        0.0000             nan     0.4093   -0.0000
  2640        0.0000             nan     0.4093   -0.0000
  2660        0.0000             nan     0.4093   -0.0000
  2680        0.0000             nan     0.4093   -0.0000
  2700        0.0000             nan     0.4093   -0.0000
  2720        0.0000             nan     0.4093    0.0000
  2740        0.0000             nan     0.4093   -0.0000
  2760        0.0000             nan     0.4093   -0.0000
  2780        0.0000             nan     0.4093   -0.0000
  2800        0.0000             nan     0.4093    0.0000
  2820        0.0000             nan     0.4093   -0.0000
  2840        0.0000             nan     0.4093   -0.0000
  2860        0.0000             nan     0.4093   -0.0000
  2880        0.0000             nan     0.4093   -0.0000
  2900        0.0000             nan     0.4093   -0.0000
  2920        0.0000             nan     0.4093   -0.0000
  2940        0.0000             nan     0.4093   -0.0000
  2960        0.0000             nan     0.4093    0.0000
  2980        0.0000             nan     0.4093   -0.0000
  3000        0.0000             nan     0.4093   -0.0000
  3020        0.0000             nan     0.4093   -0.0000
  3040        0.0000             nan     0.4093   -0.0000
  3060        0.0000             nan     0.4093    0.0000
  3080        0.0000             nan     0.4093   -0.0000
  3100        0.0000             nan     0.4093   -0.0000
  3120        0.0000             nan     0.4093   -0.0000
  3140        0.0000             nan     0.4093   -0.0000
  3159        0.0000             nan     0.4093   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0141             nan     0.4257    0.0215
     2        0.0052             nan     0.4257    0.0082
     3        0.0022             nan     0.4257    0.0030
     4        0.0010             nan     0.4257    0.0012
     5        0.0005             nan     0.4257    0.0004
     6        0.0003             nan     0.4257    0.0001
     7        0.0003             nan     0.4257    0.0001
     8        0.0002             nan     0.4257    0.0000
     9        0.0002             nan     0.4257    0.0000
    10        0.0002             nan     0.4257    0.0000
    20        0.0001             nan     0.4257   -0.0000
    40        0.0000             nan     0.4257   -0.0000
    60        0.0000             nan     0.4257   -0.0000
    80        0.0000             nan     0.4257   -0.0000
   100        0.0000             nan     0.4257   -0.0000
   120        0.0000             nan     0.4257   -0.0000
   140        0.0000             nan     0.4257   -0.0000
   160        0.0000             nan     0.4257   -0.0000
   180        0.0000             nan     0.4257   -0.0000
   200        0.0000             nan     0.4257   -0.0000
   220        0.0000             nan     0.4257   -0.0000
   240        0.0000             nan     0.4257   -0.0000
   260        0.0000             nan     0.4257   -0.0000
   280        0.0000             nan     0.4257   -0.0000
   300        0.0000             nan     0.4257   -0.0000
   320        0.0000             nan     0.4257   -0.0000
   340        0.0000             nan     0.4257   -0.0000
   360        0.0000             nan     0.4257   -0.0000
   380        0.0000             nan     0.4257   -0.0000
   400        0.0000             nan     0.4257   -0.0000
   420        0.0000             nan     0.4257   -0.0000
   440        0.0000             nan     0.4257   -0.0000
   460        0.0000             nan     0.4257   -0.0000
   480        0.0000             nan     0.4257   -0.0000
   500        0.0000             nan     0.4257   -0.0000
   520        0.0000             nan     0.4257   -0.0000
   540        0.0000             nan     0.4257   -0.0000
   560        0.0000             nan     0.4257   -0.0000
   580        0.0000             nan     0.4257   -0.0000
   600        0.0000             nan     0.4257    0.0000
   620        0.0000             nan     0.4257   -0.0000
   640        0.0000             nan     0.4257   -0.0000
   660        0.0000             nan     0.4257   -0.0000
   680        0.0000             nan     0.4257   -0.0000
   700        0.0000             nan     0.4257   -0.0000
   720        0.0000             nan     0.4257   -0.0000
   740        0.0000             nan     0.4257    0.0000
   760        0.0000             nan     0.4257    0.0000
   780        0.0000             nan     0.4257    0.0000
   800        0.0000             nan     0.4257   -0.0000
   820        0.0000             nan     0.4257   -0.0000
   840        0.0000             nan     0.4257   -0.0000
   860        0.0000             nan     0.4257   -0.0000
   880        0.0000             nan     0.4257   -0.0000
   900        0.0000             nan     0.4257   -0.0000
   920        0.0000             nan     0.4257   -0.0000
   940        0.0000             nan     0.4257   -0.0000
   960        0.0000             nan     0.4257    0.0000
   980        0.0000             nan     0.4257    0.0000
  1000        0.0000             nan     0.4257   -0.0000
  1020        0.0000             nan     0.4257    0.0000
  1040        0.0000             nan     0.4257   -0.0000
  1060        0.0000             nan     0.4257    0.0000
  1080        0.0000             nan     0.4257   -0.0000
  1100        0.0000             nan     0.4257    0.0000
  1120        0.0000             nan     0.4257    0.0000
  1140        0.0000             nan     0.4257   -0.0000
  1160        0.0000             nan     0.4257   -0.0000
  1180        0.0000             nan     0.4257    0.0000
  1200        0.0000             nan     0.4257   -0.0000
  1220        0.0000             nan     0.4257   -0.0000
  1240        0.0000             nan     0.4257   -0.0000
  1260        0.0000             nan     0.4257    0.0000
  1280        0.0000             nan     0.4257    0.0000
  1300        0.0000             nan     0.4257   -0.0000
  1320        0.0000             nan     0.4257   -0.0000
  1340        0.0000             nan     0.4257   -0.0000
  1360        0.0000             nan     0.4257   -0.0000
  1380        0.0000             nan     0.4257   -0.0000
  1400        0.0000             nan     0.4257   -0.0000
  1420        0.0000             nan     0.4257    0.0000
  1440        0.0000             nan     0.4257    0.0000
  1460        0.0000             nan     0.4257    0.0000
  1480        0.0000             nan     0.4257   -0.0000
  1500        0.0000             nan     0.4257    0.0000
  1520        0.0000             nan     0.4257   -0.0000
  1540        0.0000             nan     0.4257   -0.0000
  1560        0.0000             nan     0.4257   -0.0000
  1580        0.0000             nan     0.4257   -0.0000
  1600        0.0000             nan     0.4257   -0.0000
  1620        0.0000             nan     0.4257   -0.0000
  1640        0.0000             nan     0.4257   -0.0000
  1660        0.0000             nan     0.4257   -0.0000
  1680        0.0000             nan     0.4257   -0.0000
  1700        0.0000             nan     0.4257   -0.0000
  1720        0.0000             nan     0.4257   -0.0000
  1740        0.0000             nan     0.4257    0.0000
  1760        0.0000             nan     0.4257    0.0000
  1780        0.0000             nan     0.4257    0.0000
  1800        0.0000             nan     0.4257   -0.0000
  1818        0.0000             nan     0.4257   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0117             nan     0.4962    0.0258
     2        0.0038             nan     0.4962    0.0079
     3        0.0016             nan     0.4962    0.0022
     4        0.0009             nan     0.4962    0.0007
     5        0.0006             nan     0.4962    0.0003
     6        0.0005             nan     0.4962    0.0001
     7        0.0005             nan     0.4962    0.0000
     8        0.0004             nan     0.4962    0.0000
     9        0.0004             nan     0.4962   -0.0000
    10        0.0004             nan     0.4962   -0.0000
    20        0.0003             nan     0.4962   -0.0000
    40        0.0002             nan     0.4962   -0.0000
    60        0.0001             nan     0.4962   -0.0000
    80        0.0001             nan     0.4962   -0.0000
   100        0.0001             nan     0.4962   -0.0000
   120        0.0001             nan     0.4962   -0.0000
   140        0.0000             nan     0.4962   -0.0000
   160        0.0000             nan     0.4962   -0.0000
   180        0.0000             nan     0.4962   -0.0000
   200        0.0000             nan     0.4962   -0.0000
   220        0.0000             nan     0.4962   -0.0000
   240        0.0000             nan     0.4962   -0.0000
   260        0.0000             nan     0.4962   -0.0000
   280        0.0000             nan     0.4962   -0.0000
   300        0.0000             nan     0.4962   -0.0000
   320        0.0000             nan     0.4962   -0.0000
   340        0.0000             nan     0.4962   -0.0000
   360        0.0000             nan     0.4962   -0.0000
   380        0.0000             nan     0.4962   -0.0000
   400        0.0000             nan     0.4962   -0.0000
   420        0.0000             nan     0.4962   -0.0000
   440        0.0000             nan     0.4962   -0.0000
   460        0.0000             nan     0.4962   -0.0000
   480        0.0000             nan     0.4962   -0.0000
   500        0.0000             nan     0.4962   -0.0000
   520        0.0000             nan     0.4962   -0.0000
   540        0.0000             nan     0.4962   -0.0000
   560        0.0000             nan     0.4962   -0.0000
   580        0.0000             nan     0.4962   -0.0000
   600        0.0000             nan     0.4962   -0.0000
   620        0.0000             nan     0.4962   -0.0000
   640        0.0000             nan     0.4962   -0.0000
   660        0.0000             nan     0.4962   -0.0000
   680        0.0000             nan     0.4962   -0.0000
   700        0.0000             nan     0.4962   -0.0000
   720        0.0000             nan     0.4962   -0.0000
   740        0.0000             nan     0.4962   -0.0000
   760        0.0000             nan     0.4962   -0.0000
   780        0.0000             nan     0.4962   -0.0000
   800        0.0000             nan     0.4962   -0.0000
   820        0.0000             nan     0.4962   -0.0000
   840        0.0000             nan     0.4962   -0.0000
   860        0.0000             nan     0.4962   -0.0000
   880        0.0000             nan     0.4962    0.0000
   900        0.0000             nan     0.4962   -0.0000
   920        0.0000             nan     0.4962   -0.0000
   940        0.0000             nan     0.4962   -0.0000
   960        0.0000             nan     0.4962   -0.0000
   980        0.0000             nan     0.4962   -0.0000
  1000        0.0000             nan     0.4962   -0.0000
  1020        0.0000             nan     0.4962    0.0000
  1040        0.0000             nan     0.4962    0.0000
  1060        0.0000             nan     0.4962    0.0000
  1080        0.0000             nan     0.4962   -0.0000
  1100        0.0000             nan     0.4962   -0.0000
  1120        0.0000             nan     0.4962   -0.0000
  1140        0.0000             nan     0.4962    0.0000
  1160        0.0000             nan     0.4962    0.0000
  1180        0.0000             nan     0.4962   -0.0000
  1200        0.0000             nan     0.4962   -0.0000
  1220        0.0000             nan     0.4962   -0.0000
  1240        0.0000             nan     0.4962   -0.0000
  1260        0.0000             nan     0.4962   -0.0000
  1280        0.0000             nan     0.4962   -0.0000
  1300        0.0000             nan     0.4962   -0.0000
  1320        0.0000             nan     0.4962   -0.0000
  1340        0.0000             nan     0.4962   -0.0000
  1360        0.0000             nan     0.4962   -0.0000
  1380        0.0000             nan     0.4962   -0.0000
  1400        0.0000             nan     0.4962   -0.0000
  1420        0.0000             nan     0.4962   -0.0000
  1440        0.0000             nan     0.4962   -0.0000
  1460        0.0000             nan     0.4962    0.0000
  1480        0.0000             nan     0.4962   -0.0000
  1500        0.0000             nan     0.4962   -0.0000
  1520        0.0000             nan     0.4962    0.0000
  1540        0.0000             nan     0.4962   -0.0000
  1560        0.0000             nan     0.4962   -0.0000
  1580        0.0000             nan     0.4962    0.0000
  1600        0.0000             nan     0.4962   -0.0000
  1620        0.0000             nan     0.4962   -0.0000
  1640        0.0000             nan     0.4962    0.0000
  1660        0.0000             nan     0.4962   -0.0000
  1680        0.0000             nan     0.4962   -0.0000
  1700        0.0000             nan     0.4962    0.0000
  1720        0.0000             nan     0.4962   -0.0000
  1740        0.0000             nan     0.4962   -0.0000
  1760        0.0000             nan     0.4962   -0.0000
  1780        0.0000             nan     0.4962   -0.0000
  1800        0.0000             nan     0.4962   -0.0000
  1820        0.0000             nan     0.4962   -0.0000
  1840        0.0000             nan     0.4962   -0.0000
  1860        0.0000             nan     0.4962   -0.0000
  1880        0.0000             nan     0.4962   -0.0000
  1892        0.0000             nan     0.4962   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0129             nan     0.5090    0.0281
     2        0.0048             nan     0.5090    0.0088
     3        0.0020             nan     0.5090    0.0025
     4        0.0011             nan     0.5090    0.0008
     5        0.0008             nan     0.5090    0.0002
     6        0.0007             nan     0.5090    0.0001
     7        0.0006             nan     0.5090    0.0000
     8        0.0005             nan     0.5090    0.0001
     9        0.0005             nan     0.5090    0.0000
    10        0.0005             nan     0.5090    0.0000
    20        0.0003             nan     0.5090    0.0000
    40        0.0002             nan     0.5090   -0.0000
    60        0.0001             nan     0.5090   -0.0000
    80        0.0001             nan     0.5090   -0.0000
   100        0.0001             nan     0.5090   -0.0000
   120        0.0001             nan     0.5090   -0.0000
   140        0.0000             nan     0.5090   -0.0000
   160        0.0000             nan     0.5090   -0.0000
   180        0.0000             nan     0.5090   -0.0000
   200        0.0000             nan     0.5090   -0.0000
   220        0.0000             nan     0.5090   -0.0000
   240        0.0000             nan     0.5090   -0.0000
   260        0.0000             nan     0.5090   -0.0000
   280        0.0000             nan     0.5090   -0.0000
   300        0.0000             nan     0.5090   -0.0000
   320        0.0000             nan     0.5090   -0.0000
   340        0.0000             nan     0.5090   -0.0000
   360        0.0000             nan     0.5090   -0.0000
   380        0.0000             nan     0.5090   -0.0000
   400        0.0000             nan     0.5090   -0.0000
   420        0.0000             nan     0.5090   -0.0000
   440        0.0000             nan     0.5090   -0.0000
   460        0.0000             nan     0.5090   -0.0000
   480        0.0000             nan     0.5090   -0.0000
   500        0.0000             nan     0.5090   -0.0000
   520        0.0000             nan     0.5090   -0.0000
   540        0.0000             nan     0.5090   -0.0000
   560        0.0000             nan     0.5090   -0.0000
   580        0.0000             nan     0.5090    0.0000
   600        0.0000             nan     0.5090   -0.0000
   620        0.0000             nan     0.5090   -0.0000
   640        0.0000             nan     0.5090   -0.0000
   660        0.0000             nan     0.5090   -0.0000
   680        0.0000             nan     0.5090   -0.0000
   700        0.0000             nan     0.5090    0.0000
   720        0.0000             nan     0.5090   -0.0000
   740        0.0000             nan     0.5090   -0.0000
   760        0.0000             nan     0.5090   -0.0000
   780        0.0000             nan     0.5090    0.0000
   800        0.0000             nan     0.5090   -0.0000
   820        0.0000             nan     0.5090   -0.0000
   840        0.0000             nan     0.5090   -0.0000
   860        0.0000             nan     0.5090   -0.0000
   880        0.0000             nan     0.5090   -0.0000
   900        0.0000             nan     0.5090   -0.0000
   920        0.0000             nan     0.5090   -0.0000
   940        0.0000             nan     0.5090   -0.0000
   960        0.0000             nan     0.5090   -0.0000
   980        0.0000             nan     0.5090   -0.0000
  1000        0.0000             nan     0.5090    0.0000
  1020        0.0000             nan     0.5090   -0.0000
  1040        0.0000             nan     0.5090   -0.0000
  1060        0.0000             nan     0.5090   -0.0000
  1080        0.0000             nan     0.5090   -0.0000
  1100        0.0000             nan     0.5090   -0.0000
  1120        0.0000             nan     0.5090   -0.0000
  1140        0.0000             nan     0.5090   -0.0000
  1160        0.0000             nan     0.5090   -0.0000
  1180        0.0000             nan     0.5090   -0.0000
  1200        0.0000             nan     0.5090    0.0000
  1220        0.0000             nan     0.5090   -0.0000
  1240        0.0000             nan     0.5090   -0.0000
  1260        0.0000             nan     0.5090   -0.0000
  1280        0.0000             nan     0.5090    0.0000
  1300        0.0000             nan     0.5090    0.0000
  1320        0.0000             nan     0.5090   -0.0000
  1340        0.0000             nan     0.5090   -0.0000
  1360        0.0000             nan     0.5090   -0.0000
  1380        0.0000             nan     0.5090   -0.0000
  1400        0.0000             nan     0.5090   -0.0000
  1420        0.0000             nan     0.5090   -0.0000
  1440        0.0000             nan     0.5090   -0.0000
  1460        0.0000             nan     0.5090   -0.0000
  1480        0.0000             nan     0.5090    0.0000
  1500        0.0000             nan     0.5090   -0.0000
  1520        0.0000             nan     0.5090   -0.0000
  1540        0.0000             nan     0.5090   -0.0000
  1560        0.0000             nan     0.5090   -0.0000
  1580        0.0000             nan     0.5090    0.0000
  1600        0.0000             nan     0.5090    0.0000
  1620        0.0000             nan     0.5090   -0.0000
  1640        0.0000             nan     0.5090   -0.0000
  1660        0.0000             nan     0.5090   -0.0000
  1680        0.0000             nan     0.5090   -0.0000
  1700        0.0000             nan     0.5090   -0.0000
  1720        0.0000             nan     0.5090    0.0000
  1740        0.0000             nan     0.5090   -0.0000
  1760        0.0000             nan     0.5090   -0.0000
  1780        0.0000             nan     0.5090    0.0000
  1800        0.0000             nan     0.5090   -0.0000
  1820        0.0000             nan     0.5090   -0.0000
  1840        0.0000             nan     0.5090   -0.0000
  1860        0.0000             nan     0.5090   -0.0000
  1880        0.0000             nan     0.5090   -0.0000
  1900        0.0000             nan     0.5090   -0.0000
  1920        0.0000             nan     0.5090    0.0000
  1940        0.0000             nan     0.5090   -0.0000
  1960        0.0000             nan     0.5090    0.0000
  1980        0.0000             nan     0.5090    0.0000
  2000        0.0000             nan     0.5090   -0.0000
  2020        0.0000             nan     0.5090   -0.0000
  2040        0.0000             nan     0.5090   -0.0000
  2060        0.0000             nan     0.5090   -0.0000
  2080        0.0000             nan     0.5090   -0.0000
  2100        0.0000             nan     0.5090   -0.0000
  2120        0.0000             nan     0.5090   -0.0000
  2140        0.0000             nan     0.5090   -0.0000
  2160        0.0000             nan     0.5090    0.0000
  2180        0.0000             nan     0.5090    0.0000
  2200        0.0000             nan     0.5090   -0.0000
  2220        0.0000             nan     0.5090   -0.0000
  2240        0.0000             nan     0.5090   -0.0000
  2260        0.0000             nan     0.5090   -0.0000
  2280        0.0000             nan     0.5090   -0.0000
  2300        0.0000             nan     0.5090    0.0000
  2320        0.0000             nan     0.5090   -0.0000
  2340        0.0000             nan     0.5090   -0.0000
  2360        0.0000             nan     0.5090   -0.0000
  2380        0.0000             nan     0.5090   -0.0000
  2400        0.0000             nan     0.5090   -0.0000
  2420        0.0000             nan     0.5090   -0.0000
  2440        0.0000             nan     0.5090   -0.0000
  2460        0.0000             nan     0.5090   -0.0000
  2480        0.0000             nan     0.5090   -0.0000
  2500        0.0000             nan     0.5090   -0.0000
  2520        0.0000             nan     0.5090   -0.0000
  2540        0.0000             nan     0.5090   -0.0000
  2560        0.0000             nan     0.5090   -0.0000
  2580        0.0000             nan     0.5090   -0.0000
  2600        0.0000             nan     0.5090   -0.0000
  2620        0.0000             nan     0.5090   -0.0000
  2640        0.0000             nan     0.5090   -0.0000
  2660        0.0000             nan     0.5090   -0.0000
  2680        0.0000             nan     0.5090    0.0000
  2700        0.0000             nan     0.5090   -0.0000
  2720        0.0000             nan     0.5090    0.0000
  2740        0.0000             nan     0.5090    0.0000
  2760        0.0000             nan     0.5090    0.0000
  2780        0.0000             nan     0.5090    0.0000
  2800        0.0000             nan     0.5090   -0.0000
  2820        0.0000             nan     0.5090   -0.0000
  2840        0.0000             nan     0.5090   -0.0000
  2860        0.0000             nan     0.5090   -0.0000
  2880        0.0000             nan     0.5090   -0.0000
  2900        0.0000             nan     0.5090   -0.0000
  2920        0.0000             nan     0.5090   -0.0000
  2940        0.0000             nan     0.5090    0.0000
  2960        0.0000             nan     0.5090   -0.0000
  2980        0.0000             nan     0.5090   -0.0000
  3000        0.0000             nan     0.5090   -0.0000
  3020        0.0000             nan     0.5090   -0.0000
  3040        0.0000             nan     0.5090   -0.0000
  3060        0.0000             nan     0.5090   -0.0000
  3080        0.0000             nan     0.5090   -0.0000
  3100        0.0000             nan     0.5090   -0.0000
  3120        0.0000             nan     0.5090    0.0000
  3140        0.0000             nan     0.5090   -0.0000
  3160        0.0000             nan     0.5090   -0.0000
  3180        0.0000             nan     0.5090   -0.0000
  3200        0.0000             nan     0.5090   -0.0000
  3220        0.0000             nan     0.5090   -0.0000
  3240        0.0000             nan     0.5090   -0.0000
  3260        0.0000             nan     0.5090   -0.0000
  3280        0.0000             nan     0.5090   -0.0000
  3300        0.0000             nan     0.5090   -0.0000
  3320        0.0000             nan     0.5090   -0.0000
  3340        0.0000             nan     0.5090    0.0000
  3360        0.0000             nan     0.5090    0.0000
  3380        0.0000             nan     0.5090   -0.0000
  3400        0.0000             nan     0.5090    0.0000
  3420        0.0000             nan     0.5090   -0.0000
  3440        0.0000             nan     0.5090    0.0000
  3460        0.0000             nan     0.5090    0.0000
  3480        0.0000             nan     0.5090   -0.0000
  3500        0.0000             nan     0.5090   -0.0000
  3520        0.0000             nan     0.5090   -0.0000
  3540        0.0000             nan     0.5090   -0.0000
  3560        0.0000             nan     0.5090   -0.0000
  3580        0.0000             nan     0.5090   -0.0000
  3600        0.0000             nan     0.5090   -0.0000
  3620        0.0000             nan     0.5090   -0.0000
  3640        0.0000             nan     0.5090   -0.0000
  3660        0.0000             nan     0.5090   -0.0000
  3680        0.0000             nan     0.5090   -0.0000
  3700        0.0000             nan     0.5090   -0.0000
  3720        0.0000             nan     0.5090    0.0000
  3740        0.0000             nan     0.5090   -0.0000
  3760        0.0000             nan     0.5090    0.0000
  3780        0.0000             nan     0.5090   -0.0000
  3800        0.0000             nan     0.5090    0.0000
  3820        0.0000             nan     0.5090   -0.0000
  3840        0.0000             nan     0.5090    0.0000
  3860        0.0000             nan     0.5090    0.0000
  3880        0.0000             nan     0.5090   -0.0000
  3900        0.0000             nan     0.5090   -0.0000
  3920        0.0000             nan     0.5090   -0.0000
  3940        0.0000             nan     0.5090   -0.0000
  3960        0.0000             nan     0.5090    0.0000
  3980        0.0000             nan     0.5090   -0.0000
  4000        0.0000             nan     0.5090   -0.0000
  4020        0.0000             nan     0.5090   -0.0000
  4040        0.0000             nan     0.5090   -0.0000
  4060        0.0000             nan     0.5090   -0.0000
  4080        0.0000             nan     0.5090   -0.0000
  4100        0.0000             nan     0.5090   -0.0000
  4120        0.0000             nan     0.5090   -0.0000
  4140        0.0000             nan     0.5090   -0.0000
  4157        0.0000             nan     0.5090   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0142             nan     0.5702    0.0252
     2        0.0055             nan     0.5702    0.0080
     3        0.0029             nan     0.5702    0.0026
     4        0.0020             nan     0.5702    0.0010
     5        0.0017             nan     0.5702    0.0003
     6        0.0014             nan     0.5702    0.0002
     7        0.0012             nan     0.5702    0.0002
     8        0.0011             nan     0.5702    0.0001
     9        0.0010             nan     0.5702    0.0001
    10        0.0009             nan     0.5702    0.0001
    20        0.0005             nan     0.5702    0.0000
    40        0.0003             nan     0.5702    0.0000
    60        0.0002             nan     0.5702   -0.0000
    80        0.0002             nan     0.5702   -0.0000
   100        0.0001             nan     0.5702   -0.0000
   120        0.0001             nan     0.5702   -0.0000
   140        0.0001             nan     0.5702   -0.0000
   160        0.0001             nan     0.5702   -0.0000
   180        0.0001             nan     0.5702   -0.0000
   200        0.0001             nan     0.5702   -0.0000
   220        0.0001             nan     0.5702   -0.0000
   240        0.0001             nan     0.5702    0.0000
   260        0.0000             nan     0.5702   -0.0000
   280        0.0000             nan     0.5702   -0.0000
   300        0.0000             nan     0.5702   -0.0000
   320        0.0000             nan     0.5702   -0.0000
   340        0.0000             nan     0.5702   -0.0000
   360        0.0000             nan     0.5702   -0.0000
   380        0.0000             nan     0.5702   -0.0000
   400        0.0000             nan     0.5702   -0.0000
   420        0.0000             nan     0.5702   -0.0000
   440        0.0000             nan     0.5702   -0.0000
   460        0.0000             nan     0.5702   -0.0000
   480        0.0000             nan     0.5702   -0.0000
   500        0.0000             nan     0.5702   -0.0000
   520        0.0000             nan     0.5702   -0.0000
   540        0.0000             nan     0.5702   -0.0000
   560        0.0000             nan     0.5702   -0.0000
   580        0.0000             nan     0.5702   -0.0000
   600        0.0000             nan     0.5702   -0.0000
   620        0.0000             nan     0.5702   -0.0000
   640        0.0000             nan     0.5702   -0.0000
   660        0.0000             nan     0.5702   -0.0000
   680        0.0000             nan     0.5702   -0.0000
   700        0.0000             nan     0.5702   -0.0000
   720        0.0000             nan     0.5702   -0.0000
   740        0.0000             nan     0.5702   -0.0000
   760        0.0000             nan     0.5702   -0.0000
   780        0.0000             nan     0.5702   -0.0000
   800        0.0000             nan     0.5702   -0.0000
   820        0.0000             nan     0.5702   -0.0000
   840        0.0000             nan     0.5702   -0.0000
   860        0.0000             nan     0.5702   -0.0000
   880        0.0000             nan     0.5702   -0.0000
   900        0.0000             nan     0.5702   -0.0000
   920        0.0000             nan     0.5702   -0.0000
   940        0.0000             nan     0.5702   -0.0000
   960        0.0000             nan     0.5702   -0.0000
   980        0.0000             nan     0.5702   -0.0000
  1000        0.0000             nan     0.5702   -0.0000
  1020        0.0000             nan     0.5702   -0.0000
  1040        0.0000             nan     0.5702   -0.0000
  1060        0.0000             nan     0.5702   -0.0000
  1080        0.0000             nan     0.5702   -0.0000
  1100        0.0000             nan     0.5702   -0.0000
  1120        0.0000             nan     0.5702   -0.0000
  1140        0.0000             nan     0.5702   -0.0000
  1160        0.0000             nan     0.5702   -0.0000
  1180        0.0000             nan     0.5702   -0.0000
  1200        0.0000             nan     0.5702   -0.0000
  1220        0.0000             nan     0.5702   -0.0000
  1240        0.0000             nan     0.5702   -0.0000
  1260        0.0000             nan     0.5702   -0.0000
  1280        0.0000             nan     0.5702   -0.0000
  1300        0.0000             nan     0.5702   -0.0000
  1320        0.0000             nan     0.5702   -0.0000
  1340        0.0000             nan     0.5702   -0.0000
  1360        0.0000             nan     0.5702   -0.0000
  1380        0.0000             nan     0.5702   -0.0000
  1400        0.0000             nan     0.5702   -0.0000
  1420        0.0000             nan     0.5702   -0.0000
  1440        0.0000             nan     0.5702   -0.0000
  1460        0.0000             nan     0.5702   -0.0000
  1480        0.0000             nan     0.5702   -0.0000
  1500        0.0000             nan     0.5702   -0.0000
  1520        0.0000             nan     0.5702   -0.0000
  1540        0.0000             nan     0.5702   -0.0000
  1560        0.0000             nan     0.5702   -0.0000
  1580        0.0000             nan     0.5702    0.0000
  1600        0.0000             nan     0.5702   -0.0000
  1620        0.0000             nan     0.5702   -0.0000
  1640        0.0000             nan     0.5702    0.0000
  1660        0.0000             nan     0.5702   -0.0000
  1680        0.0000             nan     0.5702    0.0000
  1700        0.0000             nan     0.5702   -0.0000
  1720        0.0000             nan     0.5702   -0.0000
  1740        0.0000             nan     0.5702   -0.0000
  1760        0.0000             nan     0.5702   -0.0000
  1780        0.0000             nan     0.5702    0.0000
  1800        0.0000             nan     0.5702    0.0000
  1820        0.0000             nan     0.5702   -0.0000
  1840        0.0000             nan     0.5702   -0.0000
  1860        0.0000             nan     0.5702   -0.0000
  1880        0.0000             nan     0.5702   -0.0000
  1900        0.0000             nan     0.5702   -0.0000
  1920        0.0000             nan     0.5702   -0.0000
  1940        0.0000             nan     0.5702   -0.0000
  1960        0.0000             nan     0.5702   -0.0000
  1980        0.0000             nan     0.5702    0.0000
  2000        0.0000             nan     0.5702   -0.0000
  2020        0.0000             nan     0.5702   -0.0000
  2040        0.0000             nan     0.5702   -0.0000
  2060        0.0000             nan     0.5702   -0.0000
  2080        0.0000             nan     0.5702   -0.0000
  2100        0.0000             nan     0.5702   -0.0000
  2120        0.0000             nan     0.5702   -0.0000
  2140        0.0000             nan     0.5702   -0.0000
  2160        0.0000             nan     0.5702   -0.0000
  2180        0.0000             nan     0.5702   -0.0000
  2200        0.0000             nan     0.5702   -0.0000
  2220        0.0000             nan     0.5702   -0.0000
  2240        0.0000             nan     0.5702   -0.0000
  2260        0.0000             nan     0.5702   -0.0000
  2280        0.0000             nan     0.5702   -0.0000
  2300        0.0000             nan     0.5702   -0.0000
  2320        0.0000             nan     0.5702   -0.0000
  2340        0.0000             nan     0.5702   -0.0000
  2360        0.0000             nan     0.5702    0.0000
  2380        0.0000             nan     0.5702   -0.0000
  2400        0.0000             nan     0.5702   -0.0000
  2420        0.0000             nan     0.5702   -0.0000
  2440        0.0000             nan     0.5702   -0.0000
  2460        0.0000             nan     0.5702    0.0000
  2480        0.0000             nan     0.5702    0.0000
  2500        0.0000             nan     0.5702   -0.0000
  2520        0.0000             nan     0.5702   -0.0000
  2540        0.0000             nan     0.5702   -0.0000
  2560        0.0000             nan     0.5702   -0.0000
  2580        0.0000             nan     0.5702    0.0000
  2600        0.0000             nan     0.5702    0.0000
  2620        0.0000             nan     0.5702   -0.0000
  2640        0.0000             nan     0.5702   -0.0000
  2660        0.0000             nan     0.5702   -0.0000
  2680        0.0000             nan     0.5702   -0.0000
  2700        0.0000             nan     0.5702   -0.0000
  2720        0.0000             nan     0.5702   -0.0000
  2740        0.0000             nan     0.5702   -0.0000
  2760        0.0000             nan     0.5702   -0.0000
  2780        0.0000             nan     0.5702   -0.0000
  2800        0.0000             nan     0.5702   -0.0000
  2820        0.0000             nan     0.5702   -0.0000
  2840        0.0000             nan     0.5702   -0.0000
  2860        0.0000             nan     0.5702   -0.0000
  2880        0.0000             nan     0.5702   -0.0000
  2900        0.0000             nan     0.5702   -0.0000
  2920        0.0000             nan     0.5702   -0.0000
  2940        0.0000             nan     0.5702    0.0000
  2960        0.0000             nan     0.5702   -0.0000
  2980        0.0000             nan     0.5702   -0.0000
  3000        0.0000             nan     0.5702   -0.0000
  3020        0.0000             nan     0.5702   -0.0000
  3040        0.0000             nan     0.5702    0.0000
  3060        0.0000             nan     0.5702   -0.0000
  3080        0.0000             nan     0.5702    0.0000
  3100        0.0000             nan     0.5702   -0.0000
  3120        0.0000             nan     0.5702   -0.0000
  3140        0.0000             nan     0.5702   -0.0000
  3160        0.0000             nan     0.5702   -0.0000
  3180        0.0000             nan     0.5702   -0.0000
  3200        0.0000             nan     0.5702   -0.0000
  3220        0.0000             nan     0.5702   -0.0000
  3240        0.0000             nan     0.5702   -0.0000
  3260        0.0000             nan     0.5702   -0.0000
  3280        0.0000             nan     0.5702   -0.0000
  3300        0.0000             nan     0.5702    0.0000
  3320        0.0000             nan     0.5702   -0.0000
  3337        0.0000             nan     0.5702   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0084             nan     0.5774    0.0316
     2        0.0022             nan     0.5774    0.0067
     3        0.0008             nan     0.5774    0.0013
     4        0.0005             nan     0.5774    0.0003
     5        0.0004             nan     0.5774    0.0001
     6        0.0004             nan     0.5774    0.0000
     7        0.0003             nan     0.5774   -0.0000
     8        0.0003             nan     0.5774    0.0000
     9        0.0003             nan     0.5774   -0.0000
    10        0.0003             nan     0.5774   -0.0000
    20        0.0001             nan     0.5774   -0.0000
    40        0.0001             nan     0.5774   -0.0000
    60        0.0001             nan     0.5774   -0.0000
    80        0.0000             nan     0.5774   -0.0000
   100        0.0000             nan     0.5774   -0.0000
   120        0.0000             nan     0.5774   -0.0000
   140        0.0000             nan     0.5774   -0.0000
   160        0.0000             nan     0.5774   -0.0000
   180        0.0000             nan     0.5774   -0.0000
   200        0.0000             nan     0.5774   -0.0000
   220        0.0000             nan     0.5774   -0.0000
   240        0.0000             nan     0.5774   -0.0000
   260        0.0000             nan     0.5774   -0.0000
   280        0.0000             nan     0.5774   -0.0000
   300        0.0000             nan     0.5774   -0.0000
   320        0.0000             nan     0.5774   -0.0000
   340        0.0000             nan     0.5774   -0.0000
   360        0.0000             nan     0.5774   -0.0000
   380        0.0000             nan     0.5774   -0.0000
   400        0.0000             nan     0.5774   -0.0000
   420        0.0000             nan     0.5774   -0.0000
   440        0.0000             nan     0.5774   -0.0000
   460        0.0000             nan     0.5774   -0.0000
   480        0.0000             nan     0.5774    0.0000
   500        0.0000             nan     0.5774   -0.0000
   520        0.0000             nan     0.5774   -0.0000
   540        0.0000             nan     0.5774   -0.0000
   560        0.0000             nan     0.5774    0.0000
   580        0.0000             nan     0.5774   -0.0000
   600        0.0000             nan     0.5774   -0.0000
   620        0.0000             nan     0.5774   -0.0000
   640        0.0000             nan     0.5774    0.0000
   660        0.0000             nan     0.5774   -0.0000
   680        0.0000             nan     0.5774   -0.0000
   700        0.0000             nan     0.5774   -0.0000
   720        0.0000             nan     0.5774    0.0000
   740        0.0000             nan     0.5774   -0.0000
   760        0.0000             nan     0.5774   -0.0000
   780        0.0000             nan     0.5774   -0.0000
   800        0.0000             nan     0.5774   -0.0000
   820        0.0000             nan     0.5774   -0.0000
   840        0.0000             nan     0.5774   -0.0000
   860        0.0000             nan     0.5774   -0.0000
   880        0.0000             nan     0.5774   -0.0000
   900        0.0000             nan     0.5774   -0.0000
   920        0.0000             nan     0.5774   -0.0000
   940        0.0000             nan     0.5774   -0.0000
   960        0.0000             nan     0.5774   -0.0000
   980        0.0000             nan     0.5774   -0.0000
  1000        0.0000             nan     0.5774   -0.0000
  1020        0.0000             nan     0.5774   -0.0000
  1040        0.0000             nan     0.5774   -0.0000
  1060        0.0000             nan     0.5774   -0.0000
  1080        0.0000             nan     0.5774   -0.0000
  1100        0.0000             nan     0.5774   -0.0000
  1120        0.0000             nan     0.5774   -0.0000
  1140        0.0000             nan     0.5774   -0.0000
  1160        0.0000             nan     0.5774   -0.0000
  1180        0.0000             nan     0.5774   -0.0000
  1200        0.0000             nan     0.5774    0.0000
  1220        0.0000             nan     0.5774   -0.0000
  1240        0.0000             nan     0.5774    0.0000
  1260        0.0000             nan     0.5774   -0.0000
  1280        0.0000             nan     0.5774   -0.0000
  1300        0.0000             nan     0.5774   -0.0000
  1320        0.0000             nan     0.5774   -0.0000
  1340        0.0000             nan     0.5774   -0.0000
  1360        0.0000             nan     0.5774    0.0000
  1380        0.0000             nan     0.5774   -0.0000
  1400        0.0000             nan     0.5774   -0.0000
  1420        0.0000             nan     0.5774   -0.0000
  1440        0.0000             nan     0.5774   -0.0000
  1460        0.0000             nan     0.5774   -0.0000
  1480        0.0000             nan     0.5774   -0.0000
  1500        0.0000             nan     0.5774   -0.0000
  1520        0.0000             nan     0.5774   -0.0000
  1540        0.0000             nan     0.5774    0.0000
  1560        0.0000             nan     0.5774   -0.0000
  1580        0.0000             nan     0.5774    0.0000
  1600        0.0000             nan     0.5774    0.0000
  1620        0.0000             nan     0.5774   -0.0000
  1640        0.0000             nan     0.5774   -0.0000
  1660        0.0000             nan     0.5774   -0.0000
  1680        0.0000             nan     0.5774    0.0000
  1700        0.0000             nan     0.5774   -0.0000
  1720        0.0000             nan     0.5774   -0.0000
  1740        0.0000             nan     0.5774   -0.0000
  1760        0.0000             nan     0.5774   -0.0000
  1764        0.0000             nan     0.5774   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0102             nan     0.5997    0.0287
     2        0.0036             nan     0.5997    0.0065
     3        0.0019             nan     0.5997    0.0014
     4        0.0016             nan     0.5997    0.0003
     5        0.0012             nan     0.5997    0.0003
     6        0.0010             nan     0.5997    0.0001
     7        0.0009             nan     0.5997    0.0000
     8        0.0009             nan     0.5997   -0.0000
     9        0.0008             nan     0.5997    0.0000
    10        0.0007             nan     0.5997    0.0000
    20        0.0004             nan     0.5997   -0.0000
    40        0.0002             nan     0.5997   -0.0000
    60        0.0002             nan     0.5997   -0.0000
    80        0.0001             nan     0.5997   -0.0000
   100        0.0001             nan     0.5997   -0.0000
   120        0.0001             nan     0.5997   -0.0000
   140        0.0001             nan     0.5997   -0.0000
   160        0.0001             nan     0.5997   -0.0000
   180        0.0001             nan     0.5997   -0.0000
   200        0.0001             nan     0.5997   -0.0000
   220        0.0001             nan     0.5997   -0.0000
   240        0.0000             nan     0.5997   -0.0000
   260        0.0000             nan     0.5997   -0.0000
   280        0.0000             nan     0.5997   -0.0000
   300        0.0000             nan     0.5997   -0.0000
   320        0.0000             nan     0.5997   -0.0000
   340        0.0000             nan     0.5997    0.0000
   360        0.0000             nan     0.5997   -0.0000
   380        0.0000             nan     0.5997   -0.0000
   400        0.0000             nan     0.5997   -0.0000
   420        0.0000             nan     0.5997   -0.0000
   440        0.0000             nan     0.5997   -0.0000
   460        0.0000             nan     0.5997   -0.0000
   480        0.0000             nan     0.5997   -0.0000
   500        0.0000             nan     0.5997   -0.0000
   520        0.0000             nan     0.5997   -0.0000
   540        0.0000             nan     0.5997   -0.0000
   560        0.0000             nan     0.5997   -0.0000
   580        0.0000             nan     0.5997   -0.0000
   600        0.0000             nan     0.5997   -0.0000
   620        0.0000             nan     0.5997   -0.0000
   640        0.0000             nan     0.5997   -0.0000
   660        0.0000             nan     0.5997   -0.0000
   680        0.0000             nan     0.5997   -0.0000
   700        0.0000             nan     0.5997    0.0000
   720        0.0000             nan     0.5997   -0.0000
   740        0.0000             nan     0.5997   -0.0000
   760        0.0000             nan     0.5997   -0.0000
   780        0.0000             nan     0.5997   -0.0000
   800        0.0000             nan     0.5997   -0.0000
   820        0.0000             nan     0.5997   -0.0000
   840        0.0000             nan     0.5997   -0.0000
   860        0.0000             nan     0.5997   -0.0000
   880        0.0000             nan     0.5997   -0.0000
   900        0.0000             nan     0.5997    0.0000
   920        0.0000             nan     0.5997   -0.0000
   940        0.0000             nan     0.5997   -0.0000
   960        0.0000             nan     0.5997    0.0000
   980        0.0000             nan     0.5997    0.0000
  1000        0.0000             nan     0.5997   -0.0000
  1020        0.0000             nan     0.5997   -0.0000
  1040        0.0000             nan     0.5997   -0.0000
  1060        0.0000             nan     0.5997   -0.0000
  1080        0.0000             nan     0.5997   -0.0000
  1100        0.0000             nan     0.5997   -0.0000
  1120        0.0000             nan     0.5997   -0.0000
  1140        0.0000             nan     0.5997   -0.0000
  1160        0.0000             nan     0.5997   -0.0000
  1180        0.0000             nan     0.5997   -0.0000
  1200        0.0000             nan     0.5997   -0.0000
  1220        0.0000             nan     0.5997   -0.0000
  1240        0.0000             nan     0.5997   -0.0000
  1260        0.0000             nan     0.5997    0.0000
  1280        0.0000             nan     0.5997   -0.0000
  1300        0.0000             nan     0.5997   -0.0000
  1320        0.0000             nan     0.5997   -0.0000
  1340        0.0000             nan     0.5997    0.0000
  1360        0.0000             nan     0.5997   -0.0000
  1380        0.0000             nan     0.5997    0.0000
  1400        0.0000             nan     0.5997   -0.0000
  1420        0.0000             nan     0.5997    0.0000
  1440        0.0000             nan     0.5997   -0.0000
  1460        0.0000             nan     0.5997   -0.0000
  1480        0.0000             nan     0.5997   -0.0000
  1500        0.0000             nan     0.5997   -0.0000
  1520        0.0000             nan     0.5997    0.0000
  1540        0.0000             nan     0.5997   -0.0000
  1560        0.0000             nan     0.5997   -0.0000
  1580        0.0000             nan     0.5997   -0.0000
  1600        0.0000             nan     0.5997   -0.0000
  1620        0.0000             nan     0.5997   -0.0000
  1640        0.0000             nan     0.5997    0.0000
  1660        0.0000             nan     0.5997   -0.0000
  1680        0.0000             nan     0.5997   -0.0000
  1700        0.0000             nan     0.5997   -0.0000
  1720        0.0000             nan     0.5997   -0.0000
  1740        0.0000             nan     0.5997    0.0000
  1760        0.0000             nan     0.5997   -0.0000
  1780        0.0000             nan     0.5997   -0.0000
  1800        0.0000             nan     0.5997   -0.0000
  1820        0.0000             nan     0.5997   -0.0000
  1840        0.0000             nan     0.5997   -0.0000
  1860        0.0000             nan     0.5997   -0.0000
  1880        0.0000             nan     0.5997   -0.0000
  1900        0.0000             nan     0.5997   -0.0000
  1920        0.0000             nan     0.5997   -0.0000
  1940        0.0000             nan     0.5997    0.0000
  1960        0.0000             nan     0.5997   -0.0000
  1980        0.0000             nan     0.5997   -0.0000
  2000        0.0000             nan     0.5997   -0.0000
  2020        0.0000             nan     0.5997   -0.0000
  2040        0.0000             nan     0.5997   -0.0000
  2060        0.0000             nan     0.5997    0.0000
  2080        0.0000             nan     0.5997    0.0000
  2100        0.0000             nan     0.5997   -0.0000
  2120        0.0000             nan     0.5997    0.0000
  2140        0.0000             nan     0.5997   -0.0000
  2160        0.0000             nan     0.5997   -0.0000
  2180        0.0000             nan     0.5997   -0.0000
  2200        0.0000             nan     0.5997   -0.0000
  2220        0.0000             nan     0.5997   -0.0000
  2240        0.0000             nan     0.5997   -0.0000
  2260        0.0000             nan     0.5997   -0.0000
  2280        0.0000             nan     0.5997   -0.0000
  2300        0.0000             nan     0.5997   -0.0000
  2320        0.0000             nan     0.5997    0.0000
  2340        0.0000             nan     0.5997   -0.0000
  2360        0.0000             nan     0.5997   -0.0000
  2380        0.0000             nan     0.5997   -0.0000
  2400        0.0000             nan     0.5997    0.0000
  2420        0.0000             nan     0.5997   -0.0000
  2440        0.0000             nan     0.5997   -0.0000
  2460        0.0000             nan     0.5997    0.0000
  2480        0.0000             nan     0.5997   -0.0000
  2500        0.0000             nan     0.5997   -0.0000
  2520        0.0000             nan     0.5997   -0.0000
  2540        0.0000             nan     0.5997   -0.0000
  2560        0.0000             nan     0.5997   -0.0000
  2580        0.0000             nan     0.5997   -0.0000
  2600        0.0000             nan     0.5997    0.0000
  2620        0.0000             nan     0.5997   -0.0000
  2640        0.0000             nan     0.5997   -0.0000
  2660        0.0000             nan     0.5997    0.0000
  2680        0.0000             nan     0.5997   -0.0000
  2700        0.0000             nan     0.5997    0.0000
  2720        0.0000             nan     0.5997   -0.0000
  2740        0.0000             nan     0.5997    0.0000
  2760        0.0000             nan     0.5997   -0.0000
  2780        0.0000             nan     0.5997   -0.0000
  2800        0.0000             nan     0.5997   -0.0000
  2820        0.0000             nan     0.5997   -0.0000
  2840        0.0000             nan     0.5997   -0.0000
  2860        0.0000             nan     0.5997   -0.0000
  2880        0.0000             nan     0.5997   -0.0000
  2900        0.0000             nan     0.5997   -0.0000
  2920        0.0000             nan     0.5997   -0.0000
  2940        0.0000             nan     0.5997   -0.0000
  2960        0.0000             nan     0.5997    0.0000
  2980        0.0000             nan     0.5997   -0.0000
  3000        0.0000             nan     0.5997   -0.0000
  3020        0.0000             nan     0.5997   -0.0000
  3040        0.0000             nan     0.5997   -0.0000
  3060        0.0000             nan     0.5997   -0.0000
  3080        0.0000             nan     0.5997   -0.0000
  3100        0.0000             nan     0.5997   -0.0000
  3120        0.0000             nan     0.5997   -0.0000
  3140        0.0000             nan     0.5997   -0.0000
  3160        0.0000             nan     0.5997    0.0000
  3180        0.0000             nan     0.5997   -0.0000
  3200        0.0000             nan     0.5997   -0.0000
  3220        0.0000             nan     0.5997   -0.0000
  3240        0.0000             nan     0.5997   -0.0000
  3260        0.0000             nan     0.5997    0.0000
  3269        0.0000             nan     0.5997    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0362             nan     0.0529    0.0037
     2        0.0326             nan     0.0529    0.0035
     3        0.0294             nan     0.0529    0.0034
     4        0.0265             nan     0.0529    0.0029
     5        0.0240             nan     0.0529    0.0024
     6        0.0216             nan     0.0529    0.0023
     7        0.0195             nan     0.0529    0.0022
     8        0.0176             nan     0.0529    0.0020
     9        0.0159             nan     0.0529    0.0016
    10        0.0143             nan     0.0529    0.0014
    20        0.0052             nan     0.0529    0.0005
    40        0.0008             nan     0.0529    0.0001
    60        0.0002             nan     0.0529    0.0000
    80        0.0001             nan     0.0529    0.0000
   100        0.0000             nan     0.0529    0.0000
   120        0.0000             nan     0.0529   -0.0000
   140        0.0000             nan     0.0529    0.0000
   160        0.0000             nan     0.0529   -0.0000
   180        0.0000             nan     0.0529   -0.0000
   200        0.0000             nan     0.0529   -0.0000
   220        0.0000             nan     0.0529   -0.0000
   240        0.0000             nan     0.0529   -0.0000
   260        0.0000             nan     0.0529   -0.0000
   280        0.0000             nan     0.0529   -0.0000
   300        0.0000             nan     0.0529   -0.0000
   320        0.0000             nan     0.0529   -0.0000
   340        0.0000             nan     0.0529   -0.0000
   360        0.0000             nan     0.0529   -0.0000
   380        0.0000             nan     0.0529   -0.0000
   400        0.0000             nan     0.0529   -0.0000
   420        0.0000             nan     0.0529   -0.0000
   440        0.0000             nan     0.0529   -0.0000
   460        0.0000             nan     0.0529   -0.0000
   480        0.0000             nan     0.0529   -0.0000
   500        0.0000             nan     0.0529   -0.0000
   520        0.0000             nan     0.0529   -0.0000
   540        0.0000             nan     0.0529   -0.0000
   560        0.0000             nan     0.0529   -0.0000
   580        0.0000             nan     0.0529   -0.0000
   600        0.0000             nan     0.0529   -0.0000
   620        0.0000             nan     0.0529   -0.0000
   640        0.0000             nan     0.0529   -0.0000
   660        0.0000             nan     0.0529   -0.0000
   680        0.0000             nan     0.0529   -0.0000
   700        0.0000             nan     0.0529   -0.0000
   720        0.0000             nan     0.0529   -0.0000
   740        0.0000             nan     0.0529   -0.0000
   760        0.0000             nan     0.0529   -0.0000
   780        0.0000             nan     0.0529   -0.0000
   800        0.0000             nan     0.0529   -0.0000
   820        0.0000             nan     0.0529   -0.0000
   840        0.0000             nan     0.0529   -0.0000
   860        0.0000             nan     0.0529   -0.0000
   880        0.0000             nan     0.0529   -0.0000
   900        0.0000             nan     0.0529   -0.0000
   920        0.0000             nan     0.0529   -0.0000
   940        0.0000             nan     0.0529   -0.0000
   960        0.0000             nan     0.0529   -0.0000
   980        0.0000             nan     0.0529   -0.0000
  1000        0.0000             nan     0.0529   -0.0000
  1020        0.0000             nan     0.0529   -0.0000
  1040        0.0000             nan     0.0529   -0.0000
  1060        0.0000             nan     0.0529   -0.0000
  1080        0.0000             nan     0.0529   -0.0000
  1100        0.0000             nan     0.0529   -0.0000
  1120        0.0000             nan     0.0529   -0.0000
  1140        0.0000             nan     0.0529   -0.0000
  1160        0.0000             nan     0.0529   -0.0000
  1180        0.0000             nan     0.0529   -0.0000
  1200        0.0000             nan     0.0529   -0.0000
  1220        0.0000             nan     0.0529   -0.0000
  1240        0.0000             nan     0.0529   -0.0000
  1260        0.0000             nan     0.0529   -0.0000
  1280        0.0000             nan     0.0529   -0.0000
  1300        0.0000             nan     0.0529   -0.0000
  1320        0.0000             nan     0.0529   -0.0000
  1340        0.0000             nan     0.0529   -0.0000
  1360        0.0000             nan     0.0529   -0.0000
  1380        0.0000             nan     0.0529   -0.0000
  1400        0.0000             nan     0.0529   -0.0000
  1420        0.0000             nan     0.0529   -0.0000
  1440        0.0000             nan     0.0529   -0.0000
  1460        0.0000             nan     0.0529   -0.0000
  1480        0.0000             nan     0.0529   -0.0000
  1500        0.0000             nan     0.0529   -0.0000
  1520        0.0000             nan     0.0529   -0.0000
  1540        0.0000             nan     0.0529   -0.0000
  1560        0.0000             nan     0.0529   -0.0000
  1580        0.0000             nan     0.0529   -0.0000
  1600        0.0000             nan     0.0529   -0.0000
  1620        0.0000             nan     0.0529   -0.0000
  1640        0.0000             nan     0.0529   -0.0000
  1660        0.0000             nan     0.0529   -0.0000
  1680        0.0000             nan     0.0529   -0.0000
  1700        0.0000             nan     0.0529   -0.0000
  1720        0.0000             nan     0.0529   -0.0000
  1740        0.0000             nan     0.0529   -0.0000
  1760        0.0000             nan     0.0529   -0.0000
  1780        0.0000             nan     0.0529   -0.0000
  1800        0.0000             nan     0.0529   -0.0000
  1820        0.0000             nan     0.0529   -0.0000
  1840        0.0000             nan     0.0529   -0.0000
  1860        0.0000             nan     0.0529   -0.0000
  1880        0.0000             nan     0.0529   -0.0000
  1900        0.0000             nan     0.0529   -0.0000
  1920        0.0000             nan     0.0529   -0.0000
  1940        0.0000             nan     0.0529   -0.0000
  1960        0.0000             nan     0.0529   -0.0000
  1980        0.0000             nan     0.0529   -0.0000
  2000        0.0000             nan     0.0529    0.0000
  2020        0.0000             nan     0.0529   -0.0000
  2040        0.0000             nan     0.0529   -0.0000
  2060        0.0000             nan     0.0529   -0.0000
  2080        0.0000             nan     0.0529   -0.0000
  2100        0.0000             nan     0.0529   -0.0000
  2120        0.0000             nan     0.0529   -0.0000
  2140        0.0000             nan     0.0529   -0.0000
  2160        0.0000             nan     0.0529   -0.0000
  2180        0.0000             nan     0.0529   -0.0000
  2200        0.0000             nan     0.0529   -0.0000
  2220        0.0000             nan     0.0529   -0.0000
  2240        0.0000             nan     0.0529   -0.0000
  2260        0.0000             nan     0.0529   -0.0000
  2280        0.0000             nan     0.0529   -0.0000
  2300        0.0000             nan     0.0529   -0.0000
  2320        0.0000             nan     0.0529   -0.0000
  2340        0.0000             nan     0.0529   -0.0000
  2360        0.0000             nan     0.0529   -0.0000
  2380        0.0000             nan     0.0529   -0.0000
  2400        0.0000             nan     0.0529   -0.0000
  2420        0.0000             nan     0.0529   -0.0000
  2440        0.0000             nan     0.0529   -0.0000
  2460        0.0000             nan     0.0529   -0.0000
  2480        0.0000             nan     0.0529   -0.0000
  2500        0.0000             nan     0.0529   -0.0000
  2520        0.0000             nan     0.0529   -0.0000
  2540        0.0000             nan     0.0529   -0.0000
  2560        0.0000             nan     0.0529   -0.0000
  2580        0.0000             nan     0.0529   -0.0000
  2600        0.0000             nan     0.0529   -0.0000
  2620        0.0000             nan     0.0529   -0.0000
  2640        0.0000             nan     0.0529   -0.0000
  2660        0.0000             nan     0.0529   -0.0000
  2680        0.0000             nan     0.0529   -0.0000
  2700        0.0000             nan     0.0529   -0.0000
  2720        0.0000             nan     0.0529   -0.0000
  2740        0.0000             nan     0.0529   -0.0000
  2760        0.0000             nan     0.0529   -0.0000
  2780        0.0000             nan     0.0529   -0.0000
  2800        0.0000             nan     0.0529   -0.0000
  2820        0.0000             nan     0.0529   -0.0000
  2840        0.0000             nan     0.0529   -0.0000
  2860        0.0000             nan     0.0529   -0.0000
  2880        0.0000             nan     0.0529   -0.0000
  2900        0.0000             nan     0.0529   -0.0000
  2920        0.0000             nan     0.0529   -0.0000
  2940        0.0000             nan     0.0529   -0.0000
  2960        0.0000             nan     0.0529   -0.0000
  2980        0.0000             nan     0.0529   -0.0000
  3000        0.0000             nan     0.0529   -0.0000
  3020        0.0000             nan     0.0529   -0.0000
  3040        0.0000             nan     0.0529   -0.0000
  3060        0.0000             nan     0.0529   -0.0000
  3080        0.0000             nan     0.0529   -0.0000
  3100        0.0000             nan     0.0529   -0.0000
  3120        0.0000             nan     0.0529   -0.0000
  3140        0.0000             nan     0.0529    0.0000
  3160        0.0000             nan     0.0529   -0.0000
  3180        0.0000             nan     0.0529   -0.0000
  3200        0.0000             nan     0.0529   -0.0000
  3220        0.0000             nan     0.0529   -0.0000
  3240        0.0000             nan     0.0529   -0.0000
  3260        0.0000             nan     0.0529   -0.0000
  3280        0.0000             nan     0.0529   -0.0000
  3300        0.0000             nan     0.0529   -0.0000
  3320        0.0000             nan     0.0529   -0.0000
  3340        0.0000             nan     0.0529   -0.0000
  3360        0.0000             nan     0.0529   -0.0000
  3380        0.0000             nan     0.0529   -0.0000
  3400        0.0000             nan     0.0529   -0.0000
  3420        0.0000             nan     0.0529   -0.0000
  3440        0.0000             nan     0.0529   -0.0000
  3460        0.0000             nan     0.0529   -0.0000
  3480        0.0000             nan     0.0529   -0.0000
  3500        0.0000             nan     0.0529   -0.0000
  3520        0.0000             nan     0.0529   -0.0000
  3540        0.0000             nan     0.0529   -0.0000
  3560        0.0000             nan     0.0529    0.0000
  3580        0.0000             nan     0.0529   -0.0000
  3600        0.0000             nan     0.0529   -0.0000
  3620        0.0000             nan     0.0529   -0.0000
  3640        0.0000             nan     0.0529   -0.0000
  3660        0.0000             nan     0.0529   -0.0000
  3680        0.0000             nan     0.0529   -0.0000
  3700        0.0000             nan     0.0529   -0.0000
  3720        0.0000             nan     0.0529   -0.0000
  3740        0.0000             nan     0.0529   -0.0000
  3760        0.0000             nan     0.0529   -0.0000
  3780        0.0000             nan     0.0529   -0.0000
  3800        0.0000             nan     0.0529   -0.0000
  3820        0.0000             nan     0.0529   -0.0000
  3840        0.0000             nan     0.0529   -0.0000
  3860        0.0000             nan     0.0529   -0.0000
  3880        0.0000             nan     0.0529   -0.0000
  3900        0.0000             nan     0.0529   -0.0000
  3920        0.0000             nan     0.0529   -0.0000
  3940        0.0000             nan     0.0529   -0.0000
  3960        0.0000             nan     0.0529   -0.0000
  3980        0.0000             nan     0.0529   -0.0000
  3983        0.0000             nan     0.0529    0.0000

Stochastic Gradient Boosting 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  shrinkage   interaction.depth  n.minobsinnode  n.trees  RMSE       
  0.04824136   8                 17              1700     0.009845581
  0.05292379   9                  7              3983     0.007506446
  0.05598015   8                 22              3905     0.011599292
  0.06995097   6                 22              1765     0.011182117
  0.07598614   1                 13              4982     0.011925882
  0.08499722   8                  5              4281     0.007570372
  0.12133511   5                 23              1040     0.012188399
  0.12947221   7                 10               673     0.009117112
  0.16996586   4                 13              4554     0.010344968
  0.18207158   4                 16              3266     0.011304158
  0.20326478   3                 23              1905     0.014571912
  0.25595545   3                  7              4062     0.010259735
  0.33599424   5                 22              2875     0.015262394
  0.40927966  10                  6              3159     0.013748479
  0.42565542   8                  6              1818     0.014127131
  0.49619373   8                 23              1892     0.018337421
  0.50902870   4                 17              4157     0.015652331
  0.57021573   2                 14              3337     0.015765310
  0.57737946   9                 15              1764     0.018005647
  0.59974237   3                 16              3269     0.016296645
  Rsquared   MAE          Selected
  0.9976199  0.006865089          
  0.9986030  0.005623171  *       
  0.9967055  0.008310612          
  0.9969682  0.007981868          
  0.9965001  0.008878727          
  0.9985757  0.005802573          
  0.9963978  0.008902535          
  0.9979301  0.006805035          
  0.9973643  0.007663068          
  0.9968339  0.008413772          
  0.9947451  0.011248965          
  0.9973933  0.007747346          
  0.9942932  0.011583441          
  0.9953311  0.010847173          
  0.9950554  0.011096941          
  0.9918154  0.014235818          
  0.9939515  0.011972740          
  0.9938253  0.011583470          
  0.9920534  0.013823952          
  0.9935006  0.012079793          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were n.trees = 3983, interaction.depth =
 9, shrinkage = 0.05292379 and n.minobsinnode = 7.
[1] "Mon Mar 12 09:39:30 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 09:42:52 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "gbm_h2o"                        
Multivariate Adaptive Regression Splines 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results:

  RMSE         Rsquared  MAE        
  0.007435549  0.998617  0.005687262

Tuning parameter 'degree' was held constant at a value of 1
[1] "Mon Mar 12 09:42:58 2018"
Generalized Linear Model 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results:

  RMSE          Rsquared   MAE        
  0.0009634527  0.9999769  0.000757459

[1] "Mon Mar 12 09:43:04 2018"
Timing stopped at: 0.02 0 0.01
Error : no valid set of coefficients has been found: please supply starting values
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Timing stopped at: 0.02 0 0.01
Error : no valid set of coefficients has been found: please supply starting values
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Timing stopped at: 0 0 0
Error : no valid set of coefficients has been found: please supply starting values
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 09:43:26 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "glm.nb"                         
Boosted Generalized Linear Model 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  mstop  prune  RMSE         Rsquared   MAE          Selected
  135    no     0.008425808  0.9982394  0.006252778          
  209    yes    0.008266582  0.9983008  0.006104633          
  341    no     0.008138529  0.9983536  0.006001478          
  353    no     0.008129175  0.9983575  0.005993362          
  364    no     0.008120041  0.9983614  0.005986146          
  379    no     0.008106584  0.9983667  0.005974991          
  381    yes    0.008105592  0.9983671  0.005975994          
  575    yes    0.007965149  0.9984240  0.005865779          
  632    no     0.007930752  0.9984379  0.005838218          
  654    yes    0.007918562  0.9984428  0.005826877          
  668    yes    0.007911198  0.9984458  0.005821625          
  782    no     0.007853519  0.9984687  0.005775527          
  797    no     0.007844330  0.9984721  0.005769471          
  813    yes    0.007836767  0.9984751  0.005764455          
  832    yes    0.007829428  0.9984782  0.005757357          
  857    no     0.007817017  0.9984829  0.005748303          
  911    yes    0.007796749  0.9984910  0.005731808          
  997    yes    0.007764147  0.9985038  0.005704931  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mstop = 997 and prune = yes.
[1] "Mon Mar 12 09:43:43 2018"
glmnet 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  alpha      lambda       RMSE         Rsquared   MAE          Selected
  0.1344562  0.518191144  0.120176899  0.9905048  0.099188534          
  0.2080064  0.085231121  0.036079042  0.9946709  0.029627470          
  0.3400364  0.594961177  0.200516998        NaN  0.165522025          
  0.3528485  1.322110703  0.200516998        NaN  0.165522025          
  0.3529606  0.195866027  0.093260464  0.9914554  0.076490855          
  0.3634999  0.870681559  0.200516998        NaN  0.165522025          
  0.3784709  0.717823124  0.200516998        NaN  0.165522025          
  0.3809253  0.009951542  0.010259559  0.9982247  0.008106634          
  0.5749706  0.054072040  0.040901431  0.9946238  0.033075675          
  0.6317869  5.166726127  0.200516998        NaN  0.165522025          
  0.6531948  0.027160804  0.024753301  0.9960967  0.019936705          
  0.6537831  0.009051519  0.011299573  0.9981570  0.009000975          
  0.6674360  0.004553882  0.008244523  0.9986642  0.006332159          
  0.7810396  0.588590613  0.200516998        NaN  0.165522025          
  0.7966071  2.885466617  0.200516998        NaN  0.165522025          
  0.8123921  0.006964251  0.010918255  0.9980895  0.008565773          
  0.8315435  0.019987006  0.021567219  0.9971064  0.017521422          
  0.8562675  0.693525063  0.200516998        NaN  0.165522025          
  0.9108732  0.021283058  0.023426821  0.9974227  0.019167452          
  0.9965235  0.001194395  0.006293305  0.9990631  0.004691476  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were alpha = 0.9965235 and lambda
 = 0.001194395.
[1] "Mon Mar 12 09:43:50 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 09:47:10 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "glmnet_h2o"                     
Start:  AIC=-5534.4
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df  Deviance     AIC
<none>    0.0004475 -5534.4
- V7    1 0.0004761 -5505.3
- V8    1 0.0004926 -5488.3
- V6    1 0.0005057 -5475.1
- V9    1 0.0007945 -5248.8
- V4    1 0.0012289 -5030.2
- V2    1 0.0018250 -4832.1
- V10   1 0.0032573 -4541.9
- V5    1 0.0036135 -4489.9
- V3    1 0.0048195 -4345.6
Start:  AIC=-5584.63
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df  Deviance     AIC
<none>    0.0004249 -5584.6
- V8    1 0.0004606 -5546.0
- V7    1 0.0004631 -5543.3
- V6    1 0.0004958 -5509.0
- V9    1 0.0008110 -5261.5
- V4    1 0.0012433 -5046.6
- V2    1 0.0017921 -4862.6
- V10   1 0.0030188 -4600.4
- V5    1 0.0034174 -4538.0
- V3    1 0.0044069 -4410.1
Start:  AIC=-5507.52
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df  Deviance     AIC
<none>    0.0004609 -5507.5
- V8    1 0.0004909 -5478.0
- V7    1 0.0005080 -5460.9
- V6    1 0.0005453 -5425.4
- V9    1 0.0008558 -5200.1
- V4    1 0.0012237 -5021.3
- V2    1 0.0019922 -4777.6
- V10   1 0.0030484 -4564.9
- V5    1 0.0034516 -4502.8
- V3    1 0.0043366 -4388.7
Start:  AIC=-8317.81
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df  Deviance     AIC
<none>    0.0006720 -8317.8
- V8    1 0.0007268 -8260.8
- V7    1 0.0007300 -8257.5
- V6    1 0.0007808 -8206.9
- V9    1 0.0012383 -7860.1
- V4    1 0.0018551 -7556.2
- V2    1 0.0028256 -7239.7
- V10   1 0.0047039 -6856.5
- V5    1 0.0052855 -6768.8
- V3    1 0.0068638 -6572.3
Generalized Linear Model with Stepwise Feature Selection 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results:

  RMSE          Rsquared   MAE        
  0.0009634527  0.9999769  0.000757459

[1] "Mon Mar 12 09:47:16 2018"
Independent Component Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  n.comp  RMSE          Rsquared   MAE           Selected
  2       0.0147906430  0.9945733  0.0112677938          
  4       0.0059295284  0.9991278  0.0043852671          
  6       0.0057921039  0.9991668  0.0042626286          
  7       0.0034600049  0.9997023  0.0025004722          
  8       0.0009787085  0.9999761  0.0007718763          
  9       0.0009634527  0.9999769  0.0007574590  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was n.comp = 9.
[1] "Mon Mar 12 09:47:24 2018"
Partial Least Squares 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  ncomp  RMSE          Rsquared   MAE           Selected
  2      0.0125781602  0.9960944  0.0099806447          
  4      0.0055737534  0.9992295  0.0041898624          
  6      0.0030909818  0.9997624  0.0022423158          
  7      0.0015789849  0.9999317  0.0012119971          
  8      0.0009865240  0.9999757  0.0007782836          
  9      0.0009634527  0.9999769  0.0007574590  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 9.
[1] "Mon Mar 12 09:47:30 2018"
k-Nearest Neighbors 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  kmax  distance    kernel        RMSE        Rsquared   MAE         Selected
   34   2.08882068  triangular    0.01528926  0.9946740  0.01157050          
   53   1.48789018  triangular    0.01480425  0.9949585  0.01118871          
   86   2.13481556  rectangular   0.01881244  0.9917902  0.01393078          
   89   1.76490902  rectangular   0.01840105  0.9921254  0.01360510          
   89   2.40065607  gaussian      0.01682835  0.9936468  0.01253519          
   91   2.26158856  cos           0.01588062  0.9942386  0.01197620          
   95   2.19731545  inv           0.01592255  0.9942809  0.01200083          
   96   0.77287751  epanechnikov  0.01465474  0.9949997  0.01102321          
  144   1.33639076  triweight     0.01432109  0.9952420  0.01078738          
  158   2.85444240  cos           0.01652728  0.9937749  0.01244877          
  164   0.74131747  gaussian      0.01501249  0.9948344  0.01119262          
  164   1.10715461  epanechnikov  0.01468696  0.9949658  0.01101738          
  167   0.51261060  gaussian      0.01500134  0.9947573  0.01123329          
  196   2.13123149  rectangular   0.01881244  0.9917902  0.01393078          
  200   2.66049338  rectangular   0.01944332  0.9912653  0.01424606          
  204   0.65404246  biweight      0.01428807  0.9951990  0.01078862  *       
  208   1.00504758  inv           0.01451324  0.9951636  0.01091268          
  215   2.18585075  triangular    0.01550734  0.9944775  0.01173664          
  228   1.02596522  epanechnikov  0.01463216  0.9950005  0.01097242          
  250   0.06703756  triangular    0.03996997  0.9724989  0.03050133          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were kmax = 204, distance = 0.6540425
 and kernel = biweight.
[1] "Mon Mar 12 09:51:26 2018"
k-Nearest Neighbors 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  k    RMSE        Rsquared   MAE         Selected
   34  0.02895181  0.9860555  0.01935272  *       
   53  0.03700500  0.9794933  0.02459684          
   86  0.05013832  0.9672498  0.03348083          
   89  0.05129554  0.9660248  0.03434019          
   91  0.05200668  0.9651986  0.03494752          
   95  0.05347972  0.9636526  0.03607958          
   96  0.05391250  0.9631794  0.03642584          
  144  0.07073164  0.9437837  0.04903604          
  158  0.07544843  0.9378517  0.05283229          
  164  0.07744382  0.9351092  0.05442166          
  167  0.07841079  0.9337190  0.05521931          
  196  0.08770647  0.9212022  0.06300980          
  200  0.08903673  0.9194157  0.06419205          
  204  0.09027135  0.9177781  0.06516704          
  208  0.09162009  0.9156767  0.06637219          
  215  0.09387791  0.9123516  0.06832573          
  228  0.09811472  0.9063957  0.07197826          
  250  0.10562838  0.8944141  0.07842335          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was k = 34.
[1] "Mon Mar 12 09:51:34 2018"
Polynomial Kernel Regularized Least Squares 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  lambda        degree  RMSE       Rsquared    MAE        Selected
  0.0000470198  3       2.2331487  0.02542664  1.8796021          
  0.0001096559  2       3.7802555  0.25396292  3.1884162          
  0.0005013972  3       0.3136641  0.01633603  0.2560672          
  0.0005810891  3       0.2914486  0.01579508  0.2371724          
  0.0005818392  2       0.7425007  0.26256156  0.6289429          
  0.0006569001  3       0.2760342  0.01535760  0.2240971          
  0.0007804667  3       0.2583282  0.01476172  0.2098582          
  0.0008028356  1       5.5455767  0.29862377  4.7407380          
  0.0074964069  2       0.2086126  0.28208568  0.1718957          
  0.0144189761  3       0.1993027  0.06664024  0.1634829          
  0.0184490471  2       0.2014297  0.31309263  0.1662514          
  0.0185744369  1       0.3936968  0.31529827  0.3278371          
  0.0217360307  1       0.3642721  0.30975857  0.3007263          
  0.0803892244  3       0.1982156  0.42895977  0.1630330          
  0.0961691112  3       0.1981578  0.47450271  0.1629935          
  0.1153348289  1       0.1962994  0.27096042  0.1600701          
  0.1437862388  2       0.1984729  0.38955827  0.1636500          
  0.1911338598  3       0.1979970  0.59017082  0.1628766          
  0.3583985477  2       0.1982075  0.58100384  0.1633965          
  0.9607653197  1       0.1750161  0.32200628  0.1402379  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 0.9607653 and degree = 1.
[1] "Mon Mar 12 09:53:26 2018"

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.23365527  0.21442869  0.21474655  0.07732186  0.01779031  0.02507439 
         V8          V9         V10 
-0.07900662  0.08086734  0.19720432 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6          V7
25% 0.2206786 0.2058046 0.1821857 0.03890003 -0.008309449 0.001074385
50% 0.2394645 0.2216967 0.2319440 0.08610591  0.015263253 0.023849783
75% 0.2532756 0.2308408 0.2602334 0.11401773  0.043035339 0.048572636
             V8         V9       V10
25% -0.09659688 0.02819421 0.1812687
50% -0.08197436 0.07296370 0.1989304
75% -0.06554291 0.12222299 0.2189320

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.387816530  0.084452629  0.153017344  0.185452317 -0.059554338 -0.009635527 
          V8           V9          V10 
-0.005913371  0.028371622  0.263280122 

 Quartiles of Marginal Effects:
 
           V2         V3        V4        V5          V6          V7
25% 0.3858609 0.08121558 0.1443922 0.1733458 -0.06389791 -0.01391933
50% 0.3888918 0.08423044 0.1555638 0.1909340 -0.06001565 -0.01041908
75% 0.3907222 0.08753841 0.1655784 0.1997312 -0.05628335 -0.00683388
              V8         V9       V10
25% -0.008498259 0.02554226 0.2614070
50% -0.003627642 0.02952665 0.2643691
75% -0.001677095 0.03281388 0.2658200

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.304138501  0.190706792  0.173666255  0.151788713 -0.018920657  0.004826628 
          V8           V9          V10 
-0.057022552  0.056297438  0.216883617 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6           V7
25% 0.2989358 0.1859423 0.1574297 0.1367758 -0.02620855 -0.001932704
50% 0.3068841 0.1909945 0.1793057 0.1547252 -0.01843283  0.005276730
75% 0.3113709 0.1951085 0.1968645 0.1687985 -0.01187735  0.010898334
             V8         V9       V10
25% -0.06638215 0.04724939 0.2122057
50% -0.05426327 0.05616725 0.2194007
75% -0.04736402 0.06643129 0.2232675

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.2512939137  0.2353391409  0.1537725932  0.1276561750 -0.0007852621 
           V7            V8            V9           V10 
 0.0178063867 -0.1018334908  0.0915240774  0.2285077369 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7          V8
25% 0.2289893 0.2193907 0.1114924 0.1057919 -0.02948086 -0.01420864 -0.11654119
50% 0.2472227 0.2319171 0.1620483 0.1332060 -0.00783435  0.01735139 -0.09948522
75% 0.2733752 0.2477160 0.2004027 0.1576140  0.02083285  0.04939977 -0.08589699
            V9       V10
25% 0.05010806 0.2063069
50% 0.08995995 0.2299178
75% 0.12983042 0.2523190

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.240529058  0.236834453  0.198238974  0.116333603  0.005575297  0.014401789 
          V8           V9          V10 
-0.075572174  0.063999947  0.209592677 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6         V7          V8
25% 0.2372960 0.2332344 0.1946566 0.1132384 0.003844002 0.01261824 -0.07692436
50% 0.2425939 0.2394405 0.2012757 0.1182806 0.007324120 0.01618253 -0.07537574
75% 0.2458142 0.2425686 0.2047681 0.1208208 0.009091603 0.01806565 -0.07390978
            V9       V10
25% 0.06206766 0.2061913
50% 0.06638991 0.2118066
75% 0.06808904 0.2148278

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.34284831  0.14252048  0.16123729  0.16129339  0.01086884  0.01166691 
         V8          V9         V10 
-0.09306552  0.08788202  0.20706988 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7          V8
25% 0.3338469 0.1291249 0.1393883 0.1407513 0.001618165 0.001784731 -0.10008035
50% 0.3450951 0.1399288 0.1685103 0.1584615 0.010660321 0.010867119 -0.09006487
75% 0.3538454 0.1504455 0.1934641 0.1813650 0.020557705 0.022427391 -0.08419613
            V9       V10
25% 0.07036199 0.1960683
50% 0.08921684 0.2074085
75% 0.10818349 0.2188183

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.26083379  0.21637288  0.20339316  0.09215886  0.01705505  0.02697833 
         V8          V9         V10 
-0.10142516  0.09042890  0.20589679 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6          V7
25% 0.2416499 0.2027338 0.1681544 0.06170565 -0.009096889 0.001334412
50% 0.2574305 0.2126288 0.2138203 0.10336638  0.011560961 0.024013797
75% 0.2792105 0.2284403 0.2533404 0.12659372  0.040901062 0.052673160
             V8         V9       V10
25% -0.11625559 0.03186741 0.1841817
50% -0.09710916 0.08704328 0.2072873
75% -0.08327438 0.13962096 0.2293290

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.24244047  0.21769847  0.21499935  0.08165580  0.01641700  0.02316760 
         V8          V9         V10 
-0.08289482  0.08137326  0.20124500 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6          V7
25% 0.2316292 0.2093170 0.1863195 0.04842187 -0.007236276 0.001025998
50% 0.2450522 0.2204080 0.2329607 0.08832992  0.012219490 0.020847847
75% 0.2570932 0.2294404 0.2586364 0.11548199  0.040859280 0.044835199
             V8         V9       V10
25% -0.09921356 0.02670696 0.1865648
50% -0.08268634 0.07115963 0.2007144
75% -0.07049051 0.12555490 0.2202286

 Average Marginal Effects:
 
        V2         V3         V4         V5         V6         V7         V8 
0.06745480 0.07367701 0.07713154 0.03709634 0.04068697 0.04290316 0.01171289 
        V9        V10 
0.05305676 0.06940423 

 Quartiles of Marginal Effects:
 
            V2         V3         V4         V5         V6         V7
25% 0.06739655 0.07360749 0.07706507 0.03704706 0.04064261 0.04285836
50% 0.06751061 0.07373925 0.07720134 0.03714177 0.04075711 0.04297241
75% 0.06755951 0.07379901 0.07727178 0.03718446 0.04080078 0.04302047
            V8         V9        V10
25% 0.01167394 0.05299538 0.06933568
50% 0.01174823 0.05311822 0.06946251
75% 0.01178086 0.05317419 0.06951944

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.2489506847  0.2410436140  0.1913611004  0.1273047790  0.0001979495 
           V7            V8            V9           V10 
 0.0107178068 -0.0755446527  0.0588918029  0.2142407747 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5            V6         V7
25% 0.2481679 0.2402299 0.1906085 0.1266458 -0.0002011765 0.01028724
50% 0.2494698 0.2416547 0.1921855 0.1277374  0.0006053032 0.01115436
75% 0.2501988 0.2423976 0.1929834 0.1283654  0.0011199067 0.01164178
             V8         V9       V10
25% -0.07583250 0.05844703 0.2134380
50% -0.07541824 0.05948316 0.2147979
75% -0.07510594 0.05987646 0.2154597

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.25374692  0.21977317  0.20764467  0.08094575  0.01682786  0.02716399 
         V8          V9         V10 
-0.09835507  0.08980219  0.20878326 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6            V7
25% 0.2343471 0.2058965 0.1694628 0.04320333 -0.012643224 -0.0007851038
50% 0.2499672 0.2159363 0.2208164 0.09649747  0.009895754  0.0224364770
75% 0.2727666 0.2324829 0.2602715 0.12010398  0.044882697  0.0531806466
             V8         V9       V10
25% -0.11418674 0.02573600 0.1869549
50% -0.09290219 0.08593965 0.2116672
75% -0.07656304 0.13929371 0.2328174

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.28371337  0.21360408  0.17551545  0.14597775 -0.01906785  0.00557478 
         V8          V9         V10 
-0.06076145  0.05048492  0.22027439 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7          V8
25% 0.2795473 0.2091917 0.1698508 0.1381037 -0.02348119 0.001423139 -0.06454769
50% 0.2857642 0.2163845 0.1787612 0.1476193 -0.01888425 0.005762783 -0.05882054
75% 0.2903890 0.2197511 0.1851674 0.1554968 -0.01495445 0.009548504 -0.05697513
            V9       V10
25% 0.04719584 0.2162425
50% 0.05145366 0.2225737
75% 0.05575244 0.2262596

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.22953715  0.23001285  0.20211647  0.10386559  0.01122707  0.01904884 
         V8          V9         V10 
-0.07017768  0.07287618  0.20488522 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5         V6         V7          V8
25% 0.2284320 0.2288805 0.2008749 0.1029144 0.01063019 0.01845551 -0.07059006
50% 0.2302970 0.2309372 0.2032833 0.1046125 0.01187677 0.01973519 -0.07005029
75% 0.2313412 0.2319715 0.2043519 0.1052775 0.01251417 0.02037972 -0.06956209
            V9       V10
25% 0.07219995 0.2037153
50% 0.07371403 0.2057184
75% 0.07430780 0.2066921

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.25687754  0.21790824  0.20878280  0.08432337  0.01732558  0.02739113 
         V8          V9         V10 
-0.09950794  0.09012812  0.20592261 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5          V6           V7
25% 0.2383571 0.2045814 0.1700857 0.04976313 -0.01067756 0.0008996204
50% 0.2538565 0.2142737 0.2214892 0.09708601  0.01121662 0.0233033364
75% 0.2749475 0.2299380 0.2605983 0.12238847  0.04303479 0.0535133529
             V8         V9       V10
25% -0.11481419 0.02777434 0.1841313
50% -0.09466219 0.08608985 0.2075652
75% -0.07927243 0.14003929 0.2294969

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.22728895  0.22728235  0.20264726  0.10071416  0.01261664  0.02030674 
         V8          V9         V10 
-0.06955518  0.07351301  0.20291322 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5         V6         V7          V8
25% 0.2239178 0.2237241 0.1991633 0.09783733 0.01074371 0.01834540 -0.07099794
50% 0.2295015 0.2300008 0.2059057 0.10289441 0.01455788 0.02229116 -0.06928645
75% 0.2328075 0.2333349 0.2092933 0.10509053 0.01636631 0.02421828 -0.06773415
            V9       V10
25% 0.07142419 0.1992503
50% 0.07595645 0.2053938
75% 0.07786192 0.2083478

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.29247161  0.19312978  0.19055835  0.12448124  0.01246938  0.01102740 
         V8          V9         V10 
-0.09189888  0.08721029  0.20536433 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6          V7
25% 0.2789611 0.1798680 0.1631643 0.1052363 -0.001400268 -0.00373673
50% 0.2922600 0.1901998 0.2013698 0.1239673  0.011622731  0.01001769
75% 0.3065859 0.2017526 0.2303223 0.1450053  0.027323233  0.02729911
             V8         V9       V10
25% -0.10197082 0.05479626 0.1890028
50% -0.09009320 0.08361407 0.2062273
75% -0.08123042 0.12277668 0.2226941

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.22234379  0.20735015  0.21184076  0.05603047  0.02128353  0.02776777 
         V8          V9         V10 
-0.08218971  0.09274460  0.19408862 

 Quartiles of Marginal Effects:
 
           V2        V3        V4           V5          V6           V7
25% 0.2087649 0.1979676 0.1686016 -0.007723821 -0.01547972 -0.007747082
50% 0.2300648 0.2190804 0.2352154  0.066624064  0.01543623  0.021901948
75% 0.2514580 0.2359409 0.2875478  0.115549838  0.06565451  0.070969342
             V8         V9       V10
25% -0.11042517 0.02168441 0.1712462
50% -0.08309858 0.08773764 0.2061317
75% -0.05471810 0.14165990 0.2255754

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.22524938  0.20740229  0.21136280  0.04820807  0.02339250  0.02854146 
         V8          V9         V10 
-0.08593761  0.09411152  0.19659399 

 Quartiles of Marginal Effects:
 
           V2        V3        V4          V5          V6          V7
25% 0.2111655 0.1997614 0.1693636 -0.01326739 -0.01427299 -0.01005208
50% 0.2333483 0.2163017 0.2299880  0.06346324  0.01787638  0.02422212
75% 0.2515602 0.2349112 0.2874961  0.11819002  0.07838524  0.07805449
             V8         V9       V10
25% -0.11242668 0.01724779 0.1710719
50% -0.08378468 0.09038284 0.2048478
75% -0.05334655 0.14793830 0.2318056

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.24740032  0.22112852  0.21126140  0.07398390  0.02455065  0.02535967 
         V8          V9         V10 
-0.09583598  0.08359004  0.21258598 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6           V7
25% 0.2260394 0.2049906 0.1644664 0.03132340 -0.008840977 -0.006129192
50% 0.2419986 0.2169557 0.2236858 0.09183679  0.015249094  0.018107130
75% 0.2684504 0.2351682 0.2679196 0.12184617  0.056417393  0.052452900
             V8         V9       V10
25% -0.11293150 0.01668385 0.1906367
50% -0.09060833 0.07730321 0.2166203
75% -0.07172579 0.13611561 0.2400164

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.263056352  0.234333179  0.183345403  0.137311191 -0.007088592  0.008564022 
          V8           V9          V10 
-0.071560952  0.053542695  0.217221050 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6          V7
25% 0.2614715 0.2324903 0.1815429 0.1357091 -0.008018107 0.007602326
50% 0.2640504 0.2356085 0.1850570 0.1381099 -0.006436701 0.009270502
75% 0.2656417 0.2369905 0.1868122 0.1397455 -0.005350548 0.010361877
             V8         V9       V10
25% -0.07231505 0.05266715 0.2156119
50% -0.07110863 0.05466834 0.2182553
75% -0.07063386 0.05553567 0.2196976

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.23895182  0.21068573  0.21644140  0.07469464  0.01189555  0.01844310 
         V8          V9         V10 
-0.07862744  0.08754769  0.19848589 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5          V6           V7
25% 0.2274681 0.2033712 0.1800570 0.03247495 -0.01202111 -0.003221195
50% 0.2450365 0.2181009 0.2336598 0.07851933  0.01084557  0.019163350
75% 0.2592266 0.2271668 0.2614470 0.11104200  0.03658491  0.041463930
             V8         V9       V10
25% -0.09620183 0.03203546 0.1835719
50% -0.07957051 0.07862587 0.2011638
75% -0.06487891 0.13108462 0.2234332

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.393484390  0.083338792  0.163191059  0.175302695 -0.060568229 -0.015201990 
          V8           V9          V10 
 0.005448864  0.025124079  0.258569755 

 Quartiles of Marginal Effects:
 
           V2         V3        V4        V5          V6          V7
25% 0.3915952 0.08005154 0.1542940 0.1628321 -0.06466387 -0.01920414
50% 0.3947428 0.08315356 0.1651190 0.1804868 -0.06118328 -0.01598602
75% 0.3964637 0.08675164 0.1752947 0.1888337 -0.05776780 -0.01286168
             V8         V9       V10
25% 0.002017726 0.02243799 0.2568001
50% 0.007863694 0.02615682 0.2597507
75% 0.009971272 0.02920525 0.2611092

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.311518142  0.185066422  0.180898258  0.144538261 -0.019290193  0.001909002 
          V8           V9          V10 
-0.051882619  0.056258293  0.213483470 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6           V7
25% 0.3063672 0.1796608 0.1639706 0.1282835 -0.02584764 -0.004206216
50% 0.3146812 0.1851522 0.1858140 0.1484801 -0.01895670  0.002436956
75% 0.3184725 0.1895951 0.2030806 0.1614082 -0.01273093  0.007772712
             V8         V9       V10
25% -0.06066594 0.04782079 0.2096414
50% -0.04789865 0.05648348 0.2155402
75% -0.04274632 0.06622344 0.2197432

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.26012353  0.21195019  0.17577090  0.11154194  0.04983219 -0.02482288 
         V8          V9         V10 
-0.10455752  0.11567860  0.22162718 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5         V6           V7         V8
25% 0.2468115 0.1946546 0.1392325 0.09698027 0.03036810 -0.054649065 -0.1224581
50% 0.2660885 0.2061138 0.1796862 0.11719515 0.04690997 -0.026849734 -0.1037462
75% 0.2786218 0.2242929 0.2208892 0.13439870 0.06723155  0.002603679 -0.0891918
            V9       V10
25% 0.08840022 0.2011588
50% 0.12048880 0.2194338
75% 0.14583167 0.2436313

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.242909728  0.231517434  0.206915038  0.110848599  0.004182403  0.011634566 
          V8           V9          V10 
-0.071922350  0.067990052  0.208827700 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7          V8
25% 0.2395280 0.2282698 0.2032015 0.1081576 0.002376365 0.009643019 -0.07341210
50% 0.2454640 0.2342583 0.2097097 0.1124610 0.005950176 0.013498079 -0.07166967
75% 0.2484256 0.2373374 0.2135803 0.1150680 0.007377801 0.014986001 -0.07028433
            V9       V10
25% 0.06553369 0.2055877
50% 0.07025554 0.2113823
75% 0.07227351 0.2142719

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.349766938  0.136033477  0.165475570  0.157491544  0.022770557  0.003263294 
          V8           V9          V10 
-0.094185409  0.091040634  0.201895232 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5         V6           V7          V8
25% 0.3410018 0.1236154 0.1424438 0.1380166 0.01368449 -0.005253121 -0.10118587
50% 0.3522519 0.1331352 0.1700512 0.1565710 0.02315965  0.003029567 -0.09039891
75% 0.3607879 0.1443918 0.1977289 0.1766452 0.03223823  0.011742264 -0.08533481
            V9       V10
25% 0.07415661 0.1900252
50% 0.09299574 0.2020429
75% 0.11226243 0.2136623

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.264083892  0.208398446  0.208626018  0.086065140  0.022173280  0.009547105 
          V8           V9          V10 
-0.098130134  0.102653944  0.207807629 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5          V6           V7
25% 0.2445719 0.1956386 0.1731115 0.05351631 0.001772886 -0.015640577
50% 0.2617492 0.2043494 0.2178157 0.09900498 0.017455455  0.006100316
75% 0.2816400 0.2194281 0.2569383 0.12031470 0.041359829  0.030621326
             V8         V9       V10
25% -0.11339406 0.04397124 0.1830037
50% -0.09484574 0.09994286 0.2087410
75% -0.08122401 0.15360329 0.2325808

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.24712851  0.21360477  0.21747935  0.07918113  0.01114170  0.01694547 
         V8          V9         V10 
-0.08208288  0.08714240  0.20186058 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6           V7
25% 0.2362825 0.2056302 0.1879478 0.04477804 -0.010328858 -0.003028579
50% 0.2488600 0.2168393 0.2319721 0.08333744  0.008985425  0.016764815
75% 0.2623388 0.2251657 0.2611861 0.11296709  0.033698335  0.038150532
             V8         V9       V10
25% -0.09861648 0.02939104 0.1868273
50% -0.08108066 0.07892440 0.2005271
75% -0.06874862 0.13409208 0.2234866

 Average Marginal Effects:
 
        V2         V3         V4         V5         V6         V7         V8 
0.06853122 0.07334704 0.08019533 0.03782132 0.03848441 0.04040476 0.01270579 
        V9        V10 
0.05632254 0.07004093 

 Quartiles of Marginal Effects:
 
            V2         V3         V4         V5         V6         V7
25% 0.06847082 0.07328856 0.08011356 0.03776344 0.03843326 0.04035016
50% 0.06859238 0.07341041 0.08027143 0.03786182 0.03854994 0.04047051
75% 0.06864368 0.07347111 0.08033720 0.03790616 0.03859653 0.04051786
            V8         V9        V10
25% 0.01266675 0.05626345 0.06997847
50% 0.01274177 0.05638566 0.07010197
75% 0.01277546 0.05644781 0.07016262

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.2514656663  0.2358104148  0.2011844601  0.1221772075 -0.0001878358 
           V7            V8            V9           V10 
 0.0087722566 -0.0717130802  0.0601383556  0.2128647918 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5            V6          V7
25% 0.2506977 0.2349778 0.2004102 0.1214525 -0.0006448945 0.008311195
50% 0.2520367 0.2364624 0.2019431 0.1225466  0.0002429423 0.009215928
75% 0.2527478 0.2371545 0.2028516 0.1231684  0.0006275816 0.009613877
             V8         V9       V10
25% -0.07196301 0.05975013 0.2121778
50% -0.07158188 0.06068632 0.2134743
75% -0.07132517 0.06115110 0.2141185

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.255176341  0.212247065  0.217556080  0.073251208  0.026157171  0.009834909 
          V8           V9          V10 
-0.095381501  0.101994958  0.208911877 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5          V6           V7
25% 0.2352135 0.2000683 0.1744833 0.03465855 0.003770652 -0.018853030
50% 0.2530705 0.2093632 0.2287600 0.08974974 0.019952979  0.004416355
75% 0.2725914 0.2230734 0.2669288 0.11400148 0.049031035  0.034280715
             V8         V9       V10
25% -0.11182845 0.04135807 0.1825192
50% -0.09146750 0.10020808 0.2116390
75% -0.07409365 0.15159547 0.2340919

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.288155554  0.208632869  0.185086748  0.137218913 -0.018851895  0.003518342 
          V8           V9          V10 
-0.055973764  0.051287381  0.217613229 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6            V7
25% 0.2840308 0.2040703 0.1789220 0.1290124 -0.02337789 -0.0008123367
50% 0.2908571 0.2112521 0.1881244 0.1388025 -0.01850082  0.0040360258
75% 0.2948064 0.2150389 0.1942480 0.1461277 -0.01503375  0.0071512303
             V8         V9       V10
25% -0.06003501 0.04744225 0.2135552
50% -0.05378519 0.05251678 0.2203983
75% -0.05222024 0.05634326 0.2236405

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.231455051  0.225210432  0.210142264  0.101586676  0.008728156  0.015188900 
          V8           V9          V10 
-0.066544464  0.077334880  0.204225625 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6         V7          V8
25% 0.2303049 0.2241845 0.2088299 0.1006698 0.008094702 0.01454936 -0.06701143
50% 0.2323059 0.2261597 0.2111410 0.1021818 0.009429831 0.01590363 -0.06639681
75% 0.2333155 0.2271783 0.2124308 0.1029378 0.009887566 0.01639452 -0.06596909
            V9       V10
25% 0.07655993 0.2031907
50% 0.07812817 0.2050994
75% 0.07884561 0.2061015

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.25918548  0.21082122  0.21526382  0.07830870  0.02157029  0.01200040 
         V8          V9         V10 
-0.09643202  0.10143150  0.20741820 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6           V7
25% 0.2389620 0.1990068 0.1792462 0.04173329 0.0001612706 -0.013609192
50% 0.2571091 0.2074860 0.2245169 0.09170453 0.0163046157  0.008086926
75% 0.2764907 0.2208576 0.2643530 0.11610097 0.0424506518  0.033745766
             V8         V9       V10
25% -0.11169920 0.03987863 0.1818753
50% -0.09231748 0.09837213 0.2095076
75% -0.07743287 0.15498732 0.2327589

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.229308966  0.222339103  0.210369044  0.097540087  0.009979696  0.016424367 
          V8           V9          V10 
-0.066034612  0.078876576  0.202533773 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5          V6         V7          V8
25% 0.2258357 0.2189597 0.2063599 0.09467267 0.007962027 0.01436262 -0.06761353
50% 0.2319519 0.2252304 0.2134523 0.09933361 0.011980584 0.01857780 -0.06566908
75% 0.2350013 0.2283046 0.2172785 0.10166452 0.013452529 0.02006155 -0.06427602
            V9       V10
25% 0.07646196 0.1991868
50% 0.08131364 0.2052206
75% 0.08352101 0.2082576

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.296354329  0.189323820  0.194890924  0.120343620  0.015405710  0.004978376 
          V8           V9          V10 
-0.090881721  0.091629552  0.202560589 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5         V6           V7          V8
25% 0.2826296 0.1773149 0.1699882 0.1031639 0.00321554 -0.008206969 -0.10022397
50% 0.2964914 0.1872279 0.2031402 0.1207172 0.01538363  0.004097321 -0.08896919
75% 0.3102388 0.1976267 0.2342524 0.1407344 0.02795647  0.019451919 -0.08052905
            V9       V10
25% 0.05936928 0.1847308
50% 0.09276584 0.2035294
75% 0.12752267 0.2204718

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.22517946  0.20226641  0.21794157  0.05387985  0.01546440  0.02049063 
         V8          V9         V10 
-0.07925606  0.09833468  0.19540307 

 Quartiles of Marginal Effects:
 
           V2        V3        V4          V5          V6          V7
25% 0.2115321 0.1940412 0.1733462 -0.01195673 -0.01712265 -0.01121513
50% 0.2342973 0.2139727 0.2391226  0.06368571  0.01242871  0.01671404
75% 0.2573584 0.2289067 0.2839630  0.11165080  0.05330726  0.05730167
             V8         V9       V10
25% -0.10558302 0.02867309 0.1770163
50% -0.08121675 0.09079573 0.2056920
75% -0.05667002 0.14895028 0.2281185

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.22750187  0.20324569  0.22216870  0.04881225  0.01708562  0.01930935 
         V8          V9         V10 
-0.08591703  0.10053531  0.19700468 

 Quartiles of Marginal Effects:
 
           V2        V3        V4          V5          V6         V7
25% 0.2088533 0.1949050 0.1801046 -0.01569228 -0.01619230 -0.0132254
50% 0.2355189 0.2135491 0.2424664  0.05737953  0.01149532  0.0140813
75% 0.2614236 0.2310753 0.2894758  0.11340619  0.05749477  0.0592527
             V8         V9       V10
25% -0.11391663 0.02542986 0.1733109
50% -0.08124656 0.09221388 0.2037153
75% -0.05534053 0.15604262 0.2320791

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.24978952  0.21473376  0.22104681  0.06850857  0.03621970  0.01158459 
         V8          V9         V10 
-0.09499657  0.09130294  0.21012603 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5          V6           V7
25% 0.2283894 0.2013687 0.1749564 0.02578321 0.009574042 -0.021280817
50% 0.2479535 0.2129410 0.2339794 0.08804829 0.028656588  0.004012547
75% 0.2675437 0.2267516 0.2746799 0.11462938 0.064811499  0.037846096
             V8         V9       V10
25% -0.11446104 0.03522161 0.1815077
50% -0.09010003 0.08943908 0.2130666
75% -0.07076090 0.13945712 0.2379099

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.266222366  0.228898908  0.194128295  0.130457620 -0.006586419  0.007372566 
          V8           V9          V10 
-0.067585676  0.053586500  0.215257233 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6          V7
25% 0.2645839 0.2273323 0.1921938 0.1289249 -0.007453697 0.006503775
50% 0.2673827 0.2301975 0.1956406 0.1311431 -0.005913002 0.008023601
75% 0.2688315 0.2316738 0.1975997 0.1327648 -0.004937618 0.009085903
             V8         V9       V10
25% -0.06835199 0.05272987 0.2137067
50% -0.06715720 0.05466664 0.2164003
75% -0.06666732 0.05563329 0.2177680

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.24246534  0.21342869  0.21509949  0.07413735  0.01347583  0.01884965 
         V8          V9         V10 
-0.07220548  0.08082716  0.19587623 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6           V7
25% 0.2299710 0.2067650 0.1874598 0.03654787 -0.008635378 7.941387e-05
50% 0.2497691 0.2208516 0.2309036 0.07990882  0.012427980 1.910636e-02
75% 0.2621812 0.2291809 0.2594734 0.11322766  0.037988482 4.093344e-02
             V8         V9       V10
25% -0.08807148 0.03143987 0.1834622
50% -0.07314860 0.07279293 0.1978940
75% -0.06006368 0.11738881 0.2187286

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.395372453  0.081635725  0.162916479  0.178975017 -0.054168082 -0.015699832 
          V8           V9          V10 
 0.003728513  0.020012002  0.258073460 

 Quartiles of Marginal Effects:
 
           V2         V3        V4        V5          V6          V7
25% 0.3932619 0.07833572 0.1553780 0.1669552 -0.05808878 -0.01947019
50% 0.3966600 0.08153969 0.1653837 0.1844623 -0.05482188 -0.01650155
75% 0.3982681 0.08477752 0.1747541 0.1926011 -0.05126642 -0.01306467
             V8         V9       V10
25% 0.001224659 0.01694620 0.2567372
50% 0.006030682 0.02080762 0.2590652
75% 0.007882976 0.02416307 0.2605539

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 3.129497e-01  1.882681e-01  1.793292e-01  1.461888e-01 -1.926733e-02 
           V7            V8            V9           V10 
 8.885169e-05 -4.753717e-02  5.107405e-02  2.120849e-01 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6            V7
25% 0.3079001 0.1831840 0.1646187 0.1318412 -0.02537503 -0.0053040896
50% 0.3161580 0.1887633 0.1839931 0.1497916 -0.01878547  0.0003979396
75% 0.3200379 0.1927505 0.2013002 0.1633191 -0.01273639  0.0059871386
             V8         V9       V10
25% -0.05600566 0.04222239 0.2077054
50% -0.04402103 0.05087272 0.2141241
75% -0.03846223 0.06071620 0.2182158

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.26088227  0.21944284  0.19762039  0.11258222  0.03600037  0.01251755 
         V8          V9         V10 
-0.10156388  0.07905695  0.20890432 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5          V6          V7
25% 0.2433254 0.2079306 0.1665913 0.09493241 0.009665619 -0.01222204
50% 0.2616105 0.2197781 0.2040741 0.11690200 0.031765599  0.01268032
75% 0.2755339 0.2315360 0.2400175 0.13811109 0.059029808  0.03340445
             V8         V9       V10
25% -0.11564776 0.04358887 0.1815905
50% -0.10110548 0.08202038 0.2110863
75% -0.08903253 0.11070395 0.2405212

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.245525874  0.233644730  0.203366974  0.111081291  0.003035408  0.009806478 
          V8           V9          V10 
-0.067455920  0.064518811  0.206524770 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6          V7          V8
25% 0.2420819 0.2299223 0.2000591 0.1083561 0.001341845 0.008118774 -0.06884634
50% 0.2480926 0.2365214 0.2062174 0.1129778 0.004741942 0.011528878 -0.06709300
75% 0.2509308 0.2394308 0.2097963 0.1154376 0.006201516 0.013040765 -0.06581209
            V9       V10
25% 0.06259529 0.2029229
50% 0.06680981 0.2091326
75% 0.06866317 0.2118432

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.3552574841  0.1388168162  0.1663924475  0.1567207822  0.0181974381 
           V7            V8            V9           V10 
-0.0009297303 -0.0819916407  0.0831981745  0.1986214909 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6            V7
25% 0.3461055 0.1259698 0.1460123 0.1357483 0.009043265 -0.0091486145
50% 0.3576261 0.1359585 0.1712644 0.1555262 0.019381929 -0.0009648204
75% 0.3665185 0.1465820 0.1992813 0.1762519 0.028913082  0.0079373569
             V8         V9       V10
25% -0.08881716 0.06728497 0.1863920
50% -0.07825633 0.08516659 0.1979549
75% -0.07329596 0.10133696 0.2117134

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.26288608  0.21310745  0.21871528  0.08554973  0.02294159  0.01246057 
         V8          V9         V10 
-0.08781582  0.07877465  0.20771610 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5            V6           V7
25% 0.2434211 0.1992453 0.1845786 0.05457342 -0.0004244637 -0.008300084
50% 0.2601583 0.2104862 0.2304101 0.09515795  0.0202151222  0.011326916
75% 0.2815623 0.2242472 0.2723579 0.11929841  0.0453569991  0.031276999
             V8         V9       V10
25% -0.10009436 0.02510499 0.1808937
50% -0.08310817 0.07362996 0.2100526
75% -0.07180208 0.12827361 0.2345915

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.25090528  0.21631542  0.21595685  0.07900414  0.01238828  0.01686054 
         V8          V9         V10 
-0.07505946  0.08017350  0.19900194 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6          V7
25% 0.2395918 0.2085156 0.1897564 0.04734044 -0.008077201 -0.00153814
50% 0.2541554 0.2198269 0.2319044 0.08380302  0.012279823  0.01754895
75% 0.2667664 0.2272341 0.2583796 0.11613185  0.035895668  0.03790874
             V8         V9       V10
25% -0.09081293 0.02894417 0.1839056
50% -0.07471736 0.07171257 0.1984441
75% -0.06458182 0.12160766 0.2184208

 Average Marginal Effects:
 
        V2         V3         V4         V5         V6         V7         V8 
0.06816306 0.07280637 0.07806265 0.03697744 0.03669243 0.03850783 0.01353802 
        V9        V10 
0.05481352 0.06859624 

 Quartiles of Marginal Effects:
 
            V2         V3         V4         V5         V6         V7
25% 0.06809595 0.07274427 0.07798388 0.03691816 0.03664126 0.03845542
50% 0.06822459 0.07287291 0.07813413 0.03702071 0.03676099 0.03857609
75% 0.06827261 0.07293278 0.07820661 0.03706536 0.03680100 0.03861795
            V8         V9        V10
25% 0.01349659 0.05474798 0.06852327
50% 0.01357231 0.05488030 0.06866185
75% 0.01360791 0.05493732 0.06871687

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.2543414666  0.2378049920  0.1986127359  0.1216294125 -0.0009419706 
           V7            V8            V9           V10 
 0.0072603464 -0.0675049781  0.0563386356  0.2106798479 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5            V6          V7
25% 0.2534688 0.2369577 0.1979595 0.1209262 -0.0012146124 0.006921068
50% 0.2549642 0.2384833 0.1994213 0.1220037 -0.0004953279 0.007701926
75% 0.2556253 0.2391769 0.2001742 0.1226567 -0.0001403676 0.008091546
             V8         V9       V10
25% -0.06773454 0.05595600 0.2098310
50% -0.06734069 0.05691675 0.2112812
75% -0.06707381 0.05729508 0.2119137

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.25710452  0.21724955  0.22195255  0.07501712  0.02152507  0.01584217 
         V8          V9         V10 
-0.08681638  0.08172897  0.20633767 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6           V7
25% 0.2370713 0.2044806 0.1872121 0.04044611 -0.003743535 -0.007541645
50% 0.2544552 0.2151327 0.2357417 0.08892002  0.017497936  0.013793109
75% 0.2760494 0.2292572 0.2731916 0.11437773  0.046903282  0.038000789
             V8         V9       V10
25% -0.09851391 0.01997074 0.1783193
50% -0.08132588 0.07712237 0.2099229
75% -0.06743155 0.13303518 0.2341985

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.289724102  0.210721600  0.184094151  0.139067610 -0.017495359  0.001842855 
          V8           V9          V10 
-0.052714917  0.045446744  0.216499488 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5          V6           V7
25% 0.2851678 0.2060702 0.1794304 0.1306467 -0.02155094 -0.002022188
50% 0.2927424 0.2132609 0.1866236 0.1408321 -0.01698583  0.002350456
75% 0.2962311 0.2169848 0.1932918 0.1482417 -0.01371529  0.005439158
             V8         V9       V10
25% -0.05611092 0.04180414 0.2127179
50% -0.05065700 0.04664702 0.2190750
75% -0.04926610 0.05074124 0.2223507

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.233846318  0.226634467  0.206409063  0.100627793  0.007441127  0.013539106 
          V8           V9          V10 
-0.062001699  0.074482335  0.201659634 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5          V6         V7          V8
25% 0.2326308 0.2253804 0.2052663 0.09979509 0.006898434 0.01297525 -0.06241506
50% 0.2347298 0.2276256 0.2074824 0.10130170 0.008110032 0.01423665 -0.06182214
75% 0.2356826 0.2286350 0.2086060 0.10201407 0.008589791 0.01472805 -0.06139974
            V9       V10
25% 0.07379392 0.2004800
50% 0.07530568 0.2025954
75% 0.07593598 0.2034740

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.25935347  0.21536169  0.22191787  0.07875707  0.02103251  0.01424437 
         V8          V9         V10 
-0.08654236  0.08033097  0.20687035 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6           V7
25% 0.2396357 0.2019487 0.1870268 0.04663331 -0.002875171 -0.007679347
50% 0.2565647 0.2129829 0.2338131 0.09163605  0.017624414  0.012612971
75% 0.2775003 0.2266747 0.2761808 0.11544216  0.044998834  0.035590910
             V8         V9       V10
25% -0.09871204 0.02152940 0.1792906
50% -0.08102248 0.07365738 0.2095179
75% -0.06881632 0.13231024 0.2341597

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.231526244  0.223864456  0.206574324  0.097166496  0.008603625  0.014602054 
          V8           V9          V10 
-0.061397873  0.075997577  0.199906513 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5          V6         V7          V8
25% 0.2280199 0.2199942 0.2030429 0.09447424 0.006756341 0.01269486 -0.06287240
50% 0.2341707 0.2268326 0.2097814 0.09913761 0.010599497 0.01661714 -0.06091461
75% 0.2371237 0.2299077 0.2132080 0.10141878 0.012024815 0.01812548 -0.05962780
            V9       V10
25% 0.07355637 0.1963901
50% 0.07837830 0.2026997
75% 0.08047358 0.2054447

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.300994331  0.192893587  0.194863020  0.119591349  0.012622873  0.001858128 
          V8           V9          V10 
-0.080285844  0.083019452  0.199275848 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5            V6           V7
25% 0.2878621 0.1798050 0.1699211 0.1004780 -1.448791e-05 -0.010023653
50% 0.3019679 0.1900172 0.2034348 0.1188092  1.402350e-02  0.001880164
75% 0.3159642 0.2009866 0.2337264 0.1416516  2.702415e-02  0.015568251
             V8         V9       V10
25% -0.08842034 0.05061467 0.1814199
50% -0.07765833 0.08194782 0.1991645
75% -0.07048692 0.11546653 0.2182682

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.22653263  0.20577654  0.21336818  0.05601770  0.01547158  0.01930023 
         V8          V9         V10 
-0.07270033  0.08938213  0.19271920 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5          V6          V7
25% 0.2166703 0.1987721 0.1738366 -0.0121246 -0.01860791 -0.01317961
50% 0.2369534 0.2150685 0.2319042  0.0653951  0.01231507  0.01829893
75% 0.2562440 0.2303551 0.2823818  0.1151639  0.06139673  0.06227623
             V8         V9       V10
25% -0.09830779 0.02112637 0.1734030
50% -0.07281099 0.08069945 0.2024830
75% -0.05117251 0.13419359 0.2255833

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.23088987  0.20714622  0.21334506  0.04980540  0.01454965  0.01860461 
         V8          V9         V10 
-0.07918308  0.09298123  0.19470197 

 Quartiles of Marginal Effects:
 
           V2        V3        V4          V5          V6          V7
25% 0.2175332 0.1979179 0.1685470 -0.01629157 -0.01967428 -0.01447862
50% 0.2401414 0.2169277 0.2318546  0.06231064  0.01347831  0.01659118
75% 0.2589466 0.2349974 0.2884685  0.11820504  0.06374715  0.06449529
             V8         V9       V10
25% -0.10385695 0.01506538 0.1701455
50% -0.07405770 0.08566584 0.2038887
75% -0.04624381 0.15015891 0.2321951

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.25117699  0.22007719  0.21960098  0.06862452  0.02103755  0.01799561 
         V8          V9         V10 
-0.08904858  0.08742550  0.20714414 

 Quartiles of Marginal Effects:
 
           V2        V3        V4         V5           V6           V7
25% 0.2329526 0.2071215 0.1775580 0.02867681 -0.007310236 -0.009819811
50% 0.2474367 0.2200005 0.2321919 0.08765234  0.014726804  0.014432493
75% 0.2680297 0.2346873 0.2760905 0.11349687  0.048673220  0.043146152
             V8         V9       V10
25% -0.10278367 0.02288226 0.1785324
50% -0.08271380 0.08405035 0.2128719
75% -0.06662934 0.14047925 0.2361273

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.268709120  0.231041069  0.192729220  0.130405729 -0.006562869  0.005807925 
          V8           V9          V10 
-0.063558460  0.048680551  0.213462643 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5           V6          V7
25% 0.2668874 0.2292537 0.1912752 0.1288071 -0.007390453 0.004945145
50% 0.2699770 0.2323763 0.1944263 0.1311609 -0.005824851 0.006600819
75% 0.2713291 0.2338813 0.1960596 0.1328131 -0.004993022 0.007446715
             V8         V9       V10
25% -0.06423258 0.04793948 0.2117774
50% -0.06309319 0.04973058 0.2146502
75% -0.06259963 0.05063008 0.2159368

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.349045281  0.137340529  0.161305428  0.161993326  0.021070202  0.003878507 
          V8           V9          V10 
-0.093377625  0.088014331  0.204447426 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5         V6           V7          V8
25% 0.3398194 0.1238196 0.1387070 0.1419548 0.01231102 -0.004617565 -0.10024369
50% 0.3512826 0.1342633 0.1664352 0.1607486 0.02141550  0.003640687 -0.09035685
75% 0.3603257 0.1457672 0.1933836 0.1814928 0.03041072  0.013122234 -0.08486074
            V9       V10
25% 0.07172372 0.1935095
50% 0.08987381 0.2048903
75% 0.10807258 0.2159672
Radial Basis Function Kernel Regularized Least Squares 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  lambda        sigma        RMSE          Rsquared   MAE           Selected
  0.0000470198    16.402338  0.0005100622  0.9999935  0.0003581395          
  0.0001096559   103.787829  0.0004366287  0.9999953  0.0003380329  *       
  0.0005013972    14.242271  0.0008356970  0.9999825  0.0004954103          
  0.0005810891     6.296877  0.0016323274  0.9999319  0.0006995667          
  0.0005818392    44.339212  0.0006104814  0.9999907  0.0004356772          
  0.0006569001     9.650473  0.0011437248  0.9999672  0.0005873221          
  0.0007804667    11.755634  0.0010439041  0.9999727  0.0005712344          
  0.0008028356   932.173295  0.0024593354  0.9998489  0.0017060277          
  0.0074964069   165.251595  0.0024717034  0.9998492  0.0018233289          
  0.0144189761     1.563424  0.0119644350  0.9960343  0.0042535180          
  0.0184490471   334.036446  0.0050868851  0.9993611  0.0036090227          
  0.0185744369  1027.014817  0.0056228366  0.9992398  0.0039079087          
  0.0217360307  2072.617678  0.0065944468  0.9990540  0.0047051200          
  0.0803892244    14.399852  0.0046603028  0.9994751  0.0026278843          
  0.0961691112     2.835799  0.0101162462  0.9975760  0.0043433305          
  0.1153348289  1342.589950  0.0117518197  0.9979806  0.0090632311          
  0.1437862388   457.021421  0.0080821369  0.9986846  0.0056011794          
  0.1911338598    12.176776  0.0070797183  0.9988132  0.0039887579          
  0.3583985477   428.594288  0.0122948348  0.9977537  0.0090587327          
  0.9607653197  8139.853416  0.1248105282  0.9237230  0.1035780164          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 0.0001096559 and sigma
 = 103.7878.
[1] "Mon Mar 12 10:00:14 2018"
Least Angle Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  fraction   RMSE          Rsquared   MAE           Selected
  0.1344562  0.0982985365  0.9711931  0.0805162039          
  0.2080064  0.0451690661  0.9933242  0.0370348109          
  0.3400364  0.0034893439  0.9996980  0.0024690977          
  0.3528485  0.0033766191  0.9997171  0.0023857794          
  0.3529606  0.0033756506  0.9997173  0.0023850526          
  0.3634999  0.0032838902  0.9997324  0.0023170463          
  0.3784709  0.0031506598  0.9997536  0.0022225933          
  0.3809253  0.0031291987  0.9997569  0.0022077011          
  0.5749706  0.0019170387  0.9999085  0.0013582184          
  0.6317869  0.0016520727  0.9999320  0.0011838986          
  0.6531948  0.0015659907  0.9999389  0.0011217028          
  0.6537831  0.0015636677  0.9999390  0.0011200325          
  0.6674360  0.0015104608  0.9999431  0.0010816012          
  0.7810396  0.0011417037  0.9999675  0.0008332582          
  0.7966071  0.0011057895  0.9999695  0.0008158438          
  0.8123921  0.0010742705  0.9999712  0.0008044643          
  0.8315435  0.0010432869  0.9999729  0.0007943508          
  0.8562675  0.0010184046  0.9999741  0.0007882639          
  0.9108732  0.0009881475  0.9999757  0.0007721171          
  0.9965235  0.0009637097  0.9999768  0.0007574072  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was fraction = 0.9965235.
[1] "Mon Mar 12 10:00:20 2018"
Least Angle Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  step  RMSE         Rsquared   MAE          Selected
  2     0.095149153  0.9711931  0.077884666          
  4     0.011019945  0.9976624  0.008462400          
  6     0.005330694  0.9993321  0.003973407          
  7     0.004318251  0.9995591  0.003187540          
  8     0.004148191  0.9995762  0.003070276          
  9     0.003813859  0.9996396  0.002725813  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was step = 9.
[1] "Mon Mar 12 10:00:25 2018"
The lasso 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  fraction   RMSE          Rsquared   MAE           Selected
  0.1344562  0.0982985365  0.9711931  0.0805162039          
  0.2080064  0.0451690661  0.9933242  0.0370348109          
  0.3400364  0.0034893439  0.9996980  0.0024690977          
  0.3528485  0.0033766191  0.9997171  0.0023857794          
  0.3529606  0.0033756506  0.9997173  0.0023850526          
  0.3634999  0.0032838902  0.9997324  0.0023170463          
  0.3784709  0.0031506598  0.9997536  0.0022225933          
  0.3809253  0.0031291987  0.9997569  0.0022077011          
  0.5749706  0.0019170387  0.9999085  0.0013582184          
  0.6317869  0.0016520727  0.9999320  0.0011838986          
  0.6531948  0.0015659907  0.9999389  0.0011217028          
  0.6537831  0.0015636677  0.9999390  0.0011200325          
  0.6674360  0.0015104608  0.9999431  0.0010816012          
  0.7810396  0.0011417037  0.9999675  0.0008332582          
  0.7966071  0.0011057895  0.9999695  0.0008158438          
  0.8123921  0.0010742705  0.9999712  0.0008044643          
  0.8315435  0.0010432869  0.9999729  0.0007943508          
  0.8562675  0.0010184046  0.9999741  0.0007882639          
  0.9108732  0.0009881475  0.9999757  0.0007721171          
  0.9965235  0.0009637097  0.9999768  0.0007574072  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was fraction = 0.9965235.
[1] "Mon Mar 12 10:00:31 2018"
Linear Regression with Backwards Selection 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  nvmax  RMSE         Rsquared   MAE           Selected
  2      0.014756704  0.9946340  0.0116226685          
  3      0.013032674  0.9957814  0.0104765193          
  4      0.004286206  0.9995420  0.0033986577          
  6      0.001695711  0.9999285  0.0013366020          
  7      0.001391581  0.9999519  0.0010920583          
  8      0.001012850  0.9999744  0.0007901607  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nvmax = 8.
[1] "Mon Mar 12 10:00:37 2018"
Linear Regression with Forward Selection 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  nvmax  RMSE          Rsquared   MAE           Selected
  2      0.0164890364  0.9933226  0.0125755036          
  3      0.0099249204  0.9975686  0.0072435563          
  4      0.0070126124  0.9987723  0.0053681942          
  6      0.0030429047  0.9997329  0.0022377836          
  7      0.0013246351  0.9999566  0.0010279163          
  8      0.0009918017  0.9999755  0.0007857797  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nvmax = 8.
[1] "Mon Mar 12 10:00:42 2018"
Linear Regression with Stepwise Selection 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  nvmax  RMSE         Rsquared   MAE          Selected
  2      0.014756704  0.9946340  0.011622668          
  3      0.007895827  0.9984469  0.006511386          
  4      0.005059492  0.9993524  0.003839497          
  6      0.001642560  0.9999332  0.001286989          
  7      0.002661079  0.9998263  0.002035181          
  8      0.001491222  0.9999345  0.001146692  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nvmax = 8.
[1] "Mon Mar 12 10:00:48 2018"
Linear Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results:

  RMSE          Rsquared   MAE        
  0.0009634527  0.9999769  0.000757459

Tuning parameter 'intercept' was held constant at a value of TRUE
[1] "Mon Mar 12 10:00:54 2018"
Start:  AIC=-6958.17
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq       RSS     AIC
<none>              0.0004475 -6958.2
- V7    1 0.0000286 0.0004761 -6929.1
- V8    1 0.0000451 0.0004926 -6912.0
- V6    1 0.0000582 0.0005057 -6898.9
- V9    1 0.0003470 0.0007945 -6672.5
- V4    1 0.0007815 0.0012289 -6454.0
- V2    1 0.0013775 0.0018250 -6255.9
- V10   1 0.0028098 0.0032573 -5965.7
- V5    1 0.0031660 0.0036135 -5913.7
- V3    1 0.0043720 0.0048195 -5769.4
Start:  AIC=-7014.08
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq       RSS     AIC
<none>              0.0004249 -7014.1
- V8    1 0.0000357 0.0004606 -6975.5
- V7    1 0.0000382 0.0004631 -6972.8
- V6    1 0.0000709 0.0004958 -6938.4
- V9    1 0.0003861 0.0008110 -6690.9
- V4    1 0.0008184 0.0012433 -6476.0
- V2    1 0.0013672 0.0017921 -6292.1
- V10   1 0.0025939 0.0030188 -6029.8
- V5    1 0.0029926 0.0034174 -5967.4
- V3    1 0.0039820 0.0044069 -5839.5
Start:  AIC=-6928.45
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq       RSS     AIC
<none>              0.0004609 -6928.5
- V8    1 0.0000300 0.0004909 -6898.9
- V7    1 0.0000471 0.0005080 -6881.8
- V6    1 0.0000844 0.0005453 -6846.4
- V9    1 0.0003949 0.0008558 -6621.0
- V4    1 0.0007628 0.0012237 -6442.2
- V2    1 0.0015313 0.0019922 -6198.6
- V10   1 0.0025875 0.0030484 -5985.9
- V5    1 0.0029907 0.0034516 -5923.8
- V3    1 0.0038757 0.0043366 -5809.6
Start:  AIC=-10453.9
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq       RSS      AIC
<none>              0.0006720 -10453.9
- V8    1 0.0000548 0.0007268 -10396.9
- V7    1 0.0000581 0.0007300 -10393.6
- V6    1 0.0001089 0.0007808 -10343.0
- V9    1 0.0005663 0.0012383  -9996.2
- V4    1 0.0011832 0.0018551  -9692.2
- V2    1 0.0021536 0.0028256  -9375.8
- V10   1 0.0040319 0.0047039  -8992.5
- V5    1 0.0046135 0.0052855  -8904.9
- V3    1 0.0061918 0.0068638  -8708.4
Linear Regression with Stepwise Selection 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results:

  RMSE          Rsquared   MAE        
  0.0009634527  0.9999769  0.000757459

[1] "Mon Mar 12 10:01:00 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :18    NA's   :18    NA's   :18   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 10:01:09 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "logicBag"                       
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 10:01:17 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "logreg"                         
Model Tree 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  pruned  smoothed  rules  RMSE         Rsquared   MAE          Selected
  Yes     Yes       Yes    0.001599669  0.9999358  0.001193404          
  Yes     Yes       No     0.001589495  0.9999367  0.001189419  *       
  Yes     No        Yes    0.001603970  0.9999354  0.001193676          
  Yes     No        No     0.001603970  0.9999354  0.001193676          
  No      Yes       Yes    0.010514613  0.9970917  0.006217863          
  No      Yes       No     0.005483299  0.9992570  0.004068096          
  No      No        Yes    0.026076805  0.9833943  0.019813905          
  No      No        No     0.020091181  0.9900093  0.015651170          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were pruned = Yes, smoothed = Yes and
 rules = No.
[1] "Mon Mar 12 10:01:33 2018"
Model Rules 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  pruned  smoothed  RMSE         Rsquared   MAE          Selected
  Yes     Yes       0.001599669  0.9999358  0.001193404  *       
  Yes     No        0.001603970  0.9999354  0.001193676          
  No      Yes       0.010514613  0.9970917  0.006217863          
  No      No        0.026076805  0.9833943  0.019813905          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were pruned = Yes and smoothed = Yes.
[1] "Mon Mar 12 10:01:46 2018"
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 10:01:52 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "mlpKerasDecay"                  
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 10:02:02 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "mlpKerasDropout"                
Multi-Step Adaptive MCP-Net 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  alphas     nsteps  scale      RMSE        Rsquared   MAE         Selected
  0.1710105   8      1.0542918  0.06229274  0.9062721  0.05215087  *       
  0.2372057   6      1.0033500  0.06229287  0.9062721  0.05214936          
  0.3560327   8      0.5457514  0.06229302  0.9062721  0.05214805          
  0.3675637   9      3.8583856  0.06229303  0.9062721  0.05214797          
  0.3676645   7      0.6816630  0.06229303  0.9062721  0.05214797          
  0.3771499   8      2.9085273  0.06229304  0.9062721  0.05214790          
  0.3906238   8      3.3501277  0.06229306  0.9062721  0.05214782          
  0.3928328   4      1.5162653  0.06229306  0.9062721  0.05214780          
  0.5674736   6      2.3472094  0.06229317  0.9062721  0.05214708          
  0.6186082  10      2.8060079  0.06229319  0.9062721  0.05214694          
  0.6378753   5      1.3835867  0.06229319  0.9062721  0.05214690          
  0.6384048   4      3.9983871  0.06229319  0.9062721  0.05214689          
  0.6506924   3      3.8135375  0.06229320  0.9062721  0.05214687          
  0.7529356   8      0.5941996  0.06229323  0.9062721  0.05214667          
  0.7669464   9      0.5750654  0.06229324  0.9062721  0.05214665          
  0.7811529   3      1.8461318  0.06229324  0.9062721  0.05214663          
  0.7983891   5      3.4304802  0.06229324  0.9062721  0.05214660          
  0.8206408   8      0.7758591  0.06229325  0.9062721  0.05214657          
  0.8697859   5      1.3077996  0.06229326  0.9062721  0.05214651          
  0.9468711   2      0.7194458  0.06229327  0.9062721  0.05214642          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were alphas = 0.1710105, nsteps = 8
 and scale = 1.054292.
[1] "Mon Mar 12 10:02:18 2018"
Installing package into 'C:/Users/John/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Installing package into 'C:/Users/John/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
# weights:  199
initial  value 55.650720 
iter  10 value 4.905642
iter  20 value 4.389647
iter  30 value 4.346551
iter  40 value 4.326219
iter  50 value 4.314588
iter  60 value 4.314159
iter  70 value 4.314028
final  value 4.314028 
converged
# weights:  89
initial  value 51.256747 
iter  10 value 0.771462
iter  20 value 0.169988
iter  30 value 0.130043
iter  40 value 0.092491
iter  50 value 0.081174
iter  60 value 0.069051
iter  70 value 0.064975
iter  80 value 0.062090
iter  90 value 0.060575
iter 100 value 0.058742
final  value 0.058742 
stopped after 100 iterations
# weights:  133
initial  value 66.219171 
iter  10 value 2.031082
iter  20 value 0.705882
iter  30 value 0.402261
iter  40 value 0.324316
iter  50 value 0.297369
iter  60 value 0.289672
iter  70 value 0.284608
iter  80 value 0.281031
iter  90 value 0.279317
iter 100 value 0.277884
final  value 0.277884 
stopped after 100 iterations
# weights:  34
initial  value 26.307706 
iter  10 value 4.597787
iter  20 value 3.649959
iter  30 value 3.591687
iter  40 value 3.588649
final  value 3.588444 
converged
# weights:  188
initial  value 134.348792 
iter  10 value 16.758790
iter  20 value 0.670402
iter  30 value 0.296243
iter  40 value 0.208910
iter  50 value 0.167850
iter  60 value 0.136586
iter  70 value 0.124834
iter  80 value 0.114916
iter  90 value 0.111465
iter 100 value 0.109036
final  value 0.109036 
stopped after 100 iterations
# weights:  56
initial  value 23.010925 
iter  10 value 0.629092
iter  20 value 0.483835
iter  30 value 0.460378
iter  40 value 0.446932
iter  50 value 0.442989
iter  60 value 0.439803
iter  70 value 0.436121
iter  80 value 0.433760
iter  90 value 0.432773
iter 100 value 0.432283
final  value 0.432283 
stopped after 100 iterations
# weights:  78
initial  value 37.834340 
iter  10 value 5.102257
iter  20 value 4.082274
iter  30 value 3.995670
iter  40 value 3.985243
iter  50 value 3.979314
iter  60 value 3.977816
iter  70 value 3.977590
final  value 3.977588 
converged
# weights:  177
initial  value 25.698602 
iter  10 value 4.097592
iter  20 value 3.632246
iter  30 value 3.568765
iter  40 value 3.526522
iter  50 value 3.502820
iter  60 value 3.496359
iter  70 value 3.495321
iter  80 value 3.494932
final  value 3.494932 
converged
# weights:  221
initial  value 38.529036 
iter  10 value 0.689970
iter  20 value 0.087841
iter  30 value 0.047232
iter  40 value 0.028120
iter  50 value 0.020245
iter  60 value 0.017571
iter  70 value 0.015676
iter  80 value 0.014167
iter  90 value 0.012675
iter 100 value 0.011640
final  value 0.011640 
stopped after 100 iterations
# weights:  155
initial  value 19.747657 
iter  10 value 0.222508
iter  20 value 0.159436
iter  30 value 0.127756
iter  40 value 0.108375
iter  50 value 0.071199
iter  60 value 0.052719
iter  70 value 0.036083
iter  80 value 0.029859
iter  90 value 0.027977
iter 100 value 0.026372
final  value 0.026372 
stopped after 100 iterations
# weights:  89
initial  value 26.301931 
iter  10 value 7.123517
iter  20 value 6.392288
iter  30 value 6.251701
iter  40 value 6.232711
iter  50 value 6.227530
final  value 6.227377 
converged
# weights:  155
initial  value 60.515326 
iter  10 value 0.286675
iter  20 value 0.229982
iter  30 value 0.208350
iter  40 value 0.198191
iter  50 value 0.193840
iter  60 value 0.184128
iter  70 value 0.171864
iter  80 value 0.165346
iter  90 value 0.160496
iter 100 value 0.156460
final  value 0.156460 
stopped after 100 iterations
# weights:  188
initial  value 36.577784 
iter  10 value 0.198504
iter  20 value 0.159727
iter  30 value 0.124470
iter  40 value 0.095114
iter  50 value 0.066511
iter  60 value 0.053128
iter  70 value 0.044672
iter  80 value 0.041918
iter  90 value 0.039843
iter 100 value 0.037951
final  value 0.037951 
stopped after 100 iterations
# weights:  89
initial  value 29.462065 
iter  10 value 5.621707
iter  20 value 4.941950
iter  30 value 4.928997
iter  40 value 4.926597
iter  50 value 4.926278
final  value 4.926277 
converged
# weights:  210
initial  value 50.842258 
iter  10 value 0.252880
iter  20 value 0.204971
iter  30 value 0.178508
iter  40 value 0.152354
iter  50 value 0.139728
iter  60 value 0.135716
iter  70 value 0.131397
iter  80 value 0.127155
iter  90 value 0.121530
iter 100 value 0.117637
final  value 0.117637 
stopped after 100 iterations
# weights:  89
initial  value 26.983371 
iter  10 value 1.876569
iter  20 value 1.033823
iter  30 value 0.994608
iter  40 value 0.981181
iter  50 value 0.976689
iter  60 value 0.972240
iter  70 value 0.969398
iter  80 value 0.968495
iter  90 value 0.967917
iter 100 value 0.967717
final  value 0.967717 
stopped after 100 iterations
# weights:  177
initial  value 95.743136 
iter  10 value 19.456449
iter  20 value 19.029219
iter  30 value 19.022407
iter  40 value 19.019877
iter  50 value 19.019277
final  value 19.019271 
converged
# weights:  144
initial  value 134.911363 
iter  10 value 20.515460
iter  20 value 20.112358
iter  30 value 20.103848
final  value 20.103830 
converged
# weights:  89
initial  value 58.256501 
iter  10 value 10.253369
iter  20 value 10.129208
iter  30 value 10.126680
final  value 10.126679 
converged
# weights:  155
initial  value 22.510178 
iter  10 value 0.203602
iter  20 value 0.167498
iter  30 value 0.141061
iter  40 value 0.123841
iter  50 value 0.096626
iter  60 value 0.081097
iter  70 value 0.068090
iter  80 value 0.061239
iter  90 value 0.057120
iter 100 value 0.054716
final  value 0.054716 
stopped after 100 iterations
# weights:  199
initial  value 22.835565 
iter  10 value 4.544505
iter  20 value 4.360174
iter  30 value 4.353859
iter  40 value 4.352679
iter  50 value 4.352248
final  value 4.352247 
converged
# weights:  89
initial  value 22.095633 
iter  10 value 0.279717
iter  20 value 0.197689
iter  30 value 0.117379
iter  40 value 0.077187
iter  50 value 0.063467
iter  60 value 0.058993
iter  70 value 0.056132
iter  80 value 0.054851
iter  90 value 0.052968
iter 100 value 0.051566
final  value 0.051566 
stopped after 100 iterations
# weights:  133
initial  value 22.834028 
iter  10 value 1.492570
iter  20 value 0.570091
iter  30 value 0.411512
iter  40 value 0.335074
iter  50 value 0.311932
iter  60 value 0.300540
iter  70 value 0.295384
iter  80 value 0.292516
iter  90 value 0.289748
iter 100 value 0.287351
final  value 0.287351 
stopped after 100 iterations
# weights:  34
initial  value 22.914884 
iter  10 value 4.748443
iter  20 value 3.819457
iter  30 value 3.794074
iter  40 value 3.793372
final  value 3.793367 
converged
# weights:  188
initial  value 44.981340 
iter  10 value 0.266077
iter  20 value 0.211656
iter  30 value 0.191875
iter  40 value 0.177607
iter  50 value 0.154778
iter  60 value 0.134827
iter  70 value 0.123016
iter  80 value 0.112121
iter  90 value 0.108399
iter 100 value 0.106430
final  value 0.106430 
stopped after 100 iterations
# weights:  56
initial  value 25.169543 
iter  10 value 1.487254
iter  20 value 0.712339
iter  30 value 0.558783
iter  40 value 0.488005
iter  50 value 0.465365
iter  60 value 0.461594
iter  70 value 0.460043
iter  80 value 0.457185
iter  90 value 0.454812
iter 100 value 0.449376
final  value 0.449376 
stopped after 100 iterations
# weights:  78
initial  value 23.115600 
iter  10 value 4.404276
iter  20 value 4.089173
iter  30 value 4.039962
iter  40 value 3.967883
iter  50 value 3.916594
iter  60 value 3.914073
iter  70 value 3.914017
final  value 3.914016 
converged
# weights:  177
initial  value 68.005917 
iter  10 value 4.355001
iter  20 value 3.703880
iter  30 value 3.613165
iter  40 value 3.567559
iter  50 value 3.537043
iter  60 value 3.525413
iter  70 value 3.524877
iter  80 value 3.524722
iter  90 value 3.524688
final  value 3.524688 
converged
# weights:  221
initial  value 67.286037 
iter  10 value 1.115968
iter  20 value 0.158290
iter  30 value 0.078947
iter  40 value 0.043052
iter  50 value 0.026088
iter  60 value 0.016374
iter  70 value 0.013834
iter  80 value 0.013206
iter  90 value 0.011934
iter 100 value 0.010875
final  value 0.010875 
stopped after 100 iterations
# weights:  155
initial  value 25.566582 
iter  10 value 0.221990
iter  20 value 0.173017
iter  30 value 0.145235
iter  40 value 0.122192
iter  50 value 0.096967
iter  60 value 0.057626
iter  70 value 0.046060
iter  80 value 0.034459
iter  90 value 0.030581
iter 100 value 0.028456
final  value 0.028456 
stopped after 100 iterations
# weights:  89
initial  value 111.828427 
iter  10 value 7.265981
iter  20 value 6.364295
iter  30 value 6.310509
iter  40 value 6.303903
iter  50 value 6.303434
iter  50 value 6.303434
iter  50 value 6.303434
final  value 6.303434 
converged
# weights:  155
initial  value 59.888345 
iter  10 value 0.455564
iter  20 value 0.253016
iter  30 value 0.208437
iter  40 value 0.184502
iter  50 value 0.168758
iter  60 value 0.160059
iter  70 value 0.155625
iter  80 value 0.153072
iter  90 value 0.151492
iter 100 value 0.150317
final  value 0.150317 
stopped after 100 iterations
# weights:  188
initial  value 64.350015 
iter  10 value 0.246747
iter  20 value 0.168809
iter  30 value 0.117334
iter  40 value 0.081766
iter  50 value 0.068115
iter  60 value 0.055603
iter  70 value 0.045039
iter  80 value 0.042024
iter  90 value 0.040720
iter 100 value 0.039441
final  value 0.039441 
stopped after 100 iterations
# weights:  89
initial  value 51.650675 
iter  10 value 5.964347
iter  20 value 5.173374
iter  30 value 5.005143
iter  40 value 4.981773
iter  50 value 4.978212
iter  60 value 4.977604
final  value 4.977603 
converged
# weights:  210
initial  value 30.811472 
iter  10 value 0.243017
iter  20 value 0.206934
iter  30 value 0.191406
iter  40 value 0.172620
iter  50 value 0.150843
iter  60 value 0.136603
iter  70 value 0.123775
iter  80 value 0.117834
iter  90 value 0.115190
iter 100 value 0.113294
final  value 0.113294 
stopped after 100 iterations
# weights:  89
initial  value 44.244794 
iter  10 value 1.862494
iter  20 value 1.028264
iter  30 value 0.964195
iter  40 value 0.949715
iter  50 value 0.945925
iter  60 value 0.942172
iter  70 value 0.937693
iter  80 value 0.934350
iter  90 value 0.933629
iter 100 value 0.933399
final  value 0.933399 
stopped after 100 iterations
# weights:  177
initial  value 78.562418 
iter  10 value 19.772711
iter  20 value 19.481318
iter  30 value 19.476562
iter  40 value 19.465502
iter  50 value 19.461264
final  value 19.461260 
converged
# weights:  144
initial  value 186.771657 
iter  10 value 22.807168
iter  20 value 20.908734
iter  30 value 20.867988
final  value 20.867945 
converged
# weights:  89
initial  value 51.904053 
iter  10 value 10.716748
iter  20 value 10.345609
iter  30 value 10.274539
final  value 10.273972 
converged
# weights:  155
initial  value 28.071206 
iter  10 value 0.289184
iter  20 value 0.174305
iter  30 value 0.141991
iter  40 value 0.112004
iter  50 value 0.076226
iter  60 value 0.064757
iter  70 value 0.056248
iter  80 value 0.051891
iter  90 value 0.049078
iter 100 value 0.047865
final  value 0.047865 
stopped after 100 iterations
# weights:  199
initial  value 27.780993 
iter  10 value 4.621060
iter  20 value 4.286823
iter  30 value 4.264558
iter  40 value 4.255790
iter  50 value 4.254824
iter  60 value 4.254510
final  value 4.254510 
converged
# weights:  89
initial  value 26.046267 
iter  10 value 0.337745
iter  20 value 0.137646
iter  30 value 0.084551
iter  40 value 0.072075
iter  50 value 0.060789
iter  60 value 0.058703
iter  70 value 0.056142
iter  80 value 0.053840
iter  90 value 0.052714
iter 100 value 0.051666
final  value 0.051666 
stopped after 100 iterations
# weights:  133
initial  value 35.682822 
iter  10 value 1.007171
iter  20 value 0.432556
iter  30 value 0.317300
iter  40 value 0.280750
iter  50 value 0.272385
iter  60 value 0.267811
iter  70 value 0.265453
iter  80 value 0.263934
iter  90 value 0.263382
iter 100 value 0.262631
final  value 0.262631 
stopped after 100 iterations
# weights:  34
initial  value 28.447347 
iter  10 value 4.615854
iter  20 value 3.909473
iter  30 value 3.731778
iter  40 value 3.710990
iter  50 value 3.707362
final  value 3.707355 
converged
# weights:  188
initial  value 58.258518 
iter  10 value 0.248417
iter  20 value 0.187910
iter  30 value 0.169458
iter  40 value 0.151018
iter  50 value 0.132066
iter  60 value 0.125484
iter  70 value 0.119598
iter  80 value 0.113959
iter  90 value 0.110174
iter 100 value 0.106167
final  value 0.106167 
stopped after 100 iterations
# weights:  56
initial  value 35.227793 
iter  10 value 0.676933
iter  20 value 0.487260
iter  30 value 0.448450
iter  40 value 0.436906
iter  50 value 0.427389
iter  60 value 0.413486
iter  70 value 0.408687
iter  80 value 0.406748
iter  90 value 0.406083
iter 100 value 0.404953
final  value 0.404953 
stopped after 100 iterations
# weights:  78
initial  value 26.186060 
iter  10 value 4.214924
iter  20 value 3.850707
iter  30 value 3.843276
iter  40 value 3.841065
iter  50 value 3.840637
final  value 3.840634 
converged
# weights:  177
initial  value 53.704319 
iter  10 value 3.920402
iter  20 value 3.618096
iter  30 value 3.574103
iter  40 value 3.519164
iter  50 value 3.508932
iter  60 value 3.493202
iter  70 value 3.477734
iter  80 value 3.474883
final  value 3.474866 
converged
# weights:  221
initial  value 32.897912 
iter  10 value 0.161827
iter  20 value 0.142251
iter  30 value 0.107918
iter  40 value 0.078585
iter  50 value 0.044475
iter  60 value 0.035443
iter  70 value 0.025738
iter  80 value 0.018123
iter  90 value 0.016193
iter 100 value 0.014293
final  value 0.014293 
stopped after 100 iterations
# weights:  155
initial  value 61.553173 
iter  10 value 0.722262
iter  20 value 0.110126
iter  30 value 0.054178
iter  40 value 0.039863
iter  50 value 0.034945
iter  60 value 0.032635
iter  70 value 0.031067
iter  80 value 0.028269
iter  90 value 0.027415
iter 100 value 0.026640
final  value 0.026640 
stopped after 100 iterations
# weights:  89
initial  value 30.360183 
iter  10 value 7.061418
iter  20 value 6.445192
iter  30 value 6.339561
iter  40 value 6.239052
iter  50 value 6.154567
iter  60 value 6.137861
iter  70 value 6.136724
final  value 6.136722 
converged
# weights:  155
initial  value 101.087143 
iter  10 value 0.250193
iter  20 value 0.201982
iter  30 value 0.187224
iter  40 value 0.182728
iter  50 value 0.179941
iter  60 value 0.174493
iter  70 value 0.165430
iter  80 value 0.159586
iter  90 value 0.155483
iter 100 value 0.152218
final  value 0.152218 
stopped after 100 iterations
# weights:  188
initial  value 20.330524 
iter  10 value 0.187372
iter  20 value 0.158000
iter  30 value 0.120596
iter  40 value 0.096959
iter  50 value 0.066114
iter  60 value 0.054226
iter  70 value 0.047409
iter  80 value 0.041635
iter  90 value 0.039257
iter 100 value 0.036903
final  value 0.036903 
stopped after 100 iterations
# weights:  89
initial  value 43.889251 
iter  10 value 5.908680
iter  20 value 5.022383
iter  30 value 4.862595
iter  40 value 4.822130
iter  50 value 4.810333
iter  60 value 4.808566
final  value 4.808558 
converged
# weights:  210
initial  value 128.807025 
iter  10 value 125.674418
iter  20 value 2.110972
iter  30 value 0.334261
iter  40 value 0.225320
iter  50 value 0.186520
iter  60 value 0.170405
iter  70 value 0.158187
iter  80 value 0.141074
iter  90 value 0.130573
iter 100 value 0.120875
final  value 0.120875 
stopped after 100 iterations
# weights:  89
initial  value 23.186027 
iter  10 value 1.532233
iter  20 value 1.032885
iter  30 value 0.953805
iter  40 value 0.934037
iter  50 value 0.928551
iter  60 value 0.923734
iter  70 value 0.921315
iter  80 value 0.916795
iter  90 value 0.908720
iter 100 value 0.903104
final  value 0.903104 
stopped after 100 iterations
# weights:  177
initial  value 100.924421 
iter  10 value 18.796274
iter  20 value 18.602233
iter  30 value 18.591287
iter  40 value 18.588633
iter  50 value 18.588050
final  value 18.587994 
converged
# weights:  144
initial  value 142.558350 
iter  10 value 21.296153
iter  20 value 19.594760
iter  30 value 19.585149
final  value 19.585136 
converged
# weights:  89
initial  value 39.584137 
iter  10 value 10.204127
iter  20 value 10.129553
iter  30 value 10.128256
final  value 10.128249 
converged
# weights:  155
initial  value 21.580223 
iter  10 value 0.458712
iter  20 value 0.189930
iter  30 value 0.095293
iter  40 value 0.078143
iter  50 value 0.065254
iter  60 value 0.060077
iter  70 value 0.055632
iter  80 value 0.051882
iter  90 value 0.049243
iter 100 value 0.047960
final  value 0.047960 
stopped after 100 iterations
# weights:  221
initial  value 82.337539 
iter  10 value 0.278780
iter  20 value 0.207756
iter  30 value 0.177070
iter  40 value 0.136827
iter  50 value 0.080447
iter  60 value 0.051638
iter  70 value 0.030279
iter  80 value 0.019495
iter  90 value 0.017175
iter 100 value 0.015494
final  value 0.015494 
stopped after 100 iterations
Neural Network 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  size  decay         RMSE         Rsquared   MAE          Selected
   3    0.1505363399  0.034068644  0.9892979  0.022004550          
   5    0.0094575873  0.019841990  0.9908443  0.013656721          
   7    0.1860506215  0.033891553  0.9914277  0.023190118          
   8    0.0003513622  0.006277935  0.9990444  0.003622252          
   8    0.0338702219  0.020471825  0.9913276  0.013023716          
   8    0.2481023680  0.039894405  0.9910479  0.028946229          
   8    0.3335624159  0.048678749  0.9890652  0.037058047          
   8    0.6328665552  0.076714212  0.9811505  0.061764330          
  12    0.0047074044  0.017620311  0.9926427  0.012293033          
  13    5.1154613290  0.200440395  0.9159657  0.165428939          
  14    0.0001059793  0.004770615  0.9994476  0.002851955          
  14    0.0003038330  0.006235011  0.9990609  0.003487766          
  14    0.0016379824  0.010162487  0.9975325  0.006101887          
  16    0.1830050068  0.030755242  0.9922339  0.021079972          
  16    2.0940486010  0.161543001  0.9320581  0.133537316          
  17    0.0002032754  0.005571064  0.9992458  0.003252552          
  17    0.0010235172  0.008008078  0.9984689  0.004442012          
  18    0.2353431183  0.035539471  0.9915316  0.025695105          
  19    0.0011270169  0.008612625  0.9982282  0.004820484          
  20    0.0000136168  0.004358617  0.9995118  0.002757643  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 20 and decay = 1.36168e-05.
[1] "Mon Mar 12 10:02:54 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: package 'mxnet' is not available (for R version 3.4.2) 
2: package 'mxnet' is not available (for R version 3.4.2) 
3: predictions failed for Fold1: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments
 
4: predictions failed for Fold2: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments
 
5: predictions failed for Fold3: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments
 
6: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: predictions failed for Fold1: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments
 
2: predictions failed for Fold2: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments
 
3: predictions failed for Fold3: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 10:03:01 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "nnls"                           
Non-Informative Model 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results:

  RMSE      Rsquared  MAE     
  0.200517  NaN       0.165522

[1] "Mon Mar 12 10:03:06 2018"
Error : The tuning parameter grid should have columns alpha, criteria, link
In addition: Warning message:
In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Error : The tuning parameter grid should have columns alpha, criteria, link
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :12    NA's   :12    NA's   :12   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 10:03:14 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "ordinalNet"                     
Parallel Random Forest 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE          Selected
  2     0.01180940  0.9969341  0.008759787          
  4     0.01075072  0.9972696  0.008038364  *       
  6     0.01110734  0.9970074  0.008396720          
  7     0.01159692  0.9967193  0.008791802          
  8     0.01194981  0.9965004  0.009062079          
  9     0.01250328  0.9961561  0.009502693          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 4.
[1] "Mon Mar 12 10:03:56 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
# weights:  109
initial  value 53.638432 
iter  10 value 2.234013
iter  20 value 1.224521
iter  30 value 1.175450
iter  40 value 1.164081
iter  50 value 1.163329
iter  60 value 1.163202
iter  70 value 1.163184
final  value 1.163183 
converged
# weights:  49
initial  value 38.912730 
iter  10 value 0.305076
iter  20 value 0.157461
iter  30 value 0.098116
iter  40 value 0.074853
iter  50 value 0.063795
iter  60 value 0.052273
iter  70 value 0.045207
iter  80 value 0.039228
iter  90 value 0.035479
iter 100 value 0.034579
final  value 0.034579 
stopped after 100 iterations
# weights:  73
initial  value 51.572751 
iter  10 value 0.683248
iter  20 value 0.241417
iter  30 value 0.164179
iter  40 value 0.142779
iter  50 value 0.136367
iter  60 value 0.133958
iter  70 value 0.132734
iter  80 value 0.131731
iter  90 value 0.131147
iter 100 value 0.130660
final  value 0.130660 
stopped after 100 iterations
# weights:  19
initial  value 40.571867 
iter  10 value 3.649360
iter  20 value 1.507569
iter  30 value 1.370611
iter  40 value 1.361006
iter  50 value 1.359499
final  value 1.359492 
converged
# weights:  103
initial  value 62.895554 
iter  10 value 0.569273
iter  20 value 0.163978
iter  30 value 0.091428
iter  40 value 0.068294
iter  50 value 0.062091
iter  60 value 0.058889
iter  70 value 0.057084
iter  80 value 0.056066
iter  90 value 0.055223
iter 100 value 0.054387
final  value 0.054387 
stopped after 100 iterations
# weights:  31
initial  value 77.210409 
iter  10 value 0.611535
iter  20 value 0.342487
iter  30 value 0.295359
iter  40 value 0.282280
iter  50 value 0.278010
iter  60 value 0.276607
iter  70 value 0.276150
iter  80 value 0.274604
iter  90 value 0.272482
iter 100 value 0.270587
final  value 0.270587 
stopped after 100 iterations
# weights:  43
initial  value 58.833972 
iter  10 value 1.916970
iter  20 value 1.207358
iter  30 value 1.172816
iter  40 value 1.171455
iter  50 value 1.171449
final  value 1.171449 
converged
# weights:  97
initial  value 70.416587 
iter  10 value 1.895829
iter  20 value 1.060990
iter  30 value 1.000601
iter  40 value 0.990329
iter  50 value 0.989927
iter  60 value 0.989908
final  value 0.989907 
converged
# weights:  121
initial  value 39.513250 
iter  10 value 0.335527
iter  20 value 0.132984
iter  30 value 0.081742
iter  40 value 0.044866
iter  50 value 0.029229
iter  60 value 0.022786
iter  70 value 0.018368
iter  80 value 0.015575
iter  90 value 0.014295
iter 100 value 0.012889
final  value 0.012889 
stopped after 100 iterations
# weights:  85
initial  value 23.612095 
iter  10 value 0.487045
iter  20 value 0.135482
iter  30 value 0.067064
iter  40 value 0.041332
iter  50 value 0.029753
iter  60 value 0.026073
iter  70 value 0.023801
iter  80 value 0.022775
iter  90 value 0.022005
iter 100 value 0.021278
final  value 0.021278 
stopped after 100 iterations
# weights:  49
initial  value 16.046078 
iter  10 value 1.933859
iter  20 value 1.743008
iter  30 value 1.732094
final  value 1.731894 
converged
# weights:  85
initial  value 43.184015 
iter  10 value 0.538138
iter  20 value 0.129365
iter  30 value 0.088675
iter  40 value 0.078125
iter  50 value 0.075845
iter  60 value 0.073548
iter  70 value 0.072766
iter  80 value 0.071918
iter  90 value 0.071229
iter 100 value 0.070702
final  value 0.070702 
stopped after 100 iterations
# weights:  103
initial  value 44.068231 
iter  10 value 0.295852
iter  20 value 0.091296
iter  30 value 0.060401
iter  40 value 0.038771
iter  50 value 0.031150
iter  60 value 0.028038
iter  70 value 0.026954
iter  80 value 0.026349
iter  90 value 0.025820
iter 100 value 0.025426
final  value 0.025426 
stopped after 100 iterations
# weights:  49
initial  value 39.088164 
iter  10 value 1.553768
iter  20 value 1.401392
iter  30 value 1.387036
iter  40 value 1.387021
iter  40 value 1.387021
iter  40 value 1.387021
final  value 1.387021 
converged
# weights:  115
initial  value 51.983375 
iter  10 value 0.282362
iter  20 value 0.116791
iter  30 value 0.074439
iter  40 value 0.063932
iter  50 value 0.059608
iter  60 value 0.057512
iter  70 value 0.056247
iter  80 value 0.055369
iter  90 value 0.054839
iter 100 value 0.054557
final  value 0.054557 
stopped after 100 iterations
# weights:  49
initial  value 23.214500 
iter  10 value 0.792979
iter  20 value 0.528588
iter  30 value 0.477242
iter  40 value 0.461567
iter  50 value 0.454309
iter  60 value 0.450916
iter  70 value 0.449420
iter  80 value 0.448965
iter  90 value 0.448783
iter 100 value 0.448724
final  value 0.448724 
stopped after 100 iterations
# weights:  97
initial  value 46.709824 
iter  10 value 8.361796
iter  20 value 7.017639
iter  30 value 6.990425
iter  40 value 6.989672
final  value 6.989665 
converged
# weights:  79
initial  value 94.702185 
iter  10 value 13.852912
iter  20 value 13.405257
iter  30 value 13.395072
iter  40 value 13.394997
final  value 13.394996 
converged
# weights:  49
initial  value 46.604825 
iter  10 value 3.278954
iter  20 value 2.917625
iter  30 value 2.912096
iter  40 value 2.911945
final  value 2.911945 
converged
# weights:  85
initial  value 38.155651 
iter  10 value 0.237197
iter  20 value 0.091965
iter  30 value 0.059170
iter  40 value 0.042081
iter  50 value 0.035934
iter  60 value 0.032498
iter  70 value 0.031000
iter  80 value 0.029865
iter  90 value 0.029306
iter 100 value 0.028735
final  value 0.028735 
stopped after 100 iterations
# weights:  109
initial  value 32.992727 
iter  10 value 1.389826
iter  20 value 1.221887
iter  30 value 1.198893
iter  40 value 1.197103
iter  50 value 1.197059
final  value 1.197058 
converged
# weights:  49
initial  value 29.465417 
iter  10 value 1.129947
iter  20 value 0.157372
iter  30 value 0.080441
iter  40 value 0.056886
iter  50 value 0.044627
iter  60 value 0.040137
iter  70 value 0.037355
iter  80 value 0.035478
iter  90 value 0.034028
iter 100 value 0.033489
final  value 0.033489 
stopped after 100 iterations
# weights:  73
initial  value 49.300232 
iter  10 value 0.395935
iter  20 value 0.227710
iter  30 value 0.204006
iter  40 value 0.192721
iter  50 value 0.183346
iter  60 value 0.174889
iter  70 value 0.172422
iter  80 value 0.171359
iter  90 value 0.171063
iter 100 value 0.170842
final  value 0.170842 
stopped after 100 iterations
# weights:  19
initial  value 23.819398 
iter  10 value 1.550625
iter  20 value 1.431380
iter  30 value 1.425666
iter  40 value 1.425607
iter  40 value 1.425607
final  value 1.425607 
converged
# weights:  103
initial  value 22.113391 
iter  10 value 0.439688
iter  20 value 0.145436
iter  30 value 0.087944
iter  40 value 0.066238
iter  50 value 0.061141
iter  60 value 0.058587
iter  70 value 0.057043
iter  80 value 0.055791
iter  90 value 0.055336
iter 100 value 0.054781
final  value 0.054781 
stopped after 100 iterations
# weights:  31
initial  value 48.647692 
iter  10 value 2.923399
iter  20 value 0.829697
iter  30 value 0.494669
iter  40 value 0.373207
iter  50 value 0.321083
iter  60 value 0.295883
iter  70 value 0.279715
iter  80 value 0.268607
iter  90 value 0.267129
iter 100 value 0.266898
final  value 0.266898 
stopped after 100 iterations
# weights:  43
initial  value 12.299396 
iter  10 value 1.341207
iter  20 value 1.194624
iter  30 value 1.190256
iter  40 value 1.190229
final  value 1.190229 
converged
# weights:  97
initial  value 23.780613 
iter  10 value 1.713641
iter  20 value 1.142883
iter  30 value 1.054065
iter  40 value 1.026376
iter  50 value 1.020906
iter  60 value 1.019523
iter  70 value 1.019492
iter  80 value 1.019491
final  value 1.019491 
converged
# weights:  121
initial  value 70.784415 
iter  10 value 1.278110
iter  20 value 0.109667
iter  30 value 0.051317
iter  40 value 0.028900
iter  50 value 0.021644
iter  60 value 0.014956
iter  70 value 0.011096
iter  80 value 0.009031
iter  90 value 0.007430
iter 100 value 0.006649
final  value 0.006649 
stopped after 100 iterations
# weights:  85
initial  value 31.862838 
iter  10 value 0.476596
iter  20 value 0.132258
iter  30 value 0.074848
iter  40 value 0.043531
iter  50 value 0.033511
iter  60 value 0.027275
iter  70 value 0.024629
iter  80 value 0.022784
iter  90 value 0.022238
iter 100 value 0.021769
final  value 0.021769 
stopped after 100 iterations
# weights:  49
initial  value 31.202948 
iter  10 value 1.892372
iter  20 value 1.751559
iter  30 value 1.747290
final  value 1.747287 
converged
# weights:  85
initial  value 27.009459 
iter  10 value 0.503510
iter  20 value 0.212879
iter  30 value 0.109451
iter  40 value 0.089352
iter  50 value 0.082753
iter  60 value 0.078946
iter  70 value 0.076076
iter  80 value 0.074705
iter  90 value 0.073950
iter 100 value 0.073429
final  value 0.073429 
stopped after 100 iterations
# weights:  103
initial  value 40.193704 
iter  10 value 0.236727
iter  20 value 0.086399
iter  30 value 0.051497
iter  40 value 0.039423
iter  50 value 0.033826
iter  60 value 0.030608
iter  70 value 0.029152
iter  80 value 0.027878
iter  90 value 0.026774
iter 100 value 0.026093
final  value 0.026093 
stopped after 100 iterations
# weights:  49
initial  value 29.436165 
iter  10 value 1.584325
iter  20 value 1.424592
iter  30 value 1.415258
iter  40 value 1.415019
final  value 1.415019 
converged
# weights:  115
initial  value 53.453257 
iter  10 value 13.085122
iter  20 value 0.489481
iter  30 value 0.241053
iter  40 value 0.157929
iter  50 value 0.114135
iter  60 value 0.097562
iter  70 value 0.085526
iter  80 value 0.078182
iter  90 value 0.072755
iter 100 value 0.068686
final  value 0.068686 
stopped after 100 iterations
# weights:  49
initial  value 32.955453 
iter  10 value 0.749255
iter  20 value 0.506053
iter  30 value 0.473949
iter  40 value 0.462486
iter  50 value 0.459994
iter  60 value 0.459078
iter  70 value 0.458811
iter  80 value 0.458768
iter  90 value 0.458747
iter 100 value 0.458737
final  value 0.458737 
stopped after 100 iterations
# weights:  97
initial  value 70.039454 
iter  10 value 7.812891
iter  20 value 7.103365
iter  30 value 7.084503
iter  40 value 7.083392
iter  50 value 7.083208
final  value 7.083207 
converged
# weights:  79
initial  value 92.879961 
iter  10 value 14.610857
iter  20 value 13.672324
iter  30 value 13.637147
iter  40 value 13.634940
final  value 13.634878 
converged
# weights:  49
initial  value 27.536316 
iter  10 value 3.524127
iter  20 value 2.925747
iter  30 value 2.876888
iter  40 value 2.876679
final  value 2.876679 
converged
# weights:  85
initial  value 16.573396 
iter  10 value 0.361584
iter  20 value 0.112233
iter  30 value 0.064615
iter  40 value 0.046480
iter  50 value 0.037845
iter  60 value 0.034216
iter  70 value 0.032220
iter  80 value 0.031561
iter  90 value 0.030990
iter 100 value 0.030547
final  value 0.030547 
stopped after 100 iterations
# weights:  109
initial  value 103.155319 
iter  10 value 2.280544
iter  20 value 1.198282
iter  30 value 1.143256
iter  40 value 1.137580
iter  50 value 1.137005
iter  60 value 1.136739
iter  70 value 1.136728
final  value 1.136727 
converged
# weights:  49
initial  value 27.894260 
iter  10 value 0.394373
iter  20 value 0.185074
iter  30 value 0.094716
iter  40 value 0.064934
iter  50 value 0.053789
iter  60 value 0.044434
iter  70 value 0.039809
iter  80 value 0.038092
iter  90 value 0.036942
iter 100 value 0.036500
final  value 0.036500 
stopped after 100 iterations
# weights:  73
initial  value 20.801889 
iter  10 value 0.343165
iter  20 value 0.194336
iter  30 value 0.149597
iter  40 value 0.137095
iter  50 value 0.131278
iter  60 value 0.128080
iter  70 value 0.126270
iter  80 value 0.125174
iter  90 value 0.124747
iter 100 value 0.124530
final  value 0.124530 
stopped after 100 iterations
# weights:  19
initial  value 57.839193 
iter  10 value 1.741673
iter  20 value 1.368117
iter  30 value 1.355266
iter  40 value 1.354573
final  value 1.354570 
converged
# weights:  103
initial  value 18.355141 
iter  10 value 0.356944
iter  20 value 0.116451
iter  30 value 0.071396
iter  40 value 0.061307
iter  50 value 0.056793
iter  60 value 0.054004
iter  70 value 0.052797
iter  80 value 0.051847
iter  90 value 0.051424
iter 100 value 0.051049
final  value 0.051049 
stopped after 100 iterations
# weights:  31
initial  value 27.815961 
iter  10 value 1.949158
iter  20 value 0.793893
iter  30 value 0.448208
iter  40 value 0.365133
iter  50 value 0.336194
iter  60 value 0.323201
iter  70 value 0.316747
iter  80 value 0.299587
iter  90 value 0.297898
iter 100 value 0.297702
final  value 0.297702 
stopped after 100 iterations
# weights:  43
initial  value 43.966955 
iter  10 value 1.276021
iter  20 value 1.143727
iter  30 value 1.133831
iter  40 value 1.133629
final  value 1.133628 
converged
# weights:  97
initial  value 43.698529 
iter  10 value 1.338970
iter  20 value 0.972808
iter  30 value 0.960834
iter  40 value 0.959451
iter  50 value 0.959298
iter  60 value 0.959292
final  value 0.959292 
converged
# weights:  121
initial  value 7.840325 
iter  10 value 0.227373
iter  20 value 0.066417
iter  30 value 0.036317
iter  40 value 0.026926
iter  50 value 0.019091
iter  60 value 0.015200
iter  70 value 0.012309
iter  80 value 0.010723
iter  90 value 0.009138
iter 100 value 0.008285
final  value 0.008285 
stopped after 100 iterations
# weights:  85
initial  value 83.118361 
iter  10 value 0.286626
iter  20 value 0.076071
iter  30 value 0.040332
iter  40 value 0.026976
iter  50 value 0.021707
iter  60 value 0.019661
iter  70 value 0.018524
iter  80 value 0.018086
iter  90 value 0.017769
iter 100 value 0.017468
final  value 0.017468 
stopped after 100 iterations
# weights:  49
initial  value 38.021048 
iter  10 value 1.952086
iter  20 value 1.683760
iter  30 value 1.682281
iter  40 value 1.682273
iter  40 value 1.682273
iter  40 value 1.682273
final  value 1.682273 
converged
# weights:  85
initial  value 82.988994 
iter  10 value 0.307770
iter  20 value 0.175338
iter  30 value 0.132433
iter  40 value 0.120587
iter  50 value 0.116932
iter  60 value 0.114838
iter  70 value 0.112944
iter  80 value 0.110550
iter  90 value 0.096950
iter 100 value 0.085921
final  value 0.085921 
stopped after 100 iterations
# weights:  103
initial  value 18.963208 
iter  10 value 0.303979
iter  20 value 0.098626
iter  30 value 0.046138
iter  40 value 0.031439
iter  50 value 0.028330
iter  60 value 0.026457
iter  70 value 0.025405
iter  80 value 0.024843
iter  90 value 0.024209
iter 100 value 0.023646
final  value 0.023646 
stopped after 100 iterations
# weights:  49
initial  value 9.740115 
iter  10 value 1.396711
iter  20 value 1.345956
iter  30 value 1.345412
final  value 1.345410 
converged
# weights:  115
initial  value 3.186362 
iter  10 value 0.199805
iter  20 value 0.118038
iter  30 value 0.068235
iter  40 value 0.059951
iter  50 value 0.056954
iter  60 value 0.055190
iter  70 value 0.053698
iter  80 value 0.052770
iter  90 value 0.052390
iter 100 value 0.052165
final  value 0.052165 
stopped after 100 iterations
# weights:  49
initial  value 36.566788 
iter  10 value 1.030366
iter  20 value 0.631386
iter  30 value 0.531996
iter  40 value 0.480398
iter  50 value 0.442607
iter  60 value 0.431049
iter  70 value 0.426791
iter  80 value 0.426190
iter  90 value 0.425969
iter 100 value 0.425857
final  value 0.425857 
stopped after 100 iterations
# weights:  97
initial  value 96.239130 
iter  10 value 9.662115
iter  20 value 6.876352
iter  30 value 6.830252
iter  40 value 6.828960
iter  50 value 6.828723
final  value 6.828720 
converged
# weights:  79
initial  value 81.852120 
iter  10 value 14.913908
iter  20 value 13.141443
iter  30 value 13.090217
iter  40 value 13.074733
iter  50 value 13.074467
final  value 13.074467 
converged
# weights:  49
initial  value 61.454256 
iter  10 value 3.334164
iter  20 value 2.783345
iter  30 value 2.777556
iter  40 value 2.777545
iter  40 value 2.777545
iter  40 value 2.777545
final  value 2.777545 
converged
# weights:  85
initial  value 32.114682 
iter  10 value 0.248162
iter  20 value 0.084536
iter  30 value 0.053117
iter  40 value 0.039756
iter  50 value 0.034037
iter  60 value 0.031452
iter  70 value 0.029961
iter  80 value 0.029228
iter  90 value 0.028550
iter 100 value 0.027986
final  value 0.027986 
stopped after 100 iterations
# weights:  121
initial  value 24.798804 
iter  10 value 0.423066
iter  20 value 0.138069
iter  30 value 0.079870
iter  40 value 0.049420
iter  50 value 0.032318
iter  60 value 0.025288
iter  70 value 0.020569
iter  80 value 0.017718
iter  90 value 0.015704
iter 100 value 0.013698
final  value 0.013698 
stopped after 100 iterations
Neural Networks with Feature Extraction 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  size  decay         RMSE         Rsquared   MAE          Selected
   3    0.1505363399  0.033042772  0.9765274  0.021757373          
   5    0.0094575873  0.015108399  0.9946641  0.009256570          
   7    0.1860506215  0.027776035  0.9833955  0.018000258          
   8    0.0003513622  0.005699449  0.9992120  0.003305326          
   8    0.0338702219  0.021667588  0.9890151  0.014775203          
   8    0.2481023680  0.027941992  0.9838795  0.017657283          
   8    0.3335624159  0.029233859  0.9834827  0.018011807          
   8    0.6328665552  0.033864822  0.9826454  0.020504792          
  12    0.0047074044  0.010405702  0.9974274  0.005967957          
  13    5.1154613290  0.100093018  0.9509533  0.081670689          
  14    0.0001059793  0.005367840  0.9992999  0.003389443          
  14    0.0003038330  0.005585366  0.9992481  0.003443063          
  14    0.0016379824  0.008426118  0.9983308  0.005025473          
  16    0.1830050068  0.023797242  0.9877569  0.015277374          
  16    2.0940486010  0.054075504  0.9795980  0.041420145          
  17    0.0002032754  0.005509476  0.9992673  0.003387006          
  17    0.0010235172  0.007261706  0.9987322  0.004303053          
  18    0.2353431183  0.023880615  0.9881534  0.015006089          
  19    0.0011270169  0.007285021  0.9987252  0.004254461          
  20    0.0000136168  0.004291835  0.9995450  0.002728906  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 20 and decay = 1.36168e-05.
[1] "Mon Mar 12 10:04:16 2018"
Principal Component Analysis 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  ncomp  RMSE          Rsquared   MAE           Selected
  2      0.0230143194  0.9868978  0.0178501479          
  3      0.0073621957  0.9986609  0.0048716766          
  4      0.0056228337  0.9992157  0.0042264396          
  5      0.0056372735  0.9992121  0.0042014808          
  6      0.0056353671  0.9992132  0.0042188851          
  7      0.0034037724  0.9997119  0.0024597693          
  8      0.0009889991  0.9999756  0.0007803784  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 8.
[1] "Mon Mar 12 10:04:21 2018"
Installing package into 'C:/Users/John/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Component____ 9 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Component____ 9 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Component____ 9 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Component____ 9 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Component____ 9 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Component____ 9 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Component____ 6 ____
____Component____ 7 ____
____Component____ 8 ____
____Component____ 9 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

Partial Least Squares Generalized Linear Models  

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  nt  alpha.pvals.expli  RMSE          Rsquared   MAE           Selected
  2   0.099192679        0.0124323529  0.9962071  0.0096981844          
  2   0.139254712        0.0124323529  0.9962071  0.0096981844          
  4   0.051525167        0.0063079834  0.9990123  0.0046091870          
  4   0.117660601        0.0063105242  0.9990115  0.0046133543          
  4   0.142321038        0.0063105242  0.9990115  0.0046133543          
  4   0.146487697        0.0063105242  0.9990115  0.0046133543          
  4   0.150772570        0.0063105242  0.9990115  0.0046133543          
  4   0.160043738        0.0063026353  0.9990141  0.0046076693          
  6   0.049421165        0.0019497897  0.9998979  0.0014340313          
  6   0.073810307        0.0019497897  0.9998979  0.0014340313          
  6   0.089092717        0.0019497897  0.9998979  0.0014340313          
  6   0.190296160        0.0021009527  0.9998778  0.0015471917          
  7   0.034174040        0.0011963621  0.9999628  0.0008977407          
  8   0.043602830        0.0010131405  0.9999745  0.0008019657          
  8   0.067003172        0.0010129855  0.9999745  0.0008017983          
  8   0.142082099        0.0010234494  0.9999740  0.0008093583          
  8   0.145723383        0.0010234494  0.9999740  0.0008093583          
  8   0.177366225        0.0010260888  0.9999738  0.0008128841          
  9   0.004469171        0.0009634527  0.9999769  0.0007574590          
  9   0.068397681        0.0009634527  0.9999769  0.0007574590  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nt = 9 and alpha.pvals.expli
 = 0.06839768.
[1] "Mon Mar 12 10:05:01 2018"
Projection Pursuit Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  nterms  RMSE          Rsquared   MAE           Selected
   1      0.0009736873  0.9999764  0.0007599128          
   2      0.0007492829  0.9999857  0.0005819559          
   3      0.0006239334  0.9999902  0.0004604911          
   4      0.0005958670  0.9999910  0.0004357147          
   5      0.0005845998  0.9999915  0.0004397275          
   6      0.0005940967  0.9999912  0.0004446639          
   7      0.0005902640  0.9999914  0.0004460078          
   8      0.0005816001  0.9999916  0.0004307320          
   9      0.0005835037  0.9999916  0.0004317432          
  10      0.0005957924  0.9999912  0.0004520390          
  11      0.0005986791  0.9999911  0.0004466203          
  12      0.0005859497  0.9999915  0.0004385693          
  13      0.0005786704  0.9999917  0.0004361845  *       
  14      0.0005981695  0.9999911  0.0004516675          
  15      0.0005845915  0.9999914  0.0004466697          
  16      0.0006118922  0.9999906  0.0004480465          
  17      0.0006048369  0.9999910  0.0004563423          
  18      0.0006155146  0.9999906  0.0004594541          
  19      0.0006231008  0.9999903  0.0004635123          
  20      0.0006165966  0.9999906  0.0004624460          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nterms = 13.
[1] "Mon Mar 12 10:05:17 2018"
Installing package into 'C:/Users/John/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Quantile Random Forest 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE         Selected
  2     0.04407318  0.9925600  0.04008067          
  4     0.03574710  0.9940154  0.03195915          
  6     0.03223739  0.9942335  0.02827278          
  7     0.03144681  0.9941528  0.02745646          
  8     0.03075060  0.9939431  0.02667170          
  9     0.02987950  0.9938726  0.02578600  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 9.
[1] "Mon Mar 12 10:06:15 2018"
Random Forest 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  mtry  splitrule   RMSE         Rsquared   MAE          Selected
  2     extratrees  0.010297418  0.9978656  0.007464388          
  2     maxstat     0.013248359  0.9963718  0.009656056          
  4     extratrees  0.008369190  0.9984369  0.006100745          
  4     maxstat     0.011238767  0.9971507  0.008312991          
  4     variance    0.010837901  0.9972312  0.008095380          
  6     extratrees  0.008030520  0.9985197  0.005867148          
  6     maxstat     0.011568743  0.9968555  0.008536676          
  6     variance    0.011235356  0.9969450  0.008459862          
  7     variance    0.011536373  0.9967548  0.008715348          
  8     extratrees  0.007819646  0.9985701  0.005690391          
  8     maxstat     0.012145813  0.9964990  0.008981140          
  8     variance    0.011994320  0.9964723  0.009125799          
  9     extratrees  0.007767454  0.9985862  0.005649525  *       
  9     variance    0.012458594  0.9961894  0.009475299          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mtry = 9 and splitrule = extratrees.
[1] "Mon Mar 12 10:06:58 2018"
Error in code$varImp(object$finalModel, ...) : 
  No importance values available
In addition: Warning messages:
1: package 'gpls' is not available (for R version 3.4.2) 
2: package 'rPython' is not available (for R version 3.4.2) 
Random Forest 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  predFixed  RMSE        Rsquared   MAE          Selected
  2          0.01191286  0.9968298  0.008963985          
  4          0.01071420  0.9972796  0.008096941  *       
  6          0.01152416  0.9967666  0.008730279          
  7          0.01214246  0.9963954  0.009193685          
  8          0.01257167  0.9961212  0.009462236          
  9          0.01332890  0.9956193  0.010031543          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was predFixed = 4.
[1] "Mon Mar 12 10:08:53 2018"
Relaxed Lasso 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  phi         lambda      RMSE        Rsquared   MAE         Selected
  0.02234585  240.373905  0.50309115        NaN  0.46141979          
  0.17087020   61.739325  0.12037501  0.9711931  0.10998838          
  0.21801415  112.353796  0.37983148  0.9685137  0.34781327          
  0.24710582   58.354039  0.13158277  0.9711931  0.12061685          
  0.25762584   18.906520  0.09232197  0.9975472  0.09118731          
  0.33501586  121.602350  0.50309115        NaN  0.46141979          
  0.34198841  168.750771  0.50309115        NaN  0.46141979          
  0.36905154   58.212405  0.14967359  0.9711931  0.13761852          
  0.44546359   42.140049  0.06338428  0.9920949  0.05293331          
  0.49596339    9.256038  0.05561831  0.9973041  0.05359503          
  0.58830301   16.844098  0.07032466  0.9976802  0.06956504          
  0.69627356    6.831072  0.02256259  0.9981034  0.02097759  *       
  0.71041050   98.706734  0.20095361  0.9711931  0.18524145          
  0.71160519   15.968494  0.06226539  0.9976983  0.06150258          
  0.72861692  134.676748  0.50309115        NaN  0.46141979          
  0.73243848   18.715815  0.06091529  0.9976995  0.06014033          
  0.75386285   17.593545  0.05953101  0.9977001  0.05873944          
  0.80021869   16.836303  0.05655162  0.9976995  0.05570833          
  0.88683113  105.262155  0.22766908  0.9711931  0.20986062          
  0.95148080   53.286046  0.23748188  0.9711931  0.21888328          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 6.831072 and phi = 0.6962736.
[1] "Mon Mar 12 10:09:00 2018"
Random Forest 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE          Selected
  2     0.01173744  0.9969661  0.008657936          
  4     0.01067303  0.9973168  0.008013946  *       
  6     0.01122685  0.9969470  0.008456580          
  7     0.01155776  0.9967383  0.008735342          
  8     0.01203421  0.9964544  0.009112862          
  9     0.01241323  0.9962135  0.009450553          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 4.
[1] "Mon Mar 12 10:09:41 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Ridge Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  lambda        RMSE         Rsquared   MAE          Selected
  6.408213e-05  0.001487301  0.9999448  0.001043123  *       
  1.770265e-04  0.002218300  0.9998773  0.001544395          
  1.097029e-03  0.003795059  0.9996429  0.002674591          
  1.309457e-03  0.003958056  0.9996119  0.002796545          
  1.311486e-03  0.003959483  0.9996116  0.002797601          
  1.517048e-03  0.004093664  0.9995852  0.002897301          
  1.865629e-03  0.004282673  0.9995467  0.003037332          
  1.929976e-03  0.004313331  0.9995403  0.003060119          
  2.817240e-02  0.007239400  0.9991309  0.005155792          
  6.176199e-02  0.011291247  0.9988885  0.008521649          
  8.301732e-02  0.014174067  0.9987037  0.010932063          
  8.369486e-02  0.014267736  0.9986974  0.011009919          
  1.010686e-01  0.016691789  0.9985302  0.013024905          
  4.855539e-01  0.069498957  0.9928990  0.056342620          
  6.020641e-01  0.084420143  0.9907978  0.068521264          
  7.487771e-01  0.102492877  0.9880557  0.083260366          
  9.755753e-01  0.128963497  0.9837627  0.104840669          
  1.372793e+00  0.171478821  0.9764951  0.139489790          
  2.919036e+00  0.301703788  0.9544403  0.245565605          
  9.531051e+00  0.570801036  0.9184828  0.464809831          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was lambda = 6.408213e-05.
[1] "Mon Mar 12 10:09:48 2018"
Robust Linear Model 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  intercept  psi           RMSE          Rsquared   MAE           Selected
  FALSE      psi.huber     0.0009975819  0.9999754  0.0007703118          
  FALSE      psi.hampel    0.0009844823  0.9999760  0.0007739005          
  FALSE      psi.bisquare  0.0010399089  0.9999737  0.0007755473          
   TRUE      psi.huber     0.0009902803  0.9999758  0.0007453423          
   TRUE      psi.hampel    0.0009742258  0.9999764  0.0007491046  *       
   TRUE      psi.bisquare  0.0010774545  0.9999721  0.0007635059          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were intercept = TRUE and psi = psi.hampel.
[1] "Mon Mar 12 10:09:53 2018"
CART 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  cp            RMSE        Rsquared   MAE         Selected
  0.000000e+00  0.02756207  0.9811994  0.02166128          
  3.359606e-05  0.02756207  0.9811994  0.02166128  *       
  7.027569e-05  0.02766165  0.9810667  0.02172526          
  7.971312e-05  0.02768804  0.9810332  0.02175744          
  8.330967e-05  0.02768804  0.9810332  0.02175744          
  1.448113e-04  0.02796975  0.9806638  0.02216235          
  1.981275e-04  0.02824279  0.9802862  0.02233089          
  2.142430e-04  0.02851326  0.9799008  0.02265714          
  3.028939e-04  0.02933813  0.9786412  0.02332315          
  3.543486e-04  0.02990073  0.9778604  0.02385388          
  4.057685e-04  0.03039048  0.9770893  0.02429832          
  4.567378e-04  0.03048429  0.9769102  0.02430481          
  2.741991e-03  0.04121301  0.9575711  0.03293894          
  3.519248e-03  0.04306884  0.9535863  0.03438036          
  2.223549e-02  0.06040427  0.9096875  0.04830224          
  1.087014e-01  0.08327386  0.8211956  0.06646798          
  1.150740e-01  0.10104218  0.7447989  0.08196176          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was cp = 3.359606e-05.
[1] "Mon Mar 12 10:10:00 2018"
CART 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results:

  RMSE        Rsquared   MAE       
  0.04936395  0.9389781  0.04020932

[1] "Mon Mar 12 10:10:05 2018"
CART 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  maxdepth  RMSE        Rsquared   MAE         Selected
   1        0.11629084  0.6658791  0.09637119          
   2        0.09267716  0.7862271  0.07376455          
   5        0.05513771  0.9243393  0.04454949          
  13        0.04936395  0.9389781  0.04020932  *       
  14        0.04936395  0.9389781  0.04020932          
  15        0.04936395  0.9389781  0.04020932          
  16        0.04936395  0.9389781  0.04020932          
  19        0.04936395  0.9389781  0.04020932          
  22        0.04936395  0.9389781  0.04020932          
  25        0.04936395  0.9389781  0.04020932          
  26        0.04936395  0.9389781  0.04020932          
  27        0.04936395  0.9389781  0.04020932          
  29        0.04936395  0.9389781  0.04020932          
  30        0.04936395  0.9389781  0.04020932          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was maxdepth = 13.
[1] "Mon Mar 12 10:10:11 2018"
Quantile Regression with LASSO penalty 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  lambda        RMSE         Rsquared   MAE           Selected
  6.408213e-05  0.001119675  0.9999697  0.0007951257  *       
  1.770265e-04  0.001293508  0.9999594  0.0008458614          
  1.097029e-03  0.004577686  0.9994982  0.0024146732          
  1.309457e-03  0.004570264  0.9994997  0.0024167050          
  1.311486e-03  0.004570183  0.9994997  0.0024166548          
  1.517048e-03  0.004564362  0.9995005  0.0024168016          
  1.865629e-03  0.004571195  0.9994981  0.0024181483          
  1.929976e-03  0.004571253  0.9994979  0.0024183193          
  2.817240e-02  0.004365406  0.9995526  0.0031637503          
  6.176199e-02  0.010973385  0.9986755  0.0089682284          
  8.301732e-02  0.127948113  0.9207623  0.1045309733          
  8.369486e-02  0.130845827  0.9159447  0.1069080773          
  1.010686e-01  0.200764500        NaN  0.1653452920          
  4.855539e-01  0.200764500        NaN  0.1653452920          
  6.020641e-01  0.200764500        NaN  0.1653452920          
  7.487771e-01  0.200764500        NaN  0.1653452920          
  9.755753e-01  0.200764500        NaN  0.1653452920          
  1.372793e+00  0.200764500        NaN  0.1653452920          
  2.919036e+00  0.200764500        NaN  0.1653452921          
  9.531051e+00  0.200764500        NaN  0.1653452921          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was lambda = 6.408213e-05.
[1] "Mon Mar 12 10:10:18 2018"
Non-Convex Penalized Quantile Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  lambda        penalty  RMSE         Rsquared   MAE           Selected
  6.408213e-05  SCAD     0.001006107  0.9999753  0.0007424108  *       
  1.770265e-04  MCP      0.001055035  0.9999729  0.0007873552          
  1.097029e-03  SCAD     0.001055036  0.9999729  0.0007873548          
  1.309457e-03  SCAD     0.001055036  0.9999729  0.0007873548          
  1.311486e-03  SCAD     0.001055036  0.9999729  0.0007873548          
  1.517048e-03  SCAD     0.001055036  0.9999729  0.0007873548          
  1.865629e-03  SCAD     0.004536658  0.9995072  0.0024064127          
  1.929976e-03  MCP      0.004536659  0.9995072  0.0024064129          
  2.817240e-02  MCP      0.004712018  0.9994549  0.0030957620          
  6.176199e-02  SCAD     0.015939582  0.9943013  0.0112244111          
  8.301732e-02  MCP      0.031376089  0.9656042  0.0246864387          
  8.369486e-02  MCP      0.031411625  0.9655856  0.0246990266          
  1.010686e-01  MCP      0.200764500        NaN  0.1653452920          
  4.855539e-01  SCAD     0.200764500        NaN  0.1653452920          
  6.020641e-01  SCAD     0.200764500        NaN  0.1653452920          
  7.487771e-01  MCP      0.200764500        NaN  0.1653452920          
  9.755753e-01  MCP      0.200764500        NaN  0.1653452920          
  1.372793e+00  SCAD     0.200764500        NaN  0.1653452920          
  2.919036e+00  MCP      0.200764500        NaN  0.1653452921          
  9.531051e+00  MCP      0.200764500        NaN  0.1653452921          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 6.408213e-05 and penalty
 = SCAD.
[1] "Mon Mar 12 10:10:26 2018"
Regularized Random Forest 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  mtry  coefReg     coefImp     RMSE        Rsquared   MAE          Selected
  2     0.49596339  0.20089334  0.01170736  0.9969639  0.008692049          
  2     0.69627356  0.21447782  0.01177233  0.9969501  0.008715113          
  4     0.25762584  0.33767075  0.01053506  0.9973819  0.007884755          
  4     0.58830301  0.11511013  0.01073932  0.9972985  0.008036678          
  4     0.71160519  0.07886705  0.01061156  0.9973460  0.007933401          
  4     0.73243848  0.82670071  0.01060198  0.9973413  0.007981893          
  4     0.75386285  0.70894060  0.01065730  0.9973146  0.007964412          
  4     0.80021869  0.96223616  0.01045312  0.9974272  0.007819303  *       
  6     0.24710582  0.99956990  0.01122735  0.9969397  0.008434782          
  6     0.36905154  0.30228978  0.01118965  0.9969560  0.008411301          
  6     0.44546359  0.55925583  0.01137703  0.9968635  0.008562665          
  6     0.95148080  0.68160210  0.01114889  0.9969846  0.008449683          
  7     0.17087020  0.95027667  0.01149022  0.9967722  0.008670889          
  8     0.21801415  0.42563514  0.01192416  0.9965180  0.009049984          
  8     0.33501586  0.84812804  0.01198295  0.9964858  0.009089103          
  8     0.71041050  0.09178656  0.01203778  0.9964454  0.009110554          
  8     0.72861692  0.14022908  0.01182291  0.9965721  0.008972953          
  8     0.88683113  0.08668412  0.01191531  0.9965272  0.009037616          
  9     0.02234585  0.12518554  0.01282150  0.9959420  0.010061005          
  9     0.34198841  0.28207990  0.01234882  0.9962557  0.009430166          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mtry = 4, coefReg = 0.8002187
 and coefImp = 0.9622362.
[1] "Mon Mar 12 10:14:39 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: Warning messages:
1: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
2: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Regularized Random Forest 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  mtry  coefReg     RMSE        Rsquared   MAE          Selected
  2     0.49596339  0.01179938  0.9969060  0.008828094          
  2     0.69627356  0.01176804  0.9969498  0.008767134          
  4     0.25762584  0.01065061  0.9973236  0.007976289  *       
  4     0.58830301  0.01082886  0.9972213  0.008056927          
  4     0.71160519  0.01071378  0.9972868  0.007985507          
  4     0.73243848  0.01077906  0.9972462  0.008052532          
  4     0.75386285  0.01067129  0.9972990  0.007984598          
  4     0.80021869  0.01067142  0.9973017  0.007978848          
  6     0.24710582  0.01120412  0.9969589  0.008440256          
  6     0.36905154  0.01110000  0.9970209  0.008377155          
  6     0.44546359  0.01112884  0.9970037  0.008377431          
  6     0.95148080  0.01113396  0.9970030  0.008419422          
  7     0.17087020  0.01135993  0.9968608  0.008599009          
  8     0.21801415  0.01198167  0.9964830  0.009084579          
  8     0.33501586  0.01193831  0.9964990  0.009082380          
  8     0.71041050  0.01183795  0.9965658  0.009032511          
  8     0.72861692  0.01186991  0.9965483  0.009043142          
  8     0.88683113  0.01206503  0.9964301  0.009155857          
  9     0.02234585  0.02109607  0.9879175  0.016518902          
  9     0.34198841  0.01238888  0.9962205  0.009416889          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mtry = 4 and coefReg = 0.2576258.
[1] "Mon Mar 12 10:16:39 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: parameter=none Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) : 
  the leading minor of order 399 is not positive definite
 
2: model fit failed for Fold2: parameter=none Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) : 
  the leading minor of order 359 is not positive definite
 
3: model fit failed for Fold3: parameter=none Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) : 
  the leading minor of order 321 is not positive definite
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: parameter=none Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) : 
  the leading minor of order 273 is not positive definite
 
2: model fit failed for Fold2: parameter=none Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) : 
  the leading minor of order 276 is not positive definite
 
3: model fit failed for Fold3: parameter=none Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) : 
  the leading minor of order 266 is not positive definite
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 10:22:13 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "rvmLinear"                      
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 11:16:20 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "rvmPoly"                        
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 11:38:40 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "rvmRadial"                      
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%
Subtractive Clustering and Fuzzy c-Means Rules 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  r.a  eps.high   eps.low     RMSE        Rsquared   MAE         Selected
   1   0.6880955  0.04362408  0.03836227  0.9739596  0.02820029  *       
   3   0.8180109  0.09646713         NaN        NaN         NaN          
   5   0.4746516  0.21358247         NaN        NaN         NaN          
   6   0.9241990  0.04067022         NaN        NaN         NaN          
   8   0.3741424  0.29607127         NaN        NaN         NaN          
   8   0.6373135  0.49510843         NaN        NaN         NaN          
   8   0.7837311  0.47243048         NaN        NaN         NaN          
   8   0.8707163  0.46097297         NaN        NaN         NaN          
  10   0.8242175  0.74568480         NaN        NaN         NaN          
  12   0.7012606  0.21878426         NaN        NaN         NaN          
  14   0.1407463  0.07418264         NaN        NaN         NaN          
  15   0.7405050  0.38769341         NaN        NaN         NaN          
  15   0.8158114  0.48254494         NaN        NaN         NaN          
  16   0.9541871  0.31195949         NaN        NaN         NaN          
  16   0.9665047  0.93803816         NaN        NaN         NaN          
  17   0.3651204  0.26628504         NaN        NaN         NaN          
  17   0.4579053  0.42849612         NaN        NaN         NaN          
  17   0.9526995  0.80113324         NaN        NaN         NaN          
  18   0.8200459  0.21556815         NaN        NaN         NaN          
  19   0.5011145  0.49968899         NaN        NaN         NaN          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were r.a = 1, eps.high = 0.6880955
 and eps.low = 0.04362408.
[1] "Mon Mar 12 12:24:27 2018"
Partial Least Squares 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  ncomp  RMSE         Rsquared   MAE           Selected
  2      0.012578160  0.9960944  0.0099806447          
  3      0.006494240  0.9989537  0.0043885029          
  4      0.005573753  0.9992295  0.0041898624          
  5      0.004523030  0.9994603  0.0032032942          
  6      0.003090982  0.9997624  0.0022423158          
  7      0.001578985  0.9999317  0.0012119971          
  8      0.000986524  0.9999757  0.0007782836  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 8.
[1] "Mon Mar 12 12:24:33 2018"
Spike and Slab Regression 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  vars  RMSE         Rsquared   MAE          Selected
  2     0.060725694  0.9883197  0.049795436          
  4     0.007697378  0.9986835  0.005795317          
  6     0.004318423  0.9995591  0.003187985          
  7     0.002012160  0.9999000  0.001490375          
  8     0.001876718  0.9999125  0.001384023          
  9     0.001599029  0.9999363  0.001168469  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was vars = 9.
[1] "Mon Mar 12 12:24:53 2018"
Sparse Partial Least Squares 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  kappa       eta         K  RMSE          Rsquared   MAE           Selected
  0.06722808  0.69627356  2  0.0185073607  0.9917002  0.0141951169          
  0.10400319  0.49596339  2  0.0113230557  0.9968641  0.0081175968          
  0.17001819  0.71160519  1  0.0266392897  0.9826416  0.0196768367          
  0.17642427  0.80021869  9  0.0009634538  0.9999769  0.0007574589  *       
  0.17648030  0.58830301  2  0.0113230557  0.9968641  0.0081175968          
  0.18174993  0.75386285  7  0.0021862362  0.9998787  0.0016415689          
  0.18923544  0.73243848  8  0.0009766287  0.9999762  0.0007700954          
  0.19046266  0.25762584  4  0.0058433174  0.9991533  0.0043201408          
  0.28748532  0.44546359  6  0.0031301760  0.9997561  0.0022789227          
  0.31589344  0.95148080  7  0.0021862362  0.9998787  0.0016415689          
  0.32659739  0.36905154  3  0.0085184956  0.9981838  0.0052167440          
  0.32689157  0.24710582  9  0.0009634538  0.9999769  0.0007574589          
  0.33371802  0.17087020  9  0.0009634538  0.9999769  0.0007574589          
  0.39051978  0.71041050  1  0.0266392897  0.9826416  0.0196768367          
  0.39830356  0.88683113  1  0.0261708766  0.9797951  0.0205007376          
  0.40619605  0.21801415  4  0.0058433174  0.9991533  0.0043201408          
  0.41577173  0.33501586  8  0.0009766287  0.9999762  0.0007700954          
  0.42813376  0.72861692  2  0.0185073607  0.9917002  0.0141951169          
  0.45543662  0.34198841  3  0.0085184956  0.9981838  0.0052167440          
  0.49826173  0.02234585  2  0.0113230557  0.9968641  0.0081175968          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were K = 9, eta = 0.8002187 and kappa
 = 0.1764243.
[1] "Mon Mar 12 12:25:02 2018"
Supervised Principal Component Analysis 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  threshold  n.components  RMSE       Rsquared   MAE        Selected
  0.1344562  3             0.4615527  0.9986609  0.4614940  *       
  0.2080064  2             0.4620380  0.9868978  0.4614613          
  0.3400364  3             0.4615527  0.9986609  0.4614940          
  0.3528485  3             0.4615527  0.9986609  0.4614940          
  0.3529606  2             0.4620380  0.9868978  0.4614613          
  0.3634999  3             0.4615527  0.9986609  0.4614940          
  0.3784709  3             0.4615527  0.9986609  0.4614940          
  0.3809253  1             0.4687110  0.8364174  0.4615846          
  0.5749706  2             0.4620380  0.9868978  0.4614613          
  0.6317869  3             0.4615527  0.9986609  0.4614940          
  0.6531948  2             0.4620380  0.9868978  0.4614613          
  0.6537831  1             0.4687110  0.8364174  0.4615846          
  0.6674360  1             0.4687110  0.8364174  0.4615846          
  0.7810396  3             0.4615527  0.9986609  0.4614940          
  0.7966071  3             0.4615527  0.9986609  0.4614940          
  0.8123921  1             0.4687110  0.8364174  0.4615846          
  0.8315435  2             0.4620380  0.9868978  0.4614613          
  0.8562675  3             0.4615527  0.9986609  0.4614940          
  0.9108732  2             0.4620380  0.9868978  0.4614613          
  0.9965235  1             0.4687110  0.8364174  0.4615846          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were threshold = 0.1344562 and
 n.components = 3.
[1] "Mon Mar 12 12:25:10 2018"
Error : 'x' should be a character matrix with a single column for string kernel methods
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 12:25:16 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "svmBoundrangeString"            
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 12:25:22 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "svmExpoString"                  
Support Vector Machines with Linear Kernel 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  C            RMSE        Rsquared   MAE         Selected
    0.1264678  0.01281918  0.9983073  0.01165599  *       
    0.2717017  0.01281918  0.9983073  0.01165599          
    1.0721789  0.01281918  0.9983073  0.01165599          
    1.2249545  0.01281918  0.9983073  0.01165599          
    1.2263825  0.01281918  0.9983073  0.01165599          
    1.3684076  0.01281918  0.9983073  0.01165599          
    1.5988816  0.01281918  0.9983073  0.01165599          
    1.6402092  0.01281918  0.9983073  0.01165599          
   12.3339197  0.01281918  0.9983073  0.01165599          
   22.2665057  0.01281918  0.9983073  0.01165599          
   27.8175184  0.01281918  0.9983073  0.01165599          
   27.9882033  0.01281918  0.9983073  0.01165599          
   32.2570078  0.01281918  0.9983073  0.01165599          
  105.0981537  0.01281918  0.9983073  0.01165599          
  123.5633266  0.01281918  0.9983073  0.01165599          
  145.6015019  0.01281918  0.9983073  0.01165599          
  177.6817836  0.01281918  0.9983073  0.01165599          
  229.7646259  0.01281918  0.9983073  0.01165599          
  405.3709019  0.01281918  0.9983073  0.01165599          
  987.6471861  0.01281918  0.9983073  0.01165599          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was C = 0.1264678.
[1] "Mon Mar 12 12:25:31 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Linear Kernel 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  cost         RMSE        Rsquared   MAE         Selected
    0.1264678  0.01281918  0.9983073  0.01165599  *       
    0.2717017  0.01281918  0.9983073  0.01165599          
    1.0721789  0.01281918  0.9983073  0.01165599          
    1.2249545  0.01281918  0.9983073  0.01165599          
    1.2263825  0.01281918  0.9983073  0.01165599          
    1.3684076  0.01281918  0.9983073  0.01165599          
    1.5988816  0.01281918  0.9983073  0.01165599          
    1.6402092  0.01281918  0.9983073  0.01165599          
   12.3339197  0.01281918  0.9983073  0.01165599          
   22.2665057  0.01281918  0.9983073  0.01165599          
   27.8175184  0.01281918  0.9983073  0.01165599          
   27.9882033  0.01281918  0.9983073  0.01165599          
   32.2570078  0.01281918  0.9983073  0.01165599          
  105.0981537  0.01281918  0.9983073  0.01165599          
  123.5633266  0.01281918  0.9983073  0.01165599          
  145.6015019  0.01281918  0.9983073  0.01165599          
  177.6817836  0.01281918  0.9983073  0.01165599          
  229.7646259  0.01281918  0.9983073  0.01165599          
  405.3709019  0.01281918  0.9983073  0.01165599          
  987.6471861  0.01281918  0.9983073  0.01165599          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was cost = 0.1264678.
[1] "Mon Mar 12 12:25:39 2018"
L2 Regularized Support Vector Machine (dual) with Linear Kernel 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  cost          Loss  RMSE        Rsquared   MAE         Selected
    0.00629806  L2    0.06358126  0.9675652  0.05271700          
    0.01745915  L1    0.09050094  0.9241357  0.07480849          
    0.10887371  L2    0.04691929  0.9864247  0.03916333          
    0.13003490  L2    0.04632127  0.9875968  0.03866695          
    0.13023707  L2    0.04631744  0.9873723  0.03868626          
    0.15072570  L2    0.04646591  0.9866954  0.03881539          
    0.18549053  L2    0.04642066  0.9881206  0.03868075          
    0.19191062  L1    0.07091739  0.9489554  0.05875828          
    2.82727577  L1    0.06793721  0.9526483  0.05627324          
    6.21492805  L2    0.04622205  0.9879592  0.03856778          
    8.36227656  L1    0.06775799  0.9529035  0.05612244          
    8.43075963  L1    0.06775736  0.9529043  0.05612191          
   10.18744915  L1    0.06772359  0.9529340  0.05609420          
   49.20698963  L2    0.04603504  0.9877815  0.03844256  *       
   61.05943812  L2    0.04658644  0.9876784  0.03888770          
   75.99549020  L1    0.06766787  0.9530042  0.05604775          
   99.10385647  L1    0.06766586  0.9530067  0.05604607          
  139.61890783  L2    0.04638662  0.9877022  0.03861429          
  297.64832487  L1    0.06766144  0.9530123  0.05604239          
  975.81866782  L1    0.06765990  0.9530142  0.05604111          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were cost = 49.20699 and Loss = L2.
[1] "Mon Mar 12 12:25:46 2018"
Support Vector Machines with Polynomial Kernel 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  degree  scale         C             RMSE        Rsquared   MAE        
  1       4.257129e-03  2.523329e-01  0.02179363  0.9944544  0.017849700
  1       4.908706e-02  2.906122e-01  0.01203058  0.9983706  0.010608612
  2       2.041340e-04  1.019431e+03  0.01283170  0.9982743  0.011673160
  2       2.321038e-04  1.046129e+00  0.05493243  0.9752767  0.045534601
  2       9.043848e-04  7.241431e-01  0.01896556  0.9958209  0.015484400
  2       2.298348e-03  1.047470e+01  0.01241424  0.9982417  0.011216687
  2       1.314050e-02  1.034246e-01  0.01348219  0.9981415  0.011038358
  2       5.918892e-02  7.095292e-02  0.01125282  0.9985963  0.009453196
  2       7.632703e-02  1.689568e+02  0.01112559  0.9987467  0.009506369
  2       9.914015e-02  4.966333e+01  0.01126834  0.9987071  0.009663390
  2       1.745755e-01  6.914794e+02  0.01114666  0.9985884  0.009562540
  2       1.106186e+00  3.737577e+01  0.01047708  0.9987305  0.008903778
  3       1.313579e-05  1.148468e-01  0.19862615  0.8969517  0.163782971
  3       8.049855e-05  6.106281e+02  0.01285746  0.9982651  0.011696903
  3       1.431197e-04  2.610864e+00  0.02123113  0.9947851  0.017381636
  3       5.969379e-04  2.111202e+02  0.01282587  0.9981768  0.011670624
  3       6.499662e-04  5.869048e-01  0.02101615  0.9949584  0.017206217
  3       5.833206e-02  8.115357e-02  0.01256333  0.9983125  0.011072429
  3       7.284842e-02  1.342912e-01  0.01240235  0.9982732  0.010939588
  3       5.024796e-01  7.696049e-02  0.01336743  0.9968343  0.011765614
  Selected
          
          
          
          
          
          
          
          
          
          
          
  *       
          
          
          
          
          
          
          
          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were degree = 2, scale = 1.106186 and C
 = 37.37577.
[1] "Mon Mar 12 12:25:56 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Support Vector Machines with Radial Basis Function Kernel 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  sigma        C             RMSE        Rsquared   MAE         Selected
  0.003453270  109.76323030  0.02781266  0.9841627  0.01789520          
  0.003973999    0.11891232  0.08298889  0.8797458  0.04704871          
  0.004068630   19.47669821  0.02781266  0.9841627  0.01789520          
  0.004371151  186.53248387  0.02781266  0.9841627  0.01789520          
  0.008115649   18.42707196  0.02781266  0.9841627  0.01789520          
  0.009069147    0.16989637  0.06834354  0.9162682  0.03739877          
  0.009200543    0.97721888  0.02951817  0.9828082  0.01852850          
  0.055608612    0.27347480  0.05110075  0.9519567  0.02775870          
  0.066869815   13.12929025  0.02781266  0.9841627  0.01789520          
  0.099580006    0.17208056  0.06785195  0.9174301  0.03709717          
  0.104031606   14.55336322  0.02781266  0.9841627  0.01789520          
  0.210533834  273.86014509  0.02781266  0.9841627  0.01789520          
  0.229337315    9.66460966  0.02781266  0.9841627  0.01789520  *       
  0.464623647    0.07000231  0.10665409  0.8091644  0.06528752          
  0.936081434    0.13746042  0.07672613  0.8952761  0.04263747          
  1.032732435    0.17281603  0.06768722  0.9178093  0.03699423          
  1.099672480    0.57780420  0.03453333  0.9772851  0.02046172          
  1.546218504    0.03719606  0.13513006  0.7086051  0.09149967          
  2.626351917    0.57394898  0.03462872  0.9771609  0.02049474          
  3.408452581  678.55040950  0.02781266  0.9841627  0.01789520          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were sigma = 0.2293373 and C = 9.66461.
[1] "Mon Mar 12 12:26:07 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  C            RMSE        Rsquared   MAE         Selected
    0.1264678  0.08031853  0.8865233  0.04505206          
    0.2717017  0.05132483  0.9516045  0.02787247          
    1.0721789  0.02917926  0.9831293  0.01841510          
    1.2249545  0.02882218  0.9834629  0.01829321          
    1.2263825  0.02881272  0.9834718  0.01828462          
    1.3684076  0.02848141  0.9837598  0.01816446          
    1.5988816  0.02809842  0.9840032  0.01803943          
    1.6402092  0.02805395  0.9840280  0.01802640          
   12.3339197  0.02781266  0.9841627  0.01789520  *       
   22.2665057  0.02781266  0.9841627  0.01789520          
   27.8175184  0.02781266  0.9841627  0.01789520          
   27.9882033  0.02781266  0.9841627  0.01789520          
   32.2570078  0.02781266  0.9841627  0.01789520          
  105.0981537  0.02781266  0.9841627  0.01789520          
  123.5633266  0.02781266  0.9841627  0.01789520          
  145.6015019  0.02781266  0.9841627  0.01789520          
  177.6817836  0.02781266  0.9841627  0.01789520          
  229.7646259  0.02781266  0.9841627  0.01789520          
  405.3709019  0.02781266  0.9841627  0.01789520          
  987.6471861  0.02781266  0.9841627  0.01789520          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was C = 12.33392.
[1] "Mon Mar 12 12:26:18 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  sigma        C             RMSE        Rsquared   MAE         Selected
  0.003453270  109.76323030  0.02781266  0.9841627  0.01789520          
  0.003973999    0.11891232  0.08298889  0.8797458  0.04704871          
  0.004068630   19.47669821  0.02781266  0.9841627  0.01789520          
  0.004371151  186.53248387  0.02781266  0.9841627  0.01789520          
  0.008115649   18.42707196  0.02781266  0.9841627  0.01789520          
  0.009069147    0.16989637  0.06834354  0.9162682  0.03739877          
  0.009200543    0.97721888  0.02951817  0.9828082  0.01852850          
  0.055608612    0.27347480  0.05110075  0.9519567  0.02775870          
  0.066869815   13.12929025  0.02781266  0.9841627  0.01789520          
  0.099580006    0.17208056  0.06785195  0.9174301  0.03709717          
  0.104031606   14.55336322  0.02781266  0.9841627  0.01789520          
  0.210533834  273.86014509  0.02781266  0.9841627  0.01789520          
  0.229337315    9.66460966  0.02781266  0.9841627  0.01789520  *       
  0.464623647    0.07000231  0.10665409  0.8091644  0.06528752          
  0.936081434    0.13746042  0.07672613  0.8952761  0.04263747          
  1.032732435    0.17281603  0.06768722  0.9178093  0.03699423          
  1.099672480    0.57780420  0.03453333  0.9772851  0.02046172          
  1.546218504    0.03719606  0.13513006  0.7086051  0.09149967          
  2.626351917    0.57394898  0.03462872  0.9771609  0.02049474          
  3.408452581  678.55040950  0.02781266  0.9841627  0.01789520          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were sigma = 0.2293373 and C = 9.66461.
[1] "Mon Mar 12 12:32:26 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 12:32:32 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "svmSpectrumString"              
Bagged CART 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results:

  RMSE        Rsquared   MAE       
  0.04020256  0.9600671  0.03223642

[1] "Mon Mar 12 12:32:38 2018"
Partial Least Squares 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  ncomp  RMSE         Rsquared   MAE           Selected
  2      0.012578160  0.9960944  0.0099806447          
  3      0.006494240  0.9989537  0.0043885029          
  4      0.005573753  0.9992295  0.0041898624          
  5      0.004523030  0.9994603  0.0032032942          
  6      0.003090982  0.9997624  0.0022423158          
  7      0.001578985  0.9999317  0.0012119971          
  8      0.000986524  0.9999757  0.0007782836  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 8.
[1] "Mon Mar 12 12:34:22 2018"
Wang and Mendel Fuzzy Rules 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  num.labels  type.mf    RMSE        Rsquared   MAE         Selected
   4          TRIANGLE   0.05693847  0.9285650  0.04334671          
   5          TRAPEZOID  0.05111296  0.9628237  0.04179955          
   8          TRAPEZOID  0.03775480  0.9744786  0.02955293          
   8          TRIANGLE   0.03333405  0.9719539  0.02363342          
   9          GAUSSIAN   0.02595436  0.9833477  0.02018783          
   9          TRIANGLE   0.03309123  0.9724017  0.02316114          
  12          TRAPEZOID  0.04739631  0.9514269  0.02980527          
  14          GAUSSIAN   0.02581417  0.9833915  0.01972804          
  14          TRAPEZOID  0.06153990  0.9057580  0.03346308          
  14          TRIANGLE   0.06146175  0.9015099  0.03235619          
  16          TRIANGLE   0.07327921  0.8619814  0.03751277          
  17          GAUSSIAN   0.02454995  0.9850564  0.01899856  *       
  17          TRAPEZOID  0.08451294  0.8243809  0.04270429          
  17          TRIANGLE   0.08469243  0.8216013  0.04266184          
  18          TRIANGLE   0.09753626  0.7632251  0.05184760          
  19          TRAPEZOID  0.09310160  0.7849115  0.04941016          
  20          GAUSSIAN   0.02472083  0.9848280  0.01916534          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were num.labels = 17 and type.mf = GAUSSIAN.
[1] "Mon Mar 12 13:38:44 2018"
eXtreme Gradient Boosting 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  lambda        alpha         nrounds  eta         RMSE        Rsquared 
  0.0000470198  3.029478e-02   22      0.83169213  0.01238899  0.9962369
  0.0001096559  3.018679e-03   21      2.59983808  0.01269375  0.9960643
  0.0005013972  3.614314e-02    8      1.76850157  0.02058201  0.9943729
  0.0005810891  1.002521e-01   97      1.50298064  0.01351341  0.9955197
  0.0005818392  8.740072e-03   12      2.54480997  0.01410378  0.9955851
  0.0006569001  5.879146e-02   71      0.21432550  0.01326628  0.9956822
  0.0007804667  4.594015e-02   83      2.69776177  0.01276386  0.9959814
  0.0008028356  1.941463e-04   34      2.65785164  0.01216008  0.9963825
  0.0074964069  1.687788e-03   56      2.47235952  0.01256112  0.9961236
  0.0144189761  5.720105e-01   69      0.14759345  0.02043693  0.9901227
  0.0184490471  7.002574e-04   31      1.60163794  0.01261976  0.9960832
  0.0185744369  1.720003e-04  100      1.70422316  0.01245080  0.9961760
  0.0217360307  7.150740e-05   96      1.42288636  0.01247229  0.9961669
  0.0803892244  3.564942e-02   10      2.46521591  0.01595713  0.9950894
  0.0961691112  2.717413e-01    9      0.36006487  0.02456739  0.9898497
  0.1153348289  1.230469e-04   43      0.37374874  0.01232123  0.9962382
  0.1437862388  4.732377e-04   85      1.82531300  0.01229315  0.9962638
  0.1911338598  4.396272e-02   15      0.01768144  0.01425444  0.9951601
  0.3583985477  5.127929e-04   29      1.16493867  0.01241661  0.9961932
  0.9607653197  1.293389e-05   13      1.21321921  0.01322625  0.9960237
  MAE          Selected
  0.009754828          
  0.009725207          
  0.016248808          
  0.010394823          
  0.010892034          
  0.010248061          
  0.009799356          
  0.009420087  *       
  0.009639460          
  0.014625859          
  0.009735888          
  0.009567444          
  0.009673502          
  0.012474411          
  0.017815129          
  0.009456145          
  0.009468718          
  0.010991864          
  0.009465727          
  0.010330757          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 34, lambda =
 0.0008028356, alpha = 0.0001941463 and eta = 2.657852.
[1] "Mon Mar 12 13:39:02 2018"
eXtreme Gradient Boosting 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  eta         max_depth  gamma       colsample_bytree  min_child_weight
  0.04824136   8         5.89500523  0.6528176          8              
  0.05292379   9         1.20021623  0.4527824         13              
  0.05598015   8         8.21738636  0.4231594          5              
  0.06995097   6         8.48269990  0.6876366         12              
  0.07598614   1         4.04406403  0.6133487          0              
  0.08499722   8         0.05893814  0.3298109         19              
  0.12133511   5         8.66612695  0.3588833          5              
  0.12947221   7         2.77230710  0.3162106         16              
  0.16996586   4         3.88312891  0.4098309          1              
  0.18207158   4         5.33879313  0.3428897          1              
  0.20326478   3         8.85950547  0.4276982          3              
  0.25595545   3         1.24582912  0.4168656         18              
  0.33599424   5         8.24119840  0.4857983         17              
  0.40927966  10         0.49197816  0.4828972          8              
  0.42565542   8         0.71441834  0.4965513          0              
  0.49619373   8         8.99253925  0.5147984         19              
  0.50902870   4         6.08437667  0.6721553         12              
  0.57021573   2         4.74295455  0.3508352          8              
  0.57737946   9         5.00993546  0.6116425         10              
  0.59974237   3         5.68074385  0.6229936          2              
  subsample  nrounds  RMSE        Rsquared   MAE         Selected
  0.6968263  341      0.15743251  0.7868786  0.12187842          
  0.6047576  797      0.08396701  0.9432531  0.05869105          
  0.4931205  782      0.20051411        NaN  0.16554465          
  0.6597905  353      0.18258796  0.6859444  0.14647033          
  0.4120717  997      0.15736127  0.7783687  0.12119460          
  0.9567661  857      0.03089139  0.9797229  0.02300296  *       
  0.8567454  209      0.16894783  0.7356972  0.13421682          
  0.2878859  135      0.15277805  0.8183144  0.11981960          
  0.8313166  911      0.11671662  0.8710338  0.08529358          
  0.6636637  654      0.14454807  0.7528022  0.11004126          
  0.3942141  381      0.20055954        NaN  0.16564423          
  0.6511107  813      0.08156712  0.9090689  0.05792855          
  0.9549835  575      0.15992966  0.6576431  0.12471014          
  0.7587886  632      0.05359481  0.9482106  0.03960675          
  0.7161342  364      0.06783654  0.9094278  0.05096109          
  0.2577258  379      0.20075024        NaN  0.16589969          
  0.8806905  832      0.13365097  0.6853169  0.10212684          
  0.7974970  668      0.13576827  0.6658791  0.10376328          
  0.5332557  353      0.14816541  0.5345430  0.11814705          
  0.7239872  654      0.13642381  0.6393109  0.10612390          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 857, max_depth = 8, eta
 = 0.08499722, gamma = 0.05893814, colsample_bytree =
 0.3298109, min_child_weight = 19 and subsample = 0.9567661.
[1] "Mon Mar 12 13:41:48 2018"
Self-Organizing Maps 

752 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 503, 500 
Resampling results across tuning parameters:

  xdim  ydim  topo         user.weights  RMSE        Rsquared   MAE       
   1    14    rectangular  0.34887889    0.06287955  0.9030381  0.05026397
   3    17    rectangular  0.93053741    0.04061077  0.9596074  0.03197396
   5     5    rectangular  0.06485002    0.05275869  0.9304418  0.04169293
   5     9    hexagonal    0.73631755    0.04088361  0.9585999  0.03317963
   5    10    rectangular  0.87753002    0.04214610  0.9558599  0.03299812
   6    19    rectangular  0.14919762    0.03427821  0.9711193  0.02669579
   7     8    hexagonal    0.51768561    0.04067403  0.9596016  0.03219721
   7     9    hexagonal    0.83333563    0.03912214  0.9622620  0.03156137
   8     8    rectangular  0.37195604    0.03756921  0.9653592  0.02980500
   8    11    hexagonal    0.98405330    0.03495532  0.9698007  0.02722092
   8    13    rectangular  0.19764982    0.03557377  0.9683789  0.02811702
   8    16    rectangular  0.75596931    0.03194460  0.9754302  0.02434201
   8    18    rectangular  0.27719184    0.03105509  0.9764427  0.02461693
  10    17    hexagonal    0.52817144    0.03037713  0.9776754  0.02397338
  12    15    rectangular  0.75773865    0.03127212  0.9768550  0.02396614
  15    15    rectangular  0.29975564    0.02857538  0.9805733  0.02223859
  15    17    rectangular  0.77833412    0.02781978  0.9819550  0.02154486
  16    20    hexagonal    0.61224152    0.02665988  0.9841995  0.02045223
  16    20    rectangular  0.87433283    0.03144877  0.9781402  0.02336992
  17    20    hexagonal    0.67937735    0.02773129  0.9821355  0.02109407
  Selected
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
  *       
          
          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were xdim = 16, ydim = 20, user.weights
 = 0.6122415 and topo = hexagonal.
[1] "Mon Mar 12 13:42:32 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  No variation for for: V11, V12
Fitting Repeat 1 

# weights:  144
initial  value 607.966047 
iter  10 value 382.722850
iter  20 value 382.244049
final  value 382.241886 
converged
Fitting Repeat 2 

# weights:  144
initial  value 1050.321548 
iter  10 value 329.366170
iter  20 value 328.848911
iter  30 value 328.846261
final  value 328.845227 
converged
Fitting Repeat 3 

# weights:  144
initial  value 461.755291 
iter  10 value 341.850390
final  value 341.716816 
converged
Fitting Repeat 4 

# weights:  144
initial  value 543.317989 
iter  10 value 332.068331
iter  20 value 331.615507
final  value 331.613257 
converged
Fitting Repeat 5 

# weights:  144
initial  value 666.981077 
iter  10 value 404.026015
iter  20 value 403.454329
final  value 403.451058 
converged
Fitting Repeat 1 

# weights:  78
initial  value 916.299951 
iter  10 value 367.456721
iter  20 value 360.928629
iter  30 value 360.582582
iter  40 value 360.546543
iter  50 value 348.226609
iter  60 value 344.903229
iter  70 value 344.888809
iter  80 value 344.850335
iter  90 value 344.803955
iter 100 value 344.686619
final  value 344.686619 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  78
initial  value 660.555219 
iter  10 value 361.691164
iter  20 value 360.619557
iter  30 value 360.237970
iter  40 value 345.091371
iter  50 value 344.772495
iter  60 value 344.699293
iter  70 value 344.667474
iter  80 value 344.655817
iter  90 value 344.651547
iter 100 value 344.650435
final  value 344.650435 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  78
initial  value 764.412754 
iter  10 value 365.989432
iter  20 value 360.953539
iter  30 value 360.567764
iter  40 value 360.544977
iter  50 value 350.956042
iter  60 value 344.925237
iter  70 value 344.899445
iter  80 value 344.898621
iter  90 value 344.851485
iter 100 value 344.725245
final  value 344.725245 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  78
initial  value 1029.620374 
iter  10 value 365.239067
iter  20 value 360.924233
iter  30 value 360.569791
iter  40 value 360.548891
iter  50 value 360.343991
iter  60 value 345.304373
iter  70 value 344.859327
iter  80 value 344.800973
iter  90 value 344.722952
iter 100 value 344.678410
final  value 344.678410 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  78
initial  value 525.210357 
iter  10 value 360.583058
iter  20 value 360.546701
iter  30 value 346.067635
iter  40 value 344.918594
iter  50 value 344.877543
iter  60 value 344.823195
iter  70 value 344.718717
iter  80 value 344.668458
iter  90 value 344.656258
iter 100 value 344.648390
final  value 344.648390 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  188
initial  value 795.886067 
iter  10 value 324.322706
iter  20 value 321.539125
iter  20 value 321.539124
iter  20 value 321.539124
final  value 321.539124 
converged
Fitting Repeat 2 

# weights:  188
initial  value 849.210485 
iter  10 value 374.819168
final  value 370.983172 
converged
Fitting Repeat 3 

# weights:  188
initial  value 1042.439440 
iter  10 value 371.028363
iter  20 value 367.495626
iter  20 value 367.495625
iter  20 value 367.495625
final  value 367.495625 
converged
Fitting Repeat 4 

# weights:  188
initial  value 723.477758 
iter  10 value 350.205924
final  value 345.492342 
converged
Fitting Repeat 5 

# weights:  188
initial  value 1040.269125 
iter  10 value 361.552584
final  value 358.785327 
converged
Fitting Repeat 1 

# weights:  177
initial  value 467.922275 
iter  10 value 360.606857
iter  20 value 360.580812
final  value 360.580608 
converged
Fitting Repeat 2 

# weights:  177
initial  value 521.632056 
iter  10 value 360.618450
iter  20 value 360.580793
final  value 360.580618 
converged
Fitting Repeat 3 

# weights:  177
initial  value 844.864944 
iter  10 value 360.618892
iter  20 value 360.580793
final  value 360.580619 
converged
Fitting Repeat 4 

# weights:  177
initial  value 565.419038 
iter  10 value 360.684732
iter  20 value 360.580961
final  value 360.580622 
converged
Fitting Repeat 5 

# weights:  177
initial  value 577.747056 
iter  10 value 360.775782
iter  20 value 360.585332
iter  30 value 360.580654
final  value 360.580614 
converged
Fitting Repeat 1 

# weights:  122
initial  value 650.153112 
iter  10 value 373.200400
iter  20 value 370.767610
final  value 370.767518 
converged
Fitting Repeat 2 

# weights:  122
initial  value 1097.713581 
iter  10 value 373.053666
iter  20 value 370.767555
final  value 370.767538 
converged
Fitting Repeat 3 

# weights:  122
initial  value 706.176190 
iter  10 value 371.960395
iter  20 value 370.767526
final  value 370.767521 
converged
Fitting Repeat 4 

# weights:  122
initial  value 936.856300 
iter  10 value 370.978014
iter  20 value 370.767520
iter  20 value 370.767519
iter  20 value 370.767518
final  value 370.767518 
converged
Fitting Repeat 5 

# weights:  122
initial  value 851.863165 
iter  10 value 370.908181
iter  20 value 370.767521
iter  20 value 370.767519
iter  20 value 370.767519
final  value 370.767519 
converged
Fitting Repeat 1 

# weights:  210
initial  value 410.952273 
iter  10 value 330.384672
iter  20 value 330.320633
iter  30 value 330.318423
iter  30 value 330.318422
iter  30 value 330.318421
final  value 330.318421 
converged
Fitting Repeat 2 

# weights:  210
initial  value 531.140883 
iter  10 value 339.637003
iter  20 value 324.111914
iter  30 value 322.789730
iter  40 value 322.721989
iter  50 value 322.711336
iter  60 value 322.704146
iter  70 value 322.693220
iter  80 value 322.687786
iter  90 value 322.685968
iter 100 value 322.683635
final  value 322.683635 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  210
initial  value 1093.688692 
iter  10 value 357.817585
iter  20 value 357.719093
iter  30 value 357.715041
final  value 357.715024 
converged
Fitting Repeat 4 

# weights:  210
initial  value 512.136782 
iter  10 value 353.462890
iter  20 value 353.411774
final  value 353.409772 
converged
Fitting Repeat 5 

# weights:  210
initial  value 685.412081 
iter  10 value 361.555682
iter  20 value 361.099727
iter  30 value 361.071053
final  value 361.070842 
converged
Fitting Repeat 1 

# weights:  144
initial  value 1266.354978 
iter  10 value 389.222318
iter  20 value 388.139222
iter  30 value 388.128485
iter  40 value 388.088057
iter  50 value 376.527039
iter  60 value 369.971267
iter  70 value 369.912372
iter  80 value 369.879452
iter  90 value 369.702960
iter 100 value 369.611730
final  value 369.611730 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  144
initial  value 960.879799 
iter  10 value 349.717836
iter  20 value 337.603019
iter  30 value 336.779209
iter  40 value 336.568191
iter  50 value 336.539497
iter  60 value 336.525704
iter  70 value 336.508818
iter  80 value 336.500393
iter  90 value 336.496319
iter 100 value 336.493544
final  value 336.493544 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  144
initial  value 751.070764 
iter  10 value 355.557858
iter  20 value 350.312215
iter  30 value 349.778245
iter  40 value 349.701025
final  value 349.697015 
converged
Fitting Repeat 4 

# weights:  144
initial  value 793.532527 
iter  10 value 363.558381
iter  20 value 363.265840
iter  30 value 363.262914
final  value 363.262740 
converged
Fitting Repeat 5 

# weights:  144
initial  value 1093.285422 
iter  10 value 382.637251
iter  20 value 382.547598
iter  30 value 382.545076
iter  40 value 382.544689
iter  50 value 382.537391
iter  60 value 382.520695
iter  70 value 382.250982
iter  80 value 366.498013
iter  90 value 366.359689
iter 100 value 366.247165
final  value 366.247165 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  56
initial  value 611.764848 
iter  10 value 363.028557
iter  20 value 352.460116
iter  30 value 352.400532
iter  40 value 352.352168
iter  50 value 340.565410
iter  60 value 330.146077
iter  70 value 330.102221
iter  80 value 330.082208
iter  90 value 330.064902
iter 100 value 330.057198
final  value 330.057198 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 651.395550 
iter  10 value 370.199640
iter  20 value 358.633019
iter  30 value 358.576336
iter  40 value 358.562877
iter  50 value 343.488659
iter  60 value 342.942751
iter  70 value 342.827768
iter  80 value 342.787356
iter  90 value 342.764943
iter 100 value 342.760939
final  value 342.760939 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 683.041528 
iter  10 value 360.507728
iter  20 value 359.862132
iter  30 value 359.761576
iter  40 value 359.754283
iter  50 value 345.747632
iter  60 value 345.242615
iter  70 value 345.230602
iter  80 value 345.225133
iter  90 value 345.223330
iter 100 value 345.214399
final  value 345.214399 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 794.692120 
iter  10 value 407.865179
iter  20 value 394.406874
iter  30 value 394.318178
iter  40 value 394.214441
iter  50 value 375.051985
iter  60 value 374.329341
iter  70 value 374.318627
iter  80 value 374.315874
iter  90 value 374.307887
iter 100 value 374.292297
final  value 374.292297 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 842.565175 
iter  10 value 391.448518
iter  20 value 376.720534
iter  30 value 376.533238
iter  40 value 376.514017
iter  50 value 376.352670
iter  60 value 362.905880
iter  70 value 362.176181
iter  80 value 362.145546
iter  90 value 362.134028
iter 100 value 362.125593
final  value 362.125593 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  23
initial  value 668.386687 
iter  10 value 364.155434
iter  20 value 363.362438
iter  30 value 363.287521
iter  40 value 363.283598
iter  50 value 355.118475
iter  60 value 350.930960
iter  70 value 350.870601
iter  80 value 350.866754
iter  90 value 350.864956
final  value 350.864588 
converged
Fitting Repeat 2 

# weights:  23
initial  value 725.878516 
iter  10 value 350.802063
iter  20 value 349.855710
iter  30 value 349.705863
iter  40 value 338.617259
iter  50 value 331.964788
iter  60 value 331.703328
iter  70 value 331.667956
iter  80 value 331.643517
iter  90 value 331.635315
iter 100 value 331.633454
final  value 331.633454 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  23
initial  value 533.908055 
iter  10 value 377.008300
iter  20 value 376.493054
iter  30 value 376.399353
iter  40 value 356.277768
iter  50 value 355.966777
iter  60 value 355.936097
iter  70 value 355.923943
iter  80 value 355.919413
iter  90 value 355.918829
iter 100 value 355.918674
final  value 355.918674 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  23
initial  value 784.040185 
iter  10 value 307.411326
iter  20 value 306.142239
iter  30 value 305.912294
iter  40 value 305.848292
iter  50 value 305.843146
iter  60 value 305.763172
iter  70 value 296.487553
iter  80 value 293.993741
iter  90 value 293.970558
iter 100 value 293.968940
final  value 293.968940 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  23
initial  value 771.704039 
iter  10 value 353.405017
iter  20 value 352.148022
iter  30 value 351.945494
iter  40 value 351.916639
iter  50 value 351.320336
iter  60 value 334.205204
iter  70 value 334.177243
iter  80 value 334.176352
final  value 334.176347 
converged
Fitting Repeat 1 

# weights:  210
initial  value 1005.136625 
iter  10 value 369.458147
iter  20 value 367.181274
iter  30 value 367.159730
final  value 367.157389 
converged
Fitting Repeat 2 

# weights:  210
initial  value 689.045614 
iter  10 value 356.005052
iter  20 value 353.897398
final  value 353.876975 
converged
Fitting Repeat 3 

# weights:  210
initial  value 870.649818 
iter  10 value 349.434853
iter  20 value 347.041520
iter  30 value 347.017976
final  value 347.016289 
converged
Fitting Repeat 4 

# weights:  210
initial  value 824.193455 
iter  10 value 361.988569
iter  20 value 359.726502
iter  30 value 359.704939
final  value 359.702910 
converged
Fitting Repeat 5 

# weights:  210
initial  value 701.308411 
iter  10 value 374.958009
iter  20 value 372.920641
final  value 372.901151 
converged
Fitting Repeat 1 

# weights:  199
initial  value 622.667743 
iter  10 value 364.839406
iter  20 value 360.376470
iter  30 value 360.335985
final  value 360.333010 
converged
Fitting Repeat 2 

# weights:  199
initial  value 1212.397265 
iter  10 value 362.424892
iter  20 value 360.349241
iter  30 value 360.333367
iter  40 value 360.332501
iter  50 value 345.377905
iter  60 value 343.490241
iter  70 value 343.367283
iter  80 value 343.299112
iter  90 value 343.282470
iter 100 value 343.242001
final  value 343.242001 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 949.336551 
iter  10 value 367.436743
iter  20 value 360.406416
iter  30 value 360.333397
final  value 360.333113 
converged
Fitting Repeat 4 

# weights:  199
initial  value 1118.179734 
iter  10 value 365.836567
iter  20 value 360.387967
iter  30 value 360.332995
final  value 360.332976 
converged
Fitting Repeat 5 

# weights:  199
initial  value 966.079092 
iter  10 value 368.187320
iter  20 value 360.415069
iter  30 value 360.337021
final  value 360.332955 
converged
Fitting Repeat 1 

# weights:  100
initial  value 725.938301 
iter  10 value 345.219402
iter  20 value 339.665007
iter  30 value 339.617332
iter  40 value 339.615935
iter  50 value 339.594884
iter  60 value 327.805561
iter  70 value 325.653674
iter  80 value 325.411580
iter  90 value 325.382431
iter 100 value 325.365955
final  value 325.365955 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  100
initial  value 765.349038 
iter  10 value 351.866516
iter  20 value 345.810515
iter  30 value 345.759335
iter  40 value 345.757233
iter  50 value 345.754800
iter  60 value 345.754484
iter  70 value 345.754128
iter  80 value 345.753715
iter  90 value 345.753222
iter 100 value 345.752614
final  value 345.752614 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  100
initial  value 1034.604398 
iter  10 value 393.970156
iter  20 value 387.807666
iter  30 value 387.754084
iter  40 value 387.751894
iter  50 value 387.742364
iter  60 value 372.971511
iter  70 value 371.059930
iter  80 value 370.983499
iter  90 value 370.963247
iter 100 value 370.943176
final  value 370.943176 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  100
initial  value 564.624513 
iter  10 value 364.602665
iter  20 value 361.780941
iter  30 value 361.764736
iter  40 value 361.761020
iter  50 value 361.760020
iter  60 value 361.758760
iter  70 value 361.757013
iter  80 value 361.754251
iter  90 value 361.748930
iter 100 value 361.733893
final  value 361.733893 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  100
initial  value 717.102358 
iter  10 value 321.149916
iter  20 value 314.471274
iter  30 value 314.411119
iter  40 value 314.409910
iter  50 value 314.407929
iter  60 value 301.612977
iter  70 value 300.333862
iter  80 value 300.277192
iter  90 value 300.264965
iter 100 value 300.255649
final  value 300.255649 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  56
initial  value 687.524416 
iter  10 value 352.608954
iter  20 value 352.463471
iter  20 value 352.463469
iter  20 value 352.463468
final  value 352.463468 
converged
Fitting Repeat 2 

# weights:  56
initial  value 964.533447 
iter  10 value 357.294734
iter  20 value 357.121743
final  value 357.121722 
converged
Fitting Repeat 3 

# weights:  56
initial  value 693.063443 
iter  10 value 372.712761
iter  20 value 372.597784
iter  20 value 372.597783
iter  20 value 372.597782
final  value 372.597782 
converged
Fitting Repeat 4 

# weights:  56
initial  value 851.519204 
iter  10 value 369.711437
iter  20 value 369.483022
iter  30 value 369.479037
iter  30 value 369.479037
iter  30 value 369.479036
final  value 369.479036 
converged
Fitting Repeat 5 

# weights:  56
initial  value 504.283829 
iter  10 value 323.219763
final  value 323.161251 
converged
Fitting Repeat 1 

# weights:  12
initial  value 696.540798 
iter  10 value 335.362443
iter  20 value 334.045531
iter  30 value 333.806427
iter  40 value 327.862320
iter  50 value 314.744395
iter  60 value 314.547741
final  value 314.547335 
converged
Fitting Repeat 2 

# weights:  12
initial  value 849.692813 
iter  10 value 355.182469
iter  20 value 354.003323
iter  30 value 353.947995
iter  40 value 353.945479
iter  50 value 353.940961
iter  60 value 353.929229
iter  70 value 353.852731
iter  80 value 337.341100
iter  90 value 335.563578
iter 100 value 335.561911
final  value 335.561911 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  12
initial  value 874.183129 
iter  10 value 353.307569
iter  20 value 352.067801
iter  30 value 352.052230
iter  40 value 345.733149
iter  50 value 336.030908
iter  60 value 335.944257
final  value 335.944108 
converged
Fitting Repeat 4 

# weights:  12
initial  value 619.575493 
iter  10 value 352.378889
iter  20 value 351.750722
iter  30 value 351.699810
final  value 351.699794 
converged
Fitting Repeat 5 

# weights:  12
initial  value 817.568283 
iter  10 value 339.073940
iter  20 value 337.783742
iter  30 value 337.712992
iter  40 value 337.458093
iter  50 value 322.557601
iter  60 value 322.476099
final  value 322.475806 
converged
Fitting Repeat 1 

# weights:  133
initial  value 875.034786 
iter  10 value 360.693879
iter  20 value 360.115717
iter  30 value 343.819770
iter  40 value 343.293211
iter  50 value 343.234630
iter  60 value 343.214725
iter  70 value 343.202950
iter  80 value 343.197482
iter  90 value 343.195101
iter 100 value 343.193436
final  value 343.193436 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  133
initial  value 672.396239 
iter  10 value 347.477524
iter  20 value 346.305379
iter  30 value 345.868621
iter  40 value 345.767851
iter  50 value 345.764748
iter  60 value 345.762927
iter  70 value 345.752731
iter  80 value 345.735973
iter  90 value 345.605627
iter 100 value 332.054224
final  value 332.054224 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 862.103293 
iter  10 value 368.158431
iter  20 value 368.122743
iter  30 value 368.045517
iter  40 value 368.030350
iter  50 value 368.028530
iter  50 value 368.028527
iter  50 value 368.028526
final  value 368.028526 
converged
Fitting Repeat 4 

# weights:  133
initial  value 933.671640 
iter  10 value 379.040680
iter  20 value 378.697227
iter  30 value 378.686533
iter  40 value 378.685069
iter  50 value 364.876102
iter  60 value 364.270367
iter  70 value 364.004788
iter  80 value 363.944839
iter  90 value 363.906086
iter 100 value 363.892156
final  value 363.892156 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  133
initial  value 497.751257 
iter  10 value 359.355298
iter  20 value 358.839537
iter  30 value 358.808988
iter  40 value 358.791652
iter  50 value 358.743909
iter  60 value 344.453387
iter  70 value 340.601841
iter  80 value 340.426752
iter  90 value 340.357284
iter 100 value 340.295895
final  value 340.295895 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  188
initial  value 1242.656568 
iter  10 value 373.376430
iter  20 value 372.763520
final  value 372.763503 
converged
Fitting Repeat 2 

# weights:  188
initial  value 949.319244 
iter  10 value 377.655922
iter  20 value 375.197387
final  value 375.195626 
converged
Fitting Repeat 3 

# weights:  188
initial  value 732.882306 
iter  10 value 372.073102
iter  20 value 369.900842
final  value 369.900227 
converged
Fitting Repeat 4 

# weights:  188
initial  value 1067.500638 
iter  10 value 389.927523
iter  20 value 388.486019
final  value 388.485620 
converged
Fitting Repeat 5 

# weights:  188
initial  value 703.382879 
iter  10 value 375.050606
iter  20 value 374.062120
final  value 374.061885 
converged
Fitting Repeat 1 

# weights:  78
initial  value 767.673770 
iter  10 value 360.579504
iter  20 value 360.327357
final  value 360.327025 
converged
Fitting Repeat 2 

# weights:  78
initial  value 643.838696 
iter  10 value 360.510463
iter  20 value 360.326949
final  value 360.326939 
converged
Fitting Repeat 3 

# weights:  78
initial  value 659.095725 
iter  10 value 360.508442
iter  20 value 360.327196
final  value 360.327044 
converged
Fitting Repeat 4 

# weights:  78
initial  value 756.875255 
iter  10 value 360.565539
iter  20 value 360.327196
final  value 360.326379 
converged
Fitting Repeat 5 

# weights:  78
initial  value 545.443517 
iter  10 value 360.427615
iter  20 value 360.326103
iter  20 value 360.326103
iter  20 value 360.326103
final  value 360.326103 
converged
Fitting Repeat 1 

# weights:  144
initial  value 847.926136 
iter  10 value 416.584339
iter  20 value 416.184461
iter  30 value 416.176136
iter  40 value 416.173366
iter  40 value 416.173363
iter  40 value 416.173362
final  value 416.173362 
converged
Fitting Repeat 2 

# weights:  144
initial  value 781.527081 
iter  10 value 396.139999
iter  20 value 360.047532
iter  30 value 359.953980
iter  40 value 359.953659
iter  40 value 359.953658
iter  40 value 359.953658
final  value 359.953658 
converged
Fitting Repeat 3 

# weights:  144
initial  value 776.278835 
iter  10 value 347.671035
iter  20 value 346.273269
iter  30 value 346.169236
final  value 346.167945 
converged
Fitting Repeat 4 

# weights:  144
initial  value 967.735736 
iter  10 value 357.965131
iter  20 value 355.478926
iter  30 value 355.457442
final  value 355.457269 
converged
Fitting Repeat 5 

# weights:  144
initial  value 831.145524 
iter  10 value 392.913562
iter  20 value 373.738974
iter  30 value 373.101909
final  value 373.098284 
converged
Fitting Repeat 1 

# weights:  221
initial  value 752.535980 
iter  10 value 362.354439
iter  20 value 361.972631
iter  20 value 361.972631
iter  20 value 361.972631
final  value 361.972631 
converged
Fitting Repeat 2 

# weights:  221
initial  value 632.819988 
iter  10 value 362.650421
final  value 361.972631 
converged
Fitting Repeat 3 

# weights:  221
initial  value 1144.182864 
iter  10 value 363.248805
final  value 361.972631 
converged
Fitting Repeat 4 

# weights:  221
initial  value 1236.057765 
iter  10 value 362.585505
iter  20 value 361.972635
iter  20 value 361.972632
iter  20 value 361.972631
final  value 361.972631 
converged
Fitting Repeat 5 

# weights:  221
initial  value 796.989753 
iter  10 value 367.154907
iter  20 value 361.972739
final  value 361.972631 
converged
Fitting Repeat 1 

# weights:  56
initial  value 826.567835 
iter  10 value 342.012080
iter  20 value 339.032027
iter  30 value 339.007056
iter  40 value 339.000734
iter  50 value 338.906426
iter  60 value 320.168430
iter  70 value 319.602652
iter  80 value 319.562753
iter  90 value 319.538346
iter 100 value 319.529380
final  value 319.529380 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 658.415857 
iter  10 value 358.201009
iter  20 value 356.176497
iter  30 value 356.170750
iter  40 value 351.974288
iter  50 value 340.307494
iter  60 value 340.035487
iter  70 value 340.008367
iter  80 value 339.995048
iter  90 value 339.977399
iter 100 value 339.966803
final  value 339.966803 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 790.491140 
iter  10 value 361.383073
iter  20 value 358.758772
iter  30 value 358.738871
iter  40 value 358.734574
iter  50 value 358.719180
iter  60 value 358.612505
iter  70 value 343.318783
iter  80 value 342.968948
iter  90 value 342.921626
iter 100 value 342.898854
final  value 342.898854 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 950.398106 
iter  10 value 358.990737
iter  20 value 354.989778
iter  30 value 354.964339
iter  40 value 354.959792
iter  50 value 351.067775
iter  60 value 339.168515
iter  70 value 338.790608
iter  80 value 338.732438
iter  90 value 338.716652
iter 100 value 338.699921
final  value 338.699921 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 830.354353 
iter  10 value 336.897150
iter  20 value 332.519976
iter  30 value 332.490991
iter  40 value 332.486966
iter  50 value 332.474919
iter  60 value 332.457525
iter  70 value 332.075048
iter  80 value 315.392644
iter  90 value 314.888834
iter 100 value 314.823736
final  value 314.823736 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  144
initial  value 758.155774 
iter  10 value 366.876313
iter  20 value 365.804485
final  value 365.795340 
converged
Fitting Repeat 2 

# weights:  144
initial  value 790.894446 
iter  10 value 345.141501
iter  20 value 344.303142
iter  30 value 344.296843
final  value 344.295491 
converged
Fitting Repeat 3 

# weights:  144
initial  value 460.058248 
iter  10 value 350.729516
final  value 350.608767 
converged
Fitting Repeat 4 

# weights:  144
initial  value 924.464546 
iter  10 value 353.831334
iter  20 value 352.920474
iter  30 value 352.913254
final  value 352.911900 
converged
Fitting Repeat 5 

# weights:  144
initial  value 593.540888 
iter  10 value 334.353763
iter  20 value 333.910129
final  value 333.907961 
converged
Fitting Repeat 1 

# weights:  78
initial  value 479.654408 
iter  10 value 351.913419
iter  20 value 350.136150
iter  30 value 350.036271
iter  40 value 350.028304
iter  50 value 337.309966
iter  60 value 335.243718
iter  70 value 335.035484
iter  80 value 335.008846
iter  90 value 334.957191
iter 100 value 334.939583
final  value 334.939583 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  78
initial  value 545.046266 
iter  10 value 352.543887
iter  20 value 350.190400
iter  30 value 350.031835
iter  40 value 350.011146
iter  50 value 349.888493
iter  60 value 335.263662
iter  70 value 335.178077
iter  80 value 335.167256
iter  90 value 335.042919
iter 100 value 334.972085
final  value 334.972085 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  78
initial  value 779.883715 
iter  10 value 355.948758
iter  20 value 350.415362
iter  30 value 350.055739
iter  40 value 350.028962
iter  50 value 338.746529
iter  60 value 335.119705
iter  70 value 335.062645
iter  80 value 334.946677
iter  90 value 334.934613
iter 100 value 334.929909
final  value 334.929909 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  78
initial  value 861.009105 
iter  10 value 351.020547
iter  20 value 350.079419
iter  30 value 350.029797
iter  40 value 349.978119
iter  50 value 335.614250
iter  60 value 335.182602
iter  70 value 335.180997
iter  80 value 335.166166
iter  90 value 335.122295
iter 100 value 335.025877
final  value 335.025877 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  78
initial  value 655.787358 
iter  10 value 355.197794
iter  20 value 350.484937
iter  30 value 350.069323
iter  40 value 350.030004
iter  50 value 349.926348
iter  60 value 336.545018
iter  70 value 335.188233
iter  80 value 335.180590
iter  90 value 335.147340
iter 100 value 335.030261
final  value 335.030261 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  188
initial  value 1030.957675 
iter  10 value 332.530111
final  value 329.530392 
converged
Fitting Repeat 2 

# weights:  188
initial  value 578.925737 
iter  10 value 353.725664
final  value 353.677657 
converged
Fitting Repeat 3 

# weights:  188
initial  value 1109.111536 
iter  10 value 344.708379
final  value 342.152423 
converged
Fitting Repeat 4 

# weights:  188
initial  value 661.416257 
iter  10 value 309.596642
final  value 307.148967 
converged
Fitting Repeat 5 

# weights:  188
initial  value 645.644078 
iter  10 value 417.247955
final  value 414.744045 
converged
Fitting Repeat 1 

# weights:  177
initial  value 1132.509715 
iter  10 value 350.208245
iter  20 value 350.065638
iter  30 value 350.062473
final  value 350.062456 
converged
Fitting Repeat 2 

# weights:  177
initial  value 736.172125 
iter  10 value 352.391968
iter  20 value 350.078679
iter  30 value 350.062527
final  value 350.062456 
converged
Fitting Repeat 3 

# weights:  177
initial  value 849.270292 
iter  10 value 352.517994
iter  20 value 350.113677
iter  30 value 350.062697
final  value 350.062469 
converged
Fitting Repeat 4 

# weights:  177
initial  value 929.313390 
iter  10 value 351.074182
iter  20 value 336.216692
iter  30 value 335.684962
iter  40 value 335.668250
iter  50 value 335.664728
iter  60 value 335.662934
iter  70 value 335.658969
iter  80 value 335.657036
iter  90 value 335.655588
iter 100 value 335.654777
final  value 335.654777 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  177
initial  value 508.968514 
iter  10 value 350.089415
iter  20 value 350.062585
final  value 350.062459 
converged
Fitting Repeat 1 

# weights:  122
initial  value 844.103343 
iter  10 value 360.834141
iter  20 value 360.213234
iter  20 value 360.213232
iter  20 value 360.213232
final  value 360.213232 
converged
Fitting Repeat 2 

# weights:  122
initial  value 542.449545 
iter  10 value 360.300955
final  value 360.213231 
converged
Fitting Repeat 3 

# weights:  122
initial  value 699.250577 
iter  10 value 360.487764
iter  20 value 360.213233
iter  20 value 360.213232
iter  20 value 360.213232
final  value 360.213232 
converged
Fitting Repeat 4 

# weights:  122
initial  value 668.754802 
iter  10 value 360.224600
final  value 360.213231 
converged
Fitting Repeat 5 

# weights:  122
initial  value 657.505399 
iter  10 value 360.418998
iter  20 value 360.213242
final  value 360.213231 
converged
Fitting Repeat 1 

# weights:  210
initial  value 843.011094 
iter  10 value 356.187100
iter  20 value 355.829914
iter  30 value 355.827864
final  value 355.827737 
converged
Fitting Repeat 2 

# weights:  210
initial  value 826.966382 
iter  10 value 371.321163
iter  20 value 369.232508
iter  30 value 369.190135
final  value 369.189189 
converged
Fitting Repeat 3 

# weights:  210
initial  value 816.281738 
iter  10 value 382.041703
iter  20 value 367.095508
iter  30 value 365.096474
iter  40 value 365.054312
final  value 365.052169 
converged
Fitting Repeat 4 

# weights:  210
initial  value 1110.562158 
iter  10 value 375.238981
iter  20 value 372.393232
iter  30 value 372.341842
final  value 372.340951 
converged
Fitting Repeat 5 

# weights:  210
initial  value 1205.993644 
iter  10 value 350.633398
iter  20 value 348.897031
iter  30 value 348.844694
iter  40 value 348.840937
final  value 348.840923 
converged
Fitting Repeat 1 

# weights:  144
initial  value 583.859059 
iter  10 value 355.330748
iter  20 value 339.661622
iter  30 value 338.762734
iter  40 value 338.611128
iter  50 value 338.591368
iter  60 value 338.584155
iter  70 value 338.577714
iter  80 value 338.571726
iter  90 value 338.568381
iter 100 value 338.565920
final  value 338.565920 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  144
initial  value 768.539199 
iter  10 value 352.160490
iter  20 value 351.437607
iter  30 value 351.388143
iter  40 value 351.381081
final  value 351.380978 
converged
Fitting Repeat 3 

# weights:  144
initial  value 704.408193 
iter  10 value 365.963243
iter  20 value 365.130561
iter  30 value 365.093102
iter  40 value 365.086749
final  value 365.086447 
converged
Fitting Repeat 4 

# weights:  144
initial  value 519.625349 
iter  10 value 358.091972
iter  20 value 356.854742
iter  30 value 356.802298
iter  40 value 356.790439
final  value 356.790334 
converged
Fitting Repeat 5 

# weights:  144
initial  value 1184.169970 
iter  10 value 360.446097
iter  20 value 360.299347
iter  30 value 360.263613
iter  40 value 360.075007
iter  50 value 343.289552
iter  60 value 343.100043
iter  70 value 343.021231
iter  80 value 342.983525
iter  90 value 342.969253
iter 100 value 342.963784
final  value 342.963784 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  56
initial  value 581.637332 
iter  10 value 349.190256
iter  20 value 339.603432
iter  30 value 339.558454
iter  40 value 339.547090
iter  50 value 339.535198
iter  60 value 339.455424
iter  70 value 325.220787
iter  80 value 323.972609
iter  90 value 323.956593
iter 100 value 323.949948
final  value 323.949948 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 491.252854 
iter  10 value 349.641412
iter  20 value 345.307431
iter  30 value 345.288541
iter  40 value 345.280924
iter  50 value 331.532881
iter  60 value 331.113601
iter  70 value 331.088783
iter  80 value 331.074830
iter  90 value 331.064530
iter 100 value 331.060910
final  value 331.060910 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 999.958077 
iter  10 value 364.112541
iter  20 value 361.311974
iter  30 value 361.278144
iter  40 value 361.273875
iter  50 value 353.631920
iter  60 value 349.241851
iter  70 value 349.216525
iter  80 value 349.211210
iter  90 value 349.198426
iter 100 value 349.183628
final  value 349.183628 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 800.497486 
iter  10 value 364.735667
iter  20 value 347.191884
iter  30 value 347.059154
iter  40 value 347.052987
iter  50 value 347.050617
iter  60 value 335.584057
iter  70 value 335.523043
iter  80 value 335.504097
iter  90 value 335.496097
iter 100 value 335.488769
final  value 335.488769 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 570.337504 
iter  10 value 342.831058
iter  20 value 335.865018
iter  30 value 335.826413
iter  40 value 335.820302
iter  50 value 335.817320
iter  60 value 335.815994
iter  70 value 335.814198
iter  80 value 335.811320
iter  90 value 335.805447
iter 100 value 335.786219
final  value 335.786219 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  23
initial  value 724.687359 
iter  10 value 321.097774
iter  20 value 320.907920
iter  30 value 320.902512
iter  40 value 307.551881
iter  50 value 307.014272
iter  60 value 307.009128
final  value 307.008983 
converged
Fitting Repeat 2 

# weights:  23
initial  value 764.325351 
iter  10 value 349.529094
iter  20 value 349.175898
iter  30 value 349.054663
iter  40 value 347.429043
iter  50 value 333.453952
iter  60 value 333.352846
iter  70 value 333.349983
iter  80 value 333.349752
final  value 333.349742 
converged
Fitting Repeat 3 

# weights:  23
initial  value 563.187225 
iter  10 value 344.681054
iter  20 value 344.226444
iter  30 value 342.704422
iter  40 value 327.146872
iter  50 value 326.896717
iter  60 value 326.888279
iter  70 value 326.887379
final  value 326.887276 
converged
Fitting Repeat 4 

# weights:  23
initial  value 754.439863 
iter  10 value 359.067578
iter  20 value 357.831031
iter  30 value 357.709396
iter  40 value 357.702251
final  value 357.702081 
converged
Fitting Repeat 5 

# weights:  23
initial  value 538.749323 
iter  10 value 322.153905
iter  20 value 321.661532
iter  30 value 321.514934
iter  40 value 309.443054
iter  50 value 309.405315
iter  60 value 309.404539
iter  70 value 309.404357
final  value 309.404133 
converged
Fitting Repeat 1 

# weights:  210
initial  value 993.789040 
iter  10 value 364.464840
iter  20 value 361.910996
iter  30 value 361.885660
final  value 361.883768 
converged
Fitting Repeat 2 

# weights:  210
initial  value 505.451808 
iter  10 value 330.707398
iter  20 value 330.063638
final  value 330.060033 
converged
Fitting Repeat 3 

# weights:  210
initial  value 662.588948 
iter  10 value 365.772433
iter  20 value 363.904388
iter  30 value 363.887144
final  value 363.885238 
converged
Fitting Repeat 4 

# weights:  210
initial  value 683.702221 
iter  10 value 371.435601
iter  20 value 369.617741
iter  30 value 369.600934
final  value 369.599299 
converged
Fitting Repeat 5 

# weights:  210
initial  value 765.912624 
iter  10 value 346.249520
iter  20 value 344.388953
iter  30 value 344.371744
iter  40 value 344.369884
iter  40 value 344.369884
iter  40 value 344.369884
final  value 344.369884 
converged
Fitting Repeat 1 

# weights:  199
initial  value 555.547754 
iter  10 value 352.833391
iter  20 value 349.841265
iter  30 value 349.816247
final  value 349.814977 
converged
Fitting Repeat 2 

# weights:  199
initial  value 1007.872677 
iter  10 value 356.321624
iter  20 value 349.881482
iter  30 value 349.819929
final  value 349.815101 
converged
Fitting Repeat 3 

# weights:  199
initial  value 1052.429281 
iter  10 value 356.108132
iter  20 value 349.879020
iter  30 value 349.815368
final  value 349.815303 
converged
Fitting Repeat 4 

# weights:  199
initial  value 823.683839 
iter  10 value 357.524262
iter  20 value 349.895347
iter  30 value 349.819779
final  value 349.815050 
converged
Fitting Repeat 5 

# weights:  199
initial  value 623.854397 
iter  10 value 354.567039
iter  20 value 349.861253
iter  30 value 349.817804
final  value 349.815036 
converged
Fitting Repeat 1 

# weights:  100
initial  value 626.295862 
iter  10 value 361.864190
iter  20 value 358.403079
iter  30 value 358.380218
iter  40 value 358.376680
iter  50 value 355.134973
iter  60 value 341.454219
iter  70 value 341.107828
iter  80 value 341.081382
iter  90 value 341.064073
iter 100 value 341.052513
final  value 341.052513 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  100
initial  value 597.029467 
iter  10 value 363.442605
iter  20 value 360.433664
iter  30 value 360.414986
iter  40 value 360.409540
iter  50 value 360.244264
iter  60 value 345.101369
iter  70 value 344.726589
iter  80 value 344.645165
iter  90 value 344.592319
iter 100 value 344.557715
final  value 344.557715 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  100
initial  value 662.054788 
iter  10 value 363.911314
iter  20 value 360.095546
iter  30 value 360.069864
iter  40 value 360.065278
iter  50 value 354.831625
iter  60 value 342.794838
iter  70 value 342.699715
iter  80 value 342.691721
iter  90 value 342.685016
iter 100 value 342.677341
final  value 342.677341 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  100
initial  value 1078.958697 
iter  10 value 335.733129
iter  20 value 332.410134
iter  30 value 332.385549
iter  40 value 332.381774
iter  50 value 332.374687
iter  60 value 331.943656
iter  70 value 315.413990
iter  80 value 315.007388
iter  90 value 314.992510
iter 100 value 314.980456
final  value 314.980456 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  100
initial  value 1004.483494 
iter  10 value 350.879407
iter  20 value 344.648138
iter  30 value 344.594858
iter  40 value 344.592271
iter  50 value 344.523212
iter  60 value 328.318238
iter  70 value 327.774380
iter  80 value 327.739675
iter  90 value 327.733034
iter 100 value 327.717680
final  value 327.717680 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  56
initial  value 584.235016 
iter  10 value 361.947407
iter  20 value 361.846245
iter  20 value 361.846244
iter  20 value 361.846243
final  value 361.846243 
converged
Fitting Repeat 2 

# weights:  56
initial  value 942.861414 
iter  10 value 368.186061
final  value 368.015643 
converged
Fitting Repeat 3 

# weights:  56
initial  value 762.009526 
iter  10 value 378.289716
iter  20 value 378.094031
final  value 378.093946 
converged
Fitting Repeat 4 

# weights:  56
initial  value 876.750920 
iter  10 value 361.209122
iter  20 value 360.964427
iter  30 value 360.962551
final  value 360.962053 
converged
Fitting Repeat 5 

# weights:  56
initial  value 573.566278 
iter  10 value 371.685076
iter  20 value 371.587202
iter  20 value 371.587201
iter  20 value 371.587201
final  value 371.587201 
converged
Fitting Repeat 1 

# weights:  12
initial  value 832.018174 
iter  10 value 334.097210
iter  20 value 332.607554
iter  30 value 332.455641
iter  40 value 332.414998
iter  50 value 330.696749
iter  60 value 315.902343
iter  70 value 315.537816
final  value 315.536444 
converged
Fitting Repeat 2 

# weights:  12
initial  value 1010.730345 
iter  10 value 346.031959
iter  20 value 344.918536
iter  30 value 344.815903
final  value 344.815142 
converged
Fitting Repeat 3 

# weights:  12
initial  value 618.650181 
iter  10 value 363.169751
iter  20 value 362.347762
iter  30 value 362.282691
iter  40 value 361.035925
iter  50 value 345.307796
iter  60 value 345.219708
final  value 345.219501 
converged
Fitting Repeat 4 

# weights:  12
initial  value 598.954800 
iter  10 value 317.451647
iter  20 value 316.419135
iter  30 value 316.248241
iter  40 value 311.860492
iter  50 value 295.670757
iter  60 value 295.544634
final  value 295.542837 
converged
Fitting Repeat 5 

# weights:  12
initial  value 792.586679 
iter  10 value 367.233320
iter  20 value 365.509152
iter  30 value 365.497999
iter  40 value 364.601247
iter  50 value 348.581367
iter  60 value 348.570737
final  value 348.570615 
converged
Fitting Repeat 1 

# weights:  133
initial  value 840.195198 
iter  10 value 364.085539
iter  20 value 362.586703
iter  30 value 362.116941
iter  40 value 362.016639
iter  50 value 362.008596
iter  60 value 362.005740
iter  70 value 361.952396
iter  80 value 353.425288
iter  90 value 347.656508
iter 100 value 347.514285
final  value 347.514285 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  133
initial  value 531.924742 
iter  10 value 323.300997
iter  20 value 322.937748
iter  30 value 322.929330
iter  40 value 321.155190
iter  50 value 304.894572
iter  60 value 303.681985
iter  70 value 303.546466
iter  80 value 303.479716
iter  90 value 303.408322
iter 100 value 303.375810
final  value 303.375810 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 585.865837 
iter  10 value 403.561618
iter  20 value 403.033494
iter  30 value 402.997038
iter  40 value 402.993833
final  value 402.992938 
converged
Fitting Repeat 4 

# weights:  133
initial  value 852.339755 
iter  10 value 363.835111
iter  20 value 362.350928
iter  30 value 361.990388
iter  40 value 361.839433
iter  50 value 361.832186
iter  60 value 361.744187
iter  70 value 346.734527
iter  80 value 345.957128
iter  90 value 345.892236
iter 100 value 345.836814
final  value 345.836814 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  133
initial  value 631.063817 
iter  10 value 342.273420
iter  20 value 342.230573
iter  30 value 342.228930
iter  40 value 342.223852
iter  50 value 341.862612
iter  60 value 327.930233
iter  70 value 327.612185
iter  80 value 327.548727
iter  90 value 327.510574
iter 100 value 327.476470
final  value 327.476470 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  188
initial  value 1012.438511 
iter  10 value 391.441114
iter  20 value 390.282905
final  value 390.282781 
converged
Fitting Repeat 2 

# weights:  188
initial  value 932.159758 
iter  10 value 362.239149
iter  20 value 359.715427
final  value 359.713446 
converged
Fitting Repeat 3 

# weights:  188
initial  value 659.981697 
iter  10 value 347.167468
final  value 346.858504 
converged
Fitting Repeat 4 

# weights:  188
initial  value 638.081365 
iter  10 value 336.664469
iter  20 value 336.526989
final  value 336.526965 
converged
Fitting Repeat 5 

# weights:  188
initial  value 682.516325 
iter  10 value 396.674398
iter  20 value 395.414104
final  value 395.414059 
converged
Fitting Repeat 1 

# weights:  78
initial  value 460.540449 
iter  10 value 349.854436
iter  20 value 349.808008
iter  20 value 349.808007
iter  20 value 349.808007
final  value 349.808007 
converged
Fitting Repeat 2 

# weights:  78
initial  value 532.155292 
iter  10 value 349.900478
iter  20 value 349.808677
final  value 349.808672 
converged
Fitting Repeat 3 

# weights:  78
initial  value 737.933540 
iter  10 value 350.006018
iter  20 value 349.811314
iter  30 value 349.808331
final  value 349.808327 
converged
Fitting Repeat 4 

# weights:  78
initial  value 567.127104 
iter  10 value 349.966559
iter  20 value 349.808590
final  value 349.808465 
converged
Fitting Repeat 5 

# weights:  78
initial  value 955.592306 
iter  10 value 350.055509
iter  20 value 349.809238
final  value 349.809194 
converged
Fitting Repeat 1 

# weights:  144
initial  value 870.575047 
iter  10 value 357.304517
iter  20 value 357.159677
iter  30 value 357.135200
final  value 357.135108 
converged
Fitting Repeat 2 

# weights:  144
initial  value 935.213769 
iter  10 value 369.875483
iter  20 value 367.615690
iter  30 value 367.601140
final  value 367.601037 
converged
Fitting Repeat 3 

# weights:  144
initial  value 499.841793 
iter  10 value 352.807140
iter  20 value 352.694617
final  value 352.693977 
converged
Fitting Repeat 4 

# weights:  144
initial  value 594.588838 
iter  10 value 378.452460
iter  20 value 378.341922
final  value 378.341830 
converged
Fitting Repeat 5 

# weights:  144
initial  value 589.284945 
iter  10 value 344.592834
iter  20 value 344.504000
final  value 344.503103 
converged
Fitting Repeat 1 

# weights:  221
initial  value 1053.505034 
iter  10 value 357.395542
final  value 351.450506 
converged
Fitting Repeat 2 

# weights:  221
initial  value 853.125067 
iter  10 value 355.121542
iter  20 value 351.450829
final  value 351.450506 
converged
Fitting Repeat 3 

# weights:  221
initial  value 711.858862 
iter  10 value 353.504679
iter  20 value 351.450629
final  value 351.450506 
converged
Fitting Repeat 4 

# weights:  221
initial  value 997.846443 
iter  10 value 357.389680
final  value 351.450506 
converged
Fitting Repeat 5 

# weights:  221
initial  value 864.446670 
iter  10 value 352.849744
final  value 351.450506 
converged
Fitting Repeat 1 

# weights:  56
initial  value 903.332421 
iter  10 value 356.582033
iter  20 value 352.359049
iter  30 value 352.328075
iter  40 value 352.326331
iter  50 value 352.307496
iter  60 value 352.196488
iter  70 value 335.844244
iter  80 value 333.738219
iter  90 value 333.716245
iter 100 value 333.688667
final  value 333.688667 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 939.785926 
iter  10 value 367.846740
iter  20 value 363.977651
iter  30 value 363.952517
iter  40 value 363.947951
iter  50 value 361.754161
iter  60 value 344.981608
iter  70 value 344.819391
iter  80 value 344.803980
iter  90 value 344.785987
iter 100 value 344.773799
final  value 344.773799 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 929.111559 
iter  10 value 327.339120
iter  20 value 324.362539
iter  30 value 324.333724
iter  40 value 324.330729
iter  50 value 324.246362
iter  60 value 309.011543
iter  70 value 307.881231
iter  80 value 307.803457
iter  90 value 307.796387
iter 100 value 307.780286
final  value 307.780286 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 551.339930 
iter  10 value 385.877036
iter  20 value 384.736609
iter  30 value 384.729177
iter  40 value 384.642315
iter  50 value 370.260106
iter  60 value 369.333422
iter  70 value 369.283929
iter  80 value 369.273748
iter  90 value 369.246892
iter 100 value 369.233718
final  value 369.233718 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 713.182620 
iter  10 value 357.409321
iter  20 value 353.515261
iter  30 value 353.490967
iter  40 value 353.487482
iter  50 value 353.484365
iter  60 value 353.483320
iter  70 value 353.481833
iter  80 value 353.479475
iter  90 value 353.475033
iter 100 value 353.463355
final  value 353.463355 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  144
initial  value 572.929343 
iter  10 value 340.097746
iter  20 value 339.643543
final  value 339.641713 
converged
Fitting Repeat 2 

# weights:  144
initial  value 943.929293 
iter  10 value 351.765031
iter  20 value 350.937562
iter  30 value 350.932209
final  value 350.930032 
converged
Fitting Repeat 3 

# weights:  144
initial  value 489.892330 
iter  10 value 359.358048
iter  20 value 359.152668
final  value 359.152197 
converged
Fitting Repeat 4 

# weights:  144
initial  value 1093.710658 
iter  10 value 385.607876
iter  20 value 384.695416
iter  30 value 384.688195
final  value 384.686774 
converged
Fitting Repeat 5 

# weights:  144
initial  value 401.343943 
iter  10 value 330.394153
final  value 330.322015 
converged
Fitting Repeat 1 

# weights:  78
initial  value 571.011545 
iter  10 value 358.513704
iter  20 value 343.100446
iter  30 value 342.668356
iter  40 value 342.643763
iter  50 value 342.642457
iter  60 value 342.641598
iter  70 value 342.641282
iter  80 value 342.640715
iter  90 value 342.640509
iter 100 value 342.640275
final  value 342.640275 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  78
initial  value 922.741538 
iter  10 value 363.434012
iter  20 value 358.426366
iter  30 value 357.806967
iter  40 value 357.736260
iter  50 value 357.731817
iter  60 value 344.661306
iter  70 value 342.905256
iter  80 value 342.895649
iter  90 value 342.883924
iter 100 value 342.791051
final  value 342.791051 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  78
initial  value 838.840231 
iter  10 value 358.465998
iter  20 value 357.856313
iter  30 value 357.740448
iter  40 value 357.733263
iter  50 value 357.623595
iter  60 value 344.406760
iter  70 value 342.886773
iter  80 value 342.817882
iter  90 value 342.731193
iter 100 value 342.680433
final  value 342.680433 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  78
initial  value 722.002316 
iter  10 value 358.907823
iter  20 value 357.756766
iter  30 value 357.735765
iter  40 value 351.429973
iter  50 value 342.955368
iter  60 value 342.823456
iter  70 value 342.739190
iter  80 value 342.678777
iter  90 value 342.653186
iter 100 value 342.641630
final  value 342.641630 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  78
initial  value 787.986402 
iter  10 value 363.376422
iter  20 value 358.369524
iter  30 value 344.042479
iter  40 value 342.696815
iter  50 value 342.650503
iter  60 value 342.648349
iter  70 value 342.647315
iter  80 value 342.646114
iter  90 value 342.641608
iter 100 value 342.640526
final  value 342.640526 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  188
initial  value 845.566902 
iter  10 value 400.724434
iter  20 value 393.638723
final  value 393.638718 
converged
Fitting Repeat 2 

# weights:  188
initial  value 573.229973 
iter  10 value 343.089168
final  value 343.089152 
converged
Fitting Repeat 3 

# weights:  188
initial  value 437.083682 
iter  10 value 335.812898
final  value 335.473247 
converged
Fitting Repeat 4 

# weights:  188
initial  value 679.494186 
iter  10 value 372.063365
final  value 369.631412 
converged
Fitting Repeat 5 

# weights:  188
initial  value 680.101553 
iter  10 value 399.191171
final  value 397.179142 
converged
Fitting Repeat 1 

# weights:  177
initial  value 769.687389 
iter  10 value 359.042091
iter  20 value 357.981076
iter  30 value 357.783032
iter  40 value 357.768175
final  value 357.768088 
converged
Fitting Repeat 2 

# weights:  177
initial  value 659.633879 
iter  10 value 355.561394
iter  20 value 344.020209
iter  30 value 343.387032
iter  40 value 343.380126
iter  50 value 343.376241
iter  60 value 343.373872
iter  70 value 343.372174
iter  80 value 343.370963
iter  90 value 343.370295
iter 100 value 343.369895
final  value 343.369895 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  177
initial  value 1243.484751 
iter  10 value 360.749536
iter  20 value 357.787454
iter  30 value 357.768365
final  value 357.768115 
converged
Fitting Repeat 4 

# weights:  177
initial  value 661.441561 
iter  10 value 358.131584
iter  20 value 357.772092
iter  30 value 357.768108
final  value 357.768098 
converged
Fitting Repeat 5 

# weights:  177
initial  value 779.811061 
iter  10 value 377.388332
iter  20 value 360.190979
iter  30 value 357.788326
iter  40 value 357.768284
final  value 357.768091 
converged
Fitting Repeat 1 

# weights:  122
initial  value 1146.390475 
iter  10 value 367.973067
final  value 367.941014 
converged
Fitting Repeat 2 

# weights:  122
initial  value 660.810666 
iter  10 value 368.333167
final  value 367.941024 
converged
Fitting Repeat 3 

# weights:  122
initial  value 1008.871660 
iter  10 value 368.529097
iter  20 value 367.941065
final  value 367.941014 
converged
Fitting Repeat 4 

# weights:  122
initial  value 1180.820367 
iter  10 value 369.017039
iter  20 value 367.941837
final  value 367.941014 
converged
Fitting Repeat 5 

# weights:  122
initial  value 651.151939 
iter  10 value 368.421191
iter  20 value 367.941023
final  value 367.941015 
converged
Fitting Repeat 1 

# weights:  210
initial  value 395.197092 
iter  10 value 356.840328
iter  20 value 356.770892
iter  30 value 356.768432
final  value 356.768427 
converged
Fitting Repeat 2 

# weights:  210
initial  value 1261.951267 
iter  10 value 366.693244
iter  20 value 366.493284
iter  30 value 366.490050
final  value 366.490040 
converged
Fitting Repeat 3 

# weights:  210
initial  value 425.538399 
iter  10 value 338.949611
iter  20 value 338.864008
iter  30 value 338.861441
iter  30 value 338.861439
iter  30 value 338.861438
final  value 338.861438 
converged
Fitting Repeat 4 

# weights:  210
initial  value 1006.265155 
iter  10 value 364.725027
iter  20 value 354.022831
iter  30 value 336.772118
iter  40 value 335.283939
iter  50 value 335.089969
iter  60 value 335.071585
iter  70 value 335.065064
iter  80 value 335.063204
iter  90 value 335.062628
iter 100 value 335.062145
final  value 335.062145 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  210
initial  value 1214.349232 
iter  10 value 375.621207
iter  20 value 375.544076
iter  30 value 375.541664
final  value 375.541653 
converged
Fitting Repeat 1 

# weights:  144
initial  value 841.443668 
iter  10 value 357.669787
iter  20 value 353.127528
iter  30 value 352.340267
iter  40 value 352.238017
iter  50 value 352.219236
iter  60 value 351.891608
iter  70 value 333.571051
iter  80 value 332.985137
iter  90 value 332.863579
iter 100 value 332.771563
final  value 332.771563 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  144
initial  value 557.556172 
iter  10 value 373.760814
iter  20 value 373.133935
iter  30 value 373.125629
final  value 373.125382 
converged
Fitting Repeat 3 

# weights:  144
initial  value 863.764107 
iter  10 value 332.221366
iter  20 value 331.622546
iter  30 value 331.615059
iter  40 value 318.808156
iter  50 value 315.490950
iter  60 value 315.262451
iter  70 value 315.125730
iter  80 value 315.050506
iter  90 value 315.015450
iter 100 value 314.989704
final  value 314.989704 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  144
initial  value 1078.571531 
iter  10 value 357.024742
iter  20 value 340.027511
iter  30 value 338.896613
iter  40 value 338.613805
iter  50 value 338.585477
iter  60 value 338.574484
iter  70 value 338.571449
iter  80 value 338.568398
iter  90 value 338.565533
iter 100 value 338.564671
final  value 338.564671 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  144
initial  value 464.010983 
iter  10 value 358.622171
iter  20 value 358.592348
iter  30 value 358.587770
iter  40 value 349.240161
iter  50 value 341.921500
iter  60 value 340.821200
iter  70 value 340.712396
iter  80 value 340.586865
iter  90 value 340.510180
iter 100 value 340.492570
final  value 340.492570 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  56
initial  value 724.506758 
iter  10 value 352.851392
iter  20 value 339.557645
iter  30 value 339.445143
iter  40 value 339.436177
iter  50 value 326.624466
iter  60 value 325.695092
iter  70 value 325.661115
iter  80 value 325.639753
iter  90 value 325.630041
iter 100 value 325.621200
final  value 325.621200 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 538.229565 
iter  10 value 342.082476
iter  20 value 336.535656
iter  30 value 336.514997
iter  40 value 336.506719
iter  50 value 335.804619
iter  60 value 324.401570
iter  70 value 324.342644
iter  80 value 324.324000
iter  90 value 324.305386
iter 100 value 324.293572
final  value 324.293572 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 763.992970 
iter  10 value 378.278938
iter  20 value 364.616725
iter  30 value 364.530984
iter  40 value 364.517474
iter  50 value 364.438010
iter  60 value 349.579949
iter  70 value 348.791018
iter  80 value 348.769175
iter  90 value 348.764698
iter 100 value 348.756403
final  value 348.756403 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 852.060911 
iter  10 value 375.864680
iter  20 value 359.519883
iter  30 value 359.401806
iter  40 value 359.391748
iter  50 value 342.546695
iter  60 value 342.427946
iter  70 value 342.398803
iter  80 value 342.388459
iter  90 value 342.382812
iter 100 value 342.377770
final  value 342.377770 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 682.360921 
iter  10 value 365.655476
iter  20 value 364.896588
iter  30 value 364.819113
iter  40 value 364.786033
iter  50 value 364.679478
iter  60 value 349.133150
iter  70 value 348.439811
iter  80 value 348.423426
iter  90 value 348.409523
iter 100 value 348.403412
final  value 348.403412 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  23
initial  value 740.720783 
iter  10 value 380.056181
iter  20 value 379.292109
iter  30 value 378.756876
iter  40 value 362.285926
iter  50 value 362.188728
iter  60 value 362.183783
final  value 362.183393 
converged
Fitting Repeat 2 

# weights:  23
initial  value 842.893985 
iter  10 value 348.784075
iter  20 value 347.768061
iter  30 value 347.653772
iter  40 value 347.280315
iter  50 value 330.643879
iter  60 value 330.534649
iter  70 value 330.532933
final  value 330.532888 
converged
Fitting Repeat 3 

# weights:  23
initial  value 680.743299 
iter  10 value 343.356176
iter  20 value 342.449712
iter  30 value 342.327887
iter  40 value 342.289165
iter  50 value 341.596107
iter  60 value 331.022752
iter  70 value 330.953231
iter  80 value 330.951732
final  value 330.951540 
converged
Fitting Repeat 4 

# weights:  23
initial  value 561.865800 
iter  10 value 362.764818
iter  20 value 362.172103
iter  30 value 362.076389
iter  40 value 361.802771
iter  50 value 346.903268
iter  60 value 345.929711
iter  70 value 345.863322
iter  80 value 345.837853
iter  90 value 345.828620
iter 100 value 345.827201
final  value 345.827201 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  23
initial  value 750.502784 
iter  10 value 353.623751
iter  20 value 352.606871
iter  30 value 352.558554
iter  40 value 340.753995
iter  50 value 339.608990
iter  60 value 339.576239
iter  70 value 339.567807
iter  80 value 339.563048
iter  90 value 339.562563
final  value 339.562399 
converged
Fitting Repeat 1 

# weights:  210
initial  value 828.974045 
iter  10 value 351.353614
iter  20 value 349.448404
iter  30 value 349.429325
final  value 349.428864 
converged
Fitting Repeat 2 

# weights:  210
initial  value 1007.314846 
iter  10 value 369.981420
iter  20 value 368.083382
iter  30 value 368.066209
final  value 368.064025 
converged
Fitting Repeat 3 

# weights:  210
initial  value 677.492716 
iter  10 value 391.339057
iter  20 value 389.593498
final  value 389.577206 
converged
Fitting Repeat 4 

# weights:  210
initial  value 460.690636 
iter  10 value 330.423827
iter  20 value 330.004704
final  value 330.003565 
converged
Fitting Repeat 5 

# weights:  210
initial  value 1109.397482 
iter  10 value 347.231261
iter  20 value 345.923731
iter  30 value 345.911912
final  value 345.911209 
converged
Fitting Repeat 1 

# weights:  199
initial  value 729.055019 
iter  10 value 364.750691
iter  20 value 357.595371
iter  30 value 357.524654
final  value 357.520301 
converged
Fitting Repeat 2 

# weights:  199
initial  value 635.929744 
iter  10 value 363.872732
iter  20 value 357.585249
iter  30 value 357.520364
final  value 357.520301 
converged
Fitting Repeat 3 

# weights:  199
initial  value 469.195378 
iter  10 value 358.705626
iter  20 value 357.525792
iter  30 value 357.520591
final  value 357.520398 
converged
Fitting Repeat 4 

# weights:  199
initial  value 638.003602 
iter  10 value 363.447272
iter  20 value 357.580344
iter  30 value 357.522227
final  value 357.520301 
converged
Fitting Repeat 5 

# weights:  199
initial  value 954.849318 
iter  10 value 365.003279
iter  20 value 357.598284
iter  30 value 357.520769
final  value 357.520654 
converged
Fitting Repeat 1 

# weights:  100
initial  value 1185.137598 
iter  10 value 345.126564
iter  20 value 343.013174
iter  30 value 342.974559
iter  40 value 342.970644
iter  50 value 342.955577
iter  60 value 328.781541
iter  70 value 323.905434
iter  80 value 323.873900
iter  90 value 323.846507
iter 100 value 323.822389
final  value 323.822389 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  100
initial  value 549.461042 
iter  10 value 350.684547
iter  20 value 348.022783
iter  30 value 348.008706
iter  40 value 348.005275
iter  50 value 347.990460
iter  60 value 342.720683
iter  70 value 332.031794
iter  80 value 331.614631
iter  90 value 331.536559
iter 100 value 331.506276
final  value 331.506276 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  100
initial  value 913.752337 
iter  10 value 365.724221
iter  20 value 360.461148
iter  30 value 360.418417
iter  40 value 360.416462
final  value 360.416379 
converged
Fitting Repeat 4 

# weights:  100
initial  value 890.072962 
iter  10 value 343.646234
iter  20 value 337.733516
iter  30 value 337.682402
iter  40 value 337.680031
iter  50 value 337.677121
iter  60 value 337.676141
iter  70 value 337.674948
iter  80 value 337.673365
iter  90 value 337.671006
iter 100 value 337.666843
final  value 337.666843 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  100
initial  value 909.280434 
iter  10 value 383.866437
iter  20 value 378.360327
iter  30 value 378.314278
iter  40 value 378.311783
iter  50 value 378.000940
iter  60 value 362.814767
iter  70 value 362.280073
iter  80 value 362.251744
iter  90 value 362.204240
iter 100 value 362.156904
final  value 362.156904 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  56
initial  value 760.095534 
iter  10 value 322.507097
iter  20 value 322.310320
final  value 322.310220 
converged
Fitting Repeat 2 

# weights:  56
initial  value 601.238088 
iter  10 value 338.970376
iter  20 value 338.869844
iter  20 value 338.869843
iter  20 value 338.869843
final  value 338.869843 
converged
Fitting Repeat 3 

# weights:  56
initial  value 809.046816 
iter  10 value 384.516111
iter  20 value 384.318763
final  value 384.318615 
converged
Fitting Repeat 4 

# weights:  56
initial  value 662.843192 
iter  10 value 350.655533
iter  20 value 350.534935
iter  30 value 350.533120
final  value 350.533108 
converged
Fitting Repeat 5 

# weights:  56
initial  value 754.693679 
iter  10 value 366.018785
iter  20 value 365.841894
final  value 365.841854 
converged
Fitting Repeat 1 

# weights:  12
initial  value 818.749748 
iter  10 value 357.562213
iter  20 value 356.235975
iter  30 value 356.171932
iter  40 value 351.045472
iter  50 value 341.924479
iter  60 value 341.914658
final  value 341.914566 
converged
Fitting Repeat 2 

# weights:  12
initial  value 908.103201 
iter  10 value 390.244807
iter  20 value 388.725167
iter  30 value 388.659996
iter  40 value 372.999756
iter  50 value 371.789115
iter  60 value 371.772425
final  value 371.772411 
converged
Fitting Repeat 3 

# weights:  12
initial  value 819.902044 
iter  10 value 352.788514
iter  20 value 351.460306
iter  30 value 351.401030
iter  40 value 329.364092
iter  50 value 329.214215
final  value 329.213498 
converged
Fitting Repeat 4 

# weights:  12
initial  value 734.438656 
iter  10 value 339.585661
iter  20 value 338.418916
iter  30 value 338.312247
iter  40 value 337.997028
iter  50 value 324.955415
iter  60 value 323.963580
iter  70 value 323.958500
final  value 323.958370 
converged
Fitting Repeat 5 

# weights:  12
initial  value 541.539577 
iter  10 value 370.813563
iter  20 value 370.409340
iter  30 value 370.326773
iter  40 value 355.177708
iter  50 value 353.896860
final  value 353.895986 
converged
Fitting Repeat 1 

# weights:  133
initial  value 893.947970 
iter  10 value 328.873431
iter  20 value 314.346028
iter  30 value 313.326266
iter  40 value 313.153704
iter  50 value 313.049200
iter  60 value 313.034402
iter  70 value 313.025273
iter  80 value 313.018395
iter  90 value 313.015417
iter 100 value 313.010771
final  value 313.010771 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  133
initial  value 759.837503 
iter  10 value 368.399884
iter  20 value 368.025143
iter  30 value 367.989772
iter  40 value 367.984863
final  value 367.984133 
converged
Fitting Repeat 3 

# weights:  133
initial  value 431.459334 
iter  10 value 341.698666
iter  20 value 338.741918
iter  30 value 338.726219
iter  40 value 338.718250
iter  50 value 338.714581
iter  60 value 338.707459
iter  70 value 338.683847
iter  80 value 337.042385
iter  90 value 321.456218
iter 100 value 321.160107
final  value 321.160107 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  133
initial  value 677.439567 
iter  10 value 322.036097
iter  20 value 322.005753
iter  30 value 318.951083
iter  40 value 306.478831
iter  50 value 306.223586
iter  60 value 306.169251
iter  70 value 306.137384
iter  80 value 306.106786
iter  90 value 306.079494
iter 100 value 306.067099
final  value 306.067099 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  133
initial  value 780.355551 
iter  10 value 373.612114
iter  20 value 372.742952
iter  30 value 372.513697
iter  40 value 372.488786
iter  50 value 372.486876
final  value 372.486749 
converged
Fitting Repeat 1 

# weights:  188
initial  value 1086.904634 
iter  10 value 348.095949
iter  20 value 346.915554
final  value 346.915204 
converged
Fitting Repeat 2 

# weights:  188
initial  value 988.683128 
iter  10 value 383.686640
iter  20 value 382.287923
final  value 382.286566 
converged
Fitting Repeat 3 

# weights:  188
initial  value 571.217526 
iter  10 value 356.094719
iter  20 value 354.980480
final  value 354.979738 
converged
Fitting Repeat 4 

# weights:  188
initial  value 1059.143134 
iter  10 value 362.485313
iter  20 value 359.213315
final  value 359.212637 
converged
Fitting Repeat 5 

# weights:  188
initial  value 856.320967 
iter  10 value 400.479326
iter  20 value 398.226412
final  value 398.225190 
converged
Fitting Repeat 1 

# weights:  78
initial  value 737.433066 
iter  10 value 357.810116
iter  20 value 357.515352
final  value 357.515161 
converged
Fitting Repeat 2 

# weights:  78
initial  value 1160.998094 
iter  10 value 357.671056
iter  20 value 357.516568
iter  30 value 357.513903
final  value 357.513839 
converged
Fitting Repeat 3 

# weights:  78
initial  value 747.898739 
iter  10 value 357.871859
iter  20 value 357.516064
final  value 357.514053 
converged
Fitting Repeat 4 

# weights:  78
initial  value 574.314771 
iter  10 value 357.661906
iter  20 value 357.513872
final  value 357.513835 
converged
Fitting Repeat 5 

# weights:  78
initial  value 1002.424551 
iter  10 value 357.774833
iter  20 value 357.515548
iter  20 value 357.515546
iter  20 value 357.515543
final  value 357.515543 
converged
Fitting Repeat 1 

# weights:  144
initial  value 862.317522 
iter  10 value 354.900644
iter  20 value 344.047595
iter  30 value 343.852511
iter  40 value 343.836790
iter  50 value 343.830525
iter  60 value 343.827058
iter  70 value 343.826485
iter  80 value 343.826259
iter  90 value 343.825766
final  value 343.825674 
converged
Fitting Repeat 2 

# weights:  144
initial  value 943.528907 
iter  10 value 334.186422
iter  20 value 332.057093
iter  30 value 331.976589
iter  40 value 331.969137
final  value 331.969036 
converged
Fitting Repeat 3 

# weights:  144
initial  value 863.325243 
iter  10 value 333.657821
iter  20 value 333.470003
iter  30 value 333.453964
final  value 333.453835 
converged
Fitting Repeat 4 

# weights:  144
initial  value 785.751788 
iter  10 value 378.582438
iter  20 value 376.902674
iter  30 value 376.079045
iter  40 value 375.962233
final  value 375.961601 
converged
Fitting Repeat 5 

# weights:  144
initial  value 615.328540 
iter  10 value 349.506423
iter  20 value 335.940848
iter  30 value 335.033522
iter  40 value 334.881184
iter  50 value 334.854744
iter  60 value 334.835325
iter  70 value 334.821966
iter  80 value 334.812771
iter  90 value 334.811739
iter 100 value 334.809824
final  value 334.809824 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  221
initial  value 663.282328 
iter  10 value 365.130997
iter  20 value 359.158435
final  value 359.158392 
converged
Fitting Repeat 2 

# weights:  221
initial  value 967.672938 
iter  10 value 359.881422
final  value 359.158392 
converged
Fitting Repeat 3 

# weights:  221
initial  value 785.545691 
iter  10 value 366.321801
iter  20 value 359.158393
iter  20 value 359.158392
iter  20 value 359.158392
final  value 359.158392 
converged
Fitting Repeat 4 

# weights:  221
initial  value 721.656217 
iter  10 value 365.583512
iter  20 value 359.158441
final  value 359.158392 
converged
Fitting Repeat 5 

# weights:  221
initial  value 1164.285689 
iter  10 value 360.277998
final  value 359.158392 
converged
Fitting Repeat 1 

# weights:  56
initial  value 440.065138 
iter  10 value 361.899959
iter  20 value 361.518604
iter  30 value 361.515118
iter  40 value 361.513390
iter  50 value 361.512880
iter  60 value 361.512314
iter  70 value 361.511661
iter  80 value 361.510867
iter  90 value 361.509845
iter 100 value 361.508420
final  value 361.508420 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 819.343487 
iter  10 value 356.924160
iter  20 value 353.475319
iter  30 value 353.423949
iter  40 value 353.417845
iter  50 value 353.384120
iter  60 value 350.061499
iter  70 value 340.064523
iter  80 value 339.884279
iter  90 value 339.856095
iter 100 value 339.845806
final  value 339.845806 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 701.032956 
iter  10 value 335.167568
iter  20 value 332.382026
iter  30 value 332.362433
iter  40 value 332.357613
iter  50 value 332.336929
iter  60 value 332.246000
iter  70 value 315.457325
iter  80 value 313.828087
iter  90 value 313.781014
iter 100 value 313.748223
final  value 313.748223 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 744.823910 
iter  10 value 336.791604
iter  20 value 333.733633
iter  30 value 333.714336
iter  40 value 333.709144
iter  50 value 333.682978
iter  60 value 316.850199
iter  70 value 316.521539
iter  80 value 316.483868
iter  90 value 316.478167
iter 100 value 316.475280
final  value 316.475280 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 780.266433 
iter  10 value 359.042874
iter  20 value 355.958092
iter  30 value 355.942572
iter  40 value 355.939935
iter  50 value 355.932605
iter  60 value 355.921884
iter  70 value 355.862066
iter  80 value 345.263719
iter  90 value 344.183993
iter 100 value 344.140139
final  value 344.140139 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  56
initial  value 1028.981499 
iter  10 value 561.781284
iter  20 value 560.928172
iter  30 value 560.811205
iter  40 value 560.752928
iter  50 value 560.748746
iter  60 value 560.747001
iter  70 value 560.744674
iter  80 value 560.740983
iter  90 value 560.733277
iter 100 value 560.703864
final  value 560.703864 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 1135.762450 
iter  10 value 546.025550
iter  20 value 544.337864
iter  30 value 544.120097
iter  40 value 544.025506
iter  50 value 544.007244
iter  60 value 519.753698
iter  70 value 517.042499
iter  80 value 517.011839
iter  90 value 516.996282
iter 100 value 516.979645
final  value 516.979645 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 900.707784 
iter  10 value 549.969934
iter  20 value 549.149137
iter  30 value 549.090166
iter  40 value 549.039809
iter  50 value 536.579380
iter  60 value 529.062185
iter  70 value 529.009817
iter  80 value 528.988905
iter  90 value 528.966918
iter 100 value 528.947360
final  value 528.947360 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 1147.723953 
iter  10 value 514.328694
iter  20 value 512.781573
iter  30 value 512.717320
iter  40 value 512.657985
iter  50 value 512.644404
iter  60 value 512.635975
iter  70 value 512.607419
iter  80 value 507.666639
iter  90 value 487.359252
iter 100 value 487.252761
final  value 487.252761 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 1568.570866 
iter  10 value 540.391049
iter  20 value 539.332176
iter  30 value 532.883678
iter  40 value 512.023479
iter  50 value 511.912097
iter  60 value 511.880866
iter  70 value 511.876077
iter  80 value 511.874011
iter  90 value 511.855966
iter 100 value 511.842781
final  value 511.842781 
stopped after 100 iterations
Model Averaged Neural Network 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results across tuning parameters:

  size  decay         bag    RMSE       Rsquared   MAE        Selected
   1    1.045122e-02   TRUE  0.8240858  0.4538973  0.6405690          
   2    5.946477e-03   TRUE  0.8236994  0.4479295  0.6377730          
   5    2.646889e-05   TRUE  0.8430243  0.5875769  0.6955657          
   5    4.524156e-04   TRUE  0.8237490  0.4504758  0.6392863          
   5    2.015502e-03   TRUE  0.8235101  0.4487716  0.6365096  *       
   7    2.799366e-05  FALSE  0.8430243  0.6315302  0.6955656          
   7    9.887900e-03  FALSE  0.8235223  0.4440144  0.6350892          
   9    5.378071e-04   TRUE  0.8251197  0.4494876  0.6471906          
  11    1.295439e+00  FALSE  0.8459537  0.8590860  0.6980829          
  12    2.189734e-03   TRUE  0.8248990  0.4419916  0.6477958          
  13    6.111460e-05   TRUE  0.8430243  0.6539901  0.6955656          
  13    6.124422e-03   TRUE  0.8270445  0.4440659  0.6567861          
  13    2.496491e-02   TRUE  0.8388499  0.6684547  0.6873022          
  16    1.706717e-02  FALSE  0.8383015  0.5486406  0.6872389          
  17    4.966176e-01   TRUE  0.8440445  0.8574539  0.6964421          
  17    2.136145e+00   TRUE  0.8467778  0.8691793  0.6988009          
  18    3.637014e-04  FALSE  0.8407027  0.5994376  0.6909734          
  19    1.038090e-04   TRUE  0.8430244  0.6488700  0.6955656          
  19    1.132085e-02   TRUE  0.8384556  0.5658837  0.6871622          
  20    1.775860e-01  FALSE  0.8433844  0.8511721  0.6958739          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 5, decay = 0.002015502 and
 bag = TRUE.
[1] "Mon Mar 12 13:43:21 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 13:43:28 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "bag"                            
Bagged MARS 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results across tuning parameters:

  degree  nprune  RMSE        Rsquared   MAE         Selected
  1       2       0.12310340  0.9639150  0.09842461          
  1       3       0.07453218  0.9865770  0.05767025          
  1       4       0.05219044  0.9934274  0.04182773          
  1       5       0.03416441  0.9971073  0.02518848          
  1       7       0.02201828  0.9987955  0.01651325  *       
  2       2       0.12351504  0.9634845  0.09892592          
  2       5       0.03553223  0.9968875  0.02614280          
  2       6       0.02465075  0.9984918  0.01863274          
  2       7       0.02225851  0.9987686  0.01665951          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nprune = 7 and degree = 1.
[1] "Mon Mar 12 13:44:07 2018"
Bagged MARS using gCV Pruning 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results:

  RMSE        Rsquared   MAE       
  0.02191368  0.9988076  0.01641248

Tuning parameter 'degree' was held constant at a value of 1
[1] "Mon Mar 12 13:44:46 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :5     NA's   :5     NA's   :5    
Error : Stopping
In addition: There were 16 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :5     NA's   :5     NA's   :5    
Error : Stopping
In addition: There were 16 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Mon Mar 12 13:45:31 2018"        "bas interactive latent features"
 [5] "ignore"                          "none"                           
 [7] "range01"                         "ACEREBOUT"                      
 [9] "14th20hp3cv"                     "bam"                            
bartMachine initializing with 64 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 85.4/477.6MB
Iteration 200/1250  mem: 128.8/477.6MB
Iteration 300/1250  mem: 74.3/477.6MB
Iteration 400/1250  mem: 114.7/477.6MB
Iteration 500/1250  mem: 56/477.6MB
Iteration 600/1250  mem: 94.3/477.6MB
Iteration 700/1250  mem: 132.8/477.6MB
Iteration 800/1250  mem: 161.2/477.6MB
Iteration 900/1250  mem: 76.9/477.6MB
Iteration 1000/1250  mem: 90.9/477.6MB
Iteration 1100/1250  mem: 101.7/477.6MB
Iteration 1200/1250  mem: 111.3/477.6MB
done building BART in 2.672 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 37 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 169.1/477.6MB
Iteration 200/1250  mem: 91.6/477.6MB
Iteration 300/1250  mem: 120.7/477.6MB
Iteration 400/1250  mem: 154.5/477.6MB
Iteration 500/1250  mem: 89.7/477.6MB
Iteration 600/1250  mem: 126.5/477.6MB
Iteration 700/1250  mem: 162.7/477.6MB
Iteration 800/1250  mem: 101.6/477.6MB
Iteration 900/1250  mem: 139.8/477.6MB
Iteration 1000/1250  mem: 174.4/477.6MB
Iteration 1100/1250  mem: 114.2/477.6MB
Iteration 1200/1250  mem: 146.3/477.6MB
done building BART in 1.593 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 85 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 178.1/477.6MB
Iteration 200/1250  mem: 147.7/477.6MB
Iteration 300/1250  mem: 202.3/477.6MB
Iteration 400/1250  mem: 156/477.6MB
Iteration 500/1250  mem: 183.8/477.6MB
Iteration 600/1250  mem: 209.1/477.6MB
Iteration 700/1250  mem: 144.1/477.6MB
Iteration 800/1250  mem: 160/477.6MB
Iteration 900/1250  mem: 162/477.6MB
Iteration 1000/1250  mem: 260.6/477.6MB
Iteration 1100/1250  mem: 241.5/477.6MB
Iteration 1200/1250  mem: 207.7/477.6MB
done building BART in 3.484 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 82 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 237.5/477.6MB
Iteration 200/1250  mem: 224.4/477.6MB
Iteration 300/1250  mem: 249.8/477.6MB
Iteration 400/1250  mem: 225.7/477.6MB
Iteration 500/1250  mem: 224.5/477.6MB
Iteration 600/1250  mem: 290/477.6MB
Iteration 700/1250  mem: 251/477.6MB
Iteration 800/1250  mem: 275.2/477.6MB
Iteration 900/1250  mem: 285.8/477.6MB
Iteration 1000/1250  mem: 280.6/477.6MB
Iteration 1100/1250  mem: 266.2/477.6MB
Iteration 1200/1250  mem: 250.1/477.6MB
done building BART in 3.438 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 57 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 336.1/477.6MB
Iteration 200/1250  mem: 266.4/477.6MB
Iteration 300/1250  mem: 290.6/477.6MB
Iteration 400/1250  mem: 312.3/477.6MB
Iteration 500/1250  mem: 333.3/477.6MB
Iteration 600/1250  mem: 348.6/477.6MB
Iteration 700/1250  mem: 363.2/477.6MB
Iteration 800/1250  mem: 287.2/477.6MB
Iteration 900/1250  mem: 307.9/477.6MB
Iteration 1000/1250  mem: 332.8/477.6MB
Iteration 1100/1250  mem: 351.4/477.6MB
Iteration 1200/1250  mem: 368.8/477.6MB
done building BART in 2.406 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 93 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 374.1/477.6MB
Iteration 200/1250  mem: 114.8/477.6MB
Iteration 300/1250  mem: 146.3/477.6MB
Iteration 400/1250  mem: 101.9/477.6MB
Iteration 500/1250  mem: 108.7/477.6MB
Iteration 600/1250  mem: 98/477.6MB
Iteration 700/1250  mem: 159.6/477.6MB
Iteration 800/1250  mem: 120.7/477.6MB
Iteration 900/1250  mem: 178.3/477.6MB
Iteration 1000/1250  mem: 138.2/477.6MB
Iteration 1100/1250  mem: 184.4/477.6MB
Iteration 1200/1250  mem: 223.9/477.6MB
done building BART in 3.875 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 65 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 173.2/477.6MB
Iteration 200/1250  mem: 230.9/477.6MB
Iteration 300/1250  mem: 208.6/477.6MB
Iteration 400/1250  mem: 179.9/477.6MB
Iteration 500/1250  mem: 232.5/477.6MB
Iteration 600/1250  mem: 183.1/477.6MB
Iteration 700/1250  mem: 226.4/477.6MB
Iteration 800/1250  mem: 183.8/477.6MB
Iteration 900/1250  mem: 230.2/477.6MB
Iteration 1000/1250  mem: 273.5/477.6MB
Iteration 1100/1250  mem: 212.4/477.6MB
Iteration 1200/1250  mem: 237.8/477.6MB
done building BART in 2.672 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 32 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 279.9/477.6MB
Iteration 200/1250  mem: 291/477.6MB
Iteration 300/1250  mem: 301.9/477.6MB
Iteration 400/1250  mem: 201.1/477.6MB
Iteration 500/1250  mem: 209.7/477.6MB
Iteration 600/1250  mem: 213.9/477.6MB
Iteration 700/1250  mem: 220/477.6MB
Iteration 800/1250  mem: 228/477.6MB
Iteration 900/1250  mem: 236.5/477.6MB
Iteration 1000/1250  mem: 244.2/477.6MB
Iteration 1100/1250  mem: 252.3/477.6MB
Iteration 1200/1250  mem: 262.6/477.6MB
done building BART in 1.265 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 19 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 299.2/477.6MB
Iteration 200/1250  mem: 265.7/477.6MB
Iteration 300/1250  mem: 233.8/477.6MB
Iteration 400/1250  mem: 302.5/477.6MB
Iteration 500/1250  mem: 269.9/477.6MB
Iteration 600/1250  mem: 238.4/477.6MB
Iteration 700/1250  mem: 308.4/477.6MB
Iteration 800/1250  mem: 276.3/477.6MB
Iteration 900/1250  mem: 249.4/477.6MB
Iteration 1000/1250  mem: 319.1/477.6MB
Iteration 1100/1250  mem: 291.5/477.6MB
Iteration 1200/1250  mem: 267.6/477.6MB
done building BART in 0.765 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 95 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 303.2/477.6MB
Iteration 200/1250  mem: 272.2/477.6MB
Iteration 300/1250  mem: 268.5/477.6MB
Iteration 400/1250  mem: 282.2/477.6MB
Iteration 500/1250  mem: 297.2/477.6MB
Iteration 600/1250  mem: 283.5/477.6MB
Iteration 700/1250  mem: 314.1/477.6MB
Iteration 800/1250  mem: 321/477.6MB
Iteration 900/1250  mem: 301.8/477.6MB
Iteration 1000/1250  mem: 353.2/477.6MB
Iteration 1100/1250  mem: 394.4/477.6MB
Iteration 1200/1250  mem: 300.6/477.6MB
done building BART in 4.062 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 384.3/477.6MB
Iteration 200/1250  mem: 382.6/477.6MB
Iteration 300/1250  mem: 405.9/477.6MB
Iteration 400/1250  mem: 347.8/477.6MB
Iteration 500/1250  mem: 145.9/477.6MB
Iteration 600/1250  mem: 111.3/477.6MB
Iteration 700/1250  mem: 176.8/477.6MB
Iteration 800/1250  mem: 140/477.6MB
Iteration 900/1250  mem: 177.6/477.6MB
Iteration 1000/1250  mem: 197.9/477.6MB
Iteration 1100/1250  mem: 194.6/477.6MB
Iteration 1200/1250  mem: 176.6/482.9MB
done building BART in 3.766 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 48 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 183.8/481.8MB
Iteration 200/1250  mem: 131.1/477.6MB
Iteration 300/1250  mem: 193.8/477.6MB
Iteration 400/1250  mem: 161.5/477.6MB
Iteration 500/1250  mem: 138.3/477.6MB
Iteration 600/1250  mem: 216.1/477.6MB
Iteration 700/1250  mem: 204.5/477.6MB
Iteration 800/1250  mem: 200.1/477.6MB
Iteration 900/1250  mem: 197.2/477.6MB
Iteration 1000/1250  mem: 196.5/477.6MB
Iteration 1100/1250  mem: 188.5/477.6MB
Iteration 1200/1250  mem: 172.3/477.6MB
done building BART in 1.969 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 30 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 197.4/477.6MB
Iteration 200/1250  mem: 207.9/477.6MB
Iteration 300/1250  mem: 213.2/477.6MB
Iteration 400/1250  mem: 221.6/477.6MB
Iteration 500/1250  mem: 227.8/477.6MB
Iteration 600/1250  mem: 232.5/477.6MB
Iteration 700/1250  mem: 234.6/477.6MB
Iteration 800/1250  mem: 238.8/477.6MB
Iteration 900/1250  mem: 240.5/477.6MB
Iteration 1000/1250  mem: 243.4/477.6MB
Iteration 1100/1250  mem: 245.5/477.6MB
Iteration 1200/1250  mem: 245.8/477.6MB
done building BART in 1.188 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 10 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 241.7/477.6MB
Iteration 200/1250  mem: 280.4/477.6MB
Iteration 300/1250  mem: 213/477.6MB
Iteration 400/1250  mem: 251/477.6MB
Iteration 500/1250  mem: 289.1/477.6MB
Iteration 600/1250  mem: 223.8/477.6MB
Iteration 700/1250  mem: 260.8/477.6MB
Iteration 800/1250  mem: 299.8/477.6MB
Iteration 900/1250  mem: 233.7/477.6MB
Iteration 1000/1250  mem: 272.6/477.6MB
Iteration 1100/1250  mem: 208.7/477.6MB
Iteration 1200/1250  mem: 248/477.6MB
done building BART in 0.422 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 62 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 239.3/477.6MB
Iteration 200/1250  mem: 296.2/477.6MB
Iteration 300/1250  mem: 271/477.6MB
Iteration 400/1250  mem: 242.7/477.6MB
Iteration 500/1250  mem: 289.4/477.6MB
Iteration 600/1250  mem: 241.7/477.6MB
Iteration 700/1250  mem: 290/477.6MB
Iteration 800/1250  mem: 258.4/477.6MB
Iteration 900/1250  mem: 312.4/477.6MB
Iteration 1000/1250  mem: 271.8/477.6MB
Iteration 1100/1250  mem: 311.4/477.6MB
Iteration 1200/1250  mem: 342.5/477.6MB
done building BART in 2.64 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 86 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 307.9/477.6MB
Iteration 200/1250  mem: 358.3/477.6MB
Iteration 300/1250  mem: 351.5/477.6MB
Iteration 400/1250  mem: 340.7/477.6MB
Iteration 500/1250  mem: 308/477.6MB
Iteration 600/1250  mem: 347.5/477.6MB
Iteration 700/1250  mem: 383.3/477.6MB
Iteration 800/1250  mem: 407.5/477.6MB
Iteration 900/1250  mem: 405.6/477.6MB
Iteration 1000/1250  mem: 393/477.6MB
Iteration 1100/1250  mem: 370.4/477.6MB
Iteration 1200/1250  mem: 340.2/479.2MB
done building BART in 3.5 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 39 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 389.3/479.2MB
Iteration 200/1250  mem: 381.8/479.2MB
Iteration 300/1250  mem: 375.1/477.6MB
Iteration 400/1250  mem: 373.4/477.6MB
Iteration 500/1250  mem: 375.5/477.6MB
Iteration 600/1250  mem: 379.7/477.6MB
Iteration 700/1250  mem: 389.9/477.6MB
Iteration 800/1250  mem: 141.5/477.6MB
Iteration 900/1250  mem: 160.1/477.6MB
Iteration 1000/1250  mem: 176.6/477.6MB
Iteration 1100/1250  mem: 89.8/477.6MB
Iteration 1200/1250  mem: 103.2/477.6MB
done building BART in 3.125 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 67 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 178.6/477.6MB
Iteration 200/1250  mem: 102.2/477.6MB
Iteration 300/1250  mem: 128.6/477.6MB
Iteration 400/1250  mem: 157/477.6MB
Iteration 500/1250  mem: 197.8/477.6MB
Iteration 600/1250  mem: 162.4/477.6MB
Iteration 700/1250  mem: 153.6/477.6MB
Iteration 800/1250  mem: 142.5/477.6MB
Iteration 900/1250  mem: 200.7/477.6MB
Iteration 1000/1250  mem: 147.9/477.6MB
Iteration 1100/1250  mem: 183.3/477.6MB
Iteration 1200/1250  mem: 209/477.6MB
done building BART in 2.688 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 99 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 190.4/477.6MB
Iteration 200/1250  mem: 196.5/477.6MB
Iteration 300/1250  mem: 238.4/477.6MB
Iteration 400/1250  mem: 207/477.6MB
Iteration 500/1250  mem: 237.1/477.6MB
Iteration 600/1250  mem: 252/477.6MB
Iteration 700/1250  mem: 237.3/477.6MB
Iteration 800/1250  mem: 222.1/477.6MB
Iteration 900/1250  mem: 302.8/477.6MB
Iteration 1000/1250  mem: 290.9/477.6MB
Iteration 1100/1250  mem: 280.1/477.6MB
Iteration 1200/1250  mem: 262.2/477.6MB
done building BART in 4.219 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 28 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 275.9/477.6MB
Iteration 200/1250  mem: 278.5/477.6MB
Iteration 300/1250  mem: 281.8/477.6MB
Iteration 400/1250  mem: 287.7/477.6MB
Iteration 500/1250  mem: 291.8/477.6MB
Iteration 600/1250  mem: 295.7/477.6MB
Iteration 700/1250  mem: 300.5/477.6MB
Iteration 800/1250  mem: 302.3/477.6MB
Iteration 900/1250  mem: 305/477.6MB
Iteration 1000/1250  mem: 307/477.6MB
Iteration 1100/1250  mem: 308.9/477.6MB
Iteration 1200/1250  mem: 310.9/477.6MB
done building BART in 1.141 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 64 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 332.3/477.6MB
Iteration 200/1250  mem: 378.1/477.6MB
Iteration 300/1250  mem: 325.6/477.6MB
Iteration 400/1250  mem: 363.5/477.6MB
Iteration 500/1250  mem: 303.3/477.6MB
Iteration 600/1250  mem: 340.3/477.6MB
Iteration 700/1250  mem: 376.8/477.6MB
Iteration 800/1250  mem: 321.8/477.6MB
Iteration 900/1250  mem: 364.6/477.6MB
Iteration 1000/1250  mem: 309.5/477.6MB
Iteration 1100/1250  mem: 344.8/477.6MB
Iteration 1200/1250  mem: 374.1/477.6MB
done building BART in 2.625 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 37 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 334.8/477.6MB
Iteration 200/1250  mem: 364.4/477.6MB
Iteration 300/1250  mem: 389/477.6MB
Iteration 400/1250  mem: 414.1/477.6MB
Iteration 500/1250  mem: 332.8/477.6MB
Iteration 600/1250  mem: 354.1/477.6MB
Iteration 700/1250  mem: 373/477.6MB
Iteration 800/1250  mem: 396.5/477.6MB
Iteration 900/1250  mem: 417.4/477.6MB
Iteration 1000/1250  mem: 440.9/477.6MB
Iteration 1100/1250  mem: 357.7/477.6MB
Iteration 1200/1250  mem: 380.2/477.6MB
done building BART in 1.485 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 85 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 123.6/477.6MB
Iteration 200/1250  mem: 137.1/477.6MB
Iteration 300/1250  mem: 149.7/477.6MB
Iteration 400/1250  mem: 147.5/477.6MB
Iteration 500/1250  mem: 131.4/477.6MB
Iteration 600/1250  mem: 127.1/477.6MB
Iteration 700/1250  mem: 118.9/477.6MB
Iteration 800/1250  mem: 208.4/477.6MB
Iteration 900/1250  mem: 177.9/477.6MB
Iteration 1000/1250  mem: 134.7/477.6MB
Iteration 1100/1250  mem: 204.5/477.6MB
Iteration 1200/1250  mem: 152.9/477.6MB
done building BART in 3.39 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 82 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 174.8/477.6MB
Iteration 200/1250  mem: 253.2/477.6MB
Iteration 300/1250  mem: 228.6/477.6MB
Iteration 400/1250  mem: 226.9/477.6MB
Iteration 500/1250  mem: 249.7/477.6MB
Iteration 600/1250  mem: 222.5/477.6MB
Iteration 700/1250  mem: 271.9/477.6MB
Iteration 800/1250  mem: 214.6/477.6MB
Iteration 900/1250  mem: 223.9/477.6MB
Iteration 1000/1250  mem: 222.3/477.6MB
Iteration 1100/1250  mem: 311.2/477.6MB
Iteration 1200/1250  mem: 293.9/477.6MB
done building BART in 3.312 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 57 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 253.3/477.6MB
Iteration 200/1250  mem: 258/477.6MB
Iteration 300/1250  mem: 264.3/477.6MB
Iteration 400/1250  mem: 268/477.6MB
Iteration 500/1250  mem: 269.4/477.6MB
Iteration 600/1250  mem: 264.5/477.6MB
Iteration 700/1250  mem: 261.6/477.6MB
Iteration 800/1250  mem: 253.5/477.6MB
Iteration 900/1250  mem: 352.1/477.6MB
Iteration 1000/1250  mem: 348.4/477.6MB
Iteration 1100/1250  mem: 346.4/477.6MB
Iteration 1200/1250  mem: 344.5/477.6MB
done building BART in 2.265 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 93 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 320.9/477.6MB
Iteration 200/1250  mem: 319.7/477.6MB
Iteration 300/1250  mem: 332.7/477.6MB
Iteration 400/1250  mem: 336.2/477.6MB
Iteration 500/1250  mem: 319.5/477.6MB
Iteration 600/1250  mem: 388.3/477.6MB
Iteration 700/1250  mem: 360.9/477.6MB
Iteration 800/1250  mem: 404.1/477.6MB
Iteration 900/1250  mem: 342.7/477.6MB
Iteration 1000/1250  mem: 369/477.6MB
Iteration 1100/1250  mem: 390.6/477.6MB
Iteration 1200/1250  mem: 406.1/477.6MB
done building BART in 4.109 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 65 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 203.4/477.6MB
Iteration 200/1250  mem: 158.9/496MB
Iteration 300/1250  mem: 202.3/496MB
Iteration 400/1250  mem: 242.4/498.1MB
Iteration 500/1250  mem: 169.3/499.6MB
Iteration 600/1250  mem: 194.9/494.9MB
Iteration 700/1250  mem: 224.3/498.1MB
Iteration 800/1250  mem: 256.4/497.5MB
Iteration 900/1250  mem: 181.9/499.6MB
Iteration 1000/1250  mem: 205.4/500.7MB
Iteration 1100/1250  mem: 220.9/503.3MB
Iteration 1200/1250  mem: 227.1/504.4MB
done building BART in 2.531 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 32 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 256.1/504.4MB
Iteration 200/1250  mem: 259.6/504.4MB
Iteration 300/1250  mem: 259.5/503.3MB
Iteration 400/1250  mem: 264/500.2MB
Iteration 500/1250  mem: 269/502.3MB
Iteration 600/1250  mem: 274.4/499.1MB
Iteration 700/1250  mem: 280.1/501.2MB
Iteration 800/1250  mem: 288.7/497.5MB
Iteration 900/1250  mem: 295.3/500.2MB
Iteration 1000/1250  mem: 303.5/500.7MB
Iteration 1100/1250  mem: 207.6/500.7MB
Iteration 1200/1250  mem: 217.5/500.7MB
done building BART in 1.25 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 19 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 255.3/498.1MB
Iteration 200/1250  mem: 222.3/498.6MB
Iteration 300/1250  mem: 290.8/498.6MB
Iteration 400/1250  mem: 262.4/499.1MB
Iteration 500/1250  mem: 232.1/499.1MB
Iteration 600/1250  mem: 301/499.1MB
Iteration 700/1250  mem: 270.6/496.5MB
Iteration 800/1250  mem: 238.3/496.5MB
Iteration 900/1250  mem: 309.1/496.5MB
Iteration 1000/1250  mem: 281.9/494.4MB
Iteration 1100/1250  mem: 254.3/494.4MB
Iteration 1200/1250  mem: 323.9/494.4MB
done building BART in 0.766 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 95 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 253.5/484.4MB
Iteration 200/1250  mem: 314.1/485MB
Iteration 300/1250  mem: 275.6/487.6MB
Iteration 400/1250  mem: 317.8/477.6MB
Iteration 500/1250  mem: 293.4/485.5MB
Iteration 600/1250  mem: 328.1/489.2MB
Iteration 700/1250  mem: 314.3/496.5MB
Iteration 800/1250  mem: 352.8/500.2MB
Iteration 900/1250  mem: 371.4/504.4MB
Iteration 1000/1250  mem: 366.2/508MB
Iteration 1100/1250  mem: 346.8/509.6MB
Iteration 1200/1250  mem: 320.4/511.2MB
done building BART in 4.0 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 385/499.6MB
Iteration 200/1250  mem: 390.7/498.1MB
Iteration 300/1250  mem: 317.7/497.5MB
Iteration 400/1250  mem: 351.6/497MB
Iteration 500/1250  mem: 380.3/498.6MB
Iteration 600/1250  mem: 407.4/499.1MB
Iteration 700/1250  mem: 431.3/497.5MB
Iteration 800/1250  mem: 340.3/502.3MB
Iteration 900/1250  mem: 445.3/503.3MB
Iteration 1000/1250  mem: 428.7/505.9MB
Iteration 1100/1250  mem: 396.1/508MB
Iteration 1200/1250  mem: 353/510.7MB
done building BART in 3.672 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 48 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 472.6/511.2MB
Iteration 200/1250  mem: 403.2/504.4MB
Iteration 300/1250  mem: 459.9/505.9MB
Iteration 400/1250  mem: 402.1/502.8MB
Iteration 500/1250  mem: 464.6/502.8MB
Iteration 600/1250  mem: 420.3/496MB
Iteration 700/1250  mem: 394.3/491.8MB
Iteration 800/1250  mem: 466.4/493.4MB
Iteration 900/1250  mem: 450.2/492.3MB
Iteration 1000/1250  mem: 142.6/477.6MB
Iteration 1100/1250  mem: 134.9/477.6MB
Iteration 1200/1250  mem: 144.7/477.6MB
done building BART in 2.031 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 30 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 126.3/477.6MB
Iteration 200/1250  mem: 156.2/477.6MB
Iteration 300/1250  mem: 181.4/477.6MB
Iteration 400/1250  mem: 114.2/477.6MB
Iteration 500/1250  mem: 130.2/477.6MB
Iteration 600/1250  mem: 139.9/477.6MB
Iteration 700/1250  mem: 148.9/477.6MB
Iteration 800/1250  mem: 153.1/477.6MB
Iteration 900/1250  mem: 158.3/477.6MB
Iteration 1000/1250  mem: 166.1/477.6MB
Iteration 1100/1250  mem: 172.9/477.6MB
Iteration 1200/1250  mem: 180.6/477.6MB
done building BART in 1.14 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 10 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 186.5/477.6MB
Iteration 200/1250  mem: 129.7/477.6MB
Iteration 300/1250  mem: 168.4/477.6MB
Iteration 400/1250  mem: 207.1/477.6MB
Iteration 500/1250  mem: 151.1/477.6MB
Iteration 600/1250  mem: 189.8/477.6MB
Iteration 700/1250  mem: 228.4/477.6MB
Iteration 800/1250  mem: 174.1/477.6MB
Iteration 900/1250  mem: 213.7/477.6MB
Iteration 1000/1250  mem: 157/477.6MB
Iteration 1100/1250  mem: 196.6/477.6MB
Iteration 1200/1250  mem: 143.8/477.6MB
done building BART in 0.406 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 62 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 157.3/477.6MB
Iteration 200/1250  mem: 234.1/477.6MB
Iteration 300/1250  mem: 226.9/477.6MB
Iteration 400/1250  mem: 222.9/477.6MB
Iteration 500/1250  mem: 226.9/477.6MB
Iteration 600/1250  mem: 216.4/477.6MB
Iteration 700/1250  mem: 263.7/477.6MB
Iteration 800/1250  mem: 189.4/477.6MB
Iteration 900/1250  mem: 201.4/479.2MB
Iteration 1000/1250  mem: 207/480.2MB
Iteration 1100/1250  mem: 208.1/481.3MB
Iteration 1200/1250  mem: 212/481.8MB
done building BART in 2.5 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 86 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 246.7/477.6MB
Iteration 200/1250  mem: 285.6/477.6MB
Iteration 300/1250  mem: 271.6/477.6MB
Iteration 400/1250  mem: 250/477.6MB
Iteration 500/1250  mem: 297.2/477.6MB
Iteration 600/1250  mem: 248.5/477.6MB
Iteration 700/1250  mem: 261.9/478.2MB
Iteration 800/1250  mem: 254.7/479.2MB
Iteration 900/1250  mem: 348.3/479.2MB
Iteration 1000/1250  mem: 335.4/479.7MB
Iteration 1100/1250  mem: 317.3/481.3MB
Iteration 1200/1250  mem: 298.4/482.3MB
done building BART in 3.438 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 39 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 350.1/481.8MB
Iteration 200/1250  mem: 344.4/482.3MB
Iteration 300/1250  mem: 337.3/483.4MB
Iteration 400/1250  mem: 328.3/477.6MB
Iteration 500/1250  mem: 319.8/477.6MB
Iteration 600/1250  mem: 314.9/482.3MB
Iteration 700/1250  mem: 313/477.6MB
Iteration 800/1250  mem: 313.3/477.6MB
Iteration 900/1250  mem: 313.2/477.6MB
Iteration 1000/1250  mem: 316/480.2MB
Iteration 1100/1250  mem: 317.3/480.2MB
Iteration 1200/1250  mem: 310.1/482.3MB
done building BART in 3.031 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 67 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 349.3/486MB
Iteration 200/1250  mem: 360.1/483.4MB
Iteration 300/1250  mem: 372.4/481.3MB
Iteration 400/1250  mem: 396.5/478.2MB
Iteration 500/1250  mem: 328.9/477.6MB
Iteration 600/1250  mem: 377.1/477.6MB
Iteration 700/1250  mem: 350.7/477.6MB
Iteration 800/1250  mem: 331.9/477.6MB
Iteration 900/1250  mem: 393.7/477.6MB
Iteration 1000/1250  mem: 343.2/477.6MB
Iteration 1100/1250  mem: 377.7/479.2MB
Iteration 1200/1250  mem: 405.8/481.3MB
done building BART in 2.719 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 99 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 389.7/477.6MB
Iteration 200/1250  mem: 382.7/477.6MB
Iteration 300/1250  mem: 127.1/477.6MB
Iteration 400/1250  mem: 102.9/477.6MB
Iteration 500/1250  mem: 152/477.6MB
Iteration 600/1250  mem: 143.7/477.6MB
Iteration 700/1250  mem: 194.2/477.6MB
Iteration 800/1250  mem: 230.5/477.6MB
Iteration 900/1250  mem: 154.4/477.6MB
Iteration 1000/1250  mem: 169.8/477.6MB
Iteration 1100/1250  mem: 188.2/477.6MB
Iteration 1200/1250  mem: 210.3/477.6MB
done building BART in 4.0 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 28 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 181.4/477.6MB
Iteration 200/1250  mem: 167.9/477.6MB
Iteration 300/1250  mem: 270.6/477.6MB
Iteration 400/1250  mem: 254.6/477.6MB
Iteration 500/1250  mem: 239.3/477.6MB
Iteration 600/1250  mem: 222.1/478.7MB
Iteration 700/1250  mem: 200.3/477.6MB
Iteration 800/1250  mem: 180.4/477.6MB
Iteration 900/1250  mem: 280.3/477.6MB
Iteration 1000/1250  mem: 263.9/477.6MB
Iteration 1100/1250  mem: 247.2/477.6MB
Iteration 1200/1250  mem: 235.6/477.6MB
done building BART in 1.016 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 64 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 229.1/477.6MB
Iteration 200/1250  mem: 265.5/477.6MB
Iteration 300/1250  mem: 222.1/477.6MB
Iteration 400/1250  mem: 265.4/477.6MB
Iteration 500/1250  mem: 208.8/477.6MB
Iteration 600/1250  mem: 247.8/477.6MB
Iteration 700/1250  mem: 281.5/477.6MB
Iteration 800/1250  mem: 304.4/477.6MB
Iteration 900/1250  mem: 314.8/477.6MB
Iteration 1000/1250  mem: 316.8/477.6MB
Iteration 1100/1250  mem: 317.6/477.6MB
Iteration 1200/1250  mem: 317.8/477.6MB
done building BART in 2.531 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 37 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 244.3/477.6MB
Iteration 200/1250  mem: 259.9/477.6MB
Iteration 300/1250  mem: 277.2/477.6MB
Iteration 400/1250  mem: 298.1/477.6MB
Iteration 500/1250  mem: 317.1/477.6MB
Iteration 600/1250  mem: 337.9/477.6MB
Iteration 700/1250  mem: 359/477.6MB
Iteration 800/1250  mem: 263.1/477.6MB
Iteration 900/1250  mem: 283.5/477.6MB
Iteration 1000/1250  mem: 301.8/477.6MB
Iteration 1100/1250  mem: 320.3/477.6MB
Iteration 1200/1250  mem: 338.8/477.6MB
done building BART in 1.5 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 85 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 305.6/477.6MB
Iteration 200/1250  mem: 303.4/477.6MB
Iteration 300/1250  mem: 302.7/477.6MB
Iteration 400/1250  mem: 290/477.6MB
Iteration 500/1250  mem: 388.3/477.6MB
Iteration 600/1250  mem: 383.5/477.6MB
Iteration 700/1250  mem: 362.1/477.6MB
Iteration 800/1250  mem: 322.9/477.6MB
Iteration 900/1250  mem: 395.8/477.6MB
Iteration 1000/1250  mem: 353.2/477.6MB
Iteration 1100/1250  mem: 427.6/477.6MB
Iteration 1200/1250  mem: 376.6/478.7MB
done building BART in 3.375 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 82 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 389.7/477.6MB
Iteration 200/1250  mem: 353.9/477.6MB
Iteration 300/1250  mem: 351.8/477.6MB
Iteration 400/1250  mem: 92.2/477.6MB
Iteration 500/1250  mem: 106.9/477.6MB
Iteration 600/1250  mem: 118.8/477.6MB
Iteration 700/1250  mem: 206.9/477.6MB
Iteration 800/1250  mem: 175.9/477.6MB
Iteration 900/1250  mem: 140.3/477.6MB
Iteration 1000/1250  mem: 210.7/477.6MB
Iteration 1100/1250  mem: 158.1/478.2MB
Iteration 1200/1250  mem: 210.6/477.6MB
done building BART in 3.188 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 57 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 242.2/478.2MB
Iteration 200/1250  mem: 211.4/481.8MB
Iteration 300/1250  mem: 173.8/482.3MB
Iteration 400/1250  mem: 256/481.3MB
Iteration 500/1250  mem: 215/481.3MB
Iteration 600/1250  mem: 174/479.7MB
Iteration 700/1250  mem: 254.3/478.2MB
Iteration 800/1250  mem: 219.1/478.7MB
Iteration 900/1250  mem: 188.8/478.2MB
Iteration 1000/1250  mem: 273.1/479.2MB
Iteration 1100/1250  mem: 239.4/480.8MB
Iteration 1200/1250  mem: 198.5/477.6MB
done building BART in 2.172 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 93 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 312.2/477.6MB
Iteration 200/1250  mem: 317.8/477.6MB
Iteration 300/1250  mem: 239.2/477.6MB
Iteration 400/1250  mem: 262.6/477.6MB
Iteration 500/1250  mem: 275.3/477.6MB
Iteration 600/1250  mem: 265.8/479.7MB
Iteration 700/1250  mem: 362.2/480.8MB
Iteration 800/1250  mem: 335.8/480.8MB
Iteration 900/1250  mem: 300/483.9MB
Iteration 1000/1250  mem: 379.3/483.9MB
Iteration 1100/1250  mem: 333/477.6MB
Iteration 1200/1250  mem: 409.1/482.9MB
done building BART in 3.5 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 65 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 343.1/479.7MB
Iteration 200/1250  mem: 328.3/478.2MB
Iteration 300/1250  mem: 318.8/477.6MB
Iteration 400/1250  mem: 324.5/477.6MB
Iteration 500/1250  mem: 341.3/477.6MB
Iteration 600/1250  mem: 364.5/477.6MB
Iteration 700/1250  mem: 385.3/477.6MB
Iteration 800/1250  mem: 396.9/477.6MB
Iteration 900/1250  mem: 406.4/477.6MB
Iteration 1000/1250  mem: 411.6/477.6MB
Iteration 1100/1250  mem: 408.3/479.2MB
Iteration 1200/1250  mem: 399.2/480.8MB
done building BART in 2.469 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 32 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 404.6/483.4MB
Iteration 200/1250  mem: 393.5/478.7MB
Iteration 300/1250  mem: 386.6/477.6MB
Iteration 400/1250  mem: 90.2/477.6MB
Iteration 500/1250  mem: 93.7/477.6MB
Iteration 600/1250  mem: 93.8/477.6MB
Iteration 700/1250  mem: 94/477.6MB
Iteration 800/1250  mem: 94.2/477.6MB
Iteration 900/1250  mem: 94.2/477.6MB
Iteration 1000/1250  mem: 94.5/477.6MB
Iteration 1100/1250  mem: 86.7/477.6MB
Iteration 1200/1250  mem: 82/477.6MB
done building BART in 1.219 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 19 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 103.8/477.6MB
Iteration 200/1250  mem: 174.5/477.6MB
Iteration 300/1250  mem: 124.2/477.6MB
Iteration 400/1250  mem: 192.3/477.6MB
Iteration 500/1250  mem: 146.4/477.6MB
Iteration 600/1250  mem: 97.9/477.6MB
Iteration 700/1250  mem: 168.4/477.6MB
Iteration 800/1250  mem: 119.9/477.6MB
Iteration 900/1250  mem: 190.4/477.6MB
Iteration 1000/1250  mem: 145/477.6MB
Iteration 1100/1250  mem: 102.1/477.6MB
Iteration 1200/1250  mem: 172.6/477.6MB
done building BART in 0.687 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 95 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 245.5/477.6MB
Iteration 200/1250  mem: 234.5/477.6MB
Iteration 300/1250  mem: 221.5/477.6MB
Iteration 400/1250  mem: 193.5/477.6MB
Iteration 500/1250  mem: 148.6/477.6MB
Iteration 600/1250  mem: 235.9/477.6MB
Iteration 700/1250  mem: 201.6/477.6MB
Iteration 800/1250  mem: 182.4/477.6MB
Iteration 900/1250  mem: 159.4/477.6MB
Iteration 1000/1250  mem: 254.9/477.6MB
Iteration 1100/1250  mem: 213.5/477.6MB
Iteration 1200/1250  mem: 297/477.6MB
done building BART in 3.687 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 195.1/477.6MB
Iteration 200/1250  mem: 294.5/477.6MB
Iteration 300/1250  mem: 293.4/477.6MB
Iteration 400/1250  mem: 292.8/477.6MB
Iteration 500/1250  mem: 284.9/477.6MB
Iteration 600/1250  mem: 252.2/477.6MB
Iteration 700/1250  mem: 330.7/477.6MB
Iteration 800/1250  mem: 273.4/477.6MB
Iteration 900/1250  mem: 334.7/477.6MB
Iteration 1000/1250  mem: 257/477.6MB
Iteration 1100/1250  mem: 311.9/477.6MB
Iteration 1200/1250  mem: 363.6/477.6MB
done building BART in 3.515 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 48 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 314/477.6MB
Iteration 200/1250  mem: 355.2/477.6MB
Iteration 300/1250  mem: 265.9/477.6MB
Iteration 400/1250  mem: 312.9/477.6MB
Iteration 500/1250  mem: 359/477.6MB
Iteration 600/1250  mem: 282.5/477.6MB
Iteration 700/1250  mem: 330/477.6MB
Iteration 800/1250  mem: 374/477.6MB
Iteration 900/1250  mem: 293.1/477.6MB
Iteration 1000/1250  mem: 339.2/477.6MB
Iteration 1100/1250  mem: 385.1/477.6MB
Iteration 1200/1250  mem: 302.7/477.6MB
done building BART in 1.75 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 30 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 391.1/477.6MB
Iteration 200/1250  mem: 372.8/477.6MB
Iteration 300/1250  mem: 350.1/477.6MB
Iteration 400/1250  mem: 331.3/477.6MB
Iteration 500/1250  mem: 311.6/477.6MB
Iteration 600/1250  mem: 298.3/477.6MB
Iteration 700/1250  mem: 403.6/477.6MB
Iteration 800/1250  mem: 392.4/477.6MB
Iteration 900/1250  mem: 379.3/477.6MB
Iteration 1000/1250  mem: 367.9/477.6MB
Iteration 1100/1250  mem: 117.9/477.6MB
Iteration 1200/1250  mem: 104.9/477.6MB
done building BART in 1.172 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 10 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 87.6/477.6MB
Iteration 200/1250  mem: 125.9/477.6MB
Iteration 300/1250  mem: 164.2/477.6MB
Iteration 400/1250  mem: 80.7/477.6MB
Iteration 500/1250  mem: 120.4/477.6MB
Iteration 600/1250  mem: 158.3/477.6MB
Iteration 700/1250  mem: 196.1/477.6MB
Iteration 800/1250  mem: 113.2/477.6MB
Iteration 900/1250  mem: 151.3/477.6MB
Iteration 1000/1250  mem: 189.4/477.6MB
Iteration 1100/1250  mem: 104.5/477.6MB
Iteration 1200/1250  mem: 143.6/477.6MB
done building BART in 0.344 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 62 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 171.2/477.6MB
Iteration 200/1250  mem: 142/477.6MB
Iteration 300/1250  mem: 236.2/477.6MB
Iteration 400/1250  mem: 193.9/477.6MB
Iteration 500/1250  mem: 144.7/477.6MB
Iteration 600/1250  mem: 232.1/477.6MB
Iteration 700/1250  mem: 191.1/477.6MB
Iteration 800/1250  mem: 158.3/477.6MB
Iteration 900/1250  mem: 249.7/477.6MB
Iteration 1000/1250  mem: 213.3/477.6MB
Iteration 1100/1250  mem: 169.1/477.6MB
Iteration 1200/1250  mem: 258.7/477.6MB
done building BART in 2.328 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 86 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 191.9/477.6MB
Iteration 200/1250  mem: 259.7/477.6MB
Iteration 300/1250  mem: 196.7/477.6MB
Iteration 400/1250  mem: 257.1/477.6MB
Iteration 500/1250  mem: 307.1/477.6MB
Iteration 600/1250  mem: 226/477.6MB
Iteration 700/1250  mem: 264.5/478.2MB
Iteration 800/1250  mem: 301.9/478.2MB
Iteration 900/1250  mem: 336.8/479.2MB
Iteration 1000/1250  mem: 223.6/479.7MB
Iteration 1100/1250  mem: 240.5/481.8MB
Iteration 1200/1250  mem: 254.8/482.9MB
done building BART in 3.126 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 39 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 349.2/481.3MB
Iteration 200/1250  mem: 272.1/478.2MB
Iteration 300/1250  mem: 347.3/481.3MB
Iteration 400/1250  mem: 279.9/480.8MB
Iteration 500/1250  mem: 357.7/481.3MB
Iteration 600/1250  mem: 295.5/480.8MB
Iteration 700/1250  mem: 230.6/480.2MB
Iteration 800/1250  mem: 309/481.3MB
Iteration 900/1250  mem: 243.2/481.3MB
Iteration 1000/1250  mem: 322/481.3MB
Iteration 1100/1250  mem: 253/481.8MB
Iteration 1200/1250  mem: 327.4/481.8MB
done building BART in 2.922 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 67 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 289.1/481.3MB
Iteration 200/1250  mem: 263.9/477.6MB
Iteration 300/1250  mem: 248.6/477.6MB
Iteration 400/1250  mem: 367.8/477.6MB
Iteration 500/1250  mem: 267/477.6MB
Iteration 600/1250  mem: 284.6/477.6MB
Iteration 700/1250  mem: 294.8/477.6MB
Iteration 800/1250  mem: 294.5/477.6MB
Iteration 900/1250  mem: 294.2/477.6MB
Iteration 1000/1250  mem: 287.9/477.6MB
Iteration 1100/1250  mem: 284.6/477.6MB
Iteration 1200/1250  mem: 402.9/477.6MB
done building BART in 2.609 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 99 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 410.7/477.6MB
Iteration 200/1250  mem: 413.9/477.6MB
Iteration 300/1250  mem: 173.3/480.8MB
Iteration 400/1250  mem: 165.5/481.3MB
Iteration 500/1250  mem: 136.3/487.6MB
Iteration 600/1250  mem: 222.1/489.2MB
Iteration 700/1250  mem: 153/491.8MB
Iteration 800/1250  mem: 218.3/492.3MB
Iteration 900/1250  mem: 281.7/493.9MB
Iteration 1000/1250  mem: 190.5/496MB
Iteration 1100/1250  mem: 242.7/497MB
Iteration 1200/1250  mem: 291.6/497.5MB
done building BART in 3.78 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 28 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 284.6/493.9MB
Iteration 200/1250  mem: 233.6/496.5MB
Iteration 300/1250  mem: 183.6/496MB
Iteration 400/1250  mem: 282.1/496MB
Iteration 500/1250  mem: 229.7/496.5MB
Iteration 600/1250  mem: 328.4/496.5MB
Iteration 700/1250  mem: 274.4/494.4MB
Iteration 800/1250  mem: 221.7/491.8MB
Iteration 900/1250  mem: 319.3/491.8MB
Iteration 1000/1250  mem: 267.1/495.5MB
Iteration 1100/1250  mem: 215.4/494.9MB
Iteration 1200/1250  mem: 312.7/494.9MB
done building BART in 0.938 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 85 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 293/491.8MB
Iteration 200/1250  mem: 333.3/491.8MB
Iteration 300/1250  mem: 359.9/492.8MB
Iteration 400/1250  mem: 379.3/492.8MB
Iteration 500/1250  mem: 255.3/490.7MB
Iteration 600/1250  mem: 281.2/492.8MB
Iteration 700/1250  mem: 304.4/492.3MB
Iteration 800/1250  mem: 321.6/493.9MB
Iteration 900/1250  mem: 335.5/494.9MB
Iteration 1000/1250  mem: 329.7/496MB
Iteration 1100/1250  mem: 320/497.5MB
Iteration 1200/1250  mem: 302.9/498.6MB
done building BART in 4.484 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
Bayesian Additive Regression Trees 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results across tuning parameters:

  num_trees  k          alpha      beta        nu          RMSE      
  10         2.5159726  0.9426329  2.04959563  2.32408928  0.04696163
  19         2.3118831  0.9087458  1.47206230  1.05858732  0.03748576
  28         1.3796146  0.9071687  2.96002136  4.81540718  0.03580225
  30         0.3522798  0.9253146  1.50167983  4.14667917  0.03688556
  32         1.9203194  0.9118284  3.34800037  4.29748287  0.03707635
  37         2.4959201  0.9669040  3.89690409  3.02588108  0.03406043
  39         0.3725497  0.9952245  3.39876916  0.09346051         NaN
  48         1.4421888  0.9125242  0.51939696  4.42914271  0.03180596
  57         4.2603474  0.9764594  3.77782247  1.26824549  0.02879724
  62         1.9503262  0.9341760  2.61523397  4.54841716  0.03656085
  64         0.6551208  0.9251416  2.11575748  2.12913726  0.03174300
  65         2.3225543  0.9159793  2.38788467  3.37242364  0.02850044
  67         2.8311083  0.9375851  0.08560428  1.23995975  0.02966299
  82         2.6934678  0.9969388  0.88989442  2.59506929  0.03318838
  85         3.9133518  0.9408547  3.92515343  1.08942163  0.02715882
  86         4.4413589  0.9009053  1.73354196  3.93424974  0.02837457
  90         1.3006208  0.9784807  3.41189544  4.98851374  0.03515597
  93         2.5448992  0.9238916  0.68502724  0.15244292  0.03240132
  95         0.8468623  0.9472324  3.24603870  1.93480647  0.02924985
  99         3.5411740  0.9732781  0.39271161  4.96910186  0.03437975
  Rsquared   MAE         Selected
  0.9944749  0.03554480          
  0.9965791  0.02920492          
  0.9968321  0.02794807          
  0.9966719  0.02895807          
  0.9966222  0.02923726          
  0.9971366  0.02691031          
        NaN         NaN          
  0.9975072  0.02437150          
  0.9979800  0.02249037          
  0.9965975  0.02696819          
  0.9975203  0.02329111          
  0.9979775  0.02189274          
  0.9978217  0.02303435          
  0.9972758  0.02481402          
  0.9982110  0.02103076  *       
  0.9981048  0.02169740          
  0.9967310  0.02433758          
  0.9974904  0.02511480          
  0.9978839  0.02254247          
  0.9971228  0.02687707          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were num_trees = 85, k = 3.913352, alpha
 = 0.9408547, beta = 3.925153 and nu = 1.089422.
[1] "Mon Mar 12 13:53:58 2018"
.....
Bayesian Generalized Linear Model 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results:

  RMSE        Rsquared   MAE        
  0.00409519  0.9999589  0.003261278

[1] "Mon Mar 12 13:54:12 2018"
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
The Bayesian lasso 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results across tuning parameters:

  sparsity      RMSE        Rsquared   MAE          Selected
  0.0009225586  0.00442040  0.9999588  0.003491932          
  0.0999489173  0.00442040  0.9999588  0.003491932          
  0.2058524624  0.00442040  0.9999588  0.003491932          
  0.2241455968  0.00442040  0.9999588  0.003491932          
  0.2445843588  0.00442040  0.9999588  0.003491932          
  0.3019970097  0.00442040  0.9999588  0.003491932          
  0.3201849442  0.00442040  0.9999588  0.003491932          
  0.4204412897  0.00442040  0.9999588  0.003491932          
  0.5192069609  0.00442040  0.9999588  0.003491932          
  0.5740000883  0.00442040  0.9999588  0.003491932          
  0.6027395471  0.00442040  0.9999588  0.003491932          
  0.6068464124  0.00442040  0.9999588  0.003491932          
  0.6326535100  0.00442040  0.9999588  0.003491932          
  0.7935149106  0.00442040  0.9999588  0.003491932          
  0.8248124935  0.00442040  0.9999588  0.003491932          
  0.8373193899  0.00442040  0.9999588  0.003491932  *       
  0.8795796616  0.02583105  0.9988888  0.020963402          
  0.9210008914  0.02583105  0.9988888  0.020963402          
  0.9397148718  0.02583105  0.9988888  0.020963402          
  0.9821200476  0.02583105  0.9988888  0.020963402          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was sparsity = 0.8373194.
[1] "Mon Mar 12 13:54:26 2018"
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=8
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
Bayesian Ridge Regression (Model Averaged) 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results:

  RMSE         Rsquared   MAE      
  0.004431534  0.9999591  0.0035042

[1] "Mon Mar 12 13:54:40 2018"
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=8
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=8
t=800, m=8
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
t=100, m=9
t=200, m=9
t=300, m=9
t=400, m=9
t=500, m=9
t=600, m=9
t=700, m=9
t=800, m=9
t=900, m=9
Bayesian Ridge Regression 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results:

  RMSE         Rsquared   MAE        
  0.004427316  0.9999586  0.003491626

[1] "Mon Mar 12 13:54:54 2018"
Boosted Linear Model 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results across tuning parameters:

  nu          mstop  RMSE       Rsquared   MAE        Selected
  0.04320312  112    0.5619610  0.9272480  0.4548812          
  0.04563146  160    0.5494351  0.9091207  0.4445152          
  0.07948347  301    0.4800549  0.8091530  0.3863922          
  0.10245411  469    0.4165970  0.7776400  0.3292883          
  0.15681437  439    0.3798108  0.7750089  0.2940550          
  0.16627783  103    0.5017297  0.8363076  0.4048016          
  0.17377421  210    0.4388446  0.7758462  0.3497568          
  0.23105427  123    0.4588546  0.7828200  0.3680623          
  0.23464907  287    0.3780928  0.7739101  0.2924689          
  0.27796360   50    0.5114191  0.8407053  0.4132818          
  0.27924200  303    0.3567115  0.7725445  0.2695724          
  0.30001122  151    0.4117087  0.7798579  0.3249320          
  0.30241352    1    0.6055667  0.9310591  0.4947694          
  0.30587893  460    0.3290271  0.7620025  0.2355361          
  0.32367745  396    0.3319917  0.7641024  0.2394165          
  0.34016677  316    0.3391591  0.7673843  0.2491493          
  0.42523265  491    0.3216117  0.7577799  0.2217192          
  0.46981954  412    0.3219021  0.7575603  0.2227402          
  0.51138961  260    0.3272456  0.7602429  0.2328601          
  0.53307479  418    0.3210436  0.7567692  0.2201775  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mstop = 418 and nu = 0.5330748.
[1] "Mon Mar 12 13:56:02 2018"
Conditional Inference Random Forest 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE         Selected
  1     0.25008140  0.9296430  0.19102718          
  2     0.09299569  0.9865733  0.06984198          
  3     0.05513929  0.9936987  0.04078431          
  4     0.05154453  0.9941893  0.03756084  *       
  5     0.05312625  0.9935335  0.03849720          
  6     0.05391312  0.9931848  0.03908282          
  8     0.05807965  0.9918540  0.04290702          
  9     0.06068992  0.9910645  0.04513684          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 4.
[1] "Mon Mar 12 13:57:22 2018"
Error in UseMethod("varimp") : 
  no applicable method for 'varimp' applied to an object of class "RandomForest"
In addition: Warning messages:
1: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
2: In train.default(x = data.frame(training[, 2:length(training[1,  :
  missing values found in aggregated results
Conditional Inference Tree 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results across tuning parameters:

  mincriterion  RMSE        Rsquared   MAE         Selected
  0.0009225586  0.08396638  0.9828552  0.06444696          
  0.0999489173  0.08396638  0.9828552  0.06444696          
  0.2058524624  0.08396638  0.9828552  0.06444696          
  0.2241455968  0.08396638  0.9828552  0.06444696          
  0.2445843588  0.08396638  0.9828552  0.06444696          
  0.3019970097  0.08396638  0.9828552  0.06444696          
  0.3201849442  0.08396638  0.9828552  0.06444696          
  0.4204412897  0.08396638  0.9828552  0.06444696          
  0.5192069609  0.08396638  0.9828552  0.06444696          
  0.5740000883  0.08396638  0.9828552  0.06444696          
  0.6027395471  0.08396638  0.9828552  0.06444696          
  0.6068464124  0.08396638  0.9828552  0.06444696          
  0.6326535100  0.08396638  0.9828552  0.06444696          
  0.7935149106  0.08396638  0.9828552  0.06444696  *       
  0.8248124935  0.08420522  0.9827563  0.06467029          
  0.8373193899  0.08432576  0.9827070  0.06483981          
  0.8795796616  0.08442372  0.9826659  0.06493678          
  0.9210008914  0.08442372  0.9826659  0.06493678          
  0.9397148718  0.08453763  0.9826042  0.06495124          
  0.9821200476  0.08524807  0.9823077  0.06576338          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mincriterion = 0.7935149.
[1] "Mon Mar 12 13:57:29 2018"
Conditional Inference Tree 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results across tuning parameters:

  maxdepth  mincriterion  RMSE        Rsquared   MAE         Selected
   1        0.50319452    0.36692357  0.6654735  0.30425027          
   2        0.46237663    0.21202850  0.8884653  0.17317185          
   4        0.07045596    0.10977151  0.9701260  0.08673879          
   4        0.27592293    0.10977151  0.9701260  0.08673879          
   4        0.38406389    0.10977151  0.9701260  0.08673879          
   5        0.07450995    0.08986187  0.9800860  0.07029057          
   5        0.49918401    0.08986187  0.9800860  0.07029057          
   7        0.28843775    0.08396638  0.9828552  0.06444696  *       
   8        0.85206947    0.08442372  0.9826659  0.06493678          
   9        0.39006523    0.08396638  0.9828552  0.06444696          
  10        0.13102416    0.08396638  0.9828552  0.06444696          
  10        0.46451085    0.08396638  0.9828552  0.06444696          
  10        0.56622166    0.08396638  0.9828552  0.06444696          
  12        0.53869357    0.08396638  0.9828552  0.06444696          
  13        0.78267035    0.08396638  0.9828552  0.06444696          
  13        0.88827178    0.08442372  0.9826659  0.06493678          
  14        0.26012416    0.08396638  0.9828552  0.06444696          
  14        0.50897985    0.08396638  0.9828552  0.06444696          
  15        0.16937247    0.08396638  0.9828552  0.06444696          
  15        0.70823481    0.08396638  0.9828552  0.06444696          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were maxdepth = 7 and mincriterion
 = 0.2884378.
[1] "Mon Mar 12 13:57:37 2018"
Cubist 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results across tuning parameters:

  committees  neighbors  RMSE         Rsquared   MAE          Selected
   8          2          0.004958512  0.9999396  0.003602469          
   8          3          0.004875544  0.9999417  0.003543973          
  14          6          0.004844908  0.9999416  0.003478867          
  17          9          0.004782782  0.9999423  0.003401491          
  27          8          0.004511092  0.9999484  0.003228571          
  28          2          0.004563189  0.9999474  0.003327644          
  29          4          0.004458771  0.9999498  0.003211204          
  39          2          0.004571118  0.9999476  0.003293792          
  40          5          0.004469991  0.9999497  0.003176703          
  47          0          0.004741853  0.9999435  0.003357204          
  47          6          0.004466434  0.9999497  0.003183404          
  50          3          0.004475434  0.9999495  0.003232380          
  51          0          0.004708736  0.9999443  0.003352431          
  51          9          0.004453882  0.9999499  0.003167221          
  54          7          0.004439457  0.9999502  0.003170534          
  57          6          0.004439010  0.9999502  0.003184670  *       
  71          9          0.004464552  0.9999495  0.003161784          
  79          8          0.004445897  0.9999499  0.003148815          
  86          5          0.004458629  0.9999499  0.003170221          
  89          8          0.004458910  0.9999497  0.003150997          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were committees = 57 and neighbors = 6.
[1] "Mon Mar 12 13:59:51 2018"
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
Stacked AutoEncoder Deep Neural Network 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results across tuning parameters:

  layer1  layer2  layer3  hidden_dropout  visible_dropout  RMSE       Rsquared 
   2      11      10      0.35867924      0.32537250       0.6598718  0.3781046
   3      10       3      0.25761090      0.14820222       0.7393784  0.5427618
   5       7       3      0.51800374      0.67415701       0.7472916  0.6830140
   6       3       6      0.26279397      0.58053508       0.8678155  0.4638033
   6       9       4      0.58590006      0.60164760       0.8068773  0.3670766
   7      11      14      0.68195822      0.42362335       0.7500028  0.7766673
   8       3      20      0.59478460      0.01308447       2.5482080  0.8345326
   9       7       4      0.09089447      0.62007998       0.9052171  0.7864317
  11      18      16      0.66111893      0.17755437       1.0673157  0.8508426
  12       9       8      0.45766595      0.63677840       0.7789130  0.8508662
  13       4       6      0.37025756      0.29807922       0.7885465  0.6927843
  13      10       5      0.41787982      0.47213931       0.6466416  0.7327360
  14      12       9      0.01498075      0.17359436       0.7443854  0.5480990
  17      12      20      0.15573152      0.36330970       1.1198988  0.6622888
  17      16       9      0.68690185      0.15251903       0.6513167  0.8435874
  17      18       2      0.30336984      0.55079496       0.7106293  0.7349275
  18       6      16      0.59708170      0.69839192       0.9829764  0.6241094
  19       5      10      0.56805677      0.27087291       0.9788345  0.5020572
  19      11       6      0.11987977      0.02134201       0.6607659  0.7951734
  20      15      15      0.06872453      0.69567426       0.7174335  0.7030154
  MAE        Selected
  0.5391793          
  0.6069099          
  0.6173969          
  0.7235070          
  0.6630460          
  0.6212089          
  2.4639131          
  0.7727869          
  0.9105476          
  0.6394609          
  0.6458999          
  0.5330520  *       
  0.6067947          
  0.9806857          
  0.5359705          
  0.5837966          
  0.8316262          
  0.8268845          
  0.5438141          
  0.5914674          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were layer1 = 13, layer2 = 10, layer3 =
 5, hidden_dropout = 0.4178798 and visible_dropout = 0.4721393.
[1] "Mon Mar 12 14:00:02 2018"
Multivariate Adaptive Regression Spline 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results across tuning parameters:

  degree  nprune  RMSE        Rsquared   MAE         Selected
  1       2       0.12789084  0.9600612  0.09943522          
  1       3       0.08863814  0.9803845  0.06395019          
  1       4       0.06179256  0.9906961  0.04777719          
  1       5       0.04847637  0.9942216  0.03103171          
  1       7       0.02264668  0.9987274  0.01726701  *       
  2       2       0.12789084  0.9600612  0.09943522          
  2       5       0.04847637  0.9942216  0.03103171          
  2       6       0.02528782  0.9984160  0.01942580          
  2       7       0.02264668  0.9987274  0.01726701          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nprune = 7 and degree = 1.
[1] "Mon Mar 12 14:00:09 2018"
Extreme Learning Machine 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results across tuning parameters:

  nhid  actfun   RMSE         Rsquared   MAE          Selected
   1    purelin  0.803817314  0.4533399  0.693553071          
   2    radbas   0.545206005  0.4069236  0.431542577          
   4    radbas   0.322204019  0.7111490  0.250657022          
   5    radbas   0.421704111  0.5515890  0.321196420          
   5    sin      0.118622139  0.9584752  0.087358890          
   6    radbas   0.224104482  0.8493449  0.173945057          
   7    sin      0.115501768  0.9669967  0.088185727          
   8    radbas   0.189053381  0.8896212  0.145009874          
  10    tansig   0.048421009  0.9940594  0.035849945          
  11    radbas   0.073204481  0.9857904  0.054089544          
  12    radbas   0.084292712  0.9803370  0.063884099          
  12    sin      0.025816378  0.9980189  0.019396972          
  13    purelin  0.004094970  0.9999589  0.003260748  *       
  16    purelin  0.004094970  0.9999589  0.003260748          
  16    tansig   0.015290280  0.9994299  0.011609183          
  17    radbas   0.028904978  0.9975503  0.021397714          
  18    purelin  0.004094970  0.9999589  0.003260748          
  18    sin      0.008980402  0.9997972  0.006536256          
  19    purelin  0.004094970  0.9999589  0.003260748          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nhid = 13 and actfun = purelin.
[1] "Mon Mar 12 14:00:15 2018"
Elasticnet 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results across tuning parameters:

  lambda        fraction    RMSE        Rsquared   MAE          Selected
  1.012827e-05  0.50319452  0.00853676  0.9998181  0.006040542  *       
  3.978263e-05  0.46237663  0.01076479  0.9997099  0.007583103          
  1.718362e-04  0.27592293  0.28024147  0.9762205  0.229801731          
  2.212451e-04  0.07045596  0.54853552  0.9711847  0.451116175          
  2.934311e-04  0.38406389  0.20875137  0.9870534  0.171457004          
  6.486076e-04  0.49918401  0.19002391  0.9889334  0.156086368          
  8.338917e-04  0.07450995  0.57105643  0.9711847  0.469767315          
  3.331561e-03  0.28843775  0.41676138  0.9711847  0.342113420          
  1.303892e-02  0.85206947  0.03027274  0.9981857  0.022769216          
  2.779717e-02  0.39006523  0.33353270  0.9717559  0.273398174          
  4.134636e-02  0.13102416  0.53104755  0.9711847  0.436625043          
  4.376012e-02  0.46451085  0.27707928  0.9861916  0.226962905          
  6.250591e-02  0.56622166  0.19962939  0.9925012  0.163136560          
  5.768853e-01  0.53869357  0.10985614  0.9909481  0.087744275          
  8.889451e-01  0.78267035  0.22359824  0.9912018  0.180202649          
  1.056614e+00  0.88827178  0.37224429  0.9844260  0.301627119          
  1.894427e+00  0.26012416  0.26187373  0.9837124  0.213806948          
  3.357417e+00  0.50897985  0.26691270  0.9895848  0.216985380          
  4.347997e+00  0.16937247  0.29811891  0.9812624  0.243706807          
  7.811245e+00  0.70823481  1.00631577  0.9589470  0.818036679          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were fraction = 0.5031945 and lambda
 = 1.012827e-05.
[1] "Mon Mar 12 14:00:21 2018"
Tree Models from Genetic Algorithms 

751 samples
  9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 500, 501 
Resampling results across tuning parameters:

  alpha     RMSE       Rsquared   MAE         Selected
  1.003690  0.1001743  0.9755545  0.07953324  *       
  1.399796  0.1140261  0.9680606  0.08994055          
  1.823410  0.1232284  0.9624947  0.09737707          
  1.896582  0.1240265  0.9619892  0.09876407          
  1.978337  0.1322677  0.9567094  0.10601903          
  2.207988  0.1465765  0.9470200  0.11787833          
  2.280740  0.1494609  0.9449370  0.11978244          
  2.681765  0.1494609  0.9449370  0.11978244          
  3.076828  0.1494609  0.9449370  0.11978244          
  3.296000  0.1561558  0.9394309  0.12621137          
  3.410958  0.1580085  0.9389969  0.12713683          
  3.427386  0.1560814  0.9399052  0.12612880          
  3.530614  0.1646573  0.9330402  0.13253675          
  4.174060  0.1712097  0.9285958  0.13700615          
  4.299250  0.1792856  0.9216740  0.14428093          
  4.349278  0.1792856  0.9216740  0.14428093          
  4.518319  0.1792856  0.9216740  0.14428093          
  4.684004  0.1854316  0.9155017  0.15002149          
  4.758859  0.1919448  0.9088937  0.15446259          
  4.928480  0.1792856  0.9216740  0.14428093          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was alpha = 1.00369.
[1] "Mon Mar 12 14:45:52 2018"


R version 3.4.3 (2017-11-30) -- "Kite-Eating Tree"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> options(repos=structure(c(CRAN="https://rweb.crmda.ku.edu/cran/")))
> ## capture messages and errors to a file.https://rweb.crmda.ku.edu/cran/
> #zz <- file("all.Rout", open="wt")https://cran.cnr.berkeley.edu
> #sink(zz, type="message") edit for rebaseless
> #chek for R package updates
> #try(log("a")) ## test --no-edit
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> task.subject<-"14th20hp3cv"
> pc.mlr<-c("ACE")#"ALTA","HOPPER"
> which.computer<-Sys.info()[['nodename']]
> out.file<-paste("out",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,".csv",sep="")
> importance.file<-paste("importance",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,sep="")
> 
> base.folder<-getwd()
> cpout.folder<-paste(base.folder,"/",which.computer,sep = "")
> setwd(cpout.folder)
> 
> if(length(which(list.files() == out.file))<1) write.table( "0.01,0.01,100,100,100,Wed Aug 02 16:37:25 2017,dummy,8,1,basic latent features,ignore,none,asis,1.12784979099243,random,333,53,adaptive_cv,16,5,2,2,19,0.0107744822639878,FALSE,,,,,,,,,," ,file =,out.file,  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,".csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,".csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,"mlr.csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,"mlr.csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> 
> cv.iters=3
> tuneLength=20
> tuneLength2=8
> normings=c("YeoJohnson","ICA", "centernscale","expoTrans","range01","asis","quantile")#,"centernscale"
> 
> gensTTesto<-c(56,53,4,12,13,14,15,20,45,54,55, 44,3,1,52)#,  51,c(4)#c(1:40)#c(5,10,11,13,14,15,16,17,18,19,20,21,24,28,38,39,40)
> gensTTest<-vector()
> write.table( t(gensTTesto),file = "initial tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
> try({
+   gensTTest<-t(read.csv("tasks to test.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   gensTTest<-as.vector(gensTTest)
+ })
> if(!exists("gensTTest")) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> gensTTesto<-c(gensTTesto[length(gensTTesto):1])
> if(length(gensTTest)<1) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> 
> 
> ########packages install check######
> 
> #list.of.packages <- c("caret","caretEnsemble","mlr","MLmetrics","tgp")
> #list.of.packages <- c("gower","dimRed","DEoptimR","caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> #new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> #if(length(new.packages)) install.packages(new.packages, dep = TRUE)
> 
> 
> #install.packages("mlr", dependencies = c("Depends", "Suggests"))
> #install.packages("caret", dependencies = c("Depends", "Suggests"))
> #install.packages("caret",repos = "http://cran.r-project.org",dependencies = c("Depends", "Imports", "Suggests"))
> #install.packages("SuperLearner", dependencies = c("Depends", "Suggests"))
> #install.packages("rattle", dependencies = c("Depends", "Suggests"))
> 
> # Load libraries
> #library(mlbench)
> 
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> #library(caretEnsemble)
> library(MLmetrics)

Attaching package: 'MLmetrics'

The following objects are masked from 'package:caret':

    MAE, RMSE

The following object is masked from 'package:base':

    Recall

> 
> ########error no repeat#########
> 
> 
> try({
+   before.last.alg<-as.matrix(read.csv("beforelast algorithm.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.alg<-as.matrix(read.csv("last algorithm tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   #write.table(paste(date(), last.alg,.Platform$OS.type,.Platform$r_arch,which.computer,sep=" "),file = "algos after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.alg==before.last.alg){print("algorithm may be broken")}
+   write.table(last.alg,file = "beforelast algorithm.csv",  quote = F, row.names = F,col.names = F)
+ })
> try({
+   before.last.tsk<-as.matrix(read.csv("beforelast task.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.tsk<-as.matrix(read.csv("last task tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   write.table(paste(date(),last.alg, last.tsk,cv.iters,tuneLength,.Platform$OS.type,.Platform$r_arch,which.computer,sep=","),file = "test after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.tsk==before.last.tsk){print("task may be broken")}
+   write.table(last.tsk,file = "beforelast task.csv",  quote = F, row.names = F,col.names = F)
+ })
> bad.models=c("spaccceeee")
> previous.fails<-(read.csv("test after which reset.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
> previous.fails<-previous.fails[previous.fails[,8]==which.computer,]
> lgf<-length(previous.fails[,2])
> for(lt in 2:lgf)  {
+   if(previous.fails[lt,2]==previous.fails[lt-1,2])  {
+     bad.models=union(bad.models,c(paste(previous.fails[lt,2])))  }}
> 
> #######not to redo a test function#####
> check.redundant<-function(df=df.previous.calcs,norming="asis",trans.y=1,withextra="missing",missingdata="leaveempty",datasource="mean" ,column.to.predict=200,allmodel="ctree")
+ {
+   for(intern in 1:length(df[,1])){
+     if((any(df[intern,] == norming, na.rm=T))&&
+        (any(df[intern,] == withextra, na.rm=T))&&
+        (any(df[intern,] == missingdata, na.rm=T))&&
+        (any(df[intern,] == datasource, na.rm=T))&&
+        (any(df[intern,] == column.to.predict, na.rm=T))&&
+        (any(df[intern,] == allmodel, na.rm=T))&&
+        (  (df[intern,9] == trans.y)))
+     {return(TRUE)}
+   }
+   return(FALSE)
+ }
> #####caret init#####
> best.ranged <- c("avNNet", "nnet", "pcaNNet", "glm.nb")
> best.asis <- c("svmLinear3", "relaxo", "superpc", "xgbTree")
> best.cns <- c("gam", "bam", "svmLinear2", "msaenet", "BstLm", "gbm") 
> 
> cv6hp5 <- c( "BstLm", "qrnn")#earth
> cv3hp32 <- c("Rborist", "pcaNNet", "SBC")
> cv7x5hp32 <- c("gbm", "krlsPoly", "kknn", "xgbLinear","RRF", "cubist", "rlm" )
> cv6hp5.avoid <- c("pcaNNet")
> cv3hp32.avoid <- c("glm.nb", "gamboost", "ctree2","glmboost", "leapSeq","ctree","svmLinear2")
> cv7x5hp32.avoid <- c("SBC","bagearthgcv","gcvearth","lmStepAIC","glmStepAIC","bridge","lm","glm","bayesglm","blassoAveraged","treebag","rpart1SE")
> 
> allmodels <- c("avNNet", "bagEarth", "bagEarthGCV",
+                "bayesglm", "bdk", "blackboost", "Boruta", "brnn", "BstLm" ,
+                "bstTree", "cforest", "ctree", "ctree2", "cubist", "DENFIS",
+                "dnn", "earth", "elm", "enet",   "evtree",
+                "extraTrees",  "gamLoess",  "gaussprLinear", "gaussprPoly", "gaussprRadial",
+                "gcvEarth","glm", "glmboost",  "icr", "kernelpls",
+                "kknn", "knn",  "krlsRadial", "lars" , "lasso",
+                "leapBackward", "leapForward", "leapSeq", "lm", "M5", "M5Rules",
+                "mlpWeightDecay", "neuralnet" , "partDSA",
+                "pcaNNet", "pcr", "penalized", "pls", "plsRglm", "ppr",
+                "qrf" , "ranger",  "rf")
> allmodels <- c("rlm", "rpart", "rpart2",
+                "RRF", "RRFglobal",  "simpls",
+                "svmLinear", "svmPoly", "svmRadial", "svmRadialCost",
+                "widekernelpls",  "xgbLinear",
+                "xgbTree")
> allmodels <- c("avNNet","BstLm","bstTree","cforest","ctree","ctree2",
+                "cubist","earth","enet","evtree","glmboost",
+                "icr","kernelpls","kknn","lasso","pcaNNet",
+                "pcr","pls","qrf","ranger","rf")
> 
> allmodels <- c("kknn", "cubist", "avNNet", "xgbLinear", "RRF", "pcaNNet","earth","nnet","gbm","enet","lasso","BstLm",
+                "foba", "leapBackward", "gcvEarth", "SBC","glm.nb","gamboost","ctree2","relaxo", 
+                "bartMachine","extraTrees","bam","gam","randomGLM")
> #allmodels <- c("bam")
> #allmodels <- c("rf")"rqlasso",, "xyf" "rvmPoly", "rvmRadial",    "spls", "superpc" ,   "treebag",  "svmLinear2",  "SBC",
> #allmodels <- c("bartMachine", "xgbLinear", "pcaNNet","svmLinear","glmnet","cforest","cubist","rf","ranger")"glmnet",
> #wow rfRules is really slow "rfRules","WM", takes 50min
> # brak everythig "rbfDDA","ridge","rqnc",
> # use "rf" to test all
> library(caret)
> allmodels <- unique(modelLookup()[modelLookup()$forReg,c(1)])
> #allmodels <-c("avNNet", "nnet", "pcaNNet",  "glm.nb", "gam" ,
> #              "bam","msaenet", "svmLinear2","svmLinear3",
> #              "relaxo",  "superpc", "xgbTree", "BstLm")
> #allmodels<- c("svmLinear","svmPoly","svmRadial")
> #library(doParallel); cl <- makeCluster(detectCores()); registerDoParallel(cl)
> #allmodels<-c("bartMachine","extraTrees")#,"randomGLM"
> 
> 
> adaptControl <- trainControl(method = "adaptive_cv",
+                              number = 7, repeats = 5,
+                              adaptive = list(min = 4, alpha = 0.05,
+                                              method = "gls", complete = FALSE),
+                              search = "random")
> adaptControl <-trainControl(method = "cv", number = cv.iters,  search = "random")
> simpleControl <- trainControl(method = "cv",
+                               number = cv.iters,
+                               search = "random")
> 
> 
> #########MLR init######
> #R.utils::gcDLLs()
> #list.of.packages <- c("ParamHelpers","devtools","mlrMBO","RJSONIO","plot3D","plotly")
> #install.packages("mlrMBO", dependencies = c("Depends", "Suggests"))
> list.of.packages <- c("caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> if(length(new.packages)) install.packages(new.packages, dep = TRUE)
Warning message:
packages 'logicFS', ' RWeka', 'rPython', 'rrf' are not available (for R version 3.4.3) 
> 
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> tuneLengthMLR<-tuneLength
> mlr.iters<-cv.iters
> #######data read process start#####
> seed.var =222+round(runif(1,min=0,max=100))
> column.to.predict=1
> print(date());
[1] "Fri Mar 09 22:16:48 2018"
> 
> setwd(base.folder)
> if(!exists("gen.count")){gen.count=56}
> gens.names<-as.matrix(read.table("gens names.csv", sep = ",",header = FALSE,row.names=1,fill=TRUE, quote="",dec="."))
> count.toy.data.passed<-1
> for(gend.data in gensTTest){
+   count.toy.data.passed<-count.toy.data.passed+1
+   setwd(base.folder)
+   data.source<-as.matrix(read.csv(paste("Generats/",gens.names[gend.data],".csv", sep = ""), sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   datasource<-gens.names[gend.data,1]
+   setwd(cpout.folder)
+   missingdatas=c("ignore")
+   for(missingdata in missingdatas){
+     withextras=c("none")
+     for(withextra in withextras){
+       ################data wrestling###############
+       
+       dependant.selection=complete.cases(data.source[,column.to.predict])
+       df.previous.calcs=as.data.frame(read.csv(file=out.file, header = FALSE, sep = ",", quote = "",
+                                                dec = ".", fill = TRUE, comment.char = ""))
+       unimportant.computations<-vector(mode = "logical",length=length(df.previous.calcs[,1])  )
+       for(intern in 1:length(df.previous.calcs[,1])){
+         if((any(df.previous.calcs[intern,] == withextra, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == missingdata, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == datasource, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == column.to.predict, na.rm=T)))
+         {unimportant.computations[intern]<-T}}
+       
+       df.previous.calcs<-df.previous.calcs[unimportant.computations,]
+       
+       #data.source=data.frame( data.source[,column.to.predict],data.source[,1:2], data.source[,4:(column.to.predict-1)], data.source[,(column.to.predict+1):length( data.source[1,])])
+       
+         for(norming in normings) {
+         for(trans.y in 1:2) {
+           df.toprocess=data.source
+           y.untransformed<-df.toprocess[,1]
+           
+           if(norming=="centernscale"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("center", "scale"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="range01"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="expoTrans"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("expoTrans"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="YeoJohnson"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("YeoJohnson"))#"center", "scale",
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           if((norming=="asis")&&(trans.y==2)){next}
+           
+           
+           ################preprocess###########
+           df.toprocess=data.frame(df.toprocess[dependant.selection,])
+           y.untransformed=y.untransformed[dependant.selection]
+           if(norming=="quantile"){
+             for(Clol in trans.y:length(data.source[1,])){
+               df.toprocess[,Clol]<- (rank(df.toprocess[,Clol],na.last = "keep",ties.method = "average")-1) }
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           loess.model<-loess(y.untransformed~ df.toprocess[,1],span = 0.21, degree = 1)
+   
+           #df.toprocess = data.frame(df.toprocess,)
+           nzv <- nearZeroVar(df.toprocess[,])#, saveMetrics= TRUE
+           #nzv[nzv$nzv,][1:10,]
+           if(length(nzv)>1){
+             df.toprocess = (df.toprocess[, -nzv])}
+           df.toprocess = signif(df.toprocess,digits = 3)
+           
+           seed.var =222+round(runif(1,min=0,max=100))
+           set.seed(seed.var)
+           inTrain <- createDataPartition(y = df.toprocess[,1],
+                                          p = .75,
+                                          list = FALSE)
+           training <- df.toprocess[ inTrain,]
+           testing  <- df.toprocess[-inTrain,]
+           write.table(df.toprocess,file = "sanity check 1.csv",  quote = F, row.names = F,col.names = F)
+           
+           ###########for all models#################
+           setwd(base.folder)
+           if(max(which.computer==pc.mlr)>0)
+             source("MLR part.R")
+           else
+             source("Caret part.R")
+           
+          setwd(cpout.folder)
+           if(norming == normings[length(normings)]){
+             if(count.toy.data.passed>length(gensTTest)){gensTTest<-c(gensTTesto)}
+             write.table( t(gensTTest[count.toy.data.passed:length(gensTTest)]),file = "tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
+             
+             }
+           
+         }
+       }
+     }
+   }
+   
+ }
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  These variables have zero variances: V11, V12
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  These variables have zero variances: V11, V12
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  No variation for for: V11, V12
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  No variation for for: V11, V12
bartMachine initializing with 19 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 15.1/477.6MB
Iteration 200/1250  mem: 12.4/477.6MB
Iteration 300/1250  mem: 6.5/477.6MB
Iteration 400/1250  mem: 16.4/477.6MB
Iteration 500/1250  mem: 12/477.6MB
Iteration 600/1250  mem: 7.3/477.6MB
Iteration 700/1250  mem: 17.3/477.6MB
Iteration 800/1250  mem: 26.7/477.6MB
Iteration 900/1250  mem: 7.7/477.6MB
Iteration 1000/1250  mem: 17.2/477.6MB
Iteration 1100/1250  mem: 26.8/477.6MB
Iteration 1200/1250  mem: 36.3/477.6MB
done building BART in 1.046 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 79 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 62.8/477.6MB
Iteration 200/1250  mem: 50.3/477.6MB
Iteration 300/1250  mem: 35.2/477.6MB
Iteration 400/1250  mem: 81.3/477.6MB
Iteration 500/1250  mem: 129.7/477.6MB
Iteration 600/1250  mem: 67.9/477.6MB
Iteration 700/1250  mem: 117.1/477.6MB
Iteration 800/1250  mem: 60.3/477.6MB
Iteration 900/1250  mem: 107.7/477.6MB
Iteration 1000/1250  mem: 155.1/477.6MB
Iteration 1100/1250  mem: 76.6/477.6MB
Iteration 1200/1250  mem: 121.5/477.6MB
done building BART in 1.538 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 30 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 163.3/477.6MB
Iteration 200/1250  mem: 180.4/477.6MB
Iteration 300/1250  mem: 194.6/477.6MB
Iteration 400/1250  mem: 72.3/477.6MB
Iteration 500/1250  mem: 87.1/477.6MB
Iteration 600/1250  mem: 103.7/477.6MB
Iteration 700/1250  mem: 118.5/477.6MB
Iteration 800/1250  mem: 133.3/477.6MB
Iteration 900/1250  mem: 149.9/477.6MB
Iteration 1000/1250  mem: 164.7/477.6MB
Iteration 1100/1250  mem: 179.5/477.6MB
Iteration 1200/1250  mem: 194.3/477.6MB
done building BART in 0.504 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 81 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 128/477.6MB
Iteration 200/1250  mem: 171.1/477.6MB
Iteration 300/1250  mem: 84.2/477.6MB
Iteration 400/1250  mem: 128.9/477.6MB
Iteration 500/1250  mem: 173.6/477.6MB
Iteration 600/1250  mem: 95/477.6MB
Iteration 700/1250  mem: 140.2/477.6MB
Iteration 800/1250  mem: 182.8/477.6MB
Iteration 900/1250  mem: 225.5/477.6MB
Iteration 1000/1250  mem: 88.7/477.6MB
Iteration 1100/1250  mem: 133.9/477.6MB
Iteration 1200/1250  mem: 176.4/477.6MB
done building BART in 1.089 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 70 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 121.5/477.6MB
Iteration 200/1250  mem: 156.5/477.6MB
Iteration 300/1250  mem: 192.4/477.6MB
Iteration 400/1250  mem: 100.2/477.6MB
Iteration 500/1250  mem: 136.9/477.6MB
Iteration 600/1250  mem: 173.7/477.6MB
Iteration 700/1250  mem: 84.9/477.6MB
Iteration 800/1250  mem: 120.4/477.6MB
Iteration 900/1250  mem: 155.8/477.6MB
Iteration 1000/1250  mem: 193.8/477.6MB
Iteration 1100/1250  mem: 104.6/477.6MB
Iteration 1200/1250  mem: 141.1/477.6MB
done building BART in 0.775 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 22 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 174.5/477.6MB
Iteration 200/1250  mem: 185.4/477.6MB
Iteration 300/1250  mem: 196.2/477.6MB
Iteration 400/1250  mem: 207/477.6MB
Iteration 500/1250  mem: 31.9/477.6MB
Iteration 600/1250  mem: 43.3/477.6MB
Iteration 700/1250  mem: 54.8/477.6MB
Iteration 800/1250  mem: 66.2/477.6MB
Iteration 900/1250  mem: 77.6/477.6MB
Iteration 1000/1250  mem: 89.1/477.6MB
Iteration 1100/1250  mem: 98.6/477.6MB
Iteration 1200/1250  mem: 110.1/477.6MB
done building BART in 0.32 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 60 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 155.1/477.6MB
Iteration 200/1250  mem: 67.4/477.6MB
Iteration 300/1250  mem: 106.4/477.6MB
Iteration 400/1250  mem: 143.1/477.6MB
Iteration 500/1250  mem: 55/477.6MB
Iteration 600/1250  mem: 93.2/477.6MB
Iteration 700/1250  mem: 129.7/477.6MB
Iteration 800/1250  mem: 168/477.6MB
Iteration 900/1250  mem: 86/477.6MB
Iteration 1000/1250  mem: 124.3/477.6MB
Iteration 1100/1250  mem: 160.7/477.6MB
Iteration 1200/1250  mem: 63.2/477.6MB
done building BART in 1.039 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 82 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 131.7/477.6MB
Iteration 200/1250  mem: 174.7/477.6MB
Iteration 300/1250  mem: 97.7/477.6MB
Iteration 400/1250  mem: 143.1/477.6MB
Iteration 500/1250  mem: 186.3/477.6MB
Iteration 600/1250  mem: 113/477.6MB
Iteration 700/1250  mem: 155.3/477.6MB
Iteration 800/1250  mem: 200/477.6MB
Iteration 900/1250  mem: 128.9/477.6MB
Iteration 1000/1250  mem: 172.5/477.6MB
Iteration 1100/1250  mem: 100.4/477.6MB
Iteration 1200/1250  mem: 144.6/477.6MB
done building BART in 0.973 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 66 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 205.9/477.6MB
Iteration 200/1250  mem: 127.2/477.6MB
Iteration 300/1250  mem: 166.3/477.6MB
Iteration 400/1250  mem: 205.4/477.6MB
Iteration 500/1250  mem: 129.2/477.6MB
Iteration 600/1250  mem: 169.3/477.6MB
Iteration 700/1250  mem: 211.6/477.6MB
Iteration 800/1250  mem: 80.5/477.6MB
Iteration 900/1250  mem: 123.9/477.6MB
Iteration 1000/1250  mem: 167.4/477.6MB
Iteration 1100/1250  mem: 108.2/477.6MB
Iteration 1200/1250  mem: 153.7/477.6MB
done building BART in 1.131 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 65 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 215.2/477.6MB
Iteration 200/1250  mem: 139.5/477.6MB
Iteration 300/1250  mem: 174.7/477.6MB
Iteration 400/1250  mem: 211.6/477.6MB
Iteration 500/1250  mem: 136.7/477.6MB
Iteration 600/1250  mem: 171.9/477.6MB
Iteration 700/1250  mem: 209/477.6MB
Iteration 800/1250  mem: 244.1/477.6MB
Iteration 900/1250  mem: 174.8/477.6MB
Iteration 1000/1250  mem: 211.4/477.6MB
Iteration 1100/1250  mem: 250.1/477.6MB
Iteration 1200/1250  mem: 139.1/477.6MB
done building BART in 1.146 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 25 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 175.7/477.6MB
Iteration 200/1250  mem: 188.2/477.6MB
Iteration 300/1250  mem: 200.6/477.6MB
Iteration 400/1250  mem: 213/477.6MB
Iteration 500/1250  mem: 225.4/477.6MB
Iteration 600/1250  mem: 237.9/477.6MB
Iteration 700/1250  mem: 135.2/477.6MB
Iteration 800/1250  mem: 147.6/477.6MB
Iteration 900/1250  mem: 160/477.6MB
Iteration 1000/1250  mem: 172.3/477.6MB
Iteration 1100/1250  mem: 184.7/477.6MB
Iteration 1200/1250  mem: 197/477.6MB
done building BART in 0.265 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 50 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 231.9/477.6MB
Iteration 200/1250  mem: 139.9/477.6MB
Iteration 300/1250  mem: 164.2/477.6MB
Iteration 400/1250  mem: 190.2/477.6MB
Iteration 500/1250  mem: 216.2/477.6MB
Iteration 600/1250  mem: 242.2/477.6MB
Iteration 700/1250  mem: 148.4/477.6MB
Iteration 800/1250  mem: 174.8/477.6MB
Iteration 900/1250  mem: 199.1/477.6MB
Iteration 1000/1250  mem: 225.5/477.6MB
Iteration 1100/1250  mem: 249.9/477.6MB
Iteration 1200/1250  mem: 157.4/477.6MB
done building BART in 0.576 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 48 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 199.8/477.6MB
Iteration 200/1250  mem: 225.2/477.6MB
Iteration 300/1250  mem: 248/477.6MB
Iteration 400/1250  mem: 273.4/477.6MB
Iteration 500/1250  mem: 175.9/477.6MB
Iteration 600/1250  mem: 201.4/477.6MB
Iteration 700/1250  mem: 224.7/477.6MB
Iteration 800/1250  mem: 247.9/477.6MB
Iteration 900/1250  mem: 273.4/477.6MB
Iteration 1000/1250  mem: 178.7/477.6MB
Iteration 1100/1250  mem: 202.7/477.6MB
Iteration 1200/1250  mem: 226.8/477.6MB
done building BART in 0.521 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 41 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 263.1/477.6MB
Iteration 200/1250  mem: 286.1/477.6MB
Iteration 300/1250  mem: 185.2/477.6MB
Iteration 400/1250  mem: 204.9/477.6MB
Iteration 500/1250  mem: 226.4/477.6MB
Iteration 600/1250  mem: 248/477.6MB
Iteration 700/1250  mem: 269.5/477.6MB
Iteration 800/1250  mem: 289.2/477.6MB
Iteration 900/1250  mem: 191.8/477.6MB
Iteration 1000/1250  mem: 212.4/477.6MB
Iteration 1100/1250  mem: 235.1/477.6MB
Iteration 1200/1250  mem: 255.6/477.6MB
done building BART in 0.441 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 89 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 197.8/477.6MB
Iteration 200/1250  mem: 241.9/477.6MB
Iteration 300/1250  mem: 286/477.6MB
Iteration 400/1250  mem: 218.1/477.6MB
Iteration 500/1250  mem: 261.3/477.6MB
Iteration 600/1250  mem: 306.2/477.6MB
Iteration 700/1250  mem: 243.6/477.6MB
Iteration 800/1250  mem: 289/477.6MB
Iteration 900/1250  mem: 61.1/477.6MB
Iteration 1000/1250  mem: 107.2/477.6MB
Iteration 1100/1250  mem: 49.8/477.6MB
Iteration 1200/1250  mem: 95.7/477.6MB
done building BART in 1.159 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 39 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 142.2/477.6MB
Iteration 200/1250  mem: 58.9/477.6MB
Iteration 300/1250  mem: 78.2/477.6MB
Iteration 400/1250  mem: 98.9/477.6MB
Iteration 500/1250  mem: 118.1/477.6MB
Iteration 600/1250  mem: 138.8/477.6MB
Iteration 700/1250  mem: 159.5/477.6MB
Iteration 800/1250  mem: 73.9/477.6MB
Iteration 900/1250  mem: 93.6/477.6MB
Iteration 1000/1250  mem: 113.2/477.6MB
Iteration 1100/1250  mem: 134.7/477.6MB
Iteration 1200/1250  mem: 154.4/477.6MB
done building BART in 0.41 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 69 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 97.4/477.6MB
Iteration 200/1250  mem: 132.1/477.6MB
Iteration 300/1250  mem: 167.9/477.6MB
Iteration 400/1250  mem: 91.2/477.6MB
Iteration 500/1250  mem: 125.4/477.6MB
Iteration 600/1250  mem: 159.5/477.6MB
Iteration 700/1250  mem: 87.1/477.6MB
Iteration 800/1250  mem: 122.6/477.6MB
Iteration 900/1250  mem: 156.2/477.6MB
Iteration 1000/1250  mem: 191.7/477.6MB
Iteration 1100/1250  mem: 118.9/477.6MB
Iteration 1200/1250  mem: 154.3/477.6MB
done building BART in 0.735 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 87 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 111.8/477.6MB
Iteration 200/1250  mem: 159.4/477.6MB
Iteration 300/1250  mem: 203.9/477.6MB
Iteration 400/1250  mem: 133.6/477.6MB
Iteration 500/1250  mem: 180.8/477.6MB
Iteration 600/1250  mem: 111.3/477.6MB
Iteration 700/1250  mem: 157.2/477.6MB
Iteration 800/1250  mem: 203.1/477.6MB
Iteration 900/1250  mem: 137.6/477.6MB
Iteration 1000/1250  mem: 180.8/477.6MB
Iteration 1100/1250  mem: 226.2/477.6MB
Iteration 1200/1250  mem: 85.8/477.6MB
done building BART in 1.132 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 58 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 143.5/477.6MB
Iteration 200/1250  mem: 173.7/477.6MB
Iteration 300/1250  mem: 83.2/477.6MB
Iteration 400/1250  mem: 114.6/477.6MB
Iteration 500/1250  mem: 143.9/477.6MB
Iteration 600/1250  mem: 173.2/477.6MB
Iteration 700/1250  mem: 81.8/477.6MB
Iteration 800/1250  mem: 111.3/477.6MB
Iteration 900/1250  mem: 143/477.6MB
Iteration 1000/1250  mem: 172.5/477.6MB
Iteration 1100/1250  mem: 201.9/477.6MB
Iteration 1200/1250  mem: 110.7/477.6MB
done building BART in 0.604 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 32 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 145.7/477.6MB
Iteration 200/1250  mem: 164.6/477.6MB
Iteration 300/1250  mem: 180.7/477.6MB
Iteration 400/1250  mem: 199.6/477.6MB
Iteration 500/1250  mem: 218.5/477.6MB
Iteration 600/1250  mem: 114.7/477.6MB
Iteration 700/1250  mem: 135.3/477.6MB
Iteration 800/1250  mem: 155.8/477.6MB
Iteration 900/1250  mem: 176.4/477.6MB
Iteration 1000/1250  mem: 194.7/477.6MB
Iteration 1100/1250  mem: 217.6/477.6MB
Iteration 1200/1250  mem: 125.5/477.6MB
done building BART in 0.427 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 19 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 150.6/477.6MB
Iteration 200/1250  mem: 160.8/477.6MB
Iteration 300/1250  mem: 168.3/477.6MB
Iteration 400/1250  mem: 178.4/477.6MB
Iteration 500/1250  mem: 188.6/477.6MB
Iteration 600/1250  mem: 198.7/477.6MB
Iteration 700/1250  mem: 208.8/477.6MB
Iteration 800/1250  mem: 216.4/477.6MB
Iteration 900/1250  mem: 226.5/477.6MB
Iteration 1000/1250  mem: 236.6/477.6MB
Iteration 1100/1250  mem: 128.6/477.6MB
Iteration 1200/1250  mem: 137.9/477.6MB
done building BART in 0.228 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 79 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 192.7/477.6MB
Iteration 200/1250  mem: 238.2/477.6MB
Iteration 300/1250  mem: 161.5/477.6MB
Iteration 400/1250  mem: 207.7/477.6MB
Iteration 500/1250  mem: 254/477.6MB
Iteration 600/1250  mem: 186.4/477.6MB
Iteration 700/1250  mem: 232.6/477.6MB
Iteration 800/1250  mem: 45.7/477.6MB
Iteration 900/1250  mem: 92.1/477.6MB
Iteration 1000/1250  mem: 138.6/477.6MB
Iteration 1100/1250  mem: 73.5/477.6MB
Iteration 1200/1250  mem: 120.3/477.6MB
done building BART in 1.089 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 30 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 163/477.6MB
Iteration 200/1250  mem: 178.4/477.6MB
Iteration 300/1250  mem: 76.6/477.6MB
Iteration 400/1250  mem: 91.6/477.6MB
Iteration 500/1250  mem: 106.6/477.6MB
Iteration 600/1250  mem: 123.3/477.6MB
Iteration 700/1250  mem: 138.3/477.6MB
Iteration 800/1250  mem: 153.3/477.6MB
Iteration 900/1250  mem: 168.3/477.6MB
Iteration 1000/1250  mem: 185/477.6MB
Iteration 1100/1250  mem: 79.2/477.6MB
Iteration 1200/1250  mem: 95/477.6MB
done building BART in 0.346 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 81 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 150.3/477.6MB
Iteration 200/1250  mem: 196.6/477.6MB
Iteration 300/1250  mem: 118.2/477.6MB
Iteration 400/1250  mem: 162.4/477.6MB
Iteration 500/1250  mem: 206.6/477.6MB
Iteration 600/1250  mem: 131.2/477.6MB
Iteration 700/1250  mem: 174.9/477.6MB
Iteration 800/1250  mem: 218.6/477.6MB
Iteration 900/1250  mem: 145.7/477.6MB
Iteration 1000/1250  mem: 188.8/477.6MB
Iteration 1100/1250  mem: 231.9/477.6MB
Iteration 1200/1250  mem: 156.8/477.6MB
done building BART in 0.985 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 70 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 221.3/477.6MB
Iteration 200/1250  mem: 132.6/477.6MB
Iteration 300/1250  mem: 169.5/477.6MB
Iteration 400/1250  mem: 204.3/477.6MB
Iteration 500/1250  mem: 241.1/477.6MB
Iteration 600/1250  mem: 72.3/477.6MB
Iteration 700/1250  mem: 108.7/477.6MB
Iteration 800/1250  mem: 145/477.6MB
Iteration 900/1250  mem: 181.3/477.6MB
Iteration 1000/1250  mem: 92.4/477.6MB
Iteration 1100/1250  mem: 129/477.6MB
Iteration 1200/1250  mem: 165.7/477.6MB
done building BART in 0.934 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 22 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 200.1/477.6MB
Iteration 200/1250  mem: 83.5/477.6MB
Iteration 300/1250  mem: 95.5/477.6MB
Iteration 400/1250  mem: 107.5/477.6MB
Iteration 500/1250  mem: 118.1/477.6MB
Iteration 600/1250  mem: 130.1/477.6MB
Iteration 700/1250  mem: 140.7/477.6MB
Iteration 800/1250  mem: 152.7/477.6MB
Iteration 900/1250  mem: 163.3/477.6MB
Iteration 1000/1250  mem: 175.3/477.6MB
Iteration 1100/1250  mem: 185.8/477.6MB
Iteration 1200/1250  mem: 197.9/477.6MB
done building BART in 0.237 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 60 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 116.3/477.6MB
Iteration 200/1250  mem: 154/477.6MB
Iteration 300/1250  mem: 191.8/477.6MB
Iteration 400/1250  mem: 100/477.6MB
Iteration 500/1250  mem: 138.2/477.6MB
Iteration 600/1250  mem: 176.5/477.6MB
Iteration 700/1250  mem: 214.7/477.6MB
Iteration 800/1250  mem: 134.8/477.6MB
Iteration 900/1250  mem: 174.1/477.6MB
Iteration 1000/1250  mem: 211.4/477.6MB
Iteration 1100/1250  mem: 137.2/477.6MB
Iteration 1200/1250  mem: 177.7/477.6MB
done building BART in 0.825 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 82 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 243.6/477.6MB
Iteration 200/1250  mem: 171.2/477.6MB
Iteration 300/1250  mem: 215.1/477.6MB
Iteration 400/1250  mem: 257.1/477.6MB
Iteration 500/1250  mem: 179.1/477.6MB
Iteration 600/1250  mem: 224.1/477.6MB
Iteration 700/1250  mem: 266.9/477.6MB
Iteration 800/1250  mem: 109/477.6MB
Iteration 900/1250  mem: 152.9/477.6MB
Iteration 1000/1250  mem: 196.8/477.6MB
Iteration 1100/1250  mem: 124.1/477.6MB
Iteration 1200/1250  mem: 167.5/477.6MB
done building BART in 1.128 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 66 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 111.8/477.6MB
Iteration 200/1250  mem: 148.7/477.6MB
Iteration 300/1250  mem: 190.4/477.6MB
Iteration 400/1250  mem: 231.6/477.6MB
Iteration 500/1250  mem: 152.3/477.6MB
Iteration 600/1250  mem: 197.7/477.6MB
Iteration 700/1250  mem: 133.4/477.6MB
Iteration 800/1250  mem: 180.6/477.6MB
Iteration 900/1250  mem: 227.8/477.6MB
Iteration 1000/1250  mem: 173.9/477.6MB
Iteration 1100/1250  mem: 217.8/477.6MB
Iteration 1200/1250  mem: 261.8/477.6MB
done building BART in 1.008 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 65 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 217.2/477.6MB
Iteration 200/1250  mem: 249.9/477.6MB
Iteration 300/1250  mem: 280.3/477.6MB
Iteration 400/1250  mem: 104.1/477.6MB
Iteration 500/1250  mem: 133.5/477.6MB
Iteration 600/1250  mem: 165.2/477.6MB
Iteration 700/1250  mem: 196.8/477.6MB
Iteration 800/1250  mem: 116.8/477.6MB
Iteration 900/1250  mem: 148.8/477.6MB
Iteration 1000/1250  mem: 180.9/477.6MB
Iteration 1100/1250  mem: 99.4/477.6MB
Iteration 1200/1250  mem: 131.6/477.6MB
done building BART in 0.853 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 25 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 163.1/477.6MB
Iteration 200/1250  mem: 174.8/477.6MB
Iteration 300/1250  mem: 188.8/477.6MB
Iteration 400/1250  mem: 200.5/477.6MB
Iteration 500/1250  mem: 101.3/477.6MB
Iteration 600/1250  mem: 112/477.6MB
Iteration 700/1250  mem: 124.5/477.6MB
Iteration 800/1250  mem: 136.9/477.6MB
Iteration 900/1250  mem: 149.4/477.6MB
Iteration 1000/1250  mem: 161.8/477.6MB
Iteration 1100/1250  mem: 174.3/477.6MB
Iteration 1200/1250  mem: 185/477.6MB
done building BART in 0.242 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 50 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 112/477.6MB
Iteration 200/1250  mem: 137.1/477.6MB
Iteration 300/1250  mem: 163.6/477.6MB
Iteration 400/1250  mem: 188.7/477.6MB
Iteration 500/1250  mem: 215.2/477.6MB
Iteration 600/1250  mem: 130.1/477.6MB
Iteration 700/1250  mem: 154.8/477.6MB
Iteration 800/1250  mem: 181.1/477.6MB
Iteration 900/1250  mem: 207.5/477.6MB
Iteration 1000/1250  mem: 121.2/477.6MB
Iteration 1100/1250  mem: 147/477.6MB
Iteration 1200/1250  mem: 172.7/477.6MB
done building BART in 0.543 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 48 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 214.5/477.6MB
Iteration 200/1250  mem: 238.5/477.6MB
Iteration 300/1250  mem: 148.5/477.6MB
Iteration 400/1250  mem: 173.4/477.6MB
Iteration 500/1250  mem: 198.2/477.6MB
Iteration 600/1250  mem: 223.1/477.6MB
Iteration 700/1250  mem: 246.2/477.6MB
Iteration 800/1250  mem: 155.5/477.6MB
Iteration 900/1250  mem: 179.7/477.6MB
Iteration 1000/1250  mem: 203.9/477.6MB
Iteration 1100/1250  mem: 228.1/477.6MB
Iteration 1200/1250  mem: 252.3/477.6MB
done building BART in 0.495 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 41 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 173.1/477.6MB
Iteration 200/1250  mem: 196/477.6MB
Iteration 300/1250  mem: 216.4/477.6MB
Iteration 400/1250  mem: 239.3/477.6MB
Iteration 500/1250  mem: 259.7/477.6MB
Iteration 600/1250  mem: 158.7/477.6MB
Iteration 700/1250  mem: 180.9/477.6MB
Iteration 800/1250  mem: 200.7/477.6MB
Iteration 900/1250  mem: 222.9/477.6MB
Iteration 1000/1250  mem: 245.1/477.6MB
Iteration 1100/1250  mem: 264.8/477.6MB
Iteration 1200/1250  mem: 167.9/477.6MB
done building BART in 0.437 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 89 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 229.7/477.6MB
Iteration 200/1250  mem: 272/477.6MB
Iteration 300/1250  mem: 189.2/477.6MB
Iteration 400/1250  mem: 235.2/477.6MB
Iteration 500/1250  mem: 278.8/477.6MB
Iteration 600/1250  mem: 201.5/477.6MB
Iteration 700/1250  mem: 247.5/477.6MB
Iteration 800/1250  mem: 293.6/477.6MB
Iteration 900/1250  mem: 215/477.6MB
Iteration 1000/1250  mem: 259.4/477.6MB
Iteration 1100/1250  mem: 306.4/477.6MB
Iteration 1200/1250  mem: 226.9/477.6MB
done building BART in 0.965 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 39 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 274.8/477.6MB
Iteration 200/1250  mem: 297/477.6MB
Iteration 300/1250  mem: 316.3/477.6MB
Iteration 400/1250  mem: 206/477.6MB
Iteration 500/1250  mem: 225.5/477.6MB
Iteration 600/1250  mem: 244.9/477.6MB
Iteration 700/1250  mem: 264.3/477.6MB
Iteration 800/1250  mem: 285.9/477.6MB
Iteration 900/1250  mem: 305.4/477.6MB
Iteration 1000/1250  mem: 324.8/477.6MB
Iteration 1100/1250  mem: 56.8/477.6MB
Iteration 1200/1250  mem: 76.1/477.6MB
done building BART in 0.602 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 69 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 128.7/477.6MB
Iteration 200/1250  mem: 162.4/477.6MB
Iteration 300/1250  mem: 65.2/477.6MB
Iteration 400/1250  mem: 99/477.6MB
Iteration 500/1250  mem: 132.7/477.6MB
Iteration 600/1250  mem: 168.9/477.6MB
Iteration 700/1250  mem: 72.8/477.6MB
Iteration 800/1250  mem: 106.6/477.6MB
Iteration 900/1250  mem: 143/477.6MB
Iteration 1000/1250  mem: 176.8/477.6MB
Iteration 1100/1250  mem: 78.1/477.6MB
Iteration 1200/1250  mem: 113.1/477.6MB
done building BART in 0.716 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 87 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 184.8/477.6MB
Iteration 200/1250  mem: 92/477.6MB
Iteration 300/1250  mem: 138.2/477.6MB
Iteration 400/1250  mem: 184.4/477.6MB
Iteration 500/1250  mem: 94.5/477.6MB
Iteration 600/1250  mem: 139.7/477.6MB
Iteration 700/1250  mem: 185/477.6MB
Iteration 800/1250  mem: 230.2/477.6MB
Iteration 900/1250  mem: 141.3/477.6MB
Iteration 1000/1250  mem: 186.4/477.6MB
Iteration 1100/1250  mem: 229/477.6MB
Iteration 1200/1250  mem: 141.1/477.6MB
done building BART in 0.927 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 58 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 202.1/477.6MB
Iteration 200/1250  mem: 231.1/477.6MB
Iteration 300/1250  mem: 260.2/477.6MB
Iteration 400/1250  mem: 152.4/477.6MB
Iteration 500/1250  mem: 183.5/477.6MB
Iteration 600/1250  mem: 212.1/477.6MB
Iteration 700/1250  mem: 243.2/477.6MB
Iteration 800/1250  mem: 138.5/477.6MB
Iteration 900/1250  mem: 166.8/477.6MB
Iteration 1000/1250  mem: 197.5/477.6MB
Iteration 1100/1250  mem: 225.8/477.6MB
Iteration 1200/1250  mem: 256.5/477.6MB
done building BART in 0.617 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 32 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 159/477.6MB
Iteration 200/1250  mem: 177/477.6MB
Iteration 300/1250  mem: 195/477.6MB
Iteration 400/1250  mem: 214.5/477.6MB
Iteration 500/1250  mem: 234.1/477.6MB
Iteration 600/1250  mem: 252.1/477.6MB
Iteration 700/1250  mem: 273.1/477.6MB
Iteration 800/1250  mem: 159.5/477.6MB
Iteration 900/1250  mem: 179.5/477.6MB
Iteration 1000/1250  mem: 197.4/477.6MB
Iteration 1100/1250  mem: 217.3/477.6MB
Iteration 1200/1250  mem: 235.3/477.6MB
done building BART in 0.419 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 19 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 258/477.6MB
Iteration 200/1250  mem: 269.6/477.6MB
Iteration 300/1250  mem: 278.4/477.6MB
Iteration 400/1250  mem: 287.1/477.6MB
Iteration 500/1250  mem: 298.7/477.6MB
Iteration 600/1250  mem: 33/477.6MB
Iteration 700/1250  mem: 43/477.6MB
Iteration 800/1250  mem: 52.9/477.6MB
Iteration 900/1250  mem: 62.8/477.6MB
Iteration 1000/1250  mem: 72.8/477.6MB
Iteration 1100/1250  mem: 82.7/477.6MB
Iteration 1200/1250  mem: 92.7/477.6MB
done building BART in 0.307 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 79 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 148.4/477.6MB
Iteration 200/1250  mem: 58.8/477.6MB
Iteration 300/1250  mem: 106.3/477.6MB
Iteration 400/1250  mem: 153.9/477.6MB
Iteration 500/1250  mem: 71.6/477.6MB
Iteration 600/1250  mem: 118.3/477.6MB
Iteration 700/1250  mem: 166.9/477.6MB
Iteration 800/1250  mem: 85.4/477.6MB
Iteration 900/1250  mem: 135.3/477.6MB
Iteration 1000/1250  mem: 183/477.6MB
Iteration 1100/1250  mem: 100.2/477.6MB
Iteration 1200/1250  mem: 147.8/477.6MB
done building BART in 1.012 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 30 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 192.3/477.6MB
Iteration 200/1250  mem: 206.1/477.6MB
Iteration 300/1250  mem: 96.5/477.6MB
Iteration 400/1250  mem: 113.1/477.6MB
Iteration 500/1250  mem: 127.8/477.6MB
Iteration 600/1250  mem: 144.3/477.6MB
Iteration 700/1250  mem: 160.9/477.6MB
Iteration 800/1250  mem: 175.6/477.6MB
Iteration 900/1250  mem: 192.1/477.6MB
Iteration 1000/1250  mem: 208.7/477.6MB
Iteration 1100/1250  mem: 223.4/477.6MB
Iteration 1200/1250  mem: 110.9/477.6MB
done building BART in 0.352 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 81 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 167.8/477.6MB
Iteration 200/1250  mem: 214.9/477.6MB
Iteration 300/1250  mem: 125.6/477.6MB
Iteration 400/1250  mem: 172.1/477.6MB
Iteration 500/1250  mem: 216/477.6MB
Iteration 600/1250  mem: 136.4/477.6MB
Iteration 700/1250  mem: 181.5/477.6MB
Iteration 800/1250  mem: 226.7/477.6MB
Iteration 900/1250  mem: 146.9/477.6MB
Iteration 1000/1250  mem: 190.1/477.6MB
Iteration 1100/1250  mem: 236/477.6MB
Iteration 1200/1250  mem: 155/477.6MB
done building BART in 0.946 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 70 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 222.6/477.6MB
Iteration 200/1250  mem: 258.7/477.6MB
Iteration 300/1250  mem: 59.1/477.6MB
Iteration 400/1250  mem: 98.1/477.6MB
Iteration 500/1250  mem: 134.7/477.6MB
Iteration 600/1250  mem: 171.2/477.6MB
Iteration 700/1250  mem: 79/477.6MB
Iteration 800/1250  mem: 117.4/477.6MB
Iteration 900/1250  mem: 155.7/477.6MB
Iteration 1000/1250  mem: 191.5/477.6MB
Iteration 1100/1250  mem: 99.1/477.6MB
Iteration 1200/1250  mem: 136/477.6MB
done building BART in 0.915 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 22 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 169.6/477.6MB
Iteration 200/1250  mem: 180.7/477.6MB
Iteration 300/1250  mem: 191.8/477.6MB
Iteration 400/1250  mem: 205.7/477.6MB
Iteration 500/1250  mem: 85.2/477.6MB
Iteration 600/1250  mem: 96.7/477.6MB
Iteration 700/1250  mem: 108.2/477.6MB
Iteration 800/1250  mem: 119.7/477.6MB
Iteration 900/1250  mem: 131.3/477.6MB
Iteration 1000/1250  mem: 142.8/477.6MB
Iteration 1100/1250  mem: 154.3/477.6MB
Iteration 1200/1250  mem: 165.8/477.6MB
done building BART in 0.236 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 60 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 212/477.6MB
Iteration 200/1250  mem: 121.8/477.6MB
Iteration 300/1250  mem: 160.5/477.6MB
Iteration 400/1250  mem: 199.1/477.6MB
Iteration 500/1250  mem: 109.8/477.6MB
Iteration 600/1250  mem: 149.1/477.6MB
Iteration 700/1250  mem: 186.3/477.6MB
Iteration 800/1250  mem: 225.7/477.6MB
Iteration 900/1250  mem: 142.7/477.6MB
Iteration 1000/1250  mem: 183.7/477.6MB
Iteration 1100/1250  mem: 222.2/477.6MB
Iteration 1200/1250  mem: 140/477.6MB
done building BART in 0.865 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 82 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 208.7/477.6MB
Iteration 200/1250  mem: 148.4/477.6MB
Iteration 300/1250  mem: 192.4/477.6MB
Iteration 400/1250  mem: 236.4/477.6MB
Iteration 500/1250  mem: 175.9/477.6MB
Iteration 600/1250  mem: 219.8/477.6MB
Iteration 700/1250  mem: 162/477.6MB
Iteration 800/1250  mem: 204.9/477.6MB
Iteration 900/1250  mem: 250/477.6MB
Iteration 1000/1250  mem: 193.3/477.6MB
Iteration 1100/1250  mem: 237.9/477.6MB
Iteration 1200/1250  mem: 281.4/477.6MB
done building BART in 0.968 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 66 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 240.8/477.6MB
Iteration 200/1250  mem: 279.9/477.6MB
Iteration 300/1250  mem: 71.3/477.6MB
Iteration 400/1250  mem: 113.5/477.6MB
Iteration 500/1250  mem: 153.5/477.6MB
Iteration 600/1250  mem: 94.2/477.6MB
Iteration 700/1250  mem: 140.5/477.6MB
Iteration 800/1250  mem: 84.3/477.6MB
Iteration 900/1250  mem: 131.9/477.6MB
Iteration 1000/1250  mem: 179.5/477.6MB
Iteration 1100/1250  mem: 128.2/477.6MB
Iteration 1200/1250  mem: 174.3/477.6MB
done building BART in 1.083 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 65 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 139.2/477.6MB
Iteration 200/1250  mem: 171.6/477.6MB
Iteration 300/1250  mem: 205.2/477.6MB
Iteration 400/1250  mem: 126.1/477.6MB
Iteration 500/1250  mem: 158.8/477.6MB
Iteration 600/1250  mem: 191.5/477.6MB
Iteration 700/1250  mem: 224.3/477.6MB
Iteration 800/1250  mem: 149.6/477.6MB
Iteration 900/1250  mem: 183.1/477.6MB
Iteration 1000/1250  mem: 216.5/477.6MB
Iteration 1100/1250  mem: 141.6/477.6MB
Iteration 1200/1250  mem: 173.7/477.6MB
done building BART in 0.706 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 25 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 207/477.6MB
Iteration 200/1250  mem: 218.3/477.6MB
Iteration 300/1250  mem: 232/477.6MB
Iteration 400/1250  mem: 243.4/477.6MB
Iteration 500/1250  mem: 148.1/477.6MB
Iteration 600/1250  mem: 161.9/477.6MB
Iteration 700/1250  mem: 174.1/477.6MB
Iteration 800/1250  mem: 186.2/477.6MB
Iteration 900/1250  mem: 198.3/477.6MB
Iteration 1000/1250  mem: 212.2/477.6MB
Iteration 1100/1250  mem: 224.3/477.6MB
Iteration 1200/1250  mem: 236.4/477.6MB
done building BART in 0.248 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 50 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 164.2/477.6MB
Iteration 200/1250  mem: 190.8/477.6MB
Iteration 300/1250  mem: 218.7/477.6MB
Iteration 400/1250  mem: 245.3/477.6MB
Iteration 500/1250  mem: 163.4/477.6MB
Iteration 600/1250  mem: 190.2/477.6MB
Iteration 700/1250  mem: 217.1/477.6MB
Iteration 800/1250  mem: 243.9/477.6MB
Iteration 900/1250  mem: 270.6/477.6MB
Iteration 1000/1250  mem: 188.2/477.6MB
Iteration 1100/1250  mem: 215.1/477.6MB
Iteration 1200/1250  mem: 240.1/477.6MB
done building BART in 0.552 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 48 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 171.7/477.6MB
Iteration 200/1250  mem: 197.1/477.6MB
Iteration 300/1250  mem: 221/477.6MB
Iteration 400/1250  mem: 246.4/477.6MB
Iteration 500/1250  mem: 271.9/477.6MB
Iteration 600/1250  mem: 35.7/477.6MB
Iteration 700/1250  mem: 59.5/477.6MB
Iteration 800/1250  mem: 83.4/477.6MB
Iteration 900/1250  mem: 109/477.6MB
Iteration 1000/1250  mem: 134.7/477.6MB
Iteration 1100/1250  mem: 45.3/477.6MB
Iteration 1200/1250  mem: 70.9/477.6MB
done building BART in 0.6 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 41 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 110.5/477.6MB
Iteration 200/1250  mem: 131/477.6MB
Iteration 300/1250  mem: 154/477.6MB
Iteration 400/1250  mem: 53.8/477.6MB
Iteration 500/1250  mem: 74.8/477.6MB
Iteration 600/1250  mem: 97.9/477.6MB
Iteration 700/1250  mem: 118.9/477.6MB
Iteration 800/1250  mem: 142/477.6MB
Iteration 900/1250  mem: 163/477.6MB
Iteration 1000/1250  mem: 65.7/477.6MB
Iteration 1100/1250  mem: 86.7/477.6MB
Iteration 1200/1250  mem: 110/477.6MB
done building BART in 0.44 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 89 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 173.1/477.6MB
Iteration 200/1250  mem: 93/477.6MB
Iteration 300/1250  mem: 138/477.6MB
Iteration 400/1250  mem: 182.9/477.6MB
Iteration 500/1250  mem: 99.9/477.6MB
Iteration 600/1250  mem: 145/477.6MB
Iteration 700/1250  mem: 190.1/477.6MB
Iteration 800/1250  mem: 107.4/477.6MB
Iteration 900/1250  mem: 150.9/477.6MB
Iteration 1000/1250  mem: 196.8/477.6MB
Iteration 1100/1250  mem: 110.7/477.6MB
Iteration 1200/1250  mem: 154.7/477.6MB
done building BART in 0.926 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 39 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 201.7/477.6MB
Iteration 200/1250  mem: 88.5/477.6MB
Iteration 300/1250  mem: 110/477.6MB
Iteration 400/1250  mem: 129.7/477.6MB
Iteration 500/1250  mem: 151.2/477.6MB
Iteration 600/1250  mem: 170.9/477.6MB
Iteration 700/1250  mem: 192.4/477.6MB
Iteration 800/1250  mem: 212.1/477.6MB
Iteration 900/1250  mem: 101.9/477.6MB
Iteration 1000/1250  mem: 123.8/477.6MB
Iteration 1100/1250  mem: 143.5/477.6MB
Iteration 1200/1250  mem: 163.2/477.6MB
done building BART in 0.417 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 69 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 215.3/477.6MB
Iteration 200/1250  mem: 117.7/477.6MB
Iteration 300/1250  mem: 152.2/477.6MB
Iteration 400/1250  mem: 188.8/477.6MB
Iteration 500/1250  mem: 225.3/477.6MB
Iteration 600/1250  mem: 125.2/477.6MB
Iteration 700/1250  mem: 160.5/477.6MB
Iteration 800/1250  mem: 195.8/477.6MB
Iteration 900/1250  mem: 231/477.6MB
Iteration 1000/1250  mem: 131.1/477.6MB
Iteration 1100/1250  mem: 166.7/477.6MB
Iteration 1200/1250  mem: 202.3/477.6MB
done building BART in 0.736 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 87 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 138.4/477.6MB
Iteration 200/1250  mem: 185.7/477.6MB
Iteration 300/1250  mem: 233/477.6MB
Iteration 400/1250  mem: 139.2/477.6MB
Iteration 500/1250  mem: 184.3/477.6MB
Iteration 600/1250  mem: 229.5/477.6MB
Iteration 700/1250  mem: 276.8/477.6MB
Iteration 800/1250  mem: 186.6/477.6MB
Iteration 900/1250  mem: 233.8/477.6MB
Iteration 1000/1250  mem: 278.6/477.6MB
Iteration 1100/1250  mem: 192.6/477.6MB
Iteration 1200/1250  mem: 240.1/477.6MB
done building BART in 0.995 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 58 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 300.3/477.6MB
Iteration 200/1250  mem: 194.4/477.6MB
Iteration 300/1250  mem: 225.7/477.6MB
Iteration 400/1250  mem: 255.2/477.6MB
Iteration 500/1250  mem: 286.6/477.6MB
Iteration 600/1250  mem: 53.8/477.6MB
Iteration 700/1250  mem: 84.8/477.6MB
Iteration 800/1250  mem: 113.6/477.6MB
Iteration 900/1250  mem: 146.8/477.6MB
Iteration 1000/1250  mem: 175.6/477.6MB
Iteration 1100/1250  mem: 71.6/477.6MB
Iteration 1200/1250  mem: 103.5/477.6MB
done building BART in 0.762 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 32 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 139.9/477.6MB
Iteration 200/1250  mem: 157.3/477.6MB
Iteration 300/1250  mem: 174.7/477.6MB
Iteration 400/1250  mem: 195/477.6MB
Iteration 500/1250  mem: 75.5/477.6MB
Iteration 600/1250  mem: 93.9/477.6MB
Iteration 700/1250  mem: 112.3/477.6MB
Iteration 800/1250  mem: 133/477.6MB
Iteration 900/1250  mem: 153.8/477.6MB
Iteration 1000/1250  mem: 172.2/477.6MB
Iteration 1100/1250  mem: 190.6/477.6MB
Iteration 1200/1250  mem: 211.3/477.6MB
done building BART in 0.406 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 48 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 131.8/477.6MB
Iteration 200/1250  mem: 164.4/477.6MB
Iteration 300/1250  mem: 197/477.6MB
Iteration 400/1250  mem: 98.6/477.6MB
Iteration 500/1250  mem: 132.9/477.6MB
Iteration 600/1250  mem: 164.5/477.6MB
Iteration 700/1250  mem: 198.7/477.6MB
Iteration 800/1250  mem: 103/477.6MB
Iteration 900/1250  mem: 134.9/477.6MB
Iteration 1000/1250  mem: 166.9/477.6MB
Iteration 1100/1250  mem: 201.5/477.6MB
Iteration 1200/1250  mem: 233.4/477.6MB
done building BART in 0.622 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
Bayesian Additive Regression Trees 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  num_trees  k          alpha      beta       nu         RMSE       Rsquared 
  19         4.3596983  0.9341044  3.9699674  4.6831559  0.2533818  0.9241414
  22         1.2663450  0.9420858  1.8676102  4.6722125  0.2726308  0.9028600
  25         0.4757692  0.9199145  0.3059371  4.2545695  0.2606852  0.8946232
  30         2.2599303  0.9782560  2.8616846  4.0572549  0.2533283  0.9292504
  32         2.7501422  0.9011917  0.2833554  3.2939912  0.3077547  0.9088632
  39         3.2863476  0.9025990  2.8424097  3.6544425  0.2857420  0.9189420
  41         4.7496553  0.9248810  3.1058293  4.1250797  0.2644403  0.9289902
  48         2.3381597  0.9669634  3.2113581  0.7726170  0.2530870  0.9300584
  50         1.3399492  0.9744047  0.1638697  4.3125999  0.3363143  0.8753375
  58         3.4037502  0.9725964  3.5206183  2.9382394  0.2720415  0.9207294
  60         4.8534458  0.9375591  0.7880835  4.3330556  0.4322365  0.7516838
  65         0.2742538  0.9037345  0.5783814  1.6618567  0.8631318  0.5131348
  66         0.9688189  0.9951653  0.3065354  3.6514736  0.5010996  0.5692526
  69         0.3765049  0.9162530  2.9310666  1.7863997  0.8088284  0.4502159
  70         4.4631333  0.9367273  3.8571464  2.8223188  0.3871392  0.8200469
  79         2.9623006  0.9602907  0.2042658  4.1879842  0.3739292  0.8228857
  81         0.7901718  0.9388938  1.3085626  4.5470324  0.4420730  0.7018198
  82         3.3239612  0.9779873  2.0917532  2.0155777  0.3313625  0.8752839
  87         3.1159944  0.9355251  2.1131217  0.5086929  0.3232478  0.8790770
  89         0.4587208  0.9481641  0.1593540  3.6807048  0.4058711  0.7011554
  MAE        Selected
  0.1272761          
  0.1500245          
  0.1524367          
  0.1334564          
  0.1781347          
  0.1424534          
  0.1426259          
  0.1579823  *       
  0.1954852          
  0.1444584          
  0.2448535          
  0.6429861          
  0.3402111          
  0.6519945          
  0.2147764          
  0.2211599          
  0.3172138          
  0.1915742          
  0.1922002          
  0.3085366          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were num_trees = 48, k = 2.33816, alpha
 = 0.9669634, beta = 3.211358 and nu = 0.772617.
[1] "Sat Mar 10 01:00:59 2018"
Error in investigate_var_importance(object, plot = FALSE) : 
  could not find function "investigate_var_importance"
Error : package arm is required
Error : package arm is required
Error : package arm is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:01:15 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "bayesglm"                
t=100, m=4
t=200, m=6
t=300, m=2
t=400, m=3
t=500, m=3
t=600, m=4
t=700, m=3
t=800, m=3
t=900, m=5
t=100, m=4
t=200, m=2
t=300, m=2
t=400, m=5
t=500, m=3
t=600, m=4
t=700, m=4
t=800, m=4
t=900, m=5
t=100, m=6
t=200, m=3
t=300, m=2
t=400, m=4
t=500, m=3
t=600, m=3
t=700, m=4
t=800, m=5
t=900, m=2
t=100, m=2
t=200, m=3
t=300, m=5
t=400, m=2
t=500, m=2
t=600, m=5
t=700, m=2
t=800, m=2
t=900, m=2
The Bayesian lasso 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  sparsity   RMSE       Rsquared   MAE        Selected
  0.1062531  0.3447367  0.8174432  0.2602303          
  0.1381929  0.3447367  0.8174432  0.2602303          
  0.1673884  0.3447367  0.8174432  0.2602303          
  0.2284886  0.3447367  0.8174432  0.2602303          
  0.2461119  0.3446934  0.8174584  0.2602025          
  0.3247722  0.3436005  0.8185593  0.2593467  *       
  0.3447800  0.3466705  0.8178168  0.2594086          
  0.4177590  0.3442104  0.8219246  0.2566141          
  0.4464257  0.3553131  0.8190336  0.2619462          
  0.5334745  0.3467146  0.8176466  0.2627819          
  0.5579596  0.3467146  0.8176466  0.2627819          
  0.6113360  0.3467146  0.8176466  0.2627819          
  0.6177791  0.3467146  0.8176466  0.2627819          
  0.6533015  0.3467146  0.8176466  0.2627819          
  0.6663501  0.3467146  0.8176466  0.2627819          
  0.7598991  0.3467146  0.8176466  0.2627819          
  0.7849139  0.3467146  0.8176466  0.2627819          
  0.8010843  0.3467146  0.8176466  0.2627819          
  0.8506656  0.3467146  0.8176466  0.2627819          
  0.8747652  0.3467146  0.8176466  0.2627819          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was sparsity = 0.3247722.
[1] "Sat Mar 10 01:01:34 2018"
t=100, m=4
t=200, m=3
t=300, m=5
t=400, m=4
t=500, m=6
t=600, m=4
t=700, m=4
t=800, m=3
t=900, m=4
t=100, m=4
t=200, m=4
t=300, m=4
t=400, m=8
t=500, m=4
t=600, m=3
t=700, m=4
t=800, m=7
t=900, m=2
t=100, m=5
t=200, m=1
t=300, m=4
t=400, m=5
t=500, m=6
t=600, m=6
t=700, m=5
t=800, m=3
t=900, m=3
t=100, m=6
t=200, m=4
t=300, m=3
t=400, m=3
t=500, m=3
t=600, m=2
t=700, m=3
t=800, m=3
t=900, m=6
Bayesian Ridge Regression (Model Averaged) 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE       Rsquared   MAE      
  0.3439062  0.8184976  0.2594456

[1] "Sat Mar 10 01:01:50 2018"
t=100, m=2
t=200, m=3
t=300, m=3
t=400, m=2
t=500, m=2
t=600, m=4
t=700, m=2
t=800, m=2
t=900, m=3
t=100, m=2
t=200, m=2
t=300, m=1
t=400, m=1
t=500, m=1
t=600, m=1
t=700, m=3
t=800, m=4
t=900, m=3
t=100, m=3
t=200, m=3
t=300, m=3
t=400, m=2
t=500, m=5
t=600, m=1
t=700, m=3
t=800, m=2
t=900, m=2
t=100, m=4
t=200, m=2
t=300, m=3
t=400, m=5
t=500, m=3
t=600, m=6
t=700, m=8
t=800, m=4
t=900, m=6
Bayesian Ridge Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE       Rsquared   MAE      
  0.3470489  0.8174949  0.2696975

[1] "Sat Mar 10 01:02:05 2018"
Boosted Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  nu          mstop  RMSE       Rsquared   MAE        Selected
  0.03385561  306    0.5561336  0.6105526  0.3075170          
  0.04610529  326    0.5211503  0.6216610  0.2942049          
  0.05595476  437    0.4720891  0.6608605  0.2835228          
  0.05799715   84    0.6220678  0.5807434  0.3503928          
  0.09566258  392    0.4369117  0.6973841  0.2803777          
  0.11706450  309    0.4384494  0.6954041  0.2803695          
  0.15270813   69    0.5492228  0.6149527  0.3050235          
  0.16152591  223    0.4377413  0.6966199  0.2806218          
  0.27173965  115    0.4423722  0.6916009  0.2809496          
  0.28111153  209    0.4255540  0.7116053  0.2912099          
  0.33046703  123    0.4300268  0.7046206  0.2823946          
  0.35588361  380    0.4203664  0.7214339  0.2982695          
  0.37429613  425    0.4194809  0.7221469  0.2988704          
  0.39470444  163    0.4264511  0.7130588  0.2953981          
  0.39921055  400    0.4195719  0.7218402  0.2990055          
  0.40876928  267    0.4220578  0.7200422  0.2979668          
  0.52329186   54    0.4402593  0.6929375  0.2812744          
  0.53568337  333    0.4183886  0.7233946  0.2994962  *       
  0.57000870  173    0.4228065  0.7181874  0.2991053          
  0.58244281  279    0.4189528  0.7226746  0.2994304          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 333 and nu = 0.5356834.
[1] "Sat Mar 10 01:03:26 2018"
Conditional Inference Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE        Selected
  1     0.7118348  0.4012478  0.4327786          
  2     0.6853260  0.6171160  0.4111057          
  3     0.6453286  0.6661673  0.3775649          
  4     0.5933764  0.7444825  0.3324648          
  5     0.5299741  0.7745279  0.2757580          
  6     0.4742100  0.7952963  0.2284879          
  7     0.4271357  0.7996080  0.1918682          
  8     0.3988274  0.7944881  0.1914982  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 8.
[1] "Sat Mar 10 01:03:47 2018"
Error in varimp(object, ...) : could not find function "varimp"
Conditional Inference Tree 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  mincriterion  RMSE       Rsquared   MAE        Selected
  0.1062531     0.4476923  0.7139613  0.2687478          
  0.1381929     0.4476923  0.7139613  0.2687478          
  0.1673884     0.4476923  0.7139613  0.2687478          
  0.2284886     0.4476923  0.7139613  0.2687478          
  0.2461119     0.4476923  0.7139613  0.2687478          
  0.3247722     0.4476923  0.7139613  0.2687478          
  0.3447800     0.4476923  0.7139613  0.2687478          
  0.4177590     0.4476923  0.7139613  0.2687478          
  0.4464257     0.4476923  0.7139613  0.2687478          
  0.5334745     0.4476923  0.7139613  0.2687478          
  0.5579596     0.4476923  0.7139613  0.2687478          
  0.6113360     0.4476923  0.7139613  0.2687478          
  0.6177791     0.4476923  0.7139613  0.2687478          
  0.6533015     0.4476923  0.7139613  0.2687478          
  0.6663501     0.4476923  0.7139613  0.2687478          
  0.7598991     0.4476923  0.7139613  0.2687478          
  0.7849139     0.4476923  0.7139613  0.2687478          
  0.8010843     0.4476923  0.7139613  0.2687478          
  0.8506656     0.4476923  0.7139613  0.2687478          
  0.8747652     0.4476923  0.7139613  0.2687478  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mincriterion = 0.8747652.
[1] "Sat Mar 10 01:04:04 2018"
Conditional Inference Tree 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  maxdepth  mincriterion  RMSE       Rsquared   MAE        Selected
   2        0.87193966    0.4722931  0.6711266  0.3160862          
   3        0.09515384    0.4558572  0.7040106  0.2845025          
   3        0.25326900    0.4558572  0.7040106  0.2845025          
   4        0.45198606    0.4479287  0.7134452  0.2721314          
   4        0.55002844    0.4479287  0.7134452  0.2721314          
   5        0.65726952    0.4476923  0.7139613  0.2687478  *       
   6        0.94993106    0.4476923  0.7139613  0.2687478          
   7        0.26798984    0.4476923  0.7139613  0.2687478          
   7        0.46763193    0.4476923  0.7139613  0.2687478          
   9        0.68075005    0.4476923  0.7139613  0.2687478          
   9        0.97068916    0.4476923  0.7139613  0.2687478          
  10        0.05485076    0.4476923  0.7139613  0.2687478          
  10        0.07530099    0.4476923  0.7139613  0.2687478          
  10        0.19376378    0.4476923  0.7139613  0.2687478          
  10        0.89262666    0.4476923  0.7139613  0.2687478          
  12        0.15803436    0.4476923  0.7139613  0.2687478          
  12        0.59246012    0.4476923  0.7139613  0.2687478          
  13        0.62319889    0.4476923  0.7139613  0.2687478          
  13        0.66479223    0.4476923  0.7139613  0.2687478          
  14        0.09174417    0.4476923  0.7139613  0.2687478          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were maxdepth = 5 and mincriterion
 = 0.6572695.
[1] "Sat Mar 10 01:04:22 2018"
Cubist 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  committees  neighbors  RMSE        Rsquared   MAE         Selected
   6          6          0.06923325  0.9939400  0.04136759          
   8          6          0.06908151  0.9941688  0.04036695          
  10          1          0.07071939  0.9940140  0.04379942          
  10          8          0.07002071  0.9940506  0.04122547          
  16          7          0.07002049  0.9940944  0.04126076          
  20          6          0.07080945  0.9938318  0.04146988          
  26          1          0.07161097  0.9941902  0.04291626          
  27          4          0.06961419  0.9941334  0.04108740          
  46          2          0.06863204  0.9942158  0.03968733          
  47          4          0.06537256  0.9947535  0.03709980          
  56          2          0.06544887  0.9947502  0.03729590          
  60          7          0.06293914  0.9955559  0.03435921          
  63          8          0.06411940  0.9951384  0.03477197          
  66          3          0.06511043  0.9948949  0.03687271          
  67          8          0.06574243  0.9951521  0.03599357          
  69          5          0.06349011  0.9950849  0.03472214          
  88          1          0.06225273  0.9956350  0.03443413          
  90          6          0.06232397  0.9955699  0.03382503          
  95          3          0.06183183  0.9953604  0.03428653  *       
  98          5          0.06194688  0.9955527  0.03350049          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were committees = 95 and neighbors = 3.
[1] "Sat Mar 10 01:05:08 2018"
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
Stacked AutoEncoder Deep Neural Network 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  layer1  layer2  layer3  hidden_dropout  visible_dropout  RMSE     
   4       6       9      0.32683178      0.65410975       0.7241673
   4      18       8      0.69474429      0.65564182       0.7286117
   5       3       5      0.05353899      0.59563973       0.7259768
   6      10      16      0.50079480      0.56801568       0.7254118
   6      12       2      0.04958720      0.46115876       0.7276137
   8      14       2      0.49742170      0.51162196       0.7236983
   8      20       6      0.54352013      0.57751115       0.7226871
   9      10      14      0.56198766      0.10816638       0.7269924
  10       7      16      0.02867720      0.60376398       0.7315300
  12      14      15      0.61610820      0.41135352       0.7425251
  12      20       9      0.13791461      0.60662779       0.7207685
  13       3       2      0.10121675      0.23265994       0.7252918
  13       5      20      0.05364370      0.51120631       0.7242758
  14       3       5      0.51293665      0.25009596       0.7265543
  14      18       8      0.67500062      0.39512463       0.7208744
  16       5       9      0.22899845      0.63658454       0.7265825
  16      13      13      0.03574652      0.58631779       0.7204110
  17      14      16      0.36605681      0.28218088       0.7242473
  18       3      11      0.02788695      0.51529867       0.7270864
  18      13       8      0.36979630      0.07121701       0.7193662
  Rsquared    MAE        Selected
  0.01288764  0.4468209          
  0.20130606  0.4631913          
  0.08776229  0.4425314          
  0.09343387  0.4565569          
  0.07968223  0.4481944          
  0.15607610  0.4440878          
  0.12409883  0.4451680          
  0.36083534  0.4626421          
  0.05845367  0.4509919          
  0.06114807  0.4724447          
  0.04997151  0.4363107          
  0.09301142  0.4501831          
  0.07842410  0.4381055          
  0.09522543  0.4478439          
  0.31602185  0.4541818          
  0.09955349  0.4428719          
  0.04548995  0.4380210          
  0.37007620  0.4655409          
  0.03490037  0.4447964          
  0.01103515  0.4410571  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were layer1 = 18, layer2 = 13, layer3 =
 8, hidden_dropout = 0.3697963 and visible_dropout = 0.07121701.
[1] "Sat Mar 10 01:05:25 2018"
Loading required package: earth
Loading required package: plotmo
Loading required package: plotrix
Loading required package: TeachingDemos
Multivariate Adaptive Regression Spline 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  degree  nprune  RMSE       Rsquared   MAE         Selected
  1       2       0.5026163  0.5769792  0.32178815          
  1       3       0.1342640  0.9759327  0.09922468          
  1       4       0.1341351  0.9761592  0.09453350  *       
  1       5       0.1464759  0.9749629  0.09896930          
  2       2       0.5026163  0.5769792  0.32178815          
  2       3       0.1342640  0.9759327  0.09922468          
  2       4       0.1341351  0.9761592  0.09453350          
  2       5       0.1706225  0.9659274  0.10039122          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nprune = 4 and degree = 1.
[1] "Sat Mar 10 01:05:44 2018"
Loading required package: MASS
Extreme Learning Machine 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  nhid  actfun   RMSE       Rsquared   MAE        Selected
   3    radbas   0.6505518  0.2848405  0.4221041          
   3    tansig   0.6895031  0.1264719  0.4432616          
   4    sin      0.6718391  0.2511185  0.4608430          
   5    purelin  0.6942283  0.1785593  0.4597639          
   5    radbas   0.5815295  0.3737353  0.3789134          
   7    purelin  0.4713345  0.5976903  0.3644470          
   7    tansig   0.6753205  0.2118988  0.4528751          
   8    radbas   0.7294662  0.1996497  0.4824164          
   9    radbas   0.5729499  0.4817118  0.4276830          
  11    purelin  0.3655383  0.7970557  0.2797667  *       
  11    tansig   0.4437170  0.7068533  0.3294308          
  12    sin      0.4544745  0.6797555  0.3411664          
  13    sin      0.5185260  0.6633091  0.3449699          
  13    tansig   0.5509365  0.5999377  0.4006076          
  15    purelin  0.3655383  0.7970557  0.2797667          
  15    sin      0.5417876  0.6139314  0.3626213          
  16    purelin  0.3655383  0.7970557  0.2797667          
  17    purelin  0.3655383  0.7970557  0.2797667          
  17    sin      0.4596214  0.6613430  0.3333722          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nhid = 11 and actfun = purelin.
[1] "Sat Mar 10 01:06:00 2018"
Elasticnet 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  lambda        fraction    RMSE       Rsquared   MAE        Selected
  4.340291e-05  0.87193966  0.3560684  0.8084043  0.2679203          
  6.747726e-05  0.25326900  0.5191503  0.8310144  0.2736002          
  1.010020e-04  0.09515384  0.6447622  0.8310144  0.3756433          
  2.349262e-04  0.45198606  0.3905929  0.8310144  0.1955552          
  2.996896e-04  0.55002844  0.3528079  0.8310144  0.2162754          
  8.884509e-04  0.65726952  0.3459268  0.8277983  0.2408608  *       
  1.171332e-03  0.94993106  0.3619584  0.8007754  0.2761500          
  3.210364e-03  0.46763193  0.3832710  0.8310144  0.1965572          
  4.770390e-03  0.26798984  0.5083531  0.8310144  0.2649892          
  1.587988e-02  0.68075005  0.3470100  0.8259006  0.2441071          
  2.227191e-02  0.97068916  0.3644892  0.7973800  0.2772205          
  4.656033e-02  0.05485076  0.6786895  0.8310144  0.4041780          
  5.089497e-02  0.19376378  0.5661644  0.8310144  0.3112339          
  8.313985e-02  0.07530099  0.6616458  0.8310144  0.3896915          
  9.956366e-02  0.89262666  0.3600152  0.7998959  0.2682069          
  3.625722e-01  0.59246012  0.3536404  0.8258142  0.2352105          
  5.122520e-01  0.15803436  0.5826067  0.8310144  0.3243459          
  6.404805e-01  0.66479223  0.3545434  0.8069763  0.2483303          
  1.270555e+00  0.62319889  0.3587271  0.7980814  0.2483221          
  1.772521e+00  0.09174417  0.6297190  0.8310144  0.3630754          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were fraction = 0.6572695 and lambda
 = 0.0008884509.
[1] "Sat Mar 10 01:06:16 2018"
Error : package evtree is required
In addition: There were 42 warnings (use warnings() to see them)
Error : package evtree is required
Error : package evtree is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:06:32 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "evtree"                  
Random Forest by Randomization 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  mtry  numRandomCuts  RMSE       Rsquared   MAE         Selected
  1     22             0.6262624  0.3871694  0.39731974          
  2      3             0.4940874  0.7690716  0.28385986          
  2      7             0.5020173  0.7141379  0.29867135          
  3     12             0.4230271  0.7962401  0.24189411          
  3     14             0.4264064  0.7908960  0.24795666          
  3     17             0.4355408  0.7788592  0.25185176          
  4     12             0.3619545  0.8523511  0.19154303          
  4     24             0.3729292  0.8372561  0.20140711          
  5      7             0.3006823  0.9039112  0.15046710          
  5     18             0.3044818  0.8967336  0.15354203          
  6      2             0.2349497  0.9461065  0.11195162          
  6      5             0.2369298  0.9453054  0.11112244          
  6     23             0.2563298  0.9264243  0.12297482          
  6     25             0.2586832  0.9275679  0.11805786          
  7     15             0.2026076  0.9573527  0.09322527          
  8      3             0.1647558  0.9724525  0.07216535  *       
  8      4             0.1651589  0.9733881  0.07231788          
  8     16             0.1735430  0.9693950  0.07527400          
  8     17             0.1740724  0.9698918  0.07591736          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 8 and numRandomCuts = 3.
[1] "Sat Mar 10 01:07:06 2018"
Ridge Regression with Variable Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  lambda        k  RMSE       Rsquared   MAE        Selected
  4.340291e-05  8  0.3649275  0.7975641  0.2790751          
  6.747726e-05  3  0.3580833  0.8020206  0.2766486          
  1.010020e-04  1  0.3399263  0.8310144  0.2701838          
  2.349262e-04  5  0.3644162  0.7996059  0.2781553          
  2.996896e-04  5  0.3644129  0.7996041  0.2781358          
  8.884509e-04  6  0.3669956  0.7992790  0.2820576          
  1.171332e-03  9  0.3655111  0.7969530  0.2794280          
  3.210364e-03  5  0.3642675  0.7995215  0.2772598          
  4.770390e-03  3  0.3398947  0.8216054  0.2600035          
  1.587988e-02  7  0.3664575  0.7974129  0.2770791          
  2.227191e-02  9  0.3665333  0.7982635  0.2761665          
  4.656033e-02  1  0.3377373  0.8310144  0.2575633          
  5.089497e-02  2  0.3487373  0.8098478  0.2538276          
  8.313985e-02  1  0.3376763  0.8310144  0.2480705  *       
  9.956366e-02  9  0.3497843  0.8086631  0.2433295          
  3.625722e-01  6        NaN        NaN        NaN          
  5.122520e-01  2        NaN        NaN        NaN          
  6.404805e-01  6        NaN        NaN        NaN          
  1.270555e+00  6        NaN        NaN        NaN          
  1.772521e+00  1        NaN        NaN        NaN          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were k = 1 and lambda = 0.08313985.
[1] "Sat Mar 10 01:07:23 2018"
Loading required package: mgcv
Loading required package: nlme
This is mgcv 1.8-22. For overview type 'help("mgcv-package")'.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 30 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 13 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:07:46 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "gam"                     
Boosted Generalized Additive Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  mstop  prune  RMSE        Rsquared   MAE         Selected
  107    no     0.06246669  0.9972804  0.03721667          
  139    yes    0.05889960  0.9975386  0.03522016          
  168    yes    0.05664976  0.9976559  0.03356639          
  229    yes    0.05306867  0.9977427  0.03053816          
  247    no     0.05233385  0.9977601  0.02986241          
  325    no     0.04997712  0.9977976  0.02814633          
  345    no     0.04960194  0.9977872  0.02795719          
  418    yes    0.04858086  0.9977994  0.02752035          
  447    yes    0.04844590  0.9977984  0.02742742          
  534    no     0.04730693  0.9977848  0.02699731          
  558    no     0.04711843  0.9977810  0.02692741          
  612    yes    0.04806103  0.9977903  0.02726963          
  618    yes    0.04806103  0.9977903  0.02726963          
  654    yes    0.04806103  0.9977903  0.02726963          
  667    no     0.04653693  0.9977479  0.02657654          
  760    no     0.04619538  0.9977255  0.02624469          
  785    yes    0.04806103  0.9977903  0.02726963          
  802    no     0.04604053  0.9977147  0.02614165          
  851    no     0.04591963  0.9977015  0.02606157  *       
  875    yes    0.04806103  0.9977903  0.02726963          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 851 and prune = no.
[1] "Sat Mar 10 01:08:23 2018"
Loading required package: gam
Loading required package: splines
Loading required package: foreach
Loaded gam 1.14-4


Attaching package: 'gam'

The following objects are masked from 'package:mgcv':

    gam, gam.control, gam.fit, plot.gam, predict.gam, s, summary.gam

Generalized Additive Model using Splines 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  df         RMSE       Rsquared   MAE        Selected
  0.5312657  0.3655384  0.7970557  0.2797665          
  0.6909645  0.3655384  0.7970557  0.2797665          
  0.8369418  0.3655384  0.7970557  0.2797665          
  1.1424429  0.3618572  0.8023862  0.2771905          
  1.2305597  0.3593492  0.8059150  0.2750843          
  1.6238612  0.3442502  0.8253358  0.2612259          
  1.7238999  0.3390915  0.8314141  0.2563774          
  2.0887952  0.3152827  0.8569918  0.2350854          
  2.2321283  0.3039691  0.8680200  0.2251960          
  2.6673726  0.2659494  0.9009781  0.1919393          
  2.7897979  0.2549186  0.9095073  0.1821792          
  3.0566800  0.2316172  0.9261426  0.1637958          
  3.0888957  0.2288723  0.9279845  0.1616396          
  3.2665077  0.2144033  0.9372824  0.1501335          
  3.3317507  0.2093300  0.9403862  0.1461989          
  3.7994954  0.1775736  0.9579994  0.1217266          
  3.9245697  0.1704826  0.9615060  0.1166117          
  4.0054216  0.1661681  0.9635658  0.1135631          
  4.2533278  0.1543879  0.9689021  0.1051329          
  4.3738262  0.1493577  0.9710501  0.1015469  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was df = 4.373826.
[1] "Sat Mar 10 01:08:46 2018"
Error in .local(object, ...) : test vector does not match model !
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:09:06 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "gaussprLinear"           
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:09:31 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "gaussprPoly"             
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:09:52 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "gaussprRadial"           
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3956             nan     0.0081    0.0045
     2        0.3917             nan     0.0081    0.0032
     3        0.3887             nan     0.0081    0.0040
     4        0.3862             nan     0.0081    0.0036
     5        0.3813             nan     0.0081    0.0037
     6        0.3780             nan     0.0081    0.0033
     7        0.3738             nan     0.0081    0.0045
     8        0.3706             nan     0.0081    0.0040
     9        0.3662             nan     0.0081    0.0037
    10        0.3627             nan     0.0081    0.0032
    20        0.3273             nan     0.0081    0.0030
    40        0.2690             nan     0.0081    0.0022
    60        0.2296             nan     0.0081    0.0017
    80        0.2001             nan     0.0081    0.0010
   100        0.1752             nan     0.0081    0.0007
   120        0.1602             nan     0.0081    0.0007
   140        0.1462             nan     0.0081    0.0004
   160        0.1356             nan     0.0081    0.0002
   180        0.1287             nan     0.0081    0.0004
   200        0.1232             nan     0.0081   -0.0001
   220        0.1193             nan     0.0081   -0.0001
   240        0.1134             nan     0.0081   -0.0001
   260        0.1077             nan     0.0081    0.0000
   280        0.1048             nan     0.0081    0.0003
   300        0.1011             nan     0.0081    0.0001
   320        0.0977             nan     0.0081   -0.0003
   340        0.0946             nan     0.0081   -0.0001
   360        0.0920             nan     0.0081   -0.0004
   380        0.0893             nan     0.0081   -0.0001
   400        0.0865             nan     0.0081    0.0000
   420        0.0850             nan     0.0081   -0.0000
   440        0.0829             nan     0.0081   -0.0001
   460        0.0810             nan     0.0081   -0.0003
   480        0.0787             nan     0.0081   -0.0001
   500        0.0761             nan     0.0081   -0.0000
   520        0.0741             nan     0.0081   -0.0001
   540        0.0725             nan     0.0081   -0.0002
   560        0.0707             nan     0.0081   -0.0002
   580        0.0691             nan     0.0081   -0.0000
   600        0.0677             nan     0.0081   -0.0001
   620        0.0664             nan     0.0081   -0.0003
   640        0.0652             nan     0.0081   -0.0001
   660        0.0635             nan     0.0081   -0.0001
   680        0.0623             nan     0.0081   -0.0000
   700        0.0611             nan     0.0081   -0.0000
   720        0.0597             nan     0.0081   -0.0001
   740        0.0582             nan     0.0081   -0.0000
   760        0.0567             nan     0.0081   -0.0000
   780        0.0554             nan     0.0081   -0.0001
   800        0.0543             nan     0.0081   -0.0001
   820        0.0526             nan     0.0081   -0.0001
   840        0.0519             nan     0.0081   -0.0001
   860        0.0504             nan     0.0081   -0.0001
   880        0.0496             nan     0.0081   -0.0002
   900        0.0486             nan     0.0081    0.0000
   920        0.0474             nan     0.0081   -0.0000
   940        0.0461             nan     0.0081   -0.0001
   960        0.0449             nan     0.0081   -0.0001
   980        0.0439             nan     0.0081   -0.0002
  1000        0.0428             nan     0.0081   -0.0001
  1020        0.0415             nan     0.0081   -0.0000
  1040        0.0404             nan     0.0081   -0.0001
  1060        0.0396             nan     0.0081   -0.0001
  1080        0.0389             nan     0.0081   -0.0001
  1100        0.0381             nan     0.0081   -0.0002
  1120        0.0373             nan     0.0081   -0.0001
  1140        0.0366             nan     0.0081   -0.0000
  1160        0.0362             nan     0.0081   -0.0001
  1180        0.0353             nan     0.0081   -0.0000
  1200        0.0349             nan     0.0081   -0.0001
  1220        0.0341             nan     0.0081   -0.0001
  1231        0.0335             nan     0.0081   -0.0001

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3917             nan     0.0234    0.0073
     2        0.3837             nan     0.0234    0.0084
     3        0.3734             nan     0.0234    0.0067
     4        0.3657             nan     0.0234    0.0074
     5        0.3589             nan     0.0234    0.0071
     6        0.3517             nan     0.0234    0.0059
     7        0.3464             nan     0.0234    0.0060
     8        0.3399             nan     0.0234    0.0062
     9        0.3328             nan     0.0234    0.0050
    10        0.3262             nan     0.0234    0.0039
    20        0.2744             nan     0.0234    0.0037
    40        0.2053             nan     0.0234    0.0010
    60        0.1695             nan     0.0234    0.0009
    80        0.1474             nan     0.0234    0.0001
   100        0.1352             nan     0.0234   -0.0004
   120        0.1266             nan     0.0234    0.0004
   140        0.1196             nan     0.0234    0.0002
   160        0.1139             nan     0.0234   -0.0006
   180        0.1078             nan     0.0234   -0.0006
   200        0.1026             nan     0.0234   -0.0006
   220        0.1000             nan     0.0234   -0.0004
   240        0.0962             nan     0.0234   -0.0007
   260        0.0922             nan     0.0234   -0.0002
   280        0.0888             nan     0.0234   -0.0000
   300        0.0856             nan     0.0234   -0.0004
   320        0.0829             nan     0.0234   -0.0007
   340        0.0805             nan     0.0234   -0.0000
   360        0.0786             nan     0.0234   -0.0003
   380        0.0755             nan     0.0234    0.0000
   400        0.0732             nan     0.0234   -0.0001
   420        0.0712             nan     0.0234   -0.0003
   440        0.0690             nan     0.0234   -0.0002
   460        0.0672             nan     0.0234   -0.0003
   480        0.0658             nan     0.0234   -0.0003
   500        0.0650             nan     0.0234   -0.0004
   520        0.0634             nan     0.0234   -0.0002
   540        0.0624             nan     0.0234   -0.0003
   560        0.0609             nan     0.0234   -0.0004
   580        0.0594             nan     0.0234   -0.0004
   600        0.0582             nan     0.0234   -0.0004
   620        0.0571             nan     0.0234   -0.0003
   640        0.0556             nan     0.0234   -0.0004
   660        0.0543             nan     0.0234   -0.0001
   680        0.0541             nan     0.0234   -0.0000
   700        0.0532             nan     0.0234   -0.0003
   720        0.0525             nan     0.0234   -0.0002
   740        0.0512             nan     0.0234    0.0000
   760        0.0499             nan     0.0234   -0.0002
   780        0.0488             nan     0.0234   -0.0001
   800        0.0481             nan     0.0234   -0.0002
   820        0.0475             nan     0.0234   -0.0004
   840        0.0466             nan     0.0234   -0.0003
   860        0.0460             nan     0.0234   -0.0003
   880        0.0454             nan     0.0234   -0.0002
   900        0.0448             nan     0.0234   -0.0005
   920        0.0440             nan     0.0234   -0.0002
   940        0.0431             nan     0.0234   -0.0001
   960        0.0427             nan     0.0234   -0.0004
   980        0.0419             nan     0.0234   -0.0002
  1000        0.0412             nan     0.0234   -0.0003
  1020        0.0404             nan     0.0234   -0.0001
  1040        0.0397             nan     0.0234   -0.0002
  1060        0.0390             nan     0.0234   -0.0001
  1080        0.0386             nan     0.0234   -0.0001
  1100        0.0381             nan     0.0234   -0.0002
  1120        0.0375             nan     0.0234   -0.0003
  1140        0.0367             nan     0.0234   -0.0001
  1160        0.0364             nan     0.0234   -0.0002
  1180        0.0361             nan     0.0234   -0.0003
  1200        0.0356             nan     0.0234   -0.0002
  1220        0.0349             nan     0.0234   -0.0002
  1240        0.0344             nan     0.0234   -0.0001
  1260        0.0338             nan     0.0234   -0.0001
  1280        0.0333             nan     0.0234   -0.0002
  1300        0.0329             nan     0.0234   -0.0002
  1320        0.0325             nan     0.0234   -0.0001
  1340        0.0320             nan     0.0234   -0.0001
  1360        0.0315             nan     0.0234   -0.0001
  1380        0.0310             nan     0.0234   -0.0003
  1400        0.0305             nan     0.0234   -0.0001
  1420        0.0301             nan     0.0234   -0.0002
  1440        0.0295             nan     0.0234   -0.0001
  1460        0.0290             nan     0.0234   -0.0002
  1480        0.0285             nan     0.0234   -0.0002
  1500        0.0281             nan     0.0234   -0.0002
  1520        0.0275             nan     0.0234   -0.0003
  1540        0.0271             nan     0.0234   -0.0002
  1560        0.0269             nan     0.0234   -0.0001
  1580        0.0266             nan     0.0234   -0.0001
  1600        0.0263             nan     0.0234   -0.0001
  1620        0.0260             nan     0.0234   -0.0002
  1640        0.0256             nan     0.0234   -0.0001
  1660        0.0253             nan     0.0234   -0.0000
  1680        0.0250             nan     0.0234   -0.0002
  1700        0.0246             nan     0.0234   -0.0001
  1720        0.0243             nan     0.0234   -0.0002
  1740        0.0241             nan     0.0234   -0.0001
  1760        0.0238             nan     0.0234   -0.0002
  1780        0.0234             nan     0.0234   -0.0001
  1800        0.0230             nan     0.0234   -0.0001
  1820        0.0227             nan     0.0234   -0.0001
  1840        0.0224             nan     0.0234   -0.0001
  1860        0.0221             nan     0.0234   -0.0001
  1880        0.0217             nan     0.0234   -0.0001
  1900        0.0214             nan     0.0234   -0.0001
  1920        0.0211             nan     0.0234   -0.0000
  1940        0.0208             nan     0.0234   -0.0002
  1960        0.0205             nan     0.0234   -0.0000
  1980        0.0203             nan     0.0234   -0.0001
  2000        0.0201             nan     0.0234   -0.0001
  2020        0.0198             nan     0.0234   -0.0000
  2040        0.0195             nan     0.0234   -0.0001
  2060        0.0193             nan     0.0234   -0.0001
  2080        0.0192             nan     0.0234   -0.0001
  2100        0.0189             nan     0.0234   -0.0001
  2120        0.0186             nan     0.0234   -0.0002
  2140        0.0184             nan     0.0234   -0.0000
  2160        0.0181             nan     0.0234   -0.0001
  2180        0.0179             nan     0.0234   -0.0000
  2200        0.0177             nan     0.0234   -0.0001
  2220        0.0174             nan     0.0234   -0.0001
  2240        0.0172             nan     0.0234   -0.0000
  2260        0.0170             nan     0.0234   -0.0001
  2280        0.0167             nan     0.0234   -0.0001
  2300        0.0166             nan     0.0234   -0.0001
  2320        0.0163             nan     0.0234   -0.0001
  2340        0.0162             nan     0.0234   -0.0002
  2360        0.0160             nan     0.0234   -0.0001
  2380        0.0158             nan     0.0234   -0.0001
  2400        0.0157             nan     0.0234   -0.0000
  2420        0.0155             nan     0.0234   -0.0001
  2440        0.0154             nan     0.0234   -0.0001
  2460        0.0152             nan     0.0234   -0.0001
  2480        0.0151             nan     0.0234   -0.0001
  2500        0.0149             nan     0.0234   -0.0002
  2520        0.0147             nan     0.0234   -0.0001
  2540        0.0145             nan     0.0234   -0.0001
  2560        0.0144             nan     0.0234   -0.0001
  2580        0.0141             nan     0.0234   -0.0001
  2600        0.0139             nan     0.0234   -0.0001
  2620        0.0137             nan     0.0234   -0.0001
  2640        0.0136             nan     0.0234   -0.0001
  2660        0.0134             nan     0.0234   -0.0001
  2680        0.0132             nan     0.0234   -0.0001
  2700        0.0130             nan     0.0234   -0.0000
  2720        0.0129             nan     0.0234   -0.0001
  2740        0.0127             nan     0.0234   -0.0001
  2760        0.0126             nan     0.0234   -0.0001
  2780        0.0125             nan     0.0234   -0.0001
  2800        0.0122             nan     0.0234   -0.0001
  2820        0.0121             nan     0.0234   -0.0002
  2840        0.0120             nan     0.0234   -0.0001
  2860        0.0119             nan     0.0234   -0.0001
  2880        0.0117             nan     0.0234   -0.0001
  2900        0.0116             nan     0.0234   -0.0001
  2920        0.0115             nan     0.0234   -0.0001
  2940        0.0113             nan     0.0234   -0.0001
  2960        0.0112             nan     0.0234   -0.0000
  2980        0.0110             nan     0.0234   -0.0001
  3000        0.0109             nan     0.0234   -0.0000
  3020        0.0108             nan     0.0234   -0.0000
  3040        0.0107             nan     0.0234   -0.0001
  3057        0.0106             nan     0.0234   -0.0001

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3606             nan     0.1203    0.0435
     2        0.3323             nan     0.1203    0.0328
     3        0.3043             nan     0.1203    0.0315
     4        0.2819             nan     0.1203    0.0147
     5        0.2565             nan     0.1203    0.0181
     6        0.2365             nan     0.1203    0.0184
     7        0.2198             nan     0.1203    0.0191
     8        0.1997             nan     0.1203    0.0066
     9        0.1898             nan     0.1203    0.0120
    10        0.1717             nan     0.1203    0.0061
    20        0.1146             nan     0.1203   -0.0031
    40        0.0852             nan     0.1203   -0.0011
    60        0.0683             nan     0.1203   -0.0012
    80        0.0587             nan     0.1203   -0.0015
   100        0.0514             nan     0.1203   -0.0017
   120        0.0459             nan     0.1203   -0.0006
   140        0.0410             nan     0.1203   -0.0005
   160        0.0376             nan     0.1203   -0.0007
   180        0.0335             nan     0.1203   -0.0020
   200        0.0293             nan     0.1203   -0.0006
   220        0.0264             nan     0.1203   -0.0008
   240        0.0251             nan     0.1203   -0.0006
   260        0.0230             nan     0.1203   -0.0005
   280        0.0218             nan     0.1203   -0.0007
   300        0.0205             nan     0.1203    0.0000
   320        0.0187             nan     0.1203   -0.0003
   340        0.0174             nan     0.1203   -0.0004
   360        0.0159             nan     0.1203   -0.0002
   380        0.0146             nan     0.1203   -0.0008
   400        0.0140             nan     0.1203   -0.0005
   420        0.0131             nan     0.1203   -0.0006
   440        0.0122             nan     0.1203   -0.0003
   460        0.0113             nan     0.1203   -0.0009
   480        0.0104             nan     0.1203   -0.0005
   500        0.0096             nan     0.1203   -0.0003
   520        0.0090             nan     0.1203   -0.0002
   540        0.0082             nan     0.1203   -0.0002
   560        0.0076             nan     0.1203   -0.0001
   580        0.0072             nan     0.1203   -0.0002
   600        0.0067             nan     0.1203   -0.0003
   620        0.0063             nan     0.1203   -0.0001
   640        0.0060             nan     0.1203   -0.0002
   660        0.0055             nan     0.1203   -0.0003
   680        0.0053             nan     0.1203   -0.0004
   700        0.0049             nan     0.1203   -0.0002
   720        0.0046             nan     0.1203   -0.0001
   740        0.0043             nan     0.1203   -0.0001
   760        0.0040             nan     0.1203   -0.0001
   780        0.0036             nan     0.1203   -0.0002
   800        0.0033             nan     0.1203   -0.0001
   820        0.0032             nan     0.1203   -0.0001
   837        0.0030             nan     0.1203   -0.0001

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3378             nan     0.2260    0.0642
     2        0.3043             nan     0.2260    0.0428
     3        0.2436             nan     0.2260    0.0206
     4        0.2121             nan     0.2260    0.0250
     5        0.2037             nan     0.2260   -0.0023
     6        0.1822             nan     0.2260    0.0115
     7        0.1717             nan     0.2260    0.0097
     8        0.1717             nan     0.2260   -0.0156
     9        0.1592             nan     0.2260    0.0019
    10        0.1511             nan     0.2260    0.0050
    20        0.1047             nan     0.2260   -0.0031
    40        0.0871             nan     0.2260   -0.0035
    60        0.0736             nan     0.2260   -0.0022
    80        0.0630             nan     0.2260   -0.0006
   100        0.0537             nan     0.2260   -0.0022
   120        0.0491             nan     0.2260   -0.0035
   140        0.0414             nan     0.2260   -0.0035
   160        0.0383             nan     0.2260   -0.0001
   180        0.0341             nan     0.2260   -0.0042
   200        0.0297             nan     0.2260   -0.0012
   220        0.0262             nan     0.2260   -0.0022
   240        0.0224             nan     0.2260   -0.0003
   260        0.0198             nan     0.2260   -0.0013
   280        0.0179             nan     0.2260   -0.0010
   300        0.0170             nan     0.2260   -0.0010
   320        0.0144             nan     0.2260   -0.0004
   340        0.0132             nan     0.2260   -0.0001
   360        0.0116             nan     0.2260   -0.0006
   380        0.0108             nan     0.2260   -0.0007
   400        0.0097             nan     0.2260   -0.0007
   420        0.0088             nan     0.2260   -0.0003
   440        0.0077             nan     0.2260   -0.0002
   460        0.0071             nan     0.2260   -0.0005
   480        0.0063             nan     0.2260   -0.0004
   500        0.0057             nan     0.2260   -0.0002
   520        0.0051             nan     0.2260   -0.0002
   540        0.0047             nan     0.2260   -0.0002
   560        0.0043             nan     0.2260   -0.0003
   580        0.0039             nan     0.2260   -0.0002
   600        0.0036             nan     0.2260   -0.0002
   620        0.0033             nan     0.2260   -0.0003
   640        0.0030             nan     0.2260   -0.0001
   660        0.0028             nan     0.2260   -0.0001
   680        0.0025             nan     0.2260   -0.0000
   700        0.0023             nan     0.2260   -0.0002
   720        0.0022             nan     0.2260   -0.0002
   740        0.0020             nan     0.2260   -0.0002
   760        0.0018             nan     0.2260   -0.0001
   780        0.0016             nan     0.2260   -0.0002
   800        0.0016             nan     0.2260   -0.0001
   820        0.0015             nan     0.2260   -0.0002
   840        0.0014             nan     0.2260   -0.0000
   860        0.0013             nan     0.2260   -0.0001
   880        0.0012             nan     0.2260   -0.0000
   900        0.0011             nan     0.2260   -0.0000
   920        0.0010             nan     0.2260   -0.0001
   940        0.0009             nan     0.2260   -0.0001
   960        0.0009             nan     0.2260   -0.0001
   980        0.0008             nan     0.2260   -0.0000
  1000        0.0007             nan     0.2260   -0.0001
  1020        0.0007             nan     0.2260   -0.0000
  1040        0.0006             nan     0.2260   -0.0000
  1060        0.0006             nan     0.2260   -0.0000
  1080        0.0005             nan     0.2260   -0.0000
  1100        0.0005             nan     0.2260   -0.0000
  1120        0.0005             nan     0.2260   -0.0000
  1140        0.0005             nan     0.2260   -0.0000
  1160        0.0004             nan     0.2260   -0.0000
  1180        0.0004             nan     0.2260   -0.0000
  1200        0.0003             nan     0.2260   -0.0000
  1220        0.0003             nan     0.2260   -0.0000
  1240        0.0003             nan     0.2260   -0.0000
  1260        0.0003             nan     0.2260   -0.0000
  1280        0.0003             nan     0.2260   -0.0000
  1300        0.0002             nan     0.2260   -0.0000
  1320        0.0002             nan     0.2260   -0.0000
  1340        0.0002             nan     0.2260   -0.0000
  1360        0.0002             nan     0.2260   -0.0000
  1380        0.0002             nan     0.2260   -0.0000
  1400        0.0002             nan     0.2260   -0.0000
  1420        0.0002             nan     0.2260   -0.0000
  1440        0.0002             nan     0.2260   -0.0000
  1460        0.0001             nan     0.2260   -0.0000
  1480        0.0001             nan     0.2260   -0.0000
  1500        0.0001             nan     0.2260   -0.0000
  1520        0.0001             nan     0.2260   -0.0000
  1540        0.0001             nan     0.2260   -0.0000
  1560        0.0001             nan     0.2260   -0.0000
  1580        0.0001             nan     0.2260   -0.0000
  1600        0.0001             nan     0.2260   -0.0000
  1620        0.0001             nan     0.2260   -0.0000
  1640        0.0001             nan     0.2260   -0.0000
  1660        0.0001             nan     0.2260   -0.0000
  1680        0.0001             nan     0.2260   -0.0000
  1700        0.0001             nan     0.2260   -0.0000
  1720        0.0001             nan     0.2260   -0.0000
  1740        0.0000             nan     0.2260   -0.0000
  1760        0.0000             nan     0.2260   -0.0000
  1780        0.0000             nan     0.2260   -0.0000
  1800        0.0000             nan     0.2260   -0.0000
  1820        0.0000             nan     0.2260   -0.0000
  1840        0.0000             nan     0.2260   -0.0000
  1860        0.0000             nan     0.2260   -0.0000
  1880        0.0000             nan     0.2260   -0.0000
  1900        0.0000             nan     0.2260   -0.0000
  1920        0.0000             nan     0.2260   -0.0000
  1940        0.0000             nan     0.2260   -0.0000
  1960        0.0000             nan     0.2260   -0.0000
  1980        0.0000             nan     0.2260   -0.0000
  2000        0.0000             nan     0.2260   -0.0000
  2020        0.0000             nan     0.2260   -0.0000
  2040        0.0000             nan     0.2260   -0.0000
  2060        0.0000             nan     0.2260   -0.0000
  2080        0.0000             nan     0.2260   -0.0000
  2100        0.0000             nan     0.2260   -0.0000
  2120        0.0000             nan     0.2260   -0.0000
  2140        0.0000             nan     0.2260   -0.0000
  2160        0.0000             nan     0.2260   -0.0000
  2180        0.0000             nan     0.2260   -0.0000
  2200        0.0000             nan     0.2260   -0.0000
  2220        0.0000             nan     0.2260   -0.0000
  2240        0.0000             nan     0.2260   -0.0000
  2260        0.0000             nan     0.2260   -0.0000
  2280        0.0000             nan     0.2260   -0.0000
  2300        0.0000             nan     0.2260   -0.0000
  2320        0.0000             nan     0.2260   -0.0000
  2340        0.0000             nan     0.2260   -0.0000
  2360        0.0000             nan     0.2260   -0.0000
  2380        0.0000             nan     0.2260   -0.0000
  2400        0.0000             nan     0.2260   -0.0000
  2420        0.0000             nan     0.2260   -0.0000
  2440        0.0000             nan     0.2260   -0.0000
  2460        0.0000             nan     0.2260   -0.0000
  2480        0.0000             nan     0.2260   -0.0000
  2500        0.0000             nan     0.2260   -0.0000
  2520        0.0000             nan     0.2260   -0.0000
  2540        0.0000             nan     0.2260   -0.0000
  2560        0.0000             nan     0.2260   -0.0000
  2580        0.0000             nan     0.2260   -0.0000
  2600        0.0000             nan     0.2260   -0.0000
  2620        0.0000             nan     0.2260   -0.0000
  2640        0.0000             nan     0.2260   -0.0000
  2660        0.0000             nan     0.2260   -0.0000
  2680        0.0000             nan     0.2260   -0.0000
  2700        0.0000             nan     0.2260   -0.0000
  2720        0.0000             nan     0.2260   -0.0000
  2740        0.0000             nan     0.2260   -0.0000
  2760        0.0000             nan     0.2260   -0.0000
  2780        0.0000             nan     0.2260   -0.0000
  2790        0.0000             nan     0.2260   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3287             nan     0.2340    0.0679
     2        0.2737             nan     0.2340    0.0310
     3        0.2538             nan     0.2340    0.0269
     4        0.2334             nan     0.2340    0.0104
     5        0.2162             nan     0.2340    0.0014
     6        0.2202             nan     0.2340   -0.0180
     7        0.2152             nan     0.2340    0.0032
     8        0.2011             nan     0.2340   -0.0003
     9        0.2047             nan     0.2340   -0.0172
    10        0.1982             nan     0.2340    0.0074
    20        0.1656             nan     0.2340    0.0007
    40        0.1284             nan     0.2340   -0.0023
    60        0.1088             nan     0.2340   -0.0031
    80        0.1007             nan     0.2340   -0.0094
   100        0.0862             nan     0.2340   -0.0019
   120        0.0747             nan     0.2340   -0.0035
   140        0.0647             nan     0.2340   -0.0029
   160        0.0629             nan     0.2340   -0.0020
   180        0.0593             nan     0.2340   -0.0055
   200        0.0528             nan     0.2340   -0.0030
   220        0.0485             nan     0.2340   -0.0024
   240        0.0435             nan     0.2340   -0.0017
   260        0.0408             nan     0.2340   -0.0046
   280        0.0387             nan     0.2340   -0.0008
   300        0.0366             nan     0.2340   -0.0027
   320        0.0338             nan     0.2340   -0.0022
   340        0.0300             nan     0.2340   -0.0007
   360        0.0282             nan     0.2340   -0.0013
   380        0.0258             nan     0.2340   -0.0020
   400        0.0234             nan     0.2340   -0.0016
   420        0.0217             nan     0.2340   -0.0012
   440        0.0201             nan     0.2340   -0.0005
   460        0.0188             nan     0.2340   -0.0008
   480        0.0178             nan     0.2340   -0.0005
   500        0.0165             nan     0.2340   -0.0004
   520        0.0154             nan     0.2340   -0.0004
   540        0.0141             nan     0.2340   -0.0010
   560        0.0136             nan     0.2340   -0.0009
   580        0.0122             nan     0.2340   -0.0007
   600        0.0114             nan     0.2340   -0.0006
   620        0.0112             nan     0.2340   -0.0002
   640        0.0109             nan     0.2340   -0.0004
   660        0.0106             nan     0.2340   -0.0007
   680        0.0099             nan     0.2340   -0.0001
   700        0.0097             nan     0.2340   -0.0009
   720        0.0092             nan     0.2340   -0.0002
   740        0.0088             nan     0.2340   -0.0008
   760        0.0083             nan     0.2340   -0.0005
   780        0.0078             nan     0.2340   -0.0006
   800        0.0074             nan     0.2340   -0.0004
   820        0.0072             nan     0.2340   -0.0004
   840        0.0068             nan     0.2340   -0.0001
   860        0.0064             nan     0.2340   -0.0001
   880        0.0061             nan     0.2340   -0.0003
   900        0.0057             nan     0.2340   -0.0004
   920        0.0056             nan     0.2340   -0.0001
   940        0.0056             nan     0.2340   -0.0002
   960        0.0053             nan     0.2340   -0.0004
   980        0.0052             nan     0.2340   -0.0003
  1000        0.0050             nan     0.2340   -0.0003
  1020        0.0045             nan     0.2340   -0.0001
  1040        0.0045             nan     0.2340   -0.0001
  1060        0.0043             nan     0.2340   -0.0003
  1080        0.0042             nan     0.2340   -0.0001
  1100        0.0041             nan     0.2340   -0.0001
  1120        0.0040             nan     0.2340   -0.0004
  1140        0.0039             nan     0.2340   -0.0003
  1160        0.0037             nan     0.2340   -0.0001
  1180        0.0037             nan     0.2340   -0.0003
  1200        0.0035             nan     0.2340   -0.0002
  1220        0.0035             nan     0.2340   -0.0002
  1240        0.0034             nan     0.2340   -0.0000
  1260        0.0033             nan     0.2340   -0.0001
  1280        0.0032             nan     0.2340   -0.0001
  1300        0.0031             nan     0.2340   -0.0002
  1320        0.0031             nan     0.2340   -0.0001
  1340        0.0030             nan     0.2340   -0.0001
  1360        0.0029             nan     0.2340   -0.0002
  1380        0.0028             nan     0.2340   -0.0000
  1400        0.0027             nan     0.2340   -0.0001
  1420        0.0026             nan     0.2340   -0.0002
  1440        0.0025             nan     0.2340   -0.0001
  1460        0.0023             nan     0.2340   -0.0000
  1480        0.0022             nan     0.2340   -0.0001
  1500        0.0022             nan     0.2340   -0.0001
  1520        0.0021             nan     0.2340   -0.0001
  1540        0.0021             nan     0.2340   -0.0002
  1560        0.0020             nan     0.2340   -0.0001
  1580        0.0021             nan     0.2340   -0.0002
  1600        0.0020             nan     0.2340   -0.0001
  1620        0.0020             nan     0.2340   -0.0001
  1640        0.0019             nan     0.2340   -0.0001
  1660        0.0019             nan     0.2340   -0.0001
  1680        0.0018             nan     0.2340   -0.0000
  1700        0.0017             nan     0.2340   -0.0001
  1720        0.0017             nan     0.2340   -0.0000
  1740        0.0016             nan     0.2340   -0.0001
  1760        0.0016             nan     0.2340   -0.0001
  1780        0.0015             nan     0.2340   -0.0001
  1800        0.0015             nan     0.2340   -0.0001
  1820        0.0014             nan     0.2340   -0.0001
  1840        0.0014             nan     0.2340   -0.0000
  1860        0.0014             nan     0.2340   -0.0000
  1880        0.0013             nan     0.2340   -0.0001
  1900        0.0013             nan     0.2340   -0.0001
  1920        0.0012             nan     0.2340   -0.0001
  1940        0.0012             nan     0.2340   -0.0000
  1960        0.0013             nan     0.2340   -0.0000
  1980        0.0012             nan     0.2340   -0.0000
  2000        0.0011             nan     0.2340   -0.0001
  2020        0.0011             nan     0.2340    0.0000
  2040        0.0011             nan     0.2340   -0.0001
  2060        0.0010             nan     0.2340   -0.0000
  2080        0.0010             nan     0.2340   -0.0000
  2100        0.0009             nan     0.2340   -0.0000
  2120        0.0009             nan     0.2340   -0.0000
  2140        0.0009             nan     0.2340   -0.0001
  2160        0.0008             nan     0.2340   -0.0000
  2180        0.0008             nan     0.2340   -0.0000
  2200        0.0008             nan     0.2340   -0.0001
  2220        0.0008             nan     0.2340   -0.0000
  2240        0.0008             nan     0.2340   -0.0000
  2260        0.0007             nan     0.2340   -0.0000
  2280        0.0007             nan     0.2340   -0.0000
  2300        0.0007             nan     0.2340   -0.0000
  2320        0.0006             nan     0.2340   -0.0000
  2340        0.0006             nan     0.2340   -0.0000
  2360        0.0006             nan     0.2340   -0.0000
  2380        0.0006             nan     0.2340   -0.0000
  2400        0.0006             nan     0.2340   -0.0000
  2420        0.0006             nan     0.2340   -0.0000
  2440        0.0005             nan     0.2340   -0.0000
  2460        0.0005             nan     0.2340   -0.0000
  2480        0.0005             nan     0.2340   -0.0000
  2500        0.0005             nan     0.2340   -0.0000
  2520        0.0005             nan     0.2340   -0.0000
  2540        0.0004             nan     0.2340   -0.0000
  2560        0.0004             nan     0.2340   -0.0000
  2580        0.0004             nan     0.2340   -0.0000
  2600        0.0004             nan     0.2340   -0.0000
  2620        0.0004             nan     0.2340   -0.0000
  2640        0.0004             nan     0.2340   -0.0000
  2660        0.0004             nan     0.2340   -0.0000
  2680        0.0004             nan     0.2340   -0.0000
  2700        0.0004             nan     0.2340   -0.0000
  2720        0.0003             nan     0.2340   -0.0000
  2740        0.0003             nan     0.2340   -0.0000
  2760        0.0003             nan     0.2340   -0.0000
  2780        0.0003             nan     0.2340   -0.0000
  2800        0.0003             nan     0.2340   -0.0000
  2820        0.0003             nan     0.2340   -0.0000
  2840        0.0003             nan     0.2340   -0.0000
  2860        0.0003             nan     0.2340    0.0000
  2880        0.0003             nan     0.2340   -0.0000
  2900        0.0003             nan     0.2340   -0.0000
  2920        0.0002             nan     0.2340   -0.0000
  2940        0.0002             nan     0.2340   -0.0000
  2960        0.0002             nan     0.2340   -0.0000
  2980        0.0002             nan     0.2340   -0.0000
  3000        0.0002             nan     0.2340   -0.0000
  3020        0.0002             nan     0.2340    0.0000
  3040        0.0002             nan     0.2340   -0.0000
  3060        0.0002             nan     0.2340   -0.0000
  3080        0.0002             nan     0.2340   -0.0000
  3100        0.0002             nan     0.2340   -0.0000
  3120        0.0002             nan     0.2340   -0.0000
  3140        0.0002             nan     0.2340   -0.0000
  3160        0.0002             nan     0.2340   -0.0000
  3180        0.0002             nan     0.2340   -0.0000
  3200        0.0002             nan     0.2340   -0.0000
  3220        0.0002             nan     0.2340   -0.0000
  3240        0.0002             nan     0.2340   -0.0000
  3260        0.0001             nan     0.2340   -0.0000
  3280        0.0001             nan     0.2340   -0.0000
  3300        0.0001             nan     0.2340   -0.0000
  3320        0.0001             nan     0.2340   -0.0000
  3340        0.0001             nan     0.2340   -0.0000
  3360        0.0001             nan     0.2340   -0.0000
  3380        0.0001             nan     0.2340   -0.0000
  3400        0.0001             nan     0.2340   -0.0000
  3420        0.0001             nan     0.2340   -0.0000
  3440        0.0001             nan     0.2340   -0.0000
  3460        0.0001             nan     0.2340   -0.0000
  3480        0.0001             nan     0.2340   -0.0000
  3500        0.0001             nan     0.2340   -0.0000
  3520        0.0001             nan     0.2340   -0.0000
  3540        0.0001             nan     0.2340   -0.0000
  3560        0.0001             nan     0.2340   -0.0000
  3580        0.0001             nan     0.2340   -0.0000
  3600        0.0001             nan     0.2340   -0.0000
  3620        0.0001             nan     0.2340   -0.0000
  3640        0.0001             nan     0.2340   -0.0000
  3660        0.0001             nan     0.2340   -0.0000
  3680        0.0001             nan     0.2340   -0.0000
  3700        0.0001             nan     0.2340   -0.0000
  3720        0.0001             nan     0.2340   -0.0000
  3740        0.0001             nan     0.2340   -0.0000
  3760        0.0001             nan     0.2340   -0.0000
  3780        0.0001             nan     0.2340   -0.0000
  3800        0.0001             nan     0.2340   -0.0000
  3820        0.0001             nan     0.2340   -0.0000
  3840        0.0001             nan     0.2340   -0.0000
  3860        0.0001             nan     0.2340   -0.0000
  3880        0.0000             nan     0.2340   -0.0000
  3900        0.0000             nan     0.2340   -0.0000
  3920        0.0000             nan     0.2340   -0.0000
  3924        0.0000             nan     0.2340   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2941             nan     0.2895    0.0867
     2        0.2294             nan     0.2895    0.0539
     3        0.1913             nan     0.2895    0.0312
     4        0.1658             nan     0.2895    0.0170
     5        0.1432             nan     0.2895    0.0265
     6        0.1286             nan     0.2895    0.0136
     7        0.1204             nan     0.2895    0.0065
     8        0.1158             nan     0.2895    0.0068
     9        0.1138             nan     0.2895    0.0026
    10        0.1094             nan     0.2895   -0.0160
    20        0.0785             nan     0.2895   -0.0026
    40        0.0550             nan     0.2895   -0.0034
    60        0.0383             nan     0.2895   -0.0023
    80        0.0284             nan     0.2895   -0.0008
   100        0.0211             nan     0.2895   -0.0018
   120        0.0170             nan     0.2895   -0.0016
   140        0.0143             nan     0.2895   -0.0010
   160        0.0122             nan     0.2895   -0.0019
   180        0.0105             nan     0.2895   -0.0013
   200        0.0084             nan     0.2895   -0.0009
   220        0.0062             nan     0.2895   -0.0003
   240        0.0046             nan     0.2895   -0.0003
   260        0.0038             nan     0.2895   -0.0004
   280        0.0030             nan     0.2895   -0.0003
   300        0.0027             nan     0.2895   -0.0005
   320        0.0018             nan     0.2895   -0.0002
   340        0.0014             nan     0.2895   -0.0001
   360        0.0013             nan     0.2895   -0.0001
   380        0.0010             nan     0.2895   -0.0001
   400        0.0008             nan     0.2895   -0.0001
   420        0.0006             nan     0.2895   -0.0000
   440        0.0005             nan     0.2895   -0.0000
   460        0.0004             nan     0.2895   -0.0001
   480        0.0003             nan     0.2895   -0.0000
   500        0.0003             nan     0.2895   -0.0000
   520        0.0002             nan     0.2895   -0.0000
   540        0.0002             nan     0.2895   -0.0000
   560        0.0001             nan     0.2895   -0.0000
   580        0.0001             nan     0.2895   -0.0000
   600        0.0001             nan     0.2895   -0.0000
   620        0.0001             nan     0.2895   -0.0000
   640        0.0001             nan     0.2895   -0.0000
   660        0.0001             nan     0.2895   -0.0000
   680        0.0001             nan     0.2895   -0.0000
   700        0.0000             nan     0.2895   -0.0000
   720        0.0000             nan     0.2895   -0.0000
   740        0.0000             nan     0.2895   -0.0000
   760        0.0000             nan     0.2895   -0.0000
   780        0.0000             nan     0.2895   -0.0000
   800        0.0000             nan     0.2895   -0.0000
   820        0.0000             nan     0.2895   -0.0000
   840        0.0000             nan     0.2895   -0.0000
   860        0.0000             nan     0.2895   -0.0000
   880        0.0000             nan     0.2895   -0.0000
   900        0.0000             nan     0.2895   -0.0000
   920        0.0000             nan     0.2895   -0.0000
   940        0.0000             nan     0.2895   -0.0000
   960        0.0000             nan     0.2895   -0.0000
   980        0.0000             nan     0.2895   -0.0000
  1000        0.0000             nan     0.2895   -0.0000
  1020        0.0000             nan     0.2895   -0.0000
  1040        0.0000             nan     0.2895   -0.0000
  1060        0.0000             nan     0.2895   -0.0000
  1080        0.0000             nan     0.2895   -0.0000
  1100        0.0000             nan     0.2895   -0.0000
  1120        0.0000             nan     0.2895   -0.0000
  1140        0.0000             nan     0.2895   -0.0000
  1160        0.0000             nan     0.2895   -0.0000
  1180        0.0000             nan     0.2895   -0.0000
  1200        0.0000             nan     0.2895   -0.0000
  1220        0.0000             nan     0.2895   -0.0000
  1240        0.0000             nan     0.2895   -0.0000
  1260        0.0000             nan     0.2895   -0.0000
  1280        0.0000             nan     0.2895   -0.0000
  1300        0.0000             nan     0.2895   -0.0000
  1320        0.0000             nan     0.2895   -0.0000
  1340        0.0000             nan     0.2895   -0.0000
  1360        0.0000             nan     0.2895   -0.0000
  1380        0.0000             nan     0.2895   -0.0000
  1400        0.0000             nan     0.2895   -0.0000
  1420        0.0000             nan     0.2895   -0.0000
  1440        0.0000             nan     0.2895   -0.0000
  1460        0.0000             nan     0.2895   -0.0000
  1480        0.0000             nan     0.2895   -0.0000
  1500        0.0000             nan     0.2895   -0.0000
  1520        0.0000             nan     0.2895   -0.0000
  1540        0.0000             nan     0.2895   -0.0000
  1560        0.0000             nan     0.2895   -0.0000
  1580        0.0000             nan     0.2895   -0.0000
  1600        0.0000             nan     0.2895   -0.0000
  1620        0.0000             nan     0.2895   -0.0000
  1640        0.0000             nan     0.2895   -0.0000
  1660        0.0000             nan     0.2895   -0.0000
  1680        0.0000             nan     0.2895   -0.0000
  1700        0.0000             nan     0.2895   -0.0000
  1720        0.0000             nan     0.2895   -0.0000
  1740        0.0000             nan     0.2895   -0.0000
  1760        0.0000             nan     0.2895   -0.0000
  1780        0.0000             nan     0.2895   -0.0000
  1800        0.0000             nan     0.2895   -0.0000
  1820        0.0000             nan     0.2895   -0.0000
  1840        0.0000             nan     0.2895   -0.0000
  1860        0.0000             nan     0.2895   -0.0000
  1880        0.0000             nan     0.2895   -0.0000
  1900        0.0000             nan     0.2895   -0.0000
  1920        0.0000             nan     0.2895   -0.0000
  1940        0.0000             nan     0.2895   -0.0000
  1960        0.0000             nan     0.2895   -0.0000
  1980        0.0000             nan     0.2895   -0.0000
  2000        0.0000             nan     0.2895   -0.0000
  2020        0.0000             nan     0.2895   -0.0000
  2040        0.0000             nan     0.2895   -0.0000
  2060        0.0000             nan     0.2895   -0.0000
  2080        0.0000             nan     0.2895   -0.0000
  2100        0.0000             nan     0.2895   -0.0000
  2120        0.0000             nan     0.2895   -0.0000
  2140        0.0000             nan     0.2895   -0.0000
  2160        0.0000             nan     0.2895   -0.0000
  2180        0.0000             nan     0.2895   -0.0000
  2200        0.0000             nan     0.2895   -0.0000
  2220        0.0000             nan     0.2895   -0.0000
  2240        0.0000             nan     0.2895   -0.0000
  2260        0.0000             nan     0.2895   -0.0000
  2280        0.0000             nan     0.2895   -0.0000
  2300        0.0000             nan     0.2895   -0.0000
  2320        0.0000             nan     0.2895   -0.0000
  2340        0.0000             nan     0.2895   -0.0000
  2360        0.0000             nan     0.2895   -0.0000
  2380        0.0000             nan     0.2895   -0.0000
  2400        0.0000             nan     0.2895   -0.0000
  2420        0.0000             nan     0.2895   -0.0000
  2440        0.0000             nan     0.2895   -0.0000
  2460        0.0000             nan     0.2895   -0.0000
  2480        0.0000             nan     0.2895   -0.0000
  2500        0.0000             nan     0.2895   -0.0000
  2520        0.0000             nan     0.2895    0.0000
  2540        0.0000             nan     0.2895   -0.0000
  2560        0.0000             nan     0.2895   -0.0000
  2580        0.0000             nan     0.2895   -0.0000
  2600        0.0000             nan     0.2895   -0.0000
  2620        0.0000             nan     0.2895   -0.0000
  2640        0.0000             nan     0.2895   -0.0000
  2660        0.0000             nan     0.2895   -0.0000
  2680        0.0000             nan     0.2895   -0.0000
  2700        0.0000             nan     0.2895   -0.0000
  2720        0.0000             nan     0.2895   -0.0000
  2740        0.0000             nan     0.2895   -0.0000
  2760        0.0000             nan     0.2895   -0.0000
  2780        0.0000             nan     0.2895   -0.0000
  2800        0.0000             nan     0.2895   -0.0000
  2820        0.0000             nan     0.2895   -0.0000
  2840        0.0000             nan     0.2895   -0.0000
  2860        0.0000             nan     0.2895   -0.0000
  2880        0.0000             nan     0.2895   -0.0000
  2900        0.0000             nan     0.2895   -0.0000
  2920        0.0000             nan     0.2895   -0.0000
  2940        0.0000             nan     0.2895   -0.0000
  2960        0.0000             nan     0.2895   -0.0000
  2980        0.0000             nan     0.2895   -0.0000
  3000        0.0000             nan     0.2895   -0.0000
  3020        0.0000             nan     0.2895   -0.0000
  3040        0.0000             nan     0.2895   -0.0000
  3060        0.0000             nan     0.2895   -0.0000
  3080        0.0000             nan     0.2895   -0.0000
  3100        0.0000             nan     0.2895   -0.0000
  3120        0.0000             nan     0.2895   -0.0000
  3140        0.0000             nan     0.2895   -0.0000
  3160        0.0000             nan     0.2895   -0.0000
  3180        0.0000             nan     0.2895   -0.0000
  3200        0.0000             nan     0.2895   -0.0000
  3220        0.0000             nan     0.2895   -0.0000
  3240        0.0000             nan     0.2895   -0.0000
  3260        0.0000             nan     0.2895   -0.0000
  3280        0.0000             nan     0.2895   -0.0000
  3300        0.0000             nan     0.2895   -0.0000
  3320        0.0000             nan     0.2895   -0.0000
  3340        0.0000             nan     0.2895   -0.0000
  3360        0.0000             nan     0.2895   -0.0000
  3380        0.0000             nan     0.2895   -0.0000
  3400        0.0000             nan     0.2895   -0.0000
  3420        0.0000             nan     0.2895   -0.0000
  3440        0.0000             nan     0.2895   -0.0000
  3460        0.0000             nan     0.2895   -0.0000
  3480        0.0000             nan     0.2895   -0.0000
  3500        0.0000             nan     0.2895   -0.0000
  3520        0.0000             nan     0.2895   -0.0000
  3540        0.0000             nan     0.2895   -0.0000
  3560        0.0000             nan     0.2895   -0.0000
  3580        0.0000             nan     0.2895   -0.0000
  3600        0.0000             nan     0.2895   -0.0000
  3620        0.0000             nan     0.2895   -0.0000
  3640        0.0000             nan     0.2895   -0.0000
  3660        0.0000             nan     0.2895   -0.0000
  3680        0.0000             nan     0.2895   -0.0000
  3700        0.0000             nan     0.2895   -0.0000
  3720        0.0000             nan     0.2895   -0.0000
  3740        0.0000             nan     0.2895   -0.0000
  3760        0.0000             nan     0.2895   -0.0000
  3780        0.0000             nan     0.2895   -0.0000
  3800        0.0000             nan     0.2895   -0.0000
  3820        0.0000             nan     0.2895   -0.0000
  3840        0.0000             nan     0.2895   -0.0000
  3860        0.0000             nan     0.2895   -0.0000
  3880        0.0000             nan     0.2895   -0.0000
  3900        0.0000             nan     0.2895   -0.0000
  3920        0.0000             nan     0.2895   -0.0000
  3940        0.0000             nan     0.2895   -0.0000
  3960        0.0000             nan     0.2895   -0.0000
  3980        0.0000             nan     0.2895   -0.0000
  4000        0.0000             nan     0.2895   -0.0000
  4020        0.0000             nan     0.2895   -0.0000
  4040        0.0000             nan     0.2895   -0.0000
  4060        0.0000             nan     0.2895   -0.0000
  4080        0.0000             nan     0.2895   -0.0000
  4100        0.0000             nan     0.2895   -0.0000
  4120        0.0000             nan     0.2895   -0.0000
  4140        0.0000             nan     0.2895   -0.0000
  4160        0.0000             nan     0.2895   -0.0000
  4180        0.0000             nan     0.2895   -0.0000
  4200        0.0000             nan     0.2895   -0.0000
  4220        0.0000             nan     0.2895   -0.0000
  4240        0.0000             nan     0.2895   -0.0000
  4260        0.0000             nan     0.2895   -0.0000
  4280        0.0000             nan     0.2895   -0.0000
  4300        0.0000             nan     0.2895   -0.0000
  4320        0.0000             nan     0.2895   -0.0000
  4340        0.0000             nan     0.2895   -0.0000
  4360        0.0000             nan     0.2895   -0.0000
  4373        0.0000             nan     0.2895   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2343             nan     0.3621    0.1258
     2        0.1713             nan     0.3621    0.0686
     3        0.1397             nan     0.3621    0.0198
     4        0.1388             nan     0.3621   -0.0097
     5        0.1338             nan     0.3621    0.0043
     6        0.1144             nan     0.3621    0.0000
     7        0.0992             nan     0.3621    0.0021
     8        0.0884             nan     0.3621   -0.0012
     9        0.0850             nan     0.3621   -0.0075
    10        0.0866             nan     0.3621   -0.0147
    20        0.0454             nan     0.3621   -0.0036
    40        0.0180             nan     0.3621   -0.0016
    60        0.0079             nan     0.3621   -0.0008
    80        0.0028             nan     0.3621   -0.0004
   100        0.0011             nan     0.3621   -0.0000
   120        0.0004             nan     0.3621   -0.0001
   140        0.0002             nan     0.3621   -0.0000
   160        0.0001             nan     0.3621   -0.0000
   180        0.0000             nan     0.3621   -0.0000
   200        0.0000             nan     0.3621   -0.0000
   220        0.0000             nan     0.3621   -0.0000
   240        0.0000             nan     0.3621   -0.0000
   260        0.0000             nan     0.3621   -0.0000
   280        0.0000             nan     0.3621   -0.0000
   300        0.0000             nan     0.3621   -0.0000
   320        0.0000             nan     0.3621   -0.0000
   340        0.0000             nan     0.3621   -0.0000
   360        0.0000             nan     0.3621   -0.0000
   380        0.0000             nan     0.3621   -0.0000
   400        0.0000             nan     0.3621   -0.0000
   420        0.0000             nan     0.3621   -0.0000
   440        0.0000             nan     0.3621   -0.0000
   460        0.0000             nan     0.3621   -0.0000
   480        0.0000             nan     0.3621   -0.0000
   500        0.0000             nan     0.3621   -0.0000
   520        0.0000             nan     0.3621   -0.0000
   540        0.0000             nan     0.3621   -0.0000
   560        0.0000             nan     0.3621   -0.0000
   580        0.0000             nan     0.3621   -0.0000
   600        0.0000             nan     0.3621   -0.0000
   620        0.0000             nan     0.3621   -0.0000
   640        0.0000             nan     0.3621   -0.0000
   660        0.0000             nan     0.3621   -0.0000
   680        0.0000             nan     0.3621   -0.0000
   700        0.0000             nan     0.3621   -0.0000
   720        0.0000             nan     0.3621   -0.0000
   740        0.0000             nan     0.3621   -0.0000
   760        0.0000             nan     0.3621   -0.0000
   780        0.0000             nan     0.3621   -0.0000
   800        0.0000             nan     0.3621   -0.0000
   820        0.0000             nan     0.3621   -0.0000
   840        0.0000             nan     0.3621   -0.0000
   860        0.0000             nan     0.3621    0.0000
   880        0.0000             nan     0.3621   -0.0000
   900        0.0000             nan     0.3621   -0.0000
   920        0.0000             nan     0.3621    0.0000
   940        0.0000             nan     0.3621   -0.0000
   960        0.0000             nan     0.3621   -0.0000
   980        0.0000             nan     0.3621   -0.0000
  1000        0.0000             nan     0.3621   -0.0000
  1020        0.0000             nan     0.3621   -0.0000
  1040        0.0000             nan     0.3621   -0.0000
  1060        0.0000             nan     0.3621   -0.0000
  1080        0.0000             nan     0.3621   -0.0000
  1100        0.0000             nan     0.3621   -0.0000
  1120        0.0000             nan     0.3621   -0.0000
  1140        0.0000             nan     0.3621   -0.0000
  1160        0.0000             nan     0.3621   -0.0000
  1180        0.0000             nan     0.3621   -0.0000
  1200        0.0000             nan     0.3621   -0.0000
  1220        0.0000             nan     0.3621   -0.0000
  1240        0.0000             nan     0.3621   -0.0000
  1260        0.0000             nan     0.3621   -0.0000
  1280        0.0000             nan     0.3621   -0.0000
  1300        0.0000             nan     0.3621   -0.0000
  1320        0.0000             nan     0.3621   -0.0000
  1340        0.0000             nan     0.3621   -0.0000
  1360        0.0000             nan     0.3621   -0.0000
  1380        0.0000             nan     0.3621   -0.0000
  1400        0.0000             nan     0.3621   -0.0000
  1420        0.0000             nan     0.3621   -0.0000
  1440        0.0000             nan     0.3621   -0.0000
  1460        0.0000             nan     0.3621   -0.0000
  1480        0.0000             nan     0.3621   -0.0000
  1500        0.0000             nan     0.3621   -0.0000
  1520        0.0000             nan     0.3621   -0.0000
  1540        0.0000             nan     0.3621   -0.0000
  1560        0.0000             nan     0.3621   -0.0000
  1580        0.0000             nan     0.3621   -0.0000
  1600        0.0000             nan     0.3621   -0.0000
  1620        0.0000             nan     0.3621   -0.0000
  1640        0.0000             nan     0.3621   -0.0000
  1660        0.0000             nan     0.3621   -0.0000
  1680        0.0000             nan     0.3621   -0.0000
  1700        0.0000             nan     0.3621   -0.0000
  1720        0.0000             nan     0.3621   -0.0000
  1740        0.0000             nan     0.3621   -0.0000
  1760        0.0000             nan     0.3621   -0.0000
  1780        0.0000             nan     0.3621   -0.0000
  1800        0.0000             nan     0.3621    0.0000
  1820        0.0000             nan     0.3621   -0.0000
  1840        0.0000             nan     0.3621    0.0000
  1860        0.0000             nan     0.3621   -0.0000
  1880        0.0000             nan     0.3621   -0.0000
  1900        0.0000             nan     0.3621   -0.0000
  1920        0.0000             nan     0.3621   -0.0000
  1940        0.0000             nan     0.3621   -0.0000
  1960        0.0000             nan     0.3621   -0.0000
  1980        0.0000             nan     0.3621   -0.0000
  2000        0.0000             nan     0.3621   -0.0000
  2020        0.0000             nan     0.3621   -0.0000
  2040        0.0000             nan     0.3621   -0.0000
  2060        0.0000             nan     0.3621   -0.0000
  2080        0.0000             nan     0.3621   -0.0000
  2100        0.0000             nan     0.3621   -0.0000
  2120        0.0000             nan     0.3621   -0.0000
  2140        0.0000             nan     0.3621   -0.0000
  2160        0.0000             nan     0.3621   -0.0000
  2180        0.0000             nan     0.3621   -0.0000
  2200        0.0000             nan     0.3621   -0.0000
  2220        0.0000             nan     0.3621   -0.0000
  2240        0.0000             nan     0.3621   -0.0000
  2260        0.0000             nan     0.3621   -0.0000
  2280        0.0000             nan     0.3621   -0.0000
  2300        0.0000             nan     0.3621   -0.0000
  2320        0.0000             nan     0.3621   -0.0000
  2340        0.0000             nan     0.3621   -0.0000
  2360        0.0000             nan     0.3621   -0.0000
  2380        0.0000             nan     0.3621   -0.0000
  2400        0.0000             nan     0.3621   -0.0000
  2420        0.0000             nan     0.3621   -0.0000
  2440        0.0000             nan     0.3621   -0.0000
  2460        0.0000             nan     0.3621   -0.0000
  2480        0.0000             nan     0.3621   -0.0000
  2500        0.0000             nan     0.3621   -0.0000
  2520        0.0000             nan     0.3621   -0.0000
  2540        0.0000             nan     0.3621   -0.0000
  2560        0.0000             nan     0.3621   -0.0000
  2580        0.0000             nan     0.3621   -0.0000
  2600        0.0000             nan     0.3621   -0.0000
  2620        0.0000             nan     0.3621   -0.0000
  2640        0.0000             nan     0.3621   -0.0000
  2660        0.0000             nan     0.3621   -0.0000
  2680        0.0000             nan     0.3621   -0.0000
  2700        0.0000             nan     0.3621   -0.0000
  2720        0.0000             nan     0.3621   -0.0000
  2740        0.0000             nan     0.3621   -0.0000
  2760        0.0000             nan     0.3621   -0.0000
  2780        0.0000             nan     0.3621   -0.0000
  2800        0.0000             nan     0.3621   -0.0000
  2820        0.0000             nan     0.3621   -0.0000
  2840        0.0000             nan     0.3621   -0.0000
  2860        0.0000             nan     0.3621   -0.0000
  2880        0.0000             nan     0.3621   -0.0000
  2900        0.0000             nan     0.3621   -0.0000
  2920        0.0000             nan     0.3621   -0.0000
  2940        0.0000             nan     0.3621   -0.0000
  2960        0.0000             nan     0.3621    0.0000
  2980        0.0000             nan     0.3621   -0.0000
  3000        0.0000             nan     0.3621   -0.0000
  3020        0.0000             nan     0.3621    0.0000
  3040        0.0000             nan     0.3621   -0.0000
  3060        0.0000             nan     0.3621   -0.0000
  3080        0.0000             nan     0.3621   -0.0000
  3100        0.0000             nan     0.3621   -0.0000
  3120        0.0000             nan     0.3621   -0.0000
  3140        0.0000             nan     0.3621   -0.0000
  3160        0.0000             nan     0.3621   -0.0000
  3180        0.0000             nan     0.3621   -0.0000
  3200        0.0000             nan     0.3621   -0.0000
  3220        0.0000             nan     0.3621   -0.0000
  3240        0.0000             nan     0.3621   -0.0000
  3260        0.0000             nan     0.3621   -0.0000
  3280        0.0000             nan     0.3621    0.0000
  3300        0.0000             nan     0.3621   -0.0000
  3320        0.0000             nan     0.3621   -0.0000
  3340        0.0000             nan     0.3621   -0.0000
  3360        0.0000             nan     0.3621   -0.0000
  3380        0.0000             nan     0.3621   -0.0000
  3400        0.0000             nan     0.3621   -0.0000
  3420        0.0000             nan     0.3621   -0.0000
  3440        0.0000             nan     0.3621    0.0000
  3460        0.0000             nan     0.3621   -0.0000
  3480        0.0000             nan     0.3621    0.0000
  3500        0.0000             nan     0.3621   -0.0000
  3520        0.0000             nan     0.3621   -0.0000
  3540        0.0000             nan     0.3621   -0.0000
  3560        0.0000             nan     0.3621   -0.0000
  3580        0.0000             nan     0.3621   -0.0000
  3600        0.0000             nan     0.3621   -0.0000
  3620        0.0000             nan     0.3621   -0.0000
  3640        0.0000             nan     0.3621   -0.0000
  3660        0.0000             nan     0.3621    0.0000
  3680        0.0000             nan     0.3621   -0.0000
  3700        0.0000             nan     0.3621   -0.0000
  3720        0.0000             nan     0.3621   -0.0000
  3740        0.0000             nan     0.3621   -0.0000
  3760        0.0000             nan     0.3621   -0.0000
  3780        0.0000             nan     0.3621   -0.0000
  3799        0.0000             nan     0.3621   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2214             nan     0.4467    0.1903
     2        0.1584             nan     0.4467    0.0782
     3        0.1282             nan     0.4467    0.0191
     4        0.1077             nan     0.4467    0.0183
     5        0.1014             nan     0.4467    0.0028
     6        0.1077             nan     0.4467   -0.0281
     7        0.1010             nan     0.4467   -0.0204
     8        0.0986             nan     0.4467   -0.0007
     9        0.0907             nan     0.4467   -0.0006
    10        0.0863             nan     0.4467   -0.0066
    20        0.0378             nan     0.4467   -0.0135
    40        0.0124             nan     0.4467   -0.0022
    60        0.0038             nan     0.4467   -0.0005
    80        0.0009             nan     0.4467   -0.0002
   100        0.0003             nan     0.4467   -0.0000
   120        0.0001             nan     0.4467   -0.0000
   140        0.0000             nan     0.4467   -0.0000
   160        0.0000             nan     0.4467   -0.0000
   180        0.0000             nan     0.4467   -0.0000
   200        0.0000             nan     0.4467   -0.0000
   220        0.0000             nan     0.4467   -0.0000
   240        0.0000             nan     0.4467   -0.0000
   260        0.0000             nan     0.4467   -0.0000
   280        0.0000             nan     0.4467   -0.0000
   300        0.0000             nan     0.4467   -0.0000
   320        0.0000             nan     0.4467   -0.0000
   340        0.0000             nan     0.4467   -0.0000
   360        0.0000             nan     0.4467   -0.0000
   380        0.0000             nan     0.4467   -0.0000
   400        0.0000             nan     0.4467   -0.0000
   420        0.0000             nan     0.4467   -0.0000
   440        0.0000             nan     0.4467   -0.0000
   460        0.0000             nan     0.4467   -0.0000
   480        0.0000             nan     0.4467   -0.0000
   500        0.0000             nan     0.4467   -0.0000
   520        0.0000             nan     0.4467   -0.0000
   540        0.0000             nan     0.4467   -0.0000
   560        0.0000             nan     0.4467   -0.0000
   580        0.0000             nan     0.4467   -0.0000
   600        0.0000             nan     0.4467   -0.0000
   620        0.0000             nan     0.4467   -0.0000
   640        0.0000             nan     0.4467   -0.0000
   660        0.0000             nan     0.4467   -0.0000
   680        0.0000             nan     0.4467   -0.0000
   700        0.0000             nan     0.4467   -0.0000
   720        0.0000             nan     0.4467   -0.0000
   740        0.0000             nan     0.4467   -0.0000
   760        0.0000             nan     0.4467   -0.0000
   780        0.0000             nan     0.4467   -0.0000
   800        0.0000             nan     0.4467   -0.0000
   820        0.0000             nan     0.4467   -0.0000
   840        0.0000             nan     0.4467   -0.0000
   860        0.0000             nan     0.4467   -0.0000
   880        0.0000             nan     0.4467   -0.0000
   900        0.0000             nan     0.4467   -0.0000
   920        0.0000             nan     0.4467   -0.0000
   940        0.0000             nan     0.4467   -0.0000
   960        0.0000             nan     0.4467   -0.0000
   980        0.0000             nan     0.4467   -0.0000
  1000        0.0000             nan     0.4467   -0.0000
  1020        0.0000             nan     0.4467   -0.0000
  1040        0.0000             nan     0.4467   -0.0000
  1060        0.0000             nan     0.4467   -0.0000
  1080        0.0000             nan     0.4467   -0.0000
  1100        0.0000             nan     0.4467   -0.0000
  1120        0.0000             nan     0.4467   -0.0000
  1140        0.0000             nan     0.4467   -0.0000
  1160        0.0000             nan     0.4467   -0.0000
  1180        0.0000             nan     0.4467   -0.0000
  1200        0.0000             nan     0.4467   -0.0000
  1220        0.0000             nan     0.4467   -0.0000
  1240        0.0000             nan     0.4467   -0.0000
  1260        0.0000             nan     0.4467   -0.0000
  1280        0.0000             nan     0.4467   -0.0000
  1300        0.0000             nan     0.4467   -0.0000
  1320        0.0000             nan     0.4467   -0.0000
  1340        0.0000             nan     0.4467   -0.0000
  1360        0.0000             nan     0.4467   -0.0000
  1380        0.0000             nan     0.4467   -0.0000
  1400        0.0000             nan     0.4467   -0.0000
  1420        0.0000             nan     0.4467   -0.0000
  1440        0.0000             nan     0.4467   -0.0000
  1460        0.0000             nan     0.4467   -0.0000
  1480        0.0000             nan     0.4467   -0.0000
  1500        0.0000             nan     0.4467   -0.0000
  1520        0.0000             nan     0.4467   -0.0000
  1540        0.0000             nan     0.4467   -0.0000
  1560        0.0000             nan     0.4467   -0.0000
  1580        0.0000             nan     0.4467   -0.0000
  1600        0.0000             nan     0.4467   -0.0000
  1620        0.0000             nan     0.4467   -0.0000
  1640        0.0000             nan     0.4467   -0.0000
  1660        0.0000             nan     0.4467   -0.0000
  1680        0.0000             nan     0.4467   -0.0000
  1700        0.0000             nan     0.4467   -0.0000
  1720        0.0000             nan     0.4467   -0.0000
  1740        0.0000             nan     0.4467   -0.0000
  1760        0.0000             nan     0.4467   -0.0000
  1780        0.0000             nan     0.4467   -0.0000
  1800        0.0000             nan     0.4467   -0.0000
  1820        0.0000             nan     0.4467   -0.0000
  1840        0.0000             nan     0.4467   -0.0000
  1860        0.0000             nan     0.4467   -0.0000
  1880        0.0000             nan     0.4467   -0.0000
  1900        0.0000             nan     0.4467   -0.0000
  1920        0.0000             nan     0.4467   -0.0000
  1940        0.0000             nan     0.4467   -0.0000
  1960        0.0000             nan     0.4467   -0.0000
  1980        0.0000             nan     0.4467   -0.0000
  2000        0.0000             nan     0.4467   -0.0000
  2020        0.0000             nan     0.4467   -0.0000
  2040        0.0000             nan     0.4467   -0.0000
  2060        0.0000             nan     0.4467   -0.0000
  2080        0.0000             nan     0.4467   -0.0000
  2100        0.0000             nan     0.4467   -0.0000
  2120        0.0000             nan     0.4467   -0.0000
  2140        0.0000             nan     0.4467   -0.0000
  2160        0.0000             nan     0.4467   -0.0000
  2180        0.0000             nan     0.4467   -0.0000
  2200        0.0000             nan     0.4467   -0.0000
  2220        0.0000             nan     0.4467   -0.0000
  2232        0.0000             nan     0.4467   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2006             nan     0.5710    0.1779
     2        0.1238             nan     0.5710    0.0546
     3        0.0944             nan     0.5710   -0.0203
     4        0.0865             nan     0.5710   -0.0031
     5        0.0857             nan     0.5710   -0.0151
     6        0.0795             nan     0.5710    0.0065
     7        0.0786             nan     0.5710   -0.0268
     8        0.0785             nan     0.5710   -0.0300
     9        0.0828             nan     0.5710   -0.0229
    10        0.0772             nan     0.5710   -0.0070
    20        0.0433             nan     0.5710   -0.0110
    40        0.0182             nan     0.5710   -0.0086
    60        0.0074             nan     0.5710   -0.0025
    80        0.0028             nan     0.5710   -0.0011
   100        0.0011             nan     0.5710   -0.0003
   120        0.0003             nan     0.5710   -0.0001
   140        0.0001             nan     0.5710   -0.0000
   160        0.0000             nan     0.5710   -0.0000
   180        0.0000             nan     0.5710   -0.0000
   200        0.0000             nan     0.5710   -0.0000
   220        0.0000             nan     0.5710   -0.0000
   240        0.0000             nan     0.5710   -0.0000
   260        0.0000             nan     0.5710   -0.0000
   280        0.0000             nan     0.5710   -0.0000
   300        0.0000             nan     0.5710   -0.0000
   320        0.0000             nan     0.5710   -0.0000
   340        0.0000             nan     0.5710   -0.0000
   360        0.0000             nan     0.5710   -0.0000
   380        0.0000             nan     0.5710    0.0000
   400        0.0000             nan     0.5710   -0.0000
   420        0.0000             nan     0.5710   -0.0000
   440        0.0000             nan     0.5710   -0.0000
   460        0.0000             nan     0.5710   -0.0000
   480        0.0000             nan     0.5710   -0.0000
   500        0.0000             nan     0.5710   -0.0000
   520        0.0000             nan     0.5710   -0.0000
   540        0.0000             nan     0.5710   -0.0000
   560        0.0000             nan     0.5710   -0.0000
   580        0.0000             nan     0.5710   -0.0000
   600        0.0000             nan     0.5710   -0.0000
   620        0.0000             nan     0.5710   -0.0000
   640        0.0000             nan     0.5710   -0.0000
   660        0.0000             nan     0.5710   -0.0000
   680        0.0000             nan     0.5710   -0.0000
   700        0.0000             nan     0.5710   -0.0000
   720        0.0000             nan     0.5710   -0.0000
   740        0.0000             nan     0.5710   -0.0000
   760        0.0000             nan     0.5710   -0.0000
   780        0.0000             nan     0.5710   -0.0000
   800        0.0000             nan     0.5710   -0.0000
   820        0.0000             nan     0.5710   -0.0000
   840        0.0000             nan     0.5710   -0.0000
   860        0.0000             nan     0.5710   -0.0000
   880        0.0000             nan     0.5710   -0.0000
   900        0.0000             nan     0.5710   -0.0000
   920        0.0000             nan     0.5710    0.0000
   940        0.0000             nan     0.5710   -0.0000
   960        0.0000             nan     0.5710   -0.0000
   980        0.0000             nan     0.5710   -0.0000
  1000        0.0000             nan     0.5710   -0.0000
  1020        0.0000             nan     0.5710   -0.0000
  1040        0.0000             nan     0.5710   -0.0000
  1060        0.0000             nan     0.5710   -0.0000
  1080        0.0000             nan     0.5710   -0.0000
  1100        0.0000             nan     0.5710   -0.0000
  1120        0.0000             nan     0.5710   -0.0000
  1140        0.0000             nan     0.5710   -0.0000
  1160        0.0000             nan     0.5710   -0.0000
  1180        0.0000             nan     0.5710   -0.0000
  1200        0.0000             nan     0.5710   -0.0000
  1220        0.0000             nan     0.5710   -0.0000
  1240        0.0000             nan     0.5710   -0.0000
  1260        0.0000             nan     0.5710   -0.0000
  1280        0.0000             nan     0.5710    0.0000
  1300        0.0000             nan     0.5710   -0.0000
  1320        0.0000             nan     0.5710   -0.0000
  1340        0.0000             nan     0.5710   -0.0000
  1360        0.0000             nan     0.5710   -0.0000
  1380        0.0000             nan     0.5710   -0.0000
  1400        0.0000             nan     0.5710   -0.0000
  1420        0.0000             nan     0.5710   -0.0000
  1440        0.0000             nan     0.5710   -0.0000
  1460        0.0000             nan     0.5710   -0.0000
  1480        0.0000             nan     0.5710   -0.0000
  1500        0.0000             nan     0.5710   -0.0000
  1520        0.0000             nan     0.5710   -0.0000
  1540        0.0000             nan     0.5710   -0.0000
  1560        0.0000             nan     0.5710   -0.0000
  1580        0.0000             nan     0.5710   -0.0000
  1600        0.0000             nan     0.5710   -0.0000
  1620        0.0000             nan     0.5710   -0.0000
  1640        0.0000             nan     0.5710   -0.0000
  1660        0.0000             nan     0.5710   -0.0000
  1680        0.0000             nan     0.5710   -0.0000
  1700        0.0000             nan     0.5710   -0.0000
  1720        0.0000             nan     0.5710   -0.0000
  1740        0.0000             nan     0.5710   -0.0000
  1760        0.0000             nan     0.5710   -0.0000
  1780        0.0000             nan     0.5710    0.0000
  1800        0.0000             nan     0.5710   -0.0000
  1820        0.0000             nan     0.5710   -0.0000
  1840        0.0000             nan     0.5710   -0.0000
  1860        0.0000             nan     0.5710   -0.0000
  1880        0.0000             nan     0.5710   -0.0000
  1900        0.0000             nan     0.5710   -0.0000
  1920        0.0000             nan     0.5710   -0.0000
  1940        0.0000             nan     0.5710   -0.0000
  1960        0.0000             nan     0.5710   -0.0000
  1980        0.0000             nan     0.5710   -0.0000
  2000        0.0000             nan     0.5710   -0.0000
  2020        0.0000             nan     0.5710   -0.0000
  2040        0.0000             nan     0.5710   -0.0000
  2060        0.0000             nan     0.5710   -0.0000
  2080        0.0000             nan     0.5710   -0.0000
  2100        0.0000             nan     0.5710   -0.0000
  2120        0.0000             nan     0.5710   -0.0000
  2140        0.0000             nan     0.5710   -0.0000
  2160        0.0000             nan     0.5710   -0.0000
  2180        0.0000             nan     0.5710   -0.0000
  2200        0.0000             nan     0.5710   -0.0000
  2220        0.0000             nan     0.5710   -0.0000
  2240        0.0000             nan     0.5710   -0.0000
  2260        0.0000             nan     0.5710   -0.0000
  2280        0.0000             nan     0.5710    0.0000
  2300        0.0000             nan     0.5710   -0.0000
  2320        0.0000             nan     0.5710   -0.0000
  2340        0.0000             nan     0.5710   -0.0000
  2360        0.0000             nan     0.5710   -0.0000
  2380        0.0000             nan     0.5710   -0.0000
  2400        0.0000             nan     0.5710   -0.0000
  2420        0.0000             nan     0.5710   -0.0000
  2440        0.0000             nan     0.5710   -0.0000
  2460        0.0000             nan     0.5710   -0.0000
  2480        0.0000             nan     0.5710   -0.0000
  2500        0.0000             nan     0.5710   -0.0000
  2520        0.0000             nan     0.5710   -0.0000
  2540        0.0000             nan     0.5710   -0.0000
  2560        0.0000             nan     0.5710   -0.0000
  2580        0.0000             nan     0.5710   -0.0000
  2600        0.0000             nan     0.5710   -0.0000
  2620        0.0000             nan     0.5710   -0.0000
  2640        0.0000             nan     0.5710   -0.0000
  2660        0.0000             nan     0.5710   -0.0000
  2680        0.0000             nan     0.5710    0.0000
  2700        0.0000             nan     0.5710   -0.0000
  2720        0.0000             nan     0.5710   -0.0000
  2740        0.0000             nan     0.5710    0.0000
  2760        0.0000             nan     0.5710   -0.0000
  2780        0.0000             nan     0.5710   -0.0000
  2800        0.0000             nan     0.5710   -0.0000
  2820        0.0000             nan     0.5710   -0.0000
  2840        0.0000             nan     0.5710   -0.0000
  2860        0.0000             nan     0.5710   -0.0000
  2880        0.0000             nan     0.5710   -0.0000
  2900        0.0000             nan     0.5710   -0.0000
  2920        0.0000             nan     0.5710   -0.0000
  2940        0.0000             nan     0.5710   -0.0000
  2960        0.0000             nan     0.5710   -0.0000
  2980        0.0000             nan     0.5710   -0.0000
  3000        0.0000             nan     0.5710   -0.0000
  3020        0.0000             nan     0.5710   -0.0000
  3040        0.0000             nan     0.5710   -0.0000
  3060        0.0000             nan     0.5710   -0.0000
  3080        0.0000             nan     0.5710    0.0000
  3089        0.0000             nan     0.5710   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.6638             nan     0.0081    0.0050
     2        0.6566             nan     0.0081    0.0048
     3        0.6495             nan     0.0081    0.0060
     4        0.6436             nan     0.0081    0.0049
     5        0.6380             nan     0.0081    0.0048
     6        0.6342             nan     0.0081    0.0041
     7        0.6300             nan     0.0081    0.0051
     8        0.6245             nan     0.0081    0.0050
     9        0.6191             nan     0.0081    0.0050
    10        0.6129             nan     0.0081    0.0054
    20        0.5695             nan     0.0081    0.0043
    40        0.4897             nan     0.0081    0.0033
    60        0.4246             nan     0.0081    0.0017
    80        0.3779             nan     0.0081    0.0021
   100        0.3430             nan     0.0081   -0.0001
   120        0.3186             nan     0.0081   -0.0002
   140        0.3011             nan     0.0081   -0.0017
   160        0.2855             nan     0.0081    0.0003
   180        0.2714             nan     0.0081    0.0007
   200        0.2616             nan     0.0081    0.0002
   220        0.2462             nan     0.0081    0.0005
   240        0.2332             nan     0.0081   -0.0010
   260        0.2249             nan     0.0081   -0.0006
   280        0.2180             nan     0.0081   -0.0004
   300        0.2099             nan     0.0081   -0.0002
   320        0.2011             nan     0.0081    0.0000
   340        0.1945             nan     0.0081   -0.0006
   360        0.1899             nan     0.0081   -0.0005
   380        0.1835             nan     0.0081   -0.0004
   400        0.1752             nan     0.0081   -0.0007
   420        0.1700             nan     0.0081   -0.0007
   440        0.1651             nan     0.0081    0.0000
   460        0.1614             nan     0.0081   -0.0002
   480        0.1571             nan     0.0081   -0.0001
   500        0.1521             nan     0.0081   -0.0002
   520        0.1473             nan     0.0081   -0.0001
   540        0.1431             nan     0.0081   -0.0004
   560        0.1399             nan     0.0081   -0.0003
   580        0.1356             nan     0.0081   -0.0004
   600        0.1326             nan     0.0081   -0.0003
   620        0.1282             nan     0.0081   -0.0003
   640        0.1247             nan     0.0081   -0.0005
   660        0.1226             nan     0.0081   -0.0003
   680        0.1192             nan     0.0081   -0.0007
   700        0.1158             nan     0.0081   -0.0003
   720        0.1116             nan     0.0081   -0.0002
   740        0.1096             nan     0.0081   -0.0001
   760        0.1076             nan     0.0081   -0.0001
   780        0.1051             nan     0.0081   -0.0001
   800        0.1019             nan     0.0081   -0.0004
   820        0.0981             nan     0.0081   -0.0002
   840        0.0954             nan     0.0081   -0.0002
   860        0.0921             nan     0.0081   -0.0000
   880        0.0902             nan     0.0081   -0.0003
   900        0.0882             nan     0.0081   -0.0002
   920        0.0864             nan     0.0081   -0.0001
   940        0.0845             nan     0.0081   -0.0001
   960        0.0826             nan     0.0081   -0.0001
   980        0.0803             nan     0.0081   -0.0003
  1000        0.0783             nan     0.0081   -0.0002
  1020        0.0769             nan     0.0081   -0.0003
  1040        0.0748             nan     0.0081    0.0001
  1060        0.0726             nan     0.0081   -0.0002
  1080        0.0705             nan     0.0081   -0.0002
  1100        0.0689             nan     0.0081   -0.0002
  1120        0.0674             nan     0.0081   -0.0003
  1140        0.0651             nan     0.0081   -0.0001
  1160        0.0630             nan     0.0081   -0.0001
  1180        0.0618             nan     0.0081   -0.0001
  1200        0.0604             nan     0.0081   -0.0002
  1220        0.0593             nan     0.0081   -0.0001
  1231        0.0585             nan     0.0081   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.6586             nan     0.0234    0.0086
     2        0.6482             nan     0.0234    0.0106
     3        0.6384             nan     0.0234    0.0087
     4        0.6294             nan     0.0234    0.0107
     5        0.6148             nan     0.0234    0.0075
     6        0.6068             nan     0.0234    0.0085
     7        0.6003             nan     0.0234    0.0085
     8        0.5861             nan     0.0234    0.0060
     9        0.5766             nan     0.0234    0.0050
    10        0.5698             nan     0.0234    0.0082
    20        0.4965             nan     0.0234    0.0028
    40        0.4106             nan     0.0234    0.0002
    60        0.3607             nan     0.0234   -0.0008
    80        0.3324             nan     0.0234    0.0015
   100        0.3146             nan     0.0234    0.0012
   120        0.2962             nan     0.0234   -0.0004
   140        0.2825             nan     0.0234   -0.0019
   160        0.2678             nan     0.0234   -0.0003
   180        0.2545             nan     0.0234   -0.0005
   200        0.2463             nan     0.0234   -0.0004
   220        0.2323             nan     0.0234   -0.0008
   240        0.2241             nan     0.0234   -0.0013
   260        0.2177             nan     0.0234   -0.0005
   280        0.2094             nan     0.0234   -0.0002
   300        0.2035             nan     0.0234   -0.0016
   320        0.1972             nan     0.0234   -0.0004
   340        0.1902             nan     0.0234   -0.0003
   360        0.1861             nan     0.0234   -0.0007
   380        0.1816             nan     0.0234   -0.0002
   400        0.1766             nan     0.0234   -0.0008
   420        0.1721             nan     0.0234   -0.0013
   440        0.1670             nan     0.0234   -0.0010
   460        0.1632             nan     0.0234   -0.0001
   480        0.1583             nan     0.0234   -0.0008
   500        0.1553             nan     0.0234   -0.0011
   520        0.1527             nan     0.0234   -0.0008
   540        0.1490             nan     0.0234   -0.0006
   560        0.1439             nan     0.0234   -0.0005
   580        0.1399             nan     0.0234   -0.0011
   600        0.1380             nan     0.0234   -0.0008
   620        0.1362             nan     0.0234   -0.0003
   640        0.1331             nan     0.0234   -0.0011
   660        0.1298             nan     0.0234   -0.0008
   680        0.1266             nan     0.0234   -0.0006
   700        0.1237             nan     0.0234   -0.0008
   720        0.1213             nan     0.0234   -0.0006
   740        0.1188             nan     0.0234   -0.0004
   760        0.1154             nan     0.0234   -0.0005
   780        0.1131             nan     0.0234   -0.0004
   800        0.1104             nan     0.0234   -0.0006
   820        0.1074             nan     0.0234   -0.0007
   840        0.1047             nan     0.0234   -0.0003
   860        0.1021             nan     0.0234   -0.0008
   880        0.0996             nan     0.0234   -0.0006
   900        0.0978             nan     0.0234   -0.0005
   920        0.0963             nan     0.0234   -0.0005
   940        0.0941             nan     0.0234   -0.0008
   960        0.0928             nan     0.0234   -0.0001
   980        0.0904             nan     0.0234   -0.0003
  1000        0.0890             nan     0.0234   -0.0004
  1020        0.0872             nan     0.0234   -0.0004
  1040        0.0857             nan     0.0234   -0.0005
  1060        0.0842             nan     0.0234   -0.0005
  1080        0.0828             nan     0.0234   -0.0005
  1100        0.0812             nan     0.0234   -0.0003
  1120        0.0796             nan     0.0234   -0.0002
  1140        0.0778             nan     0.0234   -0.0003
  1160        0.0765             nan     0.0234   -0.0000
  1180        0.0755             nan     0.0234   -0.0002
  1200        0.0740             nan     0.0234   -0.0005
  1220        0.0730             nan     0.0234   -0.0007
  1240        0.0715             nan     0.0234   -0.0002
  1260        0.0701             nan     0.0234   -0.0004
  1280        0.0695             nan     0.0234   -0.0006
  1300        0.0684             nan     0.0234   -0.0006
  1320        0.0672             nan     0.0234   -0.0004
  1340        0.0659             nan     0.0234   -0.0002
  1360        0.0645             nan     0.0234   -0.0004
  1380        0.0633             nan     0.0234   -0.0002
  1400        0.0620             nan     0.0234   -0.0004
  1420        0.0609             nan     0.0234   -0.0002
  1440        0.0598             nan     0.0234   -0.0002
  1460        0.0587             nan     0.0234   -0.0004
  1480        0.0573             nan     0.0234   -0.0003
  1500        0.0561             nan     0.0234   -0.0004
  1520        0.0548             nan     0.0234   -0.0004
  1540        0.0538             nan     0.0234   -0.0002
  1560        0.0529             nan     0.0234   -0.0006
  1580        0.0520             nan     0.0234   -0.0004
  1600        0.0510             nan     0.0234   -0.0006
  1620        0.0502             nan     0.0234   -0.0003
  1640        0.0494             nan     0.0234   -0.0002
  1660        0.0481             nan     0.0234   -0.0002
  1680        0.0473             nan     0.0234   -0.0003
  1700        0.0465             nan     0.0234   -0.0003
  1720        0.0458             nan     0.0234   -0.0004
  1740        0.0446             nan     0.0234   -0.0001
  1760        0.0439             nan     0.0234   -0.0003
  1780        0.0433             nan     0.0234   -0.0003
  1800        0.0427             nan     0.0234   -0.0003
  1820        0.0418             nan     0.0234   -0.0004
  1840        0.0411             nan     0.0234   -0.0003
  1860        0.0404             nan     0.0234   -0.0002
  1880        0.0394             nan     0.0234   -0.0001
  1900        0.0388             nan     0.0234   -0.0003
  1920        0.0382             nan     0.0234   -0.0001
  1940        0.0377             nan     0.0234   -0.0002
  1960        0.0371             nan     0.0234   -0.0003
  1980        0.0365             nan     0.0234   -0.0001
  2000        0.0358             nan     0.0234   -0.0002
  2020        0.0354             nan     0.0234   -0.0002
  2040        0.0347             nan     0.0234   -0.0002
  2060        0.0342             nan     0.0234   -0.0002
  2080        0.0336             nan     0.0234   -0.0001
  2100        0.0331             nan     0.0234   -0.0002
  2120        0.0325             nan     0.0234   -0.0002
  2140        0.0319             nan     0.0234   -0.0001
  2160        0.0315             nan     0.0234   -0.0001
  2180        0.0309             nan     0.0234   -0.0000
  2200        0.0303             nan     0.0234   -0.0001
  2220        0.0298             nan     0.0234   -0.0001
  2240        0.0293             nan     0.0234   -0.0001
  2260        0.0288             nan     0.0234   -0.0002
  2280        0.0283             nan     0.0234   -0.0001
  2300        0.0279             nan     0.0234   -0.0002
  2320        0.0274             nan     0.0234   -0.0001
  2340        0.0268             nan     0.0234   -0.0001
  2360        0.0263             nan     0.0234   -0.0001
  2380        0.0259             nan     0.0234   -0.0001
  2400        0.0256             nan     0.0234   -0.0001
  2420        0.0253             nan     0.0234   -0.0002
  2440        0.0249             nan     0.0234   -0.0002
  2460        0.0245             nan     0.0234   -0.0002
  2480        0.0241             nan     0.0234   -0.0001
  2500        0.0237             nan     0.0234   -0.0002
  2520        0.0233             nan     0.0234   -0.0001
  2540        0.0230             nan     0.0234   -0.0001
  2560        0.0226             nan     0.0234   -0.0001
  2580        0.0223             nan     0.0234   -0.0002
  2600        0.0220             nan     0.0234   -0.0001
  2620        0.0217             nan     0.0234   -0.0001
  2640        0.0214             nan     0.0234   -0.0001
  2660        0.0211             nan     0.0234   -0.0001
  2680        0.0208             nan     0.0234   -0.0001
  2700        0.0205             nan     0.0234   -0.0001
  2720        0.0203             nan     0.0234   -0.0001
  2740        0.0200             nan     0.0234   -0.0001
  2760        0.0197             nan     0.0234   -0.0001
  2780        0.0194             nan     0.0234   -0.0002
  2800        0.0191             nan     0.0234   -0.0001
  2820        0.0188             nan     0.0234   -0.0001
  2840        0.0186             nan     0.0234   -0.0001
  2860        0.0185             nan     0.0234   -0.0001
  2880        0.0183             nan     0.0234   -0.0000
  2900        0.0179             nan     0.0234   -0.0001
  2920        0.0177             nan     0.0234   -0.0001
  2940        0.0173             nan     0.0234   -0.0001
  2960        0.0171             nan     0.0234   -0.0000
  2980        0.0169             nan     0.0234   -0.0000
  3000        0.0167             nan     0.0234   -0.0002
  3020        0.0165             nan     0.0234   -0.0001
  3040        0.0163             nan     0.0234   -0.0001
  3057        0.0160             nan     0.0234   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5977             nan     0.1203    0.0527
     2        0.5421             nan     0.1203    0.0493
     3        0.4954             nan     0.1203    0.0268
     4        0.4570             nan     0.1203    0.0357
     5        0.4300             nan     0.1203    0.0262
     6        0.4174             nan     0.1203    0.0023
     7        0.3842             nan     0.1203    0.0252
     8        0.3650             nan     0.1203    0.0125
     9        0.3542             nan     0.1203    0.0149
    10        0.3290             nan     0.1203    0.0028
    20        0.2367             nan     0.1203   -0.0042
    40        0.1691             nan     0.1203   -0.0036
    60        0.1420             nan     0.1203   -0.0026
    80        0.1151             nan     0.1203   -0.0026
   100        0.1018             nan     0.1203   -0.0019
   120        0.0881             nan     0.1203   -0.0036
   140        0.0771             nan     0.1203   -0.0017
   160        0.0664             nan     0.1203   -0.0032
   180        0.0574             nan     0.1203   -0.0013
   200        0.0525             nan     0.1203   -0.0015
   220        0.0479             nan     0.1203   -0.0003
   240        0.0432             nan     0.1203   -0.0008
   260        0.0397             nan     0.1203   -0.0023
   280        0.0367             nan     0.1203   -0.0008
   300        0.0340             nan     0.1203   -0.0013
   320        0.0312             nan     0.1203   -0.0012
   340        0.0283             nan     0.1203   -0.0009
   360        0.0261             nan     0.1203   -0.0008
   380        0.0242             nan     0.1203   -0.0008
   400        0.0220             nan     0.1203   -0.0010
   420        0.0207             nan     0.1203   -0.0010
   440        0.0189             nan     0.1203   -0.0006
   460        0.0178             nan     0.1203   -0.0005
   480        0.0164             nan     0.1203   -0.0003
   500        0.0149             nan     0.1203   -0.0004
   520        0.0138             nan     0.1203   -0.0001
   540        0.0130             nan     0.1203   -0.0007
   560        0.0116             nan     0.1203   -0.0003
   580        0.0110             nan     0.1203   -0.0006
   600        0.0100             nan     0.1203   -0.0004
   620        0.0092             nan     0.1203   -0.0007
   640        0.0085             nan     0.1203   -0.0003
   660        0.0081             nan     0.1203   -0.0002
   680        0.0077             nan     0.1203   -0.0002
   700        0.0074             nan     0.1203   -0.0004
   720        0.0070             nan     0.1203   -0.0003
   740        0.0064             nan     0.1203   -0.0003
   760        0.0061             nan     0.1203   -0.0001
   780        0.0057             nan     0.1203   -0.0002
   800        0.0053             nan     0.1203   -0.0003
   820        0.0051             nan     0.1203   -0.0000
   837        0.0049             nan     0.1203   -0.0003

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5612             nan     0.2260    0.0855
     2        0.4869             nan     0.2260    0.0474
     3        0.4300             nan     0.2260    0.0243
     4        0.3904             nan     0.2260    0.0073
     5        0.3878             nan     0.2260    0.0005
     6        0.3839             nan     0.2260   -0.0073
     7        0.3477             nan     0.2260   -0.0114
     8        0.3445             nan     0.2260   -0.0136
     9        0.3451             nan     0.2260   -0.0149
    10        0.3294             nan     0.2260    0.0043
    20        0.2617             nan     0.2260   -0.0202
    40        0.1925             nan     0.2260   -0.0214
    60        0.1587             nan     0.2260   -0.0028
    80        0.1354             nan     0.2260   -0.0034
   100        0.1165             nan     0.2260   -0.0050
   120        0.0987             nan     0.2260   -0.0052
   140        0.0824             nan     0.2260   -0.0026
   160        0.0731             nan     0.2260   -0.0087
   180        0.0623             nan     0.2260   -0.0043
   200        0.0524             nan     0.2260   -0.0002
   220        0.0451             nan     0.2260   -0.0028
   240        0.0393             nan     0.2260   -0.0026
   260        0.0361             nan     0.2260   -0.0000
   280        0.0290             nan     0.2260   -0.0003
   300        0.0241             nan     0.2260   -0.0011
   320        0.0228             nan     0.2260   -0.0006
   340        0.0210             nan     0.2260   -0.0023
   360        0.0190             nan     0.2260   -0.0004
   380        0.0167             nan     0.2260   -0.0016
   400        0.0152             nan     0.2260   -0.0007
   420        0.0132             nan     0.2260   -0.0006
   440        0.0123             nan     0.2260   -0.0005
   460        0.0115             nan     0.2260   -0.0003
   480        0.0101             nan     0.2260   -0.0006
   500        0.0088             nan     0.2260   -0.0007
   520        0.0076             nan     0.2260   -0.0004
   540        0.0073             nan     0.2260   -0.0007
   560        0.0065             nan     0.2260   -0.0006
   580        0.0061             nan     0.2260   -0.0002
   600        0.0054             nan     0.2260   -0.0001
   620        0.0050             nan     0.2260   -0.0003
   640        0.0046             nan     0.2260   -0.0003
   660        0.0043             nan     0.2260   -0.0001
   680        0.0040             nan     0.2260   -0.0002
   700        0.0036             nan     0.2260   -0.0001
   720        0.0034             nan     0.2260   -0.0001
   740        0.0033             nan     0.2260   -0.0001
   760        0.0031             nan     0.2260   -0.0002
   780        0.0027             nan     0.2260   -0.0000
   800        0.0025             nan     0.2260   -0.0001
   820        0.0023             nan     0.2260   -0.0002
   840        0.0022             nan     0.2260   -0.0000
   860        0.0021             nan     0.2260   -0.0002
   880        0.0019             nan     0.2260   -0.0001
   900        0.0018             nan     0.2260   -0.0000
   920        0.0016             nan     0.2260   -0.0001
   940        0.0015             nan     0.2260   -0.0001
   960        0.0014             nan     0.2260   -0.0001
   980        0.0014             nan     0.2260   -0.0001
  1000        0.0012             nan     0.2260   -0.0000
  1020        0.0012             nan     0.2260   -0.0001
  1040        0.0011             nan     0.2260   -0.0000
  1060        0.0010             nan     0.2260   -0.0001
  1080        0.0010             nan     0.2260   -0.0000
  1100        0.0009             nan     0.2260   -0.0000
  1120        0.0009             nan     0.2260   -0.0001
  1140        0.0008             nan     0.2260   -0.0000
  1160        0.0008             nan     0.2260   -0.0001
  1180        0.0007             nan     0.2260   -0.0000
  1200        0.0007             nan     0.2260   -0.0001
  1220        0.0006             nan     0.2260   -0.0000
  1240        0.0006             nan     0.2260   -0.0001
  1260        0.0005             nan     0.2260   -0.0000
  1280        0.0005             nan     0.2260   -0.0000
  1300        0.0005             nan     0.2260   -0.0000
  1320        0.0004             nan     0.2260   -0.0000
  1340        0.0004             nan     0.2260   -0.0000
  1360        0.0004             nan     0.2260   -0.0000
  1380        0.0004             nan     0.2260   -0.0000
  1400        0.0004             nan     0.2260   -0.0000
  1420        0.0003             nan     0.2260   -0.0000
  1440        0.0003             nan     0.2260   -0.0000
  1460        0.0003             nan     0.2260   -0.0000
  1480        0.0003             nan     0.2260   -0.0000
  1500        0.0003             nan     0.2260   -0.0000
  1520        0.0002             nan     0.2260   -0.0000
  1540        0.0002             nan     0.2260   -0.0000
  1560        0.0002             nan     0.2260   -0.0000
  1580        0.0002             nan     0.2260   -0.0000
  1600        0.0002             nan     0.2260   -0.0000
  1620        0.0002             nan     0.2260   -0.0000
  1640        0.0002             nan     0.2260   -0.0000
  1660        0.0002             nan     0.2260   -0.0000
  1680        0.0002             nan     0.2260   -0.0000
  1700        0.0002             nan     0.2260   -0.0000
  1720        0.0001             nan     0.2260   -0.0000
  1740        0.0001             nan     0.2260   -0.0000
  1760        0.0001             nan     0.2260   -0.0000
  1780        0.0001             nan     0.2260   -0.0000
  1800        0.0001             nan     0.2260   -0.0000
  1820        0.0001             nan     0.2260   -0.0000
  1840        0.0001             nan     0.2260   -0.0000
  1860        0.0001             nan     0.2260   -0.0000
  1880        0.0001             nan     0.2260   -0.0000
  1900        0.0001             nan     0.2260   -0.0000
  1920        0.0001             nan     0.2260   -0.0000
  1940        0.0001             nan     0.2260   -0.0000
  1960        0.0001             nan     0.2260   -0.0000
  1980        0.0001             nan     0.2260   -0.0000
  2000        0.0001             nan     0.2260   -0.0000
  2020        0.0001             nan     0.2260   -0.0000
  2040        0.0001             nan     0.2260   -0.0000
  2060        0.0001             nan     0.2260   -0.0000
  2080        0.0001             nan     0.2260   -0.0000
  2100        0.0001             nan     0.2260   -0.0000
  2120        0.0000             nan     0.2260   -0.0000
  2140        0.0000             nan     0.2260   -0.0000
  2160        0.0000             nan     0.2260   -0.0000
  2180        0.0000             nan     0.2260   -0.0000
  2200        0.0000             nan     0.2260   -0.0000
  2220        0.0000             nan     0.2260   -0.0000
  2240        0.0000             nan     0.2260   -0.0000
  2260        0.0000             nan     0.2260   -0.0000
  2280        0.0000             nan     0.2260   -0.0000
  2300        0.0000             nan     0.2260   -0.0000
  2320        0.0000             nan     0.2260   -0.0000
  2340        0.0000             nan     0.2260   -0.0000
  2360        0.0000             nan     0.2260   -0.0000
  2380        0.0000             nan     0.2260   -0.0000
  2400        0.0000             nan     0.2260   -0.0000
  2420        0.0000             nan     0.2260   -0.0000
  2440        0.0000             nan     0.2260   -0.0000
  2460        0.0000             nan     0.2260   -0.0000
  2480        0.0000             nan     0.2260   -0.0000
  2500        0.0000             nan     0.2260   -0.0000
  2520        0.0000             nan     0.2260   -0.0000
  2540        0.0000             nan     0.2260   -0.0000
  2560        0.0000             nan     0.2260   -0.0000
  2580        0.0000             nan     0.2260   -0.0000
  2600        0.0000             nan     0.2260   -0.0000
  2620        0.0000             nan     0.2260   -0.0000
  2640        0.0000             nan     0.2260   -0.0000
  2660        0.0000             nan     0.2260   -0.0000
  2680        0.0000             nan     0.2260   -0.0000
  2700        0.0000             nan     0.2260   -0.0000
  2720        0.0000             nan     0.2260   -0.0000
  2740        0.0000             nan     0.2260   -0.0000
  2760        0.0000             nan     0.2260   -0.0000
  2780        0.0000             nan     0.2260   -0.0000
  2790        0.0000             nan     0.2260   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5852             nan     0.2340    0.0957
     2        0.5039             nan     0.2340    0.0315
     3        0.4636             nan     0.2340    0.0328
     4        0.4565             nan     0.2340   -0.0357
     5        0.4239             nan     0.2340    0.0090
     6        0.4161             nan     0.2340   -0.0201
     7        0.4124             nan     0.2340   -0.0191
     8        0.4008             nan     0.2340   -0.0069
     9        0.4029             nan     0.2340   -0.0358
    10        0.4044             nan     0.2340   -0.0224
    20        0.3420             nan     0.2340   -0.0164
    40        0.2774             nan     0.2340   -0.0083
    60        0.2337             nan     0.2340   -0.0126
    80        0.2122             nan     0.2340   -0.0111
   100        0.1741             nan     0.2340   -0.0076
   120        0.1605             nan     0.2340   -0.0038
   140        0.1337             nan     0.2340   -0.0080
   160        0.1234             nan     0.2340   -0.0129
   180        0.1047             nan     0.2340   -0.0057
   200        0.0940             nan     0.2340   -0.0022
   220        0.0861             nan     0.2340   -0.0039
   240        0.0790             nan     0.2340   -0.0030
   260        0.0732             nan     0.2340   -0.0016
   280        0.0690             nan     0.2340   -0.0053
   300        0.0602             nan     0.2340   -0.0029
   320        0.0559             nan     0.2340   -0.0037
   340        0.0501             nan     0.2340   -0.0018
   360        0.0454             nan     0.2340   -0.0012
   380        0.0420             nan     0.2340   -0.0025
   400        0.0400             nan     0.2340   -0.0019
   420        0.0382             nan     0.2340    0.0002
   440        0.0352             nan     0.2340   -0.0027
   460        0.0307             nan     0.2340   -0.0021
   480        0.0298             nan     0.2340   -0.0009
   500        0.0288             nan     0.2340    0.0001
   520        0.0250             nan     0.2340   -0.0003
   540        0.0241             nan     0.2340   -0.0007
   560        0.0232             nan     0.2340   -0.0006
   580        0.0230             nan     0.2340   -0.0010
   600        0.0216             nan     0.2340   -0.0006
   620        0.0208             nan     0.2340   -0.0016
   640        0.0196             nan     0.2340    0.0000
   660        0.0192             nan     0.2340   -0.0003
   680        0.0187             nan     0.2340   -0.0023
   700        0.0177             nan     0.2340   -0.0006
   720        0.0168             nan     0.2340   -0.0014
   740        0.0161             nan     0.2340   -0.0012
   760        0.0154             nan     0.2340   -0.0005
   780        0.0153             nan     0.2340   -0.0013
   800        0.0146             nan     0.2340   -0.0009
   820        0.0140             nan     0.2340   -0.0012
   840        0.0136             nan     0.2340   -0.0002
   860        0.0126             nan     0.2340   -0.0002
   880        0.0122             nan     0.2340   -0.0005
   900        0.0120             nan     0.2340   -0.0007
   920        0.0118             nan     0.2340   -0.0003
   940        0.0118             nan     0.2340   -0.0013
   960        0.0109             nan     0.2340   -0.0006
   980        0.0104             nan     0.2340   -0.0002
  1000        0.0099             nan     0.2340   -0.0002
  1020        0.0099             nan     0.2340   -0.0002
  1040        0.0093             nan     0.2340   -0.0003
  1060        0.0087             nan     0.2340   -0.0009
  1080        0.0085             nan     0.2340   -0.0007
  1100        0.0082             nan     0.2340   -0.0006
  1120        0.0078             nan     0.2340   -0.0004
  1140        0.0076             nan     0.2340   -0.0003
  1160        0.0077             nan     0.2340   -0.0005
  1180        0.0075             nan     0.2340   -0.0005
  1200        0.0067             nan     0.2340   -0.0001
  1220        0.0067             nan     0.2340   -0.0005
  1240        0.0062             nan     0.2340   -0.0002
  1260        0.0061             nan     0.2340   -0.0001
  1280        0.0059             nan     0.2340   -0.0004
  1300        0.0058             nan     0.2340   -0.0005
  1320        0.0055             nan     0.2340   -0.0001
  1340        0.0054             nan     0.2340   -0.0001
  1360        0.0051             nan     0.2340   -0.0001
  1380        0.0049             nan     0.2340   -0.0003
  1400        0.0049             nan     0.2340   -0.0001
  1420        0.0047             nan     0.2340   -0.0005
  1440        0.0046             nan     0.2340   -0.0000
  1460        0.0045             nan     0.2340   -0.0001
  1480        0.0045             nan     0.2340   -0.0006
  1500        0.0042             nan     0.2340   -0.0003
  1520        0.0042             nan     0.2340   -0.0001
  1540        0.0041             nan     0.2340   -0.0001
  1560        0.0041             nan     0.2340   -0.0001
  1580        0.0040             nan     0.2340   -0.0003
  1600        0.0038             nan     0.2340   -0.0001
  1620        0.0038             nan     0.2340   -0.0001
  1640        0.0038             nan     0.2340   -0.0002
  1660        0.0037             nan     0.2340   -0.0001
  1680        0.0036             nan     0.2340   -0.0000
  1700        0.0036             nan     0.2340   -0.0003
  1720        0.0035             nan     0.2340   -0.0002
  1740        0.0035             nan     0.2340   -0.0002
  1760        0.0033             nan     0.2340   -0.0001
  1780        0.0033             nan     0.2340   -0.0001
  1800        0.0031             nan     0.2340   -0.0002
  1820        0.0029             nan     0.2340   -0.0003
  1840        0.0029             nan     0.2340   -0.0001
  1860        0.0029             nan     0.2340   -0.0001
  1880        0.0028             nan     0.2340   -0.0002
  1900        0.0028             nan     0.2340   -0.0002
  1920        0.0027             nan     0.2340   -0.0001
  1940        0.0026             nan     0.2340   -0.0001
  1960        0.0027             nan     0.2340   -0.0002
  1980        0.0026             nan     0.2340   -0.0000
  2000        0.0024             nan     0.2340   -0.0001
  2020        0.0024             nan     0.2340   -0.0002
  2040        0.0023             nan     0.2340   -0.0000
  2060        0.0022             nan     0.2340   -0.0000
  2080        0.0020             nan     0.2340   -0.0001
  2100        0.0020             nan     0.2340   -0.0001
  2120        0.0020             nan     0.2340   -0.0001
  2140        0.0018             nan     0.2340   -0.0001
  2160        0.0018             nan     0.2340   -0.0001
  2180        0.0018             nan     0.2340   -0.0002
  2200        0.0017             nan     0.2340   -0.0001
  2220        0.0016             nan     0.2340   -0.0000
  2240        0.0016             nan     0.2340   -0.0000
  2260        0.0015             nan     0.2340   -0.0000
  2280        0.0015             nan     0.2340   -0.0001
  2300        0.0015             nan     0.2340   -0.0001
  2320        0.0014             nan     0.2340   -0.0001
  2340        0.0015             nan     0.2340   -0.0001
  2360        0.0014             nan     0.2340   -0.0000
  2380        0.0014             nan     0.2340   -0.0000
  2400        0.0014             nan     0.2340   -0.0000
  2420        0.0014             nan     0.2340   -0.0001
  2440        0.0013             nan     0.2340   -0.0000
  2460        0.0013             nan     0.2340   -0.0000
  2480        0.0013             nan     0.2340   -0.0001
  2500        0.0012             nan     0.2340   -0.0001
  2520        0.0012             nan     0.2340   -0.0000
  2540        0.0011             nan     0.2340   -0.0001
  2560        0.0011             nan     0.2340   -0.0001
  2580        0.0011             nan     0.2340   -0.0000
  2600        0.0011             nan     0.2340   -0.0001
  2620        0.0010             nan     0.2340   -0.0000
  2640        0.0010             nan     0.2340   -0.0000
  2660        0.0010             nan     0.2340   -0.0001
  2680        0.0010             nan     0.2340   -0.0000
  2700        0.0010             nan     0.2340   -0.0001
  2720        0.0010             nan     0.2340   -0.0001
  2740        0.0010             nan     0.2340   -0.0000
  2760        0.0009             nan     0.2340   -0.0000
  2780        0.0009             nan     0.2340   -0.0000
  2800        0.0009             nan     0.2340   -0.0001
  2820        0.0009             nan     0.2340   -0.0000
  2840        0.0009             nan     0.2340   -0.0000
  2860        0.0009             nan     0.2340   -0.0000
  2880        0.0009             nan     0.2340   -0.0000
  2900        0.0008             nan     0.2340   -0.0000
  2920        0.0008             nan     0.2340   -0.0000
  2940        0.0008             nan     0.2340   -0.0000
  2960        0.0008             nan     0.2340   -0.0001
  2980        0.0008             nan     0.2340   -0.0000
  3000        0.0008             nan     0.2340    0.0000
  3020        0.0008             nan     0.2340   -0.0000
  3040        0.0008             nan     0.2340   -0.0000
  3060        0.0008             nan     0.2340   -0.0000
  3080        0.0008             nan     0.2340   -0.0000
  3100        0.0007             nan     0.2340   -0.0000
  3120        0.0007             nan     0.2340   -0.0000
  3140        0.0007             nan     0.2340   -0.0000
  3160        0.0007             nan     0.2340   -0.0000
  3180        0.0007             nan     0.2340   -0.0000
  3200        0.0007             nan     0.2340   -0.0000
  3220        0.0006             nan     0.2340   -0.0000
  3240        0.0006             nan     0.2340   -0.0000
  3260        0.0006             nan     0.2340   -0.0000
  3280        0.0006             nan     0.2340   -0.0000
  3300        0.0006             nan     0.2340   -0.0001
  3320        0.0006             nan     0.2340   -0.0000
  3340        0.0006             nan     0.2340   -0.0000
  3360        0.0006             nan     0.2340   -0.0000
  3380        0.0006             nan     0.2340   -0.0000
  3400        0.0006             nan     0.2340   -0.0000
  3420        0.0005             nan     0.2340   -0.0000
  3440        0.0005             nan     0.2340   -0.0000
  3460        0.0005             nan     0.2340   -0.0000
  3480        0.0005             nan     0.2340   -0.0000
  3500        0.0005             nan     0.2340   -0.0000
  3520        0.0005             nan     0.2340   -0.0000
  3540        0.0005             nan     0.2340   -0.0000
  3560        0.0005             nan     0.2340   -0.0000
  3580        0.0005             nan     0.2340   -0.0000
  3600        0.0005             nan     0.2340   -0.0000
  3620        0.0005             nan     0.2340   -0.0000
  3640        0.0004             nan     0.2340   -0.0000
  3660        0.0004             nan     0.2340   -0.0000
  3680        0.0004             nan     0.2340   -0.0000
  3700        0.0004             nan     0.2340   -0.0000
  3720        0.0004             nan     0.2340   -0.0000
  3740        0.0004             nan     0.2340   -0.0000
  3760        0.0004             nan     0.2340   -0.0000
  3780        0.0004             nan     0.2340   -0.0000
  3800        0.0004             nan     0.2340   -0.0000
  3820        0.0004             nan     0.2340   -0.0000
  3840        0.0004             nan     0.2340   -0.0000
  3860        0.0004             nan     0.2340   -0.0000
  3880        0.0004             nan     0.2340   -0.0000
  3900        0.0004             nan     0.2340   -0.0000
  3920        0.0004             nan     0.2340   -0.0000
  3924        0.0003             nan     0.2340   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5718             nan     0.2895    0.1468
     2        0.4716             nan     0.2895    0.0646
     3        0.3981             nan     0.2895    0.0908
     4        0.3533             nan     0.2895    0.0520
     5        0.2867             nan     0.2895    0.0154
     6        0.2613             nan     0.2895    0.0236
     7        0.2475             nan     0.2895   -0.0007
     8        0.2456             nan     0.2895   -0.0047
     9        0.2052             nan     0.2895    0.0014
    10        0.1877             nan     0.2895   -0.0053
    20        0.1405             nan     0.2895    0.0044
    40        0.1001             nan     0.2895   -0.0095
    60        0.0727             nan     0.2895   -0.0086
    80        0.0534             nan     0.2895   -0.0011
   100        0.0426             nan     0.2895   -0.0053
   120        0.0317             nan     0.2895   -0.0017
   140        0.0264             nan     0.2895   -0.0039
   160        0.0207             nan     0.2895   -0.0015
   180        0.0160             nan     0.2895   -0.0025
   200        0.0143             nan     0.2895   -0.0013
   220        0.0119             nan     0.2895   -0.0008
   240        0.0100             nan     0.2895   -0.0014
   260        0.0089             nan     0.2895   -0.0009
   280        0.0071             nan     0.2895   -0.0001
   300        0.0065             nan     0.2895   -0.0005
   320        0.0052             nan     0.2895   -0.0006
   340        0.0044             nan     0.2895   -0.0002
   360        0.0037             nan     0.2895   -0.0004
   380        0.0029             nan     0.2895   -0.0001
   400        0.0025             nan     0.2895   -0.0001
   420        0.0022             nan     0.2895   -0.0003
   440        0.0017             nan     0.2895   -0.0002
   460        0.0015             nan     0.2895   -0.0000
   480        0.0012             nan     0.2895   -0.0001
   500        0.0010             nan     0.2895   -0.0001
   520        0.0008             nan     0.2895   -0.0000
   540        0.0006             nan     0.2895   -0.0001
   560        0.0005             nan     0.2895   -0.0001
   580        0.0004             nan     0.2895   -0.0000
   600        0.0004             nan     0.2895   -0.0000
   620        0.0003             nan     0.2895   -0.0000
   640        0.0003             nan     0.2895   -0.0000
   660        0.0002             nan     0.2895   -0.0000
   680        0.0002             nan     0.2895   -0.0000
   700        0.0001             nan     0.2895   -0.0000
   720        0.0001             nan     0.2895   -0.0000
   740        0.0001             nan     0.2895   -0.0000
   760        0.0001             nan     0.2895   -0.0000
   780        0.0001             nan     0.2895   -0.0000
   800        0.0001             nan     0.2895   -0.0000
   820        0.0001             nan     0.2895   -0.0000
   840        0.0000             nan     0.2895   -0.0000
   860        0.0000             nan     0.2895   -0.0000
   880        0.0000             nan     0.2895   -0.0000
   900        0.0000             nan     0.2895    0.0000
   920        0.0000             nan     0.2895   -0.0000
   940        0.0000             nan     0.2895   -0.0000
   960        0.0000             nan     0.2895   -0.0000
   980        0.0000             nan     0.2895   -0.0000
  1000        0.0000             nan     0.2895   -0.0000
  1020        0.0000             nan     0.2895   -0.0000
  1040        0.0000             nan     0.2895   -0.0000
  1060        0.0000             nan     0.2895   -0.0000
  1080        0.0000             nan     0.2895   -0.0000
  1100        0.0000             nan     0.2895   -0.0000
  1120        0.0000             nan     0.2895   -0.0000
  1140        0.0000             nan     0.2895   -0.0000
  1160        0.0000             nan     0.2895   -0.0000
  1180        0.0000             nan     0.2895   -0.0000
  1200        0.0000             nan     0.2895   -0.0000
  1220        0.0000             nan     0.2895   -0.0000
  1240        0.0000             nan     0.2895   -0.0000
  1260        0.0000             nan     0.2895   -0.0000
  1280        0.0000             nan     0.2895   -0.0000
  1300        0.0000             nan     0.2895   -0.0000
  1320        0.0000             nan     0.2895   -0.0000
  1340        0.0000             nan     0.2895   -0.0000
  1360        0.0000             nan     0.2895   -0.0000
  1380        0.0000             nan     0.2895   -0.0000
  1400        0.0000             nan     0.2895   -0.0000
  1420        0.0000             nan     0.2895   -0.0000
  1440        0.0000             nan     0.2895   -0.0000
  1460        0.0000             nan     0.2895   -0.0000
  1480        0.0000             nan     0.2895   -0.0000
  1500        0.0000             nan     0.2895   -0.0000
  1520        0.0000             nan     0.2895   -0.0000
  1540        0.0000             nan     0.2895   -0.0000
  1560        0.0000             nan     0.2895    0.0000
  1580        0.0000             nan     0.2895   -0.0000
  1600        0.0000             nan     0.2895   -0.0000
  1620        0.0000             nan     0.2895   -0.0000
  1640        0.0000             nan     0.2895   -0.0000
  1660        0.0000             nan     0.2895   -0.0000
  1680        0.0000             nan     0.2895   -0.0000
  1700        0.0000             nan     0.2895   -0.0000
  1720        0.0000             nan     0.2895   -0.0000
  1740        0.0000             nan     0.2895   -0.0000
  1760        0.0000             nan     0.2895   -0.0000
  1780        0.0000             nan     0.2895   -0.0000
  1800        0.0000             nan     0.2895   -0.0000
  1820        0.0000             nan     0.2895   -0.0000
  1840        0.0000             nan     0.2895   -0.0000
  1860        0.0000             nan     0.2895   -0.0000
  1880        0.0000             nan     0.2895   -0.0000
  1900        0.0000             nan     0.2895   -0.0000
  1920        0.0000             nan     0.2895   -0.0000
  1940        0.0000             nan     0.2895   -0.0000
  1960        0.0000             nan     0.2895   -0.0000
  1980        0.0000             nan     0.2895    0.0000
  2000        0.0000             nan     0.2895   -0.0000
  2020        0.0000             nan     0.2895   -0.0000
  2040        0.0000             nan     0.2895   -0.0000
  2060        0.0000             nan     0.2895   -0.0000
  2080        0.0000             nan     0.2895   -0.0000
  2100        0.0000             nan     0.2895   -0.0000
  2120        0.0000             nan     0.2895   -0.0000
  2140        0.0000             nan     0.2895   -0.0000
  2160        0.0000             nan     0.2895   -0.0000
  2180        0.0000             nan     0.2895   -0.0000
  2200        0.0000             nan     0.2895   -0.0000
  2220        0.0000             nan     0.2895   -0.0000
  2240        0.0000             nan     0.2895   -0.0000
  2260        0.0000             nan     0.2895   -0.0000
  2280        0.0000             nan     0.2895   -0.0000
  2300        0.0000             nan     0.2895   -0.0000
  2320        0.0000             nan     0.2895   -0.0000
  2340        0.0000             nan     0.2895   -0.0000
  2360        0.0000             nan     0.2895   -0.0000
  2380        0.0000             nan     0.2895   -0.0000
  2400        0.0000             nan     0.2895   -0.0000
  2420        0.0000             nan     0.2895   -0.0000
  2440        0.0000             nan     0.2895   -0.0000
  2460        0.0000             nan     0.2895   -0.0000
  2480        0.0000             nan     0.2895   -0.0000
  2500        0.0000             nan     0.2895   -0.0000
  2520        0.0000             nan     0.2895   -0.0000
  2540        0.0000             nan     0.2895   -0.0000
  2560        0.0000             nan     0.2895   -0.0000
  2580        0.0000             nan     0.2895   -0.0000
  2600        0.0000             nan     0.2895   -0.0000
  2620        0.0000             nan     0.2895   -0.0000
  2640        0.0000             nan     0.2895   -0.0000
  2660        0.0000             nan     0.2895   -0.0000
  2680        0.0000             nan     0.2895   -0.0000
  2700        0.0000             nan     0.2895   -0.0000
  2720        0.0000             nan     0.2895   -0.0000
  2740        0.0000             nan     0.2895   -0.0000
  2760        0.0000             nan     0.2895   -0.0000
  2780        0.0000             nan     0.2895   -0.0000
  2800        0.0000             nan     0.2895   -0.0000
  2820        0.0000             nan     0.2895   -0.0000
  2840        0.0000             nan     0.2895   -0.0000
  2860        0.0000             nan     0.2895   -0.0000
  2880        0.0000             nan     0.2895   -0.0000
  2900        0.0000             nan     0.2895   -0.0000
  2920        0.0000             nan     0.2895   -0.0000
  2940        0.0000             nan     0.2895   -0.0000
  2960        0.0000             nan     0.2895   -0.0000
  2980        0.0000             nan     0.2895   -0.0000
  3000        0.0000             nan     0.2895   -0.0000
  3020        0.0000             nan     0.2895   -0.0000
  3040        0.0000             nan     0.2895   -0.0000
  3060        0.0000             nan     0.2895   -0.0000
  3080        0.0000             nan     0.2895   -0.0000
  3100        0.0000             nan     0.2895   -0.0000
  3120        0.0000             nan     0.2895   -0.0000
  3140        0.0000             nan     0.2895   -0.0000
  3160        0.0000             nan     0.2895   -0.0000
  3180        0.0000             nan     0.2895   -0.0000
  3200        0.0000             nan     0.2895   -0.0000
  3220        0.0000             nan     0.2895   -0.0000
  3240        0.0000             nan     0.2895   -0.0000
  3260        0.0000             nan     0.2895   -0.0000
  3280        0.0000             nan     0.2895    0.0000
  3300        0.0000             nan     0.2895   -0.0000
  3320        0.0000             nan     0.2895   -0.0000
  3340        0.0000             nan     0.2895   -0.0000
  3360        0.0000             nan     0.2895   -0.0000
  3380        0.0000             nan     0.2895   -0.0000
  3400        0.0000             nan     0.2895   -0.0000
  3420        0.0000             nan     0.2895   -0.0000
  3440        0.0000             nan     0.2895   -0.0000
  3460        0.0000             nan     0.2895   -0.0000
  3480        0.0000             nan     0.2895   -0.0000
  3500        0.0000             nan     0.2895   -0.0000
  3520        0.0000             nan     0.2895   -0.0000
  3540        0.0000             nan     0.2895   -0.0000
  3560        0.0000             nan     0.2895   -0.0000
  3580        0.0000             nan     0.2895   -0.0000
  3600        0.0000             nan     0.2895   -0.0000
  3620        0.0000             nan     0.2895   -0.0000
  3640        0.0000             nan     0.2895   -0.0000
  3660        0.0000             nan     0.2895   -0.0000
  3680        0.0000             nan     0.2895   -0.0000
  3700        0.0000             nan     0.2895   -0.0000
  3720        0.0000             nan     0.2895   -0.0000
  3740        0.0000             nan     0.2895   -0.0000
  3760        0.0000             nan     0.2895   -0.0000
  3780        0.0000             nan     0.2895   -0.0000
  3800        0.0000             nan     0.2895   -0.0000
  3820        0.0000             nan     0.2895   -0.0000
  3840        0.0000             nan     0.2895   -0.0000
  3860        0.0000             nan     0.2895   -0.0000
  3880        0.0000             nan     0.2895   -0.0000
  3900        0.0000             nan     0.2895   -0.0000
  3920        0.0000             nan     0.2895   -0.0000
  3940        0.0000             nan     0.2895   -0.0000
  3960        0.0000             nan     0.2895   -0.0000
  3980        0.0000             nan     0.2895   -0.0000
  4000        0.0000             nan     0.2895   -0.0000
  4020        0.0000             nan     0.2895   -0.0000
  4040        0.0000             nan     0.2895   -0.0000
  4060        0.0000             nan     0.2895   -0.0000
  4080        0.0000             nan     0.2895   -0.0000
  4100        0.0000             nan     0.2895   -0.0000
  4120        0.0000             nan     0.2895   -0.0000
  4140        0.0000             nan     0.2895   -0.0000
  4160        0.0000             nan     0.2895    0.0000
  4180        0.0000             nan     0.2895   -0.0000
  4200        0.0000             nan     0.2895   -0.0000
  4220        0.0000             nan     0.2895   -0.0000
  4240        0.0000             nan     0.2895   -0.0000
  4260        0.0000             nan     0.2895   -0.0000
  4280        0.0000             nan     0.2895   -0.0000
  4300        0.0000             nan     0.2895   -0.0000
  4320        0.0000             nan     0.2895   -0.0000
  4340        0.0000             nan     0.2895   -0.0000
  4360        0.0000             nan     0.2895   -0.0000
  4373        0.0000             nan     0.2895   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.4686             nan     0.3621    0.2001
     2        0.3952             nan     0.3621    0.0961
     3        0.3080             nan     0.3621    0.0188
     4        0.2852             nan     0.3621   -0.0024
     5        0.2876             nan     0.3621   -0.0446
     6        0.2850             nan     0.3621   -0.0586
     7        0.2809             nan     0.3621   -0.0413
     8        0.2579             nan     0.3621   -0.0115
     9        0.2638             nan     0.3621   -0.0540
    10        0.2250             nan     0.3621   -0.0177
    20        0.1650             nan     0.3621   -0.0327
    40        0.1022             nan     0.3621   -0.0031
    60        0.0443             nan     0.3621   -0.0057
    80        0.0226             nan     0.3621   -0.0039
   100        0.0091             nan     0.3621   -0.0005
   120        0.0038             nan     0.3621   -0.0005
   140        0.0019             nan     0.3621   -0.0002
   160        0.0008             nan     0.3621   -0.0000
   180        0.0003             nan     0.3621   -0.0000
   200        0.0002             nan     0.3621   -0.0000
   220        0.0001             nan     0.3621   -0.0000
   240        0.0000             nan     0.3621   -0.0000
   260        0.0000             nan     0.3621   -0.0000
   280        0.0000             nan     0.3621   -0.0000
   300        0.0000             nan     0.3621   -0.0000
   320        0.0000             nan     0.3621   -0.0000
   340        0.0000             nan     0.3621   -0.0000
   360        0.0000             nan     0.3621   -0.0000
   380        0.0000             nan     0.3621   -0.0000
   400        0.0000             nan     0.3621   -0.0000
   420        0.0000             nan     0.3621   -0.0000
   440        0.0000             nan     0.3621   -0.0000
   460        0.0000             nan     0.3621   -0.0000
   480        0.0000             nan     0.3621   -0.0000
   500        0.0000             nan     0.3621   -0.0000
   520        0.0000             nan     0.3621   -0.0000
   540        0.0000             nan     0.3621   -0.0000
   560        0.0000             nan     0.3621    0.0000
   580        0.0000             nan     0.3621   -0.0000
   600        0.0000             nan     0.3621   -0.0000
   620        0.0000             nan     0.3621   -0.0000
   640        0.0000             nan     0.3621   -0.0000
   660        0.0000             nan     0.3621   -0.0000
   680        0.0000             nan     0.3621   -0.0000
   700        0.0000             nan     0.3621   -0.0000
   720        0.0000             nan     0.3621   -0.0000
   740        0.0000             nan     0.3621   -0.0000
   760        0.0000             nan     0.3621   -0.0000
   780        0.0000             nan     0.3621   -0.0000
   800        0.0000             nan     0.3621   -0.0000
   820        0.0000             nan     0.3621   -0.0000
   840        0.0000             nan     0.3621   -0.0000
   860        0.0000             nan     0.3621   -0.0000
   880        0.0000             nan     0.3621   -0.0000
   900        0.0000             nan     0.3621   -0.0000
   920        0.0000             nan     0.3621   -0.0000
   940        0.0000             nan     0.3621   -0.0000
   960        0.0000             nan     0.3621   -0.0000
   980        0.0000             nan     0.3621   -0.0000
  1000        0.0000             nan     0.3621   -0.0000
  1020        0.0000             nan     0.3621   -0.0000
  1040        0.0000             nan     0.3621   -0.0000
  1060        0.0000             nan     0.3621   -0.0000
  1080        0.0000             nan     0.3621   -0.0000
  1100        0.0000             nan     0.3621   -0.0000
  1120        0.0000             nan     0.3621   -0.0000
  1140        0.0000             nan     0.3621   -0.0000
  1160        0.0000             nan     0.3621   -0.0000
  1180        0.0000             nan     0.3621   -0.0000
  1200        0.0000             nan     0.3621   -0.0000
  1220        0.0000             nan     0.3621   -0.0000
  1240        0.0000             nan     0.3621   -0.0000
  1260        0.0000             nan     0.3621   -0.0000
  1280        0.0000             nan     0.3621   -0.0000
  1300        0.0000             nan     0.3621   -0.0000
  1320        0.0000             nan     0.3621   -0.0000
  1340        0.0000             nan     0.3621   -0.0000
  1360        0.0000             nan     0.3621   -0.0000
  1380        0.0000             nan     0.3621   -0.0000
  1400        0.0000             nan     0.3621   -0.0000
  1420        0.0000             nan     0.3621   -0.0000
  1440        0.0000             nan     0.3621   -0.0000
  1460        0.0000             nan     0.3621   -0.0000
  1480        0.0000             nan     0.3621   -0.0000
  1500        0.0000             nan     0.3621   -0.0000
  1520        0.0000             nan     0.3621   -0.0000
  1540        0.0000             nan     0.3621   -0.0000
  1560        0.0000             nan     0.3621   -0.0000
  1580        0.0000             nan     0.3621   -0.0000
  1600        0.0000             nan     0.3621   -0.0000
  1620        0.0000             nan     0.3621   -0.0000
  1640        0.0000             nan     0.3621   -0.0000
  1660        0.0000             nan     0.3621   -0.0000
  1680        0.0000             nan     0.3621   -0.0000
  1700        0.0000             nan     0.3621   -0.0000
  1720        0.0000             nan     0.3621   -0.0000
  1740        0.0000             nan     0.3621    0.0000
  1760        0.0000             nan     0.3621   -0.0000
  1780        0.0000             nan     0.3621   -0.0000
  1800        0.0000             nan     0.3621   -0.0000
  1820        0.0000             nan     0.3621   -0.0000
  1840        0.0000             nan     0.3621   -0.0000
  1860        0.0000             nan     0.3621   -0.0000
  1880        0.0000             nan     0.3621   -0.0000
  1900        0.0000             nan     0.3621   -0.0000
  1920        0.0000             nan     0.3621   -0.0000
  1940        0.0000             nan     0.3621    0.0000
  1960        0.0000             nan     0.3621   -0.0000
  1980        0.0000             nan     0.3621    0.0000
  2000        0.0000             nan     0.3621   -0.0000
  2020        0.0000             nan     0.3621   -0.0000
  2040        0.0000             nan     0.3621   -0.0000
  2060        0.0000             nan     0.3621   -0.0000
  2080        0.0000             nan     0.3621   -0.0000
  2100        0.0000             nan     0.3621   -0.0000
  2120        0.0000             nan     0.3621   -0.0000
  2140        0.0000             nan     0.3621   -0.0000
  2160        0.0000             nan     0.3621   -0.0000
  2180        0.0000             nan     0.3621   -0.0000
  2200        0.0000             nan     0.3621    0.0000
  2220        0.0000             nan     0.3621   -0.0000
  2240        0.0000             nan     0.3621   -0.0000
  2260        0.0000             nan     0.3621   -0.0000
  2280        0.0000             nan     0.3621   -0.0000
  2300        0.0000             nan     0.3621   -0.0000
  2320        0.0000             nan     0.3621   -0.0000
  2340        0.0000             nan     0.3621   -0.0000
  2360        0.0000             nan     0.3621   -0.0000
  2380        0.0000             nan     0.3621   -0.0000
  2400        0.0000             nan     0.3621    0.0000
  2420        0.0000             nan     0.3621   -0.0000
  2440        0.0000             nan     0.3621   -0.0000
  2460        0.0000             nan     0.3621   -0.0000
  2480        0.0000             nan     0.3621   -0.0000
  2500        0.0000             nan     0.3621   -0.0000
  2520        0.0000             nan     0.3621   -0.0000
  2540        0.0000             nan     0.3621   -0.0000
  2560        0.0000             nan     0.3621   -0.0000
  2580        0.0000             nan     0.3621   -0.0000
  2600        0.0000             nan     0.3621    0.0000
  2620        0.0000             nan     0.3621   -0.0000
  2640        0.0000             nan     0.3621   -0.0000
  2660        0.0000             nan     0.3621   -0.0000
  2680        0.0000             nan     0.3621   -0.0000
  2700        0.0000             nan     0.3621   -0.0000
  2720        0.0000             nan     0.3621   -0.0000
  2740        0.0000             nan     0.3621   -0.0000
  2760        0.0000             nan     0.3621   -0.0000
  2780        0.0000             nan     0.3621   -0.0000
  2800        0.0000             nan     0.3621   -0.0000
  2820        0.0000             nan     0.3621   -0.0000
  2840        0.0000             nan     0.3621    0.0000
  2860        0.0000             nan     0.3621   -0.0000
  2880        0.0000             nan     0.3621   -0.0000
  2900        0.0000             nan     0.3621   -0.0000
  2920        0.0000             nan     0.3621   -0.0000
  2940        0.0000             nan     0.3621   -0.0000
  2960        0.0000             nan     0.3621   -0.0000
  2980        0.0000             nan     0.3621    0.0000
  3000        0.0000             nan     0.3621   -0.0000
  3020        0.0000             nan     0.3621   -0.0000
  3040        0.0000             nan     0.3621   -0.0000
  3060        0.0000             nan     0.3621   -0.0000
  3080        0.0000             nan     0.3621   -0.0000
  3100        0.0000             nan     0.3621   -0.0000
  3120        0.0000             nan     0.3621   -0.0000
  3140        0.0000             nan     0.3621   -0.0000
  3160        0.0000             nan     0.3621    0.0000
  3180        0.0000             nan     0.3621   -0.0000
  3200        0.0000             nan     0.3621   -0.0000
  3220        0.0000             nan     0.3621   -0.0000
  3240        0.0000             nan     0.3621   -0.0000
  3260        0.0000             nan     0.3621   -0.0000
  3280        0.0000             nan     0.3621   -0.0000
  3300        0.0000             nan     0.3621   -0.0000
  3320        0.0000             nan     0.3621   -0.0000
  3340        0.0000             nan     0.3621   -0.0000
  3360        0.0000             nan     0.3621   -0.0000
  3380        0.0000             nan     0.3621   -0.0000
  3400        0.0000             nan     0.3621   -0.0000
  3420        0.0000             nan     0.3621   -0.0000
  3440        0.0000             nan     0.3621    0.0000
  3460        0.0000             nan     0.3621   -0.0000
  3480        0.0000             nan     0.3621   -0.0000
  3500        0.0000             nan     0.3621   -0.0000
  3520        0.0000             nan     0.3621   -0.0000
  3540        0.0000             nan     0.3621   -0.0000
  3560        0.0000             nan     0.3621   -0.0000
  3580        0.0000             nan     0.3621   -0.0000
  3600        0.0000             nan     0.3621   -0.0000
  3620        0.0000             nan     0.3621   -0.0000
  3640        0.0000             nan     0.3621   -0.0000
  3660        0.0000             nan     0.3621   -0.0000
  3680        0.0000             nan     0.3621   -0.0000
  3700        0.0000             nan     0.3621   -0.0000
  3720        0.0000             nan     0.3621   -0.0000
  3740        0.0000             nan     0.3621   -0.0000
  3760        0.0000             nan     0.3621   -0.0000
  3780        0.0000             nan     0.3621   -0.0000
  3799        0.0000             nan     0.3621   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3560             nan     0.4467    0.2118
     2        0.3659             nan     0.4467   -0.0613
     3        0.2747             nan     0.4467    0.1026
     4        0.2133             nan     0.4467    0.0320
     5        0.1866             nan     0.4467   -0.0516
     6        0.1816             nan     0.4467   -0.0315
     7        0.1705             nan     0.4467   -0.0122
     8        0.1589             nan     0.4467   -0.0169
     9        0.1370             nan     0.4467    0.0139
    10        0.1393             nan     0.4467   -0.0267
    20        0.0865             nan     0.4467   -0.0072
    40        0.0224             nan     0.4467   -0.0006
    60        0.0047             nan     0.4467   -0.0010
    80        0.0014             nan     0.4467   -0.0005
   100        0.0003             nan     0.4467   -0.0000
   120        0.0001             nan     0.4467   -0.0000
   140        0.0000             nan     0.4467   -0.0000
   160        0.0000             nan     0.4467   -0.0000
   180        0.0000             nan     0.4467   -0.0000
   200        0.0000             nan     0.4467   -0.0000
   220        0.0000             nan     0.4467   -0.0000
   240        0.0000             nan     0.4467   -0.0000
   260        0.0000             nan     0.4467   -0.0000
   280        0.0000             nan     0.4467   -0.0000
   300        0.0000             nan     0.4467   -0.0000
   320        0.0000             nan     0.4467   -0.0000
   340        0.0000             nan     0.4467   -0.0000
   360        0.0000             nan     0.4467   -0.0000
   380        0.0000             nan     0.4467   -0.0000
   400        0.0000             nan     0.4467    0.0000
   420        0.0000             nan     0.4467   -0.0000
   440        0.0000             nan     0.4467   -0.0000
   460        0.0000             nan     0.4467   -0.0000
   480        0.0000             nan     0.4467   -0.0000
   500        0.0000             nan     0.4467   -0.0000
   520        0.0000             nan     0.4467   -0.0000
   540        0.0000             nan     0.4467   -0.0000
   560        0.0000             nan     0.4467   -0.0000
   580        0.0000             nan     0.4467   -0.0000
   600        0.0000             nan     0.4467   -0.0000
   620        0.0000             nan     0.4467   -0.0000
   640        0.0000             nan     0.4467   -0.0000
   660        0.0000             nan     0.4467    0.0000
   680        0.0000             nan     0.4467   -0.0000
   700        0.0000             nan     0.4467   -0.0000
   720        0.0000             nan     0.4467   -0.0000
   740        0.0000             nan     0.4467   -0.0000
   760        0.0000             nan     0.4467    0.0000
   780        0.0000             nan     0.4467   -0.0000
   800        0.0000             nan     0.4467   -0.0000
   820        0.0000             nan     0.4467   -0.0000
   840        0.0000             nan     0.4467   -0.0000
   860        0.0000             nan     0.4467   -0.0000
   880        0.0000             nan     0.4467   -0.0000
   900        0.0000             nan     0.4467   -0.0000
   920        0.0000             nan     0.4467   -0.0000
   940        0.0000             nan     0.4467   -0.0000
   960        0.0000             nan     0.4467   -0.0000
   980        0.0000             nan     0.4467   -0.0000
  1000        0.0000             nan     0.4467   -0.0000
  1020        0.0000             nan     0.4467   -0.0000
  1040        0.0000             nan     0.4467   -0.0000
  1060        0.0000             nan     0.4467   -0.0000
  1080        0.0000             nan     0.4467   -0.0000
  1100        0.0000             nan     0.4467   -0.0000
  1120        0.0000             nan     0.4467   -0.0000
  1140        0.0000             nan     0.4467   -0.0000
  1160        0.0000             nan     0.4467    0.0000
  1180        0.0000             nan     0.4467   -0.0000
  1200        0.0000             nan     0.4467    0.0000
  1220        0.0000             nan     0.4467   -0.0000
  1240        0.0000             nan     0.4467   -0.0000
  1260        0.0000             nan     0.4467   -0.0000
  1280        0.0000             nan     0.4467   -0.0000
  1300        0.0000             nan     0.4467   -0.0000
  1320        0.0000             nan     0.4467   -0.0000
  1340        0.0000             nan     0.4467   -0.0000
  1360        0.0000             nan     0.4467    0.0000
  1380        0.0000             nan     0.4467   -0.0000
  1400        0.0000             nan     0.4467   -0.0000
  1420        0.0000             nan     0.4467   -0.0000
  1440        0.0000             nan     0.4467   -0.0000
  1460        0.0000             nan     0.4467   -0.0000
  1480        0.0000             nan     0.4467   -0.0000
  1500        0.0000             nan     0.4467   -0.0000
  1520        0.0000             nan     0.4467   -0.0000
  1540        0.0000             nan     0.4467   -0.0000
  1560        0.0000             nan     0.4467   -0.0000
  1580        0.0000             nan     0.4467   -0.0000
  1600        0.0000             nan     0.4467   -0.0000
  1620        0.0000             nan     0.4467   -0.0000
  1640        0.0000             nan     0.4467   -0.0000
  1660        0.0000             nan     0.4467   -0.0000
  1680        0.0000             nan     0.4467   -0.0000
  1700        0.0000             nan     0.4467   -0.0000
  1720        0.0000             nan     0.4467    0.0000
  1740        0.0000             nan     0.4467   -0.0000
  1760        0.0000             nan     0.4467   -0.0000
  1780        0.0000             nan     0.4467   -0.0000
  1800        0.0000             nan     0.4467   -0.0000
  1820        0.0000             nan     0.4467   -0.0000
  1840        0.0000             nan     0.4467   -0.0000
  1860        0.0000             nan     0.4467   -0.0000
  1880        0.0000             nan     0.4467   -0.0000
  1900        0.0000             nan     0.4467   -0.0000
  1920        0.0000             nan     0.4467   -0.0000
  1940        0.0000             nan     0.4467   -0.0000
  1960        0.0000             nan     0.4467   -0.0000
  1980        0.0000             nan     0.4467   -0.0000
  2000        0.0000             nan     0.4467   -0.0000
  2020        0.0000             nan     0.4467   -0.0000
  2040        0.0000             nan     0.4467   -0.0000
  2060        0.0000             nan     0.4467   -0.0000
  2080        0.0000             nan     0.4467   -0.0000
  2100        0.0000             nan     0.4467   -0.0000
  2120        0.0000             nan     0.4467   -0.0000
  2140        0.0000             nan     0.4467   -0.0000
  2160        0.0000             nan     0.4467    0.0000
  2180        0.0000             nan     0.4467   -0.0000
  2200        0.0000             nan     0.4467   -0.0000
  2220        0.0000             nan     0.4467   -0.0000
  2232        0.0000             nan     0.4467    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3348             nan     0.5710    0.2038
     2        0.2826             nan     0.5710    0.0186
     3        0.2765             nan     0.5710   -0.0096
     4        0.2601             nan     0.5710    0.0009
     5        0.2338             nan     0.5710   -0.0204
     6        0.2173             nan     0.5710   -0.0161
     7        0.2099             nan     0.5710   -0.0190
     8        0.2017             nan     0.5710   -0.0409
     9        0.1496             nan     0.5710    0.0163
    10        0.1432             nan     0.5710   -0.0490
    20        0.0644             nan     0.5710   -0.0231
    40        0.0130             nan     0.5710   -0.0057
    60        0.0057             nan     0.5710   -0.0014
    80        0.0023             nan     0.5710   -0.0011
   100        0.0008             nan     0.5710    0.0001
   120        0.0003             nan     0.5710   -0.0001
   140        0.0001             nan     0.5710   -0.0000
   160        0.0000             nan     0.5710   -0.0000
   180        0.0000             nan     0.5710   -0.0000
   200        0.0000             nan     0.5710   -0.0000
   220        0.0000             nan     0.5710   -0.0000
   240        0.0000             nan     0.5710   -0.0000
   260        0.0000             nan     0.5710   -0.0000
   280        0.0000             nan     0.5710   -0.0000
   300        0.0000             nan     0.5710   -0.0000
   320        0.0000             nan     0.5710   -0.0000
   340        0.0000             nan     0.5710   -0.0000
   360        0.0000             nan     0.5710   -0.0000
   380        0.0000             nan     0.5710   -0.0000
   400        0.0000             nan     0.5710   -0.0000
   420        0.0000             nan     0.5710   -0.0000
   440        0.0000             nan     0.5710   -0.0000
   460        0.0000             nan     0.5710   -0.0000
   480        0.0000             nan     0.5710   -0.0000
   500        0.0000             nan     0.5710   -0.0000
   520        0.0000             nan     0.5710   -0.0000
   540        0.0000             nan     0.5710   -0.0000
   560        0.0000             nan     0.5710   -0.0000
   580        0.0000             nan     0.5710   -0.0000
   600        0.0000             nan     0.5710   -0.0000
   620        0.0000             nan     0.5710   -0.0000
   640        0.0000             nan     0.5710   -0.0000
   660        0.0000             nan     0.5710    0.0000
   680        0.0000             nan     0.5710   -0.0000
   700        0.0000             nan     0.5710   -0.0000
   720        0.0000             nan     0.5710   -0.0000
   740        0.0000             nan     0.5710   -0.0000
   760        0.0000             nan     0.5710   -0.0000
   780        0.0000             nan     0.5710   -0.0000
   800        0.0000             nan     0.5710   -0.0000
   820        0.0000             nan     0.5710   -0.0000
   840        0.0000             nan     0.5710   -0.0000
   860        0.0000             nan     0.5710   -0.0000
   880        0.0000             nan     0.5710   -0.0000
   900        0.0000             nan     0.5710   -0.0000
   920        0.0000             nan     0.5710   -0.0000
   940        0.0000             nan     0.5710   -0.0000
   960        0.0000             nan     0.5710   -0.0000
   980        0.0000             nan     0.5710    0.0000
  1000        0.0000             nan     0.5710   -0.0000
  1020        0.0000             nan     0.5710   -0.0000
  1040        0.0000             nan     0.5710   -0.0000
  1060        0.0000             nan     0.5710   -0.0000
  1080        0.0000             nan     0.5710   -0.0000
  1100        0.0000             nan     0.5710    0.0000
  1120        0.0000             nan     0.5710   -0.0000
  1140        0.0000             nan     0.5710   -0.0000
  1160        0.0000             nan     0.5710   -0.0000
  1180        0.0000             nan     0.5710   -0.0000
  1200        0.0000             nan     0.5710   -0.0000
  1220        0.0000             nan     0.5710    0.0000
  1240        0.0000             nan     0.5710   -0.0000
  1260        0.0000             nan     0.5710   -0.0000
  1280        0.0000             nan     0.5710   -0.0000
  1300        0.0000             nan     0.5710   -0.0000
  1320        0.0000             nan     0.5710   -0.0000
  1340        0.0000             nan     0.5710   -0.0000
  1360        0.0000             nan     0.5710   -0.0000
  1380        0.0000             nan     0.5710   -0.0000
  1400        0.0000             nan     0.5710   -0.0000
  1420        0.0000             nan     0.5710   -0.0000
  1440        0.0000             nan     0.5710   -0.0000
  1460        0.0000             nan     0.5710   -0.0000
  1480        0.0000             nan     0.5710   -0.0000
  1500        0.0000             nan     0.5710   -0.0000
  1520        0.0000             nan     0.5710   -0.0000
  1540        0.0000             nan     0.5710   -0.0000
  1560        0.0000             nan     0.5710   -0.0000
  1580        0.0000             nan     0.5710   -0.0000
  1600        0.0000             nan     0.5710   -0.0000
  1620        0.0000             nan     0.5710   -0.0000
  1640        0.0000             nan     0.5710   -0.0000
  1660        0.0000             nan     0.5710   -0.0000
  1680        0.0000             nan     0.5710   -0.0000
  1700        0.0000             nan     0.5710   -0.0000
  1720        0.0000             nan     0.5710   -0.0000
  1740        0.0000             nan     0.5710   -0.0000
  1760        0.0000             nan     0.5710   -0.0000
  1780        0.0000             nan     0.5710   -0.0000
  1800        0.0000             nan     0.5710    0.0000
  1820        0.0000             nan     0.5710   -0.0000
  1840        0.0000             nan     0.5710   -0.0000
  1860        0.0000             nan     0.5710    0.0000
  1880        0.0000             nan     0.5710   -0.0000
  1900        0.0000             nan     0.5710   -0.0000
  1920        0.0000             nan     0.5710   -0.0000
  1940        0.0000             nan     0.5710   -0.0000
  1960        0.0000             nan     0.5710   -0.0000
  1980        0.0000             nan     0.5710   -0.0000
  2000        0.0000             nan     0.5710   -0.0000
  2020        0.0000             nan     0.5710   -0.0000
  2040        0.0000             nan     0.5710   -0.0000
  2060        0.0000             nan     0.5710   -0.0000
  2080        0.0000             nan     0.5710   -0.0000
  2100        0.0000             nan     0.5710   -0.0000
  2120        0.0000             nan     0.5710   -0.0000
  2140        0.0000             nan     0.5710   -0.0000
  2160        0.0000             nan     0.5710   -0.0000
  2180        0.0000             nan     0.5710   -0.0000
  2200        0.0000             nan     0.5710    0.0000
  2220        0.0000             nan     0.5710   -0.0000
  2240        0.0000             nan     0.5710   -0.0000
  2260        0.0000             nan     0.5710   -0.0000
  2280        0.0000             nan     0.5710   -0.0000
  2300        0.0000             nan     0.5710   -0.0000
  2320        0.0000             nan     0.5710   -0.0000
  2340        0.0000             nan     0.5710   -0.0000
  2360        0.0000             nan     0.5710   -0.0000
  2380        0.0000             nan     0.5710    0.0000
  2400        0.0000             nan     0.5710   -0.0000
  2420        0.0000             nan     0.5710   -0.0000
  2440        0.0000             nan     0.5710   -0.0000
  2460        0.0000             nan     0.5710   -0.0000
  2480        0.0000             nan     0.5710   -0.0000
  2500        0.0000             nan     0.5710   -0.0000
  2520        0.0000             nan     0.5710   -0.0000
  2540        0.0000             nan     0.5710   -0.0000
  2560        0.0000             nan     0.5710   -0.0000
  2580        0.0000             nan     0.5710   -0.0000
  2600        0.0000             nan     0.5710   -0.0000
  2620        0.0000             nan     0.5710   -0.0000
  2640        0.0000             nan     0.5710    0.0000
  2660        0.0000             nan     0.5710   -0.0000
  2680        0.0000             nan     0.5710   -0.0000
  2700        0.0000             nan     0.5710   -0.0000
  2720        0.0000             nan     0.5710   -0.0000
  2740        0.0000             nan     0.5710   -0.0000
  2760        0.0000             nan     0.5710   -0.0000
  2780        0.0000             nan     0.5710   -0.0000
  2800        0.0000             nan     0.5710   -0.0000
  2820        0.0000             nan     0.5710   -0.0000
  2840        0.0000             nan     0.5710   -0.0000
  2860        0.0000             nan     0.5710   -0.0000
  2880        0.0000             nan     0.5710   -0.0000
  2900        0.0000             nan     0.5710   -0.0000
  2920        0.0000             nan     0.5710   -0.0000
  2940        0.0000             nan     0.5710   -0.0000
  2960        0.0000             nan     0.5710   -0.0000
  2980        0.0000             nan     0.5710   -0.0000
  3000        0.0000             nan     0.5710    0.0000
  3020        0.0000             nan     0.5710   -0.0000
  3040        0.0000             nan     0.5710   -0.0000
  3060        0.0000             nan     0.5710   -0.0000
  3080        0.0000             nan     0.5710   -0.0000
  3089        0.0000             nan     0.5710   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5491             nan     0.0081    0.0046
     2        0.5436             nan     0.0081    0.0047
     3        0.5380             nan     0.0081    0.0050
     4        0.5336             nan     0.0081    0.0050
     5        0.5276             nan     0.0081    0.0046
     6        0.5233             nan     0.0081    0.0047
     7        0.5197             nan     0.0081    0.0037
     8        0.5161             nan     0.0081    0.0045
     9        0.5102             nan     0.0081    0.0037
    10        0.5073             nan     0.0081    0.0038
    20        0.4721             nan     0.0081    0.0032
    40        0.4040             nan     0.0081    0.0029
    60        0.3531             nan     0.0081    0.0018
    80        0.3158             nan     0.0081    0.0011
   100        0.2915             nan     0.0081    0.0005
   120        0.2734             nan     0.0081    0.0009
   140        0.2562             nan     0.0081    0.0007
   160        0.2407             nan     0.0081    0.0004
   180        0.2306             nan     0.0081   -0.0003
   200        0.2211             nan     0.0081   -0.0004
   220        0.2128             nan     0.0081   -0.0001
   240        0.2063             nan     0.0081   -0.0006
   260        0.2000             nan     0.0081    0.0002
   280        0.1951             nan     0.0081    0.0004
   300        0.1861             nan     0.0081    0.0002
   320        0.1808             nan     0.0081   -0.0001
   340        0.1741             nan     0.0081   -0.0002
   360        0.1702             nan     0.0081    0.0000
   380        0.1664             nan     0.0081   -0.0002
   400        0.1607             nan     0.0081   -0.0000
   420        0.1567             nan     0.0081   -0.0002
   440        0.1517             nan     0.0081   -0.0001
   460        0.1466             nan     0.0081   -0.0002
   480        0.1420             nan     0.0081   -0.0002
   500        0.1381             nan     0.0081   -0.0001
   520        0.1338             nan     0.0081    0.0000
   540        0.1313             nan     0.0081   -0.0001
   560        0.1288             nan     0.0081   -0.0003
   580        0.1239             nan     0.0081   -0.0001
   600        0.1207             nan     0.0081   -0.0004
   620        0.1184             nan     0.0081   -0.0001
   640        0.1150             nan     0.0081   -0.0002
   660        0.1117             nan     0.0081   -0.0001
   680        0.1087             nan     0.0081   -0.0001
   700        0.1067             nan     0.0081   -0.0001
   720        0.1042             nan     0.0081   -0.0001
   740        0.1023             nan     0.0081   -0.0003
   760        0.1005             nan     0.0081   -0.0002
   780        0.0987             nan     0.0081   -0.0002
   800        0.0967             nan     0.0081   -0.0004
   820        0.0949             nan     0.0081   -0.0001
   840        0.0930             nan     0.0081   -0.0001
   860        0.0911             nan     0.0081   -0.0000
   880        0.0888             nan     0.0081   -0.0002
   900        0.0868             nan     0.0081   -0.0002
   920        0.0847             nan     0.0081   -0.0002
   940        0.0826             nan     0.0081   -0.0001
   960        0.0809             nan     0.0081   -0.0002
   980        0.0789             nan     0.0081   -0.0001
  1000        0.0764             nan     0.0081    0.0000
  1020        0.0744             nan     0.0081   -0.0002
  1040        0.0728             nan     0.0081   -0.0001
  1060        0.0712             nan     0.0081   -0.0002
  1080        0.0694             nan     0.0081   -0.0000
  1100        0.0686             nan     0.0081   -0.0002
  1120        0.0676             nan     0.0081   -0.0002
  1140        0.0663             nan     0.0081   -0.0001
  1160        0.0649             nan     0.0081   -0.0002
  1180        0.0630             nan     0.0081   -0.0001
  1200        0.0620             nan     0.0081   -0.0002
  1220        0.0606             nan     0.0081   -0.0001
  1231        0.0602             nan     0.0081   -0.0002

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5454             nan     0.0234    0.0066
     2        0.5386             nan     0.0234    0.0092
     3        0.5291             nan     0.0234    0.0100
     4        0.5234             nan     0.0234    0.0020
     5        0.5126             nan     0.0234    0.0094
     6        0.5087             nan     0.0234    0.0048
     7        0.4996             nan     0.0234    0.0089
     8        0.4929             nan     0.0234    0.0082
     9        0.4854             nan     0.0234    0.0077
    10        0.4790             nan     0.0234    0.0070
    20        0.4132             nan     0.0234    0.0051
    40        0.3375             nan     0.0234    0.0003
    60        0.2938             nan     0.0234    0.0010
    80        0.2661             nan     0.0234    0.0003
   100        0.2397             nan     0.0234    0.0004
   120        0.2228             nan     0.0234   -0.0001
   140        0.2077             nan     0.0234   -0.0003
   160        0.1972             nan     0.0234   -0.0005
   180        0.1878             nan     0.0234    0.0000
   200        0.1800             nan     0.0234   -0.0002
   220        0.1731             nan     0.0234   -0.0007
   240        0.1677             nan     0.0234   -0.0008
   260        0.1627             nan     0.0234   -0.0007
   280        0.1564             nan     0.0234   -0.0006
   300        0.1490             nan     0.0234   -0.0005
   320        0.1446             nan     0.0234   -0.0011
   340        0.1414             nan     0.0234   -0.0005
   360        0.1383             nan     0.0234   -0.0009
   380        0.1357             nan     0.0234    0.0001
   400        0.1325             nan     0.0234   -0.0007
   420        0.1281             nan     0.0234   -0.0003
   440        0.1238             nan     0.0234   -0.0004
   460        0.1206             nan     0.0234   -0.0002
   480        0.1179             nan     0.0234   -0.0003
   500        0.1160             nan     0.0234   -0.0002
   520        0.1137             nan     0.0234   -0.0005
   540        0.1104             nan     0.0234   -0.0008
   560        0.1079             nan     0.0234   -0.0004
   580        0.1061             nan     0.0234   -0.0008
   600        0.1046             nan     0.0234   -0.0007
   620        0.1029             nan     0.0234   -0.0010
   640        0.1007             nan     0.0234   -0.0005
   660        0.0990             nan     0.0234   -0.0004
   680        0.0970             nan     0.0234   -0.0004
   700        0.0959             nan     0.0234   -0.0005
   720        0.0943             nan     0.0234   -0.0004
   740        0.0926             nan     0.0234   -0.0005
   760        0.0910             nan     0.0234   -0.0007
   780        0.0895             nan     0.0234   -0.0005
   800        0.0873             nan     0.0234   -0.0004
   820        0.0861             nan     0.0234   -0.0003
   840        0.0851             nan     0.0234   -0.0005
   860        0.0831             nan     0.0234   -0.0003
   880        0.0812             nan     0.0234   -0.0008
   900        0.0797             nan     0.0234   -0.0009
   920        0.0782             nan     0.0234   -0.0006
   940        0.0767             nan     0.0234   -0.0004
   960        0.0754             nan     0.0234   -0.0003
   980        0.0737             nan     0.0234   -0.0002
  1000        0.0729             nan     0.0234   -0.0002
  1020        0.0720             nan     0.0234   -0.0005
  1040        0.0703             nan     0.0234   -0.0003
  1060        0.0690             nan     0.0234   -0.0003
  1080        0.0679             nan     0.0234   -0.0001
  1100        0.0669             nan     0.0234   -0.0003
  1120        0.0660             nan     0.0234   -0.0003
  1140        0.0648             nan     0.0234   -0.0005
  1160        0.0637             nan     0.0234   -0.0003
  1180        0.0631             nan     0.0234   -0.0003
  1200        0.0621             nan     0.0234   -0.0004
  1220        0.0610             nan     0.0234   -0.0004
  1240        0.0601             nan     0.0234   -0.0002
  1260        0.0587             nan     0.0234   -0.0002
  1280        0.0579             nan     0.0234   -0.0005
  1300        0.0572             nan     0.0234   -0.0004
  1320        0.0567             nan     0.0234   -0.0004
  1340        0.0557             nan     0.0234   -0.0003
  1360        0.0547             nan     0.0234   -0.0004
  1380        0.0543             nan     0.0234   -0.0003
  1400        0.0535             nan     0.0234   -0.0005
  1420        0.0523             nan     0.0234   -0.0001
  1440        0.0519             nan     0.0234   -0.0004
  1460        0.0509             nan     0.0234   -0.0000
  1480        0.0501             nan     0.0234   -0.0002
  1500        0.0491             nan     0.0234   -0.0001
  1520        0.0486             nan     0.0234   -0.0003
  1540        0.0478             nan     0.0234   -0.0002
  1560        0.0474             nan     0.0234   -0.0002
  1580        0.0468             nan     0.0234   -0.0004
  1600        0.0459             nan     0.0234   -0.0003
  1620        0.0449             nan     0.0234   -0.0002
  1640        0.0444             nan     0.0234   -0.0002
  1660        0.0441             nan     0.0234   -0.0002
  1680        0.0437             nan     0.0234   -0.0001
  1700        0.0428             nan     0.0234   -0.0002
  1720        0.0422             nan     0.0234   -0.0002
  1740        0.0414             nan     0.0234   -0.0004
  1760        0.0408             nan     0.0234   -0.0001
  1780        0.0401             nan     0.0234   -0.0003
  1800        0.0396             nan     0.0234   -0.0003
  1820        0.0392             nan     0.0234   -0.0001
  1840        0.0388             nan     0.0234   -0.0004
  1860        0.0381             nan     0.0234   -0.0002
  1880        0.0374             nan     0.0234   -0.0001
  1900        0.0369             nan     0.0234   -0.0004
  1920        0.0364             nan     0.0234   -0.0002
  1940        0.0358             nan     0.0234   -0.0002
  1960        0.0353             nan     0.0234   -0.0001
  1980        0.0346             nan     0.0234   -0.0001
  2000        0.0341             nan     0.0234   -0.0002
  2020        0.0337             nan     0.0234   -0.0001
  2040        0.0332             nan     0.0234   -0.0001
  2060        0.0327             nan     0.0234   -0.0002
  2080        0.0322             nan     0.0234   -0.0001
  2100        0.0319             nan     0.0234   -0.0002
  2120        0.0312             nan     0.0234   -0.0001
  2140        0.0306             nan     0.0234   -0.0001
  2160        0.0303             nan     0.0234   -0.0002
  2180        0.0299             nan     0.0234   -0.0001
  2200        0.0294             nan     0.0234   -0.0001
  2220        0.0289             nan     0.0234   -0.0002
  2240        0.0286             nan     0.0234   -0.0001
  2260        0.0282             nan     0.0234   -0.0002
  2280        0.0277             nan     0.0234   -0.0001
  2300        0.0274             nan     0.0234   -0.0001
  2320        0.0272             nan     0.0234   -0.0001
  2340        0.0269             nan     0.0234   -0.0001
  2360        0.0265             nan     0.0234   -0.0001
  2380        0.0261             nan     0.0234   -0.0000
  2400        0.0257             nan     0.0234   -0.0002
  2420        0.0252             nan     0.0234   -0.0001
  2440        0.0248             nan     0.0234   -0.0001
  2460        0.0244             nan     0.0234   -0.0001
  2480        0.0240             nan     0.0234   -0.0001
  2500        0.0237             nan     0.0234   -0.0002
  2520        0.0234             nan     0.0234   -0.0001
  2540        0.0231             nan     0.0234   -0.0002
  2560        0.0227             nan     0.0234   -0.0002
  2580        0.0224             nan     0.0234   -0.0001
  2600        0.0221             nan     0.0234   -0.0001
  2620        0.0217             nan     0.0234   -0.0001
  2640        0.0213             nan     0.0234   -0.0001
  2660        0.0210             nan     0.0234   -0.0001
  2680        0.0207             nan     0.0234   -0.0002
  2700        0.0204             nan     0.0234   -0.0001
  2720        0.0201             nan     0.0234   -0.0001
  2740        0.0198             nan     0.0234   -0.0001
  2760        0.0196             nan     0.0234   -0.0001
  2780        0.0193             nan     0.0234   -0.0001
  2800        0.0191             nan     0.0234   -0.0001
  2820        0.0189             nan     0.0234   -0.0001
  2840        0.0186             nan     0.0234   -0.0001
  2860        0.0183             nan     0.0234   -0.0001
  2880        0.0180             nan     0.0234   -0.0002
  2900        0.0179             nan     0.0234   -0.0001
  2920        0.0177             nan     0.0234   -0.0001
  2940        0.0174             nan     0.0234   -0.0001
  2960        0.0171             nan     0.0234   -0.0001
  2980        0.0169             nan     0.0234   -0.0000
  3000        0.0166             nan     0.0234   -0.0001
  3020        0.0165             nan     0.0234   -0.0001
  3040        0.0163             nan     0.0234   -0.0001
  3057        0.0161             nan     0.0234   -0.0001

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5034             nan     0.1203    0.0539
     2        0.4483             nan     0.1203    0.0469
     3        0.4079             nan     0.1203    0.0336
     4        0.3854             nan     0.1203    0.0159
     5        0.3522             nan     0.1203    0.0195
     6        0.3244             nan     0.1203    0.0215
     7        0.2952             nan     0.1203    0.0092
     8        0.2750             nan     0.1203    0.0098
     9        0.2721             nan     0.1203    0.0006
    10        0.2582             nan     0.1203    0.0107
    20        0.2125             nan     0.1203   -0.0060
    40        0.1441             nan     0.1203   -0.0008
    60        0.1231             nan     0.1203   -0.0042
    80        0.0997             nan     0.1203   -0.0045
   100        0.0891             nan     0.1203   -0.0013
   120        0.0769             nan     0.1203   -0.0034
   140        0.0704             nan     0.1203   -0.0024
   160        0.0646             nan     0.1203   -0.0004
   180        0.0598             nan     0.1203   -0.0028
   200        0.0538             nan     0.1203   -0.0011
   220        0.0494             nan     0.1203   -0.0028
   240        0.0450             nan     0.1203   -0.0013
   260        0.0424             nan     0.1203   -0.0019
   280        0.0396             nan     0.1203   -0.0001
   300        0.0362             nan     0.1203   -0.0004
   320        0.0332             nan     0.1203   -0.0014
   340        0.0298             nan     0.1203   -0.0004
   360        0.0265             nan     0.1203   -0.0006
   380        0.0230             nan     0.1203   -0.0003
   400        0.0204             nan     0.1203   -0.0006
   420        0.0192             nan     0.1203   -0.0006
   440        0.0173             nan     0.1203   -0.0005
   460        0.0162             nan     0.1203   -0.0006
   480        0.0148             nan     0.1203   -0.0002
   500        0.0134             nan     0.1203   -0.0003
   520        0.0121             nan     0.1203   -0.0004
   540        0.0110             nan     0.1203   -0.0002
   560        0.0103             nan     0.1203   -0.0002
   580        0.0095             nan     0.1203   -0.0001
   600        0.0091             nan     0.1203   -0.0003
   620        0.0082             nan     0.1203   -0.0001
   640        0.0075             nan     0.1203   -0.0002
   660        0.0070             nan     0.1203   -0.0002
   680        0.0065             nan     0.1203   -0.0002
   700        0.0060             nan     0.1203   -0.0002
   720        0.0056             nan     0.1203   -0.0001
   740        0.0054             nan     0.1203   -0.0002
   760        0.0050             nan     0.1203   -0.0002
   780        0.0045             nan     0.1203   -0.0001
   800        0.0042             nan     0.1203   -0.0001
   820        0.0038             nan     0.1203   -0.0002
   837        0.0037             nan     0.1203   -0.0001

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.4558             nan     0.2260    0.0778
     2        0.4421             nan     0.2260   -0.0402
     3        0.3867             nan     0.2260    0.0456
     4        0.3497             nan     0.2260    0.0066
     5        0.3288             nan     0.2260   -0.0074
     6        0.2968             nan     0.2260    0.0059
     7        0.2751             nan     0.2260    0.0042
     8        0.2821             nan     0.2260   -0.0211
     9        0.2708             nan     0.2260   -0.0047
    10        0.2697             nan     0.2260   -0.0158
    20        0.2121             nan     0.2260    0.0006
    40        0.1467             nan     0.2260   -0.0086
    60        0.1158             nan     0.2260   -0.0036
    80        0.0991             nan     0.2260   -0.0027
   100        0.0891             nan     0.2260   -0.0073
   120        0.0811             nan     0.2260   -0.0049
   140        0.0658             nan     0.2260   -0.0017
   160        0.0549             nan     0.2260   -0.0039
   180        0.0461             nan     0.2260   -0.0012
   200        0.0388             nan     0.2260   -0.0008
   220        0.0345             nan     0.2260   -0.0011
   240        0.0325             nan     0.2260   -0.0029
   260        0.0280             nan     0.2260   -0.0021
   280        0.0240             nan     0.2260   -0.0014
   300        0.0217             nan     0.2260   -0.0016
   320        0.0197             nan     0.2260   -0.0011
   340        0.0167             nan     0.2260   -0.0006
   360        0.0153             nan     0.2260   -0.0006
   380        0.0142             nan     0.2260   -0.0003
   400        0.0124             nan     0.2260   -0.0009
   420        0.0106             nan     0.2260   -0.0008
   440        0.0095             nan     0.2260   -0.0003
   460        0.0094             nan     0.2260   -0.0003
   480        0.0083             nan     0.2260   -0.0008
   500        0.0072             nan     0.2260   -0.0002
   520        0.0071             nan     0.2260   -0.0004
   540        0.0064             nan     0.2260   -0.0007
   560        0.0060             nan     0.2260   -0.0002
   580        0.0054             nan     0.2260   -0.0001
   600        0.0046             nan     0.2260   -0.0003
   620        0.0040             nan     0.2260   -0.0001
   640        0.0038             nan     0.2260   -0.0001
   660        0.0034             nan     0.2260   -0.0001
   680        0.0031             nan     0.2260   -0.0003
   700        0.0028             nan     0.2260   -0.0000
   720        0.0025             nan     0.2260   -0.0001
   740        0.0024             nan     0.2260   -0.0002
   760        0.0020             nan     0.2260   -0.0001
   780        0.0019             nan     0.2260   -0.0001
   800        0.0017             nan     0.2260   -0.0001
   820        0.0016             nan     0.2260   -0.0001
   840        0.0015             nan     0.2260   -0.0001
   860        0.0014             nan     0.2260   -0.0001
   880        0.0013             nan     0.2260   -0.0001
   900        0.0012             nan     0.2260   -0.0001
   920        0.0011             nan     0.2260   -0.0001
   940        0.0010             nan     0.2260   -0.0000
   960        0.0010             nan     0.2260   -0.0000
   980        0.0009             nan     0.2260   -0.0000
  1000        0.0008             nan     0.2260   -0.0000
  1020        0.0008             nan     0.2260   -0.0001
  1040        0.0007             nan     0.2260   -0.0000
  1060        0.0006             nan     0.2260   -0.0000
  1080        0.0006             nan     0.2260   -0.0000
  1100        0.0005             nan     0.2260   -0.0000
  1120        0.0005             nan     0.2260   -0.0000
  1140        0.0005             nan     0.2260   -0.0000
  1160        0.0005             nan     0.2260   -0.0000
  1180        0.0004             nan     0.2260   -0.0000
  1200        0.0004             nan     0.2260   -0.0000
  1220        0.0004             nan     0.2260   -0.0000
  1240        0.0003             nan     0.2260   -0.0000
  1260        0.0003             nan     0.2260   -0.0000
  1280        0.0003             nan     0.2260   -0.0000
  1300        0.0003             nan     0.2260   -0.0000
  1320        0.0003             nan     0.2260   -0.0000
  1340        0.0002             nan     0.2260   -0.0000
  1360        0.0002             nan     0.2260   -0.0000
  1380        0.0002             nan     0.2260   -0.0000
  1400        0.0002             nan     0.2260   -0.0000
  1420        0.0002             nan     0.2260   -0.0000
  1440        0.0002             nan     0.2260   -0.0000
  1460        0.0002             nan     0.2260   -0.0000
  1480        0.0001             nan     0.2260   -0.0000
  1500        0.0001             nan     0.2260   -0.0000
  1520        0.0001             nan     0.2260   -0.0000
  1540        0.0001             nan     0.2260   -0.0000
  1560        0.0001             nan     0.2260   -0.0000
  1580        0.0001             nan     0.2260   -0.0000
  1600        0.0001             nan     0.2260   -0.0000
  1620        0.0001             nan     0.2260   -0.0000
  1640        0.0001             nan     0.2260   -0.0000
  1660        0.0001             nan     0.2260   -0.0000
  1680        0.0001             nan     0.2260   -0.0000
  1700        0.0001             nan     0.2260   -0.0000
  1720        0.0001             nan     0.2260   -0.0000
  1740        0.0001             nan     0.2260   -0.0000
  1760        0.0001             nan     0.2260   -0.0000
  1780        0.0001             nan     0.2260   -0.0000
  1800        0.0000             nan     0.2260   -0.0000
  1820        0.0000             nan     0.2260   -0.0000
  1840        0.0000             nan     0.2260   -0.0000
  1860        0.0000             nan     0.2260   -0.0000
  1880        0.0000             nan     0.2260   -0.0000
  1900        0.0000             nan     0.2260   -0.0000
  1920        0.0000             nan     0.2260   -0.0000
  1940        0.0000             nan     0.2260   -0.0000
  1960        0.0000             nan     0.2260   -0.0000
  1980        0.0000             nan     0.2260   -0.0000
  2000        0.0000             nan     0.2260   -0.0000
  2020        0.0000             nan     0.2260   -0.0000
  2040        0.0000             nan     0.2260   -0.0000
  2060        0.0000             nan     0.2260   -0.0000
  2080        0.0000             nan     0.2260   -0.0000
  2100        0.0000             nan     0.2260   -0.0000
  2120        0.0000             nan     0.2260   -0.0000
  2140        0.0000             nan     0.2260   -0.0000
  2160        0.0000             nan     0.2260   -0.0000
  2180        0.0000             nan     0.2260   -0.0000
  2200        0.0000             nan     0.2260   -0.0000
  2220        0.0000             nan     0.2260   -0.0000
  2240        0.0000             nan     0.2260   -0.0000
  2260        0.0000             nan     0.2260   -0.0000
  2280        0.0000             nan     0.2260   -0.0000
  2300        0.0000             nan     0.2260   -0.0000
  2320        0.0000             nan     0.2260   -0.0000
  2340        0.0000             nan     0.2260   -0.0000
  2360        0.0000             nan     0.2260   -0.0000
  2380        0.0000             nan     0.2260   -0.0000
  2400        0.0000             nan     0.2260   -0.0000
  2420        0.0000             nan     0.2260   -0.0000
  2440        0.0000             nan     0.2260   -0.0000
  2460        0.0000             nan     0.2260   -0.0000
  2480        0.0000             nan     0.2260   -0.0000
  2500        0.0000             nan     0.2260   -0.0000
  2520        0.0000             nan     0.2260   -0.0000
  2540        0.0000             nan     0.2260   -0.0000
  2560        0.0000             nan     0.2260   -0.0000
  2580        0.0000             nan     0.2260   -0.0000
  2600        0.0000             nan     0.2260   -0.0000
  2620        0.0000             nan     0.2260   -0.0000
  2640        0.0000             nan     0.2260   -0.0000
  2660        0.0000             nan     0.2260   -0.0000
  2680        0.0000             nan     0.2260   -0.0000
  2700        0.0000             nan     0.2260   -0.0000
  2720        0.0000             nan     0.2260   -0.0000
  2740        0.0000             nan     0.2260   -0.0000
  2760        0.0000             nan     0.2260   -0.0000
  2780        0.0000             nan     0.2260   -0.0000
  2790        0.0000             nan     0.2260   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.4813             nan     0.2340    0.0735
     2        0.4243             nan     0.2340    0.0421
     3        0.3848             nan     0.2340    0.0348
     4        0.3786             nan     0.2340    0.0076
     5        0.3764             nan     0.2340   -0.0083
     6        0.3784             nan     0.2340   -0.0071
     7        0.3595             nan     0.2340    0.0089
     8        0.3519             nan     0.2340   -0.0175
     9        0.3472             nan     0.2340   -0.0078
    10        0.3342             nan     0.2340    0.0076
    20        0.2783             nan     0.2340   -0.0071
    40        0.1962             nan     0.2340   -0.0138
    60        0.1613             nan     0.2340   -0.0169
    80        0.1380             nan     0.2340   -0.0082
   100        0.1159             nan     0.2340   -0.0064
   120        0.1046             nan     0.2340   -0.0045
   140        0.0966             nan     0.2340   -0.0154
   160        0.0816             nan     0.2340   -0.0048
   180        0.0701             nan     0.2340   -0.0040
   200        0.0646             nan     0.2340   -0.0008
   220        0.0564             nan     0.2340   -0.0049
   240        0.0503             nan     0.2340   -0.0014
   260        0.0450             nan     0.2340   -0.0027
   280        0.0416             nan     0.2340   -0.0006
   300        0.0375             nan     0.2340   -0.0017
   320        0.0335             nan     0.2340   -0.0010
   340        0.0289             nan     0.2340   -0.0022
   360        0.0264             nan     0.2340   -0.0023
   380        0.0259             nan     0.2340   -0.0015
   400        0.0232             nan     0.2340   -0.0006
   420        0.0234             nan     0.2340   -0.0007
   440        0.0226             nan     0.2340   -0.0012
   460        0.0209             nan     0.2340   -0.0012
   480        0.0190             nan     0.2340   -0.0007
   500        0.0171             nan     0.2340   -0.0002
   520        0.0159             nan     0.2340   -0.0004
   540        0.0143             nan     0.2340   -0.0009
   560        0.0131             nan     0.2340   -0.0006
   580        0.0127             nan     0.2340   -0.0001
   600        0.0113             nan     0.2340   -0.0008
   620        0.0105             nan     0.2340   -0.0001
   640        0.0106             nan     0.2340   -0.0005
   660        0.0095             nan     0.2340   -0.0001
   680        0.0091             nan     0.2340   -0.0008
   700        0.0083             nan     0.2340   -0.0003
   720        0.0081             nan     0.2340   -0.0004
   740        0.0076             nan     0.2340   -0.0010
   760        0.0069             nan     0.2340   -0.0004
   780        0.0065             nan     0.2340   -0.0001
   800        0.0059             nan     0.2340   -0.0001
   820        0.0056             nan     0.2340   -0.0004
   840        0.0050             nan     0.2340   -0.0002
   860        0.0046             nan     0.2340   -0.0002
   880        0.0044             nan     0.2340   -0.0002
   900        0.0041             nan     0.2340   -0.0003
   920        0.0037             nan     0.2340   -0.0001
   940        0.0035             nan     0.2340   -0.0002
   960        0.0033             nan     0.2340   -0.0002
   980        0.0031             nan     0.2340   -0.0002
  1000        0.0028             nan     0.2340   -0.0002
  1020        0.0027             nan     0.2340   -0.0001
  1040        0.0024             nan     0.2340   -0.0001
  1060        0.0024             nan     0.2340   -0.0002
  1080        0.0023             nan     0.2340   -0.0002
  1100        0.0021             nan     0.2340   -0.0001
  1120        0.0021             nan     0.2340   -0.0001
  1140        0.0019             nan     0.2340   -0.0001
  1160        0.0018             nan     0.2340   -0.0001
  1180        0.0017             nan     0.2340   -0.0001
  1200        0.0016             nan     0.2340   -0.0001
  1220        0.0014             nan     0.2340   -0.0000
  1240        0.0013             nan     0.2340   -0.0001
  1260        0.0012             nan     0.2340   -0.0000
  1280        0.0012             nan     0.2340   -0.0001
  1300        0.0011             nan     0.2340   -0.0000
  1320        0.0011             nan     0.2340   -0.0001
  1340        0.0010             nan     0.2340   -0.0001
  1360        0.0010             nan     0.2340   -0.0001
  1380        0.0009             nan     0.2340   -0.0001
  1400        0.0008             nan     0.2340   -0.0000
  1420        0.0008             nan     0.2340   -0.0000
  1440        0.0008             nan     0.2340   -0.0000
  1460        0.0007             nan     0.2340   -0.0000
  1480        0.0007             nan     0.2340   -0.0000
  1500        0.0006             nan     0.2340   -0.0000
  1520        0.0006             nan     0.2340   -0.0001
  1540        0.0006             nan     0.2340   -0.0000
  1560        0.0005             nan     0.2340   -0.0000
  1580        0.0005             nan     0.2340   -0.0000
  1600        0.0005             nan     0.2340   -0.0000
  1620        0.0004             nan     0.2340   -0.0000
  1640        0.0004             nan     0.2340   -0.0000
  1660        0.0004             nan     0.2340   -0.0000
  1680        0.0004             nan     0.2340   -0.0000
  1700        0.0004             nan     0.2340   -0.0000
  1720        0.0003             nan     0.2340   -0.0000
  1740        0.0003             nan     0.2340   -0.0000
  1760        0.0003             nan     0.2340   -0.0000
  1780        0.0003             nan     0.2340   -0.0000
  1800        0.0003             nan     0.2340   -0.0000
  1820        0.0003             nan     0.2340   -0.0000
  1840        0.0003             nan     0.2340   -0.0000
  1860        0.0002             nan     0.2340   -0.0000
  1880        0.0002             nan     0.2340   -0.0000
  1900        0.0002             nan     0.2340   -0.0000
  1920        0.0002             nan     0.2340   -0.0000
  1940        0.0002             nan     0.2340   -0.0000
  1960        0.0002             nan     0.2340   -0.0000
  1980        0.0002             nan     0.2340   -0.0000
  2000        0.0002             nan     0.2340   -0.0000
  2020        0.0002             nan     0.2340   -0.0000
  2040        0.0001             nan     0.2340   -0.0000
  2060        0.0001             nan     0.2340    0.0000
  2080        0.0001             nan     0.2340   -0.0000
  2100        0.0001             nan     0.2340   -0.0000
  2120        0.0001             nan     0.2340   -0.0000
  2140        0.0001             nan     0.2340   -0.0000
  2160        0.0001             nan     0.2340   -0.0000
  2180        0.0001             nan     0.2340   -0.0000
  2200        0.0001             nan     0.2340    0.0000
  2220        0.0001             nan     0.2340   -0.0000
  2240        0.0001             nan     0.2340   -0.0000
  2260        0.0001             nan     0.2340   -0.0000
  2280        0.0001             nan     0.2340   -0.0000
  2300        0.0001             nan     0.2340   -0.0000
  2320        0.0001             nan     0.2340   -0.0000
  2340        0.0001             nan     0.2340   -0.0000
  2360        0.0001             nan     0.2340   -0.0000
  2380        0.0001             nan     0.2340   -0.0000
  2400        0.0001             nan     0.2340   -0.0000
  2420        0.0001             nan     0.2340   -0.0000
  2440        0.0001             nan     0.2340   -0.0000
  2460        0.0001             nan     0.2340   -0.0000
  2480        0.0001             nan     0.2340   -0.0000
  2500        0.0001             nan     0.2340   -0.0000
  2520        0.0001             nan     0.2340   -0.0000
  2540        0.0000             nan     0.2340   -0.0000
  2560        0.0000             nan     0.2340   -0.0000
  2580        0.0000             nan     0.2340   -0.0000
  2600        0.0000             nan     0.2340   -0.0000
  2620        0.0000             nan     0.2340   -0.0000
  2640        0.0000             nan     0.2340   -0.0000
  2660        0.0000             nan     0.2340   -0.0000
  2680        0.0000             nan     0.2340   -0.0000
  2700        0.0000             nan     0.2340   -0.0000
  2720        0.0000             nan     0.2340   -0.0000
  2740        0.0000             nan     0.2340   -0.0000
  2760        0.0000             nan     0.2340   -0.0000
  2780        0.0000             nan     0.2340   -0.0000
  2800        0.0000             nan     0.2340   -0.0000
  2820        0.0000             nan     0.2340   -0.0000
  2840        0.0000             nan     0.2340   -0.0000
  2860        0.0000             nan     0.2340   -0.0000
  2880        0.0000             nan     0.2340   -0.0000
  2900        0.0000             nan     0.2340   -0.0000
  2920        0.0000             nan     0.2340   -0.0000
  2940        0.0000             nan     0.2340   -0.0000
  2960        0.0000             nan     0.2340   -0.0000
  2980        0.0000             nan     0.2340   -0.0000
  3000        0.0000             nan     0.2340   -0.0000
  3020        0.0000             nan     0.2340   -0.0000
  3040        0.0000             nan     0.2340   -0.0000
  3060        0.0000             nan     0.2340   -0.0000
  3080        0.0000             nan     0.2340   -0.0000
  3100        0.0000             nan     0.2340   -0.0000
  3120        0.0000             nan     0.2340   -0.0000
  3140        0.0000             nan     0.2340   -0.0000
  3160        0.0000             nan     0.2340   -0.0000
  3180        0.0000             nan     0.2340   -0.0000
  3200        0.0000             nan     0.2340   -0.0000
  3220        0.0000             nan     0.2340   -0.0000
  3240        0.0000             nan     0.2340   -0.0000
  3260        0.0000             nan     0.2340   -0.0000
  3280        0.0000             nan     0.2340   -0.0000
  3300        0.0000             nan     0.2340   -0.0000
  3320        0.0000             nan     0.2340   -0.0000
  3340        0.0000             nan     0.2340   -0.0000
  3360        0.0000             nan     0.2340   -0.0000
  3380        0.0000             nan     0.2340   -0.0000
  3400        0.0000             nan     0.2340   -0.0000
  3420        0.0000             nan     0.2340   -0.0000
  3440        0.0000             nan     0.2340   -0.0000
  3460        0.0000             nan     0.2340   -0.0000
  3480        0.0000             nan     0.2340   -0.0000
  3500        0.0000             nan     0.2340   -0.0000
  3520        0.0000             nan     0.2340   -0.0000
  3540        0.0000             nan     0.2340   -0.0000
  3560        0.0000             nan     0.2340   -0.0000
  3580        0.0000             nan     0.2340   -0.0000
  3600        0.0000             nan     0.2340   -0.0000
  3620        0.0000             nan     0.2340   -0.0000
  3640        0.0000             nan     0.2340   -0.0000
  3660        0.0000             nan     0.2340   -0.0000
  3680        0.0000             nan     0.2340   -0.0000
  3700        0.0000             nan     0.2340   -0.0000
  3720        0.0000             nan     0.2340   -0.0000
  3740        0.0000             nan     0.2340   -0.0000
  3760        0.0000             nan     0.2340   -0.0000
  3780        0.0000             nan     0.2340   -0.0000
  3800        0.0000             nan     0.2340   -0.0000
  3820        0.0000             nan     0.2340   -0.0000
  3840        0.0000             nan     0.2340   -0.0000
  3860        0.0000             nan     0.2340   -0.0000
  3880        0.0000             nan     0.2340   -0.0000
  3900        0.0000             nan     0.2340   -0.0000
  3920        0.0000             nan     0.2340   -0.0000
  3924        0.0000             nan     0.2340   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.4171             nan     0.2895    0.0792
     2        0.3351             nan     0.2895    0.0613
     3        0.2719             nan     0.2895    0.0293
     4        0.2322             nan     0.2895    0.0275
     5        0.2183             nan     0.2895   -0.0009
     6        0.1901             nan     0.2895    0.0183
     7        0.1661             nan     0.2895    0.0139
     8        0.1551             nan     0.2895    0.0024
     9        0.1499             nan     0.2895   -0.0030
    10        0.1404             nan     0.2895   -0.0023
    20        0.1048             nan     0.2895   -0.0059
    40        0.0848             nan     0.2895   -0.0075
    60        0.0650             nan     0.2895   -0.0074
    80        0.0531             nan     0.2895   -0.0048
   100        0.0469             nan     0.2895   -0.0049
   120        0.0327             nan     0.2895   -0.0022
   140        0.0270             nan     0.2895   -0.0009
   160        0.0223             nan     0.2895   -0.0008
   180        0.0173             nan     0.2895   -0.0001
   200        0.0131             nan     0.2895   -0.0008
   220        0.0103             nan     0.2895   -0.0010
   240        0.0079             nan     0.2895   -0.0004
   260        0.0058             nan     0.2895   -0.0004
   280        0.0048             nan     0.2895   -0.0005
   300        0.0038             nan     0.2895   -0.0006
   320        0.0032             nan     0.2895   -0.0002
   340        0.0024             nan     0.2895   -0.0002
   360        0.0019             nan     0.2895   -0.0002
   380        0.0012             nan     0.2895   -0.0001
   400        0.0010             nan     0.2895   -0.0001
   420        0.0008             nan     0.2895   -0.0001
   440        0.0006             nan     0.2895   -0.0000
   460        0.0005             nan     0.2895   -0.0001
   480        0.0004             nan     0.2895   -0.0000
   500        0.0003             nan     0.2895   -0.0000
   520        0.0003             nan     0.2895   -0.0000
   540        0.0003             nan     0.2895   -0.0000
   560        0.0002             nan     0.2895   -0.0000
   580        0.0002             nan     0.2895   -0.0000
   600        0.0001             nan     0.2895   -0.0000
   620        0.0001             nan     0.2895   -0.0000
   640        0.0001             nan     0.2895   -0.0000
   660        0.0001             nan     0.2895   -0.0000
   680        0.0001             nan     0.2895   -0.0000
   700        0.0001             nan     0.2895   -0.0000
   720        0.0000             nan     0.2895   -0.0000
   740        0.0000             nan     0.2895    0.0000
   760        0.0000             nan     0.2895   -0.0000
   780        0.0000             nan     0.2895   -0.0000
   800        0.0000             nan     0.2895    0.0000
   820        0.0000             nan     0.2895   -0.0000
   840        0.0000             nan     0.2895   -0.0000
   860        0.0000             nan     0.2895   -0.0000
   880        0.0000             nan     0.2895   -0.0000
   900        0.0000             nan     0.2895   -0.0000
   920        0.0000             nan     0.2895   -0.0000
   940        0.0000             nan     0.2895   -0.0000
   960        0.0000             nan     0.2895   -0.0000
   980        0.0000             nan     0.2895   -0.0000
  1000        0.0000             nan     0.2895   -0.0000
  1020        0.0000             nan     0.2895   -0.0000
  1040        0.0000             nan     0.2895   -0.0000
  1060        0.0000             nan     0.2895   -0.0000
  1080        0.0000             nan     0.2895   -0.0000
  1100        0.0000             nan     0.2895   -0.0000
  1120        0.0000             nan     0.2895   -0.0000
  1140        0.0000             nan     0.2895   -0.0000
  1160        0.0000             nan     0.2895   -0.0000
  1180        0.0000             nan     0.2895   -0.0000
  1200        0.0000             nan     0.2895   -0.0000
  1220        0.0000             nan     0.2895   -0.0000
  1240        0.0000             nan     0.2895   -0.0000
  1260        0.0000             nan     0.2895   -0.0000
  1280        0.0000             nan     0.2895   -0.0000
  1300        0.0000             nan     0.2895   -0.0000
  1320        0.0000             nan     0.2895   -0.0000
  1340        0.0000             nan     0.2895   -0.0000
  1360        0.0000             nan     0.2895   -0.0000
  1380        0.0000             nan     0.2895   -0.0000
  1400        0.0000             nan     0.2895   -0.0000
  1420        0.0000             nan     0.2895   -0.0000
  1440        0.0000             nan     0.2895   -0.0000
  1460        0.0000             nan     0.2895   -0.0000
  1480        0.0000             nan     0.2895   -0.0000
  1500        0.0000             nan     0.2895   -0.0000
  1520        0.0000             nan     0.2895   -0.0000
  1540        0.0000             nan     0.2895   -0.0000
  1560        0.0000             nan     0.2895   -0.0000
  1580        0.0000             nan     0.2895   -0.0000
  1600        0.0000             nan     0.2895   -0.0000
  1620        0.0000             nan     0.2895   -0.0000
  1640        0.0000             nan     0.2895   -0.0000
  1660        0.0000             nan     0.2895   -0.0000
  1680        0.0000             nan     0.2895   -0.0000
  1700        0.0000             nan     0.2895   -0.0000
  1720        0.0000             nan     0.2895   -0.0000
  1740        0.0000             nan     0.2895   -0.0000
  1760        0.0000             nan     0.2895   -0.0000
  1780        0.0000             nan     0.2895   -0.0000
  1800        0.0000             nan     0.2895   -0.0000
  1820        0.0000             nan     0.2895   -0.0000
  1840        0.0000             nan     0.2895   -0.0000
  1860        0.0000             nan     0.2895   -0.0000
  1880        0.0000             nan     0.2895   -0.0000
  1900        0.0000             nan     0.2895   -0.0000
  1920        0.0000             nan     0.2895   -0.0000
  1940        0.0000             nan     0.2895   -0.0000
  1960        0.0000             nan     0.2895   -0.0000
  1980        0.0000             nan     0.2895   -0.0000
  2000        0.0000             nan     0.2895    0.0000
  2020        0.0000             nan     0.2895   -0.0000
  2040        0.0000             nan     0.2895   -0.0000
  2060        0.0000             nan     0.2895   -0.0000
  2080        0.0000             nan     0.2895   -0.0000
  2100        0.0000             nan     0.2895   -0.0000
  2120        0.0000             nan     0.2895   -0.0000
  2140        0.0000             nan     0.2895   -0.0000
  2160        0.0000             nan     0.2895   -0.0000
  2180        0.0000             nan     0.2895   -0.0000
  2200        0.0000             nan     0.2895   -0.0000
  2220        0.0000             nan     0.2895   -0.0000
  2240        0.0000             nan     0.2895   -0.0000
  2260        0.0000             nan     0.2895   -0.0000
  2280        0.0000             nan     0.2895   -0.0000
  2300        0.0000             nan     0.2895   -0.0000
  2320        0.0000             nan     0.2895   -0.0000
  2340        0.0000             nan     0.2895   -0.0000
  2360        0.0000             nan     0.2895   -0.0000
  2380        0.0000             nan     0.2895   -0.0000
  2400        0.0000             nan     0.2895   -0.0000
  2420        0.0000             nan     0.2895   -0.0000
  2440        0.0000             nan     0.2895   -0.0000
  2460        0.0000             nan     0.2895   -0.0000
  2480        0.0000             nan     0.2895   -0.0000
  2500        0.0000             nan     0.2895   -0.0000
  2520        0.0000             nan     0.2895   -0.0000
  2540        0.0000             nan     0.2895   -0.0000
  2560        0.0000             nan     0.2895   -0.0000
  2580        0.0000             nan     0.2895   -0.0000
  2600        0.0000             nan     0.2895   -0.0000
  2620        0.0000             nan     0.2895   -0.0000
  2640        0.0000             nan     0.2895   -0.0000
  2660        0.0000             nan     0.2895   -0.0000
  2680        0.0000             nan     0.2895   -0.0000
  2700        0.0000             nan     0.2895   -0.0000
  2720        0.0000             nan     0.2895   -0.0000
  2740        0.0000             nan     0.2895   -0.0000
  2760        0.0000             nan     0.2895   -0.0000
  2780        0.0000             nan     0.2895   -0.0000
  2800        0.0000             nan     0.2895   -0.0000
  2820        0.0000             nan     0.2895   -0.0000
  2840        0.0000             nan     0.2895   -0.0000
  2860        0.0000             nan     0.2895   -0.0000
  2880        0.0000             nan     0.2895   -0.0000
  2900        0.0000             nan     0.2895   -0.0000
  2920        0.0000             nan     0.2895   -0.0000
  2940        0.0000             nan     0.2895   -0.0000
  2960        0.0000             nan     0.2895   -0.0000
  2980        0.0000             nan     0.2895   -0.0000
  3000        0.0000             nan     0.2895   -0.0000
  3020        0.0000             nan     0.2895   -0.0000
  3040        0.0000             nan     0.2895   -0.0000
  3060        0.0000             nan     0.2895   -0.0000
  3080        0.0000             nan     0.2895   -0.0000
  3100        0.0000             nan     0.2895   -0.0000
  3120        0.0000             nan     0.2895   -0.0000
  3140        0.0000             nan     0.2895   -0.0000
  3160        0.0000             nan     0.2895   -0.0000
  3180        0.0000             nan     0.2895   -0.0000
  3200        0.0000             nan     0.2895   -0.0000
  3220        0.0000             nan     0.2895   -0.0000
  3240        0.0000             nan     0.2895   -0.0000
  3260        0.0000             nan     0.2895   -0.0000
  3280        0.0000             nan     0.2895   -0.0000
  3300        0.0000             nan     0.2895   -0.0000
  3320        0.0000             nan     0.2895   -0.0000
  3340        0.0000             nan     0.2895   -0.0000
  3360        0.0000             nan     0.2895   -0.0000
  3380        0.0000             nan     0.2895   -0.0000
  3400        0.0000             nan     0.2895   -0.0000
  3420        0.0000             nan     0.2895   -0.0000
  3440        0.0000             nan     0.2895   -0.0000
  3460        0.0000             nan     0.2895   -0.0000
  3480        0.0000             nan     0.2895   -0.0000
  3500        0.0000             nan     0.2895   -0.0000
  3520        0.0000             nan     0.2895   -0.0000
  3540        0.0000             nan     0.2895   -0.0000
  3560        0.0000             nan     0.2895   -0.0000
  3580        0.0000             nan     0.2895   -0.0000
  3600        0.0000             nan     0.2895   -0.0000
  3620        0.0000             nan     0.2895   -0.0000
  3640        0.0000             nan     0.2895   -0.0000
  3660        0.0000             nan     0.2895   -0.0000
  3680        0.0000             nan     0.2895   -0.0000
  3700        0.0000             nan     0.2895   -0.0000
  3720        0.0000             nan     0.2895    0.0000
  3740        0.0000             nan     0.2895   -0.0000
  3760        0.0000             nan     0.2895   -0.0000
  3780        0.0000             nan     0.2895   -0.0000
  3800        0.0000             nan     0.2895   -0.0000
  3820        0.0000             nan     0.2895   -0.0000
  3840        0.0000             nan     0.2895   -0.0000
  3860        0.0000             nan     0.2895   -0.0000
  3880        0.0000             nan     0.2895   -0.0000
  3900        0.0000             nan     0.2895   -0.0000
  3920        0.0000             nan     0.2895   -0.0000
  3940        0.0000             nan     0.2895   -0.0000
  3960        0.0000             nan     0.2895   -0.0000
  3980        0.0000             nan     0.2895   -0.0000
  4000        0.0000             nan     0.2895   -0.0000
  4020        0.0000             nan     0.2895   -0.0000
  4040        0.0000             nan     0.2895   -0.0000
  4060        0.0000             nan     0.2895   -0.0000
  4080        0.0000             nan     0.2895   -0.0000
  4100        0.0000             nan     0.2895   -0.0000
  4120        0.0000             nan     0.2895   -0.0000
  4140        0.0000             nan     0.2895   -0.0000
  4160        0.0000             nan     0.2895    0.0000
  4180        0.0000             nan     0.2895   -0.0000
  4200        0.0000             nan     0.2895   -0.0000
  4220        0.0000             nan     0.2895   -0.0000
  4240        0.0000             nan     0.2895   -0.0000
  4260        0.0000             nan     0.2895   -0.0000
  4280        0.0000             nan     0.2895   -0.0000
  4300        0.0000             nan     0.2895   -0.0000
  4320        0.0000             nan     0.2895   -0.0000
  4340        0.0000             nan     0.2895   -0.0000
  4360        0.0000             nan     0.2895   -0.0000
  4373        0.0000             nan     0.2895   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3550             nan     0.3621    0.1489
     2        0.2288             nan     0.3621   -0.0033
     3        0.2080             nan     0.3621    0.0247
     4        0.1998             nan     0.3621    0.0075
     5        0.1760             nan     0.3621    0.0074
     6        0.1594             nan     0.3621    0.0041
     7        0.1406             nan     0.3621   -0.0051
     8        0.1225             nan     0.3621   -0.0009
     9        0.1208             nan     0.3621   -0.0068
    10        0.1220             nan     0.3621   -0.0179
    20        0.0697             nan     0.3621   -0.0179
    40        0.0281             nan     0.3621   -0.0014
    60        0.0111             nan     0.3621   -0.0017
    80        0.0040             nan     0.3621   -0.0004
   100        0.0018             nan     0.3621   -0.0003
   120        0.0010             nan     0.3621   -0.0001
   140        0.0004             nan     0.3621   -0.0000
   160        0.0002             nan     0.3621   -0.0001
   180        0.0001             nan     0.3621   -0.0000
   200        0.0000             nan     0.3621   -0.0000
   220        0.0000             nan     0.3621   -0.0000
   240        0.0000             nan     0.3621   -0.0000
   260        0.0000             nan     0.3621   -0.0000
   280        0.0000             nan     0.3621   -0.0000
   300        0.0000             nan     0.3621   -0.0000
   320        0.0000             nan     0.3621   -0.0000
   340        0.0000             nan     0.3621   -0.0000
   360        0.0000             nan     0.3621   -0.0000
   380        0.0000             nan     0.3621   -0.0000
   400        0.0000             nan     0.3621   -0.0000
   420        0.0000             nan     0.3621   -0.0000
   440        0.0000             nan     0.3621   -0.0000
   460        0.0000             nan     0.3621   -0.0000
   480        0.0000             nan     0.3621   -0.0000
   500        0.0000             nan     0.3621   -0.0000
   520        0.0000             nan     0.3621   -0.0000
   540        0.0000             nan     0.3621   -0.0000
   560        0.0000             nan     0.3621   -0.0000
   580        0.0000             nan     0.3621   -0.0000
   600        0.0000             nan     0.3621   -0.0000
   620        0.0000             nan     0.3621   -0.0000
   640        0.0000             nan     0.3621   -0.0000
   660        0.0000             nan     0.3621   -0.0000
   680        0.0000             nan     0.3621   -0.0000
   700        0.0000             nan     0.3621   -0.0000
   720        0.0000             nan     0.3621   -0.0000
   740        0.0000             nan     0.3621    0.0000
   760        0.0000             nan     0.3621   -0.0000
   780        0.0000             nan     0.3621   -0.0000
   800        0.0000             nan     0.3621   -0.0000
   820        0.0000             nan     0.3621   -0.0000
   840        0.0000             nan     0.3621   -0.0000
   860        0.0000             nan     0.3621   -0.0000
   880        0.0000             nan     0.3621   -0.0000
   900        0.0000             nan     0.3621   -0.0000
   920        0.0000             nan     0.3621   -0.0000
   940        0.0000             nan     0.3621    0.0000
   960        0.0000             nan     0.3621   -0.0000
   980        0.0000             nan     0.3621   -0.0000
  1000        0.0000             nan     0.3621   -0.0000
  1020        0.0000             nan     0.3621   -0.0000
  1040        0.0000             nan     0.3621   -0.0000
  1060        0.0000             nan     0.3621   -0.0000
  1080        0.0000             nan     0.3621   -0.0000
  1100        0.0000             nan     0.3621   -0.0000
  1120        0.0000             nan     0.3621   -0.0000
  1140        0.0000             nan     0.3621   -0.0000
  1160        0.0000             nan     0.3621   -0.0000
  1180        0.0000             nan     0.3621   -0.0000
  1200        0.0000             nan     0.3621   -0.0000
  1220        0.0000             nan     0.3621   -0.0000
  1240        0.0000             nan     0.3621   -0.0000
  1260        0.0000             nan     0.3621   -0.0000
  1280        0.0000             nan     0.3621   -0.0000
  1300        0.0000             nan     0.3621   -0.0000
  1320        0.0000             nan     0.3621   -0.0000
  1340        0.0000             nan     0.3621   -0.0000
  1360        0.0000             nan     0.3621   -0.0000
  1380        0.0000             nan     0.3621   -0.0000
  1400        0.0000             nan     0.3621   -0.0000
  1420        0.0000             nan     0.3621   -0.0000
  1440        0.0000             nan     0.3621   -0.0000
  1460        0.0000             nan     0.3621   -0.0000
  1480        0.0000             nan     0.3621   -0.0000
  1500        0.0000             nan     0.3621   -0.0000
  1520        0.0000             nan     0.3621   -0.0000
  1540        0.0000             nan     0.3621   -0.0000
  1560        0.0000             nan     0.3621   -0.0000
  1580        0.0000             nan     0.3621   -0.0000
  1600        0.0000             nan     0.3621   -0.0000
  1620        0.0000             nan     0.3621   -0.0000
  1640        0.0000             nan     0.3621   -0.0000
  1660        0.0000             nan     0.3621   -0.0000
  1680        0.0000             nan     0.3621   -0.0000
  1700        0.0000             nan     0.3621   -0.0000
  1720        0.0000             nan     0.3621   -0.0000
  1740        0.0000             nan     0.3621   -0.0000
  1760        0.0000             nan     0.3621   -0.0000
  1780        0.0000             nan     0.3621   -0.0000
  1800        0.0000             nan     0.3621   -0.0000
  1820        0.0000             nan     0.3621   -0.0000
  1840        0.0000             nan     0.3621    0.0000
  1860        0.0000             nan     0.3621   -0.0000
  1880        0.0000             nan     0.3621   -0.0000
  1900        0.0000             nan     0.3621   -0.0000
  1920        0.0000             nan     0.3621   -0.0000
  1940        0.0000             nan     0.3621   -0.0000
  1960        0.0000             nan     0.3621   -0.0000
  1980        0.0000             nan     0.3621   -0.0000
  2000        0.0000             nan     0.3621   -0.0000
  2020        0.0000             nan     0.3621   -0.0000
  2040        0.0000             nan     0.3621   -0.0000
  2060        0.0000             nan     0.3621   -0.0000
  2080        0.0000             nan     0.3621   -0.0000
  2100        0.0000             nan     0.3621   -0.0000
  2120        0.0000             nan     0.3621   -0.0000
  2140        0.0000             nan     0.3621   -0.0000
  2160        0.0000             nan     0.3621   -0.0000
  2180        0.0000             nan     0.3621   -0.0000
  2200        0.0000             nan     0.3621   -0.0000
  2220        0.0000             nan     0.3621   -0.0000
  2240        0.0000             nan     0.3621   -0.0000
  2260        0.0000             nan     0.3621   -0.0000
  2280        0.0000             nan     0.3621   -0.0000
  2300        0.0000             nan     0.3621   -0.0000
  2320        0.0000             nan     0.3621    0.0000
  2340        0.0000             nan     0.3621   -0.0000
  2360        0.0000             nan     0.3621   -0.0000
  2380        0.0000             nan     0.3621    0.0000
  2400        0.0000             nan     0.3621   -0.0000
  2420        0.0000             nan     0.3621   -0.0000
  2440        0.0000             nan     0.3621   -0.0000
  2460        0.0000             nan     0.3621   -0.0000
  2480        0.0000             nan     0.3621   -0.0000
  2500        0.0000             nan     0.3621   -0.0000
  2520        0.0000             nan     0.3621   -0.0000
  2540        0.0000             nan     0.3621    0.0000
  2560        0.0000             nan     0.3621   -0.0000
  2580        0.0000             nan     0.3621   -0.0000
  2600        0.0000             nan     0.3621   -0.0000
  2620        0.0000             nan     0.3621   -0.0000
  2640        0.0000             nan     0.3621   -0.0000
  2660        0.0000             nan     0.3621   -0.0000
  2680        0.0000             nan     0.3621   -0.0000
  2700        0.0000             nan     0.3621   -0.0000
  2720        0.0000             nan     0.3621   -0.0000
  2740        0.0000             nan     0.3621   -0.0000
  2760        0.0000             nan     0.3621   -0.0000
  2780        0.0000             nan     0.3621   -0.0000
  2800        0.0000             nan     0.3621   -0.0000
  2820        0.0000             nan     0.3621   -0.0000
  2840        0.0000             nan     0.3621   -0.0000
  2860        0.0000             nan     0.3621   -0.0000
  2880        0.0000             nan     0.3621   -0.0000
  2900        0.0000             nan     0.3621   -0.0000
  2920        0.0000             nan     0.3621   -0.0000
  2940        0.0000             nan     0.3621   -0.0000
  2960        0.0000             nan     0.3621   -0.0000
  2980        0.0000             nan     0.3621   -0.0000
  3000        0.0000             nan     0.3621   -0.0000
  3020        0.0000             nan     0.3621   -0.0000
  3040        0.0000             nan     0.3621   -0.0000
  3060        0.0000             nan     0.3621   -0.0000
  3080        0.0000             nan     0.3621   -0.0000
  3100        0.0000             nan     0.3621   -0.0000
  3120        0.0000             nan     0.3621    0.0000
  3140        0.0000             nan     0.3621   -0.0000
  3160        0.0000             nan     0.3621   -0.0000
  3180        0.0000             nan     0.3621   -0.0000
  3200        0.0000             nan     0.3621   -0.0000
  3220        0.0000             nan     0.3621   -0.0000
  3240        0.0000             nan     0.3621   -0.0000
  3260        0.0000             nan     0.3621   -0.0000
  3280        0.0000             nan     0.3621   -0.0000
  3300        0.0000             nan     0.3621   -0.0000
  3320        0.0000             nan     0.3621    0.0000
  3340        0.0000             nan     0.3621   -0.0000
  3360        0.0000             nan     0.3621   -0.0000
  3380        0.0000             nan     0.3621   -0.0000
  3400        0.0000             nan     0.3621   -0.0000
  3420        0.0000             nan     0.3621   -0.0000
  3440        0.0000             nan     0.3621   -0.0000
  3460        0.0000             nan     0.3621   -0.0000
  3480        0.0000             nan     0.3621   -0.0000
  3500        0.0000             nan     0.3621   -0.0000
  3520        0.0000             nan     0.3621   -0.0000
  3540        0.0000             nan     0.3621   -0.0000
  3560        0.0000             nan     0.3621   -0.0000
  3580        0.0000             nan     0.3621   -0.0000
  3600        0.0000             nan     0.3621   -0.0000
  3620        0.0000             nan     0.3621   -0.0000
  3640        0.0000             nan     0.3621   -0.0000
  3660        0.0000             nan     0.3621   -0.0000
  3680        0.0000             nan     0.3621   -0.0000
  3700        0.0000             nan     0.3621   -0.0000
  3720        0.0000             nan     0.3621   -0.0000
  3740        0.0000             nan     0.3621   -0.0000
  3760        0.0000             nan     0.3621   -0.0000
  3780        0.0000             nan     0.3621   -0.0000
  3799        0.0000             nan     0.3621   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3954             nan     0.4467    0.2168
     2        0.2316             nan     0.4467    0.0999
     3        0.2330             nan     0.4467   -0.0322
     4        0.2016             nan     0.4467    0.0107
     5        0.1874             nan     0.4467   -0.0428
     6        0.1434             nan     0.4467   -0.0123
     7        0.1476             nan     0.4467   -0.0347
     8        0.1263             nan     0.4467   -0.0180
     9        0.1254             nan     0.4467   -0.0303
    10        0.1217             nan     0.4467   -0.0564
    20        0.0575             nan     0.4467   -0.0211
    40        0.0150             nan     0.4467   -0.0033
    60        0.0038             nan     0.4467   -0.0007
    80        0.0008             nan     0.4467   -0.0002
   100        0.0002             nan     0.4467   -0.0000
   120        0.0001             nan     0.4467   -0.0000
   140        0.0000             nan     0.4467   -0.0000
   160        0.0000             nan     0.4467   -0.0000
   180        0.0000             nan     0.4467   -0.0000
   200        0.0000             nan     0.4467   -0.0000
   220        0.0000             nan     0.4467   -0.0000
   240        0.0000             nan     0.4467   -0.0000
   260        0.0000             nan     0.4467   -0.0000
   280        0.0000             nan     0.4467    0.0000
   300        0.0000             nan     0.4467   -0.0000
   320        0.0000             nan     0.4467   -0.0000
   340        0.0000             nan     0.4467   -0.0000
   360        0.0000             nan     0.4467   -0.0000
   380        0.0000             nan     0.4467   -0.0000
   400        0.0000             nan     0.4467   -0.0000
   420        0.0000             nan     0.4467   -0.0000
   440        0.0000             nan     0.4467   -0.0000
   460        0.0000             nan     0.4467   -0.0000
   480        0.0000             nan     0.4467   -0.0000
   500        0.0000             nan     0.4467   -0.0000
   520        0.0000             nan     0.4467   -0.0000
   540        0.0000             nan     0.4467   -0.0000
   560        0.0000             nan     0.4467   -0.0000
   580        0.0000             nan     0.4467   -0.0000
   600        0.0000             nan     0.4467   -0.0000
   620        0.0000             nan     0.4467   -0.0000
   640        0.0000             nan     0.4467   -0.0000
   660        0.0000             nan     0.4467   -0.0000
   680        0.0000             nan     0.4467   -0.0000
   700        0.0000             nan     0.4467   -0.0000
   720        0.0000             nan     0.4467    0.0000
   740        0.0000             nan     0.4467   -0.0000
   760        0.0000             nan     0.4467   -0.0000
   780        0.0000             nan     0.4467   -0.0000
   800        0.0000             nan     0.4467   -0.0000
   820        0.0000             nan     0.4467   -0.0000
   840        0.0000             nan     0.4467   -0.0000
   860        0.0000             nan     0.4467   -0.0000
   880        0.0000             nan     0.4467   -0.0000
   900        0.0000             nan     0.4467   -0.0000
   920        0.0000             nan     0.4467   -0.0000
   940        0.0000             nan     0.4467   -0.0000
   960        0.0000             nan     0.4467   -0.0000
   980        0.0000             nan     0.4467   -0.0000
  1000        0.0000             nan     0.4467   -0.0000
  1020        0.0000             nan     0.4467   -0.0000
  1040        0.0000             nan     0.4467   -0.0000
  1060        0.0000             nan     0.4467   -0.0000
  1080        0.0000             nan     0.4467   -0.0000
  1100        0.0000             nan     0.4467   -0.0000
  1120        0.0000             nan     0.4467   -0.0000
  1140        0.0000             nan     0.4467   -0.0000
  1160        0.0000             nan     0.4467   -0.0000
  1180        0.0000             nan     0.4467   -0.0000
  1200        0.0000             nan     0.4467   -0.0000
  1220        0.0000             nan     0.4467   -0.0000
  1240        0.0000             nan     0.4467   -0.0000
  1260        0.0000             nan     0.4467   -0.0000
  1280        0.0000             nan     0.4467   -0.0000
  1300        0.0000             nan     0.4467   -0.0000
  1320        0.0000             nan     0.4467   -0.0000
  1340        0.0000             nan     0.4467   -0.0000
  1360        0.0000             nan     0.4467    0.0000
  1380        0.0000             nan     0.4467   -0.0000
  1400        0.0000             nan     0.4467   -0.0000
  1420        0.0000             nan     0.4467   -0.0000
  1440        0.0000             nan     0.4467   -0.0000
  1460        0.0000             nan     0.4467   -0.0000
  1480        0.0000             nan     0.4467   -0.0000
  1500        0.0000             nan     0.4467   -0.0000
  1520        0.0000             nan     0.4467   -0.0000
  1540        0.0000             nan     0.4467   -0.0000
  1560        0.0000             nan     0.4467   -0.0000
  1580        0.0000             nan     0.4467   -0.0000
  1600        0.0000             nan     0.4467   -0.0000
  1620        0.0000             nan     0.4467   -0.0000
  1640        0.0000             nan     0.4467   -0.0000
  1660        0.0000             nan     0.4467   -0.0000
  1680        0.0000             nan     0.4467   -0.0000
  1700        0.0000             nan     0.4467   -0.0000
  1720        0.0000             nan     0.4467   -0.0000
  1740        0.0000             nan     0.4467   -0.0000
  1760        0.0000             nan     0.4467   -0.0000
  1780        0.0000             nan     0.4467   -0.0000
  1800        0.0000             nan     0.4467   -0.0000
  1820        0.0000             nan     0.4467   -0.0000
  1840        0.0000             nan     0.4467   -0.0000
  1860        0.0000             nan     0.4467   -0.0000
  1880        0.0000             nan     0.4467   -0.0000
  1900        0.0000             nan     0.4467   -0.0000
  1920        0.0000             nan     0.4467   -0.0000
  1940        0.0000             nan     0.4467   -0.0000
  1960        0.0000             nan     0.4467   -0.0000
  1980        0.0000             nan     0.4467   -0.0000
  2000        0.0000             nan     0.4467   -0.0000
  2020        0.0000             nan     0.4467   -0.0000
  2040        0.0000             nan     0.4467   -0.0000
  2060        0.0000             nan     0.4467   -0.0000
  2080        0.0000             nan     0.4467   -0.0000
  2100        0.0000             nan     0.4467   -0.0000
  2120        0.0000             nan     0.4467   -0.0000
  2140        0.0000             nan     0.4467   -0.0000
  2160        0.0000             nan     0.4467   -0.0000
  2180        0.0000             nan     0.4467    0.0000
  2200        0.0000             nan     0.4467   -0.0000
  2220        0.0000             nan     0.4467   -0.0000
  2232        0.0000             nan     0.4467   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2590             nan     0.5710    0.1676
     2        0.2256             nan     0.5710   -0.0412
     3        0.1985             nan     0.5710    0.0286
     4        0.1748             nan     0.5710   -0.0187
     5        0.1406             nan     0.5710   -0.0009
     6        0.1228             nan     0.5710   -0.0081
     7        0.1094             nan     0.5710   -0.0007
     8        0.1039             nan     0.5710   -0.0302
     9        0.0931             nan     0.5710   -0.0325
    10        0.0873             nan     0.5710   -0.0177
    20        0.0473             nan     0.5710   -0.0077
    40        0.0129             nan     0.5710   -0.0016
    60        0.0036             nan     0.5710   -0.0005
    80        0.0007             nan     0.5710   -0.0001
   100        0.0002             nan     0.5710   -0.0000
   120        0.0001             nan     0.5710   -0.0000
   140        0.0000             nan     0.5710   -0.0000
   160        0.0000             nan     0.5710    0.0000
   180        0.0000             nan     0.5710   -0.0000
   200        0.0000             nan     0.5710   -0.0000
   220        0.0000             nan     0.5710   -0.0000
   240        0.0000             nan     0.5710   -0.0000
   260        0.0000             nan     0.5710    0.0000
   280        0.0000             nan     0.5710   -0.0000
   300        0.0000             nan     0.5710   -0.0000
   320        0.0000             nan     0.5710   -0.0000
   340        0.0000             nan     0.5710   -0.0000
   360        0.0000             nan     0.5710   -0.0000
   380        0.0000             nan     0.5710   -0.0000
   400        0.0000             nan     0.5710   -0.0000
   420        0.0000             nan     0.5710   -0.0000
   440        0.0000             nan     0.5710   -0.0000
   460        0.0000             nan     0.5710   -0.0000
   480        0.0000             nan     0.5710   -0.0000
   500        0.0000             nan     0.5710   -0.0000
   520        0.0000             nan     0.5710   -0.0000
   540        0.0000             nan     0.5710   -0.0000
   560        0.0000             nan     0.5710   -0.0000
   580        0.0000             nan     0.5710   -0.0000
   600        0.0000             nan     0.5710   -0.0000
   620        0.0000             nan     0.5710   -0.0000
   640        0.0000             nan     0.5710   -0.0000
   660        0.0000             nan     0.5710   -0.0000
   680        0.0000             nan     0.5710   -0.0000
   700        0.0000             nan     0.5710   -0.0000
   720        0.0000             nan     0.5710   -0.0000
   740        0.0000             nan     0.5710   -0.0000
   760        0.0000             nan     0.5710   -0.0000
   780        0.0000             nan     0.5710   -0.0000
   800        0.0000             nan     0.5710   -0.0000
   820        0.0000             nan     0.5710   -0.0000
   840        0.0000             nan     0.5710   -0.0000
   860        0.0000             nan     0.5710   -0.0000
   880        0.0000             nan     0.5710   -0.0000
   900        0.0000             nan     0.5710   -0.0000
   920        0.0000             nan     0.5710   -0.0000
   940        0.0000             nan     0.5710    0.0000
   960        0.0000             nan     0.5710   -0.0000
   980        0.0000             nan     0.5710   -0.0000
  1000        0.0000             nan     0.5710   -0.0000
  1020        0.0000             nan     0.5710   -0.0000
  1040        0.0000             nan     0.5710   -0.0000
  1060        0.0000             nan     0.5710   -0.0000
  1080        0.0000             nan     0.5710   -0.0000
  1100        0.0000             nan     0.5710    0.0000
  1120        0.0000             nan     0.5710   -0.0000
  1140        0.0000             nan     0.5710   -0.0000
  1160        0.0000             nan     0.5710   -0.0000
  1180        0.0000             nan     0.5710   -0.0000
  1200        0.0000             nan     0.5710   -0.0000
  1220        0.0000             nan     0.5710   -0.0000
  1240        0.0000             nan     0.5710   -0.0000
  1260        0.0000             nan     0.5710   -0.0000
  1280        0.0000             nan     0.5710   -0.0000
  1300        0.0000             nan     0.5710    0.0000
  1320        0.0000             nan     0.5710   -0.0000
  1340        0.0000             nan     0.5710   -0.0000
  1360        0.0000             nan     0.5710   -0.0000
  1380        0.0000             nan     0.5710   -0.0000
  1400        0.0000             nan     0.5710   -0.0000
  1420        0.0000             nan     0.5710   -0.0000
  1440        0.0000             nan     0.5710   -0.0000
  1460        0.0000             nan     0.5710   -0.0000
  1480        0.0000             nan     0.5710    0.0000
  1500        0.0000             nan     0.5710   -0.0000
  1520        0.0000             nan     0.5710   -0.0000
  1540        0.0000             nan     0.5710   -0.0000
  1560        0.0000             nan     0.5710   -0.0000
  1580        0.0000             nan     0.5710   -0.0000
  1600        0.0000             nan     0.5710   -0.0000
  1620        0.0000             nan     0.5710   -0.0000
  1640        0.0000             nan     0.5710    0.0000
  1660        0.0000             nan     0.5710    0.0000
  1680        0.0000             nan     0.5710   -0.0000
  1700        0.0000             nan     0.5710   -0.0000
  1720        0.0000             nan     0.5710   -0.0000
  1740        0.0000             nan     0.5710   -0.0000
  1760        0.0000             nan     0.5710   -0.0000
  1780        0.0000             nan     0.5710   -0.0000
  1800        0.0000             nan     0.5710   -0.0000
  1820        0.0000             nan     0.5710   -0.0000
  1840        0.0000             nan     0.5710   -0.0000
  1860        0.0000             nan     0.5710   -0.0000
  1880        0.0000             nan     0.5710   -0.0000
  1900        0.0000             nan     0.5710   -0.0000
  1920        0.0000             nan     0.5710   -0.0000
  1940        0.0000             nan     0.5710   -0.0000
  1960        0.0000             nan     0.5710   -0.0000
  1980        0.0000             nan     0.5710   -0.0000
  2000        0.0000             nan     0.5710   -0.0000
  2020        0.0000             nan     0.5710   -0.0000
  2040        0.0000             nan     0.5710   -0.0000
  2060        0.0000             nan     0.5710   -0.0000
  2080        0.0000             nan     0.5710   -0.0000
  2100        0.0000             nan     0.5710   -0.0000
  2120        0.0000             nan     0.5710   -0.0000
  2140        0.0000             nan     0.5710   -0.0000
  2160        0.0000             nan     0.5710   -0.0000
  2180        0.0000             nan     0.5710   -0.0000
  2200        0.0000             nan     0.5710   -0.0000
  2220        0.0000             nan     0.5710   -0.0000
  2240        0.0000             nan     0.5710   -0.0000
  2260        0.0000             nan     0.5710   -0.0000
  2280        0.0000             nan     0.5710   -0.0000
  2300        0.0000             nan     0.5710   -0.0000
  2320        0.0000             nan     0.5710   -0.0000
  2340        0.0000             nan     0.5710   -0.0000
  2360        0.0000             nan     0.5710   -0.0000
  2380        0.0000             nan     0.5710   -0.0000
  2400        0.0000             nan     0.5710   -0.0000
  2420        0.0000             nan     0.5710   -0.0000
  2440        0.0000             nan     0.5710    0.0000
  2460        0.0000             nan     0.5710   -0.0000
  2480        0.0000             nan     0.5710   -0.0000
  2500        0.0000             nan     0.5710   -0.0000
  2520        0.0000             nan     0.5710   -0.0000
  2540        0.0000             nan     0.5710   -0.0000
  2560        0.0000             nan     0.5710   -0.0000
  2580        0.0000             nan     0.5710   -0.0000
  2600        0.0000             nan     0.5710   -0.0000
  2620        0.0000             nan     0.5710   -0.0000
  2640        0.0000             nan     0.5710   -0.0000
  2660        0.0000             nan     0.5710   -0.0000
  2680        0.0000             nan     0.5710   -0.0000
  2700        0.0000             nan     0.5710   -0.0000
  2720        0.0000             nan     0.5710   -0.0000
  2740        0.0000             nan     0.5710   -0.0000
  2760        0.0000             nan     0.5710   -0.0000
  2780        0.0000             nan     0.5710   -0.0000
  2800        0.0000             nan     0.5710   -0.0000
  2820        0.0000             nan     0.5710   -0.0000
  2840        0.0000             nan     0.5710   -0.0000
  2860        0.0000             nan     0.5710   -0.0000
  2880        0.0000             nan     0.5710   -0.0000
  2900        0.0000             nan     0.5710   -0.0000
  2920        0.0000             nan     0.5710   -0.0000
  2940        0.0000             nan     0.5710   -0.0000
  2960        0.0000             nan     0.5710   -0.0000
  2980        0.0000             nan     0.5710   -0.0000
  3000        0.0000             nan     0.5710   -0.0000
  3020        0.0000             nan     0.5710   -0.0000
  3040        0.0000             nan     0.5710   -0.0000
  3060        0.0000             nan     0.5710   -0.0000
  3080        0.0000             nan     0.5710   -0.0000
  3089        0.0000             nan     0.5710   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5357             nan     0.0081    0.0040
     2        0.5299             nan     0.0081    0.0054
     3        0.5255             nan     0.0081    0.0050
     4        0.5214             nan     0.0081    0.0049
     5        0.5143             nan     0.0081    0.0058
     6        0.5096             nan     0.0081    0.0048
     7        0.5043             nan     0.0081    0.0045
     8        0.4991             nan     0.0081    0.0047
     9        0.4934             nan     0.0081    0.0049
    10        0.4881             nan     0.0081    0.0045
    20        0.4346             nan     0.0081    0.0044
    40        0.3560             nan     0.0081    0.0029
    60        0.2983             nan     0.0081    0.0026
    80        0.2594             nan     0.0081    0.0018
   100        0.2290             nan     0.0081    0.0013
   120        0.2053             nan     0.0081    0.0005
   140        0.1848             nan     0.0081    0.0003
   160        0.1702             nan     0.0081    0.0007
   180        0.1602             nan     0.0081    0.0005
   200        0.1528             nan     0.0081    0.0004
   220        0.1449             nan     0.0081    0.0002
   240        0.1385             nan     0.0081    0.0000
   260        0.1336             nan     0.0081    0.0001
   280        0.1282             nan     0.0081   -0.0001
   300        0.1234             nan     0.0081    0.0000
   320        0.1193             nan     0.0081   -0.0002
   340        0.1149             nan     0.0081   -0.0000
   360        0.1109             nan     0.0081   -0.0001
   380        0.1071             nan     0.0081    0.0001
   400        0.1035             nan     0.0081   -0.0001
   420        0.0998             nan     0.0081   -0.0003
   440        0.0960             nan     0.0081   -0.0003
   460        0.0931             nan     0.0081   -0.0002
   480        0.0906             nan     0.0081   -0.0001
   500        0.0879             nan     0.0081   -0.0001
   520        0.0851             nan     0.0081    0.0001
   540        0.0830             nan     0.0081   -0.0002
   560        0.0807             nan     0.0081   -0.0000
   580        0.0781             nan     0.0081   -0.0001
   600        0.0762             nan     0.0081   -0.0001
   620        0.0737             nan     0.0081   -0.0002
   640        0.0715             nan     0.0081   -0.0002
   660        0.0695             nan     0.0081   -0.0001
   680        0.0680             nan     0.0081   -0.0001
   700        0.0660             nan     0.0081    0.0000
   720        0.0641             nan     0.0081    0.0000
   740        0.0617             nan     0.0081   -0.0002
   760        0.0596             nan     0.0081   -0.0001
   780        0.0583             nan     0.0081   -0.0001
   800        0.0564             nan     0.0081   -0.0000
   820        0.0549             nan     0.0081   -0.0001
   840        0.0534             nan     0.0081   -0.0001
   860        0.0518             nan     0.0081   -0.0001
   880        0.0503             nan     0.0081   -0.0002
   900        0.0488             nan     0.0081   -0.0002
   920        0.0475             nan     0.0081   -0.0001
   940        0.0465             nan     0.0081   -0.0001
   960        0.0452             nan     0.0081   -0.0001
   980        0.0439             nan     0.0081   -0.0001
  1000        0.0423             nan     0.0081   -0.0000
  1020        0.0412             nan     0.0081   -0.0001
  1040        0.0401             nan     0.0081    0.0000
  1060        0.0392             nan     0.0081   -0.0001
  1080        0.0384             nan     0.0081   -0.0001
  1100        0.0376             nan     0.0081   -0.0001
  1120        0.0366             nan     0.0081   -0.0001
  1140        0.0357             nan     0.0081   -0.0001
  1160        0.0348             nan     0.0081   -0.0000
  1180        0.0336             nan     0.0081   -0.0001
  1200        0.0327             nan     0.0081   -0.0000
  1220        0.0318             nan     0.0081   -0.0001
  1231        0.0313             nan     0.0081   -0.0001

Stochastic Gradient Boosting 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  shrinkage   interaction.depth  n.minobsinnode  n.trees  RMSE       Rsquared 
  0.00813858   6                  6              1231     0.4622210  0.7035185
  0.01656825   7                 19              1624           NaN        NaN
  0.02336954   1                  8              3057     0.5764839  0.4644543
  0.09835572   1                 20              3266           NaN        NaN
  0.12028768   1                  6               837     0.5639732  0.5653647
  0.15003695  10                 21              1724           NaN        NaN
  0.20528514   9                 25               532           NaN        NaN
  0.21379531   7                 16              4253           NaN        NaN
  0.22099643   9                 25              3332           NaN        NaN
  0.22597885  10                  9              2790     0.6717945  0.3183775
  0.23397383   2                 11              3924     0.7962394  0.1732520
  0.25309367   3                 14               691           NaN        NaN
  0.28950304   1                  5              4373     0.5796169  0.5355594
  0.36214103   6                  6              3799     0.6141559  0.5264152
  0.40211076   5                 21              2089           NaN        NaN
  0.43585217   7                 23              2667           NaN        NaN
  0.44668392   3                  5              2232     0.6025052  0.5244612
  0.46814396   7                 15              4005           NaN        NaN
  0.46975333   5                 20              1143           NaN        NaN
  0.57103990   2                  6              3089     0.5934371  0.6329137
  MAE        Selected
  0.2932558  *       
        NaN          
  0.4042898          
        NaN          
  0.4120826          
        NaN          
        NaN          
        NaN          
        NaN          
  0.5078509          
  0.6149625          
        NaN          
  0.4108418          
  0.4410855          
        NaN          
        NaN          
  0.4542573          
        NaN          
        NaN          
  0.4526449          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were n.trees = 1231, interaction.depth =
 6, shrinkage = 0.00813858 and n.minobsinnode = 6.
[1] "Sat Mar 10 01:10:12 2018"
Error in relative.influence(object, n.trees = numTrees) : 
  could not find function "relative.influence"
In addition: There were 35 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:13:05 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "gbm_h2o"                 
Multivariate Adaptive Regression Splines 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE       Rsquared   MAE     
  0.1590873  0.9735094  0.105575

Tuning parameter 'degree' was held constant at a value of 1
[1] "Sat Mar 10 01:13:21 2018"
Generalized Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE       Rsquared   MAE      
  0.3655383  0.7970557  0.2797667

[1] "Sat Mar 10 01:13:37 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
2: model fit failed for Fold1: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
3: model fit failed for Fold1: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
4: model fit failed for Fold2: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
5: model fit failed for Fold2: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
6: model fit failed for Fold2: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
7: model fit failed for Fold3: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
8: model fit failed for Fold3: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
9: model fit failed for Fold3: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
2: model fit failed for Fold1: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
3: model fit failed for Fold1: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
4: model fit failed for Fold2: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
5: model fit failed for Fold2: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
6: model fit failed for Fold2: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
7: model fit failed for Fold3: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
8: model fit failed for Fold3: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
9: model fit failed for Fold3: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3     NA's   :3    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:13:55 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "glm.nb"                  
Boosted Generalized Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  mstop  prune  RMSE       Rsquared   MAE        Selected
  107    no     0.3546855  0.8079466  0.2690118          
  139    yes    0.3445321  0.8169594  0.2573956  *       
  168    yes    0.3445321  0.8169594  0.2573956          
  229    yes    0.3445321  0.8169594  0.2573956          
  247    no     0.3631998  0.7993760  0.2777620          
  325    no     0.3644042  0.7981881  0.2788244          
  345    no     0.3646039  0.7980033  0.2789911          
  418    yes    0.3445321  0.8169594  0.2573956          
  447    yes    0.3445321  0.8169594  0.2573956          
  534    no     0.3653642  0.7972299  0.2796286          
  558    no     0.3653945  0.7971991  0.2796526          
  612    yes    0.3445321  0.8169594  0.2573956          
  618    yes    0.3445321  0.8169594  0.2573956          
  654    yes    0.3445321  0.8169594  0.2573956          
  667    no     0.3654807  0.7971134  0.2797214          
  760    no     0.3655122  0.7970815  0.2797468          
  785    yes    0.3445321  0.8169594  0.2573956          
  802    no     0.3655201  0.7970736  0.2797526          
  851    no     0.3655266  0.7970672  0.2797577          
  875    yes    0.3445321  0.8169594  0.2573956          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 139 and prune = yes.
[1] "Sat Mar 10 01:14:13 2018"
glmnet 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  alpha      lambda       RMSE       Rsquared   MAE        Selected
  0.1062531  2.523131845  0.6382694  0.8310144  0.3695525          
  0.1381929  0.009568423  0.3648380  0.7971203  0.2757039          
  0.1673884  0.002301835  0.3648178  0.7972520  0.2780014          
  0.2284886  0.057345288  0.3588453  0.8033945  0.2546248          
  0.2461119  0.138731730  0.3574949  0.8136376  0.2262318          
  0.3247722  0.364629643  0.4063930  0.8240504  0.2046254          
  0.3447800  5.095076240  0.7247539        NaN  0.4432742          
  0.4177590  0.066027698  0.3517285  0.8134225  0.2449735  *       
  0.4464257  0.010925688  0.3622928  0.8002909  0.2737400          
  0.5334745  0.450547351  0.4548697  0.8310144  0.2266423          
  0.5579596  6.143066878  0.7247539        NaN  0.4432742          
  0.6113360  0.001600857  0.3644345  0.7980817  0.2784309          
  0.6177791  0.005597232  0.3631985  0.7995422  0.2763645          
  0.6533015  0.001924785  0.3644755  0.7981222  0.2784346          
  0.6663501  3.040158523  0.7247539        NaN  0.4432742          
  0.7598991  0.203342198  0.3720659  0.8310144  0.2062150          
  0.7849139  0.004056487  0.3633450  0.7994929  0.2770551          
  0.8010843  0.390203616  0.4642678  0.8310144  0.2333450          
  0.8506656  0.268237998  0.4035414  0.8310144  0.2020438          
  0.8747652  0.002232188  0.3641491  0.7986222  0.2782060          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alpha = 0.417759 and lambda
 = 0.0660277.
[1] "Sat Mar 10 01:14:31 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:17:24 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "glmnet_h2o"              
Start:  AIC=15.44
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V9    1   2.5675  13.438
- V4    1   2.5677  13.443
- V7    1   2.5691  13.470
- V10   1   2.5723  13.533
- V5    1   2.5785  13.654
- V6    1   2.5936  13.944
- V3    1   2.6433  14.895
<none>      2.5674  15.438
- V8    1   2.9628  20.599
- V2    1  14.5704 100.242

Step:  AIC=13.44
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V10

       Df Deviance    AIC
- V4    1   2.5677 11.443
- V7    1   2.5693 11.473
- V10   1   2.5725 11.538
- V5    1   2.5789 11.660
- V6    1   2.5943 11.959
- V3    1   2.6435 12.898
<none>      2.5675 13.438
- V8    1   2.9771 18.841
- V2    1  14.5738 98.254

Step:  AIC=11.44
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V10

       Df Deviance    AIC
- V7    1   2.5696  9.480
- V10   1   2.5729  9.544
- V5    1   2.5790  9.662
- V6    1   2.5965 10.002
- V3    1   2.6435 10.898
<none>      2.5677 11.443
- V8    1   2.9778 16.852
- V2    1  14.9452 97.512

Step:  AIC=9.48
.outcome ~ V2 + V3 + V5 + V6 + V8 + V10

       Df Deviance    AIC
- V10   1   2.5737  7.560
- V5    1   2.5822  7.725
- V6    1   2.5976  8.023
- V3    1   2.6435  8.898
<none>      2.5696  9.480
- V8    1   2.9799 14.887
- V2    1  14.9580 95.555

Step:  AIC=7.56
.outcome ~ V2 + V3 + V5 + V6 + V8

       Df Deviance    AIC
- V5    1   2.5890  5.856
- V6    1   2.5998  6.064
- V3    1   2.6435  6.898
<none>      2.5737  7.560
- V8    1   2.9801 12.890
- V2    1  16.0713 97.145

Step:  AIC=5.86
.outcome ~ V2 + V3 + V6 + V8

       Df Deviance    AIC
- V6    1   2.6084  4.230
- V3    1   2.6746  5.483
<none>      2.5890  5.856
- V8    1   2.9923 11.095
- V2    1  16.0917 95.208

Step:  AIC=4.23
.outcome ~ V2 + V3 + V8

       Df Deviance    AIC
- V3    1   2.6970  3.899
<none>      2.6084  4.230
- V8    1   3.0485 10.025
- V2    1  16.1069 93.255

Step:  AIC=3.9
.outcome ~ V2 + V8

       Df Deviance    AIC
<none>      2.6970  3.899
- V8    1   3.2656 11.465
- V2    1  17.3235 94.896
Start:  AIC=52.07
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V10   1   5.3465  50.115
- V5    1   5.3518  50.164
- V4    1   5.3568  50.211
- V6    1   5.3615  50.255
- V7    1   5.3976  50.590
- V8    1   5.4046  50.655
- V9    1   5.4987  51.518
<none>      5.3417  52.070
- V3    1   5.6145  52.561
- V2    1  26.6990 130.524

Step:  AIC=50.11
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9

       Df Deviance     AIC
- V5    1   5.3570  48.213
- V4    1   5.3608  48.248
- V6    1   5.3688  48.323
- V7    1   5.3998  48.611
- V8    1   5.4149  48.751
- V9    1   5.4990  49.521
<none>      5.3465  50.115
- V3    1   5.6669  51.025
- V2    1  27.8976 130.720

Step:  AIC=48.21
.outcome ~ V2 + V3 + V4 + V6 + V7 + V8 + V9

       Df Deviance     AIC
- V4    1   5.3666  46.303
- V6    1   5.3902  46.522
- V7    1   5.4144  46.746
- V8    1   5.4188  46.786
- V9    1   5.5096  47.618
<none>      5.3570  48.213
- V3    1   5.6837  49.173
- V2    1  31.1193 134.184

Step:  AIC=46.3
.outcome ~ V2 + V3 + V6 + V7 + V8 + V9

       Df Deviance     AIC
- V6    1   5.3943  44.560
- V7    1   5.4242  44.837
- V8    1   5.4297  44.887
- V9    1   5.5177  45.691
<none>      5.3666  46.303
- V3    1   5.7404  47.669
- V2    1  31.1629 132.254

Step:  AIC=44.56
.outcome ~ V2 + V3 + V7 + V8 + V9

       Df Deviance     AIC
- V7    1   5.4345  42.931
- V8    1   5.4665  43.225
- V9    1   5.5582  44.056
<none>      5.3943  44.560
- V3    1   5.7602  45.842
- V2    1  31.2478 130.390

Step:  AIC=42.93
.outcome ~ V2 + V3 + V8 + V9

       Df Deviance     AIC
- V8    1   5.4920  41.457
- V9    1   5.6423  42.808
<none>      5.4345  42.931
- V3    1   5.8002  44.187
- V2    1  31.4446 128.704

Step:  AIC=41.46
.outcome ~ V2 + V3 + V9

       Df Deviance     AIC
- V9    1    5.706  41.371
<none>       5.492  41.457
- V3    1    5.876  42.839
- V2    1   33.057 129.204

Step:  AIC=41.37
.outcome ~ V2 + V3

       Df Deviance     AIC
<none>       5.706  41.371
- V3    1    6.142  43.051
- V2    1   33.559 127.957
Start:  AIC=51.93
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V7    1   5.4150  49.942
- V10   1   5.4241  50.029
- V4    1   5.4257  50.045
- V5    1   5.5030  50.780
- V6    1   5.5156  50.899
- V8    1   5.5522  51.243
<none>      5.4136  51.928
- V3    1   5.6757  52.387
- V9    1   5.6802  52.429
- V2    1  24.1959 127.787

Step:  AIC=49.94
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V9 + V10

       Df Deviance     AIC
- V10   1   5.4246  48.034
- V4    1   5.4266  48.053
- V5    1   5.5037  48.786
- V6    1   5.5161  48.903
- V8    1   5.5741  49.447
<none>      5.4150  49.942
- V3    1   5.6776  50.405
- V9    1   5.6824  50.448
- V2    1  24.4056 126.235

Step:  AIC=48.03
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V9

       Df Deviance     AIC
- V4    1   5.4306  46.091
- V5    1   5.5118  46.863
- V6    1   5.5226  46.965
- V8    1   5.6047  47.732
<none>      5.4246  48.034
- V3    1   5.6891  48.510
- V9    1   5.7375  48.950
- V2    1  25.4810 126.477

Step:  AIC=46.09
.outcome ~ V2 + V3 + V5 + V6 + V8 + V9

       Df Deviance     AIC
- V5    1   5.5147  44.891
- V6    1   5.5267  45.004
- V8    1   5.6063  45.748
<none>      5.4306  46.091
- V3    1   5.7352  46.930
- V9    1   5.7503  47.066
- V2    1  26.1766 125.878

Step:  AIC=44.89
.outcome ~ V2 + V3 + V6 + V8 + V9

       Df Deviance     AIC
- V6    1   5.6012  43.700
- V8    1   5.7197  44.788
<none>      5.5147  44.891
- V9    1   5.7728  45.270
- V3    1   5.8235  45.724
- V2    1  26.7915 125.085

Step:  AIC=43.7
.outcome ~ V2 + V3 + V8 + V9

       Df Deviance     AIC
- V9    1   5.8175  43.670
<none>      5.6012  43.700
- V8    1   5.8248  43.735
- V3    1   5.8845  44.266
- V2    1  27.4363 124.322

Step:  AIC=43.67
.outcome ~ V2 + V3 + V8

       Df Deviance     AIC
- V8    1   6.0008  43.284
<none>      5.8175  43.670
- V3    1   6.0945  44.089
- V2    1  28.7246 124.708

Step:  AIC=43.28
.outcome ~ V2 + V3

       Df Deviance     AIC
- V3    1   6.2111  43.075
<none>      6.0008  43.284
- V2    1  28.7293 122.717

Step:  AIC=43.07
.outcome ~ V2

       Df Deviance     AIC
<none>      6.2111  43.075
- V2    1  28.7305 120.719
Start:  AIC=59.03
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V10   1    7.255  57.147
- V7    1    7.258  57.178
- V4    1    7.260  57.207
- V5    1    7.274  57.350
- V8    1    7.280  57.415
- V6    1    7.315  57.775
- V9    1    7.391  58.566
<none>       7.243  59.026
- V3    1    7.658  61.256
- V2    1   36.106 179.114

Step:  AIC=57.15
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9

       Df Deviance     AIC
- V7    1    7.267  55.272
- V4    1    7.269  55.301
- V5    1    7.283  55.445
- V8    1    7.286  55.473
- V6    1    7.320  55.829
- V9    1    7.427  56.927
<none>       7.255  57.147
- V3    1    7.658  59.256
- V2    1   37.462 179.916

Step:  AIC=55.27
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V9

       Df Deviance     AIC
- V4    1    7.280  53.414
- V8    1    7.292  53.539
- V5    1    7.294  53.562
- V6    1    7.325  53.881
- V9    1    7.452  55.188
<none>       7.267  55.272
- V3    1    7.660  57.283
- V2    1   37.573 178.142

Step:  AIC=53.41
.outcome ~ V2 + V3 + V5 + V6 + V8 + V9

       Df Deviance     AIC
- V5    1    7.304  51.665
- V8    1    7.308  51.705
- V6    1    7.332  51.951
- V9    1    7.465  53.323
<none>       7.280  53.414
- V3    1    7.717  55.841
- V2    1   37.732 176.463

Step:  AIC=51.66
.outcome ~ V2 + V3 + V6 + V8 + V9

       Df Deviance     AIC
- V8    1    7.328  49.917
- V6    1    7.371  50.356
- V9    1    7.478  51.451
<none>       7.304  51.665
- V3    1    7.733  53.998
- V2    1   38.652 176.292

Step:  AIC=49.92
.outcome ~ V2 + V3 + V6 + V9

       Df Deviance     AIC
- V6    1    7.402  48.678
- V9    1    7.514  49.815
<none>       7.328  49.917
- V3    1    7.797  52.626
- V2    1   40.123 177.131

Step:  AIC=48.68
.outcome ~ V2 + V3 + V9

       Df Deviance     AIC
- V9    1    7.587  48.549
<none>       7.402  48.678
- V3    1    7.869  51.324
- V2    1   40.364 175.587

Step:  AIC=48.55
.outcome ~ V2 + V3

       Df Deviance     AIC
<none>       7.587  48.549
- V3    1    8.063  51.172
- V2    1   41.154 175.060
Generalized Linear Model with Stepwise Feature Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE       Rsquared   MAE      
  0.3503224  0.8106039  0.2699693

[1] "Sat Mar 10 01:17:41 2018"
Independent Component Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  n.comp  RMSE       Rsquared   MAE        Selected
  1       0.6723974  0.1820856  0.4216456          
  2       0.6844510  0.1805325  0.4335476          
  3       0.5466535  0.4672595  0.3487878          
  4       0.5519064  0.4729230  0.3568796          
  5       0.5351228  0.5433662  0.3660771          
  6       0.5750992  0.4778954  0.4006048          
  7       0.5645326  0.4898139  0.4030452          
  8       0.5292853  0.5328332  0.3847369  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was n.comp = 8.
[1] "Sat Mar 10 01:17:59 2018"
Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      0.4397052  0.6884930  0.2924733          
  2      0.3985398  0.7450389  0.2955116          
  3      0.3736333  0.7866164  0.2830753          
  4      0.3683432  0.7921746  0.2802696          
  5      0.3651038  0.7975320  0.2793702  *       
  6      0.3654336  0.7971712  0.2797140          
  7      0.3655176  0.7970652  0.2797475          
  8      0.3655335  0.7970557  0.2797596          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 5.
[1] "Sat Mar 10 01:18:15 2018"
Error in MSEP(object) : could not find function "MSEP"
k-Nearest Neighbors 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  kmax  distance   kernel        RMSE       Rsquared   MAE        Selected
   3    2.6158190  epanechnikov  0.5889526  0.3964941  0.3535673          
   4    0.7598070  biweight      0.5707059  0.4352552  0.3181719          
   5    0.2854615  triangular    0.5740847  0.4339591  0.3309669          
   6    1.3559582  inv           0.5938624  0.3522676  0.3465041          
   7    1.6500853  rectangular   0.5733947  0.4316892  0.3446576          
   9    1.9718086  rectangular   0.5674141  0.4383687  0.3340153          
   9    2.8497932  triangular    0.5703830  0.4802382  0.3229158          
  11    1.4028958  cos           0.5621884  0.4455382  0.3241182          
  12    0.8039695  cos           0.5917999  0.4305271  0.3293624          
  14    2.0422501  cos           0.5558922  0.4920995  0.3233941          
  14    2.9120675  biweight      0.5948238  0.4293316  0.3381428          
  16    0.1645523  rectangular   0.6379622  0.2821316  0.3686459          
  16    0.5812913  gaussian      0.6227484  0.3419954  0.3600167          
  17    0.2259030  triangular    0.6241662  0.3531530  0.3594118          
  17    2.6778800  epanechnikov  0.5694109  0.4829186  0.3249359          
  19    1.7773804  triweight     0.5647074  0.4756013  0.3210367          
  20    0.4741031  biweight      0.5960552  0.4396612  0.3468200          
  21    1.9943767  inv           0.5692014  0.4486486  0.3341457          
  22    0.2752325  biweight      0.6137465  0.3979519  0.3511525          
  22    1.8695967  epanechnikov  0.5537833  0.5021212  0.3198608  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were kmax = 22, distance = 1.869597
 and kernel = epanechnikov.
[1] "Sat Mar 10 01:18:34 2018"
k-Nearest Neighbors 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  k   RMSE       Rsquared   MAE        Selected
   3  0.5493516  0.4857616  0.3262963  *       
   4  0.5523842  0.4931834  0.3207927          
   5  0.5698007  0.4584852  0.3287545          
   6  0.5780162  0.4760007  0.3334209          
   7  0.5841749  0.4737378  0.3282233          
   9  0.5893399  0.5333121  0.3363324          
  11  0.5954877  0.5415859  0.3425997          
  12  0.5932603  0.5575133  0.3371054          
  14  0.5919295  0.5938587  0.3368815          
  16  0.5885158  0.6232474  0.3338490          
  17  0.5905459  0.6269825  0.3353149          
  19  0.6022507  0.6037203  0.3427330          
  20  0.6071228  0.5996263  0.3465376          
  21  0.6086622  0.6068174  0.3491249          
  22  0.6113798  0.6038023  0.3491349          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 3.
[1] "Sat Mar 10 01:18:50 2018"
Polynomial Kernel Regularized Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  lambda        degree  RMSE          Rsquared    MAE           Selected
  3.398331e-05  3          0.7240269  0.33311179     0.4421503  *       
  4.908678e-05  1       3566.1616297  0.15063168  2851.2608332          
  6.869763e-05  1       2548.1954656  0.15063057  2037.3851584          
  1.388171e-04  2          0.7354763  0.03929431     0.4536762          
  1.700434e-04  2          0.7354726  0.03929218     0.4536702          
  4.205922e-04  2          0.7354428  0.03927505     0.4536225          
  5.295414e-04  3          0.7240269  0.33311278     0.4421503          
  1.226861e-03  2          0.7353483  0.03921922     0.4534707          
  1.706586e-03  1        102.7548002  0.15053755    82.2254390          
  4.649139e-03  3          0.7240270  0.33312104     0.4421504          
  6.163082e-03  3          0.7240270  0.33312407     0.4421504          
  1.139410e-02  1         15.5616644  0.14998761    12.5031070          
  1.227144e-02  1         14.4644859  0.14993782    11.6249704          
  1.847174e-02  1          9.6846469  0.14958610     7.7969300          
  2.146598e-02  3          0.7240271  0.33315469     0.4421507          
  6.302247e-02  2          0.7310273  0.03447145     0.4467361          
  8.405619e-02  1          2.3678123  0.14587698     1.9040577          
  1.012562e-01  2          0.7296003  0.03324633     0.4446924          
  1.791958e-01  2          0.7276786  0.03468440     0.4419815          
  2.364973e-01  1          1.1483701  0.13734677     0.8779592          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 3.398331e-05 and degree = 3.
[1] "Sat Mar 10 01:19:08 2018"

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.613936558  0.008996853  0.007440505  0.070053633 -0.025293198  0.010415414 
          V8           V9          V10 
-0.005939161 -0.106028767 -0.021065101 

 Quartiles of Marginal Effects:
 
           V2          V3           V4          V5          V6          V7
25% 0.1922393 -0.10408614 -0.164361273 -0.10575833 -0.12125999 -0.14452804
50% 0.4034480 -0.01027518 -0.002894803  0.01832246 -0.01090042  0.01284408
75% 0.8211667  0.21694335  0.144151608  0.23046718  0.10449773  0.09846287
             V8          V9         V10
25% -0.23305480 -0.25726759 -0.13002700
50% -0.02434853 -0.05912738 -0.04006448
75%  0.12662758  0.06255019  0.07412809

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.19510695  0.12872520  0.09065957  0.10438916  0.07625545  0.06727146 
         V8          V9         V10 
-0.28129047 -0.01952251 -0.14332605 

 Quartiles of Marginal Effects:
 
          V2          V3          V4         V5          V6          V7
25% 2.028197 0.006221579 -0.01418982 -0.1647832 -0.10478800 -0.05555031
50% 2.207177 0.130006513  0.09198526  0.1498884  0.06593821  0.06214605
75% 2.404668 0.267000264  0.19933086  0.4444736  0.24910903  0.12710229
            V8          V9         V10
25% -0.3770689 -0.16029990 -0.26680452
50% -0.2972361 -0.04452306 -0.16725468
75% -0.1345568  0.08464950 -0.04484355

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.12913583  0.25722809 -0.01336919 -0.05217129  0.10302502  0.12338657 
         V8          V9         V10 
-0.10227469 -0.15431953 -0.62910291 

 Quartiles of Marginal Effects:
 
          V2          V3         V4         V5         V6         V7
25% 1.414757 0.006496119 -0.4217292 -0.6760301 -0.3895891 -0.3045486
50% 2.086090 0.297173250  0.0275257 -0.1018702  0.1754226  0.1459368
75% 2.853742 0.511104558  0.3588785  0.6555387  0.6643452  0.6692496
             V8         V9        V10
25% -0.36452635 -0.4841609 -1.0276314
50% -0.01251554 -0.1968742 -0.6113715
75%  0.25609598  0.1329310 -0.3089349

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.885484896 -0.104317038  0.097391324  0.019851880  0.001141244 -0.037735182 
          V8           V9          V10 
-0.326008804 -0.047349711  0.189072614 

 Quartiles of Marginal Effects:
 
           V2         V3         V4         V5           V6          V7
25% 0.8846265 -0.1047454 0.09710838 0.01910176 0.0006922477 -0.03837106
50% 0.8860355 -0.1042933 0.09751967 0.02002711 0.0011138331 -0.03785507
75% 0.8871331 -0.1039787 0.09789190 0.02052994 0.0017016192 -0.03743723
            V8          V9       V10
25% -0.3268123 -0.04803236 0.1885947
50% -0.3263264 -0.04752888 0.1892980
75% -0.3254849 -0.04684997 0.1899288

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.443532427  0.006628521  0.015252370  0.062526994 -0.015321826 -0.006070492 
          V8           V9          V10 
 0.002339314 -0.087919083 -0.020994922 

 Quartiles of Marginal Effects:
 
           V2           V3           V4          V5          V6           V7
25% 0.1027535 -0.077828865 -0.127867281 -0.08244353 -0.06795285 -0.124954624
50% 0.2584299 -0.005114913 -0.002218117  0.01997230 -0.01174754  0.008148218
75% 0.5623895  0.145118287  0.092905880  0.20260815  0.07585435  0.082793404
             V8          V9         V10
25% -0.14008713 -0.20557362 -0.08533416
50% -0.01250175 -0.03969627 -0.02086550
75%  0.07486682  0.07630666  0.06154251

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.441728836  0.238392871  0.083718306  0.007467497  0.181734778  0.059927915 
          V8           V9          V10 
-0.303575316  0.019740603 -0.455665944 

 Quartiles of Marginal Effects:
 
          V2         V3          V4          V5         V6          V7
25% 2.102969 0.05250061 -0.12562788 -0.45769769 -0.1367889 -0.14172278
50% 2.408288 0.29172019  0.06364445 -0.03770037  0.1436019  0.07479337
75% 2.776414 0.42617139  0.30396532  0.39545498  0.5890654  0.24064684
            V8          V9        V10
25% -0.5323001 -0.18517258 -0.5993586
50% -0.3317989  0.04790513 -0.4561135
75% -0.1004378  0.19287368 -0.3085477

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.088707977  0.014783810  0.011996103  0.030442515 -0.002704369 -0.019593182 
          V8           V9          V10 
 0.016372446 -0.048079591 -0.015157632 

 Quartiles of Marginal Effects:
 
             V2            V3            V4            V5           V6
25% 0.002056333 -1.353172e-02 -0.0189386775 -0.0102949303 -0.026759470
50% 0.024627520 -8.918121e-05 -0.0004582545  0.0006346391  0.000428457
75% 0.116511032  2.504849e-02  0.0258846577  0.0155421565  0.022216468
               V7            V8           V9           V10
25% -1.112596e-02 -0.0290709304 -0.034732452 -0.0155724683
50%  3.155742e-05  0.0002792203 -0.001544394 -0.0001178471
75%  2.925442e-02  0.0165372638  0.009237916  0.0215928067

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.92449059  0.10842361  0.06811580  0.10344813  0.02842569  0.07596944 
         V8          V9         V10 
-0.21425032 -0.05056217 -0.12203751 

 Quartiles of Marginal Effects:
 
          V2          V3          V4         V5          V6          V7
25% 1.650135 -0.04144597 -0.08565295 -0.1781291 -0.18174011 -0.06836760
50% 1.895882  0.07450039  0.09516712  0.1575144  0.03014932  0.03446708
75% 2.191181  0.26840223  0.18466670  0.4666806  0.23066824  0.18635921
              V8          V9          V10
25% -0.364436983 -0.19284064 -0.273516588
50% -0.174955593 -0.07061223 -0.131741389
75% -0.007271881  0.13642013  0.003893339

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.22171907  0.01279740  0.07758214  0.09619267  0.04998628 -0.01878849 
         V8          V9         V10 
-0.53693661 -0.03015691  0.11839039 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5         V6          V7         V8
25% 2.218851 0.01080350 0.07584482 0.09130097 0.04719920 -0.02112214 -0.5411984
50% 2.223343 0.01277355 0.07819583 0.09708822 0.04974916 -0.01909100 -0.5369329
75% 2.227698 0.01527572 0.07993079 0.10080203 0.05171148 -0.01700867 -0.5346175
             V9       V10
25% -0.03288007 0.1161954
50% -0.03059814 0.1187171
75% -0.02705933 0.1219011

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.54985443 -0.09360328  0.11041008  0.06266359  0.01019103 -0.03069592 
         V8          V9         V10 
-0.48234452 -0.04933290  0.21239588 

 Quartiles of Marginal Effects:
 
          V2          V3        V4         V5          V6          V7
25% 1.549135 -0.09383231 0.1101817 0.06218066 0.009843385 -0.03111258
50% 1.550198 -0.09357399 0.1104852 0.06284555 0.010138407 -0.03078730
75% 1.550961 -0.09327888 0.1107754 0.06326037 0.010593757 -0.03048089
            V8          V9       V10
25% -0.4829369 -0.04979823 0.2121850
50% -0.4824389 -0.04944159 0.2124912
75% -0.4819936 -0.04892909 0.2129345

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.73649310  0.17904721  0.05170530  0.09528100  0.13359612  0.02329809 
         V8          V9         V10 
-0.43260479  0.01076141 -0.10851884 

 Quartiles of Marginal Effects:
 
          V2        V3         V4          V5         V6            V7
25% 2.674205 0.1387641 0.02048200 0.004757031 0.09161786 -0.0003122512
50% 2.749289 0.1794620 0.05194808 0.109377456 0.12284781  0.0252419058
75% 2.792977 0.2195351 0.07759771 0.176906069 0.17037829  0.0460948512
            V8          V9         V10
25% -0.4825744 -0.02936443 -0.15874788
50% -0.4267250  0.01025164 -0.11063742
75% -0.3898243  0.03225990 -0.07710718

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.683495727  0.162160831  0.057012912  0.098451093  0.125739894  0.023669167 
          V8           V9          V10 
-0.437563135  0.006408266 -0.086533901 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5         V6           V7         V8
25% 2.626858 0.1259676 0.02609047 0.01490244 0.08042091 0.0001351456 -0.4856274
50% 2.701494 0.1638373 0.06069240 0.11569288 0.11769909 0.0243653994 -0.4337085
75% 2.750169 0.2039931 0.08392102 0.17592341 0.16705759 0.0442443842 -0.3926080
              V9         V10
25% -0.030166472 -0.13311819
50%  0.003471774 -0.08468140
75%  0.029207257 -0.05294137

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.297656078  0.245340235  0.066876356 -0.001887242  0.148958191  0.068205727 
          V8           V9          V10 
-0.262008285 -0.010552719 -0.494913267 

 Quartiles of Marginal Effects:
 
          V2         V3          V4          V5         V6         V7
25% 1.825604 0.01415995 -0.23825431 -0.49441144 -0.2103068 -0.1838486
50% 2.256854 0.26428558  0.04294528 -0.09771522  0.1304417  0.0404821
75% 2.732894 0.45723229  0.32470381  0.50747158  0.6071917  0.3093150
              V8          V9        V10
25% -0.489758864 -0.24550137 -0.6761495
50% -0.276193446  0.02872793 -0.4915604
75%  0.005462697  0.18438057 -0.3440733

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.145193850  0.013842460  0.014963594  0.039363718 -0.003006995 -0.020543012 
          V8           V9          V10 
 0.017130427 -0.058173017 -0.018831403 

 Quartiles of Marginal Effects:
 
             V2           V3           V4           V5            V6
25% 0.008224459 -0.025434302 -0.039338128 -0.024753082 -0.0457160878
50% 0.051159652 -0.001017393 -0.003016337  0.001760941 -0.0006468857
75% 0.206621020  0.052101613  0.035662988  0.051061459  0.0466476809
               V7            V8           V9           V10
25% -0.0345683406 -0.0435241811 -0.062755879 -0.0318424533
50%  0.0004588603  0.0001698205 -0.004488065 -0.0004118365
75%  0.0471109730  0.0339822337  0.025279041  0.0339423107

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.244399831 -0.040332410  0.036083723 -0.001652407  0.001558322 -0.017866672 
          V8           V9          V10 
-0.100724439 -0.018515299  0.068441226 

 Quartiles of Marginal Effects:
 
           V2          V3         V4           V5          V6          V7
25% 0.2442668 -0.04040898 0.03604040 -0.001749726 0.001497244 -0.01797016
50% 0.2444918 -0.04032858 0.03611422 -0.001638498 0.001551191 -0.01789051
75% 0.2446606 -0.04027535 0.03616858 -0.001574839 0.001636174 -0.01779348
            V8          V9        V10
25% -0.1008623 -0.01861556 0.06836917
50% -0.1007802 -0.01853697 0.06849022
75% -0.1006619 -0.01844667 0.06858054

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.85901065  0.12465780 -0.06050872  0.04118314 -0.09702097  0.25320283 
         V8          V9         V10 
-0.03284181 -0.16566335 -0.32050442 

 Quartiles of Marginal Effects:
 
          V2         V3          V4          V5          V6          V7
25% 1.425921 -0.1151909 -0.49019426 -0.37912873 -0.38465196 -0.06055611
50% 1.803035  0.1239349  0.01241423 -0.03817114  0.02845624  0.30217879
75% 2.307337  0.4136546  0.31578734  0.39260505  0.20666465  0.48638784
             V8          V9        V10
25% -0.48935954 -0.40453089 -0.5708293
50% -0.02567162 -0.20299414 -0.2996743
75%  0.53929582  0.01990169 -0.1045205

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 1.357156868 -0.105763097  0.111748265  0.049796381  0.005336136 -0.034267446 
          V8           V9          V10 
-0.446507935 -0.051319991  0.217114954 

 Quartiles of Marginal Effects:
 
          V2         V3        V4         V5          V6          V7         V8
25% 1.356416 -0.1060125 0.1115401 0.04931339 0.004982430 -0.03470788 -0.4471037
50% 1.357531 -0.1057282 0.1118471 0.04999709 0.005265772 -0.03434918 -0.4466204
75% 1.358335 -0.1054528 0.1121304 0.05036686 0.005761342 -0.03406749 -0.4460979
             V9       V10
25% -0.05180685 0.2167970
50% -0.05145284 0.2172019
75% -0.05090837 0.2177043

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.02875012  0.07031566  0.09854293  0.11812672  0.05314758  0.05178524 
         V8          V9         V10 
-0.30136354 -0.03913738 -0.02422967 

 Quartiles of Marginal Effects:
 
          V2          V3         V4          V5          V6          V7
25% 1.902672 -0.04251438 0.01696245 -0.05499228 -0.07850577 -0.03736318
50% 2.041754  0.06336493 0.10043545  0.17518006  0.02456237  0.01838481
75% 2.217233  0.18670921 0.17009317  0.36357774  0.17052644  0.11651655
            V8          V9         V10
25% -0.4206382 -0.16278013 -0.14694388
50% -0.2749294 -0.07281601 -0.03264875
75% -0.1613889  0.07111694  0.06671738

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.81610084  0.12096110 -0.03978496  0.03888837 -0.06809821  0.21782308 
         V8          V9         V10 
-0.07196704 -0.12982653 -0.28218921 

 Quartiles of Marginal Effects:
 
          V2          V3          V4          V5          V6          V7
25% 1.276258 -0.08229558 -0.41037514 -0.34309080 -0.35204954 -0.05717264
50% 1.730806  0.09366278 -0.01461228 -0.01898367 -0.02905807  0.28730727
75% 2.264404  0.37547284  0.31882414  0.45852037  0.28188243  0.39293891
             V8          V9        V10
25% -0.38964808 -0.29822640 -0.5049155
50% -0.07618354 -0.13530931 -0.2649037
75%  0.44412325  0.02680974 -0.0967208

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 2.003619e+00  1.783175e-01 -5.892558e-02  5.136292e-03 -5.388733e-02 
           V7            V8            V9           V10 
 2.370921e-01 -9.678945e-05 -2.196315e-01 -4.778135e-01 

 Quartiles of Marginal Effects:
 
          V2          V3          V4          V5          V6         V7
25% 1.406132 -0.04887885 -0.53899383 -0.59419792 -0.47577069 -0.2497632
50% 1.977344  0.15266288  0.01801995 -0.03098181  0.04251048  0.2348133
75% 2.552480  0.53308458  0.39544347  0.49906509  0.41288344  0.7657954
             V8         V9        V10
25% -0.37681784 -0.5994246 -0.8012680
50%  0.06490404 -0.3417380 -0.4392524
75%  0.49870511  0.1468817 -0.2114434

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.715171998  0.002455716 -0.054591360 -0.121575495 -0.026797281 -0.080403213 
          V8           V9          V10 
-0.052140487 -0.015865519 -0.018487741 

 Quartiles of Marginal Effects:
 
           V2           V3          V4          V5          V6          V7
25% 0.2641357 -0.143034323 -0.20049380 -0.32977474 -0.12527836 -0.27686996
50% 0.6109761  0.005804639 -0.02885377 -0.10695067 -0.01608972 -0.03625782
75% 0.9136089  0.153865440  0.09448458  0.08290745  0.06509570  0.17657862
             V8          V9         V10
25% -0.22026984 -0.17239562 -0.13714269
50% -0.03344998 -0.06010331  0.01520198
75%  0.12636449  0.11118796  0.12369939

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.89988968  0.28225199  0.07594791 -0.01754453  0.05047711 -0.18716324 
         V8          V9         V10 
-0.03367886 -0.17607099 -0.01509587 

 Quartiles of Marginal Effects:
 
          V2        V3          V4          V5          V6          V7
25% 2.566504 0.1760241 -0.10098681 -0.22932557 -0.11579878 -0.34918595
50% 2.951780 0.2757478  0.03615804 -0.08381091  0.06193137 -0.20055103
75% 3.255367 0.4475718  0.25069387  0.22934849  0.20617640 -0.04888701
             V8         V9          V10
25% -0.29267996 -0.5000585 -0.194901964
50% -0.03078656 -0.3287955 -0.001948686
75%  0.25266221  0.1566382  0.196592572

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.82525178  0.30644375  0.18105381  0.17583130  0.26143103 -0.37842385 
         V8          V9         V10 
 0.10791297 -0.16586919 -0.01292737 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5         V6         V7          V8
25% 1.085046 -0.3840533 -0.3501836 -0.4865235 -0.2873967 -0.8757856 -0.63973764
50% 2.871185  0.3605600  0.0951004  0.3013385  0.1941802 -0.5011496 -0.02027513
75% 4.605648  1.0058415  0.6615750  0.8433366  0.8147710  0.2040769  0.92428772
            V9         V10
25% -0.8021099 -0.58964285
50% -0.4036534 -0.03648996
75%  0.5065778  0.53225766

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.10409411  0.04569053 -0.06158991 -0.28032331  0.05231317  0.05653674 
         V8          V9         V10 
-0.21406587 -0.14196144  0.22101324 

 Quartiles of Marginal Effects:
 
          V2         V3          V4         V5         V6         V7         V8
25% 1.103363 0.04503684 -0.06221183 -0.2813105 0.05187414 0.05573163 -0.2152771
50% 1.104861 0.04557816 -0.06173240 -0.2807624 0.05247216 0.05670761 -0.2145330
75% 1.106116 0.04626257 -0.06075574 -0.2794496 0.05284771 0.05743680 -0.2135200
            V9       V10
25% -0.1426909 0.2209441
50% -0.1419621 0.2214087
75% -0.1412845 0.2218562

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.478411797 -0.003145284 -0.045671742 -0.082994303 -0.019265678 -0.054142226 
          V8           V9          V10 
-0.043325811 -0.007495626 -0.011869555 

 Quartiles of Marginal Effects:
 
           V2           V3          V4          V5          V6           V7
25% 0.1504261 -0.103448658 -0.15366517 -0.21722963 -0.07804263 -0.213798598
50% 0.3846463 -0.003098223 -0.01108013 -0.06813370 -0.02111481  0.009786921
75% 0.5964575  0.148498955  0.07025414  0.05933433  0.05957767  0.136973075
             V8          V9         V10
25% -0.12774051 -0.11482922 -0.09932690
50% -0.02324951 -0.02752627  0.01997313
75%  0.09961863  0.07537286  0.10503593

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 3.17512110  0.40664931  0.21316739  0.15202525  0.10625149 -0.21048934 
         V8          V9         V10 
 0.04387299 -0.08179643 -0.12864301 

 Quartiles of Marginal Effects:
 
          V2        V3          V4         V5         V6           V7
25% 2.347389 0.1065830 -0.06990888 -0.1506259 -0.1041018 -0.380087787
50% 3.135671 0.4398017  0.14480721  0.1273829  0.1568459 -0.230515253
75% 3.998862 0.6961905  0.46056907  0.4616746  0.3404366 -0.001254566
            V8         V9        V10
25% -0.3431549 -0.5573353 -0.5014408
50%  0.1974967 -0.1197808 -0.1522019
75%  0.4246786  0.4629170  0.2325345

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.060108236 -0.002807100 -0.008686420 -0.008288675 -0.001207741 -0.010302848 
          V8           V9          V10 
-0.008613353  0.002261104 -0.001335917 

 Quartiles of Marginal Effects:
 
              V2           V3           V4           V5            V6
25% -0.000276544 -0.029874936 -0.047114919 -0.041601588 -0.0226398034
50%  0.022868021 -0.002644197 -0.001243113 -0.003879536  0.0007558983
75%  0.076598171  0.019565567  0.014546365  0.021611478  0.0188732296
               V7            V8            V9          V10
25% -0.0343970742 -0.0245165870 -1.041947e-02 -0.015684880
50%  0.0005256647 -0.0004555558 -6.069042e-05  0.002130569
75%  0.0503970852  0.0183592449  1.991418e-02  0.020050864

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.620003010  0.218790057  0.064625241 -0.029013527  0.020914139 -0.183929850 
          V8           V9          V10 
 0.009967381 -0.153087143 -0.043636994 

 Quartiles of Marginal Effects:
 
          V2         V3          V4          V5          V6          V7
25% 2.224048 0.05841852 -0.17963468 -0.31326704 -0.17781114 -0.46261834
50% 2.645131 0.23043557  0.02950525 -0.04299217  0.03805527 -0.18030913
75% 3.128164 0.39724886  0.30129189  0.27852453  0.24401194  0.02675714
             V8         V9         V10
25% -0.25234750 -0.5598115 -0.29412330
50% -0.02781345 -0.3642733  0.00160147
75%  0.38149500  0.2937602  0.20202597

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.77088911  0.22207824 -0.12771642 -0.26592978  0.05147713 -0.11701883 
         V8          V9         V10 
-0.25857073 -0.27096135  0.19402645 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5         V6         V7         V8
25% 2.766638 0.2194301 -0.1309342 -0.2691995 0.04895548 -0.1192637 -0.2639883
50% 2.772346 0.2216162 -0.1287679 -0.2671998 0.05147643 -0.1164007 -0.2596118
75% 2.778878 0.2254333 -0.1248987 -0.2627111 0.05454676 -0.1146119 -0.2544740
            V9       V10
25% -0.2748250 0.1918790
50% -0.2719655 0.1944781
75% -0.2660392 0.1979644

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 1.941362951  0.111471709 -0.115510193 -0.344330395  0.044758856 -0.002156922 
          V8           V9          V10 
-0.277938595 -0.228415322  0.260851568 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5         V6           V7         V8
25% 1.940694 0.1110759 -0.1160361 -0.3449603 0.04437977 -0.002734534 -0.2788359
50% 1.941822 0.1113884 -0.1155272 -0.3445960 0.04486541 -0.002067990 -0.2781798
75% 1.942616 0.1118569 -0.1150182 -0.3437553 0.04512928 -0.001576459 -0.2775480
            V9       V10
25% -0.2289329 0.2606956
50% -0.2284551 0.2610486
75% -0.2279355 0.2614585

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 3.41245834  0.37058240 -0.02257124 -0.04121711  0.10255097 -0.23016927 
         V8          V9         V10 
-0.14096562 -0.23093281  0.02782968 

 Quartiles of Marginal Effects:
 
          V2        V3          V4           V5         V6         V7
25% 3.330595 0.3276835 -0.08854669 -0.073458587 0.07658920 -0.2689450
50% 3.414618 0.3688561 -0.02308120 -0.046064403 0.09670538 -0.2187165
75% 3.487677 0.4217163  0.03671190 -0.001813108 0.14793434 -0.1881544
             V8         V9         V10
25% -0.19081912 -0.3186092 -0.03774543
50% -0.14332783 -0.2552687  0.02311280
75% -0.06605906 -0.1425929  0.07944327

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 3.35290044  0.35541720 -0.02924453 -0.06259823  0.09634967 -0.22023086 
         V8          V9         V10 
-0.14998757 -0.23615825  0.04303894 

 Quartiles of Marginal Effects:
 
          V2        V3          V4          V5         V6         V7
25% 3.277861 0.3107640 -0.09436205 -0.10082325 0.06945238 -0.2595319
50% 3.352012 0.3557639 -0.03307532 -0.06875148 0.09358220 -0.2110486
75% 3.435740 0.4062910  0.02384181 -0.02597256 0.13829831 -0.1821228
             V8         V9         V10
25% -0.20648725 -0.3184070 -0.01811820
50% -0.15471311 -0.2653265  0.03828085
75% -0.07499154 -0.1478002  0.09610323

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 3.05157487  0.37081974  0.20056777  0.17847989  0.12541490 -0.23573839 
         V8          V9         V10 
 0.09370812 -0.08497887 -0.15448014 

 Quartiles of Marginal Effects:
 
          V2            V3         V4         V5         V6          V7
25% 2.009185 -6.531783e-05 -0.1192746 -0.2551584 -0.1533717 -0.58733351
50% 3.083659  4.113123e-01  0.1417023  0.1699058  0.1950640 -0.24341740
75% 4.160002  7.658727e-01  0.5378294  0.5300643  0.4079826  0.05091925
            V8         V9        V10
25% -0.4362507 -0.6417387 -0.5765576
50%  0.2776063 -0.2417264 -0.1929359
75%  0.5589869  0.5721976  0.2783854

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.114831316 -0.004277561 -0.017083654 -0.016245274 -0.004478330 -0.017359710 
          V8           V9          V10 
-0.015580227  0.002458775 -0.003605320 

 Quartiles of Marginal Effects:
 
             V2           V3           V4           V5           V6
25% 0.005294063 -0.050367453 -0.078544395 -0.068193040 -0.037324750
50% 0.058599452 -0.005908171 -0.001616168 -0.009452678  0.003199473
75% 0.160246080  0.041907927  0.024580836  0.036114123  0.045708294
              V7           V8           V9         V10
25% -0.065939384 -0.039190850 -0.016157619 -0.02528538
50%  0.001187468 -0.001679558  0.003175117  0.00458865
75%  0.079978978  0.019009727  0.038038565  0.03520517

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.301796197  0.009164686 -0.011144846 -0.098012414  0.027070594  0.035316125 
          V8           V9          V10 
-0.073752991 -0.039107521  0.080660802 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5         V6         V7
25% 0.3016729 0.009074974 -0.01121461 -0.09817536 0.02700817 0.03520133
50% 0.3019239 0.009154237 -0.01114038 -0.09806941 0.02710178 0.03534397
75% 0.3021054 0.009234471 -0.01100619 -0.09788566 0.02715869 0.03547477
             V8          V9        V10
25% -0.07393791 -0.03922245 0.08063400
50% -0.07383302 -0.03911003 0.08074242
75% -0.07366901 -0.03900697 0.08078784

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.579259598  0.155377355 -0.004822364  0.154673113  0.098530804 -0.368019458 
          V8           V9          V10 
 0.107002387 -0.158274117 -0.130859175 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5          V6          V7
25% 1.596817 -0.3459520 -0.5008831 -0.4033372 -0.24000927 -0.87871645
50% 2.869977  0.1952494  0.0030659  0.2390417  0.02635135 -0.34624886
75% 3.789115  0.7081177  0.4241940  0.5765391  0.59305364  0.04943995
            V8         V9        V10
25% -0.4596617 -0.7321015 -0.4982535
50%  0.2076136 -0.3884914 -0.1143458
75%  0.6359114  0.4932692  0.3166413

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.69920626  0.08864767 -0.10217100 -0.33976662  0.04764213  0.02148118 
         V8          V9         V10 
-0.26806024 -0.20694330  0.26028678 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5         V6         V7         V8
25% 1.698567 0.08820611 -0.1026968 -0.3404426 0.04726884 0.02083679 -0.2689616
50% 1.699700 0.08854877 -0.1022327 -0.3400890 0.04772964 0.02158365 -0.2683401
75% 1.700556 0.08905792 -0.1016528 -0.3392147 0.04804483 0.02205849 -0.2676673
            V9       V10
25% -0.2074849 0.2601795
50% -0.2069340 0.2605087
75% -0.2063734 0.2609158

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.67463650  0.22738516  0.03328391 -0.12287200  0.03777328 -0.15866954 
         V8          V9         V10 
-0.09447472 -0.21223658  0.06147801 

 Quartiles of Marginal Effects:
 
          V2        V3           V4          V5          V6           V7
25% 2.411070 0.1369635 -0.136034480 -0.28712370 -0.08191812 -0.316684646
50% 2.666377 0.2058953  0.003854537 -0.19506986  0.06108016 -0.136251538
75% 2.922463 0.3652830  0.209698179  0.04975172  0.17128124  0.001877277
            V8          V9         V10
25% -0.3121434 -0.42345883 -0.05032005
50% -0.1036891 -0.35035994  0.08182434
75%  0.1344913  0.07599245  0.20259664

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.521059254  0.152364939  0.001860901  0.111754547  0.065355387 -0.330912569 
          V8           V9          V10 
 0.098050550 -0.131242115 -0.143680092 

 Quartiles of Marginal Effects:
 
          V2         V3           V4         V5           V6          V7
25% 1.632382 -0.2697846 -0.423250308 -0.3317308 -0.279062272 -0.81049122
50% 2.850747  0.1501063 -0.006295171  0.2021589 -0.001329742 -0.31529888
75% 3.473638  0.6864212  0.447239010  0.4975077  0.497498454  0.07409346
            V8         V9        V10
25% -0.3930148 -0.6974492 -0.5031131
50%  0.1513624 -0.4125029 -0.1199161
75%  0.6285265  0.5010854  0.2454671

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.726715855  0.218882151  0.058149295  0.198676659  0.213039947 -0.403591372 
          V8           V9          V10 
 0.117403092 -0.206582819 -0.007138511 

 Quartiles of Marginal Effects:
 
          V2         V3          V4         V5         V6         V7         V8
25% 1.152881 -0.5187654 -0.57683233 -0.4421281 -0.3399873 -0.9339629 -0.4220957
50% 2.752433  0.1608679  0.08823318  0.4394156  0.1573468 -0.4559099  0.2087307
75% 4.343679  0.9850011  0.52505259  0.7322741  0.7905896  0.2065178  0.7510947
            V9         V10
25% -0.7873783 -0.55182251
50% -0.5162750 -0.04746306
75%  0.5172998  0.43059165

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.70244224  0.02001561  0.02862497 -0.06338177  0.07740561 -0.02838219 
         V8          V9         V10 
 0.06429332 -0.15989988  0.04243056 

 Quartiles of Marginal Effects:
 
           V2          V3            V4          V5          V6          V7
25% 0.2256515 -0.09196201 -0.0957588120 -0.22950877 -0.07613862 -0.20457159
50% 0.5398468  0.01729847 -0.0002386965 -0.06752684  0.06548328 -0.02409520
75% 0.8809101  0.14179202  0.1506147242  0.06699512  0.21168554  0.07793344
             V8          V9         V10
25% -0.09955639 -0.27189560 -0.07311482
50%  0.03705141 -0.05085547  0.02034444
75%  0.20455471  0.06711597  0.23255742

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.69539505  0.18502245  0.01908928 -0.30572516  0.20217131 -0.22281579 
         V8          V9         V10 
 0.19184074 -0.40208740  0.00212573 

 Quartiles of Marginal Effects:
 
          V2           V3          V4         V5          V6          V7
25% 2.275275 -0.003225254 -0.28553290 -0.4997387 -0.03114343 -0.35371622
50% 2.666704  0.165059849  0.01673224 -0.2907588  0.23083691 -0.25956805
75% 3.170659  0.370280941  0.23801747 -0.1349612  0.45996349 -0.09470506
             V8         V9         V10
25% -0.06378687 -0.6736036 -0.14297844
50%  0.15066572 -0.4265325  0.03207487
75%  0.40552274 -0.1162261  0.16605771

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.67564732  0.12107500  0.04368497 -0.09950619  0.13927601 -0.34605706 
         V8          V9         V10 
 0.26790090 -0.13684955  0.21941792 

 Quartiles of Marginal Effects:
 
          V2         V3         V4          V5         V6         V7         V8
25% 1.985013 -0.6639938 -0.6762043 -0.48430943 -0.5320083 -1.0129979 -0.5224365
50% 2.548617  0.1666996  0.1331890 -0.06240791  0.2316105 -0.3680752  0.1814848
75% 3.786998  0.5987378  0.5842690  0.18929676  0.8110665  0.4320139  0.9740170
            V9        V10
25% -0.6958627 -0.2481766
50% -0.2616169  0.3018183
75%  0.6587895  0.7584056

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.08635958  0.05639944  0.12389821 -0.11279919  0.14367173 -0.08859746 
         V8          V9         V10 
 0.01600356 -0.23028013  0.11572614 

 Quartiles of Marginal Effects:
 
          V2         V3        V4         V5        V6          V7         V8
25% 1.084594 0.05576944 0.1233753 -0.1135641 0.1431749 -0.08923139 0.01530569
50% 1.087188 0.05646725 0.1239563 -0.1129149 0.1438509 -0.08872551 0.01611499
75% 1.088276 0.05701525 0.1244595 -0.1121670 0.1443259 -0.08789149 0.01660145
            V9       V10
25% -0.2312237 0.1152443
50% -0.2306182 0.1158187
75% -0.2295537 0.1163997

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.47775633  0.01506164  0.03390639 -0.02504401  0.06032825 -0.01914916 
         V8          V9         V10 
 0.04369638 -0.11559014  0.02066947 

 Quartiles of Marginal Effects:
 
           V2           V3           V4          V5          V6          V7
25% 0.1063292 -0.087038996 -0.055402611 -0.16824642 -0.04739216 -0.15479534
50% 0.3117096  0.006820749  0.008006914 -0.04086397  0.04227561 -0.01786908
75% 0.5969150  0.112113128  0.131096520  0.07818628  0.14191528  0.05891445
              V8          V9          V10
25% -0.075504385 -0.18354524 -0.053742627
50%  0.003711907 -0.03116370  0.004656563
75%  0.141943971  0.06659381  0.169124018

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.92113050  0.20624655  0.01961645 -0.30442138  0.18553754 -0.37598947 
         V8          V9         V10 
 0.23237747 -0.24735807 -0.05920297 

 Quartiles of Marginal Effects:
 
          V2          V3          V4          V5         V6         V7
25% 2.229601 -0.04944488 -0.41162965 -0.58797288 -0.1628281 -0.6063931
50% 2.868295  0.27001177  0.05134785 -0.31694051  0.1818470 -0.3931460
75% 3.574452  0.41681894  0.37917275 -0.05011654  0.6783573 -0.1019868
            V8         V9        V10
25% -0.2294281 -0.6190433 -0.3670335
50%  0.1754361 -0.2411005 -0.1137641
75%  0.8108204  0.1570792  0.2915827

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.079003944  0.015674797  0.014608874  0.011497284  0.005982959 -0.009183805 
          V8           V9          V10 
 0.020226608 -0.030432802 -0.011348436 

 Quartiles of Marginal Effects:
 
             V2            V3           V4            V5            V6
25% 0.000694333 -0.0178230673 -0.008091155 -0.0160363556 -0.0068664280
50% 0.011696316 -0.0003214996  0.001850283 -0.0001390236  0.0008819545
75% 0.063172939  0.0299513654  0.023922618  0.0286314301  0.0243770105
             V7           V8            V9          V10
25% -0.02173914 -0.012386118 -0.0312051193 -0.029502551
50% -0.00185680  0.001701576 -0.0009596585 -0.001080781
75%  0.01000544  0.016650940  0.0061716789  0.018727605

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.42010000  0.11839881  0.01451451 -0.29918674  0.16990142 -0.22179532 
         V8          V9         V10 
 0.13631562 -0.39647533  0.05795319 

 Quartiles of Marginal Effects:
 
          V2         V3           V4         V5         V6         V7
25% 1.889613 -0.1466598 -0.284584704 -0.5189368 -0.1141419 -0.4562629
50% 2.466957  0.0772013  0.007699512 -0.2917695  0.1296071 -0.2486525
75% 3.055601  0.3558717  0.279929216 -0.1211516  0.4840660 -0.1150078
             V8          V9         V10
25% -0.17081727 -0.74174668 -0.12195457
50%  0.06341257 -0.41145354  0.05465877
75%  0.38794130 -0.08958587  0.25394730

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.66236345  0.25830429  0.05094350 -0.23445255  0.26361431 -0.07735133 
         V8          V9         V10 
 0.21160856 -0.40790868  0.04698233 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6          V7        V8
25% 2.656459 0.2550565 0.04560072 -0.2383762 0.2605021 -0.07950248 0.2086793
50% 2.661849 0.2589949 0.05129709 -0.2349207 0.2634041 -0.07758071 0.2112527
75% 2.673685 0.2621010 0.05351477 -0.2312854 0.2674699 -0.07496013 0.2150001
            V9        V10
25% -0.4129210 0.04454349
50% -0.4088016 0.04746070
75% -0.4024905 0.05014208

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.89557822  0.14993641  0.12943019 -0.18568170  0.22142397 -0.10171399 
         V8          V9         V10 
 0.08934424 -0.35364036  0.12638351 

 Quartiles of Marginal Effects:
 
          V2        V3        V4         V5        V6         V7         V8
25% 1.894323 0.1494862 0.1289197 -0.1862810 0.2210233 -0.1021291 0.08887276
50% 1.896047 0.1500073 0.1294760 -0.1856071 0.2214384 -0.1017929 0.08936651
75% 1.896945 0.1504085 0.1298014 -0.1852936 0.2219020 -0.1013018 0.08979406
            V9       V10
25% -0.3543159 0.1260777
50% -0.3539215 0.1264157
75% -0.3530092 0.1269146

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 3.21469609  0.31169113 -0.05474395 -0.26623572  0.26310626 -0.08357513 
         V8          V9         V10 
 0.33078917 -0.38938870 -0.08499358 

 Quartiles of Marginal Effects:
 
          V2        V3          V4         V5        V6          V7        V8
25% 3.070483 0.2623488 -0.15894851 -0.3268964 0.1949432 -0.11313141 0.2596406
50% 3.215706 0.3174052 -0.06570034 -0.2693162 0.2704859 -0.07744660 0.3305273
75% 3.325594 0.3582648  0.02384334 -0.1968957 0.3310414 -0.04859328 0.3837505
            V9          V10
25% -0.4749895 -0.157005193
50% -0.3812776 -0.086376236
75% -0.2839300  0.003410894

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 3.16305493  0.30588900 -0.04333755 -0.26596199  0.26246383 -0.08625584 
         V8          V9         V10 
 0.31931671 -0.39527897 -0.06888240 

 Quartiles of Marginal Effects:
 
          V2        V3          V4         V5        V6          V7        V8
25% 3.014660 0.2585322 -0.13955446 -0.3289472 0.1957654 -0.11493638 0.2521682
50% 3.168769 0.3069888 -0.05209051 -0.2656710 0.2657095 -0.08136295 0.3119611
75% 3.273530 0.3536367  0.03246059 -0.1998936 0.3292547 -0.05240574 0.3718126
            V9         V10
25% -0.4830360 -0.13295533
50% -0.3885568 -0.06901006
75% -0.2935396  0.01327151

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.78421282  0.15772812  0.02063484 -0.27971017  0.16504832 -0.37509518 
         V8          V9         V10 
 0.20185321 -0.22799644  0.01847264 

 Quartiles of Marginal Effects:
 
          V2         V3          V4           V5         V6          V7
25% 2.046672 -0.1300827 -0.40576098 -0.619862135 -0.1873226 -0.71429437
50% 2.754551  0.2193378  0.03160532 -0.253231236  0.1495227 -0.37364204
75% 3.519869  0.4390392  0.44169451 -0.006929894  0.8131064  0.02453689
            V8         V9         V10
25% -0.3548983 -0.6537368 -0.29600189
50%  0.1184227 -0.1812568  0.05233987
75%  0.7079067  0.2677562  0.32514092

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.134423143  0.015842030  0.021223496  0.011566682  0.016154614 -0.009905366 
          V8           V9          V10 
 0.024281222 -0.044202627 -0.010051849 

 Quartiles of Marginal Effects:
 
             V2            V3           V4            V5           V6
25% 0.003168047 -0.0311476519 -0.017653334 -0.0345791088 -0.017021584
50% 0.035267904 -0.0009940221  0.007427286 -0.0009997472  0.004312475
75% 0.135413475  0.0602627472  0.032552399  0.0423274939  0.047448484
              V7           V8          V9           V10
25% -0.041131737 -0.035518151 -0.05147167 -0.0379357458
50% -0.004456379  0.002255557 -0.00298802 -0.0009971688
75%  0.019261177  0.035227115  0.01418408  0.0453504616

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.295826542  0.005845527  0.045830925 -0.031500419  0.042965985 -0.032472303 
          V8           V9          V10 
-0.003230238 -0.067753148  0.039883425 

 Quartiles of Marginal Effects:
 
           V2          V3         V4          V5         V6          V7
25% 0.2955493 0.005760509 0.04577386 -0.03159827 0.04288559 -0.03256625
50% 0.2959506 0.005842878 0.04583887 -0.03153006 0.04298739 -0.03250578
75% 0.2960857 0.005929605 0.04593220 -0.03141678 0.04306224 -0.03236164
              V8          V9        V10
25% -0.003335251 -0.06788589 0.03980403
50% -0.003218212 -0.06780044 0.03989856
75% -0.003147434 -0.06764154 0.03997299

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.41036018 -0.01271086 -0.03958459 -0.27946002  0.04919765 -0.13174344 
         V8          V9         V10 
 0.25630698 -0.31444268  0.11959340 

 Quartiles of Marginal Effects:
 
          V2          V3          V4           V5          V6         V7
25% 1.597200 -0.49353358 -0.49291946 -0.594767492 -0.68158080 -0.5618282
50% 2.444961  0.03566362 -0.01090562 -0.244539688 -0.04919787 -0.2459900
75% 3.161797  0.42339716  0.37788114 -0.002199565  0.66783132  0.3354389
            V8         V9        V10
25% -0.2620305 -0.8116255 -0.1038432
50%  0.3061387 -0.3136311  0.1401370
75%  0.6249436  0.2595347  0.4164456

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.66440669  0.11992574  0.13630335 -0.16660475  0.20223975 -0.10243328 
         V8          V9         V10 
 0.06241103 -0.32418510  0.13194400 

 Quartiles of Marginal Effects:
 
          V2        V3        V4         V5        V6         V7         V8
25% 1.663124 0.1194616 0.1357486 -0.1672148 0.2018400 -0.1029007 0.06191002
50% 1.664952 0.1200183 0.1363671 -0.1666033 0.2022959 -0.1025153 0.06243296
75% 1.665861 0.1203853 0.1366359 -0.1661882 0.2026999 -0.1019676 0.06285161
            V9       V10
25% -0.3248719 0.1316121
50% -0.3244866 0.1319991
75% -0.3235664 0.1324753

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.51776354  0.18322840  0.04506775 -0.27926548  0.20925323 -0.18520087 
         V8          V9         V10 
 0.16688863 -0.42354969  0.05398344 

 Quartiles of Marginal Effects:
 
          V2          V3          V4         V5         V6          V7
25% 2.218755 0.009270587 -0.18136925 -0.4554388 0.01967892 -0.31126731
50% 2.439711 0.159742789  0.05492603 -0.2742505 0.19066137 -0.23092945
75% 3.041825 0.330013878  0.22740641 -0.1454436 0.41317579 -0.07574384
             V8         V9         V10
25% -0.03954718 -0.6727324 -0.03736508
50%  0.14332390 -0.4064507  0.05794650
75%  0.32975748 -0.1447218  0.20713761

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.343871012 -0.008494192 -0.048048328 -0.298248105  0.066115461 -0.144576545 
          V8           V9          V10 
 0.211483568 -0.332452629  0.090821665 

 Quartiles of Marginal Effects:
 
          V2          V3          V4          V5           V6         V7
25% 1.486461 -0.41968340 -0.44310744 -0.59744810 -0.502869466 -0.4886757
50% 2.377809  0.00833541 -0.04879708 -0.28477266  0.006874949 -0.1331382
75% 3.041646  0.40593897  0.36329902 -0.04484169  0.648998896  0.2779195
            V8         V9        V10
25% -0.2611703 -0.7965158 -0.1385449
50%  0.2522690 -0.3657728  0.1383142
75%  0.5842271  0.2672436  0.3358426

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.57318312  0.04407209  0.02953802 -0.18981776  0.09857744 -0.20752945 
         V8          V9         V10 
 0.31037841 -0.23982692  0.23162160 

 Quartiles of Marginal Effects:
 
          V2         V3          V4         V5         V6         V7         V8
25% 1.857265 -0.7002737 -0.53529252 -0.5544297 -0.7374659 -0.8666441 -0.3179470
50% 2.479794  0.1445431  0.06968113 -0.1959972  0.1222762 -0.2732282  0.1696548
75% 3.391783  0.5262206  0.48046999  0.1348323  0.7466466  0.4909301  0.8560139
            V9        V10
25% -0.8043586 -0.1707428
50% -0.3183542  0.2839085
75%  0.5287791  0.7005587

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 3.16124752  0.32663007 -0.03558886 -0.11262023  0.15585278 -0.06794249 
         V8          V9         V10 
-0.08318078 -0.18391613 -0.08415792 

 Quartiles of Marginal Effects:
 
          V2        V3          V4          V5        V6          V7
25% 3.064171 0.2689823 -0.11187218 -0.19045185 0.1064895 -0.10420513
50% 3.155262 0.3351950 -0.03867711 -0.12189917 0.1596992 -0.06794453
75% 3.266051 0.3886935  0.03540271 -0.04059727 0.2031646 -0.03487586
              V8          V9         V10
25% -0.143634040 -0.28107402 -0.12211433
50% -0.083970217 -0.20846528 -0.08641319
75% -0.003646626 -0.06892912 -0.03121290
Radial Basis Function Kernel Regularized Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  lambda        sigma        RMSE       Rsquared    MAE        Selected
  3.398331e-05     3.252680  0.6486671  0.29772970  0.3619101          
  4.908678e-05   970.340184  0.4549496  0.72096878  0.3237744          
  6.869763e-05  4162.791485  0.3700679  0.79731253  0.2787780  *       
  1.388171e-04   155.616545  0.5925786  0.59363631  0.4179193          
  1.700434e-04    63.079210  0.5634533  0.61327054  0.3915227          
  4.205922e-04    23.492104  0.5075876  0.63703850  0.3305372          
  5.295414e-04     1.585900  0.7092630  0.10874420  0.4183301          
  1.226861e-03   134.732467  0.4858377  0.68878110  0.3374996          
  1.706586e-03   847.306707  0.3713697  0.79441984  0.2759696          
  4.649139e-03    18.923428  0.4978052  0.63522279  0.3172619          
  6.163082e-03     1.309916  0.7157387  0.06970572  0.4271451          
  1.139410e-02  6033.883856  0.4494227  0.74589907  0.2510284          
  1.227144e-02  1678.590965  0.3838718  0.77656580  0.2436154          
  1.847174e-02  4997.997663  0.4759529  0.73619231  0.2595475          
  2.146598e-02     2.688397  0.6713510  0.24638207  0.3790738          
  6.302247e-02    42.673624  0.4275567  0.73248846  0.2859953          
  8.405619e-02  2332.719750  0.5519256  0.71207203  0.3053431          
  1.012562e-01    21.919521  0.4626005  0.67734920  0.2878212          
  1.791958e-01    32.151738  0.4354084  0.71585596  0.2705944          
  2.364973e-01  4295.595018  0.6739835  0.67993330  0.3996448          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 6.869763e-05 and sigma
 = 4162.791.
[1] "Sat Mar 10 01:19:25 2018"
Least Angle Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  fraction   RMSE       Rsquared   MAE        Selected
  0.1062531  0.6355970  0.8310144  0.3679634          
  0.1381929  0.6094656  0.8310144  0.3461581          
  0.1673884  0.5859263  0.8310144  0.3270347          
  0.2284886  0.5379772  0.8310144  0.2886015          
  0.2461119  0.5245442  0.8310144  0.2779150          
  0.3247722  0.4674719  0.8310144  0.2326911          
  0.3447800  0.4538884  0.8310144  0.2229913          
  0.4177590  0.4087538  0.8310144  0.1991246          
  0.4464257  0.3933767  0.8310144  0.1956063          
  0.5334745  0.3574852  0.8310144  0.2114160          
  0.5579596  0.3507893  0.8310144  0.2186050          
  0.6113360  0.3457107  0.8294017  0.2321382          
  0.6177791  0.3454969  0.8294346  0.2332378  *       
  0.6533015  0.3458220  0.8281466  0.2402475          
  0.6663501  0.3461065  0.8268480  0.2423087          
  0.7598991  0.3489209  0.8175794  0.2538558          
  0.7849139  0.3502561  0.8148880  0.2565198          
  0.8010843  0.3510251  0.8138487  0.2579051          
  0.8506656  0.3543650  0.8101851  0.2649146          
  0.8747652  0.3563319  0.8081440  0.2683254          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.6177791.
[1] "Sat Mar 10 01:19:41 2018"
Least Angle Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  step  RMSE       Rsquared   MAE        Selected
  1     0.7247539        NaN  0.4432742          
  2     0.3533114  0.8310144  0.2369009          
  3     0.3408252  0.8183568  0.2475539          
  4     0.3392679  0.8205651  0.2522875  *       
  5     0.3443154  0.8168992  0.2582367          
  6     0.3443610  0.8150244  0.2614346          
  7     0.3550558  0.8061125  0.2694734          
  8     0.3561326  0.8057387  0.2710919          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was step = 4.
[1] "Sat Mar 10 01:19:58 2018"
The lasso 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  fraction   RMSE       Rsquared   MAE        Selected
  0.1062531  0.6355970  0.8310144  0.3679634          
  0.1381929  0.6094656  0.8310144  0.3461581          
  0.1673884  0.5859263  0.8310144  0.3270347          
  0.2284886  0.5379772  0.8310144  0.2886015          
  0.2461119  0.5245442  0.8310144  0.2779150          
  0.3247722  0.4674719  0.8310144  0.2326911          
  0.3447800  0.4538884  0.8310144  0.2229913          
  0.4177590  0.4087538  0.8310144  0.1991246          
  0.4464257  0.3933767  0.8310144  0.1956063          
  0.5334745  0.3574852  0.8310144  0.2114160          
  0.5579596  0.3507893  0.8310144  0.2186050          
  0.6113360  0.3457107  0.8294017  0.2321382          
  0.6177791  0.3454969  0.8294346  0.2332378  *       
  0.6533015  0.3458220  0.8281466  0.2402475          
  0.6663501  0.3461065  0.8268480  0.2423087          
  0.7598991  0.3489209  0.8175794  0.2538558          
  0.7849139  0.3502561  0.8148880  0.2565198          
  0.8010843  0.3510251  0.8138487  0.2579051          
  0.8506656  0.3543650  0.8101851  0.2649146          
  0.8747652  0.3563319  0.8081440  0.2683254          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.6177791.
[1] "Sat Mar 10 01:20:14 2018"
Linear Regression with Backwards Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  nvmax  RMSE       Rsquared   MAE        Selected
  2      0.3438288  0.8169833  0.2621227  *       
  3      0.3580872  0.8020211  0.2766690          
  4      0.3605180  0.7992151  0.2748725          
  5      0.3644284  0.7996125  0.2782262          
  6      0.3670355  0.7993147  0.2823090          
  7      0.3660829  0.7989533  0.2815910          
  8      0.3649292  0.7975674  0.2790882          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sat Mar 10 01:20:30 2018"
Linear Regression with Forward Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  nvmax  RMSE       Rsquared   MAE        Selected
  2      0.3438288  0.8169833  0.2621227  *       
  3      0.3580872  0.8020211  0.2766690          
  4      0.3605180  0.7992151  0.2748725          
  5      0.3644284  0.7996125  0.2782262          
  6      0.3670355  0.7993147  0.2823090          
  7      0.3660829  0.7989533  0.2815910          
  8      0.3649292  0.7975674  0.2790882          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sat Mar 10 01:20:46 2018"
Linear Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE       Rsquared   MAE      
  0.3655383  0.7970557  0.2797667

Tuning parameter 'intercept' was held constant at a value of TRUE
[1] "Sat Mar 10 01:21:02 2018"
Start:  AIC=-128.46
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq     RSS      AIC
- V9    1    0.0000  2.5675 -130.455
- V4    1    0.0002  2.5677 -130.451
- V7    1    0.0017  2.5691 -130.424
- V10   1    0.0049  2.5723 -130.361
- V5    1    0.0111  2.5785 -130.240
- V6    1    0.0261  2.5936 -129.949
- V3    1    0.0759  2.6433 -128.999
<none>               2.5674 -128.456
- V8    1    0.3953  2.9628 -123.295
- V2    1   12.0030 14.5704  -43.652

Step:  AIC=-130.46
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V10

       Df Sum of Sq     RSS     AIC
- V4    1    0.0002  2.5677 -132.45
- V7    1    0.0018  2.5693 -132.42
- V10   1    0.0051  2.5725 -132.36
- V5    1    0.0114  2.5789 -132.23
- V6    1    0.0268  2.5943 -131.94
- V3    1    0.0760  2.6435 -131.00
<none>               2.5675 -130.46
- V8    1    0.4097  2.9771 -125.05
- V2    1   12.0063 14.5738  -45.64

Step:  AIC=-132.45
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V10

       Df Sum of Sq     RSS      AIC
- V7    1    0.0019  2.5696 -134.414
- V10   1    0.0052  2.5729 -134.350
- V5    1    0.0113  2.5790 -134.231
- V6    1    0.0288  2.5965 -133.892
- V3    1    0.0758  2.6435 -132.996
<none>               2.5677 -132.451
- V8    1    0.4101  2.9778 -127.042
- V2    1   12.3775 14.9452  -46.382

Step:  AIC=-134.41
.outcome ~ V2 + V3 + V5 + V6 + V8 + V10

       Df Sum of Sq     RSS      AIC
- V10   1    0.0041  2.5737 -136.333
- V5    1    0.0126  2.5822 -136.169
- V6    1    0.0280  2.5976 -135.871
- V3    1    0.0739  2.6435 -134.996
<none>               2.5696 -134.414
- V8    1    0.4103  2.9799 -129.007
- V2    1   12.3884 14.9580  -48.339

Step:  AIC=-136.33
.outcome ~ V2 + V3 + V5 + V6 + V8

       Df Sum of Sq     RSS      AIC
- V5    1    0.0153  2.5890 -138.038
- V6    1    0.0260  2.5998 -137.830
- V3    1    0.0698  2.6435 -136.996
<none>               2.5737 -136.333
- V8    1    0.4063  2.9801 -131.004
- V2    1   13.4976 16.0713  -46.749

Step:  AIC=-138.04
.outcome ~ V2 + V3 + V6 + V8

       Df Sum of Sq     RSS      AIC
- V6    1    0.0195  2.6084 -139.664
- V3    1    0.0856  2.6746 -138.411
<none>               2.5890 -138.038
- V8    1    0.4033  2.9923 -132.799
- V2    1   13.5028 16.0917  -48.686

Step:  AIC=-139.66
.outcome ~ V2 + V3 + V8

       Df Sum of Sq     RSS      AIC
- V3    1    0.0885  2.6970 -139.994
<none>               2.6084 -139.664
- V8    1    0.4400  3.0485 -133.869
- V2    1   13.4985 16.1069  -50.639

Step:  AIC=-139.99
.outcome ~ V2 + V8

       Df Sum of Sq     RSS      AIC
<none>               2.6970 -139.994
- V8    1    0.5686  3.2656 -132.429
- V2    1   14.6266 17.3235  -48.998
Start:  AIC=-91.82
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq     RSS     AIC
- V10   1    0.0048  5.3465 -93.779
- V5    1    0.0101  5.3518 -93.729
- V4    1    0.0152  5.3568 -93.682
- V6    1    0.0198  5.3615 -93.639
- V7    1    0.0559  5.3976 -93.304
- V8    1    0.0629  5.4046 -93.239
- V9    1    0.1570  5.4987 -92.376
<none>               5.3417 -91.824
- V3    1    0.2729  5.6145 -91.333
- V2    1   21.3573 26.6990 -13.370

Step:  AIC=-93.78
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9

       Df Sum of Sq     RSS     AIC
- V5    1    0.0105  5.3570 -95.681
- V4    1    0.0143  5.3608 -95.646
- V6    1    0.0223  5.3688 -95.571
- V7    1    0.0533  5.3998 -95.283
- V8    1    0.0684  5.4149 -95.143
- V9    1    0.1525  5.4990 -94.373
<none>               5.3465 -93.779
- V3    1    0.3204  5.6669 -92.869
- V2    1   22.5511 27.8976 -13.174

Step:  AIC=-95.68
.outcome ~ V2 + V3 + V4 + V6 + V7 + V8 + V9

       Df Sum of Sq     RSS     AIC
- V4    1    0.0097  5.3666 -97.591
- V6    1    0.0332  5.3902 -97.372
- V7    1    0.0574  5.4144 -97.148
- V8    1    0.0618  5.4188 -97.107
- V9    1    0.1526  5.5096 -96.276
<none>               5.3570 -95.681
- V3    1    0.3267  5.6837 -94.721
- V2    1   25.7623 31.1193  -9.710

Step:  AIC=-97.59
.outcome ~ V2 + V3 + V6 + V7 + V8 + V9

       Df Sum of Sq     RSS     AIC
- V6    1    0.0277  5.3943 -99.334
- V7    1    0.0576  5.4242 -99.057
- V8    1    0.0631  5.4297 -99.007
- V9    1    0.1511  5.5177 -98.203
<none>               5.3666 -97.591
- V3    1    0.3737  5.7404 -96.225
- V2    1   25.7962 31.1629 -11.640

Step:  AIC=-99.33
.outcome ~ V2 + V3 + V7 + V8 + V9

       Df Sum of Sq     RSS      AIC
- V7    1    0.0401  5.4345 -100.963
- V8    1    0.0722  5.4665 -100.669
- V9    1    0.1638  5.5582  -99.838
<none>               5.3943  -99.334
- V3    1    0.3659  5.7602  -98.052
- V2    1   25.8535 31.2478  -13.504

Step:  AIC=-100.96
.outcome ~ V2 + V3 + V8 + V9

       Df Sum of Sq     RSS      AIC
- V8    1    0.0575  5.4920 -102.437
- V9    1    0.2079  5.6423 -101.086
<none>               5.4345 -100.963
- V3    1    0.3658  5.8002  -99.706
- V2    1   26.0101 31.4446  -15.190

Step:  AIC=-102.44
.outcome ~ V2 + V3 + V9

       Df Sum of Sq    RSS      AIC
- V9    1    0.2143  5.706 -102.523
<none>               5.492 -102.437
- V3    1    0.3843  5.876 -101.055
- V2    1   27.5651 33.057  -14.689

Step:  AIC=-102.52
.outcome ~ V2 + V3

       Df Sum of Sq    RSS      AIC
<none>               5.706 -102.523
- V3    1    0.4358  6.142 -100.843
- V2    1   27.8522 33.559  -15.937
Start:  AIC=-97.64
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq     RSS     AIC
- V7    1    0.0015  5.4150 -99.627
- V10   1    0.0105  5.4241 -99.540
- V4    1    0.0122  5.4257 -99.525
- V5    1    0.0894  5.5030 -98.790
- V6    1    0.1020  5.5156 -98.671
- V8    1    0.1386  5.5522 -98.327
<none>               5.4136 -97.642
- V3    1    0.2621  5.6757 -97.183
- V9    1    0.2667  5.6802 -97.141
- V2    1   18.7824 24.1959 -21.783

Step:  AIC=-99.63
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V9 + V10

       Df Sum of Sq     RSS      AIC
- V10   1    0.0096  5.4246 -101.536
- V4    1    0.0115  5.4266 -101.517
- V5    1    0.0886  5.5037 -100.783
- V6    1    0.1010  5.5161 -100.666
- V8    1    0.1590  5.5741 -100.122
<none>               5.4150  -99.627
- V3    1    0.2626  5.6776  -99.165
- V9    1    0.2674  5.6824  -99.121
- V2    1   18.9905 24.4056  -23.335

Step:  AIC=-101.54
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V9

       Df Sum of Sq     RSS      AIC
- V4    1    0.0060  5.4306 -103.478
- V5    1    0.0872  5.5118 -102.706
- V6    1    0.0981  5.5226 -102.604
- V8    1    0.1801  5.6047 -101.838
<none>               5.4246 -101.536
- V3    1    0.2645  5.6891 -101.060
- V9    1    0.3129  5.7375 -100.620
- V2    1   20.0564 25.4810  -23.092

Step:  AIC=-103.48
.outcome ~ V2 + V3 + V5 + V6 + V8 + V9

       Df Sum of Sq     RSS      AIC
- V5    1    0.0841  5.5147 -104.679
- V6    1    0.0961  5.5267 -104.566
- V8    1    0.1758  5.6063 -103.822
<none>               5.4306 -103.478
- V3    1    0.3046  5.7352 -102.640
- V9    1    0.3197  5.7503 -102.504
- V2    1   20.7460 26.1766  -23.692

Step:  AIC=-104.68
.outcome ~ V2 + V3 + V6 + V8 + V9

       Df Sum of Sq     RSS      AIC
- V6    1    0.0865  5.6012 -105.870
- V8    1    0.2050  5.7197 -104.781
<none>               5.5147 -104.679
- V9    1    0.2582  5.7728 -104.300
- V3    1    0.3088  5.8235 -103.846
- V2    1   21.2768 26.7915  -24.484

Step:  AIC=-105.87
.outcome ~ V2 + V3 + V8 + V9

       Df Sum of Sq     RSS      AIC
- V9    1    0.2164  5.8175 -105.899
<none>               5.6012 -105.870
- V8    1    0.2236  5.8248 -105.834
- V3    1    0.2833  5.8845 -105.304
- V2    1   21.8352 27.4363  -25.248

Step:  AIC=-105.9
.outcome ~ V2 + V3 + V8

       Df Sum of Sq     RSS      AIC
- V8    1    0.1833  6.0008 -106.286
<none>               5.8175 -105.899
- V3    1    0.2770  6.0945 -105.481
- V2    1   22.9071 28.7246  -24.861

Step:  AIC=-106.29
.outcome ~ V2 + V3

       Df Sum of Sq     RSS      AIC
- V3    1    0.2103  6.2111 -106.495
<none>               6.0008 -106.286
- V2    1   22.7285 28.7293  -26.853

Step:  AIC=-106.49
.outcome ~ V2

       Df Sum of Sq     RSS      AIC
<none>               6.2111 -106.495
- V2    1    22.519 28.7305  -28.851
Start:  AIC=-158.65
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS      AIC
- V10   1    0.0116  7.255 -160.531
- V7    1    0.0145  7.258 -160.500
- V4    1    0.0173  7.260 -160.471
- V5    1    0.0309  7.274 -160.329
- V8    1    0.0372  7.280 -160.263
- V6    1    0.0717  7.315 -159.904
- V9    1    0.1483  7.391 -159.113
<none>               7.243 -158.653
- V3    1    0.4146  7.658 -156.422
- V2    1   28.8630 36.106  -38.565

Step:  AIC=-160.53
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9

       Df Sum of Sq    RSS      AIC
- V7    1    0.0119  7.267 -162.407
- V4    1    0.0147  7.269 -162.377
- V5    1    0.0285  7.283 -162.233
- V8    1    0.0312  7.286 -162.206
- V6    1    0.0653  7.320 -161.850
- V9    1    0.1719  7.427 -160.752
<none>               7.255 -160.531
- V3    1    0.4030  7.658 -158.422
- V2    1   30.2074 37.462  -37.763

Step:  AIC=-162.41
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V9

       Df Sum of Sq    RSS      AIC
- V4    1    0.0136  7.280 -164.264
- V8    1    0.0256  7.292 -164.139
- V5    1    0.0278  7.294 -164.117
- V6    1    0.0584  7.325 -163.798
- V9    1    0.1855  7.452 -162.490
<none>               7.267 -162.407
- V3    1    0.3937  7.660 -160.396
- V2    1   30.3069 37.573  -39.537

Step:  AIC=-164.26
.outcome ~ V2 + V3 + V5 + V6 + V8 + V9

       Df Sum of Sq    RSS      AIC
- V5    1    0.0240  7.304 -166.014
- V8    1    0.0279  7.308 -165.974
- V6    1    0.0516  7.332 -165.728
- V9    1    0.1851  7.465 -164.356
<none>               7.280 -164.264
- V3    1    0.4366  7.717 -161.838
- V2    1   30.4524 37.732  -41.216

Step:  AIC=-166.01
.outcome ~ V2 + V3 + V6 + V8 + V9

       Df Sum of Sq    RSS      AIC
- V8    1    0.0243  7.328 -167.762
- V6    1    0.0668  7.371 -167.322
- V9    1    0.1737  7.478 -166.227
<none>               7.304 -166.014
- V3    1    0.4286  7.733 -163.681
- V2    1   31.3475 38.652  -41.387

Step:  AIC=-167.76
.outcome ~ V2 + V3 + V6 + V9

       Df Sum of Sq    RSS      AIC
- V6    1     0.074  7.402 -169.000
- V9    1     0.185  7.514 -167.863
<none>               7.328 -167.762
- V3    1     0.468  7.797 -165.053
- V2    1    32.794 40.123  -40.548

Step:  AIC=-169
.outcome ~ V2 + V3 + V9

       Df Sum of Sq    RSS      AIC
- V9    1     0.184  7.587 -169.130
<none>               7.402 -169.000
- V3    1     0.467  7.869 -166.354
- V2    1    32.962 40.364  -42.092

Step:  AIC=-169.13
.outcome ~ V2 + V3

       Df Sum of Sq    RSS      AIC
<none>               7.587 -169.130
- V3    1     0.476  8.063 -166.506
- V2    1    33.567 41.154  -42.619
Linear Regression with Stepwise Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE       Rsquared   MAE      
  0.3503224  0.8106039  0.2699693

[1] "Sat Mar 10 01:21:18 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:22:00 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "logreg"                  
Error : package RWeka is required
Error : package RWeka is required
Error : package RWeka is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:22:16 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "M5"                      
Error : package RWeka is required
Error : package RWeka is required
Error : package RWeka is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:22:31 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "M5Rules"                 
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:22:48 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "mlpKerasDecay"           
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:23:03 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "mlpKerasDropout"         
Multi-Step Adaptive MCP-Net 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  alphas     nsteps  scale      RMSE       Rsquared   MAE        Selected
  0.1456278   9      1.5289136  0.3395445  0.8310144  0.2690811  *       
  0.1743736   4      1.8282158  0.3396180  0.8310144  0.2692986          
  0.2006495   2      0.9967927  0.3396672  0.8310144  0.2694430          
  0.2556397   6      3.1845993  0.3397380  0.8310144  0.2696495          
  0.2715008   6      0.2946906  0.3397532  0.8310144  0.2696936          
  0.3422950   7      0.3474640  0.3398040  0.8310144  0.2698405          
  0.3603020  10      1.1830360  0.3398138  0.8310144  0.2698687          
  0.4259831   6      2.7611275  0.3398425  0.8310144  0.2699514          
  0.4517831   4      3.0401748  0.3398516  0.8310144  0.2699773          
  0.5301271   8      2.9723633  0.3398736  0.8310144  0.2700405          
  0.5521636  10      1.6584653  0.3398787  0.8310144  0.2700550          
  0.6002024   2      0.3900431  0.3398885  0.8310144  0.2700831          
  0.6060012   3      3.8186972  0.3398896  0.8310144  0.2700861          
  0.6379714   2      0.8594890  0.3398952  0.8310144  0.2701021          
  0.6497151  10      1.6272731  0.3398971  0.8310144  0.2701076          
  0.7339092   7      2.5108996  0.3399091  0.8310144  0.2701418          
  0.7564225   3      1.7085173  0.3399119  0.8310144  0.2701496          
  0.7709759   7      3.1745240  0.3399136  0.8310144  0.2701544          
  0.8155990   7      1.5821910  0.3399184  0.8310144  0.2701682          
  0.8372887   2      2.0561542  0.3399206  0.8310144  0.2701743          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alphas = 0.1456278, nsteps = 9
 and scale = 1.528914.
[1] "Sat Mar 10 01:23:26 2018"
# weights:  34
initial  value 50.810563 
iter  10 value 24.266902
iter  20 value 24.253129
final  value 24.253128 
converged
# weights:  177
initial  value 26.203893 
iter  10 value 20.188290
iter  20 value 18.172925
iter  30 value 16.205288
iter  40 value 15.744640
iter  50 value 15.643528
iter  60 value 15.610044
iter  70 value 15.607683
iter  80 value 15.607195
iter  90 value 15.606531
iter 100 value 15.606481
final  value 15.606481 
stopped after 100 iterations
# weights:  56
initial  value 37.583746 
iter  10 value 20.167598
iter  20 value 18.018550
iter  30 value 13.503804
iter  40 value 13.335765
iter  50 value 13.301435
iter  60 value 13.267779
iter  70 value 13.209851
iter  80 value 13.172978
iter  90 value 13.150410
iter 100 value 13.135890
final  value 13.135890 
stopped after 100 iterations
# weights:  177
initial  value 28.185263 
iter  10 value 20.057541
iter  20 value 19.987157
iter  30 value 13.658513
iter  40 value 12.380225
iter  50 value 12.247153
iter  60 value 12.219290
iter  70 value 12.198178
iter  80 value 12.191187
iter  90 value 12.184932
iter 100 value 12.180726
final  value 12.180726 
stopped after 100 iterations
# weights:  155
initial  value 82.762337 
iter  10 value 23.035812
iter  20 value 23.027394
final  value 23.027394 
converged
# weights:  34
initial  value 26.721630 
iter  10 value 20.064190
iter  20 value 20.063954
iter  30 value 20.063712
iter  40 value 20.063447
iter  50 value 20.063145
iter  60 value 20.062785
iter  70 value 20.062337
iter  80 value 20.061746
iter  90 value 20.060911
iter 100 value 20.059612
final  value 20.059612 
stopped after 100 iterations
# weights:  133
initial  value 166.154116 
iter  10 value 25.745171
iter  20 value 25.711038
final  value 25.710959 
converged
# weights:  188
initial  value 49.045974 
iter  10 value 20.483108
iter  20 value 20.354288
final  value 20.354283 
converged
# weights:  144
initial  value 40.056143 
iter  10 value 20.062605
iter  20 value 20.056973
iter  30 value 13.290904
iter  40 value 12.436579
iter  50 value 12.384382
iter  60 value 12.353720
iter  70 value 12.314447
iter  80 value 12.274888
iter  90 value 12.250486
iter 100 value 12.232110
final  value 12.232110 
stopped after 100 iterations
# weights:  144
initial  value 41.057147 
iter  10 value 20.056380
iter  20 value 20.055284
iter  30 value 20.055233
iter  40 value 20.055176
iter  50 value 20.055110
iter  60 value 20.055033
iter  70 value 20.054944
iter  80 value 20.054839
iter  90 value 20.054711
iter 100 value 20.054556
final  value 20.054556 
stopped after 100 iterations
# weights:  45
initial  value 36.731106 
iter  10 value 20.057318
iter  20 value 20.049739
iter  30 value 20.044614
iter  40 value 20.030940
iter  50 value 19.985141
iter  60 value 19.587261
iter  70 value 12.924237
iter  80 value 12.296014
iter  90 value 12.194185
iter 100 value 12.181867
final  value 12.181867 
stopped after 100 iterations
# weights:  100
initial  value 56.338507 
iter  10 value 20.068830
iter  20 value 20.034249
iter  30 value 19.407795
iter  40 value 13.628792
iter  50 value 12.631179
iter  60 value 12.433655
iter  70 value 12.353584
iter  80 value 12.336828
iter  90 value 12.313817
iter 100 value 12.308088
final  value 12.308088 
stopped after 100 iterations
# weights:  100
initial  value 24.627259 
iter  10 value 20.139626
iter  20 value 18.319418
iter  30 value 13.696919
iter  40 value 13.540097
iter  50 value 13.344263
iter  60 value 13.255647
iter  70 value 13.224873
iter  80 value 13.208236
iter  90 value 13.203604
iter 100 value 13.202453
final  value 13.202453 
stopped after 100 iterations
# weights:  78
initial  value 102.929131 
iter  10 value 25.946106
iter  20 value 25.919552
final  value 25.919548 
converged
# weights:  199
initial  value 21.723826 
iter  10 value 20.056107
iter  20 value 20.055191
iter  30 value 20.055081
iter  40 value 20.054947
iter  50 value 20.054782
iter  60 value 20.054573
iter  70 value 20.054299
iter  80 value 20.053928
iter  90 value 20.053394
iter 100 value 20.052568
final  value 20.052568 
stopped after 100 iterations
# weights:  78
initial  value 36.607936 
iter  10 value 20.468906
iter  20 value 19.255227
iter  30 value 19.173805
iter  40 value 18.578632
iter  50 value 18.409922
iter  60 value 18.389790
iter  70 value 18.277026
iter  80 value 18.275386
final  value 18.275339 
converged
# weights:  155
initial  value 37.145597 
iter  10 value 20.056638
iter  20 value 20.052968
iter  30 value 20.044625
iter  40 value 20.026340
iter  50 value 19.097160
iter  60 value 12.604727
iter  70 value 12.229860
iter  80 value 12.183014
iter  90 value 12.173187
iter 100 value 12.167129
final  value 12.167129 
stopped after 100 iterations
# weights:  199
initial  value 33.446227 
iter  10 value 20.283457
iter  20 value 20.124210
iter  30 value 18.160524
iter  40 value 16.914602
iter  50 value 16.687653
iter  60 value 16.649508
iter  70 value 16.632074
iter  80 value 16.624679
iter  90 value 16.621023
iter 100 value 16.617443
final  value 16.617443 
stopped after 100 iterations
# weights:  122
initial  value 34.081346 
iter  10 value 20.519420
final  value 20.488643 
converged
# weights:  56
initial  value 43.319087 
iter  10 value 20.539281
iter  20 value 19.793950
iter  30 value 15.400003
iter  40 value 14.773567
iter  50 value 14.648233
iter  60 value 14.597096
iter  70 value 14.572284
iter  80 value 14.560089
iter  90 value 14.557947
iter 100 value 14.557771
final  value 14.557771 
stopped after 100 iterations
# weights:  34
initial  value 70.365158 
iter  10 value 38.245033
iter  20 value 38.220443
iter  20 value 38.220443
iter  20 value 38.220443
final  value 38.220443 
converged
# weights:  177
initial  value 59.266502 
iter  10 value 33.941604
iter  20 value 30.604336
iter  30 value 27.811950
iter  40 value 26.887748
iter  50 value 26.742505
iter  60 value 26.706843
iter  70 value 26.699760
iter  80 value 26.698401
iter  90 value 26.697757
iter 100 value 26.697688
final  value 26.697688 
stopped after 100 iterations
# weights:  56
initial  value 47.386682 
iter  10 value 33.838971
iter  20 value 31.936609
iter  30 value 26.057562
iter  40 value 24.866713
iter  50 value 24.653376
iter  60 value 24.336770
iter  70 value 24.166602
iter  80 value 24.092721
iter  90 value 24.052548
iter 100 value 24.040948
final  value 24.040948 
stopped after 100 iterations
# weights:  177
initial  value 51.235389 
iter  10 value 33.700650
iter  20 value 26.639778
iter  30 value 23.591519
iter  40 value 23.405710
iter  50 value 23.094443
iter  60 value 23.065027
iter  70 value 23.055148
iter  80 value 23.050532
iter  90 value 23.048897
iter 100 value 23.047690
final  value 23.047690 
stopped after 100 iterations
# weights:  155
initial  value 97.588444 
iter  10 value 36.947159
iter  20 value 36.925062
iter  20 value 36.925062
iter  20 value 36.925062
final  value 36.925062 
converged
# weights:  34
initial  value 42.984908 
iter  10 value 33.707802
iter  20 value 33.687860
iter  30 value 33.656494
iter  40 value 33.463618
iter  50 value 24.413928
iter  60 value 23.736793
iter  70 value 23.613472
iter  80 value 23.295633
iter  90 value 23.201883
iter 100 value 23.185410
final  value 23.185410 
stopped after 100 iterations
# weights:  133
initial  value 195.399433 
iter  10 value 39.822497
iter  20 value 39.787750
final  value 39.787748 
converged
# weights:  188
initial  value 43.691664 
iter  10 value 34.050139
iter  20 value 34.022533
iter  30 value 34.022060
iter  40 value 33.879728
iter  50 value 32.592020
iter  60 value 31.910397
iter  70 value 30.469313
iter  80 value 30.184738
iter  90 value 30.120759
iter 100 value 30.108977
final  value 30.108977 
stopped after 100 iterations
# weights:  144
initial  value 45.319933 
iter  10 value 33.700483
iter  20 value 33.615586
iter  30 value 23.902813
iter  40 value 23.509222
iter  50 value 23.119065
iter  60 value 23.086763
iter  70 value 23.081870
iter  80 value 23.074330
iter  90 value 23.068744
iter 100 value 23.065327
final  value 23.065327 
stopped after 100 iterations
# weights:  144
initial  value 44.827222 
iter  10 value 33.696469
final  value 33.696435 
converged
# weights:  45
initial  value 47.572116 
iter  10 value 33.698868
iter  20 value 33.692534
iter  30 value 33.687043
iter  40 value 33.668815
iter  50 value 33.512373
iter  60 value 24.424121
iter  70 value 24.367899
iter  80 value 23.879273
iter  90 value 23.666502
iter 100 value 23.634723
final  value 23.634723 
stopped after 100 iterations
# weights:  100
initial  value 51.074152 
iter  10 value 33.712319
iter  20 value 33.698020
iter  30 value 33.497971
iter  40 value 23.978210
iter  50 value 23.396967
iter  60 value 23.247042
iter  70 value 23.214026
iter  80 value 23.195738
iter  90 value 23.177636
iter 100 value 23.170725
final  value 23.170725 
stopped after 100 iterations
# weights:  100
initial  value 42.197422 
iter  10 value 33.816198
iter  20 value 32.191604
iter  30 value 24.834626
iter  40 value 24.523425
iter  50 value 24.256966
iter  60 value 24.165900
iter  70 value 24.138245
iter  80 value 24.122940
iter  90 value 24.113237
iter 100 value 24.106283
final  value 24.106283 
stopped after 100 iterations
# weights:  78
initial  value 111.923760 
iter  10 value 40.027901
iter  20 value 40.006445
final  value 40.006444 
converged
# weights:  199
initial  value 48.181133 
iter  10 value 33.697565
iter  20 value 33.696515
iter  30 value 33.695819
iter  40 value 33.695747
iter  50 value 33.695664
iter  60 value 33.695567
iter  70 value 33.695451
iter  80 value 33.695312
iter  90 value 33.695142
iter 100 value 33.694927
final  value 33.694927 
stopped after 100 iterations
# weights:  78
initial  value 68.437914 
iter  10 value 34.108644
iter  20 value 31.486226
iter  30 value 31.163247
iter  40 value 31.049595
iter  50 value 29.971237
iter  60 value 29.859961
iter  70 value 29.819607
iter  80 value 29.810172
final  value 29.810170 
converged
# weights:  155
initial  value 60.296849 
iter  10 value 33.697221
iter  20 value 28.233663
iter  30 value 23.704860
iter  40 value 23.546756
iter  50 value 23.299591
iter  60 value 23.158665
iter  70 value 23.084519
iter  80 value 23.050051
iter  90 value 23.039234
iter 100 value 23.033196
final  value 23.033196 
stopped after 100 iterations
# weights:  199
initial  value 75.531883 
iter  10 value 33.943004
iter  20 value 31.027357
iter  30 value 29.407880
iter  40 value 28.014274
iter  50 value 27.928706
iter  60 value 27.911621
iter  70 value 27.902834
iter  80 value 27.899903
iter  90 value 27.896998
iter 100 value 27.895332
final  value 27.895332 
stopped after 100 iterations
# weights:  122
initial  value 56.094223 
iter  10 value 34.261676
iter  20 value 34.135166
iter  30 value 33.441928
iter  40 value 33.041111
iter  50 value 31.311224
iter  60 value 31.262933
iter  70 value 31.258308
iter  80 value 31.255406
iter  90 value 31.254175
final  value 31.254173 
converged
# weights:  56
initial  value 39.853326 
iter  10 value 33.920230
iter  20 value 32.884597
iter  30 value 26.215210
iter  40 value 26.161154
iter  50 value 25.717041
iter  60 value 25.688339
iter  70 value 25.674425
iter  80 value 25.671612
iter  90 value 25.668607
iter 100 value 25.668459
final  value 25.668459 
stopped after 100 iterations
# weights:  34
initial  value 50.662684 
iter  10 value 34.745864
final  value 34.738567 
converged
# weights:  177
initial  value 39.450276 
iter  10 value 29.526203
iter  20 value 29.500958
final  value 29.500923 
converged
# weights:  56
initial  value 49.271266 
iter  10 value 29.433424
iter  20 value 29.360409
iter  30 value 29.261986
iter  40 value 24.506617
iter  50 value 23.708945
iter  60 value 23.692298
iter  70 value 23.691299
iter  80 value 23.686364
iter  90 value 23.569248
iter 100 value 23.504240
final  value 23.504240 
stopped after 100 iterations
# weights:  177
initial  value 53.105640 
iter  10 value 29.283269
iter  20 value 29.278477
iter  30 value 29.277509
iter  40 value 29.277226
iter  50 value 29.277180
iter  60 value 29.277130
iter  70 value 29.277074
iter  80 value 29.277012
iter  90 value 29.276942
iter 100 value 29.276862
final  value 29.276862 
stopped after 100 iterations
# weights:  155
initial  value 100.617167 
iter  10 value 33.215730
final  value 33.204881 
converged
# weights:  34
initial  value 51.305138 
iter  10 value 29.301588
iter  20 value 29.290501
iter  30 value 29.284837
iter  40 value 29.282778
iter  50 value 29.278699
iter  60 value 29.268167
iter  70 value 29.226284
iter  80 value 28.776669
iter  90 value 23.084064
iter 100 value 22.814811
final  value 22.814811 
stopped after 100 iterations
# weights:  133
initial  value 198.685120 
iter  10 value 36.628382
iter  20 value 36.615299
iter  20 value 36.615299
iter  20 value 36.615299
final  value 36.615299 
converged
# weights:  188
initial  value 54.636420 
iter  10 value 29.975074
iter  20 value 29.734542
final  value 29.734540 
converged
# weights:  144
initial  value 45.679304 
iter  10 value 29.285136
iter  20 value 29.278862
iter  30 value 29.277862
iter  40 value 29.277549
iter  50 value 29.277138
iter  60 value 29.276568
iter  70 value 29.275719
iter  80 value 29.274315
iter  90 value 29.271556
iter 100 value 29.263998
final  value 29.263998 
stopped after 100 iterations
# weights:  144
initial  value 45.257012 
iter  10 value 29.276957
final  value 29.276833 
converged
# weights:  45
initial  value 58.307935 
iter  10 value 29.278646
iter  20 value 29.277546
iter  30 value 29.277061
iter  40 value 29.277037
iter  50 value 29.277011
iter  60 value 29.276983
iter  70 value 29.276953
iter  80 value 29.276920
iter  90 value 29.276885
iter 100 value 29.276846
final  value 29.276846 
stopped after 100 iterations
# weights:  100
initial  value 58.965534 
iter  10 value 29.300447
iter  20 value 29.283517
iter  30 value 29.281551
iter  40 value 29.280282
iter  50 value 29.278255
iter  60 value 29.274153
iter  70 value 29.261549
iter  80 value 29.130379
iter  90 value 23.318826
iter 100 value 22.824681
final  value 22.824681 
stopped after 100 iterations
# weights:  100
initial  value 45.984163 
iter  10 value 29.577618
iter  20 value 29.351494
iter  30 value 27.464277
iter  40 value 24.022423
iter  50 value 23.945316
iter  60 value 23.944270
iter  70 value 23.943263
iter  80 value 23.915490
iter  90 value 23.773262
iter 100 value 23.695130
final  value 23.695130 
stopped after 100 iterations
# weights:  78
initial  value 120.098450 
iter  10 value 36.899920
final  value 36.872720 
converged
# weights:  199
initial  value 48.070903 
iter  10 value 29.278157
final  value 29.277170 
converged
# weights:  78
initial  value 59.286230 
iter  10 value 29.915015
iter  20 value 29.894050
final  value 29.894048 
converged
# weights:  155
initial  value 69.663329 
iter  10 value 29.277381
iter  20 value 29.277095
iter  30 value 29.273737
iter  40 value 29.270173
iter  50 value 29.252536
iter  60 value 25.657818
iter  70 value 22.713480
iter  80 value 22.631172
iter  90 value 22.618189
iter 100 value 22.609123
final  value 22.609123 
stopped after 100 iterations
# weights:  199
initial  value 58.512925 
iter  10 value 29.750253
iter  20 value 29.568611
final  value 29.568551 
converged
# weights:  122
initial  value 38.445159 
iter  10 value 29.932624
final  value 29.931247 
converged
# weights:  56
initial  value 48.710330 
iter  10 value 29.888564
iter  20 value 29.500744
iter  30 value 27.303592
iter  40 value 25.451330
iter  50 value 25.448469
iter  60 value 25.324805
iter  70 value 25.090596
iter  80 value 25.067959
iter  90 value 25.051782
iter 100 value 25.009070
final  value 25.009070 
stopped after 100 iterations
# weights:  155
initial  value 60.642760 
iter  10 value 41.516085
final  value 41.514459 
converged
Neural Network 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  size  decay         RMSE       Rsquared     MAE        Selected
   3    3.308369e-04  0.6341074  0.412455322  0.3458618          
   3    1.704661e+00  0.7536959  0.022430777  0.4738546          
   4    3.723257e-05  0.6872815  0.452882862  0.3994590          
   5    5.151294e-03  0.6058130  0.572485120  0.3228912          
   5    1.996046e-02  0.6209834  0.597782892  0.3515509          
   7    8.782479e-02  0.6998883  0.489529826  0.4157096          
   7    5.007101e+00  0.7713974  0.008533245  0.4991424          
   9    4.054516e-04  0.6017326  0.531346074  0.3182422          
   9    6.394279e-03  0.6077032  0.579648659  0.3276728          
  11    1.214787e-01  0.7181634  0.461393498  0.4282070          
  12    6.670140e+00  0.7694628  0.009044430  0.4962516          
  13    2.133559e-05  0.7236306  0.203115232  0.4330791          
  13    1.454061e-04  0.6656035  0.526046924  0.3656789          
  14    2.830127e-05  0.5995127  0.529834113  0.3166411  *       
  14    2.268611e+00  0.7440484  0.047220591  0.4603553          
  16    8.875772e-05  0.6654668  0.497971794  0.3644244          
  16    3.587242e-02  0.6849995  0.506103686  0.4004699          
  17    9.744362e-02  0.7156387  0.471003824  0.4251524          
  18    3.551935e-05  0.7236307  0.321212792  0.4330764          
  18    5.485211e-02  0.6909963  0.486948268  0.4072181          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 14 and decay = 2.830127e-05.
[1] "Sat Mar 10 01:24:23 2018"
Non-Negative Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE       Rsquared   MAE     
  0.7053401  0.8310144  0.439523

[1] "Sat Mar 10 01:24:39 2018"
Non-Informative Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE       Rsquared  MAE      
  0.7247539  NaN       0.4432742

[1] "Sat Mar 10 01:24:55 2018"
Parallel Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE        Selected
  1     0.6361325  0.3983424  0.4060690          
  2     0.5568318  0.6008083  0.3396326          
  3     0.4780812  0.7300074  0.2809250          
  4     0.4190216  0.8021718  0.2305081          
  5     0.3697809  0.8480855  0.1962994          
  6     0.3173805  0.8910579  0.1575074          
  7     0.2821965  0.9094188  0.1329067          
  8     0.2494926  0.9288729  0.1128907  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 8.
[1] "Sat Mar 10 01:25:15 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: Warning messages:
1: package 'mxnet' is not available (for R version 3.4.3) 
2: package 'mxnet' is not available (for R version 3.4.3) 
3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
4: executing %dopar% sequentially: no parallel backend registered 
# weights:  34
initial  value 49.849214 
iter  10 value 23.796081
iter  20 value 23.785564
final  value 23.785559 
converged
# weights:  177
initial  value 27.842063 
iter  10 value 18.543027
iter  20 value 13.811421
iter  30 value 13.478405
iter  40 value 13.473020
iter  50 value 13.472842
final  value 13.472837 
converged
# weights:  56
initial  value 36.999424 
iter  10 value 20.135453
iter  20 value 12.951352
iter  30 value 12.689701
iter  40 value 12.649444
iter  50 value 12.629407
iter  60 value 12.623826
iter  70 value 12.621997
iter  80 value 12.621260
iter  90 value 12.621105
iter 100 value 12.621058
final  value 12.621058 
stopped after 100 iterations
# weights:  177
initial  value 28.181751 
iter  10 value 20.057634
iter  20 value 20.056458
iter  30 value 20.054649
iter  40 value 20.050448
iter  50 value 20.019035
iter  60 value 12.814713
iter  70 value 12.260707
iter  80 value 12.214958
iter  90 value 12.155418
iter 100 value 12.149580
final  value 12.149580 
stopped after 100 iterations
# weights:  155
initial  value 86.232766 
iter  10 value 22.840564
iter  20 value 22.812241
final  value 22.812239 
converged
# weights:  34
initial  value 26.505670 
iter  10 value 20.108342
iter  20 value 20.107489
iter  30 value 20.106632
iter  40 value 20.105769
iter  50 value 20.104899
iter  60 value 20.104018
iter  70 value 20.103124
iter  80 value 20.102214
iter  90 value 20.101285
iter 100 value 20.100332
final  value 20.100332 
stopped after 100 iterations
# weights:  133
initial  value 167.288463 
iter  10 value 25.691736
iter  20 value 25.650136
final  value 25.650109 
converged
# weights:  188
initial  value 50.422789 
iter  10 value 18.874296
iter  20 value 14.680390
iter  30 value 14.602031
iter  40 value 14.601788
final  value 14.601787 
converged
# weights:  144
initial  value 43.439774 
iter  10 value 20.061625
iter  20 value 20.052966
iter  30 value 19.989062
iter  40 value 12.689569
iter  50 value 12.275955
iter  60 value 12.231392
iter  70 value 12.180875
iter  80 value 12.168291
iter  90 value 12.165584
iter 100 value 12.161244
final  value 12.161244 
stopped after 100 iterations
# weights:  144
initial  value 46.109107 
iter  10 value 20.056298
iter  20 value 20.048940
iter  30 value 20.029011
iter  40 value 16.404450
iter  50 value 12.445905
iter  60 value 12.236622
iter  70 value 12.170319
iter  80 value 12.155994
iter  90 value 12.152017
iter 100 value 12.144321
final  value 12.144321 
stopped after 100 iterations
# weights:  45
initial  value 37.663064 
iter  10 value 20.058688
iter  20 value 20.041649
iter  30 value 20.020037
iter  40 value 19.941578
iter  50 value 14.595304
iter  60 value 12.252616
iter  70 value 12.162606
iter  80 value 12.149087
iter  90 value 12.146300
iter 100 value 12.142556
final  value 12.142556 
stopped after 100 iterations
# weights:  100
initial  value 56.734370 
iter  10 value 20.068478
iter  20 value 13.795899
iter  30 value 12.388619
iter  40 value 12.299224
iter  50 value 12.232123
iter  60 value 12.219758
iter  70 value 12.211106
iter  80 value 12.203918
iter  90 value 12.199429
iter 100 value 12.197685
final  value 12.197685 
stopped after 100 iterations
# weights:  100
initial  value 22.742514 
iter  10 value 15.232599
iter  20 value 12.631347
iter  30 value 12.574030
iter  40 value 12.554543
iter  50 value 12.543143
iter  60 value 12.541320
iter  70 value 12.540683
iter  80 value 12.540487
iter  90 value 12.540426
iter 100 value 12.540384
final  value 12.540384 
stopped after 100 iterations
# weights:  78
initial  value 104.993997 
iter  10 value 25.919652
iter  20 value 25.821892
final  value 25.821826 
converged
# weights:  199
initial  value 20.961500 
iter  10 value 20.049174
iter  20 value 19.527717
iter  30 value 12.860292
iter  40 value 12.511988
iter  50 value 12.197025
iter  60 value 12.155891
iter  70 value 12.147045
iter  80 value 12.142631
iter  90 value 12.141909
iter 100 value 12.141227
final  value 12.141227 
stopped after 100 iterations
# weights:  78
initial  value 38.954465 
iter  10 value 16.080745
iter  20 value 15.187443
iter  30 value 15.186688
final  value 15.186688 
converged
# weights:  155
initial  value 37.397727 
iter  10 value 20.056419
iter  20 value 20.052397
iter  30 value 20.032445
iter  40 value 13.793576
iter  50 value 12.405564
iter  60 value 12.252404
iter  70 value 12.180371
iter  80 value 12.149855
iter  90 value 12.144654
iter 100 value 12.142298
final  value 12.142298 
stopped after 100 iterations
# weights:  199
initial  value 34.330799 
iter  10 value 19.586835
iter  20 value 14.100846
iter  30 value 13.990179
iter  40 value 13.843388
iter  50 value 13.840127
final  value 13.840101 
converged
# weights:  122
initial  value 32.408361 
iter  10 value 16.884741
iter  20 value 15.827405
iter  30 value 15.826862
final  value 15.826861 
converged
# weights:  56
initial  value 41.804094 
iter  10 value 20.507285
iter  20 value 13.468108
iter  30 value 13.417843
iter  40 value 13.409369
iter  50 value 13.405784
iter  60 value 13.404046
iter  70 value 13.403325
iter  80 value 13.403153
iter  90 value 13.403098
iter 100 value 13.402960
final  value 13.402960 
stopped after 100 iterations
# weights:  34
initial  value 68.890071 
iter  10 value 37.458247
iter  20 value 37.405388
final  value 37.405380 
converged
# weights:  177
initial  value 60.569043 
iter  10 value 33.572902
iter  20 value 25.162875
iter  30 value 25.114997
iter  40 value 25.112733
iter  50 value 25.112580
final  value 25.112567 
converged
# weights:  56
initial  value 49.268938 
iter  10 value 33.694098
iter  20 value 23.724122
iter  30 value 23.617675
iter  40 value 23.566550
iter  50 value 23.556645
iter  60 value 23.554294
iter  70 value 23.553952
iter  80 value 23.553829
iter  90 value 23.553702
iter 100 value 23.553636
final  value 23.553636 
stopped after 100 iterations
# weights:  177
initial  value 49.678722 
iter  10 value 33.700511
iter  20 value 33.696412
iter  30 value 33.695851
iter  40 value 33.694865
iter  50 value 33.692670
iter  60 value 33.684270
iter  70 value 32.907332
iter  80 value 23.451253
iter  90 value 23.090617
iter 100 value 23.022255
final  value 23.022255 
stopped after 100 iterations
# weights:  155
initial  value 98.521439 
iter  10 value 36.615265
iter  20 value 36.577503
final  value 36.577490 
converged
# weights:  34
initial  value 43.208827 
iter  10 value 33.412841
iter  20 value 24.343091
iter  30 value 23.111839
iter  40 value 23.076524
iter  50 value 23.071438
iter  60 value 23.069146
iter  70 value 23.066620
iter  80 value 23.065933
iter  90 value 23.065571
iter 100 value 23.065213
final  value 23.065213 
stopped after 100 iterations
# weights:  133
initial  value 196.661041 
iter  10 value 39.872214
iter  20 value 39.677925
final  value 39.677921 
converged
# weights:  188
initial  value 41.495654 
iter  10 value 26.982177
iter  20 value 25.822662
iter  30 value 25.782551
iter  40 value 25.782312
final  value 25.782312 
converged
# weights:  144
initial  value 51.156121 
iter  10 value 33.701014
iter  20 value 28.376606
iter  30 value 23.231456
iter  40 value 23.076153
iter  50 value 23.053141
iter  60 value 23.046495
iter  70 value 23.039160
iter  80 value 23.036108
iter  90 value 23.033172
iter 100 value 23.029996
final  value 23.029996 
stopped after 100 iterations
# weights:  144
initial  value 43.307274 
iter  10 value 33.694422
iter  20 value 33.693260
iter  30 value 33.690494
iter  40 value 33.677593
iter  50 value 27.904979
iter  60 value 23.217930
iter  70 value 23.056277
iter  80 value 23.010622
iter  90 value 23.003621
iter 100 value 22.999460
final  value 22.999460 
stopped after 100 iterations
# weights:  45
initial  value 47.244078 
iter  10 value 33.699655
iter  20 value 33.696399
iter  30 value 33.691145
iter  40 value 33.689147
iter  50 value 33.686469
iter  60 value 33.682683
iter  70 value 33.676903
iter  80 value 33.666946
iter  90 value 33.645606
iter 100 value 33.566169
final  value 33.566169 
stopped after 100 iterations
# weights:  100
initial  value 51.868664 
iter  10 value 33.712165
iter  20 value 33.698384
iter  30 value 33.662192
iter  40 value 23.846810
iter  50 value 23.187603
iter  60 value 23.132931
iter  70 value 23.099281
iter  80 value 23.087910
iter  90 value 23.080209
iter 100 value 23.074351
final  value 23.074351 
stopped after 100 iterations
# weights:  100
initial  value 41.425982 
iter  10 value 33.799239
iter  20 value 32.401500
iter  30 value 24.287159
iter  40 value 23.726026
iter  50 value 23.572761
iter  60 value 23.481291
iter  70 value 23.448227
iter  80 value 23.436213
iter  90 value 23.431787
iter 100 value 23.430706
final  value 23.430706 
stopped after 100 iterations
# weights:  78
initial  value 111.811722 
iter  10 value 39.867808
iter  20 value 39.826544
final  value 39.826543 
converged
# weights:  199
initial  value 48.747028 
iter  10 value 33.697170
iter  20 value 33.694669
iter  30 value 33.690622
iter  40 value 33.663513
iter  50 value 24.354228
iter  60 value 23.134230
iter  70 value 23.052508
iter  80 value 23.033480
iter  90 value 23.009681
iter 100 value 23.007801
final  value 23.007801 
stopped after 100 iterations
# weights:  78
initial  value 64.361654 
iter  10 value 29.744415
iter  20 value 26.686597
iter  30 value 26.682681
iter  30 value 26.682680
iter  30 value 26.682680
final  value 26.682680 
converged
# weights:  155
initial  value 59.513462 
iter  10 value 33.697112
iter  20 value 33.696244
iter  30 value 33.696177
iter  40 value 33.696096
iter  50 value 33.695994
iter  60 value 33.695863
iter  70 value 33.695688
iter  80 value 33.695441
iter  90 value 33.695068
iter 100 value 33.694442
final  value 33.694442 
stopped after 100 iterations
# weights:  199
initial  value 75.248903 
iter  10 value 33.427559
iter  20 value 25.075582
iter  30 value 25.027166
iter  40 value 25.025742
iter  50 value 25.025727
iter  50 value 25.025727
iter  50 value 25.025727
final  value 25.025727 
converged
# weights:  122
initial  value 51.452303 
iter  10 value 29.283698
iter  20 value 27.449485
iter  30 value 27.447781
iter  30 value 27.447781
iter  30 value 27.447781
final  value 27.447781 
converged
# weights:  56
initial  value 40.677313 
iter  10 value 32.840837
iter  20 value 24.433210
iter  30 value 24.204822
iter  40 value 24.195881
iter  50 value 24.195029
iter  60 value 24.194847
final  value 24.194833 
converged
# weights:  34
initial  value 50.294839 
iter  10 value 34.310255
iter  20 value 34.294766
final  value 34.294755 
converged
# weights:  177
initial  value 45.171386 
iter  10 value 29.312769
iter  20 value 24.320676
iter  30 value 24.208447
iter  40 value 23.978007
iter  50 value 23.840080
iter  60 value 23.814719
iter  70 value 23.813457
iter  80 value 23.813224
iter  90 value 23.813077
final  value 23.813073 
converged
# weights:  56
initial  value 49.585258 
iter  10 value 29.437718
iter  20 value 24.569788
iter  30 value 23.285757
iter  40 value 23.085006
iter  50 value 23.044099
iter  60 value 23.039032
iter  70 value 23.035707
iter  80 value 23.034461
iter  90 value 23.034091
iter 100 value 23.033965
final  value 23.033965 
stopped after 100 iterations
# weights:  177
initial  value 50.414151 
iter  10 value 29.281852
iter  20 value 29.199463
iter  30 value 23.257641
iter  40 value 22.706804
iter  50 value 22.633354
iter  60 value 22.611641
iter  70 value 22.601539
iter  80 value 22.597000
iter  90 value 22.595439
iter 100 value 22.593685
final  value 22.593685 
stopped after 100 iterations
# weights:  155
initial  value 98.797106 
iter  10 value 33.029888
iter  20 value 33.003973
final  value 33.003966 
converged
# weights:  34
initial  value 54.065542 
iter  10 value 29.302952
iter  20 value 29.142787
iter  30 value 25.277669
iter  40 value 22.811885
iter  50 value 22.659459
iter  60 value 22.653704
iter  70 value 22.651646
iter  80 value 22.647276
iter  90 value 22.642440
iter 100 value 22.639528
final  value 22.639528 
stopped after 100 iterations
# weights:  133
initial  value 198.560678 
iter  10 value 36.613562
iter  20 value 36.548547
final  value 36.548543 
converged
# weights:  188
initial  value 53.487339 
iter  10 value 29.744603
iter  20 value 25.884049
iter  30 value 25.336006
iter  40 value 25.332739
iter  50 value 25.332714
iter  50 value 25.332714
iter  50 value 25.332714
final  value 25.332714 
converged
# weights:  144
initial  value 43.471274 
iter  10 value 29.282656
iter  20 value 29.262777
iter  30 value 29.003889
iter  40 value 22.789274
iter  50 value 22.662548
iter  60 value 22.615655
iter  70 value 22.600791
iter  80 value 22.595516
iter  90 value 22.593548
iter 100 value 22.592124
final  value 22.592124 
stopped after 100 iterations
# weights:  144
initial  value 46.056190 
iter  10 value 29.276899
iter  20 value 27.779225
iter  30 value 22.943035
iter  40 value 22.645046
iter  50 value 22.617514
iter  60 value 22.615344
iter  70 value 22.614396
iter  80 value 22.608661
iter  90 value 22.598074
iter 100 value 22.595566
final  value 22.595566 
stopped after 100 iterations
# weights:  45
initial  value 58.245397 
iter  10 value 29.277903
iter  20 value 29.276906
iter  30 value 29.275339
iter  40 value 29.271507
iter  50 value 29.258233
iter  60 value 29.212531
iter  70 value 28.898252
iter  80 value 22.919708
iter  90 value 22.621508
iter 100 value 22.599847
final  value 22.599847 
stopped after 100 iterations
# weights:  100
initial  value 52.061809 
iter  10 value 29.294320
iter  20 value 27.071106
iter  30 value 22.765962
iter  40 value 22.683084
iter  50 value 22.656433
iter  60 value 22.641796
iter  70 value 22.634424
iter  80 value 22.632420
iter  90 value 22.631905
iter 100 value 22.631459
final  value 22.631459 
stopped after 100 iterations
# weights:  100
initial  value 45.374309 
iter  10 value 29.505606
iter  20 value 25.615512
iter  30 value 23.139201
iter  40 value 23.101275
iter  50 value 23.097498
iter  60 value 23.096259
iter  70 value 23.095879
iter  80 value 23.095741
iter  90 value 23.095667
iter 100 value 23.095597
final  value 23.095597 
stopped after 100 iterations
# weights:  78
initial  value 121.460819 
iter  10 value 36.816762
iter  20 value 36.765837
final  value 36.765832 
converged
# weights:  199
initial  value 45.729067 
iter  10 value 29.278150
iter  20 value 27.208285
iter  30 value 23.353824
iter  40 value 22.740305
iter  50 value 22.639013
iter  60 value 22.624151
iter  70 value 22.622856
iter  80 value 22.621452
iter  90 value 22.602636
iter 100 value 22.591563
final  value 22.591563 
stopped after 100 iterations
# weights:  78
initial  value 59.215775 
iter  10 value 27.778411
iter  20 value 25.603247
iter  30 value 25.601461
iter  30 value 25.601460
iter  30 value 25.601460
final  value 25.601460 
converged
# weights:  155
initial  value 67.936762 
iter  10 value 29.277493
iter  20 value 29.275190
iter  30 value 29.274679
iter  40 value 29.273787
iter  50 value 29.271909
iter  60 value 29.266202
iter  70 value 29.220208
iter  80 value 23.658504
iter  90 value 22.710025
iter 100 value 22.628996
final  value 22.628996 
stopped after 100 iterations
# weights:  199
initial  value 49.045971 
iter  10 value 29.677539
iter  20 value 24.940908
iter  30 value 24.884979
iter  40 value 24.884402
iter  50 value 24.884350
final  value 24.884339 
converged
# weights:  122
initial  value 36.907028 
iter  10 value 25.851776
iter  20 value 25.604232
final  value 25.603877 
converged
# weights:  56
initial  value 48.982177 
iter  10 value 29.721178
iter  20 value 23.950262
iter  30 value 23.803216
iter  40 value 23.794499
iter  50 value 23.793825
iter  60 value 23.793645
iter  70 value 23.793613
iter  80 value 23.793599
final  value 23.793598 
converged
# weights:  100
initial  value 76.851787 
iter  10 value 42.079441
iter  20 value 41.001985
iter  30 value 29.812095
iter  40 value 29.570012
iter  50 value 29.537591
iter  60 value 29.530361
iter  70 value 29.527670
iter  80 value 29.525557
iter  90 value 29.524445
iter 100 value 29.524139
final  value 29.524139 
stopped after 100 iterations
Neural Networks with Feature Extraction 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  size  decay         RMSE       Rsquared   MAE        Selected
   3    3.308369e-04  0.6402560  0.4683226  0.3586753          
   3    1.704661e+00  0.7407956  0.6767767  0.4652385          
   4    3.723257e-05  0.6284110  0.4451373  0.3657296          
   5    5.151294e-03  0.6102611  0.5235815  0.3325644          
   5    1.996046e-02  0.6110437  0.5555845  0.3291468          
   7    8.782479e-02  0.6285336  0.5937627  0.3535748          
   7    5.007101e+00  0.7681407  0.6690007  0.4972490          
   9    4.054516e-04  0.6106289  0.4814704  0.3394970          
   9    6.394279e-03  0.6062189  0.5284768  0.3278574  *       
  11    1.214787e-01  0.6305777  0.6054694  0.3581364          
  12    6.670140e+00  0.7675101  0.6684728  0.4951065          
  13    2.133559e-05  0.6276755  0.4073625  0.3782829          
  13    1.454061e-04  0.6124630  0.4643721  0.3487993          
  14    2.830127e-05  0.6311877  0.4577765  0.3742348          
  14    2.268611e+00  0.7393166  0.6663603  0.4575203          
  16    8.875772e-05  0.6133933  0.4588581  0.3493251          
  16    3.587242e-02  0.6108432  0.5768775  0.3309760          
  17    9.744362e-02  0.6234591  0.5982354  0.3501821          
  18    3.551935e-05  0.6450477  0.3549825  0.3837674          
  18    5.485211e-02  0.6183650  0.5837027  0.3407345          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 9 and decay = 0.006394279.
[1] "Sat Mar 10 01:25:34 2018"
Principal Component Analysis 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      0.6795792  0.1442147  0.4256980          
  2      0.6656980  0.1689291  0.4308792          
  3      0.5279996  0.4846725  0.3406178          
  4      0.4980699  0.6153577  0.3405067          
  5      0.4964306  0.6174137  0.3357748  *       
  6      0.5197604  0.5501511  0.3661903          
  7      0.5431553  0.5224644  0.3881068          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 5.
[1] "Sat Mar 10 01:25:51 2018"
Loading required package: plsRglm
____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.118492023693398) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0903972115833312) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 7 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.178525331569836) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.194137832149863) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.132958446955308) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 7 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 7 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 7 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0935263869352639) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.189986211294308) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 7 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.131453903950751) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 7 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.124639777326956) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.136150009883568) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.110005687689409) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.118492023693398) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 4 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0316068716812879) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.178525331569836) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.194137832149863) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.132958446955308) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0387527559418231) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0109701526816934) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 4 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 4 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.189986211294308) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0183488334994763) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0150601975619793) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.124639777326956) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.136150009883568) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.118492023693398) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0903972115833312) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0316068716812879) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 8 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.194137832149863) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 8 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0387527559418231) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0109701526816934) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0535979677923024) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0935263869352639) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.189986211294308) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0183488334994763) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.131453903950751) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0150601975619793) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.124639777326956) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 8 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.110005687689409) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0935263869352639) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

Partial Least Squares Generalized Linear Models  

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  nt  alpha.pvals.expli  RMSE       Rsquared   MAE        Selected
  1   0.17438793         0.4205774  0.6963892  0.3023270          
  2   0.01903077         0.3596047  0.7984446  0.2722776          
  2   0.05065380         0.3526953  0.8129058  0.2720632          
  3   0.09039721         0.3468602  0.8201752  0.2659297          
  3   0.11000569         0.3512594  0.8138799  0.2657704          
  3   0.13145390         0.3507371  0.8146249  0.2664982          
  4   0.09352639         0.3468602  0.8201752  0.2659297  *       
  4   0.18998621         0.3536000  0.8072929  0.2751385          
  5   0.05359797         0.3526953  0.8129058  0.2720632          
  5   0.13615001         0.3528063  0.8104942  0.2705211          
  6   0.01097015         0.3956245  0.7607312  0.2883807          
  6   0.01506020         0.3596047  0.7984446  0.2722776          
  6   0.03875276         0.3570031  0.7931490  0.2685309          
  6   0.17852533         0.3528063  0.8104942  0.2705211          
  6   0.19413783         0.3709118  0.7918245  0.2855272          
  7   0.11849202         0.3507371  0.8146249  0.2664982          
  8   0.01834883         0.3596047  0.7984446  0.2722776          
  8   0.03160687         0.3612851  0.7910410  0.2718119          
  8   0.12463978         0.3507371  0.8146249  0.2664982          
  8   0.13295845         0.3528063  0.8104942  0.2705211          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nt = 4 and alpha.pvals.expli
 = 0.09352639.
[1] "Sat Mar 10 01:26:43 2018"
Projection Pursuit Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  nterms  RMSE       Rsquared   MAE         Selected
   1      0.1039811  0.9912455  0.04982989          
   2      0.1023949  0.9918821  0.04846827  *       
   3      0.1037760  0.9912726  0.05147769          
   4      0.1042911  0.9915062  0.05185326          
   5      0.1058816  0.9903808  0.05412111          
   6      0.1033507  0.9912579  0.05306789          
   7      0.1026113  0.9913229  0.05283633          
   8      0.1037079  0.9912581  0.05329455          
   9      0.1026494  0.9913548  0.05291510          
  10      0.1027242  0.9914252  0.05278089          
  11      0.1029748  0.9913507  0.05308640          
  12      0.1029993  0.9913648  0.05314934          
  13      0.1027045  0.9914107  0.05279293          
  14      0.1027429  0.9914030  0.05277197          
  15      0.1027154  0.9914160  0.05274315          
  16      0.1025863  0.9914398  0.05265428          
  17      0.1025350  0.9914463  0.05263176          
  18      0.1025573  0.9914441  0.05266143          
  19      0.1025511  0.9914452  0.05265912          
  20      0.1025520  0.9914443  0.05266425          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nterms = 2.
[1] "Sat Mar 10 01:27:00 2018"
Quantile Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE        Selected
  1     1.0387936  0.2539544  0.8169332          
  2     0.9096141  0.3957887  0.7146630          
  3     0.8532259  0.4545740  0.6542546          
  4     0.7786725  0.4866210  0.5648739          
  5     0.7139216  0.5181984  0.4777847          
  6     0.6728104  0.5577835  0.4384351          
  7     0.5260515  0.7219360  0.3384247          
  8     0.4487019  0.7763785  0.2652327  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 8.
[1] "Sat Mar 10 01:27:42 2018"
Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  min.node.size  mtry  splitrule   RMSE       Rsquared   MAE        Selected
   3             3     extratrees  0.4559893  0.8337687  0.2409395          
   3             8     extratrees  0.2568565  0.9343051  0.1233195          
   4             1     variance    0.6363955  0.3842618  0.4070755          
   5             5     maxstat     0.4661436  0.7183759  0.2191050          
   5             5     variance    0.3763929  0.8443663  0.1975889          
   7             6     variance    0.3426067  0.8731587  0.1740467          
   7             9     variance    0.2448221  0.9308374  0.1085615  *       
   9             3     maxstat     0.5640432  0.5731650  0.2981291          
   9             5     maxstat     0.4725582  0.7030195  0.2230487          
  11             7     maxstat     0.4153035  0.7560356  0.2048140          
  12             9     extratrees  0.2739407  0.9211884  0.1368286          
  13             1     variance    0.6530592  0.3396017  0.4187879          
  13             2     maxstat     0.6264233  0.4444721  0.3536995          
  14             1     variance    0.6581465  0.3192035  0.4215556          
  14             9     extratrees  0.2933120  0.9061370  0.1476681          
  16             2     extratrees  0.5653551  0.7707530  0.3201706          
  16             6     extratrees  0.3740394  0.8696870  0.1781809          
  17             6     maxstat     0.4976730  0.6228519  0.2443660          
  18             1     extratrees  0.6397127  0.6871566  0.3824488          
  18             6     extratrees  0.3910050  0.8674831  0.1893075          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 9, splitrule = variance
 and min.node.size = 7.
[1] "Sat Mar 10 01:28:03 2018"
Error in code$varImp(object$finalModel, ...) : 
  No importance values available
In addition: Warning messages:
1: package 'gpls' is not available (for R version 3.4.3) 
2: package 'rPython' is not available (for R version 3.4.3) 
Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  predFixed  minNode  RMSE       Rsquared   MAE         Selected
  1          18       0.6679590  0.2880705  0.42627260          
  2           2       0.5231283  0.6848669  0.31921194          
  2           6       0.5522968  0.6084372  0.33922079          
  3          10       0.4917765  0.7573008  0.28909680          
  3          12       0.5139877  0.6993013  0.31284421          
  3          14       0.5063301  0.7263552  0.30279004          
  4          10       0.4308532  0.8119276  0.23565380          
  4          19       0.4691445  0.7971772  0.27888442          
  5           6       0.3475082  0.8819630  0.17788783          
  5          14       0.3871410  0.8647155  0.20080020          
  6           2       0.2641154  0.9337270  0.11942031          
  6           4       0.2834869  0.9158681  0.13215472          
  6          18       0.3527973  0.8973107  0.17997167          
  6          20       0.3570595  0.8923672  0.18665260          
  7          12       0.3017756  0.9114226  0.14034038          
  8           2       0.1638077  0.9731603  0.07125028  *       
  8           4       0.2001039  0.9569458  0.08857669          
  8          13       0.2731104  0.9206237  0.12494586          
  8          14       0.2809748  0.9129017  0.12659830          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were predFixed = 8 and minNode = 2.
[1] "Sat Mar 10 01:31:08 2018"
Loading required package: lars
Loaded lars 1.2

Relaxed Lasso 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  phi         lambda     RMSE        Rsquared   MAE         Selected
  0.05485076  30.149836   0.7709491  0.8310144   0.5059735          
  0.07530099  33.061583   0.6739879  0.7546484   0.4234947  *       
  0.09174417  53.779757   2.1927817  0.1871554   1.8058055          
  0.09515384  11.368971   0.7236288        NaN   0.4330801          
  0.15803436  44.146174   0.7397415  0.4389367   0.5344558          
  0.19376378  30.579632  15.6902766  0.6893126  12.5762520          
  0.25326900  10.662676   0.7236288        NaN   0.4330801          
  0.26798984  20.986811   0.7173988  0.8128835   0.4335095          
  0.45198606  13.002174   0.7236288        NaN   0.4330801          
  0.46763193  19.705891   0.7289833  0.8128835   0.4961973          
  0.55002844  13.515439   0.7236288        NaN   0.4330801          
  0.59246012  41.785611   0.9707542  0.8005776   0.9025353          
  0.62319889  51.006521   1.6812966  0.3030263   1.3721876          
  0.65726952  16.064927   0.7236288        NaN   0.4330801          
  0.66479223  45.742614   1.1808533  0.7199299   1.1144762          
  0.68075005  25.409638   0.8935362  0.8332919   0.7672599          
  0.87193966   9.940151   0.7236288        NaN   0.4330801          
  0.89262666  34.023040   1.3935286  0.8172594   1.3521343          
  0.94993106  16.786797   0.7236288        NaN   0.4330801          
  0.97068916  26.813859   1.5153999  0.8310144   1.4756537          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 33.06158 and phi = 0.07530099.
[1] "Sat Mar 10 01:31:25 2018"
Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE        Selected
  1     0.6366147  0.3820409  0.4030129          
  2     0.5572824  0.5895652  0.3444396          
  3     0.4922078  0.7158033  0.2884481          
  4     0.4196833  0.8045554  0.2407848          
  5     0.3651518  0.8561963  0.1950324          
  6     0.3295671  0.8781924  0.1640788          
  7     0.2878213  0.9046624  0.1376186          
  8     0.2405313  0.9336496  0.1125073  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 8.
[1] "Sat Mar 10 01:31:45 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Ridge Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  lambda        RMSE       Rsquared   MAE        Selected
  4.340291e-05  0.3655405  0.7970519  0.2797655  *       
  6.747726e-05  0.3655417  0.7970498  0.2797648          
  1.010020e-04  0.3655434  0.7970469  0.2797639          
  2.349262e-04  0.3655501  0.7970351  0.2797601          
  2.996896e-04  0.3655534  0.7970295  0.2797583          
  8.884509e-04  0.3655831  0.7969779  0.2797476          
  1.171332e-03  0.3655974  0.7969530  0.2797427          
  3.210364e-03  0.3657013  0.7967736  0.2797079          
  4.770390e-03  0.3657818  0.7966356  0.2796816          
  1.587988e-02  0.3663774  0.7956392  0.2795042          
  2.227191e-02  0.3667361  0.7950561  0.2794094          
  4.656033e-02  0.3681818  0.7927943  0.2790926          
  5.089497e-02  0.3684509  0.7923852  0.2790428          
  8.313985e-02  0.3705207  0.7893195  0.2791937          
  9.956366e-02  0.3716048  0.7877564  0.2793718          
  3.625722e-01  0.3879743  0.7655513  0.2853621          
  5.122520e-01  0.3954766  0.7558403  0.2911078          
  6.404805e-01  0.4009481  0.7489169  0.2954201          
  1.270555e+00  0.4195954  0.7265126  0.3109400          
  1.772521e+00  0.4289244  0.7161037  0.3183497          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was lambda = 4.340291e-05.
[1] "Sat Mar 10 01:32:02 2018"
Robust Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  intercept  psi           RMSE       Rsquared   MAE        Selected
  FALSE      psi.huber     0.4088704  0.7397490  0.2621494          
  FALSE      psi.hampel    0.4199972  0.7356583  0.2702864          
  FALSE      psi.bisquare  0.4266857  0.7421896  0.2529613          
   TRUE      psi.huber     0.3601349  0.8161990  0.2205655  *       
   TRUE      psi.hampel    0.3897282  0.8199488  0.2383915          
   TRUE      psi.bisquare  0.4059807  0.8188462  0.2140679          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were intercept = TRUE and psi = psi.huber.
[1] "Sat Mar 10 01:32:19 2018"
CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  cp           RMSE       Rsquared   MAE        Selected
  0.000000000  0.4541264  0.7325524  0.2711095  *       
  0.002120064  0.4543489  0.7319960  0.2744931          
  0.017763726  0.4618744  0.7234596  0.2859875          
  0.037857349  0.4755506  0.7032862  0.3170275          
  0.228149155  0.5699975  0.4790192  0.3829360          
  0.471891180  0.6166881  0.4569284  0.3895338          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.
[1] "Sat Mar 10 01:32:36 2018"
CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE       Rsquared   MAE      
  0.4611346  0.7251189  0.2820843

[1] "Sat Mar 10 01:32:52 2018"
CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  maxdepth  RMSE       Rsquared   MAE        Selected
  1         0.5699975  0.4790192  0.3829360          
  2         0.4755506  0.7032862  0.3170275          
  3         0.4618744  0.7234596  0.2859875          
  4         0.4611346  0.7251189  0.2820843  *       
  5         0.4611346  0.7251189  0.2820843          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was maxdepth = 4.
[1] "Sat Mar 10 01:33:09 2018"
Quantile Regression with LASSO penalty 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  lambda        RMSE       Rsquared   MAE        Selected
  4.340291e-05  0.3733038  0.8111094  0.2139309          
  6.747726e-05  0.3733038  0.8111094  0.2139309          
  1.010020e-04  0.3733038  0.8111094  0.2139309          
  2.349262e-04  0.3713601  0.8131317  0.2117171          
  2.996896e-04  0.3713601  0.8131317  0.2117171  *       
  8.884509e-04  0.3746277  0.8131535  0.2113081          
  1.171332e-03  0.3746277  0.8131535  0.2113081          
  3.210364e-03  0.3794492  0.8171892  0.2108308          
  4.770390e-03  0.3856610  0.8127161  0.2150180          
  1.587988e-02  0.4197927  0.8250185  0.2085479          
  2.227191e-02  0.4260981  0.8244704  0.2085961          
  4.656033e-02  0.4953444  0.8310144  0.2381644          
  5.089497e-02  0.5065827  0.8310144  0.2446421          
  8.313985e-02  0.6364700  0.8196715  0.3513160          
  9.956366e-02  0.7234515        NaN  0.4330288          
  3.625722e-01  0.7234515        NaN  0.4330288          
  5.122520e-01  0.7234515        NaN  0.4330288          
  6.404805e-01  0.7234515        NaN  0.4330288          
  1.270555e+00  0.7234515        NaN  0.4330288          
  1.772521e+00  0.7234515        NaN  0.4330288          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was lambda = 0.0002996896.
[1] "Sat Mar 10 01:33:27 2018"
Non-Convex Penalized Quantile Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  lambda        penalty  RMSE       Rsquared   MAE        Selected
  4.340291e-05  SCAD     0.3733038  0.8111094  0.2139309          
  6.747726e-05  MCP      0.3733038  0.8111094  0.2139309          
  1.010020e-04  MCP      0.3733038  0.8111094  0.2139309          
  2.349262e-04  MCP      0.3716980  0.8126884  0.2119385          
  2.996896e-04  SCAD     0.3716980  0.8126884  0.2119385          
  8.884509e-04  SCAD     0.3716980  0.8126884  0.2119385          
  1.171332e-03  SCAD     0.3716980  0.8126884  0.2119385  *       
  3.210364e-03  MCP      0.3739913  0.8113315  0.2148008          
  4.770390e-03  MCP      0.3744062  0.8128171  0.2169588          
  1.587988e-02  SCAD     0.3769320  0.8153119  0.2149967          
  2.227191e-02  SCAD     0.3785800  0.8209910  0.2078596          
  4.656033e-02  MCP      0.3878888  0.8310144  0.2036898          
  5.089497e-02  MCP      0.3878888  0.8310144  0.2036898          
  8.313985e-02  MCP      0.5018303  0.8196715  0.2898571          
  9.956366e-02  SCAD     0.7234515        NaN  0.4330288          
  3.625722e-01  SCAD     0.7234515        NaN  0.4330288          
  5.122520e-01  MCP      0.7234515        NaN  0.4330288          
  6.404805e-01  SCAD     0.7234515        NaN  0.4330288          
  1.270555e+00  SCAD     0.7234515        NaN  0.4330288          
  1.772521e+00  MCP      0.7234515        NaN  0.4330288          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 0.001171332 and penalty
 = SCAD.
[1] "Sat Mar 10 01:33:44 2018"
Regularized Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  mtry  coefReg     coefImp     RMSE       Rsquared   MAE         Selected
  1     0.87193966  0.34104363  0.6463875  0.3512609  0.41370263          
  2     0.09515384  0.19914471  0.5592139  0.5811854  0.34568391          
  2     0.25326900  0.42085754  0.5655028  0.5806124  0.35635165          
  3     0.45198606  0.78255982  0.5007329  0.6611187  0.30620906          
  3     0.55002844  0.01191750  0.4986976  0.6944850  0.29513905          
  3     0.65726952  0.02599040  0.4943617  0.6931190  0.29058325          
  4     0.46763193  0.66963399  0.4308808  0.7909147  0.23963537          
  4     0.94993106  0.24880959  0.4283537  0.7880432  0.23902973          
  5     0.26798984  0.74404662  0.3734940  0.8404933  0.20759277          
  5     0.68075005  0.72596356  0.3761650  0.8433327  0.19587333          
  6     0.05485076  0.03734481  0.3311886  0.8695948  0.15550816          
  6     0.07530099  0.16253041  0.3235865  0.8782661  0.15926235          
  6     0.19376378  0.95165259  0.3239603  0.8639924  0.16250649          
  6     0.89262666  0.36727283  0.3447072  0.8634774  0.16897839          
  6     0.97068916  0.37559074  0.3329428  0.8782428  0.15954383          
  7     0.59246012  0.60290656  0.3037905  0.8955582  0.13516754          
  8     0.09174417  0.48164113  0.2147119  0.9482383  0.09914423  *       
  8     0.15803436  0.38893795  0.2576410  0.9190247  0.11271190          
  8     0.62319889  0.35525093  0.2716827  0.9080230  0.12006016          
  8     0.66479223  0.77987306  0.2701851  0.9101473  0.11914972          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 8, coefReg = 0.09174417
 and coefImp = 0.4816411.
[1] "Sat Mar 10 01:34:16 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Regularized Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  mtry  coefReg     RMSE       Rsquared   MAE        Selected
  1     0.87193966  0.6452369  0.3526348  0.4110742          
  2     0.09515384  0.5626362  0.5853880  0.3484455          
  2     0.25326900  0.5755283  0.5492794  0.3577489          
  3     0.45198606  0.4922703  0.7075863  0.2920166          
  3     0.55002844  0.4871130  0.7222319  0.2890256          
  3     0.65726952  0.4906271  0.7069704  0.2930667          
  4     0.46763193  0.4225263  0.7989500  0.2377256          
  4     0.94993106  0.4413426  0.7730125  0.2514389          
  5     0.26798984  0.3832891  0.8330187  0.1998400          
  5     0.68075005  0.3900974  0.8273716  0.2056394          
  6     0.05485076  0.2870230  0.9287403  0.1427839          
  6     0.07530099  0.2944307  0.9096322  0.1294474          
  6     0.19376378  0.3255925  0.8843518  0.1487509          
  6     0.89262666  0.3478041  0.8644167  0.1734468          
  6     0.97068916  0.3430585  0.8634198  0.1609604          
  7     0.59246012  0.3032286  0.8986734  0.1379003          
  8     0.09174417  0.2542164  0.9268384  0.1064208  *       
  8     0.15803436  0.2733283  0.9093316  0.1176239          
  8     0.62319889  0.2797658  0.9087671  0.1251524          
  8     0.66479223  0.2782065  0.9107852  0.1218171          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 8 and coefReg = 0.09174417.
[1] "Sat Mar 10 01:34:38 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
In addition: Warning messages:
1: model fit failed for Resample04: parameter=none Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
2: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
3: model fit failed for Resample10: parameter=none Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
4: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
5: model fit failed for Resample20: parameter=none Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
6: model fit failed for Resample24: parameter=none Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
7: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:34:59 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "rvmLinear"               
Error in .local(object, ...) : test vector does not match model !
In addition: There were 41 warnings (use warnings() to see them)
Error in .local(object, ...) : test vector does not match model !
In addition: There were 12 warnings (use warnings() to see them)
Error in .local(object, ...) : test vector does not match model !
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:35:43 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "rvmPoly"                 
Error in .local(object, ...) : test vector does not match model !
In addition: Warning messages:
1: model fit failed for Fold1: sigma=0.01395 Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) : 
  'a' must have dims > 0
 
2: model fit failed for Fold1: sigma=0.01855 Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) : 
  'a' must have dims > 0
 
3: model fit failed for Fold2: sigma=0.01395 Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) : 
  'a' must have dims > 0
 
4: In diag(1/thetatmp) : NAs introduced by coercion to integer range
5: model fit failed for Fold2: sigma=0.01855 Error in diag(1/thetatmp) : invalid 'nrow' value (too large or NA)
 
6: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
7: model fit failed for Fold3: sigma=0.01855 Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
8: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
9: In train.default(x = data.frame(training[, 2:length(training[1,  :
  missing values found in aggregated results
Error in .local(object, ...) : test vector does not match model !
In addition: Warning messages:
1: In diag(1/thetatmp) : NAs introduced by coercion to integer range
2: model fit failed for Fold1: sigma=0.02429 Error in diag(1/thetatmp) : invalid 'nrow' value (too large or NA)
 
3: In diag(1/thetatmp) : NAs introduced by coercion to integer range
4: model fit failed for Fold1: sigma=0.02469 Error in diag(1/thetatmp) : invalid 'nrow' value (too large or NA)
 
5: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Error in .local(object, ...) : test vector does not match model !
In addition: Warning messages:
1: model fit failed for Resample07: sigma=0.08503 Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
2: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:36:11 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "rvmRadial"               
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |======                                                                |   8%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
Error in inpvar.ctr[i, ] : incorrect number of dimensions
In addition: There were 50 or more warnings (use warnings() to see the first 50)
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%  |                                                                              |=================================================                     |  70%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%  |                                                                              |=================================================                     |  70%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%  |                                                                              |=================================================                     |  70%  |                                                                              |==================================================                    |  71%  |                                                                              |===================================================                   |  72%
Subtractive Clustering and Fuzzy c-Means Rules 

76 samples
 9 predictor

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 76, 76, 76, 76, 76, 76, ... 
Resampling results across tuning parameters:

  r.a  RMSE       Rsquared   MAE        Selected
  0.0        NaN        NaN        NaN          
  0.5  0.6138797  0.3907678  0.3679739          
  1.0  0.5715029  0.4563617  0.3365540  *       

Tuning parameter 'eps.high' was held constant at a value of 0.5

Tuning parameter 'eps.low' was held constant at a value of 0
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were r.a = 1, eps.high = 0.5 and eps.low
 = 0.
[1] "Sat Mar 10 01:38:07 2018"
Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      0.4397052  0.6884930  0.2924733          
  2      0.3985398  0.7450389  0.2955116          
  3      0.3736333  0.7866164  0.2830753          
  4      0.3683432  0.7921746  0.2802696          
  5      0.3651038  0.7975320  0.2793702  *       
  6      0.3654336  0.7971712  0.2797140          
  7      0.3655176  0.7970652  0.2797475          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 5.
[1] "Sat Mar 10 01:38:23 2018"
Error in mvrValstats(object = object, estimate = "train") : 
  could not find function "mvrValstats"
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Loading required package: spikeslab
Loading required package: randomForest
randomForest 4.6-12
Type rfNews() to see new features/changes/bug fixes.

Attaching package: 'randomForest'

The following object is masked from 'package:ggplot2':

    margin

Loading required package: parallel

 spikeslab 1.1.5 
 
 Type spikeslab.news() to see new features, changes, and bug fixes. 
 

Spike and Slab Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  vars  RMSE       Rsquared   MAE        Selected
  1     0.3541196  0.8310144  0.2398500          
  2     0.3420275  0.8181221  0.2519443          
  3     0.3418728  0.8194281  0.2559887  *       
  4     0.3441392  0.8177867  0.2599826          
  5     0.3443533  0.8167551  0.2623368          
  6     0.3463712  0.8154098  0.2644015          
  7     0.3467550  0.8149628  0.2651017          
  8     0.3518039  0.8122647  0.2697550          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was vars = 3.
[1] "Sat Mar 10 01:38:53 2018"
Sparse Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  kappa       eta         K  RMSE       Rsquared   MAE        Selected
  0.05312657  0.87193966  4  0.3642811  0.7987187  0.2783610          
  0.06909645  0.25326900  4  0.3654080  0.7965088  0.2788861          
  0.08369418  0.09515384  2  0.4053186  0.7386290  0.2969896          
  0.11424429  0.45198606  8  0.3655379  0.7970568  0.2797678          
  0.12305597  0.55002844  1  0.3399338  0.8310144  0.2702119          
  0.16238612  0.65726952  1  0.3399338  0.8310144  0.2702119  *       
  0.17238999  0.94993106  3  0.3601024  0.8006157  0.2747499          
  0.20887952  0.46763193  7  0.3655354  0.7970590  0.2797699          
  0.22321283  0.26798984  7  0.3655354  0.7970590  0.2797699          
  0.26673726  0.68075005  7  0.3655255  0.7969899  0.2798978          
  0.27897979  0.97068916  4  0.3602043  0.8004761  0.2737310          
  0.30566800  0.05485076  1  0.4511501  0.6695027  0.3045620          
  0.30888957  0.19376378  9  0.3655383  0.7970557  0.2797667          
  0.32665077  0.07530099  2  0.4045173  0.7397338  0.2969352          
  0.33317507  0.89262666  4  0.3642811  0.7987187  0.2783610          
  0.37994954  0.59246012  6  0.3655207  0.7970300  0.2799478          
  0.39245697  0.15803436  4  0.3659344  0.7959031  0.2793520          
  0.40054216  0.66479223  8  0.3655379  0.7970568  0.2797678          
  0.42533278  0.62319889  4  0.3660820  0.7979580  0.2800961          
  0.43738262  0.09174417  5  0.3654408  0.7971429  0.2798838          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were K = 1, eta = 0.6572695 and kappa
 = 0.1623861.
[1] "Sat Mar 10 01:39:11 2018"
Supervised Principal Component Analysis 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  threshold  n.components  RMSE       Rsquared   MAE        Selected
  0.1062531  3             0.5722057  0.5234756  0.3703184          
  0.1381929  1             0.6868361  0.1388388  0.4059092          
  0.1673884  1             0.6868361  0.1388388  0.4059092          
  0.2284886  2             0.6034746  0.3633419  0.3545101          
  0.2461119  2             0.6034746  0.3633419  0.3545101          
  0.3247722  2             0.6116958  0.3272705  0.3687467          
  0.3447800  3             0.5808840  0.4868829  0.3925652          
  0.4177590  2             0.6064239  0.4031206  0.3734345          
  0.4464257  1             0.6721817  0.2615150  0.4144132          
  0.5334745  3             0.5667711  0.4850380  0.3672477          
  0.5579596  3             0.5718195  0.4691243  0.3768605          
  0.6113360  1             0.6661323  0.2730162  0.4105950          
  0.6177791  1             0.6661323  0.2730162  0.4105950          
  0.6533015  1             0.5974870  0.4213907  0.4040218          
  0.6663501  3             0.4572739  0.6453529  0.3065488  *       
  0.7598991  2             0.5500329  0.5138302  0.3600822          
  0.7849139  1             0.5974870  0.4213907  0.4040218          
  0.8010843  2             0.5450610  0.5123013  0.3538432          
  0.8506656  2             0.5450610  0.5123013  0.3538432          
  0.8747652  1             0.5747444  0.4432440  0.3733971          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were threshold = 0.6663501 and
 n.components = 3.
[1] "Sat Mar 10 01:39:28 2018"
Error : 'x' should be a character matrix with a single column for string kernel methods
In addition: Warning messages:
1: predictions failed for Fold3: threshold=0.6664, n.components=3 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
2: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:39:45 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "svmBoundrangeString"     
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:40:00 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "svmExpoString"           
Support Vector Machines with Linear Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  C             RMSE       Rsquared   MAE        Selected
    0.09432586  0.3856302  0.8229989  0.2069863          
    0.13147802  0.3757469  0.8236138  0.2067062          
    0.17810813  0.3695923  0.8256743  0.2073096          
    0.33618531  0.3602935  0.8312154  0.2085545          
    0.40379059  0.3597127  0.8307442  0.2112042          
    0.91483506  0.3571415  0.8308187  0.2124393          
    1.12638478  0.3571814  0.8307272  0.2128797          
    2.40558150  0.3561991  0.8297101  0.2135497          
    3.24087819  0.3562332  0.8296608  0.2135136          
    8.01175170  0.3553847  0.8293833  0.2139356          
   10.33447960  0.3553246  0.8293762  0.2141262          
   18.00143638  0.3552930  0.8294551  0.2139028          
   19.24867267  0.3552651  0.8294539  0.2138617          
   27.84841151  0.3553143  0.8295059  0.2139000          
   31.89486254  0.3552512  0.8295016  0.2138709  *       
   84.35994357  0.3552726  0.8295151  0.2138651          
  109.41821848  0.3553275  0.8294579  0.2138859          
  129.45122160  0.3552513  0.8295163  0.2138776          
  216.76432101  0.3552850  0.8295052  0.2138847          
  278.48937563  0.3552893  0.8295029  0.2138814          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was C = 31.89486.
[1] "Sat Mar 10 01:40:37 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Linear Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  cost          RMSE       Rsquared   MAE        Selected
    0.09432586  0.3856302  0.8229989  0.2069863          
    0.13147802  0.3757469  0.8236138  0.2067062          
    0.17810813  0.3695923  0.8256743  0.2073096          
    0.33618531  0.3602935  0.8312154  0.2085545          
    0.40379059  0.3597127  0.8307442  0.2112042          
    0.91483506  0.3571415  0.8308187  0.2124393          
    1.12638478  0.3571814  0.8307272  0.2128797          
    2.40558150  0.3561991  0.8297101  0.2135497          
    3.24087819  0.3562332  0.8296608  0.2135136          
    8.01175170  0.3553847  0.8293833  0.2139356          
   10.33447960  0.3553246  0.8293762  0.2141262          
   18.00143638  0.3552930  0.8294551  0.2139028          
   19.24867267  0.3552651  0.8294539  0.2138617          
   27.84841151  0.3553143  0.8295059  0.2139000          
   31.89486254  0.3552512  0.8295016  0.2138709  *       
   84.35994357  0.3552726  0.8295151  0.2138651          
  109.41821848  0.3553275  0.8294579  0.2138859          
  129.45122160  0.3552513  0.8295163  0.2138776          
  216.76432101  0.3552850  0.8295052  0.2138847          
  278.48937563  0.3552893  0.8295029  0.2138814          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cost = 31.89486.
[1] "Sat Mar 10 01:41:12 2018"
L2 Regularized Support Vector Machine (dual) with Linear Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  cost          Loss  RMSE       Rsquared   MAE        Selected
  4.259982e-03  L2    0.7202654  0.7001389  0.4294085          
  6.632912e-03  L1    0.7116155  0.5592696  0.4286934          
  9.942105e-03  L1    0.7056932  0.5884376  0.4249586          
  2.319201e-02  L1    0.6833696  0.6286721  0.4086907          
  2.961022e-02  L2    0.6889387  0.7139565  0.4036306          
  8.810971e-02  L2    0.6201342  0.7312303  0.3466070          
  1.162739e-01  L2    0.5982379  0.7284671  0.3276398          
  3.197865e-01  L1    0.4926371  0.6923574  0.2831800          
  4.758283e-01  L1    0.4614752  0.7074541  0.2735960          
  1.590511e+00  L2    0.3896090  0.8159657  0.2090571          
  2.233323e+00  L2    0.3793303  0.8158184  0.2103003          
  4.680687e+00  L1    0.3732177  0.7850324  0.2740371          
  5.118009e+00  L1    0.3720609  0.7867814  0.2751958          
  8.374661e+00  L1    0.3677771  0.7938902  0.2812397          
  1.003524e+01  L2    0.3627617  0.8195372  0.2161800          
  3.670696e+01  L2    0.3702258  0.8110258  0.2331087          
  5.192217e+01  L1    0.3649288  0.8035046  0.2920235          
  6.496930e+01  L2    0.3704550  0.8054752  0.2315490          
  1.291865e+02  L2    0.3617518  0.8048538  0.2525510  *       
  1.804312e+02  L1    0.3651163  0.8044900  0.2941938          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were cost = 129.1865 and Loss = L2.
[1] "Sat Mar 10 01:41:31 2018"
Support Vector Machines with Polynomial Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  degree  scale         C             RMSE       Rsquared   MAE        Selected
  1       3.194577e-05    0.24778671  0.7258984  0.6979904  0.4343787          
  1       2.200830e-04    2.48434089  0.7125948  0.7074123  0.4220515          
  1       2.488809e-03  106.77257440  0.3623565  0.8295408  0.2079766          
  1       8.236017e-03    0.03537227  0.7189971  0.7055029  0.4279689          
  1       3.049337e-02    0.04094571  0.6947690  0.7191271  0.4069011          
  1       4.189655e-01    1.08346647  0.3598652  0.8306278  0.2111952  *       
  2       1.953285e-05    0.04607641  0.7260463  0.6979915  0.4345141          
  2       2.507107e-05    0.16933548  0.7258845  0.6979918  0.4343660          
  2       1.064505e-04  619.42631922  0.3755254  0.8237910  0.2065770          
  2       2.634039e-04   71.54104347  0.4234214  0.8076277  0.2117746          
  2       3.012527e-03   33.00264776  0.3747007  0.8178789  0.2124710          
  2       4.061406e-02   59.27917160  0.6371277  0.5386950  0.4450672          
  2       5.393128e-01    1.42315466  0.7270391  0.4569418  0.5279615          
  2       1.085458e+00    0.41527643  0.7360012  0.4449146  0.5308361          
  2       1.398467e+00    1.55171234  0.7512742  0.4286858  0.5403367          
  3       3.064352e-05    4.67386774  0.7156239  0.7055065  0.4248135          
  3       6.882479e-05    1.78270660  0.7171277  0.7055112  0.4262204          
  3       1.382448e-02   16.49090237  0.3976677  0.7520197  0.2690287          
  3       2.011852e-02    1.25593690  0.3968890  0.7993318  0.2152442          
  3       3.342594e-02  103.83117368  0.5239286  0.6515522  0.3588497          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 1, scale = 0.4189655 and C
 = 1.083466.
[1] "Sat Mar 10 01:41:49 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Support Vector Machines with Radial Basis Function Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  sigma       C             RMSE       Rsquared   MAE        Selected
  0.01395401   29.48631178  0.4203855  0.7260819  0.2782034  *       
  0.01854516  716.54763206  0.5461569  0.6074561  0.3731523          
  0.03394392   10.92654911  0.4698242  0.6547412  0.3022235          
  0.04317095    0.05087363  0.6860078  0.6415364  0.3965502          
  0.06606244    0.09000063  0.6594239  0.5853127  0.3683124          
  0.07470796    0.31422927  0.5862317  0.5867019  0.3030477          
  0.13126817    1.26571398  0.5727413  0.4677711  0.3092056          
  0.13153337    1.53566181  0.5675309  0.4708272  0.3096927          
  0.13237573    1.28271368  0.5732908  0.4661016  0.3097653          
  0.13390271    0.07993210  0.6869537  0.4138446  0.3920782          
  0.17296522    0.03201652  0.7145060  0.3389332  0.4203062          
  0.18112683  804.58931184  0.5895529  0.4110960  0.3320294          
  0.18904021  685.35889919  0.5948048  0.4019390  0.3355272          
  0.19779361   81.50471503  0.6005618  0.3923109  0.3394680          
  0.20575227   39.62560263  0.6058137  0.3835082  0.3435738          
  0.20863339    0.05708286  0.7099666  0.2908609  0.4150992          
  0.21846208   14.07163675  0.6141580  0.3693435  0.3503572          
  0.24129954    9.73915432  0.6278084  0.3460857  0.3603570          
  0.26273236    0.22191871  0.6899768  0.2400926  0.3918663          
  0.26469419   27.48151824  0.6398341  0.3245516  0.3697397          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were sigma = 0.01395401 and C = 29.48631.
[1] "Sat Mar 10 01:42:07 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  C             RMSE       Rsquared   MAE        Selected
    0.09432586  0.6666756  0.5029994  0.3732433          
    0.13147802  0.6417240  0.5525712  0.3501226          
    0.17810813  0.6176301  0.5899356  0.3285765          
    0.33618531  0.5838101  0.5864250  0.3016683          
    0.40379059  0.5716261  0.5983299  0.2937375          
    0.91483506  0.5122926  0.6434208  0.2661581          
    1.12638478  0.5121077  0.6170031  0.2717882          
    2.40558150  0.5030084  0.5917610  0.2875638          
    3.24087819  0.5101808  0.5721919  0.2961691          
    8.01175170  0.5281641  0.5370902  0.3153274          
   10.33447960  0.5156760  0.5817020  0.3189957          
   18.00143638  0.5225503  0.5548901  0.3160198          
   19.24867267  0.5267651  0.5543033  0.3196518          
   27.84841151  0.5014909  0.5984631  0.3192664  *       
   31.89486254  0.5132350  0.5860333  0.3195306          
   84.35994357  0.5201842  0.5707091  0.3178701          
  109.41821848  0.5207420  0.5582319  0.3199885          
  129.45122160  0.5142277  0.5763967  0.3178172          
  216.76432101  0.5088752  0.5744468  0.3167521          
  278.48937563  0.5170957  0.5760855  0.3206576          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was C = 27.84841.
[1] "Sat Mar 10 01:42:25 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  sigma       C             RMSE       Rsquared   MAE        Selected
  0.01395401   29.48631178  0.4203855  0.7260819  0.2782034  *       
  0.01854516  716.54763206  0.5461569  0.6074561  0.3731523          
  0.03394392   10.92654911  0.4698242  0.6547412  0.3022235          
  0.04317095    0.05087363  0.6860078  0.6415364  0.3965502          
  0.06606244    0.09000063  0.6594239  0.5853127  0.3683124          
  0.07470796    0.31422927  0.5862317  0.5867019  0.3030477          
  0.13126817    1.26571398  0.5727413  0.4677711  0.3092056          
  0.13153337    1.53566181  0.5675309  0.4708272  0.3096927          
  0.13237573    1.28271368  0.5732908  0.4661016  0.3097653          
  0.13390271    0.07993210  0.6869537  0.4138446  0.3920782          
  0.17296522    0.03201652  0.7145060  0.3389332  0.4203062          
  0.18112683  804.58931184  0.5895529  0.4110960  0.3320294          
  0.18904021  685.35889919  0.5948048  0.4019390  0.3355272          
  0.19779361   81.50471503  0.6005618  0.3923109  0.3394680          
  0.20575227   39.62560263  0.6058137  0.3835082  0.3435738          
  0.20863339    0.05708286  0.7099666  0.2908609  0.4150992          
  0.21846208   14.07163675  0.6141580  0.3693435  0.3503572          
  0.24129954    9.73915432  0.6278084  0.3460857  0.3603570          
  0.26273236    0.22191871  0.6899768  0.2400926  0.3918663          
  0.26469419   27.48151824  0.6398341  0.3245516  0.3697397          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were sigma = 0.01395401 and C = 29.48631.
[1] "Sat Mar 10 01:42:43 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:43:00 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "svmSpectrumString"       
Bagged CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE       Rsquared   MAE      
  0.3744903  0.8009981  0.1942441

[1] "Sat Mar 10 01:43:18 2018"
Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      0.4397052  0.6884930  0.2924733          
  2      0.3985398  0.7450389  0.2955116          
  3      0.3736333  0.7866164  0.2830753          
  4      0.3683432  0.7921746  0.2802696          
  5      0.3651038  0.7975320  0.2793702  *       
  6      0.3654336  0.7971712  0.2797140          
  7      0.3655176  0.7970652  0.2797475          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 5.
[1] "Sat Mar 10 01:43:51 2018"
Error in mvrValstats(object = object, estimate = "train") : 
  could not find function "mvrValstats"
Error : package xgboost is required
Error : package xgboost is required
Error : package xgboost is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:44:07 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "xgbDART"                 
Error : package xgboost is required
Error : package xgboost is required
Error : package xgboost is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:44:24 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "xgbLinear"               
Error : package xgboost is required
Error : package xgboost is required
Error : package xgboost is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:44:40 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "xgbTree"                 
Error : package kohonen is required
Error : package kohonen is required
Error : package kohonen is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:44:56 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "range01"                  "HOPPER"                  
 [9] "14th20hp3cv"              "xyf"                     
Fitting Repeat 1 

# weights:  111
initial  value 72.309589 
iter  10 value 41.839869
iter  20 value 41.703200
final  value 41.702517 
converged
Fitting Repeat 2 

# weights:  111
initial  value 65.433888 
iter  10 value 41.748097
iter  20 value 41.702518
final  value 41.702517 
converged
Fitting Repeat 3 

# weights:  111
initial  value 56.430060 
iter  10 value 41.864709
final  value 41.829218 
converged
Fitting Repeat 4 

# weights:  111
initial  value 90.000414 
iter  10 value 42.070055
iter  20 value 41.829219
final  value 41.829218 
converged
Fitting Repeat 5 

# weights:  111
initial  value 65.000673 
iter  10 value 41.978551
iter  20 value 41.829219
final  value 41.829218 
converged
Fitting Repeat 1 

# weights:  56
initial  value 63.968540 
iter  10 value 39.417413
iter  20 value 39.267556
iter  20 value 39.267556
iter  20 value 39.267556
final  value 39.267556 
converged
Fitting Repeat 2 

# weights:  56
initial  value 62.073837 
iter  10 value 39.338451
final  value 39.267556 
converged
Fitting Repeat 3 

# weights:  56
initial  value 71.952477 
iter  10 value 39.503406
iter  20 value 39.267556
iter  20 value 39.267556
iter  20 value 39.267556
final  value 39.267556 
converged
Fitting Repeat 4 

# weights:  56
initial  value 63.215409 
iter  10 value 39.740322
iter  20 value 39.267556
iter  20 value 39.267556
iter  20 value 39.267556
final  value 39.267556 
converged
Fitting Repeat 5 

# weights:  56
initial  value 53.380660 
iter  10 value 39.300403
final  value 39.267556 
converged
Fitting Repeat 1 

# weights:  155
initial  value 54.204970 
iter  10 value 40.783768
iter  20 value 35.304787
iter  30 value 33.621199
iter  40 value 33.566216
iter  50 value 33.546999
iter  60 value 33.537404
iter  70 value 33.530725
iter  80 value 33.527680
iter  90 value 33.525496
iter 100 value 33.523541
final  value 33.523541 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  155
initial  value 54.883799 
iter  10 value 40.803274
iter  20 value 40.087204
iter  30 value 33.558334
iter  40 value 33.475650
iter  50 value 33.441170
iter  60 value 33.435063
iter  70 value 33.433124
iter  80 value 33.432614
iter  90 value 33.431735
iter 100 value 33.420989
final  value 33.420989 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  155
initial  value 52.069749 
iter  10 value 40.781046
iter  20 value 37.602294
iter  30 value 33.525063
iter  40 value 33.454706
iter  50 value 33.423093
iter  60 value 33.413409
iter  70 value 33.410491
iter  80 value 33.409244
iter  90 value 33.408384
iter 100 value 33.408057
final  value 33.408057 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  155
initial  value 46.728056 
iter  10 value 40.699906
iter  20 value 33.704330
iter  30 value 33.436411
iter  40 value 33.413590
iter  50 value 33.406400
iter  60 value 33.403588
iter  70 value 33.402020
iter  80 value 33.400764
iter  90 value 33.398692
iter 100 value 33.396735
final  value 33.396735 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  155
initial  value 67.585393 
iter  10 value 40.912744
iter  20 value 36.746884
iter  30 value 33.963640
iter  40 value 33.629096
iter  50 value 33.558998
iter  60 value 33.540489
iter  70 value 33.527759
iter  80 value 33.523793
iter  90 value 33.521337
iter 100 value 33.519930
final  value 33.519930 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  12
initial  value 113.499715 
iter  10 value 95.659287
iter  20 value 87.003589
iter  30 value 85.517720
iter  40 value 85.445107
iter  50 value 85.436491
iter  60 value 85.432373
final  value 85.432346 
converged
Fitting Repeat 2 

# weights:  12
initial  value 65.190599 
iter  10 value 40.814796
iter  20 value 40.577951
iter  30 value 39.205920
iter  40 value 39.082183
iter  50 value 39.055350
iter  60 value 39.052909
iter  70 value 39.052354
final  value 39.052348 
converged
Fitting Repeat 3 

# weights:  12
initial  value 45.220423 
iter  10 value 35.894645
iter  20 value 35.817358
iter  30 value 35.734477
iter  40 value 33.122648
iter  50 value 28.833840
iter  60 value 28.447330
iter  70 value 28.419906
iter  80 value 28.416183
iter  90 value 28.415132
iter  90 value 28.415132
final  value 28.415132 
converged
Fitting Repeat 4 

# weights:  12
initial  value 49.612039 
iter  10 value 36.942373
iter  20 value 32.944237
iter  30 value 31.126100
iter  40 value 30.076887
iter  50 value 29.867126
iter  60 value 29.686759
iter  70 value 29.554082
iter  80 value 29.523390
iter  90 value 29.513925
iter 100 value 29.512553
final  value 29.512553 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  12
initial  value 123.570441 
iter  10 value 80.503762
iter  20 value 75.654047
iter  30 value 70.782588
iter  40 value 70.386274
iter  50 value 70.324970
iter  60 value 70.322112
iter  70 value 70.321805
final  value 70.321799 
converged
Fitting Repeat 1 

# weights:  67
initial  value 46.420606 
iter  10 value 36.124894
iter  20 value 33.624082
iter  30 value 33.529471
iter  40 value 33.494233
iter  50 value 33.483143
iter  60 value 33.479847
iter  70 value 33.478198
iter  80 value 33.477277
iter  90 value 33.476662
iter 100 value 33.476134
final  value 33.476134 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  67
initial  value 61.002164 
iter  10 value 40.771447
iter  20 value 40.650561
iter  30 value 37.027830
iter  40 value 33.771761
iter  50 value 33.636155
iter  60 value 33.576594
iter  70 value 33.549902
iter  80 value 33.529717
iter  90 value 33.511943
iter 100 value 33.474139
final  value 33.474139 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  67
initial  value 59.015120 
iter  10 value 40.772879
iter  20 value 34.413168
iter  30 value 33.636939
iter  40 value 33.524403
iter  50 value 33.486499
iter  60 value 33.480422
iter  70 value 33.478182
iter  80 value 33.477178
iter  90 value 33.476706
iter 100 value 33.476548
final  value 33.476548 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  67
initial  value 67.253794 
iter  10 value 40.782274
iter  20 value 36.898681
iter  30 value 33.594510
iter  40 value 33.528170
iter  50 value 33.498297
iter  60 value 33.487707
iter  70 value 33.482124
iter  80 value 33.480710
iter  90 value 33.479413
iter 100 value 33.478914
final  value 33.478914 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  67
initial  value 63.304979 
iter  10 value 40.762472
iter  20 value 34.262988
iter  30 value 33.570623
iter  40 value 33.518525
iter  50 value 33.498696
iter  60 value 33.459311
iter  70 value 33.427633
iter  80 value 33.415378
iter  90 value 33.411870
iter 100 value 33.411019
final  value 33.411019 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  210
initial  value 72.907839 
iter  10 value 40.686685
iter  20 value 40.682730
iter  30 value 40.681596
iter  40 value 40.679047
iter  50 value 40.669995
iter  60 value 40.513459
iter  70 value 33.736041
iter  80 value 33.304235
iter  90 value 33.270118
iter 100 value 33.238140
final  value 33.238140 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  210
initial  value 71.838345 
iter  10 value 40.686956
iter  20 value 40.685313
iter  30 value 40.682700
iter  40 value 40.681378
iter  50 value 40.678131
iter  60 value 40.664085
iter  70 value 40.152261
iter  80 value 33.510767
iter  90 value 33.290041
iter 100 value 33.249954
final  value 33.249954 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  210
initial  value 67.442777 
iter  10 value 40.686725
iter  20 value 40.683376
iter  30 value 40.679616
iter  40 value 40.664211
iter  50 value 35.373712
iter  60 value 33.285253
iter  70 value 33.238171
iter  80 value 33.163373
iter  90 value 33.154267
iter 100 value 33.152694
final  value 33.152694 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  210
initial  value 52.521985 
iter  10 value 40.685618
iter  20 value 40.685084
iter  30 value 40.684987
iter  40 value 40.684883
iter  50 value 40.684766
iter  60 value 40.684631
iter  70 value 40.684470
iter  80 value 40.684269
iter  90 value 40.684008
iter 100 value 40.683645
final  value 40.683645 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  210
initial  value 46.247860 
iter  10 value 40.684420
iter  20 value 40.683784
iter  30 value 40.682827
iter  40 value 40.681046
iter  50 value 40.676299
iter  60 value 40.641113
iter  70 value 33.643830
iter  80 value 33.261462
iter  90 value 33.210898
iter 100 value 33.166026
final  value 33.166026 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  166
initial  value 50.402941 
iter  10 value 36.468680
iter  20 value 36.360756
iter  30 value 30.986804
iter  40 value 30.611000
iter  50 value 30.587098
iter  60 value 30.577411
iter  70 value 30.572306
iter  80 value 30.570754
iter  90 value 30.570374
iter 100 value 30.570045
final  value 30.570045 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 20.503861 
iter  10 value 11.810698
iter  20 value 11.807892
iter  30 value 11.802914
iter  40 value 11.785820
iter  50 value 8.599356
iter  60 value 4.242817
iter  70 value 4.041473
iter  80 value 3.987770
iter  90 value 3.971498
iter 100 value 3.960368
final  value 3.960368 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  166
initial  value 36.499235 
iter  10 value 11.970471
iter  20 value 7.993887
iter  30 value 5.095459
iter  40 value 5.021491
iter  50 value 4.999447
iter  60 value 4.990997
iter  70 value 4.983133
iter  80 value 4.966748
iter  90 value 4.962110
iter 100 value 4.958895
final  value 4.958895 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  166
initial  value 20.596129 
iter  10 value 19.832496
iter  20 value 19.822986
iter  30 value 19.125792
iter  40 value 9.994715
iter  50 value 9.848026
iter  60 value 9.818659
iter  70 value 9.808938
iter  80 value 9.805427
iter  90 value 9.803684
iter 100 value 9.802246
final  value 9.802246 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 83.826702 
iter  10 value 37.806366
iter  20 value 37.728426
iter  30 value 37.395927
iter  40 value 33.680068
iter  50 value 33.545101
iter  60 value 33.509774
iter  70 value 33.493537
iter  80 value 33.487315
iter  90 value 33.485209
iter 100 value 33.484368
final  value 33.484368 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  23
initial  value 107.937276 
iter  10 value 76.469437
iter  20 value 76.450284
final  value 76.450279 
converged
Fitting Repeat 2 

# weights:  23
initial  value 55.127892 
iter  10 value 35.632219
final  value 35.622593 
converged
Fitting Repeat 3 

# weights:  23
initial  value 88.963874 
iter  10 value 69.217045
final  value 69.196344 
converged
Fitting Repeat 4 

# weights:  23
initial  value 46.786091 
iter  10 value 15.354730
final  value 15.347843 
converged
Fitting Repeat 5 

# weights:  23
initial  value 76.971320 
iter  10 value 64.383744
iter  20 value 64.365843
iter  20 value 64.365843
iter  20 value 64.365843
final  value 64.365843 
converged
Fitting Repeat 1 

# weights:  67
initial  value 45.090116 
iter  10 value 19.014909
iter  20 value 16.075456
iter  30 value 15.970176
iter  40 value 15.966561
iter  50 value 15.966094
iter  60 value 15.966073
final  value 15.966069 
converged
Fitting Repeat 2 

# weights:  67
initial  value 42.764929 
iter  10 value 35.287414
iter  20 value 33.086549
iter  30 value 33.066395
iter  40 value 33.065910
iter  50 value 33.065805
final  value 33.065801 
converged
Fitting Repeat 3 

# weights:  67
initial  value 48.700547 
iter  10 value 41.006473
iter  20 value 34.968411
iter  30 value 34.916424
iter  40 value 34.909187
iter  50 value 34.907821
iter  60 value 34.906390
iter  70 value 34.905525
iter  80 value 34.905379
iter  90 value 34.905255
iter 100 value 34.902319
final  value 34.902319 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  67
initial  value 156.939471 
iter  10 value 97.077359
iter  20 value 86.225884
iter  30 value 85.540898
iter  40 value 85.506534
iter  50 value 85.488211
iter  60 value 85.481517
iter  70 value 85.477132
iter  80 value 85.475866
iter  90 value 85.474362
iter 100 value 85.473897
final  value 85.473897 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  67
initial  value 78.704449 
iter  10 value 47.803558
iter  20 value 42.995935
iter  30 value 42.457065
iter  40 value 42.449232
iter  50 value 42.447505
iter  60 value 42.441094
iter  70 value 42.439774
iter  80 value 42.439654
iter  90 value 42.439591
iter 100 value 42.439568
final  value 42.439568 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  111
initial  value 44.369506 
final  value 32.403212 
converged
Fitting Repeat 2 

# weights:  111
initial  value 84.853959 
iter  10 value 45.837510
iter  20 value 45.837086
iter  30 value 45.837080
iter  40 value 45.837073
iter  50 value 45.837067
iter  60 value 45.837059
iter  70 value 45.837052
iter  80 value 45.837044
iter  90 value 45.837036
iter 100 value 45.837027
final  value 45.837027 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  111
initial  value 40.336374 
final  value 37.426997 
converged
Fitting Repeat 4 

# weights:  111
initial  value 33.054598 
iter  10 value 12.003644
iter  20 value 11.806766
iter  30 value 9.628396
iter  40 value 9.528678
iter  50 value 9.520353
iter  60 value 9.519295
iter  70 value 9.518656
iter  80 value 9.518164
iter  90 value 9.517979
iter 100 value 9.517784
final  value 9.517784 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  111
initial  value 108.431465 
final  value 95.979628 
converged
Fitting Repeat 1 

# weights:  199
initial  value 72.588595 
iter  10 value 40.696630
iter  20 value 40.659373
iter  30 value 34.292918
iter  40 value 33.292742
iter  50 value 33.221867
iter  60 value 33.192858
iter  70 value 33.185835
iter  80 value 33.183359
iter  90 value 33.181936
iter 100 value 33.179533
final  value 33.179533 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 70.400645 
iter  10 value 40.700863
iter  20 value 40.570806
iter  30 value 33.734612
iter  40 value 33.323150
iter  50 value 33.296309
iter  60 value 33.265345
iter  70 value 33.242808
iter  80 value 33.230451
iter  90 value 33.217921
iter 100 value 33.203544
final  value 33.203544 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 46.640431 
iter  10 value 40.689084
iter  20 value 40.688977
iter  30 value 40.688856
iter  40 value 40.688718
iter  50 value 40.688556
iter  60 value 40.688360
iter  70 value 40.688113
iter  80 value 40.687789
iter  90 value 40.687334
iter 100 value 40.686634
final  value 40.686634 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 59.718169 
iter  10 value 40.695901
iter  20 value 37.912652
iter  30 value 33.462035
iter  40 value 33.315466
iter  50 value 33.243527
iter  60 value 33.208059
iter  70 value 33.201207
iter  80 value 33.192456
iter  90 value 33.186996
iter 100 value 33.185918
final  value 33.185918 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 67.862117 
iter  10 value 40.698211
iter  20 value 40.683377
iter  30 value 40.678696
iter  40 value 40.648855
iter  50 value 35.496514
iter  60 value 33.318770
iter  70 value 33.258087
iter  80 value 33.214281
iter  90 value 33.188123
iter 100 value 33.182346
final  value 33.182346 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  34
initial  value 113.172132 
iter  10 value 61.000850
iter  20 value 60.971973
final  value 60.971971 
converged
Fitting Repeat 2 

# weights:  34
initial  value 149.416002 
iter  10 value 87.310481
final  value 87.285609 
converged
Fitting Repeat 3 

# weights:  34
initial  value 70.842735 
iter  10 value 35.700725
iter  20 value 35.502837
final  value 35.502830 
converged
Fitting Repeat 4 

# weights:  34
initial  value 99.765004 
iter  10 value 65.973703
final  value 65.972401 
converged
Fitting Repeat 5 

# weights:  34
initial  value 78.610899 
iter  10 value 41.348762
iter  20 value 41.244038
final  value 41.244036 
converged
Fitting Repeat 1 

# weights:  12
initial  value 139.529938 
iter  10 value 119.205460
final  value 119.199777 
converged
Fitting Repeat 2 

# weights:  12
initial  value 24.192330 
iter  10 value 17.085327
final  value 17.085325 
converged
Fitting Repeat 3 

# weights:  12
initial  value 56.221237 
iter  10 value 38.907006
final  value 38.906416 
converged
Fitting Repeat 4 

# weights:  12
initial  value 27.918262 
iter  10 value 16.568893
final  value 16.568870 
converged
Fitting Repeat 5 

# weights:  12
initial  value 128.184063 
iter  10 value 104.248435
final  value 104.137003 
converged
Fitting Repeat 1 

# weights:  221
initial  value 60.282617 
iter  10 value 40.782290
iter  20 value 34.379177
iter  30 value 34.178368
iter  40 value 34.159374
iter  50 value 34.157509
iter  60 value 34.157283
iter  70 value 34.157257
final  value 34.157252 
converged
Fitting Repeat 2 

# weights:  221
initial  value 56.450726 
iter  10 value 40.666592
iter  20 value 34.807904
iter  30 value 34.343076
iter  40 value 34.302034
iter  50 value 34.299633
iter  60 value 34.298560
iter  70 value 34.298150
iter  80 value 34.298064
iter  90 value 34.298036
final  value 34.298033 
converged
Fitting Repeat 3 

# weights:  221
initial  value 80.230809 
iter  10 value 40.975527
iter  20 value 39.017276
iter  30 value 34.498446
iter  40 value 34.178137
iter  50 value 34.132662
iter  60 value 34.125101
iter  70 value 34.123480
iter  80 value 34.122376
iter  90 value 34.120412
iter 100 value 34.118238
final  value 34.118238 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  221
initial  value 51.580674 
iter  10 value 40.709909
iter  20 value 35.288898
iter  30 value 34.199888
iter  40 value 34.131996
iter  50 value 34.113204
iter  60 value 34.110296
iter  70 value 34.110079
iter  80 value 34.110061
final  value 34.110060 
converged
Fitting Repeat 5 

# weights:  221
initial  value 55.679544 
iter  10 value 40.731815
iter  20 value 34.304561
iter  30 value 34.154759
iter  40 value 34.137275
iter  50 value 34.132629
iter  60 value 34.132024
iter  70 value 34.131983
final  value 34.131981 
converged
Fitting Repeat 1 

# weights:  166
initial  value 123.218724 
iter  10 value 73.257628
iter  20 value 68.137805
iter  30 value 67.978410
iter  40 value 67.976851
final  value 67.976769 
converged
Fitting Repeat 2 

# weights:  166
initial  value 49.254381 
iter  10 value 35.961966
iter  20 value 34.632093
iter  30 value 34.622071
iter  40 value 34.550282
iter  50 value 34.474625
iter  60 value 34.461140
iter  70 value 34.457446
iter  80 value 34.455258
iter  90 value 34.455060
final  value 34.455050 
converged
Fitting Repeat 3 

# weights:  166
initial  value 98.697590 
iter  10 value 74.904449
iter  20 value 70.554620
iter  30 value 70.490936
iter  40 value 70.489233
iter  50 value 70.488747
iter  60 value 70.488572
iter  70 value 70.488430
iter  80 value 70.488253
iter  90 value 70.488165
final  value 70.488158 
converged
Fitting Repeat 4 

# weights:  166
initial  value 55.769293 
iter  10 value 36.469885
iter  20 value 27.672072
iter  30 value 27.609986
iter  40 value 27.607933
iter  50 value 27.606580
iter  60 value 27.367316
iter  70 value 27.206246
iter  80 value 27.196583
iter  90 value 27.192781
iter 100 value 27.192694
final  value 27.192694 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 30.509665 
iter  10 value 23.987002
iter  20 value 16.266361
iter  30 value 16.049295
iter  40 value 16.024931
iter  50 value 16.023355
iter  60 value 16.022890
iter  70 value 16.021544
iter  80 value 16.021136
iter  90 value 16.021111
final  value 16.021110 
converged
Fitting Repeat 1 

# weights:  89
initial  value 68.781937 
iter  10 value 56.828989
iter  20 value 45.772909
iter  30 value 44.920630
iter  40 value 44.868083
iter  50 value 44.853410
iter  60 value 44.848723
iter  70 value 44.845121
iter  80 value 44.842688
iter  90 value 44.841847
iter 100 value 44.841558
final  value 44.841558 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 32.546615 
iter  10 value 25.317774
iter  20 value 15.678836
iter  30 value 15.135347
iter  40 value 15.022430
iter  50 value 14.993595
iter  60 value 14.989887
iter  70 value 14.988614
iter  80 value 14.987813
iter  90 value 14.987337
iter 100 value 14.986778
final  value 14.986778 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  89
initial  value 46.917826 
iter  10 value 13.785534
iter  20 value 10.166201
iter  30 value 9.336055
iter  40 value 9.312594
iter  50 value 9.304771
iter  60 value 9.301911
iter  70 value 9.301243
iter  80 value 9.300982
iter  90 value 9.300668
iter 100 value 9.300466
final  value 9.300466 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 87.124157 
iter  10 value 41.822623
iter  20 value 35.160080
iter  30 value 32.005541
iter  40 value 31.819018
iter  50 value 31.800708
iter  60 value 31.790548
iter  70 value 31.786051
iter  80 value 31.784459
iter  90 value 31.783941
iter 100 value 31.783745
final  value 31.783745 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 42.521426 
iter  10 value 33.236214
iter  20 value 24.433029
iter  30 value 23.468071
iter  40 value 23.441354
iter  50 value 23.438082
iter  60 value 23.436692
iter  70 value 23.433565
iter  80 value 23.432740
iter  90 value 23.432048
iter 100 value 23.431920
final  value 23.431920 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  23
initial  value 55.244079 
iter  10 value 40.909651
iter  20 value 35.974794
iter  30 value 34.250691
iter  40 value 34.189217
iter  50 value 34.178313
iter  60 value 34.177816
final  value 34.177798 
converged
Fitting Repeat 2 

# weights:  23
initial  value 66.378154 
iter  10 value 41.021432
iter  20 value 34.660557
iter  30 value 34.204709
iter  40 value 34.179196
iter  50 value 34.176981
iter  60 value 34.176710
final  value 34.176707 
converged
Fitting Repeat 3 

# weights:  23
initial  value 67.440194 
iter  10 value 40.890128
iter  20 value 36.165879
iter  30 value 34.845903
iter  40 value 34.305092
iter  50 value 34.187708
iter  60 value 34.178252
iter  70 value 34.177821
iter  80 value 34.177790
final  value 34.177788 
converged
Fitting Repeat 4 

# weights:  23
initial  value 55.386653 
iter  10 value 40.701285
iter  20 value 34.490228
iter  30 value 34.202185
iter  40 value 34.182875
iter  50 value 34.179180
iter  60 value 34.177935
iter  70 value 34.177604
iter  80 value 34.177575
final  value 34.177572 
converged
Fitting Repeat 5 

# weights:  23
initial  value 68.988437 
iter  10 value 40.944664
iter  20 value 36.334097
iter  30 value 34.253109
iter  40 value 34.190258
iter  50 value 34.180212
iter  60 value 34.178652
iter  70 value 34.177646
iter  80 value 34.177575
final  value 34.177573 
converged
Fitting Repeat 1 

# weights:  100
initial  value 50.249609 
iter  10 value 37.055290
iter  20 value 36.828006
final  value 36.827927 
converged
Fitting Repeat 2 

# weights:  100
initial  value 51.795526 
iter  10 value 38.444247
iter  20 value 37.302916
iter  30 value 37.302234
iter  30 value 37.302233
iter  30 value 37.302233
final  value 37.302233 
converged
Fitting Repeat 3 

# weights:  100
initial  value 84.743185 
iter  10 value 37.946473
iter  20 value 37.302324
final  value 37.302233 
converged
Fitting Repeat 4 

# weights:  100
initial  value 70.360949 
iter  10 value 38.042572
iter  20 value 37.302246
final  value 37.302233 
converged
Fitting Repeat 5 

# weights:  100
initial  value 68.785219 
iter  10 value 40.025053
iter  20 value 37.302762
final  value 37.302233 
converged
Fitting Repeat 1 

# weights:  144
initial  value 80.069479 
iter  10 value 40.757115
iter  20 value 37.176029
iter  30 value 33.462262
iter  40 value 33.409979
iter  50 value 33.389910
iter  60 value 33.369122
iter  70 value 33.357894
iter  80 value 33.353703
iter  90 value 33.351443
iter 100 value 33.350514
final  value 33.350514 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  144
initial  value 64.712579 
iter  10 value 40.763013
iter  20 value 36.766661
iter  30 value 33.545822
iter  40 value 33.410957
iter  50 value 33.378459
iter  60 value 33.362221
iter  70 value 33.353605
iter  80 value 33.349615
iter  90 value 33.348646
iter 100 value 33.347836
final  value 33.347836 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  144
initial  value 53.433995 
iter  10 value 40.732288
iter  20 value 40.669245
iter  30 value 34.108464
iter  40 value 33.389761
iter  50 value 33.321050
iter  60 value 33.306550
iter  70 value 33.302909
iter  80 value 33.300945
iter  90 value 33.300460
iter 100 value 33.300142
final  value 33.300142 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  144
initial  value 59.146046 
iter  10 value 40.746631
iter  20 value 36.410587
iter  30 value 33.490479
iter  40 value 33.410694
iter  50 value 33.393243
iter  60 value 33.368393
iter  70 value 33.357137
iter  80 value 33.351345
iter  90 value 33.349788
iter 100 value 33.348745
final  value 33.348745 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  144
initial  value 68.008774 
iter  10 value 40.792405
iter  20 value 40.690057
iter  30 value 37.512416
iter  40 value 33.509238
iter  50 value 33.426216
iter  60 value 33.380185
iter  70 value 33.365414
iter  80 value 33.357076
iter  90 value 33.347221
iter 100 value 33.324149
final  value 33.324149 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  34
initial  value 62.821443 
iter  10 value 41.212468
final  value 41.210390 
converged
Fitting Repeat 2 

# weights:  34
initial  value 63.889944 
iter  10 value 41.213504
final  value 41.210390 
converged
Fitting Repeat 3 

# weights:  34
initial  value 57.405763 
iter  10 value 41.212119
final  value 41.210390 
converged
Fitting Repeat 4 

# weights:  34
initial  value 67.028789 
iter  10 value 41.212493
final  value 41.210390 
converged
Fitting Repeat 5 

# weights:  34
initial  value 52.528612 
iter  10 value 41.214475
final  value 41.210390 
converged
Fitting Repeat 1 

# weights:  111
initial  value 59.232656 
iter  10 value 30.680722
final  value 30.550073 
converged
Fitting Repeat 2 

# weights:  111
initial  value 56.547951 
iter  10 value 30.780502
final  value 30.550073 
converged
Fitting Repeat 3 

# weights:  111
initial  value 68.350992 
iter  10 value 30.634702
iter  20 value 30.550074
iter  20 value 30.550073
iter  20 value 30.550073
final  value 30.550073 
converged
Fitting Repeat 4 

# weights:  111
initial  value 58.657234 
iter  10 value 30.750814
final  value 30.550073 
converged
Fitting Repeat 5 

# weights:  111
initial  value 69.253393 
iter  10 value 30.682107
iter  20 value 30.550074
final  value 30.550073 
converged
Fitting Repeat 1 

# weights:  56
initial  value 42.962113 
iter  10 value 28.231194
final  value 28.214874 
converged
Fitting Repeat 2 

# weights:  56
initial  value 45.102572 
iter  10 value 28.319268
iter  20 value 28.214874
iter  20 value 28.214874
iter  20 value 28.214874
final  value 28.214874 
converged
Fitting Repeat 3 

# weights:  56
initial  value 40.574102 
iter  10 value 28.219345
final  value 28.214874 
converged
Fitting Repeat 4 

# weights:  56
initial  value 56.276228 
iter  10 value 28.363836
final  value 28.214874 
converged
Fitting Repeat 5 

# weights:  56
initial  value 60.718399 
iter  10 value 28.416964
iter  20 value 28.214874
iter  20 value 28.214874
iter  20 value 28.214874
final  value 28.214874 
converged
Fitting Repeat 1 

# weights:  155
initial  value 41.025723 
iter  10 value 29.454419
iter  20 value 26.863137
iter  30 value 22.760347
iter  40 value 22.659969
iter  50 value 22.630716
iter  60 value 22.617672
iter  70 value 22.610216
iter  80 value 22.607249
iter  90 value 22.605583
iter 100 value 22.602493
final  value 22.602493 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  155
initial  value 65.554318 
iter  10 value 29.495521
iter  20 value 29.398403
iter  30 value 28.811593
iter  40 value 22.856653
iter  50 value 22.756727
iter  60 value 22.729420
iter  70 value 22.719698
iter  80 value 22.714296
iter  90 value 22.712260
iter 100 value 22.709777
final  value 22.709777 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  155
initial  value 49.416801 
iter  10 value 29.533183
iter  20 value 25.132824
iter  30 value 22.780655
iter  40 value 22.735916
iter  50 value 22.725400
iter  60 value 22.720876
iter  70 value 22.717373
iter  80 value 22.715811
iter  90 value 22.715147
iter 100 value 22.714543
final  value 22.714543 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  155
initial  value 51.933804 
iter  10 value 29.574726
iter  20 value 28.028451
iter  30 value 23.101701
iter  40 value 22.824796
iter  50 value 22.754976
iter  60 value 22.733077
iter  70 value 22.722108
iter  80 value 22.711916
iter  90 value 22.681076
iter 100 value 22.649165
final  value 22.649165 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  155
initial  value 32.043475 
iter  10 value 29.398286
iter  20 value 23.613248
iter  30 value 22.644692
iter  40 value 22.611870
iter  50 value 22.599066
iter  60 value 22.590264
iter  70 value 22.586632
iter  80 value 22.585448
iter  90 value 22.584946
iter 100 value 22.584753
final  value 22.584753 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  12
initial  value 34.322825 
iter  10 value 30.036726
iter  20 value 30.013720
iter  30 value 29.978458
iter  40 value 29.885532
iter  50 value 24.922858
iter  60 value 20.047605
iter  70 value 19.822838
iter  80 value 19.789396
iter  90 value 19.771106
final  value 19.770670 
converged
Fitting Repeat 2 

# weights:  12
initial  value 48.513269 
iter  10 value 33.102821
iter  20 value 29.981908
iter  30 value 29.816951
iter  40 value 28.545250
iter  50 value 28.080627
iter  60 value 27.998017
iter  70 value 27.995177
iter  80 value 27.994921
iter  90 value 27.994899
iter  90 value 27.994899
iter  90 value 27.994899
final  value 27.994899 
converged
Fitting Repeat 3 

# weights:  12
initial  value 75.782398 
iter  10 value 41.835690
iter  20 value 41.727323
iter  30 value 32.875191
iter  40 value 32.368007
iter  50 value 32.289025
iter  60 value 32.279415
final  value 32.279132 
converged
Fitting Repeat 4 

# weights:  12
initial  value 59.753991 
iter  10 value 36.083795
iter  20 value 35.965666
iter  30 value 35.392823
iter  40 value 33.440367
iter  50 value 33.417043
iter  60 value 33.414079
iter  70 value 33.413990
final  value 33.413988 
converged
Fitting Repeat 5 

# weights:  12
initial  value 56.536490 
iter  10 value 38.423095
iter  20 value 34.224296
iter  30 value 33.554220
iter  40 value 33.485685
final  value 33.485499 
converged
Fitting Repeat 1 

# weights:  67
initial  value 34.102537 
iter  10 value 29.299327
iter  20 value 22.846287
iter  30 value 22.652169
iter  40 value 22.618516
iter  50 value 22.610907
iter  60 value 22.604243
iter  70 value 22.599372
iter  80 value 22.597732
iter  90 value 22.596615
iter 100 value 22.596123
final  value 22.596123 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  67
initial  value 59.638678 
iter  10 value 29.480077
iter  20 value 25.217670
iter  30 value 22.849943
iter  40 value 22.743180
iter  50 value 22.691807
iter  60 value 22.678967
iter  70 value 22.674532
iter  80 value 22.670002
iter  90 value 22.666409
iter 100 value 22.665336
final  value 22.665336 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  67
initial  value 36.048870 
iter  10 value 29.347163
iter  20 value 23.092140
iter  30 value 22.664598
iter  40 value 22.623710
iter  50 value 22.614771
iter  60 value 22.607549
iter  70 value 22.597453
iter  80 value 22.595046
iter  90 value 22.594312
iter 100 value 22.594057
final  value 22.594057 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  67
initial  value 52.806076 
iter  10 value 29.471174
iter  20 value 29.030592
iter  30 value 22.798007
iter  40 value 22.658916
iter  50 value 22.624337
iter  60 value 22.603204
iter  70 value 22.596718
iter  80 value 22.594959
iter  90 value 22.594300
iter 100 value 22.594105
final  value 22.594105 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  67
initial  value 39.917249 
iter  10 value 29.427158
iter  20 value 23.382451
iter  30 value 22.726496
iter  40 value 22.690553
iter  50 value 22.681135
iter  60 value 22.674733
iter  70 value 22.672954
iter  80 value 22.672140
iter  90 value 22.671592
iter 100 value 22.671441
final  value 22.671441 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  210
initial  value 71.813362 
iter  10 value 29.370090
iter  20 value 29.369251
iter  30 value 29.369245
iter  40 value 29.369239
iter  50 value 29.369233
iter  60 value 29.369227
iter  70 value 29.369220
iter  80 value 29.369213
iter  90 value 29.369206
iter 100 value 29.369198
final  value 29.369198 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  210
initial  value 48.002139 
iter  10 value 29.370588
iter  20 value 28.362757
iter  30 value 23.544076
iter  40 value 22.622540
iter  50 value 22.441766
iter  60 value 22.397045
iter  70 value 22.352768
iter  80 value 22.349782
iter  90 value 22.348429
iter 100 value 22.347302
final  value 22.347302 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  210
initial  value 66.380145 
iter  10 value 29.370119
iter  20 value 29.369040
iter  30 value 29.369021
iter  40 value 29.369000
iter  50 value 29.368976
iter  60 value 29.368951
iter  70 value 29.368923
iter  80 value 29.368892
iter  90 value 29.368857
iter 100 value 29.368817
final  value 29.368817 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  210
initial  value 36.255787 
iter  10 value 29.368940
iter  20 value 29.272391
iter  30 value 22.725100
iter  40 value 22.433029
iter  50 value 22.398990
iter  60 value 22.377902
iter  70 value 22.376886
iter  80 value 22.359379
iter  90 value 22.351809
iter 100 value 22.350415
final  value 22.350415 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  210
initial  value 42.325333 
iter  10 value 29.369553
final  value 29.369343 
converged
Fitting Repeat 1 

# weights:  166
initial  value 38.712240 
iter  10 value 27.012253
iter  20 value 19.194451
iter  30 value 18.867859
iter  40 value 18.821532
iter  50 value 18.810069
iter  60 value 18.808333
iter  70 value 18.806538
iter  80 value 18.799219
iter  90 value 18.785718
iter 100 value 18.777643
final  value 18.777643 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 42.631863 
iter  10 value 25.854412
iter  20 value 23.259581
iter  30 value 21.126811
iter  40 value 21.107886
iter  50 value 21.095536
iter  60 value 21.085866
iter  70 value 21.076880
iter  80 value 21.064940
iter  90 value 21.061715
iter 100 value 21.059545
final  value 21.059545 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  166
initial  value 43.638585 
iter  10 value 27.006680
iter  20 value 17.625582
iter  30 value 16.320598
iter  40 value 16.249658
iter  50 value 16.212326
iter  60 value 16.201576
iter  70 value 16.186242
iter  80 value 16.181933
iter  90 value 16.177485
iter 100 value 16.174512
final  value 16.174512 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  166
initial  value 65.491912 
iter  10 value 36.624069
iter  20 value 35.576326
iter  30 value 29.006420
iter  40 value 28.885887
iter  50 value 28.786596
iter  60 value 28.758963
iter  70 value 28.747136
iter  80 value 28.736442
iter  90 value 28.731225
iter 100 value 28.728057
final  value 28.728057 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 117.291888 
iter  10 value 50.116958
iter  20 value 45.913562
iter  30 value 44.213912
iter  40 value 44.186010
iter  50 value 44.175185
iter  60 value 44.167510
iter  70 value 44.164002
iter  80 value 44.162153
iter  90 value 44.160777
iter 100 value 44.159240
final  value 44.159240 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  23
initial  value 93.808797 
iter  10 value 52.864944
iter  20 value 52.843533
iter  20 value 52.843533
iter  20 value 52.843533
final  value 52.843533 
converged
Fitting Repeat 2 

# weights:  23
initial  value 78.827166 
iter  10 value 29.499832
final  value 29.487436 
converged
Fitting Repeat 3 

# weights:  23
initial  value 48.210825 
iter  10 value 33.199346
iter  20 value 33.190747
iter  20 value 33.190747
iter  20 value 33.190747
final  value 33.190747 
converged
Fitting Repeat 4 

# weights:  23
initial  value 58.490314 
iter  10 value 34.735398
iter  20 value 34.726144
iter  20 value 34.726144
iter  20 value 34.726144
final  value 34.726144 
converged
Fitting Repeat 5 

# weights:  23
initial  value 63.062898 
iter  10 value 49.040568
final  value 49.033434 
converged
Fitting Repeat 1 

# weights:  67
initial  value 68.202057 
iter  10 value 43.587393
iter  20 value 40.188164
iter  30 value 39.413902
iter  40 value 39.400968
iter  50 value 39.400102
iter  60 value 39.399940
final  value 39.399930 
converged
Fitting Repeat 2 

# weights:  67
initial  value 48.873714 
iter  10 value 29.992586
iter  20 value 18.708604
iter  30 value 18.040417
iter  40 value 18.024489
iter  50 value 18.016635
iter  60 value 18.015472
iter  70 value 18.015325
iter  80 value 18.015299
iter  90 value 18.015290
final  value 18.015289 
converged
Fitting Repeat 3 

# weights:  67
initial  value 61.131373 
iter  10 value 40.173349
iter  20 value 31.902935
iter  30 value 30.739431
iter  40 value 30.722032
iter  50 value 30.714911
iter  60 value 30.713931
iter  70 value 30.713791
iter  80 value 30.713612
iter  90 value 30.713315
iter 100 value 30.713206
final  value 30.713206 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  67
initial  value 58.053110 
iter  10 value 28.243732
iter  20 value 24.017509
iter  30 value 22.861850
iter  40 value 22.837268
iter  50 value 22.836096
iter  60 value 22.835917
iter  70 value 22.835874
final  value 22.835869 
converged
Fitting Repeat 5 

# weights:  67
initial  value 74.548805 
iter  10 value 42.787929
iter  20 value 34.667352
iter  30 value 32.705673
iter  40 value 32.272623
iter  50 value 32.222989
iter  60 value 32.217748
iter  70 value 32.217251
iter  80 value 32.217170
iter  90 value 32.217103
iter 100 value 32.217065
final  value 32.217065 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  111
initial  value 35.756714 
iter  10 value 20.881639
iter  20 value 20.867468
iter  30 value 20.841851
iter  40 value 20.506847
iter  50 value 16.564675
iter  60 value 16.469690
iter  70 value 16.440830
iter  80 value 16.436097
iter  90 value 16.434863
iter 100 value 16.434091
final  value 16.434091 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  111
initial  value 49.333650 
final  value 40.302810 
converged
Fitting Repeat 3 

# weights:  111
initial  value 53.311464 
iter  10 value 34.809819
iter  20 value 34.809555
iter  30 value 34.809545
iter  40 value 34.809535
iter  50 value 34.809524
iter  60 value 34.809512
iter  70 value 34.809499
iter  80 value 34.809485
iter  90 value 34.809471
iter 100 value 34.809455
final  value 34.809455 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  111
initial  value 50.881087 
iter  10 value 33.505905
iter  20 value 33.505787
iter  30 value 33.505657
iter  40 value 33.505516
iter  50 value 33.505358
iter  60 value 33.505173
iter  70 value 33.504948
iter  80 value 33.504659
iter  90 value 33.504267
iter 100 value 33.503691
final  value 33.503691 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  111
initial  value 69.176447 
iter  10 value 30.300943
final  value 30.300933 
converged
Fitting Repeat 1 

# weights:  199
initial  value 41.688182 
iter  10 value 29.377752
iter  20 value 29.370550
iter  30 value 29.369864
iter  40 value 29.368747
iter  50 value 29.366579
iter  60 value 29.360666
iter  70 value 29.314963
iter  80 value 23.422258
iter  90 value 22.533391
iter 100 value 22.456182
final  value 22.456182 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 61.106725 
iter  10 value 29.383899
iter  20 value 29.369207
iter  30 value 29.367604
iter  40 value 29.363854
iter  50 value 29.347346
iter  60 value 25.785004
iter  70 value 22.627833
iter  80 value 22.482471
iter  90 value 22.444710
iter 100 value 22.424023
final  value 22.424023 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 59.940563 
iter  10 value 29.379663
iter  20 value 29.004066
iter  30 value 22.605764
iter  40 value 22.479677
iter  50 value 22.430788
iter  60 value 22.399973
iter  70 value 22.390084
iter  80 value 22.384403
iter  90 value 22.381658
iter 100 value 22.370178
final  value 22.370178 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 44.751271 
iter  10 value 29.377175
iter  20 value 29.266699
iter  30 value 22.880624
iter  40 value 22.535347
iter  50 value 22.448671
iter  60 value 22.431822
iter  70 value 22.420270
iter  80 value 22.415923
iter  90 value 22.411305
iter 100 value 22.378655
final  value 22.378655 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 59.321266 
iter  10 value 29.381448
iter  20 value 26.447598
iter  30 value 23.024903
iter  40 value 22.556158
iter  50 value 22.502601
iter  60 value 22.460422
iter  70 value 22.424758
iter  80 value 22.402346
iter  90 value 22.395833
iter 100 value 22.387745
final  value 22.387745 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  34
initial  value 71.209269 
iter  10 value 26.686282
iter  20 value 26.638018
final  value 26.637983 
converged
Fitting Repeat 2 

# weights:  34
initial  value 60.916917 
iter  10 value 33.161996
iter  20 value 33.154091
final  value 33.154090 
converged
Fitting Repeat 3 

# weights:  34
initial  value 89.953236 
iter  10 value 39.847927
final  value 39.814119 
converged
Fitting Repeat 4 

# weights:  34
initial  value 107.017361 
iter  10 value 40.053751
final  value 40.031512 
converged
Fitting Repeat 5 

# weights:  34
initial  value 73.589779 
iter  10 value 27.875583
final  value 27.870258 
converged
Fitting Repeat 1 

# weights:  12
initial  value 53.802235 
iter  10 value 32.107397
final  value 32.105789 
converged
Fitting Repeat 2 

# weights:  12
initial  value 50.477010 
iter  10 value 25.377389
final  value 25.376399 
converged
Fitting Repeat 3 

# weights:  12
initial  value 49.086291 
iter  10 value 23.308961
final  value 23.308908 
converged
Fitting Repeat 4 

# weights:  12
initial  value 44.946839 
iter  10 value 27.044266
final  value 27.043152 
converged
Fitting Repeat 5 

# weights:  12
initial  value 49.077574 
iter  10 value 28.148187
final  value 28.148089 
converged
Fitting Repeat 1 

# weights:  221
initial  value 43.611440 
iter  10 value 29.463380
iter  20 value 23.665207
iter  30 value 23.418672
iter  40 value 23.401773
iter  50 value 23.397938
iter  60 value 23.397230
iter  70 value 23.396690
iter  80 value 23.396556
final  value 23.396551 
converged
Fitting Repeat 2 

# weights:  221
initial  value 74.637264 
iter  10 value 29.457142
iter  20 value 23.507466
iter  30 value 23.348435
iter  40 value 23.319760
iter  50 value 23.314002
iter  60 value 23.313581
iter  70 value 23.313541
final  value 23.313539 
converged
Fitting Repeat 3 

# weights:  221
initial  value 46.191605 
iter  10 value 29.475690
iter  20 value 23.509141
iter  30 value 23.354442
iter  40 value 23.338243
iter  50 value 23.335366
iter  60 value 23.334903
iter  70 value 23.334847
iter  80 value 23.334828
iter  90 value 23.334759
iter 100 value 23.334736
final  value 23.334736 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  221
initial  value 58.583794 
iter  10 value 29.535558
iter  20 value 23.655779
iter  30 value 23.398561
iter  40 value 23.343875
iter  50 value 23.334621
iter  60 value 23.334063
iter  70 value 23.333984
final  value 23.333980 
converged
Fitting Repeat 5 

# weights:  221
initial  value 60.474279 
iter  10 value 29.514221
iter  20 value 23.701862
iter  30 value 23.399191
iter  40 value 23.362713
iter  50 value 23.357209
iter  60 value 23.356184
iter  70 value 23.356057
iter  80 value 23.356043
final  value 23.356043 
converged
Fitting Repeat 1 

# weights:  166
initial  value 30.750945 
iter  10 value 20.629670
iter  20 value 20.374457
iter  30 value 20.324386
iter  40 value 20.317980
iter  50 value 20.317749
iter  60 value 20.317730
final  value 20.317729 
converged
Fitting Repeat 2 

# weights:  166
initial  value 38.638067 
iter  10 value 26.865388
iter  20 value 22.270989
iter  30 value 22.124152
iter  40 value 21.932305
iter  50 value 21.919591
iter  60 value 21.917025
iter  70 value 21.916696
iter  80 value 21.916671
final  value 21.916670 
converged
Fitting Repeat 3 

# weights:  166
initial  value 59.678708 
iter  10 value 34.436323
iter  20 value 30.210853
iter  30 value 29.710128
iter  40 value 29.675546
iter  50 value 29.672365
iter  60 value 29.672082
iter  70 value 29.672056
final  value 29.672055 
converged
Fitting Repeat 4 

# weights:  166
initial  value 61.046484 
iter  10 value 34.552876
iter  20 value 26.973993
iter  30 value 26.825515
iter  40 value 26.821678
iter  50 value 26.819962
iter  60 value 26.819221
iter  70 value 26.818998
iter  80 value 26.818667
iter  90 value 26.818393
iter 100 value 26.818321
final  value 26.818321 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 31.823405 
iter  10 value 20.965308
iter  20 value 20.060651
iter  30 value 20.058688
iter  40 value 19.961187
iter  50 value 19.893288
iter  60 value 19.880449
iter  70 value 19.873769
iter  80 value 19.873609
final  value 19.873605 
converged
Fitting Repeat 1 

# weights:  89
initial  value 52.505774 
iter  10 value 22.238524
iter  20 value 18.394879
iter  30 value 16.976571
iter  40 value 16.950093
iter  50 value 16.938149
iter  60 value 16.936764
iter  70 value 16.936026
iter  80 value 16.935899
iter  90 value 16.935836
iter 100 value 16.935820
final  value 16.935820 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 27.676144 
iter  10 value 16.751973
iter  20 value 6.648266
iter  30 value 6.373100
iter  40 value 6.312834
iter  50 value 6.302094
iter  60 value 6.300125
iter  70 value 6.299555
iter  80 value 6.299488
iter  90 value 6.299478
final  value 6.299476 
converged
Fitting Repeat 3 

# weights:  89
initial  value 52.501534 
iter  10 value 31.673761
iter  20 value 26.084462
iter  30 value 24.873111
iter  40 value 24.756511
iter  50 value 24.748625
iter  60 value 24.745295
iter  70 value 24.744177
iter  80 value 24.743910
iter  90 value 24.743855
iter 100 value 24.743841
final  value 24.743841 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 35.254817 
iter  10 value 20.599736
iter  20 value 18.108408
iter  30 value 14.052130
iter  40 value 14.017047
iter  50 value 14.005942
iter  60 value 14.004067
iter  70 value 14.003741
iter  80 value 14.003338
iter  90 value 14.002979
iter 100 value 14.002908
final  value 14.002908 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 48.205804 
iter  10 value 26.461355
iter  20 value 22.107414
iter  30 value 20.622094
iter  40 value 20.542021
iter  50 value 20.531109
iter  60 value 20.525249
iter  70 value 20.520955
iter  80 value 20.519423
iter  90 value 20.518858
iter 100 value 20.518620
final  value 20.518620 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  23
initial  value 51.146474 
iter  10 value 29.702213
iter  20 value 23.503291
iter  30 value 23.354955
iter  40 value 23.326858
iter  50 value 23.325055
iter  60 value 23.324841
final  value 23.324840 
converged
Fitting Repeat 2 

# weights:  23
initial  value 47.101206 
iter  10 value 29.628644
iter  20 value 24.212892
iter  30 value 23.435878
iter  40 value 23.334925
iter  50 value 23.326326
iter  60 value 23.324965
iter  70 value 23.324852
final  value 23.324839 
converged
Fitting Repeat 3 

# weights:  23
initial  value 54.149632 
iter  10 value 29.714373
iter  20 value 23.565150
iter  30 value 23.361770
iter  40 value 23.332563
iter  50 value 23.326256
iter  60 value 23.324873
iter  70 value 23.324840
final  value 23.324839 
converged
Fitting Repeat 4 

# weights:  23
initial  value 37.930195 
iter  10 value 29.388694
iter  20 value 23.467723
iter  30 value 23.363084
iter  40 value 23.328774
iter  50 value 23.325295
iter  60 value 23.324845
iter  70 value 23.324839
iter  70 value 23.324839
iter  70 value 23.324839
final  value 23.324839 
converged
Fitting Repeat 5 

# weights:  23
initial  value 40.664577 
iter  10 value 29.421242
iter  20 value 24.405014
iter  30 value 23.355572
iter  40 value 23.329794
iter  50 value 23.325007
iter  60 value 23.324852
final  value 23.324839 
converged
Fitting Repeat 1 

# weights:  100
initial  value 45.466674 
iter  10 value 26.932657
iter  20 value 26.379707
final  value 26.379570 
converged
Fitting Repeat 2 

# weights:  100
initial  value 40.631839 
iter  10 value 26.485021
iter  20 value 26.379653
final  value 26.379570 
converged
Fitting Repeat 3 

# weights:  100
initial  value 38.760500 
iter  10 value 27.094377
iter  20 value 26.379707
final  value 26.379570 
converged
Fitting Repeat 4 

# weights:  100
initial  value 70.220472 
iter  10 value 28.272263
iter  20 value 26.379832
final  value 26.379570 
converged
Fitting Repeat 5 

# weights:  100
initial  value 47.070925 
iter  10 value 28.546454
iter  20 value 26.380617
iter  30 value 26.379570
iter  30 value 26.379570
iter  30 value 26.379570
final  value 26.379570 
converged
Fitting Repeat 1 

# weights:  144
initial  value 39.082376 
iter  10 value 29.408799
iter  20 value 29.232402
iter  30 value 23.089398
iter  40 value 22.651751
iter  50 value 22.577714
iter  60 value 22.554817
iter  70 value 22.542270
iter  80 value 22.527041
iter  90 value 22.510249
iter 100 value 22.496332
final  value 22.496332 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  144
initial  value 36.904180 
iter  10 value 29.391582
iter  20 value 27.289330
iter  30 value 22.905311
iter  40 value 22.661341
iter  50 value 22.604246
iter  60 value 22.574249
iter  70 value 22.553396
iter  80 value 22.548035
iter  90 value 22.544861
iter 100 value 22.543157
final  value 22.543157 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  144
initial  value 51.619732 
iter  10 value 29.431091
iter  20 value 25.647285
iter  30 value 22.886314
iter  40 value 22.638741
iter  50 value 22.588268
iter  60 value 22.557184
iter  70 value 22.543723
iter  80 value 22.539604
iter  90 value 22.534765
iter 100 value 22.520192
final  value 22.520192 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  144
initial  value 53.705408 
iter  10 value 29.433439
iter  20 value 23.535228
iter  30 value 22.663326
iter  40 value 22.594186
iter  50 value 22.568652
iter  60 value 22.559940
iter  70 value 22.547770
iter  80 value 22.541806
iter  90 value 22.538715
iter 100 value 22.536868
final  value 22.536868 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  144
initial  value 73.274154 
iter  10 value 29.429002
iter  20 value 29.156228
iter  30 value 22.726151
iter  40 value 22.582433
iter  50 value 22.547577
iter  60 value 22.519975
iter  70 value 22.502900
iter  80 value 22.495813
iter  90 value 22.493427
iter 100 value 22.492177
final  value 22.492177 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  34
initial  value 51.507045 
iter  10 value 30.020523
final  value 30.020139 
converged
Fitting Repeat 2 

# weights:  34
initial  value 58.076527 
iter  10 value 30.020946
final  value 30.020139 
converged
Fitting Repeat 3 

# weights:  34
initial  value 70.163899 
iter  10 value 30.020373
final  value 30.020139 
converged
Fitting Repeat 4 

# weights:  34
initial  value 41.120405 
iter  10 value 30.023345
final  value 30.020139 
converged
Fitting Repeat 5 

# weights:  34
initial  value 49.404588 
iter  10 value 30.033895
final  value 30.020139 
converged
Fitting Repeat 1 

# weights:  111
initial  value 58.530704 
iter  10 value 43.029589
iter  20 value 43.020301
iter  20 value 43.020301
iter  20 value 43.020301
final  value 43.020301 
converged
Fitting Repeat 2 

# weights:  111
initial  value 69.827262 
iter  10 value 43.111307
iter  20 value 43.020302
final  value 43.020301 
converged
Fitting Repeat 3 

# weights:  111
initial  value 92.240844 
iter  10 value 43.257126
iter  20 value 43.020301
iter  20 value 43.020301
iter  20 value 43.020301
final  value 43.020301 
converged
Fitting Repeat 4 

# weights:  111
initial  value 82.613346 
iter  10 value 43.249541
iter  20 value 43.020301
iter  20 value 43.020301
iter  20 value 43.020301
final  value 43.020301 
converged
Fitting Repeat 5 

# weights:  111
initial  value 65.858403 
iter  10 value 43.169521
iter  20 value 43.020301
iter  20 value 43.020301
iter  20 value 43.020301
final  value 43.020301 
converged
Fitting Repeat 1 

# weights:  56
initial  value 50.853202 
iter  10 value 40.472104
final  value 40.446369 
converged
Fitting Repeat 2 

# weights:  56
initial  value 56.026973 
iter  10 value 40.714916
final  value 40.446369 
converged
Fitting Repeat 3 

# weights:  56
initial  value 61.889129 
iter  10 value 40.932157
iter  20 value 40.446370
final  value 40.446369 
converged
Fitting Repeat 4 

# weights:  56
initial  value 74.405971 
iter  10 value 40.585853
final  value 40.446369 
converged
Fitting Repeat 5 

# weights:  56
initial  value 56.966588 
iter  10 value 40.568634
final  value 40.446369 
converged
Fitting Repeat 1 

# weights:  155
initial  value 67.180350 
iter  10 value 41.936709
iter  20 value 40.924588
iter  30 value 34.665040
iter  40 value 34.493292
iter  50 value 34.477133
iter  60 value 34.461002
iter  70 value 34.456387
iter  80 value 34.454881
iter  90 value 34.454416
iter 100 value 34.453535
final  value 34.453535 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  155
initial  value 68.633689 
iter  10 value 41.927652
iter  20 value 41.150006
iter  30 value 34.599730
iter  40 value 34.426294
iter  50 value 34.391980
iter  60 value 34.380094
iter  70 value 34.377333
iter  80 value 34.376797
iter  90 value 34.376264
iter 100 value 34.376014
final  value 34.376014 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  155
initial  value 54.087770 
iter  10 value 41.859567
iter  20 value 35.800656
iter  30 value 34.527777
iter  40 value 34.472143
iter  50 value 34.462996
iter  60 value 34.458583
iter  70 value 34.454879
iter  80 value 34.452016
iter  90 value 34.448190
iter 100 value 34.446224
final  value 34.446224 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  155
initial  value 65.872282 
iter  10 value 41.932395
iter  20 value 39.294336
iter  30 value 34.503189
iter  40 value 34.412628
iter  50 value 34.369531
iter  60 value 34.346713
iter  70 value 34.331727
iter  80 value 34.326723
iter  90 value 34.324277
iter 100 value 34.323270
final  value 34.323270 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  155
initial  value 66.496209 
iter  10 value 41.976642
iter  20 value 41.774379
iter  30 value 36.395187
iter  40 value 34.501217
iter  50 value 34.423314
iter  60 value 34.380503
iter  70 value 34.366802
iter  80 value 34.361745
iter  90 value 34.358630
iter 100 value 34.357655
final  value 34.357655 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  12
initial  value 39.581239 
iter  10 value 27.327279
iter  20 value 20.844219
iter  30 value 19.244594
iter  40 value 18.902512
iter  50 value 18.882046
iter  60 value 18.853173
iter  70 value 18.845312
final  value 18.845309 
converged
Fitting Repeat 2 

# weights:  12
initial  value 45.771808 
iter  10 value 32.653302
iter  20 value 26.851371
iter  30 value 25.195935
iter  40 value 24.356672
iter  50 value 23.814908
iter  60 value 23.369295
iter  70 value 23.322950
iter  80 value 23.198558
iter  90 value 23.184671
final  value 23.183643 
converged
Fitting Repeat 3 

# weights:  12
initial  value 62.589695 
iter  10 value 45.016127
iter  20 value 38.802699
iter  30 value 38.116479
iter  40 value 37.956909
iter  50 value 37.914156
final  value 37.913638 
converged
Fitting Repeat 4 

# weights:  12
initial  value 32.726345 
iter  10 value 17.157779
iter  20 value 13.324950
iter  30 value 11.497679
iter  40 value 11.310960
iter  50 value 11.276885
iter  60 value 11.271341
iter  70 value 11.270467
final  value 11.270466 
converged
Fitting Repeat 5 

# weights:  12
initial  value 126.979853 
iter  10 value 88.821669
iter  20 value 86.016792
iter  30 value 83.761339
iter  40 value 82.750242
iter  50 value 81.947217
iter  60 value 81.594524
iter  70 value 81.396129
iter  80 value 81.357295
iter  90 value 81.289692
iter 100 value 81.270346
final  value 81.270346 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  67
initial  value 56.583495 
iter  10 value 41.859598
iter  20 value 41.691406
iter  30 value 34.790267
iter  40 value 34.449307
iter  50 value 34.418371
iter  60 value 34.413130
iter  70 value 34.409767
iter  80 value 34.406569
iter  90 value 34.405092
iter 100 value 34.404543
final  value 34.404543 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  67
initial  value 68.490943 
iter  10 value 41.851183
iter  20 value 39.285224
iter  30 value 34.520278
iter  40 value 34.432361
iter  50 value 34.421889
iter  60 value 34.415080
iter  70 value 34.412769
iter  80 value 34.411418
iter  90 value 34.410241
iter 100 value 34.408528
final  value 34.408528 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  67
initial  value 75.042741 
iter  10 value 41.860784
iter  20 value 41.604482
iter  30 value 34.594249
iter  40 value 34.451869
iter  50 value 34.423743
iter  60 value 34.412998
iter  70 value 34.410549
iter  80 value 34.409574
iter  90 value 34.408804
iter 100 value 34.407524
final  value 34.407524 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  67
initial  value 46.249059 
iter  10 value 41.538644
iter  20 value 34.429212
iter  30 value 34.348460
iter  40 value 34.338684
iter  50 value 34.336555
iter  60 value 34.335599
iter  70 value 34.334850
iter  80 value 34.333422
iter  90 value 34.332540
iter 100 value 34.332230
final  value 34.332230 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  67
initial  value 58.401397 
iter  10 value 41.823884
iter  20 value 34.847461
iter  30 value 34.468078
iter  40 value 34.432179
iter  50 value 34.420670
iter  60 value 34.413876
iter  70 value 34.409877
iter  80 value 34.406023
iter  90 value 34.404776
iter 100 value 34.403942
final  value 34.403942 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  210
initial  value 54.422164 
iter  10 value 41.757681
iter  20 value 37.109303
iter  30 value 34.175875
iter  40 value 34.090420
iter  50 value 34.072094
iter  60 value 34.069718
iter  70 value 34.069243
iter  80 value 34.068533
iter  90 value 34.068195
iter 100 value 34.068059
final  value 34.068059 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  210
initial  value 72.281965 
iter  10 value 41.758498
iter  20 value 39.464157
iter  30 value 34.662146
iter  40 value 34.174662
iter  50 value 34.148052
iter  60 value 34.147256
iter  70 value 34.144337
iter  80 value 34.136321
iter  90 value 34.114288
iter 100 value 34.079660
final  value 34.079660 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  210
initial  value 57.112309 
iter  10 value 41.758004
iter  20 value 41.365059
iter  30 value 35.863978
iter  40 value 34.309251
iter  50 value 34.183251
iter  60 value 34.174695
iter  70 value 34.105488
iter  80 value 34.103386
iter  90 value 34.099840
iter 100 value 34.099397
final  value 34.099397 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  210
initial  value 58.478317 
iter  10 value 41.757748
iter  20 value 37.001252
iter  30 value 34.308787
iter  40 value 34.160373
iter  50 value 34.137907
iter  60 value 34.135196
iter  70 value 34.127660
iter  80 value 34.109198
iter  90 value 34.078931
iter 100 value 34.073458
final  value 34.073458 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  210
initial  value 51.719664 
iter  10 value 41.757403
iter  20 value 41.755201
iter  30 value 41.754700
iter  40 value 41.753919
iter  50 value 41.752544
iter  60 value 41.749568
iter  70 value 41.739548
iter  80 value 41.540545
iter  90 value 34.258997
iter 100 value 34.161538
final  value 34.161538 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  166
initial  value 74.382591 
iter  10 value 46.346612
iter  20 value 36.949724
iter  30 value 34.083907
iter  40 value 34.009369
iter  50 value 33.974058
iter  60 value 33.953126
iter  70 value 33.924474
iter  80 value 33.906985
iter  90 value 33.882501
iter 100 value 33.867997
final  value 33.867997 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 27.248277 
iter  10 value 23.275776
iter  20 value 17.445833
iter  30 value 13.460457
iter  40 value 13.389392
iter  50 value 13.369392
iter  60 value 13.355458
iter  70 value 13.352378
iter  80 value 13.350993
iter  90 value 13.350111
iter 100 value 13.349199
final  value 13.349199 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  166
initial  value 61.185266 
iter  10 value 45.137352
iter  20 value 36.481147
iter  30 value 33.819862
iter  40 value 33.673706
iter  50 value 33.636654
iter  60 value 33.627884
iter  70 value 33.623247
iter  80 value 33.621604
iter  90 value 33.619677
iter 100 value 33.617564
final  value 33.617564 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  166
initial  value 31.234760 
iter  10 value 17.858120
iter  20 value 16.372708
iter  30 value 9.961467
iter  40 value 9.851862
iter  50 value 9.822119
iter  60 value 9.817418
iter  70 value 9.815648
iter  80 value 9.814142
iter  90 value 9.812461
iter 100 value 9.811789
final  value 9.811789 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 41.987108 
iter  10 value 36.558040
iter  20 value 26.705900
iter  30 value 21.871397
iter  40 value 21.833237
iter  50 value 21.821057
iter  60 value 21.815195
iter  70 value 21.812973
iter  80 value 21.810342
iter  90 value 21.808716
iter 100 value 21.808084
final  value 21.808084 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  23
initial  value 74.389114 
iter  10 value 53.196159
final  value 53.193731 
converged
Fitting Repeat 2 

# weights:  23
initial  value 42.049534 
iter  10 value 32.037436
final  value 32.028158 
converged
Fitting Repeat 3 

# weights:  23
initial  value 87.841571 
iter  10 value 68.913614
final  value 68.903525 
converged
Fitting Repeat 4 

# weights:  23
initial  value 72.439432 
iter  10 value 54.146747
iter  20 value 54.135306
iter  20 value 54.135306
iter  20 value 54.135306
final  value 54.135306 
converged
Fitting Repeat 5 

# weights:  23
initial  value 67.333215 
iter  10 value 31.338713
final  value 31.328399 
converged
Fitting Repeat 1 

# weights:  67
initial  value 72.504843 
iter  10 value 45.428877
iter  20 value 41.893821
iter  30 value 38.518929
iter  40 value 38.377251
iter  50 value 38.357559
iter  60 value 38.285907
iter  70 value 38.248990
iter  80 value 38.243842
iter  90 value 38.242796
iter 100 value 38.242296
final  value 38.242296 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  67
initial  value 41.810717 
iter  10 value 23.094132
iter  20 value 11.395287
iter  30 value 10.734857
iter  40 value 10.722371
iter  50 value 10.713838
iter  60 value 10.709297
iter  70 value 10.704912
iter  80 value 10.703345
iter  90 value 10.702555
iter 100 value 10.702090
final  value 10.702090 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  67
initial  value 46.635042 
iter  10 value 34.842535
iter  20 value 31.622718
iter  30 value 29.722620
iter  40 value 29.605627
iter  50 value 29.601979
iter  60 value 29.601300
iter  70 value 29.601123
iter  80 value 29.601112
iter  80 value 29.601112
iter  80 value 29.601112
final  value 29.601112 
converged
Fitting Repeat 4 

# weights:  67
initial  value 58.879650 
iter  10 value 42.214820
iter  20 value 39.509391
iter  30 value 36.732254
iter  40 value 36.570918
iter  50 value 36.562044
iter  60 value 36.561229
iter  70 value 36.561060
iter  80 value 36.560999
iter  90 value 36.560971
iter 100 value 36.560901
final  value 36.560901 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  67
initial  value 52.855552 
iter  10 value 23.254424
iter  20 value 16.323463
iter  30 value 15.817953
iter  40 value 15.796368
iter  50 value 15.786006
iter  60 value 15.784816
iter  70 value 15.784701
iter  80 value 15.784669
iter  90 value 15.784664
final  value 15.784663 
converged
Fitting Repeat 1 

# weights:  111
initial  value 23.007289 
iter  10 value 22.416330
iter  20 value 13.411797
iter  30 value 13.242608
iter  40 value 13.112234
iter  50 value 13.106997
iter  60 value 13.103346
iter  70 value 13.092690
iter  80 value 13.086206
iter  90 value 13.084527
iter 100 value 13.083231
final  value 13.083231 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  111
initial  value 50.983916 
final  value 35.769654 
converged
Fitting Repeat 3 

# weights:  111
initial  value 28.601449 
iter  10 value 18.858782
iter  20 value 18.857453
iter  30 value 18.857302
iter  40 value 18.857118
iter  50 value 18.856892
iter  60 value 18.856609
iter  70 value 18.856245
iter  80 value 18.855765
iter  90 value 18.855113
iter 100 value 18.854191
final  value 18.854191 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  111
initial  value 61.856501 
iter  10 value 31.150801
final  value 31.150799 
converged
Fitting Repeat 5 

# weights:  111
initial  value 68.752846 
final  value 49.756495 
converged
Fitting Repeat 1 

# weights:  199
initial  value 76.137636 
iter  10 value 41.768978
iter  20 value 41.679418
iter  30 value 34.659791
iter  40 value 34.206689
iter  50 value 34.163794
iter  60 value 34.124214
iter  70 value 34.119557
iter  80 value 34.113642
iter  90 value 34.100961
iter 100 value 34.098667
final  value 34.098667 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 84.896138 
iter  10 value 41.765193
iter  20 value 41.753943
iter  30 value 41.749017
iter  40 value 41.727655
iter  50 value 36.741846
iter  60 value 34.236338
iter  70 value 34.170095
iter  80 value 34.114870
iter  90 value 34.111159
iter 100 value 34.109770
final  value 34.109770 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 45.919046 
iter  10 value 41.759232
iter  20 value 41.758598
iter  30 value 41.757569
iter  40 value 41.755584
iter  50 value 41.750176
iter  60 value 41.703481
iter  70 value 34.440748
iter  80 value 34.151575
iter  90 value 34.107830
iter 100 value 34.103828
final  value 34.103828 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 49.240245 
iter  10 value 41.758966
iter  20 value 35.413040
iter  30 value 34.244015
iter  40 value 34.166773
iter  50 value 34.124425
iter  60 value 34.098088
iter  70 value 34.095152
iter  80 value 34.092635
iter  90 value 34.090874
iter 100 value 34.089287
final  value 34.089287 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 44.685626 
iter  10 value 41.761067
iter  20 value 41.760914
iter  30 value 41.760737
iter  40 value 41.760528
iter  50 value 41.760272
iter  60 value 41.759947
iter  70 value 41.759515
iter  80 value 41.758900
iter  90 value 41.757935
iter 100 value 41.756173
final  value 41.756173 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  34
initial  value 87.052285 
iter  10 value 41.805498
final  value 41.794235 
converged
Fitting Repeat 2 

# weights:  34
initial  value 74.547609 
iter  10 value 30.576165
final  value 30.572847 
converged
Fitting Repeat 3 

# weights:  34
initial  value 153.847801 
iter  10 value 104.383029
final  value 104.377405 
converged
Fitting Repeat 4 

# weights:  34
initial  value 103.036202 
iter  10 value 51.611557
final  value 51.585256 
converged
Fitting Repeat 5 

# weights:  34
initial  value 68.358897 
iter  10 value 22.324989
final  value 22.291963 
converged
Fitting Repeat 1 

# weights:  12
initial  value 66.397777 
iter  10 value 60.534820
final  value 60.526452 
converged
Fitting Repeat 2 

# weights:  12
initial  value 42.987622 
iter  10 value 37.695252
final  value 37.695219 
converged
Fitting Repeat 3 

# weights:  12
initial  value 69.160880 
iter  10 value 52.294694
final  value 52.284573 
converged
Fitting Repeat 4 

# weights:  12
initial  value 124.634536 
iter  10 value 89.185420
final  value 89.168700 
converged
Fitting Repeat 5 

# weights:  12
initial  value 63.005951 
iter  10 value 48.418839
final  value 48.382940 
converged
Fitting Repeat 1 

# weights:  221
initial  value 55.631981 
iter  10 value 41.863154
iter  20 value 35.339314
iter  30 value 35.099440
iter  40 value 35.076488
iter  50 value 35.074481
iter  60 value 35.074382
final  value 35.074374 
converged
Fitting Repeat 2 

# weights:  221
initial  value 57.955155 
iter  10 value 41.837138
iter  20 value 35.513909
iter  30 value 35.159900
iter  40 value 35.120608
iter  50 value 35.115801
iter  60 value 35.115189
iter  70 value 35.115118
final  value 35.115113 
converged
Fitting Repeat 3 

# weights:  221
initial  value 84.730826 
iter  10 value 42.000164
iter  20 value 41.032615
iter  30 value 35.506341
iter  40 value 35.219572
iter  50 value 35.144446
iter  60 value 35.105577
iter  70 value 35.091297
iter  80 value 35.089854
iter  90 value 35.089464
iter 100 value 35.089300
final  value 35.089300 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  221
initial  value 68.078547 
iter  10 value 41.913270
iter  20 value 35.548239
iter  30 value 35.145831
iter  40 value 35.096809
iter  50 value 35.090262
iter  60 value 35.089325
iter  70 value 35.089219
final  value 35.089211 
converged
Fitting Repeat 5 

# weights:  221
initial  value 59.218803 
iter  10 value 41.834482
iter  20 value 35.587218
iter  30 value 35.127326
iter  40 value 35.083548
iter  50 value 35.075497
iter  60 value 35.074817
iter  70 value 35.074524
iter  80 value 35.074402
iter  90 value 35.074376
final  value 35.074374 
converged
Fitting Repeat 1 

# weights:  166
initial  value 63.221533 
iter  10 value 11.667521
iter  20 value 11.101934
iter  30 value 10.854636
iter  40 value 10.854211
iter  50 value 10.836094
iter  60 value 10.764495
iter  70 value 10.753042
iter  80 value 10.749614
iter  90 value 10.749145
final  value 10.749112 
converged
Fitting Repeat 2 

# weights:  166
initial  value 52.743018 
iter  10 value 30.924918
iter  20 value 26.804237
iter  30 value 26.573214
iter  40 value 26.557696
iter  50 value 26.557119
iter  60 value 26.557014
iter  70 value 26.557006
final  value 26.557006 
converged
Fitting Repeat 3 

# weights:  166
initial  value 71.271288 
iter  10 value 59.168695
iter  20 value 53.347264
iter  30 value 53.044515
iter  40 value 52.966418
iter  50 value 52.902227
iter  60 value 52.838743
iter  70 value 52.836570
iter  80 value 52.836353
final  value 52.836349 
converged
Fitting Repeat 4 

# weights:  166
initial  value 68.847600 
iter  10 value 56.561809
iter  20 value 51.907018
iter  30 value 51.768987
iter  40 value 51.751296
iter  50 value 51.751118
final  value 51.751109 
converged
Fitting Repeat 5 

# weights:  166
initial  value 166.300997 
iter  10 value 100.167265
iter  20 value 95.942604
iter  30 value 95.846810
iter  40 value 95.844936
iter  50 value 95.844676
iter  60 value 95.844542
final  value 95.844540 
converged
Fitting Repeat 1 

# weights:  89
initial  value 86.757947 
iter  10 value 66.930815
iter  20 value 64.557855
iter  30 value 63.520402
iter  40 value 63.513094
iter  50 value 63.511562
iter  60 value 63.511432
iter  70 value 63.511385
final  value 63.511382 
converged
Fitting Repeat 2 

# weights:  89
initial  value 53.324870 
iter  10 value 31.436924
iter  20 value 29.297755
iter  30 value 28.816023
iter  40 value 28.810501
iter  50 value 28.809824
iter  60 value 28.809616
iter  70 value 28.809555
iter  80 value 28.809451
iter  90 value 28.809385
iter 100 value 28.809324
final  value 28.809324 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  89
initial  value 124.577814 
iter  10 value 67.839812
iter  20 value 65.159897
iter  30 value 63.066787
iter  40 value 63.034165
iter  50 value 63.029817
iter  60 value 63.028932
iter  70 value 63.028812
final  value 63.028794 
converged
Fitting Repeat 4 

# weights:  89
initial  value 58.794318 
iter  10 value 31.768976
iter  20 value 26.694787
iter  30 value 26.242282
iter  40 value 26.213959
iter  50 value 26.211530
iter  60 value 26.210854
iter  70 value 26.210590
iter  80 value 26.210414
iter  90 value 26.210323
iter 100 value 26.210244
final  value 26.210244 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 67.625700 
iter  10 value 21.364244
iter  20 value 17.749509
iter  30 value 16.567722
iter  40 value 16.314241
iter  50 value 16.298094
iter  60 value 16.297162
iter  70 value 16.296761
iter  80 value 16.296573
iter  90 value 16.296473
iter 100 value 16.296428
final  value 16.296428 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  23
initial  value 63.755343 
iter  10 value 42.058393
iter  20 value 35.697101
iter  30 value 35.129303
iter  40 value 35.114778
iter  50 value 35.105552
iter  60 value 35.098248
iter  70 value 35.096964
iter  80 value 35.096883
final  value 35.096876 
converged
Fitting Repeat 2 

# weights:  23
initial  value 61.638143 
iter  10 value 42.095590
iter  20 value 35.440430
iter  30 value 35.126011
iter  40 value 35.107359
iter  50 value 35.097899
iter  60 value 35.096940
iter  70 value 35.096886
final  value 35.096876 
converged
Fitting Repeat 3 

# weights:  23
initial  value 59.751312 
iter  10 value 42.032704
iter  20 value 37.753383
iter  30 value 35.966087
iter  40 value 35.250332
iter  50 value 35.146747
iter  60 value 35.112779
iter  70 value 35.100195
iter  80 value 35.097701
iter  90 value 35.096900
final  value 35.096880 
converged
Fitting Repeat 4 

# weights:  23
initial  value 59.040152 
iter  10 value 42.041954
iter  20 value 35.396627
iter  30 value 35.125327
iter  40 value 35.113712
iter  50 value 35.101969
iter  60 value 35.098595
iter  70 value 35.097195
iter  80 value 35.096898
iter  90 value 35.096876
iter  90 value 35.096876
iter  90 value 35.096876
final  value 35.096876 
converged
Fitting Repeat 5 

# weights:  23
initial  value 65.514181 
iter  10 value 42.003598
iter  20 value 38.269727
iter  30 value 35.473911
iter  40 value 35.126115
iter  50 value 35.116379
iter  60 value 35.112255
iter  70 value 35.101880
iter  80 value 35.097266
iter  90 value 35.096896
final  value 35.096881 
converged
Fitting Repeat 1 

# weights:  100
initial  value 62.229741 
iter  10 value 39.576306
iter  20 value 38.429018
final  value 38.428740 
converged
Fitting Repeat 2 

# weights:  100
initial  value 49.490398 
iter  10 value 38.793040
iter  20 value 38.428868
final  value 38.428740 
converged
Fitting Repeat 3 

# weights:  100
initial  value 48.848804 
iter  10 value 38.187121
iter  20 value 37.923625
final  value 37.922721 
converged
Fitting Repeat 4 

# weights:  100
initial  value 60.489291 
iter  10 value 41.388450
iter  20 value 38.438620
iter  30 value 38.428743
final  value 38.428740 
converged
Fitting Repeat 5 

# weights:  100
initial  value 76.888088 
iter  10 value 40.617838
iter  20 value 38.429326
final  value 38.428740 
converged
Fitting Repeat 1 

# weights:  144
initial  value 56.443274 
iter  10 value 41.804294
iter  20 value 41.688350
iter  30 value 34.893272
iter  40 value 34.380455
iter  50 value 34.316919
iter  60 value 34.291150
iter  70 value 34.281726
iter  80 value 34.276483
iter  90 value 34.259526
iter 100 value 34.241038
final  value 34.241038 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  144
initial  value 78.821090 
iter  10 value 41.827374
iter  20 value 41.750150
iter  30 value 36.392494
iter  40 value 34.335190
iter  50 value 34.253361
iter  60 value 34.234717
iter  70 value 34.226223
iter  80 value 34.221705
iter  90 value 34.220367
iter 100 value 34.219841
final  value 34.219841 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  144
initial  value 50.465059 
iter  10 value 41.784525
iter  20 value 34.964657
iter  30 value 34.384839
iter  40 value 34.316866
iter  50 value 34.296388
iter  60 value 34.285864
iter  70 value 34.281290
iter  80 value 34.278413
iter  90 value 34.275448
iter 100 value 34.273657
final  value 34.273657 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  144
initial  value 78.098168 
iter  10 value 41.826530
iter  20 value 41.773626
iter  30 value 41.732819
iter  40 value 36.937702
iter  50 value 34.385178
iter  60 value 34.299298
iter  70 value 34.266316
iter  80 value 34.244127
iter  90 value 34.235152
iter 100 value 34.228824
final  value 34.228824 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  144
initial  value 46.051852 
iter  10 value 41.742761
iter  20 value 36.222856
iter  30 value 34.354819
iter  40 value 34.285996
iter  50 value 34.254481
iter  60 value 34.239547
iter  70 value 34.229597
iter  80 value 34.225722
iter  90 value 34.222329
iter 100 value 34.218686
final  value 34.218686 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  34
initial  value 68.523316 
iter  10 value 42.398424
final  value 42.395997 
converged
Fitting Repeat 2 

# weights:  34
initial  value 58.731295 
iter  10 value 42.399811
final  value 42.395996 
converged
Fitting Repeat 3 

# weights:  34
initial  value 66.000514 
iter  10 value 42.403870
final  value 42.395996 
converged
Fitting Repeat 4 

# weights:  34
initial  value 55.313745 
iter  10 value 42.396354
final  value 42.395996 
converged
Fitting Repeat 5 

# weights:  34
initial  value 68.560712 
iter  10 value 42.410398
final  value 42.395996 
converged
Fitting Repeat 1 

# weights:  221
initial  value 80.271621 
iter  10 value 56.177593
iter  20 value 47.234320
iter  30 value 46.215306
iter  40 value 46.055128
iter  50 value 45.962942
iter  60 value 45.956211
iter  70 value 45.955178
iter  80 value 45.955060
iter  90 value 45.955036
final  value 45.955034 
converged
Fitting Repeat 2 

# weights:  221
initial  value 109.763235 
iter  10 value 56.215475
iter  20 value 46.988062
iter  30 value 46.061936
iter  40 value 45.984152
iter  50 value 45.958679
iter  60 value 45.955761
iter  70 value 45.955571
iter  80 value 45.955498
iter  90 value 45.955484
iter 100 value 45.955298
final  value 45.955298 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  221
initial  value 71.502154 
iter  10 value 56.126566
iter  20 value 46.381259
iter  30 value 45.950642
iter  40 value 45.932196
iter  50 value 45.926376
iter  60 value 45.925477
iter  70 value 45.925370
final  value 45.925367 
converged
Fitting Repeat 4 

# weights:  221
initial  value 128.850773 
iter  10 value 55.946944
iter  20 value 46.032922
iter  30 value 45.946340
iter  40 value 45.932115
iter  50 value 45.929549
iter  60 value 45.929360
iter  70 value 45.929331
iter  70 value 45.929331
iter  70 value 45.929331
final  value 45.929331 
converged
Fitting Repeat 5 

# weights:  221
initial  value 89.308661 
iter  10 value 56.147748
iter  20 value 46.737939
iter  30 value 46.559354
iter  40 value 46.556949
iter  50 value 46.556720
iter  60 value 46.550846
iter  70 value 46.220861
iter  80 value 45.999751
iter  90 value 45.986321
iter 100 value 45.984133
final  value 45.984133 
stopped after 100 iterations
Model Averaged Neural Network 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  size  decay         bag    RMSE       Rsquared   MAE        Selected
   1    1.926767e-03   TRUE  0.7691326  0.3918144  0.3784757          
   1    7.321145e-01   TRUE  0.8538468  0.5727504  0.4862169          
   2    1.001052e-02  FALSE  0.7696149  0.4032053  0.3776125          
   2    3.445798e+00   TRUE  0.9003867  0.6387076  0.5427996          
   3    4.372112e-01  FALSE  0.8232557  0.5437099  0.4559617          
   3    6.794005e+00   TRUE  0.9241108  0.6085997  0.5722293          
   5    2.635379e-01  FALSE  0.8058316  0.5100188  0.4379363          
   6    2.771781e-03  FALSE  0.7701513  0.3806157  0.3876649          
   6    1.108573e-02   TRUE  0.7756213  0.4346153  0.3908056          
   8    1.019226e-02   TRUE  0.7719485  0.4193734  0.3823534          
   9    1.317792e-01  FALSE  0.7911977  0.4778895  0.4202621          
  10    1.113913e-05   TRUE  0.8284319  0.2907492  0.4584650          
  10    6.050950e-01  FALSE  0.8378150  0.5735190  0.4703001          
  13    1.455864e-03  FALSE  0.7693877  0.3736767  0.3845137          
  14    3.174288e-03  FALSE  0.7671090  0.3918892  0.3774621          
  15    1.161836e-03   TRUE  0.7710744  0.3778887  0.3947317          
  15    2.882011e-02   TRUE  0.7748163  0.4437063  0.3822488          
  18    1.820930e-04  FALSE  0.7703321  0.3659735  0.3780151          
  19    2.443203e-05  FALSE  0.7849486  0.3541838  0.4063551          
  20    2.209179e-02  FALSE  0.7661418  0.4286164  0.3671099  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 20, decay = 0.02209179 and
 bag = FALSE.
[1] "Sat Mar 10 01:45:20 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:45:35 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "bag"                     
Bagged MARS 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  degree  nprune  RMSE       Rsquared   MAE         Selected
  1       2       0.3395955  0.8736327  0.19143272          
  1       3       0.1666873  0.9805704  0.07848998          
  1       4       0.1616042  0.9820088  0.08174551          
  1       5       0.1779472  0.9764949  0.09142559          
  2       2       0.3628046  0.8713353  0.19455561          
  2       3       0.1551011  0.9840738  0.07239838  *       
  2       4       0.1595515  0.9779425  0.07374685          
  2       5       0.1964875  0.9650059  0.08582662          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nprune = 3 and degree = 2.
[1] "Sat Mar 10 01:46:11 2018"
Bagged MARS using gCV Pruning 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results:

  RMSE       Rsquared   MAE      
  0.1915626  0.9693873  0.1077668

Tuning parameter 'degree' was held constant at a value of 1
[1] "Sat Mar 10 01:46:56 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :5     NA's   :5     NA's   :5    
Error : Stopping
In addition: There were 16 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :5     NA's   :5     NA's   :5    
Error : Stopping
In addition: There were 16 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:47:42 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "bam"                     
bartMachine initializing with 54 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 146.1/477.6MB
Iteration 200/1250  mem: 174.5/477.6MB
Iteration 300/1250  mem: 203/477.6MB
Iteration 400/1250  mem: 231.4/477.6MB
Iteration 500/1250  mem: 150.9/477.6MB
Iteration 600/1250  mem: 178.6/477.6MB
Iteration 700/1250  mem: 206.3/477.6MB
Iteration 800/1250  mem: 234/477.6MB
Iteration 900/1250  mem: 157.5/477.6MB
Iteration 1000/1250  mem: 185.5/477.6MB
Iteration 1100/1250  mem: 211.6/477.6MB
Iteration 1200/1250  mem: 239.6/477.6MB
done building BART in 0.545 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 31 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 169.6/477.6MB
Iteration 200/1250  mem: 189/477.6MB
Iteration 300/1250  mem: 208.5/477.6MB
Iteration 400/1250  mem: 227.9/477.6MB
Iteration 500/1250  mem: 247.4/477.6MB
Iteration 600/1250  mem: 266.8/477.6MB
Iteration 700/1250  mem: 186.7/477.6MB
Iteration 800/1250  mem: 208.3/477.6MB
Iteration 900/1250  mem: 230/477.6MB
Iteration 1000/1250  mem: 253.3/477.6MB
Iteration 1100/1250  mem: 275/477.6MB
Iteration 1200/1250  mem: 198.7/477.6MB
done building BART in 0.45 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 71 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 253/477.6MB
Iteration 200/1250  mem: 291.3/477.6MB
Iteration 300/1250  mem: 59.3/477.6MB
Iteration 400/1250  mem: 96.3/477.6MB
Iteration 500/1250  mem: 133.4/477.6MB
Iteration 600/1250  mem: 57/477.6MB
Iteration 700/1250  mem: 95.4/477.6MB
Iteration 800/1250  mem: 131.6/477.6MB
Iteration 900/1250  mem: 58.3/477.6MB
Iteration 1000/1250  mem: 93.3/477.6MB
Iteration 1100/1250  mem: 130.6/477.6MB
Iteration 1200/1250  mem: 168/477.6MB
done building BART in 0.898 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 12 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 82.1/477.6MB
Iteration 200/1250  mem: 89.4/477.6MB
Iteration 300/1250  mem: 94.3/477.6MB
Iteration 400/1250  mem: 101.6/477.6MB
Iteration 500/1250  mem: 106.5/477.6MB
Iteration 600/1250  mem: 113.8/477.6MB
Iteration 700/1250  mem: 118.6/477.6MB
Iteration 800/1250  mem: 125.9/477.6MB
Iteration 900/1250  mem: 130.8/477.6MB
Iteration 1000/1250  mem: 138.1/477.6MB
Iteration 1100/1250  mem: 143/477.6MB
Iteration 1200/1250  mem: 150.3/477.6MB
done building BART in 0.121 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 36 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 176/477.6MB
Iteration 200/1250  mem: 82.5/477.6MB
Iteration 300/1250  mem: 103/477.6MB
Iteration 400/1250  mem: 123.5/477.6MB
Iteration 500/1250  mem: 145.6/477.6MB
Iteration 600/1250  mem: 166.2/477.6MB
Iteration 700/1250  mem: 188.3/477.6MB
Iteration 800/1250  mem: 95.2/477.6MB
Iteration 900/1250  mem: 116.1/477.6MB
Iteration 1000/1250  mem: 137.1/477.6MB
Iteration 1100/1250  mem: 158/477.6MB
Iteration 1200/1250  mem: 178.9/477.6MB
done building BART in 0.427 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 94 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 135.4/477.6MB
Iteration 200/1250  mem: 189.2/477.6MB
Iteration 300/1250  mem: 120.4/477.6MB
Iteration 400/1250  mem: 177.5/477.6MB
Iteration 500/1250  mem: 124.9/477.6MB
Iteration 600/1250  mem: 182.3/477.6MB
Iteration 700/1250  mem: 241.7/477.6MB
Iteration 800/1250  mem: 197.5/477.6MB
Iteration 900/1250  mem: 259.4/477.6MB
Iteration 1000/1250  mem: 215.1/477.6MB
Iteration 1100/1250  mem: 276.1/477.6MB
Iteration 1200/1250  mem: 238.6/477.6MB
done building BART in 1.383 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 75 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 116.8/477.6MB
Iteration 200/1250  mem: 159.8/477.6MB
Iteration 300/1250  mem: 199.9/477.6MB
Iteration 400/1250  mem: 130.4/477.6MB
Iteration 500/1250  mem: 170.6/477.6MB
Iteration 600/1250  mem: 212.6/477.6MB
Iteration 700/1250  mem: 148.4/477.6MB
Iteration 800/1250  mem: 187.5/477.6MB
Iteration 900/1250  mem: 228.6/477.6MB
Iteration 1000/1250  mem: 163.7/477.6MB
Iteration 1100/1250  mem: 205.5/477.6MB
Iteration 1200/1250  mem: 139/477.6MB
done building BART in 1.173 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 18 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 174.9/477.6MB
Iteration 200/1250  mem: 184.6/477.6MB
Iteration 300/1250  mem: 194.2/477.6MB
Iteration 400/1250  mem: 203.9/477.6MB
Iteration 500/1250  mem: 213.5/477.6MB
Iteration 600/1250  mem: 223.2/477.6MB
Iteration 700/1250  mem: 232.8/477.6MB
Iteration 800/1250  mem: 240.1/477.6MB
Iteration 900/1250  mem: 249.7/477.6MB
Iteration 1000/1250  mem: 148.6/477.6MB
Iteration 1100/1250  mem: 157.2/477.6MB
Iteration 1200/1250  mem: 168/477.6MB
done building BART in 0.197 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 32 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 195.2/477.6MB
Iteration 200/1250  mem: 214.5/477.6MB
Iteration 300/1250  mem: 233.8/477.6MB
Iteration 400/1250  mem: 253.1/477.6MB
Iteration 500/1250  mem: 160.3/477.6MB
Iteration 600/1250  mem: 180.4/477.6MB
Iteration 700/1250  mem: 200.4/477.6MB
Iteration 800/1250  mem: 218.2/477.6MB
Iteration 900/1250  mem: 238.2/477.6MB
Iteration 1000/1250  mem: 258.3/477.6MB
Iteration 1100/1250  mem: 278.3/477.6MB
Iteration 1200/1250  mem: 189.5/477.6MB
done building BART in 0.406 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 52 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 230.1/477.6MB
Iteration 200/1250  mem: 255.1/477.6MB
Iteration 300/1250  mem: 280.2/477.6MB
Iteration 400/1250  mem: 186.5/477.6MB
Iteration 500/1250  mem: 211.5/477.6MB
Iteration 600/1250  mem: 238.7/477.6MB
Iteration 700/1250  mem: 263.6/477.6MB
Iteration 800/1250  mem: 288.5/477.6MB
Iteration 900/1250  mem: 194.2/477.6MB
Iteration 1000/1250  mem: 220.2/477.6MB
Iteration 1100/1250  mem: 243.8/477.6MB
Iteration 1200/1250  mem: 269.8/477.6MB
done building BART in 0.522 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 89 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 218.3/477.6MB
Iteration 200/1250  mem: 267.6/477.6MB
Iteration 300/1250  mem: 318.4/477.6MB
Iteration 400/1250  mem: 244.8/477.6MB
Iteration 500/1250  mem: 295.9/477.6MB
Iteration 600/1250  mem: 230.3/477.6MB
Iteration 700/1250  mem: 279.5/477.6MB
Iteration 800/1250  mem: 330.9/477.6MB
Iteration 900/1250  mem: 265.9/477.6MB
Iteration 1000/1250  mem: 316.3/477.6MB
Iteration 1100/1250  mem: 250.3/477.6MB
Iteration 1200/1250  mem: 300.8/477.6MB
done building BART in 1.061 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 20 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 341.7/477.6MB
Iteration 200/1250  mem: 352.1/477.6MB
Iteration 300/1250  mem: 362.4/477.6MB
Iteration 400/1250  mem: 255.5/477.6MB
Iteration 500/1250  mem: 265.5/477.6MB
Iteration 600/1250  mem: 277.1/477.6MB
Iteration 700/1250  mem: 287.1/477.6MB
Iteration 800/1250  mem: 298.7/477.6MB
Iteration 900/1250  mem: 308.7/477.6MB
Iteration 1000/1250  mem: 320.3/477.6MB
Iteration 1100/1250  mem: 330.3/477.6MB
Iteration 1200/1250  mem: 340.2/477.6MB
done building BART in 0.239 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 13 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 354.1/477.6MB
Iteration 200/1250  mem: 361.9/477.6MB
Iteration 300/1250  mem: 367.1/477.6MB
Iteration 400/1250  mem: 374.9/477.6MB
Iteration 500/1250  mem: 262.7/477.6MB
Iteration 600/1250  mem: 269.1/477.6MB
Iteration 700/1250  mem: 275.6/477.6MB
Iteration 800/1250  mem: 282/477.6MB
Iteration 900/1250  mem: 288.5/477.6MB
Iteration 1000/1250  mem: 294.9/477.6MB
Iteration 1100/1250  mem: 303/477.6MB
Iteration 1200/1250  mem: 307.8/477.6MB
done building BART in 0.154 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 96 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 369.9/477.6MB
Iteration 200/1250  mem: 45.8/477.6MB
Iteration 300/1250  mem: 99/477.6MB
Iteration 400/1250  mem: 29/477.6MB
Iteration 500/1250  mem: 81.1/477.6MB
Iteration 600/1250  mem: 135/477.6MB
Iteration 700/1250  mem: 65.8/477.6MB
Iteration 800/1250  mem: 119.4/477.6MB
Iteration 900/1250  mem: 50.4/477.6MB
Iteration 1000/1250  mem: 102.6/477.6MB
Iteration 1100/1250  mem: 154.7/477.6MB
Iteration 1200/1250  mem: 85.3/477.6MB
done building BART in 1.208 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 75 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 160.3/477.6MB
Iteration 200/1250  mem: 78.6/477.6MB
Iteration 300/1250  mem: 125.6/477.6MB
Iteration 400/1250  mem: 170.4/477.6MB
Iteration 500/1250  mem: 93.5/477.6MB
Iteration 600/1250  mem: 140.6/477.6MB
Iteration 700/1250  mem: 187.7/477.6MB
Iteration 800/1250  mem: 115.4/477.6MB
Iteration 900/1250  mem: 161.7/477.6MB
Iteration 1000/1250  mem: 210.5/477.6MB
Iteration 1100/1250  mem: 142.2/477.6MB
Iteration 1200/1250  mem: 187.7/477.6MB
done building BART in 0.993 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 45 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 238.7/477.6MB
Iteration 200/1250  mem: 142.9/477.6MB
Iteration 300/1250  mem: 166.1/477.6MB
Iteration 400/1250  mem: 189.3/477.6MB
Iteration 500/1250  mem: 214.2/477.6MB
Iteration 600/1250  mem: 237.3/477.6MB
Iteration 700/1250  mem: 138.8/477.6MB
Iteration 800/1250  mem: 164.7/477.6MB
Iteration 900/1250  mem: 188.7/477.6MB
Iteration 1000/1250  mem: 212.7/477.6MB
Iteration 1100/1250  mem: 236.7/477.6MB
Iteration 1200/1250  mem: 260.6/477.6MB
done building BART in 0.574 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 16 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 163.7/477.6MB
Iteration 200/1250  mem: 171.6/477.6MB
Iteration 300/1250  mem: 179.5/477.6MB
Iteration 400/1250  mem: 187.4/477.6MB
Iteration 500/1250  mem: 198/477.6MB
Iteration 600/1250  mem: 205.9/477.6MB
Iteration 700/1250  mem: 213.9/477.6MB
Iteration 800/1250  mem: 221.8/477.6MB
Iteration 900/1250  mem: 229.7/477.6MB
Iteration 1000/1250  mem: 237.7/477.6MB
Iteration 1100/1250  mem: 245.6/477.6MB
Iteration 1200/1250  mem: 253.5/477.6MB
done building BART in 0.159 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 49 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 166/477.6MB
Iteration 200/1250  mem: 192.9/477.6MB
Iteration 300/1250  mem: 216.8/477.6MB
Iteration 400/1250  mem: 242.2/477.6MB
Iteration 500/1250  mem: 266.2/477.6MB
Iteration 600/1250  mem: 165.5/477.6MB
Iteration 700/1250  mem: 190.4/477.6MB
Iteration 800/1250  mem: 215.2/477.6MB
Iteration 900/1250  mem: 240.1/477.6MB
Iteration 1000/1250  mem: 263/477.6MB
Iteration 1100/1250  mem: 287.9/477.6MB
Iteration 1200/1250  mem: 188.7/477.6MB
done building BART in 0.522 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 65 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 240.3/477.6MB
Iteration 200/1250  mem: 277.6/477.6MB
Iteration 300/1250  mem: 31.4/477.6MB
Iteration 400/1250  mem: 67/477.6MB
Iteration 500/1250  mem: 100.2/477.6MB
Iteration 600/1250  mem: 135.8/477.6MB
Iteration 700/1250  mem: 50.2/477.6MB
Iteration 800/1250  mem: 85/477.6MB
Iteration 900/1250  mem: 119.8/477.6MB
Iteration 1000/1250  mem: 154.6/477.6MB
Iteration 1100/1250  mem: 68.4/477.6MB
Iteration 1200/1250  mem: 102.2/477.6MB
done building BART in 0.786 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 21 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 135.4/477.6MB
Iteration 200/1250  mem: 146.3/477.6MB
Iteration 300/1250  mem: 157.3/477.6MB
Iteration 400/1250  mem: 168.2/477.6MB
Iteration 500/1250  mem: 179.2/477.6MB
Iteration 600/1250  mem: 62.9/477.6MB
Iteration 700/1250  mem: 73.1/477.6MB
Iteration 800/1250  mem: 83.2/477.6MB
Iteration 900/1250  mem: 95.3/477.6MB
Iteration 1000/1250  mem: 105.4/477.6MB
Iteration 1100/1250  mem: 115.5/477.6MB
Iteration 1200/1250  mem: 125.6/477.6MB
done building BART in 0.224 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 54 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 165.7/477.6MB
Iteration 200/1250  mem: 193.1/477.6MB
Iteration 300/1250  mem: 91.2/477.6MB
Iteration 400/1250  mem: 119.6/477.6MB
Iteration 500/1250  mem: 147.9/477.6MB
Iteration 600/1250  mem: 176.3/477.6MB
Iteration 700/1250  mem: 204.6/477.6MB
Iteration 800/1250  mem: 103.3/477.6MB
Iteration 900/1250  mem: 132.9/477.6MB
Iteration 1000/1250  mem: 160.2/477.6MB
Iteration 1100/1250  mem: 187.6/477.6MB
Iteration 1200/1250  mem: 215.6/477.6MB
done building BART in 0.613 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 31 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 124.2/477.6MB
Iteration 200/1250  mem: 143.8/477.6MB
Iteration 300/1250  mem: 160.6/477.6MB
Iteration 400/1250  mem: 180.2/477.6MB
Iteration 500/1250  mem: 199.8/477.6MB
Iteration 600/1250  mem: 219.4/477.6MB
Iteration 700/1250  mem: 112/477.6MB
Iteration 800/1250  mem: 133.1/477.6MB
Iteration 900/1250  mem: 154.2/477.6MB
Iteration 1000/1250  mem: 175.3/477.6MB
Iteration 1100/1250  mem: 196.4/477.6MB
Iteration 1200/1250  mem: 217.5/477.6MB
done building BART in 0.394 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 71 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 151.6/477.6MB
Iteration 200/1250  mem: 188.4/477.6MB
Iteration 300/1250  mem: 225.9/477.6MB
Iteration 400/1250  mem: 141.6/477.6MB
Iteration 500/1250  mem: 178.2/477.6MB
Iteration 600/1250  mem: 216.2/477.6MB
Iteration 700/1250  mem: 252/477.6MB
Iteration 800/1250  mem: 173.7/477.6MB
Iteration 900/1250  mem: 210.6/477.6MB
Iteration 1000/1250  mem: 247.5/477.6MB
Iteration 1100/1250  mem: 167.6/477.6MB
Iteration 1200/1250  mem: 204/477.6MB
done building BART in 0.811 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 12 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 233/477.6MB
Iteration 200/1250  mem: 240.6/477.6MB
Iteration 300/1250  mem: 245.6/477.6MB
Iteration 400/1250  mem: 253.1/477.6MB
Iteration 500/1250  mem: 258.1/477.6MB
Iteration 600/1250  mem: 265.6/477.6MB
Iteration 700/1250  mem: 153.1/477.6MB
Iteration 800/1250  mem: 158.2/477.6MB
Iteration 900/1250  mem: 165/477.6MB
Iteration 1000/1250  mem: 171.8/477.6MB
Iteration 1100/1250  mem: 176.9/477.6MB
Iteration 1200/1250  mem: 183.7/477.6MB
done building BART in 0.131 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 36 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 209.2/477.6MB
Iteration 200/1250  mem: 229.4/477.6MB
Iteration 300/1250  mem: 249.6/477.6MB
Iteration 400/1250  mem: 269.9/477.6MB
Iteration 500/1250  mem: 173.2/477.6MB
Iteration 600/1250  mem: 195/477.6MB
Iteration 700/1250  mem: 214.5/477.6MB
Iteration 800/1250  mem: 236.3/477.6MB
Iteration 900/1250  mem: 258.1/477.6MB
Iteration 1000/1250  mem: 279.8/477.6MB
Iteration 1100/1250  mem: 189/477.6MB
Iteration 1200/1250  mem: 210.1/477.6MB
done building BART in 0.421 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 94 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 274.6/477.6MB
Iteration 200/1250  mem: 207.6/477.6MB
Iteration 300/1250  mem: 260.4/477.6MB
Iteration 400/1250  mem: 197.2/477.6MB
Iteration 500/1250  mem: 255.2/477.6MB
Iteration 600/1250  mem: 313.2/477.6MB
Iteration 700/1250  mem: 107.2/477.6MB
Iteration 800/1250  mem: 165.4/477.6MB
Iteration 900/1250  mem: 115.6/477.6MB
Iteration 1000/1250  mem: 175.2/477.6MB
Iteration 1100/1250  mem: 129.3/477.6MB
Iteration 1200/1250  mem: 189.9/477.6MB
done building BART in 1.442 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 75 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 160.6/477.6MB
Iteration 200/1250  mem: 202.5/477.6MB
Iteration 300/1250  mem: 243.2/477.6MB
Iteration 400/1250  mem: 166.7/477.6MB
Iteration 500/1250  mem: 208.3/477.6MB
Iteration 600/1250  mem: 142/477.6MB
Iteration 700/1250  mem: 184.3/477.6MB
Iteration 800/1250  mem: 224.7/477.6MB
Iteration 900/1250  mem: 157.6/477.6MB
Iteration 1000/1250  mem: 197.3/477.6MB
Iteration 1100/1250  mem: 239.1/477.6MB
Iteration 1200/1250  mem: 173.8/477.6MB
done building BART in 0.911 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 18 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 207.2/477.6MB
Iteration 200/1250  mem: 216.9/477.6MB
Iteration 300/1250  mem: 226.6/477.6MB
Iteration 400/1250  mem: 236.3/477.6MB
Iteration 500/1250  mem: 246/477.6MB
Iteration 600/1250  mem: 253.2/477.6MB
Iteration 700/1250  mem: 262.9/477.6MB
Iteration 800/1250  mem: 272.6/477.6MB
Iteration 900/1250  mem: 281.6/477.6MB
Iteration 1000/1250  mem: 180.2/477.6MB
Iteration 1100/1250  mem: 188.5/477.6MB
Iteration 1200/1250  mem: 196.8/477.6MB
done building BART in 0.192 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 32 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 226/477.6MB
Iteration 200/1250  mem: 245.4/477.6MB
Iteration 300/1250  mem: 264.7/477.6MB
Iteration 400/1250  mem: 284.1/477.6MB
Iteration 500/1250  mem: 192.9/477.6MB
Iteration 600/1250  mem: 210.3/477.6MB
Iteration 700/1250  mem: 229.8/477.6MB
Iteration 800/1250  mem: 249.3/477.6MB
Iteration 900/1250  mem: 266.7/477.6MB
Iteration 1000/1250  mem: 286.2/477.6MB
Iteration 1100/1250  mem: 305.7/477.6MB
Iteration 1200/1250  mem: 214.2/477.6MB
done building BART in 0.392 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 52 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 254.9/477.6MB
Iteration 200/1250  mem: 280.2/477.6MB
Iteration 300/1250  mem: 305.4/477.6MB
Iteration 400/1250  mem: 212/477.6MB
Iteration 500/1250  mem: 237.2/477.6MB
Iteration 600/1250  mem: 264.6/477.6MB
Iteration 700/1250  mem: 289.8/477.6MB
Iteration 800/1250  mem: 314.9/477.6MB
Iteration 900/1250  mem: 218/477.6MB
Iteration 1000/1250  mem: 244.3/477.6MB
Iteration 1100/1250  mem: 268.2/477.6MB
Iteration 1200/1250  mem: 294.5/477.6MB
done building BART in 0.534 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 89 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 240/477.6MB
Iteration 200/1250  mem: 289.4/477.6MB
Iteration 300/1250  mem: 340.4/477.6MB
Iteration 400/1250  mem: 264.9/477.6MB
Iteration 500/1250  mem: 315.4/477.6MB
Iteration 600/1250  mem: 248.6/477.6MB
Iteration 700/1250  mem: 296.7/477.6MB
Iteration 800/1250  mem: 347/477.6MB
Iteration 900/1250  mem: 82.2/477.6MB
Iteration 1000/1250  mem: 131.2/477.6MB
Iteration 1100/1250  mem: 66.7/477.6MB
Iteration 1200/1250  mem: 115.3/477.6MB
done building BART in 1.201 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 20 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 156.6/477.6MB
Iteration 200/1250  mem: 167/477.6MB
Iteration 300/1250  mem: 177.4/477.6MB
Iteration 400/1250  mem: 187.9/477.6MB
Iteration 500/1250  mem: 80.2/477.6MB
Iteration 600/1250  mem: 91/477.6MB
Iteration 700/1250  mem: 101.7/477.6MB
Iteration 800/1250  mem: 110.7/477.6MB
Iteration 900/1250  mem: 121.5/477.6MB
Iteration 1000/1250  mem: 132.3/477.6MB
Iteration 1100/1250  mem: 143/477.6MB
Iteration 1200/1250  mem: 153.8/477.6MB
done building BART in 0.242 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 13 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 168.1/477.6MB
Iteration 200/1250  mem: 176/477.6MB
Iteration 300/1250  mem: 181.2/477.6MB
Iteration 400/1250  mem: 189/477.6MB
Iteration 500/1250  mem: 194.2/477.6MB
Iteration 600/1250  mem: 202.1/477.6MB
Iteration 700/1250  mem: 86.8/477.6MB
Iteration 800/1250  mem: 93.7/477.6MB
Iteration 900/1250  mem: 100.7/477.6MB
Iteration 1000/1250  mem: 107.7/477.6MB
Iteration 1100/1250  mem: 112.9/477.6MB
Iteration 1200/1250  mem: 119.9/477.6MB
done building BART in 0.146 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 96 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 180.9/477.6MB
Iteration 200/1250  mem: 110.7/477.6MB
Iteration 300/1250  mem: 163.5/477.6MB
Iteration 400/1250  mem: 216.4/477.6MB
Iteration 500/1250  mem: 145.4/477.6MB
Iteration 600/1250  mem: 198.9/477.6MB
Iteration 700/1250  mem: 127.7/477.6MB
Iteration 800/1250  mem: 181.2/477.6MB
Iteration 900/1250  mem: 234.8/477.6MB
Iteration 1000/1250  mem: 165.2/477.6MB
Iteration 1100/1250  mem: 216.2/477.6MB
Iteration 1200/1250  mem: 143.3/477.6MB
done building BART in 1.172 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 75 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 219.2/477.6MB
Iteration 200/1250  mem: 265.7/477.6MB
Iteration 300/1250  mem: 180.5/477.6MB
Iteration 400/1250  mem: 227.8/477.6MB
Iteration 500/1250  mem: 275.2/477.6MB
Iteration 600/1250  mem: 198.6/477.6MB
Iteration 700/1250  mem: 243.8/477.6MB
Iteration 800/1250  mem: 288.9/477.6MB
Iteration 900/1250  mem: 213.9/477.6MB
Iteration 1000/1250  mem: 261.6/477.6MB
Iteration 1100/1250  mem: 306.8/477.6MB
Iteration 1200/1250  mem: 239.4/477.6MB
done building BART in 1.004 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 45 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 290.6/477.6MB
Iteration 200/1250  mem: 317.3/477.6MB
Iteration 300/1250  mem: 218.2/477.6MB
Iteration 400/1250  mem: 243.3/477.6MB
Iteration 500/1250  mem: 268.3/477.6MB
Iteration 600/1250  mem: 291.4/477.6MB
Iteration 700/1250  mem: 316.5/477.6MB
Iteration 800/1250  mem: 220.4/477.6MB
Iteration 900/1250  mem: 244.4/477.6MB
Iteration 1000/1250  mem: 268.4/477.6MB
Iteration 1100/1250  mem: 292.4/477.6MB
Iteration 1200/1250  mem: 316.4/477.6MB
done building BART in 0.568 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 16 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 341.3/477.6MB
Iteration 200/1250  mem: 227.3/477.6MB
Iteration 300/1250  mem: 234.8/477.6MB
Iteration 400/1250  mem: 243.7/477.6MB
Iteration 500/1250  mem: 251.2/477.6MB
Iteration 600/1250  mem: 260.1/477.6MB
Iteration 700/1250  mem: 267.6/477.6MB
Iteration 800/1250  mem: 276.5/477.6MB
Iteration 900/1250  mem: 284/477.6MB
Iteration 1000/1250  mem: 292.9/477.6MB
Iteration 1100/1250  mem: 300.4/477.6MB
Iteration 1200/1250  mem: 309.4/477.6MB
done building BART in 0.184 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 49 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 341.4/477.6MB
Iteration 200/1250  mem: 19.8/477.6MB
Iteration 300/1250  mem: 45.3/477.6MB
Iteration 400/1250  mem: 70.8/477.6MB
Iteration 500/1250  mem: 95.1/477.6MB
Iteration 600/1250  mem: 120.6/477.6MB
Iteration 700/1250  mem: 21.2/477.6MB
Iteration 800/1250  mem: 44.8/477.6MB
Iteration 900/1250  mem: 70/477.6MB
Iteration 1000/1250  mem: 95.3/477.6MB
Iteration 1100/1250  mem: 120.5/477.6MB
Iteration 1200/1250  mem: 145.8/477.6MB
done building BART in 0.593 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 65 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 73.3/477.6MB
Iteration 200/1250  mem: 108.6/477.6MB
Iteration 300/1250  mem: 143.9/477.6MB
Iteration 400/1250  mem: 49.7/477.6MB
Iteration 500/1250  mem: 85/477.6MB
Iteration 600/1250  mem: 120.3/477.6MB
Iteration 700/1250  mem: 155.6/477.6MB
Iteration 800/1250  mem: 65.4/477.6MB
Iteration 900/1250  mem: 100.8/477.6MB
Iteration 1000/1250  mem: 136.2/477.6MB
Iteration 1100/1250  mem: 171.7/477.6MB
Iteration 1200/1250  mem: 80.6/477.6MB
done building BART in 0.728 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 21 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 115.2/477.6MB
Iteration 200/1250  mem: 126.3/477.6MB
Iteration 300/1250  mem: 137.4/477.6MB
Iteration 400/1250  mem: 145.7/477.6MB
Iteration 500/1250  mem: 156.8/477.6MB
Iteration 600/1250  mem: 168/477.6MB
Iteration 700/1250  mem: 179.1/477.6MB
Iteration 800/1250  mem: 190.2/477.6MB
Iteration 900/1250  mem: 200.2/477.6MB
Iteration 1000/1250  mem: 83.9/477.6MB
Iteration 1100/1250  mem: 93.4/477.6MB
Iteration 1200/1250  mem: 103/477.6MB
done building BART in 0.219 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 54 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 141.8/477.6MB
Iteration 200/1250  mem: 172.4/477.6MB
Iteration 300/1250  mem: 200.1/477.6MB
Iteration 400/1250  mem: 95.8/477.6MB
Iteration 500/1250  mem: 122.1/477.6MB
Iteration 600/1250  mem: 150.7/477.6MB
Iteration 700/1250  mem: 179.4/477.6MB
Iteration 800/1250  mem: 205.6/477.6MB
Iteration 900/1250  mem: 105/477.6MB
Iteration 1000/1250  mem: 132.8/477.6MB
Iteration 1100/1250  mem: 160.6/477.6MB
Iteration 1200/1250  mem: 185.8/477.6MB
done building BART in 0.551 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 31 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 221.7/477.6MB
Iteration 200/1250  mem: 113/477.6MB
Iteration 300/1250  mem: 131.5/477.6MB
Iteration 400/1250  mem: 150.1/477.6MB
Iteration 500/1250  mem: 172/477.6MB
Iteration 600/1250  mem: 192.3/477.6MB
Iteration 700/1250  mem: 212.5/477.6MB
Iteration 800/1250  mem: 231.1/477.6MB
Iteration 900/1250  mem: 127.9/477.6MB
Iteration 1000/1250  mem: 148.8/477.6MB
Iteration 1100/1250  mem: 169.7/477.6MB
Iteration 1200/1250  mem: 192.6/477.6MB
done building BART in 0.418 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 71 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 243.9/477.6MB
Iteration 200/1250  mem: 156.4/477.6MB
Iteration 300/1250  mem: 192.6/477.6MB
Iteration 400/1250  mem: 230.6/477.6MB
Iteration 500/1250  mem: 142.9/477.6MB
Iteration 600/1250  mem: 178.5/477.6MB
Iteration 700/1250  mem: 216.1/477.6MB
Iteration 800/1250  mem: 251.7/477.6MB
Iteration 900/1250  mem: 167.7/477.6MB
Iteration 1000/1250  mem: 204.1/477.6MB
Iteration 1100/1250  mem: 240.5/477.6MB
Iteration 1200/1250  mem: 276.8/477.6MB
done building BART in 0.784 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 12 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 184.4/477.6MB
Iteration 200/1250  mem: 189.6/477.6MB
Iteration 300/1250  mem: 197.4/477.6MB
Iteration 400/1250  mem: 202.6/477.6MB
Iteration 500/1250  mem: 210.5/477.6MB
Iteration 600/1250  mem: 215.7/477.6MB
Iteration 700/1250  mem: 220.9/477.6MB
Iteration 800/1250  mem: 228.8/477.6MB
Iteration 900/1250  mem: 234/477.6MB
Iteration 1000/1250  mem: 239.2/477.6MB
Iteration 1100/1250  mem: 247/477.6MB
Iteration 1200/1250  mem: 252.3/477.6MB
done building BART in 0.114 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 36 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 279.4/477.6MB
Iteration 200/1250  mem: 175.3/477.6MB
Iteration 300/1250  mem: 196.8/477.6MB
Iteration 400/1250  mem: 218.4/477.6MB
Iteration 500/1250  mem: 238.2/477.6MB
Iteration 600/1250  mem: 258.1/477.6MB
Iteration 700/1250  mem: 279.6/477.6MB
Iteration 800/1250  mem: 299.1/477.6MB
Iteration 900/1250  mem: 201.9/477.6MB
Iteration 1000/1250  mem: 222/477.6MB
Iteration 1100/1250  mem: 242/477.6MB
Iteration 1200/1250  mem: 264/477.6MB
done building BART in 0.421 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 94 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 210.1/477.6MB
Iteration 200/1250  mem: 259.7/477.6MB
Iteration 300/1250  mem: 309.3/477.6MB
Iteration 400/1250  mem: 66.5/477.6MB
Iteration 500/1250  mem: 117.7/477.6MB
Iteration 600/1250  mem: 168.9/477.6MB
Iteration 700/1250  mem: 103.6/477.6MB
Iteration 800/1250  mem: 157.5/477.6MB
Iteration 900/1250  mem: 94.9/477.6MB
Iteration 1000/1250  mem: 147.5/477.6MB
Iteration 1100/1250  mem: 201.3/477.6MB
Iteration 1200/1250  mem: 142.5/477.6MB
done building BART in 1.253 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 75 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 214/477.6MB
Iteration 200/1250  mem: 140.2/477.6MB
Iteration 300/1250  mem: 181.9/477.6MB
Iteration 400/1250  mem: 221.9/477.6MB
Iteration 500/1250  mem: 141.5/477.6MB
Iteration 600/1250  mem: 182.2/477.6MB
Iteration 700/1250  mem: 223/477.6MB
Iteration 800/1250  mem: 145.5/477.6MB
Iteration 900/1250  mem: 187.8/477.6MB
Iteration 1000/1250  mem: 227.9/477.6MB
Iteration 1100/1250  mem: 152.5/477.6MB
Iteration 1200/1250  mem: 192.9/477.6MB
done building BART in 0.884 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 18 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 226.4/477.6MB
Iteration 200/1250  mem: 234.2/477.6MB
Iteration 300/1250  mem: 244.7/477.6MB
Iteration 400/1250  mem: 255.1/477.6MB
Iteration 500/1250  mem: 262.9/477.6MB
Iteration 600/1250  mem: 153.1/477.6MB
Iteration 700/1250  mem: 162.2/477.6MB
Iteration 800/1250  mem: 171.3/477.6MB
Iteration 900/1250  mem: 180.3/477.6MB
Iteration 1000/1250  mem: 191.2/477.6MB
Iteration 1100/1250  mem: 200.3/477.6MB
Iteration 1200/1250  mem: 209.3/477.6MB
done building BART in 0.195 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 32 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 236.8/477.6MB
Iteration 200/1250  mem: 255/477.6MB
Iteration 300/1250  mem: 273.3/477.6MB
Iteration 400/1250  mem: 171.2/477.6MB
Iteration 500/1250  mem: 190.7/477.6MB
Iteration 600/1250  mem: 210.2/477.6MB
Iteration 700/1250  mem: 229.6/477.6MB
Iteration 800/1250  mem: 249.1/477.6MB
Iteration 900/1250  mem: 268.6/477.6MB
Iteration 1000/1250  mem: 287.9/477.6MB
Iteration 1100/1250  mem: 193.5/477.6MB
Iteration 1200/1250  mem: 211.1/477.6MB
done building BART in 0.414 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 52 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 252.8/477.6MB
Iteration 200/1250  mem: 277/477.6MB
Iteration 300/1250  mem: 301.2/477.6MB
Iteration 400/1250  mem: 202.6/477.6MB
Iteration 500/1250  mem: 228.5/477.6MB
Iteration 600/1250  mem: 252.2/477.6MB
Iteration 700/1250  mem: 278/477.6MB
Iteration 800/1250  mem: 303.9/477.6MB
Iteration 900/1250  mem: 202.8/477.6MB
Iteration 1000/1250  mem: 228.3/477.6MB
Iteration 1100/1250  mem: 251.6/477.6MB
Iteration 1200/1250  mem: 277.2/477.6MB
done building BART in 0.557 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 89 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 218.9/477.6MB
Iteration 200/1250  mem: 270.3/477.6MB
Iteration 300/1250  mem: 318.5/477.6MB
Iteration 400/1250  mem: 242.2/477.6MB
Iteration 500/1250  mem: 291.4/477.6MB
Iteration 600/1250  mem: 221.3/477.6MB
Iteration 700/1250  mem: 272.2/477.6MB
Iteration 800/1250  mem: 320.9/477.6MB
Iteration 900/1250  mem: 71.4/477.6MB
Iteration 1000/1250  mem: 118.7/477.6MB
Iteration 1100/1250  mem: 168.4/477.6MB
Iteration 1200/1250  mem: 102.3/477.6MB
done building BART in 1.175 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 20 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 141.6/477.6MB
Iteration 200/1250  mem: 152.2/477.6MB
Iteration 300/1250  mem: 162.8/477.6MB
Iteration 400/1250  mem: 173.4/477.6MB
Iteration 500/1250  mem: 184.1/477.6MB
Iteration 600/1250  mem: 194.7/477.6MB
Iteration 700/1250  mem: 84.8/477.6MB
Iteration 800/1250  mem: 94.8/477.6MB
Iteration 900/1250  mem: 104.8/477.6MB
Iteration 1000/1250  mem: 114.8/477.6MB
Iteration 1100/1250  mem: 124.8/477.6MB
Iteration 1200/1250  mem: 134.8/477.6MB
done building BART in 0.232 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 13 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 149.8/477.6MB
Iteration 200/1250  mem: 157.8/477.6MB
Iteration 300/1250  mem: 163.1/477.6MB
Iteration 400/1250  mem: 171.1/477.6MB
Iteration 500/1250  mem: 176.4/477.6MB
Iteration 600/1250  mem: 184.3/477.6MB
Iteration 700/1250  mem: 189.6/477.6MB
Iteration 800/1250  mem: 197.6/477.6MB
Iteration 900/1250  mem: 202.9/477.6MB
Iteration 1000/1250  mem: 87.4/477.6MB
Iteration 1100/1250  mem: 93.3/477.6MB
Iteration 1200/1250  mem: 101.3/477.6MB
done building BART in 0.15 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 96 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 160.6/477.6MB
Iteration 200/1250  mem: 212.7/477.6MB
Iteration 300/1250  mem: 135.9/477.6MB
Iteration 400/1250  mem: 188.5/477.6MB
Iteration 500/1250  mem: 117.6/477.6MB
Iteration 600/1250  mem: 168.9/477.6MB
Iteration 700/1250  mem: 222.4/477.6MB
Iteration 800/1250  mem: 149.6/477.6MB
Iteration 900/1250  mem: 203.4/477.6MB
Iteration 1000/1250  mem: 132.2/477.6MB
Iteration 1100/1250  mem: 184.8/477.6MB
Iteration 1200/1250  mem: 237.4/477.6MB
done building BART in 1.159 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 75 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 185/477.6MB
Iteration 200/1250  mem: 226.2/477.6MB
Iteration 300/1250  mem: 270.1/477.6MB
Iteration 400/1250  mem: 184.9/477.6MB
Iteration 500/1250  mem: 234.2/477.6MB
Iteration 600/1250  mem: 282.8/477.6MB
Iteration 700/1250  mem: 213.2/477.6MB
Iteration 800/1250  mem: 262.7/477.6MB
Iteration 900/1250  mem: 194.1/477.6MB
Iteration 1000/1250  mem: 242.5/477.6MB
Iteration 1100/1250  mem: 290.8/477.6MB
Iteration 1200/1250  mem: 243.5/477.6MB
done building BART in 1.042 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 45 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 293.4/477.6MB
Iteration 200/1250  mem: 217.5/477.6MB
Iteration 300/1250  mem: 241.1/477.6MB
Iteration 400/1250  mem: 264.8/477.6MB
Iteration 500/1250  mem: 288.4/477.6MB
Iteration 600/1250  mem: 312/477.6MB
Iteration 700/1250  mem: 233.5/477.6MB
Iteration 800/1250  mem: 257/477.6MB
Iteration 900/1250  mem: 280.4/477.6MB
Iteration 1000/1250  mem: 303.9/477.6MB
Iteration 1100/1250  mem: 327.4/477.6MB
Iteration 1200/1250  mem: 250.7/477.6MB
done building BART in 0.53 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 16 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 273.2/477.6MB
Iteration 200/1250  mem: 282.4/477.6MB
Iteration 300/1250  mem: 291.7/477.6MB
Iteration 400/1250  mem: 298.6/477.6MB
Iteration 500/1250  mem: 307.8/477.6MB
Iteration 600/1250  mem: 314.7/477.6MB
Iteration 700/1250  mem: 324/477.6MB
Iteration 800/1250  mem: 330.9/477.6MB
Iteration 900/1250  mem: 340.1/477.6MB
Iteration 1000/1250  mem: 241.3/477.6MB
Iteration 1100/1250  mem: 249.2/477.6MB
Iteration 1200/1250  mem: 257.1/477.6MB
done building BART in 0.167 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 49 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 289.8/477.6MB
Iteration 200/1250  mem: 315.2/477.6MB
Iteration 300/1250  mem: 340.6/477.6MB
Iteration 400/1250  mem: 24.2/477.6MB
Iteration 500/1250  mem: 47.7/477.6MB
Iteration 600/1250  mem: 73.3/477.6MB
Iteration 700/1250  mem: 96.8/477.6MB
Iteration 800/1250  mem: 122.4/477.6MB
Iteration 900/1250  mem: 38.7/477.6MB
Iteration 1000/1250  mem: 63.1/477.6MB
Iteration 1100/1250  mem: 87.5/477.6MB
Iteration 1200/1250  mem: 111.9/477.6MB
done building BART in 0.553 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 65 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 54.1/477.6MB
Iteration 200/1250  mem: 88.9/477.6MB
Iteration 300/1250  mem: 123.6/477.6MB
Iteration 400/1250  mem: 42.3/477.6MB
Iteration 500/1250  mem: 77.3/477.6MB
Iteration 600/1250  mem: 110.6/477.6MB
Iteration 700/1250  mem: 145.6/477.6MB
Iteration 800/1250  mem: 70.9/477.6MB
Iteration 900/1250  mem: 105.7/477.6MB
Iteration 1000/1250  mem: 140.5/477.6MB
Iteration 1100/1250  mem: 64.6/477.6MB
Iteration 1200/1250  mem: 97.7/477.6MB
done building BART in 0.721 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 21 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 129.1/477.6MB
Iteration 200/1250  mem: 139.1/477.6MB
Iteration 300/1250  mem: 151.5/477.6MB
Iteration 400/1250  mem: 161.5/477.6MB
Iteration 500/1250  mem: 171.5/477.6MB
Iteration 600/1250  mem: 184/477.6MB
Iteration 700/1250  mem: 78.1/477.6MB
Iteration 800/1250  mem: 90.1/477.6MB
Iteration 900/1250  mem: 100.2/477.6MB
Iteration 1000/1250  mem: 110.2/477.6MB
Iteration 1100/1250  mem: 120.3/477.6MB
Iteration 1200/1250  mem: 132.4/477.6MB
done building BART in 0.235 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 20 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 155.6/477.6MB
Iteration 200/1250  mem: 168.4/477.6MB
Iteration 300/1250  mem: 183.8/477.6MB
Iteration 400/1250  mem: 196.7/477.6MB
Iteration 500/1250  mem: 80.3/477.6MB
Iteration 600/1250  mem: 93.7/477.6MB
Iteration 700/1250  mem: 107.1/477.6MB
Iteration 800/1250  mem: 120.5/477.6MB
Iteration 900/1250  mem: 135.8/477.6MB
Iteration 1000/1250  mem: 149.2/477.6MB
Iteration 1100/1250  mem: 162.6/477.6MB
Iteration 1200/1250  mem: 176/477.6MB
done building BART in 0.264 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
Bayesian Additive Regression Trees 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  num_trees  k           alpha      beta       nu         RMSE       Rsquared  
  12         1.90402437  0.9457226  3.3984046  1.8246338  0.3056277  0.92429365
  13         4.05381586  0.9443441  3.3118662  0.3482501  0.3380456  0.87255454
  16         2.50038047  0.9651474  1.8999475  0.7358920  0.3930560  0.84436157
  18         4.61440816  0.9048299  1.3053029  2.2158330  0.2757296  0.94446815
  20         4.86010489  0.9103228  1.4167779  1.9585063  0.2744204  0.93930181
  21         3.86724270  0.9736288  2.5169897  0.7472517  0.3131852  0.90260844
  31         3.68403596  0.9997884  0.3105422  3.5023378  0.3822443  0.85365992
  32         2.53730354  0.9252104  0.6199114  1.2537487  0.4722927  0.73311711
  36         2.03563240  0.9747534  0.9614554  3.9954101  0.4299113  0.77528060
  45         2.50689198  0.9245740  1.6495770  3.6516966  0.3536922  0.86420154
  49         3.43320568  0.9974820  2.9680990  0.6669459  0.3688224  0.85802774
  52         0.03904269  0.9305494  1.9746333  1.3022249  0.3785307  0.86886500
  54         3.98485295  0.9648076  2.7337550  0.2505733  0.3568121  0.88330690
  65         1.80260059  0.9876552  1.5183958  1.7039257  0.4050820  0.82810690
  71         2.08470528  0.9660599  3.6450283  4.7838062  0.3713457  0.85102336
  75         1.72095411  0.9066537  1.3591322  2.6952474  0.4394296  0.76125104
  75         2.88307964  0.9187729  0.2315345  2.2816794  0.4605650  0.75153680
  89         1.05024447  0.9871638  1.2394867  4.6907366  0.5289283  0.65887948
  94         0.32329963  0.9545722  0.4937802  2.1066237  2.1825154  0.07952871
  96         2.78685913  0.9522877  1.6849274  4.6325474  0.4380617  0.76982216
  MAE        Selected
  0.1489363          
  0.1497393          
  0.1785628          
  0.1331101          
  0.1231238  *       
  0.1439282          
  0.2056492          
  0.2884762          
  0.2482945          
  0.1870251          
  0.2115896          
  0.1899541          
  0.2066828          
  0.2440662          
  0.2163232          
  0.2644256          
  0.2707403          
  0.3402146          
  1.7068092          
  0.2599615          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were num_trees = 20, k = 4.860105, alpha
 = 0.9103228, beta = 1.416778 and nu = 1.958506.
[1] "Sat Mar 10 01:49:55 2018"
Error in investigate_var_importance(object, plot = FALSE) : 
  could not find function "investigate_var_importance"
Error : package arm is required
Error : package arm is required
Error : package arm is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:50:08 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "bayesglm"                
t=100, m=4
t=200, m=3
t=300, m=5
t=400, m=4
t=500, m=8
t=600, m=3
t=700, m=3
t=800, m=3
t=900, m=5
t=100, m=4
t=200, m=1
t=300, m=4
t=400, m=3
t=500, m=7
t=600, m=2
t=700, m=4
t=800, m=5
t=900, m=5
t=100, m=4
t=200, m=5
t=300, m=5
t=400, m=6
t=500, m=5
t=600, m=5
t=700, m=4
t=800, m=5
t=900, m=4
t=100, m=3
t=200, m=3
t=300, m=3
t=400, m=3
t=500, m=5
t=600, m=5
t=700, m=7
t=800, m=7
t=900, m=5
The Bayesian lasso 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  sparsity    RMSE       Rsquared   MAE        Selected
  0.02424925  0.4194458  0.7762393  0.2955104          
  0.04247987  0.4194458  0.7762393  0.2955104          
  0.07509305  0.4194458  0.7762393  0.2955104          
  0.08797359  0.4194458  0.7762393  0.2955104          
  0.11240018  0.4194458  0.7762393  0.2955104          
  0.12932486  0.4194458  0.7762393  0.2955104          
  0.23385692  0.4194458  0.7762393  0.2955104          
  0.25065253  0.4194458  0.7762393  0.2955104          
  0.28608022  0.4194939  0.7761210  0.2955792          
  0.38469318  0.4158368  0.7802439  0.2936537          
  0.43352645  0.4186469  0.7783302  0.2976403          
  0.46643031  0.4164084  0.7808250  0.2973203          
  0.48499598  0.4164084  0.7808250  0.2973203          
  0.60646832  0.4129831  0.7839703  0.2924703          
  0.67666045  0.4129831  0.7839703  0.2924703          
  0.71440735  0.4129831  0.7839703  0.2924703          
  0.72492907  0.4129831  0.7839703  0.2924703          
  0.87664305  0.4097256  0.7878733  0.2910325          
  0.93305532  0.4097256  0.7878733  0.2910325          
  0.95202160  0.4097256  0.7878733  0.2910325  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was sparsity = 0.9520216.
[1] "Sat Mar 10 01:50:22 2018"
t=100, m=5
t=200, m=4
t=300, m=5
t=400, m=3
t=500, m=7
t=600, m=3
t=700, m=2
t=800, m=6
t=900, m=7
t=100, m=5
t=200, m=6
t=300, m=4
t=400, m=3
t=500, m=2
t=600, m=5
t=700, m=6
t=800, m=4
t=900, m=3
t=100, m=5
t=200, m=3
t=300, m=4
t=400, m=3
t=500, m=3
t=600, m=3
t=700, m=6
t=800, m=6
t=900, m=5
t=100, m=3
t=200, m=4
t=300, m=6
t=400, m=5
t=500, m=2
t=600, m=4
t=700, m=3
t=800, m=3
t=900, m=6
Bayesian Ridge Regression (Model Averaged) 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results:

  RMSE       Rsquared   MAE     
  0.4184369  0.7773856  0.294624

[1] "Sat Mar 10 01:50:35 2018"
t=100, m=2
t=200, m=3
t=300, m=1
t=400, m=2
t=500, m=2
t=600, m=3
t=700, m=3
t=800, m=3
t=900, m=3
t=100, m=1
t=200, m=1
t=300, m=2
t=400, m=1
t=500, m=1
t=600, m=1
t=700, m=1
t=800, m=1
t=900, m=1
t=100, m=4
t=200, m=3
t=300, m=4
t=400, m=2
t=500, m=1
t=600, m=8
t=700, m=1
t=800, m=2
t=900, m=3
t=100, m=2
t=200, m=1
t=300, m=2
t=400, m=1
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=1
t=900, m=3
Bayesian Ridge Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results:

  RMSE       Rsquared   MAE      
  0.4171244  0.7803424  0.3037234

[1] "Sat Mar 10 01:50:48 2018"
Boosted Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  nu           mstop  RMSE       Rsquared   MAE        Selected
  0.005677314  233    0.4434929  0.7878733  0.2411926          
  0.039731296  466    0.4553922  0.7416183  0.3185441          
  0.126819287  438    0.4745817  0.7258732  0.3340929          
  0.207170302  362    0.4752847  0.7253182  0.3345925          
  0.216951551  303    0.4751464  0.7254268  0.3344935          
  0.229102119   13    0.4125487  0.7827491  0.2829829  *       
  0.244868762  143    0.4716176  0.7282911  0.3318454          
  0.250747692  338    0.4753614  0.7252580  0.3346433          
  0.300545580   38    0.4441159  0.7512842  0.3090124          
  0.301325659  192    0.4750163  0.7255279  0.3343987          
  0.304968964  126    0.4730238  0.7270742  0.3328949          
  0.334865724  476    0.4754009  0.7252274  0.3346690          
  0.346392941  357    0.4753982  0.7252294  0.3346678          
  0.412298040  217    0.4753733  0.7252490  0.3346535          
  0.442347508  117    0.4749322  0.7255869  0.3343726          
  0.464295675   65    0.4712718  0.7283837  0.3317911          
  0.478385384  243    0.4753972  0.7252304  0.3346670          
  0.486647140   22    0.4467620  0.7486694  0.3115480          
  0.553806097   44    0.4697215  0.7296205  0.3302273          
  0.583240566   57    0.4734070  0.7268078  0.3333055          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 13 and nu = 0.2291021.
[1] "Sat Mar 10 01:52:00 2018"
Conditional Inference Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE        Selected
  1     0.8238705  0.4395150  0.4839997          
  2     0.7898172  0.6037845  0.4516725          
  3     0.7444190  0.6669635  0.4126127          
  4     0.6785152  0.7164278  0.3608109          
  5     0.6102566  0.7595365  0.3010457          
  6     0.5359493  0.7939218  0.2417295          
  7     0.4754884  0.8092707  0.2034717          
  8     0.4380428  0.8141370  0.1912121          
  9     0.4083279  0.8259458  0.1888249  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 01:52:17 2018"
Error in varimp(object, ...) : could not find function "varimp"
Conditional Inference Tree 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  mincriterion  RMSE       Rsquared  MAE        Selected
  0.02424925    0.4301779  0.756517  0.2482064          
  0.04247987    0.4301779  0.756517  0.2482064          
  0.07509305    0.4301779  0.756517  0.2482064          
  0.08797359    0.4301779  0.756517  0.2482064          
  0.11240018    0.4301779  0.756517  0.2482064          
  0.12932486    0.4301779  0.756517  0.2482064          
  0.23385692    0.4301779  0.756517  0.2482064          
  0.25065253    0.4301779  0.756517  0.2482064          
  0.28608022    0.4301779  0.756517  0.2482064          
  0.38469318    0.4301779  0.756517  0.2482064          
  0.43352645    0.4301779  0.756517  0.2482064          
  0.46643031    0.4301779  0.756517  0.2482064          
  0.48499598    0.4301779  0.756517  0.2482064          
  0.60646832    0.4301779  0.756517  0.2482064          
  0.67666045    0.4301779  0.756517  0.2482064          
  0.71440735    0.4301779  0.756517  0.2482064          
  0.72492907    0.4301779  0.756517  0.2482064          
  0.87664305    0.4301779  0.756517  0.2482064          
  0.93305532    0.4301779  0.756517  0.2482064          
  0.95202160    0.4301779  0.756517  0.2482064  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mincriterion = 0.9520216.
[1] "Sat Mar 10 01:52:32 2018"
Conditional Inference Tree 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  maxdepth  mincriterion  RMSE       Rsquared   MAE        Selected
   1        0.380804874   0.5679043  0.5624410  0.3775965          
   1        0.810763171   0.5679043  0.5624410  0.3775965          
   2        0.500076094   0.4686330  0.7145037  0.3152272          
   2        0.773448540   0.4686330  0.7145037  0.3152272          
   2        0.922881632   0.4686330  0.7145037  0.3152272          
   2        0.972020978   0.4686330  0.7145037  0.3152272          
   4        0.507460708   0.4304445  0.7561537  0.2513141          
   4        0.736807193   0.4304445  0.7561537  0.2513141          
   5        0.407126480   0.4301779  0.7565170  0.2482064  *       
   6        0.501378395   0.4301779  0.7565170  0.2482064          
   7        0.007808538   0.4301779  0.7565170  0.2482064          
   7        0.686641136   0.4301779  0.7565170  0.2482064          
   8        0.796970590   0.4301779  0.7565170  0.2482064          
  10        0.360520118   0.4301779  0.7565170  0.2482064          
  11        0.344190822   0.4301779  0.7565170  0.2482064          
  11        0.416941055   0.4301779  0.7565170  0.2482064          
  11        0.576615929   0.4301779  0.7565170  0.2482064          
  14        0.064659926   0.4301779  0.7565170  0.2482064          
  14        0.210048893   0.4301779  0.7565170  0.2482064          
  15        0.557371826   0.4301779  0.7565170  0.2482064          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were maxdepth = 5 and mincriterion
 = 0.4071265.
[1] "Sat Mar 10 01:52:45 2018"
Cubist 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  committees  neighbors  RMSE       Rsquared   MAE         Selected
   1          4          0.1402602  0.9885853  0.06289193          
   7          9          0.1458914  0.9879937  0.06148476          
  22          8          0.1364154  0.9894651  0.05869663          
  35          7          0.1343405  0.9900062  0.05779687          
  37          6          0.1345676  0.9899832  0.05787582          
  39          0          0.1343015  0.9900162  0.05557805          
  41          2          0.1373289  0.9891335  0.05877721          
  42          6          0.1343020  0.9900804  0.05766640          
  51          0          0.1314737  0.9902695  0.05429903          
  51          2          0.1337911  0.9895301  0.05668056          
  51          3          0.1328598  0.9897369  0.05609718          
  56          9          0.1306348  0.9902277  0.05544673          
  58          7          0.1294655  0.9904228  0.05513967          
  69          4          0.1264952  0.9902811  0.05287112          
  74          2          0.1276901  0.9899900  0.05312450          
  78          1          0.1292473  0.9886687  0.05474549          
  80          4          0.1249870  0.9903547  0.05205519  *       
  82          0          0.1254133  0.9904050  0.05150311          
  93          0          0.1251871  0.9901829  0.05134631          
  98          1          0.1279486  0.9887498  0.05372537          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were committees = 80 and neighbors = 4.
[1] "Sat Mar 10 01:53:26 2018"
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
Stacked AutoEncoder Deep Neural Network 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  layer1  layer2  layer3  hidden_dropout  visible_dropout  RMSE     
   2       9      10      0.59472080      0.25544873       0.8459792
   2      17      10      0.57957658      0.04875502       0.8451093
   3      11      14      0.33249082      0.10302489       0.8449604
   3      19       2      0.22842801      0.31021663       0.8433122
   4      16      15      0.44047320      0.10461524       0.8502496
   4      20       3      0.24793614      0.27419088       0.8425878
   6      11       6      0.10848449      0.17552482       0.8411459
   6      15      20      0.05434489      0.49032730       0.8431718
   7       9      16      0.16825469      0.55935741       0.8417483
   9      11       6      0.28867597      0.51123753       0.8417984
  10       2       7      0.34556083      0.18231149       0.8430565
  10      15      20      0.51941732      0.09337243       0.8412771
  11      17      14      0.47840713      0.03508026       0.8795735
  13       8      18      0.26571926      0.23854960       0.8473441
  14       9      14      0.63787996      0.66973286       0.8444684
  15       8       3      0.23784814      0.37733464       0.8412058
  15      12       5      0.04051854      0.31943511       0.8417691
  18       5      18      0.21691018      0.65670312       1.2202910
  19       3      12      0.08641154      0.29492731       0.8564954
  20      12      11      0.29486229      0.64855664       0.8614643
  Rsquared    MAE        Selected
  0.05203180  0.5184278          
  0.08618493  0.5153992          
  0.07041705  0.4914537          
  0.08969098  0.5171121          
  0.04401274  0.4900452          
  0.06548610  0.5231908          
  0.14432998  0.5027358  *       
  0.03025653  0.5064960          
  0.06352146  0.5040372          
  0.07016852  0.5002755          
  0.06136686  0.4982630          
  0.10112353  0.5117487          
  0.12953846  0.5261108          
  0.07136031  0.5275087          
  0.06911290  0.4995881          
  0.08866287  0.5058746          
  0.10754807  0.4951862          
  0.11556202  0.9541255          
  0.13128167  0.5451813          
  0.05938180  0.4954591          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were layer1 = 6, layer2 = 11, layer3 =
 6, hidden_dropout = 0.1084845 and visible_dropout = 0.1755248.
[1] "Sat Mar 10 01:53:40 2018"
Multivariate Adaptive Regression Spline 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  degree  nprune  RMSE       Rsquared   MAE        Selected
  1       2       0.4112120  0.7798121  0.2740352          
  1       3       0.1967804  0.9712425  0.1385216          
  1       4       0.1895010  0.9762346  0.1067211          
  1       5       0.1904614  0.9754148  0.1077846          
  2       2       0.4112120  0.7798121  0.2740352          
  2       3       0.1967804  0.9712425  0.1385216          
  2       4       0.1834907  0.9749233  0.1024482  *       
  2       5       0.1982466  0.9605724  0.1091759          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nprune = 4 and degree = 2.
[1] "Sat Mar 10 01:53:54 2018"
Extreme Learning Machine 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  nhid  actfun   RMSE       Rsquared    MAE        Selected
   1    radbas   0.8401450  0.05283524  0.4799172          
   1    tansig   0.8231109  0.10361957  0.4792090          
   2    purelin  0.8103827  0.12745643  0.5344676          
   2    tansig   0.8323894  0.16770588  0.5392099          
   3    tansig   0.7567872  0.21993303  0.4969434          
   5    purelin  0.6668425  0.43398341  0.4421710          
   6    radbas   0.8854250  0.04289935  0.5813891          
   8    purelin  0.5530657  0.63463459  0.3730612          
   9    purelin  0.5594474  0.61901230  0.3941554          
   9    sin      0.8991044  0.02382333  0.6024181          
  10    tansig   0.7029466  0.34997177  0.4791096          
  12    radbas   0.9094051  0.07327698  0.6032083          
  13    radbas   0.9019817  0.03158851  0.6431764          
  14    purelin  0.4817290  0.72258252  0.3427649  *       
  14    radbas   1.0860464  0.01167623  0.7737900          
  17    sin      0.9727960  0.01998694  0.6450065          
  18    sin      0.9345391  0.10865179  0.6216054          
  19    purelin  0.4817290  0.72258252  0.3427649          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nhid = 14 and actfun = purelin.
[1] "Sat Mar 10 01:54:07 2018"
Elasticnet 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  lambda        fraction     RMSE       Rsquared   MAE        Selected
  1.397962e-05  0.380804874  0.4914534  0.7878733  0.2615388          
  1.798371e-05  0.810763171  0.4448681  0.7552153  0.3096705          
  2.822008e-05  0.500076094  0.4487635  0.7857720  0.2665094          
  3.371643e-05  0.922881632  0.4662714  0.7343881  0.3291691          
  4.724992e-05  0.972020978  0.4759516  0.7267961  0.3380623          
  5.969654e-05  0.773448540  0.4391656  0.7609488  0.3039874          
  2.530122e-04  0.736807193  0.4363865  0.7645841  0.2979799          
  3.190915e-04  0.507460708  0.4476415  0.7854283  0.2669057          
  5.205726e-04  0.407126480  0.4778733  0.7878733  0.2625330          
  2.033101e-03  0.501378395  0.4484309  0.7857196  0.2664387          
  3.991708e-03  0.686641136  0.4354488  0.7698487  0.2872758          
  6.289004e-03  0.007808538  0.8327018  0.7878733  0.4981016          
  8.127854e-03  0.796970590  0.4421715  0.7576708  0.3071114          
  4.353213e-02  0.360520118  0.5039432  0.7878733  0.2581208          
  1.148055e-01  0.416941055  0.4699558  0.7878733  0.2507322          
  1.933945e-01  0.576615929  0.4249365  0.7807662  0.2743166  *       
  2.236528e-01  0.344190822  0.5157240  0.7878733  0.2513845          
  1.819107e+00  0.210048893  0.6102665  0.7878733  0.3179483          
  3.965810e+00  0.064659926  0.7608497  0.7878733  0.4382163          
  5.153824e+00  0.557371826  0.4569414  0.7298783  0.2945472          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were fraction = 0.5766159 and lambda
 = 0.1933945.
[1] "Sat Mar 10 01:54:20 2018"
Error : package evtree is required
In addition: There were 44 warnings (use warnings() to see them)
Error : package evtree is required
Error : package evtree is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:54:32 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "evtree"                  
Random Forest by Randomization 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  mtry  numRandomCuts  RMSE       Rsquared   MAE         Selected
  1     10             0.6649098  0.5531156  0.40404915          
  1     13             0.6653800  0.5467375  0.40248188          
  1     21             0.6777002  0.5001944  0.41281924          
  1     24             0.6676269  0.5015541  0.40866226          
  2     20             0.5585704  0.7027387  0.32373544          
  2     25             0.5606260  0.7046395  0.32759139          
  3     11             0.4565877  0.8197549  0.24205472          
  3     13             0.4667522  0.8014940  0.24913950          
  3     19             0.4691713  0.8006759  0.24742623          
  4     13             0.3867667  0.8731748  0.18487987          
  4     18             0.4016963  0.8528218  0.19124669          
  5      1             0.3721399  0.8778412  0.17617011          
  5     20             0.3371857  0.9025988  0.15396069          
  6     10             0.2804116  0.9346573  0.11575485          
  7      9             0.2522931  0.9504846  0.09492464          
  7     11             0.2524859  0.9507324  0.09735377          
  7     15             0.2636314  0.9501630  0.10327829          
  8      6             0.2448467  0.9589622  0.08753902          
  9      2             0.2338782  0.9570171  0.08069059  *       
  9     14             0.2852792  0.9601735  0.10081878          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 9 and numRandomCuts = 2.
[1] "Sat Mar 10 01:55:02 2018"
Error in object$beta[, ii$j] : incorrect number of dimensions
In addition: There were 16 warnings (use warnings() to see them)
Error in confusionMatrix.default(p[, 1], p[, 2]) : 
  The data must contain some levels that overlap the reference.
In addition: Warning messages:
1: predictions failed for Fold1: lambda=4.503e-01, k=1 Error in object$beta[, ii$j] : incorrect number of dimensions
 
2: predictions failed for Fold1: lambda=2.793e-01, k=4 Error in object$beta[, ii$j] : incorrect number of dimensions
 
3: predictions failed for Fold2: lambda=4.503e-01, k=1 Error in object$beta[, ii$j] : incorrect number of dimensions
 
4: predictions failed for Fold2: lambda=2.793e-01, k=4 Error in object$beta[, ii$j] : incorrect number of dimensions
 
5: predictions failed for Fold3: lambda=4.503e-01, k=1 Error in object$beta[, ii$j] : incorrect number of dimensions
 
6: predictions failed for Fold3: lambda=2.793e-01, k=4 Error in object$beta[, ii$j] : incorrect number of dimensions
 
7: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
8: In train.default(x = data.frame(training[, 2:length(training[1,  :
  missing values found in aggregated results
Ridge Regression with Variable Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 76, 76, 76, 76, 76, 76, ... 
Resampling results across tuning parameters:

  lambda  k  RMSE       Rsquared   MAE        Selected
  1e-05   2  0.4763756  0.7647892  0.3287671          
  1e-05   5  0.4938077  0.7420348  0.3407783          
  1e-05   9  0.4981430  0.7394388  0.3448661          
  1e-03   2  0.4762337  0.7648052  0.3284636          
  1e-03   5  0.4936272  0.7420625  0.3404365          
  1e-03   9  0.4979314  0.7394939  0.3444559          
  1e-01   2  0.4752264  0.7677248  0.3053347  *       
  1e-01   5  0.4883739  0.7492071  0.3124061          
  1e-01   9  0.4883739  0.7492071  0.3124061          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were k = 2 and lambda = 0.1.
[1] "Sat Mar 10 01:55:19 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 15 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 13 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:55:34 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "gam"                     
Boosted Generalized Additive Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  mstop  prune  RMSE        Rsquared   MAE         Selected
   25    yes    0.17848090  0.9749657  0.08585493          
   43    no     0.13111335  0.9874552  0.06183411          
   76    no     0.11215287  0.9930068  0.05129144          
   88    no     0.10935802  0.9936516  0.05045905          
  113    no     0.10603352  0.9943122  0.04887660          
  130    no     0.10349043  0.9947538  0.04717496          
  234    no     0.09188554  0.9961159  0.03770925          
  251    no     0.09041386  0.9962490  0.03661275          
  287    yes    0.08798670  0.9964728  0.03510902          
  385    no     0.08339920  0.9968378  0.03230676          
  434    no     0.08186263  0.9969485  0.03151749          
  467    yes    0.08097497  0.9970128  0.03114041          
  485    no     0.08062059  0.9970381  0.03103336          
  607    yes    0.07910164  0.9970622  0.03052259          
  677    yes    0.07849442  0.9970785  0.03034333          
  715    no     0.07738904  0.9972410  0.02996985          
  725    yes    0.07807392  0.9970906  0.03020387          
  877    yes    0.07733484  0.9971125  0.02996027          
  934    yes    0.07717035  0.9971183  0.02988483          
  953    no     0.07567750  0.9973501  0.02941282  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 953 and prune = no.
[1] "Sat Mar 10 01:56:06 2018"
Generalized Additive Model using Splines 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  df         RMSE       Rsquared   MAE        Selected
  0.1212462  0.4817264  0.7225856  0.3427634          
  0.2123993  0.4817264  0.7225856  0.3427634          
  0.3754652  0.4817264  0.7225856  0.3427634          
  0.4398679  0.4817264  0.7225856  0.3427634          
  0.5620009  0.4817264  0.7225856  0.3427634          
  0.6466243  0.4817264  0.7225856  0.3427634          
  1.1692846  0.4670005  0.7424080  0.3324294          
  1.2532627  0.4598870  0.7518935  0.3269095          
  1.4304011  0.4448872  0.7714370  0.3147138          
  1.9234659  0.4002502  0.8239263  0.2731801          
  2.1676323  0.3765090  0.8482023  0.2533117          
  2.3321516  0.3603677  0.8634199  0.2416335          
  2.4249799  0.3513230  0.8715317  0.2348364          
  3.0323416  0.2978070  0.9140882  0.1950354          
  3.3833023  0.2723772  0.9312568  0.1750133          
  3.5720367  0.2604362  0.9386292  0.1659311          
  3.6246453  0.2573057  0.9404859  0.1637099          
  4.3832152  0.2208602  0.9596744  0.1369881          
  4.6652766  0.2109325  0.9641164  0.1303963          
  4.7601080  0.2079771  0.9653762  0.1285847  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was df = 4.760108.
[1] "Sat Mar 10 01:56:24 2018"
Error in .local(object, ...) : test vector does not match model !
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:56:40 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "gaussprLinear"           
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:57:03 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "gaussprPoly"             
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 01:57:20 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "gaussprRadial"           
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.7628             nan     0.0299    0.0137
     2        0.7488             nan     0.0299    0.0132
     3        0.7302             nan     0.0299    0.0090
     4        0.7190             nan     0.0299    0.0119
     5        0.7103             nan     0.0299    0.0096
     6        0.6971             nan     0.0299    0.0109
     7        0.6890             nan     0.0299    0.0088
     8        0.6818             nan     0.0299    0.0079
     9        0.6705             nan     0.0299    0.0075
    10        0.6571             nan     0.0299    0.0085
    20        0.5780             nan     0.0299    0.0016
    40        0.5069             nan     0.0299   -0.0031
    60        0.4804             nan     0.0299   -0.0007
    80        0.4523             nan     0.0299   -0.0037
   100        0.4400             nan     0.0299   -0.0023
   120        0.4254             nan     0.0299   -0.0033
   140        0.4242             nan     0.0299   -0.0011
   160        0.4142             nan     0.0299   -0.0018
   180        0.4063             nan     0.0299   -0.0024
   200        0.3971             nan     0.0299   -0.0017
   220        0.3893             nan     0.0299   -0.0015
   240        0.3796             nan     0.0299   -0.0008
   260        0.3715             nan     0.0299   -0.0028
   280        0.3687             nan     0.0299   -0.0014
   300        0.3604             nan     0.0299   -0.0021
   320        0.3549             nan     0.0299   -0.0006
   340        0.3473             nan     0.0299   -0.0010
   360        0.3401             nan     0.0299   -0.0018
   380        0.3370             nan     0.0299   -0.0017
   400        0.3314             nan     0.0299   -0.0004
   420        0.3284             nan     0.0299   -0.0018
   440        0.3224             nan     0.0299   -0.0013

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.7567             nan     0.0409    0.0117
     2        0.7401             nan     0.0409    0.0185
     3        0.7260             nan     0.0409    0.0155
     4        0.7172             nan     0.0409    0.0103
     5        0.6940             nan     0.0409    0.0119
     6        0.6811             nan     0.0409    0.0135
     7        0.6666             nan     0.0409    0.0105
     8        0.6500             nan     0.0409    0.0109
     9        0.6438             nan     0.0409    0.0076
    10        0.6293             nan     0.0409    0.0106
    20        0.5658             nan     0.0409    0.0049
    40        0.4989             nan     0.0409   -0.0056
    60        0.4715             nan     0.0409    0.0009
    80        0.4501             nan     0.0409   -0.0005
   100        0.4333             nan     0.0409   -0.0029
   120        0.4198             nan     0.0409   -0.0014
   140        0.4101             nan     0.0409   -0.0024
   160        0.4014             nan     0.0409   -0.0015
   180        0.3911             nan     0.0409   -0.0028
   200        0.3827             nan     0.0409   -0.0013
   220        0.3754             nan     0.0409   -0.0019
   240        0.3686             nan     0.0409   -0.0018
   260        0.3612             nan     0.0409   -0.0022
   280        0.3530             nan     0.0409   -0.0034
   300        0.3510             nan     0.0409   -0.0034
   320        0.3436             nan     0.0409   -0.0022
   340        0.3364             nan     0.0409   -0.0027
   360        0.3331             nan     0.0409   -0.0021
   380        0.3293             nan     0.0409   -0.0026
   400        0.3236             nan     0.0409   -0.0016
   420        0.3227             nan     0.0409   -0.0011
   440        0.3211             nan     0.0409   -0.0023
   460        0.3161             nan     0.0409   -0.0027
   480        0.3142             nan     0.0409   -0.0026
   500        0.3082             nan     0.0409   -0.0020
   520        0.3031             nan     0.0409   -0.0016
   540        0.3007             nan     0.0409   -0.0031
   560        0.2965             nan     0.0409   -0.0027
   580        0.2907             nan     0.0409   -0.0012
   600        0.2886             nan     0.0409   -0.0014
   620        0.2857             nan     0.0409   -0.0023
   640        0.2828             nan     0.0409   -0.0024
   660        0.2787             nan     0.0409   -0.0033
   680        0.2739             nan     0.0409   -0.0027
   700        0.2720             nan     0.0409   -0.0014
   720        0.2701             nan     0.0409   -0.0007
   740        0.2648             nan     0.0409   -0.0018
   760        0.2628             nan     0.0409   -0.0011
   780        0.2590             nan     0.0409   -0.0025
   800        0.2565             nan     0.0409   -0.0026
   820        0.2532             nan     0.0409   -0.0009
   840        0.2487             nan     0.0409   -0.0030
   860        0.2449             nan     0.0409   -0.0038
   880        0.2407             nan     0.0409   -0.0014
   900        0.2387             nan     0.0409   -0.0036
   920        0.2362             nan     0.0409   -0.0024
   940        0.2314             nan     0.0409   -0.0013
   960        0.2297             nan     0.0409   -0.0025
   980        0.2253             nan     0.0409   -0.0006
  1000        0.2219             nan     0.0409   -0.0015
  1020        0.2213             nan     0.0409   -0.0011
  1040        0.2205             nan     0.0409   -0.0021
  1060        0.2192             nan     0.0409   -0.0014
  1080        0.2165             nan     0.0409   -0.0014
  1100        0.2136             nan     0.0409   -0.0006
  1120        0.2103             nan     0.0409   -0.0006
  1140        0.2087             nan     0.0409   -0.0016
  1160        0.2061             nan     0.0409   -0.0020
  1180        0.2032             nan     0.0409   -0.0010
  1200        0.2014             nan     0.0409   -0.0007
  1220        0.1995             nan     0.0409   -0.0008
  1240        0.1969             nan     0.0409   -0.0014
  1260        0.1951             nan     0.0409   -0.0025
  1280        0.1920             nan     0.0409   -0.0018
  1300        0.1904             nan     0.0409   -0.0013
  1320        0.1889             nan     0.0409   -0.0004
  1340        0.1866             nan     0.0409   -0.0022
  1360        0.1845             nan     0.0409   -0.0019
  1380        0.1822             nan     0.0409   -0.0027
  1400        0.1811             nan     0.0409   -0.0024
  1420        0.1792             nan     0.0409   -0.0009
  1440        0.1773             nan     0.0409   -0.0021
  1460        0.1762             nan     0.0409   -0.0020
  1480        0.1744             nan     0.0409   -0.0007
  1500        0.1735             nan     0.0409   -0.0020
  1520        0.1705             nan     0.0409   -0.0013
  1540        0.1681             nan     0.0409   -0.0007
  1560        0.1668             nan     0.0409   -0.0009
  1580        0.1657             nan     0.0409   -0.0009
  1600        0.1647             nan     0.0409   -0.0014
  1620        0.1625             nan     0.0409   -0.0009
  1640        0.1618             nan     0.0409   -0.0016
  1660        0.1603             nan     0.0409   -0.0006
  1680        0.1576             nan     0.0409   -0.0010
  1700        0.1564             nan     0.0409   -0.0004
  1720        0.1550             nan     0.0409   -0.0006
  1740        0.1542             nan     0.0409   -0.0007
  1760        0.1535             nan     0.0409   -0.0007
  1780        0.1520             nan     0.0409   -0.0024
  1800        0.1507             nan     0.0409   -0.0014
  1820        0.1498             nan     0.0409   -0.0012
  1840        0.1476             nan     0.0409   -0.0004
  1860        0.1462             nan     0.0409   -0.0013
  1880        0.1455             nan     0.0409   -0.0024
  1900        0.1448             nan     0.0409   -0.0003
  1920        0.1433             nan     0.0409   -0.0010
  1940        0.1412             nan     0.0409   -0.0019
  1960        0.1398             nan     0.0409   -0.0005
  1980        0.1383             nan     0.0409   -0.0010
  2000        0.1354             nan     0.0409   -0.0007
  2020        0.1340             nan     0.0409   -0.0027
  2040        0.1333             nan     0.0409   -0.0016
  2060        0.1319             nan     0.0409   -0.0013
  2080        0.1306             nan     0.0409   -0.0009
  2100        0.1296             nan     0.0409   -0.0007
  2120        0.1289             nan     0.0409   -0.0009
  2140        0.1281             nan     0.0409   -0.0010
  2160        0.1264             nan     0.0409   -0.0007
  2180        0.1247             nan     0.0409   -0.0016
  2200        0.1236             nan     0.0409   -0.0004
  2220        0.1224             nan     0.0409   -0.0012
  2240        0.1207             nan     0.0409   -0.0002
  2260        0.1194             nan     0.0409   -0.0002
  2280        0.1178             nan     0.0409   -0.0003
  2300        0.1168             nan     0.0409   -0.0007
  2320        0.1155             nan     0.0409   -0.0012
  2340        0.1143             nan     0.0409   -0.0015
  2360        0.1138             nan     0.0409   -0.0009
  2380        0.1133             nan     0.0409   -0.0010
  2400        0.1116             nan     0.0409   -0.0007
  2420        0.1104             nan     0.0409   -0.0003
  2440        0.1092             nan     0.0409   -0.0006
  2460        0.1077             nan     0.0409   -0.0005
  2480        0.1059             nan     0.0409   -0.0007
  2500        0.1050             nan     0.0409   -0.0005
  2520        0.1043             nan     0.0409   -0.0006
  2540        0.1035             nan     0.0409   -0.0008
  2560        0.1021             nan     0.0409   -0.0001
  2580        0.1013             nan     0.0409   -0.0008
  2600        0.1005             nan     0.0409   -0.0004
  2620        0.0994             nan     0.0409   -0.0003
  2640        0.0978             nan     0.0409   -0.0011
  2660        0.0968             nan     0.0409   -0.0007
  2680        0.0952             nan     0.0409   -0.0008
  2700        0.0944             nan     0.0409   -0.0009
  2720        0.0939             nan     0.0409   -0.0004
  2740        0.0931             nan     0.0409   -0.0006
  2760        0.0926             nan     0.0409   -0.0002
  2780        0.0919             nan     0.0409   -0.0005
  2800        0.0911             nan     0.0409   -0.0001
  2820        0.0904             nan     0.0409   -0.0004
  2840        0.0898             nan     0.0409   -0.0007
  2860        0.0892             nan     0.0409   -0.0010
  2880        0.0878             nan     0.0409   -0.0007
  2900        0.0870             nan     0.0409   -0.0004
  2920        0.0867             nan     0.0409   -0.0004
  2940        0.0862             nan     0.0409   -0.0007
  2960        0.0857             nan     0.0409   -0.0007
  2980        0.0851             nan     0.0409   -0.0004
  3000        0.0842             nan     0.0409   -0.0009
  3020        0.0828             nan     0.0409   -0.0007
  3040        0.0822             nan     0.0409   -0.0003
  3060        0.0819             nan     0.0409   -0.0005
  3080        0.0806             nan     0.0409   -0.0007
  3100        0.0802             nan     0.0409   -0.0005
  3120        0.0799             nan     0.0409   -0.0006
  3140        0.0789             nan     0.0409   -0.0006
  3160        0.0781             nan     0.0409   -0.0003
  3180        0.0772             nan     0.0409   -0.0010
  3200        0.0762             nan     0.0409   -0.0005
  3220        0.0757             nan     0.0409   -0.0002
  3240        0.0748             nan     0.0409   -0.0003
  3260        0.0741             nan     0.0409   -0.0002
  3280        0.0737             nan     0.0409   -0.0012
  3300        0.0726             nan     0.0409   -0.0002
  3320        0.0723             nan     0.0409   -0.0006
  3340        0.0712             nan     0.0409   -0.0002
  3360        0.0699             nan     0.0409   -0.0001
  3380        0.0694             nan     0.0409   -0.0004
  3400        0.0689             nan     0.0409   -0.0003
  3420        0.0683             nan     0.0409   -0.0006
  3440        0.0677             nan     0.0409   -0.0006
  3460        0.0671             nan     0.0409   -0.0006
  3480        0.0665             nan     0.0409   -0.0007
  3500        0.0662             nan     0.0409   -0.0004
  3520        0.0652             nan     0.0409   -0.0005
  3540        0.0638             nan     0.0409   -0.0004
  3560        0.0636             nan     0.0409   -0.0006
  3580        0.0631             nan     0.0409   -0.0004
  3600        0.0627             nan     0.0409   -0.0004
  3620        0.0620             nan     0.0409   -0.0005
  3624        0.0620             nan     0.0409   -0.0001

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.7482             nan     0.0628    0.0293
     2        0.7098             nan     0.0628    0.0126
     3        0.6936             nan     0.0628    0.0158
     4        0.6652             nan     0.0628    0.0110
     5        0.6509             nan     0.0628    0.0157
     6        0.6350             nan     0.0628    0.0114
     7        0.6258             nan     0.0628    0.0108
     8        0.6171             nan     0.0628   -0.0003
     9        0.5991             nan     0.0628    0.0120
    10        0.5885             nan     0.0628    0.0113
    20        0.5144             nan     0.0628   -0.0018
    40        0.4738             nan     0.0628   -0.0036
    60        0.4459             nan     0.0628   -0.0055
    80        0.4252             nan     0.0628   -0.0005
   100        0.4205             nan     0.0628   -0.0069
   120        0.4132             nan     0.0628   -0.0042
   140        0.3990             nan     0.0628   -0.0003
   160        0.3887             nan     0.0628   -0.0035
   180        0.3806             nan     0.0628   -0.0055
   200        0.3694             nan     0.0628   -0.0045
   220        0.3654             nan     0.0628   -0.0046
   240        0.3567             nan     0.0628   -0.0033
   260        0.3432             nan     0.0628   -0.0060
   280        0.3336             nan     0.0628   -0.0000
   300        0.3252             nan     0.0628   -0.0015
   320        0.3168             nan     0.0628   -0.0011
   340        0.3108             nan     0.0628   -0.0018
   360        0.3082             nan     0.0628   -0.0057
   380        0.3042             nan     0.0628   -0.0020
   400        0.3006             nan     0.0628   -0.0045
   420        0.2906             nan     0.0628   -0.0019
   440        0.2841             nan     0.0628   -0.0027
   460        0.2791             nan     0.0628   -0.0047
   480        0.2706             nan     0.0628   -0.0031
   500        0.2705             nan     0.0628   -0.0031
   520        0.2637             nan     0.0628   -0.0020
   540        0.2635             nan     0.0628   -0.0029
   560        0.2578             nan     0.0628   -0.0024
   562        0.2573             nan     0.0628   -0.0032

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.7117             nan     0.1134    0.0785
     2        0.6324             nan     0.1134    0.0674
     3        0.5967             nan     0.1134    0.0252
     4        0.5574             nan     0.1134    0.0462
     5        0.5019             nan     0.1134    0.0246
     6        0.4775             nan     0.1134    0.0178
     7        0.4640             nan     0.1134    0.0152
     8        0.4558             nan     0.1134    0.0078
     9        0.4565             nan     0.1134   -0.0191
    10        0.4193             nan     0.1134    0.0022
    20        0.3248             nan     0.1134   -0.0151
    40        0.2354             nan     0.1134   -0.0036
    60        0.1826             nan     0.1134   -0.0099
    80        0.1464             nan     0.1134   -0.0091
   100        0.1133             nan     0.1134   -0.0035
   120        0.0824             nan     0.1134   -0.0033
   140        0.0652             nan     0.1134   -0.0022
   160        0.0555             nan     0.1134   -0.0012
   180        0.0423             nan     0.1134   -0.0007
   200        0.0349             nan     0.1134   -0.0014
   220        0.0280             nan     0.1134   -0.0004
   240        0.0240             nan     0.1134   -0.0016
   260        0.0209             nan     0.1134   -0.0004
   280        0.0187             nan     0.1134   -0.0007
   300        0.0162             nan     0.1134   -0.0007
   320        0.0130             nan     0.1134   -0.0004
   340        0.0105             nan     0.1134   -0.0005
   360        0.0089             nan     0.1134   -0.0002
   380        0.0069             nan     0.1134   -0.0002
   400        0.0057             nan     0.1134   -0.0001
   420        0.0046             nan     0.1134   -0.0001
   440        0.0039             nan     0.1134   -0.0001
   460        0.0034             nan     0.1134    0.0000
   480        0.0028             nan     0.1134   -0.0001
   500        0.0024             nan     0.1134   -0.0000
   520        0.0021             nan     0.1134   -0.0000
   540        0.0018             nan     0.1134   -0.0000
   560        0.0015             nan     0.1134   -0.0000
   580        0.0012             nan     0.1134   -0.0000
   600        0.0010             nan     0.1134   -0.0000
   620        0.0008             nan     0.1134   -0.0000
   640        0.0006             nan     0.1134   -0.0000
   660        0.0005             nan     0.1134   -0.0000
   680        0.0004             nan     0.1134   -0.0000
   700        0.0003             nan     0.1134   -0.0000
   720        0.0003             nan     0.1134   -0.0000
   740        0.0002             nan     0.1134   -0.0000
   760        0.0002             nan     0.1134   -0.0000
   780        0.0001             nan     0.1134   -0.0000
   800        0.0001             nan     0.1134   -0.0000
   820        0.0001             nan     0.1134   -0.0000
   840        0.0001             nan     0.1134   -0.0000
   860        0.0001             nan     0.1134   -0.0000
   880        0.0001             nan     0.1134   -0.0000
   900        0.0000             nan     0.1134   -0.0000
   920        0.0000             nan     0.1134   -0.0000
   940        0.0000             nan     0.1134   -0.0000
   960        0.0000             nan     0.1134   -0.0000
   980        0.0000             nan     0.1134   -0.0000
  1000        0.0000             nan     0.1134   -0.0000
  1020        0.0000             nan     0.1134   -0.0000
  1040        0.0000             nan     0.1134   -0.0000
  1060        0.0000             nan     0.1134   -0.0000
  1080        0.0000             nan     0.1134   -0.0000
  1100        0.0000             nan     0.1134   -0.0000
  1120        0.0000             nan     0.1134   -0.0000
  1140        0.0000             nan     0.1134   -0.0000
  1160        0.0000             nan     0.1134   -0.0000
  1180        0.0000             nan     0.1134   -0.0000
  1200        0.0000             nan     0.1134   -0.0000
  1220        0.0000             nan     0.1134   -0.0000
  1240        0.0000             nan     0.1134   -0.0000
  1260        0.0000             nan     0.1134   -0.0000
  1280        0.0000             nan     0.1134   -0.0000
  1300        0.0000             nan     0.1134   -0.0000
  1320        0.0000             nan     0.1134   -0.0000
  1340        0.0000             nan     0.1134   -0.0000
  1360        0.0000             nan     0.1134   -0.0000
  1380        0.0000             nan     0.1134   -0.0000
  1400        0.0000             nan     0.1134   -0.0000
  1420        0.0000             nan     0.1134   -0.0000
  1440        0.0000             nan     0.1134   -0.0000
  1460        0.0000             nan     0.1134   -0.0000
  1480        0.0000             nan     0.1134   -0.0000
  1500        0.0000             nan     0.1134   -0.0000
  1520        0.0000             nan     0.1134   -0.0000
  1540        0.0000             nan     0.1134   -0.0000
  1560        0.0000             nan     0.1134   -0.0000
  1580        0.0000             nan     0.1134   -0.0000
  1600        0.0000             nan     0.1134   -0.0000
  1620        0.0000             nan     0.1134   -0.0000
  1640        0.0000             nan     0.1134   -0.0000
  1660        0.0000             nan     0.1134   -0.0000
  1680        0.0000             nan     0.1134   -0.0000
  1700        0.0000             nan     0.1134   -0.0000
  1720        0.0000             nan     0.1134   -0.0000
  1740        0.0000             nan     0.1134   -0.0000
  1760        0.0000             nan     0.1134   -0.0000
  1780        0.0000             nan     0.1134   -0.0000
  1800        0.0000             nan     0.1134   -0.0000
  1820        0.0000             nan     0.1134   -0.0000
  1840        0.0000             nan     0.1134   -0.0000
  1860        0.0000             nan     0.1134   -0.0000
  1880        0.0000             nan     0.1134   -0.0000
  1900        0.0000             nan     0.1134   -0.0000
  1920        0.0000             nan     0.1134   -0.0000
  1940        0.0000             nan     0.1134   -0.0000
  1960        0.0000             nan     0.1134   -0.0000
  1980        0.0000             nan     0.1134   -0.0000
  2000        0.0000             nan     0.1134    0.0000
  2020        0.0000             nan     0.1134   -0.0000
  2040        0.0000             nan     0.1134   -0.0000
  2060        0.0000             nan     0.1134   -0.0000
  2080        0.0000             nan     0.1134   -0.0000
  2100        0.0000             nan     0.1134   -0.0000
  2120        0.0000             nan     0.1134   -0.0000
  2140        0.0000             nan     0.1134   -0.0000
  2160        0.0000             nan     0.1134   -0.0000
  2180        0.0000             nan     0.1134   -0.0000
  2200        0.0000             nan     0.1134   -0.0000
  2220        0.0000             nan     0.1134   -0.0000
  2240        0.0000             nan     0.1134    0.0000
  2260        0.0000             nan     0.1134   -0.0000
  2280        0.0000             nan     0.1134   -0.0000
  2300        0.0000             nan     0.1134   -0.0000
  2320        0.0000             nan     0.1134   -0.0000
  2340        0.0000             nan     0.1134   -0.0000
  2360        0.0000             nan     0.1134   -0.0000
  2380        0.0000             nan     0.1134   -0.0000
  2400        0.0000             nan     0.1134   -0.0000
  2420        0.0000             nan     0.1134   -0.0000
  2440        0.0000             nan     0.1134   -0.0000
  2460        0.0000             nan     0.1134   -0.0000
  2480        0.0000             nan     0.1134   -0.0000
  2500        0.0000             nan     0.1134   -0.0000
  2520        0.0000             nan     0.1134   -0.0000
  2540        0.0000             nan     0.1134   -0.0000
  2560        0.0000             nan     0.1134   -0.0000
  2580        0.0000             nan     0.1134   -0.0000
  2600        0.0000             nan     0.1134   -0.0000
  2620        0.0000             nan     0.1134   -0.0000
  2640        0.0000             nan     0.1134   -0.0000
  2660        0.0000             nan     0.1134   -0.0000
  2680        0.0000             nan     0.1134   -0.0000
  2700        0.0000             nan     0.1134   -0.0000
  2720        0.0000             nan     0.1134   -0.0000
  2740        0.0000             nan     0.1134   -0.0000
  2760        0.0000             nan     0.1134   -0.0000
  2780        0.0000             nan     0.1134   -0.0000
  2800        0.0000             nan     0.1134   -0.0000
  2820        0.0000             nan     0.1134   -0.0000
  2840        0.0000             nan     0.1134   -0.0000
  2860        0.0000             nan     0.1134   -0.0000
  2880        0.0000             nan     0.1134   -0.0000
  2900        0.0000             nan     0.1134    0.0000
  2920        0.0000             nan     0.1134   -0.0000
  2940        0.0000             nan     0.1134   -0.0000
  2960        0.0000             nan     0.1134   -0.0000
  2980        0.0000             nan     0.1134   -0.0000
  3000        0.0000             nan     0.1134   -0.0000
  3020        0.0000             nan     0.1134   -0.0000
  3040        0.0000             nan     0.1134   -0.0000
  3060        0.0000             nan     0.1134   -0.0000
  3080        0.0000             nan     0.1134   -0.0000
  3100        0.0000             nan     0.1134   -0.0000
  3120        0.0000             nan     0.1134   -0.0000
  3140        0.0000             nan     0.1134   -0.0000
  3160        0.0000             nan     0.1134   -0.0000
  3180        0.0000             nan     0.1134   -0.0000
  3200        0.0000             nan     0.1134   -0.0000
  3220        0.0000             nan     0.1134   -0.0000
  3240        0.0000             nan     0.1134   -0.0000
  3260        0.0000             nan     0.1134   -0.0000
  3280        0.0000             nan     0.1134   -0.0000
  3300        0.0000             nan     0.1134   -0.0000
  3320        0.0000             nan     0.1134   -0.0000
  3340        0.0000             nan     0.1134   -0.0000
  3360        0.0000             nan     0.1134   -0.0000
  3380        0.0000             nan     0.1134   -0.0000
  3400        0.0000             nan     0.1134   -0.0000
  3420        0.0000             nan     0.1134    0.0000
  3440        0.0000             nan     0.1134   -0.0000
  3460        0.0000             nan     0.1134   -0.0000
  3480        0.0000             nan     0.1134   -0.0000
  3500        0.0000             nan     0.1134   -0.0000
  3520        0.0000             nan     0.1134   -0.0000
  3540        0.0000             nan     0.1134   -0.0000
  3560        0.0000             nan     0.1134   -0.0000
  3572        0.0000             nan     0.1134   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.7172             nan     0.1520    0.0542
     2        0.6527             nan     0.1520    0.0793
     3        0.6045             nan     0.1520    0.0568
     4        0.5348             nan     0.1520    0.0297
     5        0.5185             nan     0.1520    0.0241
     6        0.5000             nan     0.1520    0.0223
     7        0.4803             nan     0.1520    0.0231
     8        0.4755             nan     0.1520   -0.0017
     9        0.4703             nan     0.1520   -0.0119
    10        0.4289             nan     0.1520    0.0005
    20        0.3676             nan     0.1520   -0.0037
    40        0.2930             nan     0.1520   -0.0048
    60        0.2446             nan     0.1520   -0.0066
    80        0.1763             nan     0.1520   -0.0078
   100        0.1422             nan     0.1520   -0.0061
   120        0.1077             nan     0.1520    0.0006
   140        0.0901             nan     0.1520   -0.0026
   160        0.0769             nan     0.1520   -0.0006
   180        0.0673             nan     0.1520   -0.0037
   200        0.0528             nan     0.1520   -0.0017
   220        0.0460             nan     0.1520   -0.0015
   240        0.0393             nan     0.1520   -0.0002
   260        0.0343             nan     0.1520   -0.0010
   280        0.0305             nan     0.1520   -0.0015
   300        0.0248             nan     0.1520   -0.0005
   320        0.0228             nan     0.1520   -0.0011
   340        0.0193             nan     0.1520   -0.0016
   360        0.0155             nan     0.1520   -0.0007
   380        0.0137             nan     0.1520   -0.0004
   400        0.0120             nan     0.1520   -0.0005
   420        0.0109             nan     0.1520   -0.0004
   440        0.0094             nan     0.1520   -0.0003
   460        0.0078             nan     0.1520   -0.0002
   480        0.0063             nan     0.1520   -0.0002
   500        0.0054             nan     0.1520   -0.0001
   520        0.0048             nan     0.1520   -0.0003
   540        0.0042             nan     0.1520   -0.0001
   560        0.0036             nan     0.1520   -0.0002
   580        0.0034             nan     0.1520   -0.0000
   600        0.0029             nan     0.1520   -0.0001
   620        0.0027             nan     0.1520   -0.0001
   640        0.0021             nan     0.1520   -0.0001
   660        0.0019             nan     0.1520   -0.0001
   680        0.0016             nan     0.1520   -0.0000
   700        0.0014             nan     0.1520   -0.0001
   720        0.0011             nan     0.1520   -0.0000
   740        0.0009             nan     0.1520   -0.0000
   760        0.0008             nan     0.1520   -0.0000
   780        0.0007             nan     0.1520   -0.0001
   800        0.0006             nan     0.1520   -0.0000
   820        0.0005             nan     0.1520   -0.0000
   840        0.0004             nan     0.1520   -0.0000
   860        0.0004             nan     0.1520   -0.0000
   880        0.0003             nan     0.1520   -0.0000
   900        0.0003             nan     0.1520   -0.0000
   920        0.0002             nan     0.1520    0.0000
   940        0.0002             nan     0.1520   -0.0000
   960        0.0002             nan     0.1520   -0.0000
   980        0.0001             nan     0.1520   -0.0000
  1000        0.0001             nan     0.1520   -0.0000
  1020        0.0001             nan     0.1520   -0.0000
  1040        0.0001             nan     0.1520   -0.0000
  1060        0.0001             nan     0.1520   -0.0000
  1080        0.0001             nan     0.1520   -0.0000
  1100        0.0001             nan     0.1520   -0.0000
  1120        0.0001             nan     0.1520   -0.0000
  1140        0.0001             nan     0.1520   -0.0000
  1160        0.0000             nan     0.1520   -0.0000
  1180        0.0000             nan     0.1520   -0.0000
  1200        0.0000             nan     0.1520   -0.0000
  1220        0.0000             nan     0.1520   -0.0000
  1240        0.0000             nan     0.1520   -0.0000
  1254        0.0000             nan     0.1520   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5679             nan     0.3279    0.0888
     2        0.4502             nan     0.3279    0.0232
     3        0.4598             nan     0.3279   -0.0297
     4        0.4229             nan     0.3279    0.0411
     5        0.4045             nan     0.3279   -0.0199
     6        0.4100             nan     0.3279   -0.0248
     7        0.4091             nan     0.3279   -0.0069
     8        0.3874             nan     0.3279    0.0181
     9        0.3920             nan     0.3279   -0.0272
    10        0.3582             nan     0.3279   -0.0253
    20        0.2921             nan     0.3279   -0.0033
    40        0.2413             nan     0.3279   -0.0227
    60        0.1882             nan     0.3279   -0.0075
    80        0.1491             nan     0.3279   -0.0140
   100        0.1342             nan     0.3279   -0.0069
   120        0.1049             nan     0.3279   -0.0022
   140        0.0956             nan     0.3279    0.0028
   160        0.0703             nan     0.3279   -0.0080
   180        0.0553             nan     0.3279    0.0003
   200        0.0559             nan     0.3279   -0.0040
   220        0.0487             nan     0.3279   -0.0030
   240        0.0375             nan     0.3279   -0.0012
   260        0.0314             nan     0.3279   -0.0056
   280        0.0252             nan     0.3279   -0.0023
   300        0.0204             nan     0.3279   -0.0012
   320        0.0163             nan     0.3279   -0.0004
   340        0.0131             nan     0.3279    0.0000
   360        0.0109             nan     0.3279   -0.0004
   380        0.0096             nan     0.3279   -0.0001
   400        0.0081             nan     0.3279   -0.0008
   420        0.0063             nan     0.3279   -0.0008
   440        0.0053             nan     0.3279   -0.0003
   460        0.0044             nan     0.3279   -0.0002
   480        0.0038             nan     0.3279   -0.0004
   500        0.0033             nan     0.3279   -0.0004
   520        0.0027             nan     0.3279   -0.0003
   540        0.0024             nan     0.3279   -0.0001
   560        0.0021             nan     0.3279   -0.0003
   580        0.0018             nan     0.3279   -0.0002
   600        0.0014             nan     0.3279   -0.0001
   620        0.0012             nan     0.3279   -0.0000
   640        0.0010             nan     0.3279   -0.0001
   660        0.0008             nan     0.3279   -0.0001
   680        0.0007             nan     0.3279   -0.0000
   700        0.0006             nan     0.3279   -0.0000
   720        0.0005             nan     0.3279   -0.0000
   740        0.0004             nan     0.3279   -0.0001
   760        0.0004             nan     0.3279   -0.0001
   780        0.0003             nan     0.3279   -0.0000
   800        0.0002             nan     0.3279    0.0000
   820        0.0002             nan     0.3279   -0.0000
   840        0.0001             nan     0.3279   -0.0000
   860        0.0001             nan     0.3279   -0.0000
   880        0.0001             nan     0.3279   -0.0000
   900        0.0001             nan     0.3279   -0.0000
   920        0.0001             nan     0.3279   -0.0000
   940        0.0001             nan     0.3279   -0.0000
   960        0.0000             nan     0.3279   -0.0000
   980        0.0000             nan     0.3279   -0.0000
  1000        0.0000             nan     0.3279   -0.0000
  1020        0.0000             nan     0.3279   -0.0000
  1040        0.0000             nan     0.3279   -0.0000
  1060        0.0000             nan     0.3279   -0.0000
  1080        0.0000             nan     0.3279   -0.0000
  1100        0.0000             nan     0.3279   -0.0000
  1120        0.0000             nan     0.3279   -0.0000
  1140        0.0000             nan     0.3279   -0.0000
  1160        0.0000             nan     0.3279   -0.0000
  1180        0.0000             nan     0.3279   -0.0000
  1200        0.0000             nan     0.3279   -0.0000
  1220        0.0000             nan     0.3279   -0.0000
  1240        0.0000             nan     0.3279   -0.0000
  1260        0.0000             nan     0.3279   -0.0000
  1280        0.0000             nan     0.3279   -0.0000
  1300        0.0000             nan     0.3279   -0.0000
  1320        0.0000             nan     0.3279   -0.0000
  1340        0.0000             nan     0.3279   -0.0000
  1360        0.0000             nan     0.3279   -0.0000
  1380        0.0000             nan     0.3279   -0.0000
  1400        0.0000             nan     0.3279   -0.0000
  1420        0.0000             nan     0.3279   -0.0000
  1440        0.0000             nan     0.3279   -0.0000
  1460        0.0000             nan     0.3279   -0.0000
  1480        0.0000             nan     0.3279   -0.0000
  1500        0.0000             nan     0.3279   -0.0000
  1520        0.0000             nan     0.3279   -0.0000
  1540        0.0000             nan     0.3279   -0.0000
  1560        0.0000             nan     0.3279   -0.0000
  1580        0.0000             nan     0.3279   -0.0000
  1600        0.0000             nan     0.3279   -0.0000
  1620        0.0000             nan     0.3279   -0.0000
  1640        0.0000             nan     0.3279   -0.0000
  1660        0.0000             nan     0.3279   -0.0000
  1680        0.0000             nan     0.3279   -0.0000
  1700        0.0000             nan     0.3279   -0.0000
  1720        0.0000             nan     0.3279   -0.0000
  1740        0.0000             nan     0.3279   -0.0000
  1760        0.0000             nan     0.3279   -0.0000
  1780        0.0000             nan     0.3279   -0.0000
  1800        0.0000             nan     0.3279   -0.0000
  1820        0.0000             nan     0.3279   -0.0000
  1840        0.0000             nan     0.3279   -0.0000
  1860        0.0000             nan     0.3279   -0.0000
  1880        0.0000             nan     0.3279   -0.0000
  1900        0.0000             nan     0.3279   -0.0000
  1920        0.0000             nan     0.3279   -0.0000
  1940        0.0000             nan     0.3279   -0.0000
  1960        0.0000             nan     0.3279   -0.0000
  1980        0.0000             nan     0.3279   -0.0000
  2000        0.0000             nan     0.3279   -0.0000
  2020        0.0000             nan     0.3279   -0.0000
  2040        0.0000             nan     0.3279   -0.0000
  2060        0.0000             nan     0.3279   -0.0000
  2080        0.0000             nan     0.3279   -0.0000
  2100        0.0000             nan     0.3279    0.0000
  2120        0.0000             nan     0.3279   -0.0000
  2140        0.0000             nan     0.3279   -0.0000
  2160        0.0000             nan     0.3279   -0.0000
  2180        0.0000             nan     0.3279   -0.0000
  2200        0.0000             nan     0.3279   -0.0000
  2220        0.0000             nan     0.3279   -0.0000
  2240        0.0000             nan     0.3279   -0.0000
  2260        0.0000             nan     0.3279   -0.0000
  2280        0.0000             nan     0.3279   -0.0000
  2300        0.0000             nan     0.3279   -0.0000
  2320        0.0000             nan     0.3279   -0.0000
  2340        0.0000             nan     0.3279   -0.0000
  2360        0.0000             nan     0.3279   -0.0000
  2380        0.0000             nan     0.3279   -0.0000
  2400        0.0000             nan     0.3279   -0.0000
  2420        0.0000             nan     0.3279   -0.0000
  2440        0.0000             nan     0.3279   -0.0000
  2460        0.0000             nan     0.3279   -0.0000
  2480        0.0000             nan     0.3279   -0.0000
  2500        0.0000             nan     0.3279   -0.0000
  2520        0.0000             nan     0.3279   -0.0000
  2540        0.0000             nan     0.3279   -0.0000
  2560        0.0000             nan     0.3279   -0.0000
  2580        0.0000             nan     0.3279   -0.0000
  2600        0.0000             nan     0.3279   -0.0000
  2620        0.0000             nan     0.3279   -0.0000
  2640        0.0000             nan     0.3279   -0.0000
  2660        0.0000             nan     0.3279   -0.0000
  2680        0.0000             nan     0.3279   -0.0000
  2700        0.0000             nan     0.3279   -0.0000
  2720        0.0000             nan     0.3279   -0.0000
  2740        0.0000             nan     0.3279    0.0000
  2760        0.0000             nan     0.3279   -0.0000
  2780        0.0000             nan     0.3279   -0.0000
  2800        0.0000             nan     0.3279   -0.0000
  2820        0.0000             nan     0.3279   -0.0000
  2840        0.0000             nan     0.3279   -0.0000
  2860        0.0000             nan     0.3279   -0.0000
  2880        0.0000             nan     0.3279   -0.0000
  2900        0.0000             nan     0.3279   -0.0000
  2920        0.0000             nan     0.3279   -0.0000
  2940        0.0000             nan     0.3279   -0.0000
  2960        0.0000             nan     0.3279   -0.0000
  2980        0.0000             nan     0.3279   -0.0000
  3000        0.0000             nan     0.3279   -0.0000
  3020        0.0000             nan     0.3279   -0.0000
  3040        0.0000             nan     0.3279   -0.0000
  3060        0.0000             nan     0.3279   -0.0000
  3080        0.0000             nan     0.3279   -0.0000
  3100        0.0000             nan     0.3279   -0.0000
  3120        0.0000             nan     0.3279   -0.0000
  3140        0.0000             nan     0.3279   -0.0000
  3160        0.0000             nan     0.3279   -0.0000
  3180        0.0000             nan     0.3279   -0.0000
  3200        0.0000             nan     0.3279   -0.0000
  3220        0.0000             nan     0.3279   -0.0000
  3240        0.0000             nan     0.3279   -0.0000
  3260        0.0000             nan     0.3279   -0.0000
  3280        0.0000             nan     0.3279   -0.0000
  3300        0.0000             nan     0.3279   -0.0000
  3320        0.0000             nan     0.3279   -0.0000
  3340        0.0000             nan     0.3279   -0.0000
  3360        0.0000             nan     0.3279   -0.0000
  3380        0.0000             nan     0.3279   -0.0000
  3400        0.0000             nan     0.3279   -0.0000
  3420        0.0000             nan     0.3279   -0.0000
  3440        0.0000             nan     0.3279   -0.0000
  3460        0.0000             nan     0.3279   -0.0000
  3480        0.0000             nan     0.3279   -0.0000
  3500        0.0000             nan     0.3279   -0.0000
  3520        0.0000             nan     0.3279   -0.0000
  3540        0.0000             nan     0.3279   -0.0000
  3560        0.0000             nan     0.3279   -0.0000
  3580        0.0000             nan     0.3279   -0.0000
  3600        0.0000             nan     0.3279   -0.0000
  3620        0.0000             nan     0.3279   -0.0000
  3640        0.0000             nan     0.3279   -0.0000
  3660        0.0000             nan     0.3279   -0.0000
  3680        0.0000             nan     0.3279   -0.0000
  3700        0.0000             nan     0.3279   -0.0000
  3720        0.0000             nan     0.3279   -0.0000
  3740        0.0000             nan     0.3279   -0.0000
  3760        0.0000             nan     0.3279   -0.0000
  3780        0.0000             nan     0.3279   -0.0000
  3800        0.0000             nan     0.3279   -0.0000
  3820        0.0000             nan     0.3279   -0.0000
  3840        0.0000             nan     0.3279   -0.0000
  3860        0.0000             nan     0.3279   -0.0000
  3880        0.0000             nan     0.3279   -0.0000
  3900        0.0000             nan     0.3279   -0.0000
  3920        0.0000             nan     0.3279   -0.0000
  3940        0.0000             nan     0.3279   -0.0000
  3960        0.0000             nan     0.3279   -0.0000
  3980        0.0000             nan     0.3279   -0.0000
  4000        0.0000             nan     0.3279   -0.0000
  4020        0.0000             nan     0.3279   -0.0000
  4040        0.0000             nan     0.3279   -0.0000
  4060        0.0000             nan     0.3279   -0.0000
  4080        0.0000             nan     0.3279   -0.0000
  4100        0.0000             nan     0.3279   -0.0000
  4120        0.0000             nan     0.3279   -0.0000
  4140        0.0000             nan     0.3279   -0.0000
  4160        0.0000             nan     0.3279   -0.0000
  4180        0.0000             nan     0.3279   -0.0000
  4200        0.0000             nan     0.3279   -0.0000
  4220        0.0000             nan     0.3279   -0.0000
  4240        0.0000             nan     0.3279   -0.0000
  4260        0.0000             nan     0.3279   -0.0000
  4280        0.0000             nan     0.3279   -0.0000
  4300        0.0000             nan     0.3279   -0.0000
  4320        0.0000             nan     0.3279   -0.0000
  4340        0.0000             nan     0.3279   -0.0000
  4360        0.0000             nan     0.3279   -0.0000
  4380        0.0000             nan     0.3279   -0.0000
  4400        0.0000             nan     0.3279   -0.0000
  4420        0.0000             nan     0.3279   -0.0000
  4440        0.0000             nan     0.3279   -0.0000
  4460        0.0000             nan     0.3279   -0.0000
  4480        0.0000             nan     0.3279   -0.0000
  4500        0.0000             nan     0.3279   -0.0000
  4520        0.0000             nan     0.3279   -0.0000
  4540        0.0000             nan     0.3279   -0.0000
  4560        0.0000             nan     0.3279   -0.0000
  4580        0.0000             nan     0.3279   -0.0000
  4600        0.0000             nan     0.3279   -0.0000
  4620        0.0000             nan     0.3279   -0.0000
  4640        0.0000             nan     0.3279   -0.0000
  4660        0.0000             nan     0.3279   -0.0000
  4665        0.0000             nan     0.3279   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.6216             nan     0.4488    0.1867
     2        0.5601             nan     0.4488    0.0445
     3        0.5240             nan     0.4488    0.0413
     4        0.5013             nan     0.4488    0.0256
     5        0.4844             nan     0.4488   -0.0566
     6        0.4851             nan     0.4488   -0.0487
     7        0.4660             nan     0.4488   -0.0512
     8        0.4331             nan     0.4488   -0.0047
     9        0.4366             nan     0.4488   -0.0319
    10        0.4139             nan     0.4488   -0.0214
    20        0.3630             nan     0.4488   -0.0504
    40        0.3058             nan     0.4488   -0.0308
    60        0.2402             nan     0.4488   -0.0138
    80        0.2054             nan     0.4488   -0.0287
   100        0.1709             nan     0.4488   -0.0155
   120        0.1557             nan     0.4488   -0.0050
   140        0.1246             nan     0.4488   -0.0079
   160        0.0924             nan     0.4488   -0.0114
   180        0.0787             nan     0.4488   -0.0078
   200        0.0768             nan     0.4488   -0.0096
   220        0.0713             nan     0.4488   -0.0106
   240        0.0599             nan     0.4488   -0.0044
   260        0.0509             nan     0.4488   -0.0066
   280        0.0428             nan     0.4488   -0.0031
   300        0.0355             nan     0.4488   -0.0048
   320        0.0285             nan     0.4488   -0.0013
   340        0.0209             nan     0.4488    0.0001
   360        0.0187             nan     0.4488   -0.0017
   380        0.0162             nan     0.4488   -0.0001
   400        0.0132             nan     0.4488   -0.0014
   420        0.0126             nan     0.4488   -0.0017
   440        0.0088             nan     0.4488   -0.0002
   460        0.0069             nan     0.4488   -0.0006
   480        0.0060             nan     0.4488   -0.0004
   500        0.0054             nan     0.4488   -0.0004
   520        0.0048             nan     0.4488   -0.0009
   540        0.0042             nan     0.4488   -0.0003
   560        0.0040             nan     0.4488   -0.0005
   580        0.0034             nan     0.4488   -0.0008
   600        0.0029             nan     0.4488   -0.0004
   620        0.0024             nan     0.4488   -0.0002
   640        0.0019             nan     0.4488   -0.0002
   660        0.0016             nan     0.4488   -0.0001
   680        0.0014             nan     0.4488   -0.0002
   700        0.0013             nan     0.4488   -0.0001
   720        0.0012             nan     0.4488   -0.0001
   740        0.0011             nan     0.4488   -0.0000
   760        0.0010             nan     0.4488   -0.0001
   780        0.0008             nan     0.4488   -0.0000
   800        0.0007             nan     0.4488   -0.0000
   820        0.0006             nan     0.4488   -0.0002
   840        0.0006             nan     0.4488   -0.0001
   860        0.0005             nan     0.4488    0.0000
   880        0.0004             nan     0.4488    0.0000
   900        0.0003             nan     0.4488   -0.0000
   920        0.0003             nan     0.4488   -0.0000
   940        0.0003             nan     0.4488   -0.0000
   960        0.0002             nan     0.4488   -0.0000
   980        0.0002             nan     0.4488   -0.0000
  1000        0.0002             nan     0.4488   -0.0000
  1020        0.0001             nan     0.4488   -0.0000
  1040        0.0001             nan     0.4488   -0.0000
  1060        0.0001             nan     0.4488   -0.0000
  1080        0.0001             nan     0.4488   -0.0000
  1100        0.0001             nan     0.4488   -0.0000
  1120        0.0001             nan     0.4488   -0.0000
  1140        0.0001             nan     0.4488   -0.0000
  1160        0.0001             nan     0.4488   -0.0000
  1180        0.0001             nan     0.4488   -0.0000
  1200        0.0000             nan     0.4488   -0.0000
  1220        0.0000             nan     0.4488   -0.0000
  1240        0.0000             nan     0.4488   -0.0000
  1260        0.0000             nan     0.4488   -0.0000
  1280        0.0000             nan     0.4488   -0.0000
  1300        0.0000             nan     0.4488   -0.0000
  1320        0.0000             nan     0.4488   -0.0000
  1340        0.0000             nan     0.4488   -0.0000
  1360        0.0000             nan     0.4488   -0.0000
  1380        0.0000             nan     0.4488    0.0000
  1400        0.0000             nan     0.4488   -0.0000
  1420        0.0000             nan     0.4488   -0.0000
  1431        0.0000             nan     0.4488   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.6397             nan     0.5231    0.1719
     2        0.4885             nan     0.5231    0.0746
     3        0.4645             nan     0.5231   -0.0361
     4        0.4589             nan     0.5231   -0.0432
     5        0.4472             nan     0.5231    0.0024
     6        0.4469             nan     0.5231   -0.1026
     7        0.4482             nan     0.5231   -0.0562
     8        0.4431             nan     0.5231   -0.0317
     9        0.4237             nan     0.5231   -0.0234
    10        0.4303             nan     0.5231   -0.0605
    20        0.3892             nan     0.5231   -0.0241
    40        0.3441             nan     0.5231   -0.0542
    60        0.3241             nan     0.5231   -0.0104
    80        0.2992             nan     0.5231   -0.0130
   100        0.2304             nan     0.5231   -0.0172
   120        0.2045             nan     0.5231    0.0121
   140        0.1520             nan     0.5231   -0.0255
   160        0.1131             nan     0.5231   -0.0180
   180        0.1057             nan     0.5231   -0.0108
   200        0.0952             nan     0.5231   -0.0249
   220        0.0914             nan     0.5231   -0.0254
   240        0.0777             nan     0.5231   -0.0196
   260        0.0658             nan     0.5231   -0.0039
   280        0.0534             nan     0.5231   -0.0080
   300        0.0484             nan     0.5231   -0.0025
   320        0.0403             nan     0.5231    0.0031
   340        0.0330             nan     0.5231   -0.0032
   360        0.0314             nan     0.5231   -0.0056
   380        0.0305             nan     0.5231   -0.0050
   400        0.0270             nan     0.5231   -0.0049
   420        0.0247             nan     0.5231   -0.0046
   440        0.0174             nan     0.5231    0.0003
   460        0.0152             nan     0.5231   -0.0021
   480        0.0144             nan     0.5231   -0.0029
   500        0.0118             nan     0.5231   -0.0009
   520        0.0099             nan     0.5231   -0.0010
   540        0.0089             nan     0.5231   -0.0000
   560        0.0083             nan     0.5231   -0.0004
   580        0.0075             nan     0.5231   -0.0007
   600        0.0073             nan     0.5231   -0.0006
   620        0.0062             nan     0.5231   -0.0008
   640        0.0049             nan     0.5231   -0.0008
   660        0.0041             nan     0.5231   -0.0007
   680        0.0033             nan     0.5231   -0.0001
   700        0.0031             nan     0.5231   -0.0009
   720        0.0028             nan     0.5231   -0.0001
   740        0.0023             nan     0.5231   -0.0001
   760        0.0019             nan     0.5231   -0.0001
   780        0.0018             nan     0.5231   -0.0001
   800        0.0017             nan     0.5231   -0.0002
   820        0.0014             nan     0.5231   -0.0002
   840        0.0012             nan     0.5231   -0.0001
   860        0.0011             nan     0.5231   -0.0001
   880        0.0011             nan     0.5231   -0.0000
   900        0.0010             nan     0.5231   -0.0001
   920        0.0009             nan     0.5231   -0.0002
   940        0.0008             nan     0.5231   -0.0001
   960        0.0007             nan     0.5231   -0.0000
   980        0.0006             nan     0.5231   -0.0001
  1000        0.0006             nan     0.5231   -0.0001
  1020        0.0005             nan     0.5231   -0.0000
  1040        0.0005             nan     0.5231   -0.0000
  1060        0.0004             nan     0.5231   -0.0000
  1080        0.0004             nan     0.5231   -0.0000
  1100        0.0004             nan     0.5231   -0.0001
  1120        0.0003             nan     0.5231   -0.0000
  1140        0.0003             nan     0.5231   -0.0000
  1160        0.0003             nan     0.5231   -0.0000
  1180        0.0003             nan     0.5231   -0.0000
  1200        0.0002             nan     0.5231   -0.0000
  1220        0.0002             nan     0.5231   -0.0000
  1240        0.0002             nan     0.5231   -0.0000
  1260        0.0002             nan     0.5231   -0.0000
  1280        0.0002             nan     0.5231   -0.0000
  1300        0.0002             nan     0.5231   -0.0000
  1320        0.0001             nan     0.5231   -0.0000
  1340        0.0001             nan     0.5231   -0.0000
  1360        0.0001             nan     0.5231   -0.0000
  1380        0.0001             nan     0.5231   -0.0000
  1400        0.0001             nan     0.5231   -0.0000
  1420        0.0001             nan     0.5231   -0.0000
  1440        0.0001             nan     0.5231   -0.0000
  1460        0.0001             nan     0.5231   -0.0000
  1480        0.0001             nan     0.5231   -0.0000
  1500        0.0001             nan     0.5231   -0.0000
  1520        0.0001             nan     0.5231   -0.0000
  1540        0.0001             nan     0.5231   -0.0000
  1560        0.0001             nan     0.5231   -0.0000
  1580        0.0001             nan     0.5231   -0.0000
  1600        0.0001             nan     0.5231   -0.0000
  1620        0.0001             nan     0.5231    0.0000
  1640        0.0000             nan     0.5231   -0.0000
  1660        0.0000             nan     0.5231   -0.0000
  1680        0.0000             nan     0.5231   -0.0000
  1700        0.0000             nan     0.5231   -0.0000
  1720        0.0000             nan     0.5231   -0.0000
  1740        0.0000             nan     0.5231   -0.0000
  1760        0.0000             nan     0.5231   -0.0000
  1780        0.0000             nan     0.5231   -0.0000
  1800        0.0000             nan     0.5231   -0.0000
  1820        0.0000             nan     0.5231   -0.0000
  1840        0.0000             nan     0.5231   -0.0000
  1860        0.0000             nan     0.5231   -0.0000
  1880        0.0000             nan     0.5231   -0.0000
  1900        0.0000             nan     0.5231   -0.0000
  1920        0.0000             nan     0.5231   -0.0000
  1940        0.0000             nan     0.5231    0.0000
  1960        0.0000             nan     0.5231   -0.0000
  1980        0.0000             nan     0.5231    0.0000
  2000        0.0000             nan     0.5231   -0.0000
  2020        0.0000             nan     0.5231   -0.0000
  2040        0.0000             nan     0.5231   -0.0000
  2060        0.0000             nan     0.5231   -0.0000
  2080        0.0000             nan     0.5231   -0.0000
  2100        0.0000             nan     0.5231   -0.0000
  2120        0.0000             nan     0.5231   -0.0000
  2140        0.0000             nan     0.5231   -0.0000
  2160        0.0000             nan     0.5231   -0.0000
  2180        0.0000             nan     0.5231   -0.0000
  2200        0.0000             nan     0.5231   -0.0000
  2220        0.0000             nan     0.5231   -0.0000
  2240        0.0000             nan     0.5231   -0.0000
  2260        0.0000             nan     0.5231   -0.0000
  2280        0.0000             nan     0.5231   -0.0000
  2300        0.0000             nan     0.5231   -0.0000
  2320        0.0000             nan     0.5231   -0.0000
  2340        0.0000             nan     0.5231   -0.0000
  2360        0.0000             nan     0.5231   -0.0000
  2380        0.0000             nan     0.5231   -0.0000
  2400        0.0000             nan     0.5231   -0.0000
  2420        0.0000             nan     0.5231   -0.0000
  2440        0.0000             nan     0.5231   -0.0000
  2460        0.0000             nan     0.5231   -0.0000
  2480        0.0000             nan     0.5231   -0.0000
  2500        0.0000             nan     0.5231   -0.0000
  2520        0.0000             nan     0.5231   -0.0000
  2540        0.0000             nan     0.5231   -0.0000
  2560        0.0000             nan     0.5231   -0.0000
  2580        0.0000             nan     0.5231   -0.0000
  2600        0.0000             nan     0.5231   -0.0000
  2620        0.0000             nan     0.5231   -0.0000
  2640        0.0000             nan     0.5231   -0.0000
  2660        0.0000             nan     0.5231   -0.0000
  2680        0.0000             nan     0.5231   -0.0000
  2700        0.0000             nan     0.5231   -0.0000
  2720        0.0000             nan     0.5231   -0.0000
  2740        0.0000             nan     0.5231   -0.0000
  2760        0.0000             nan     0.5231   -0.0000
  2780        0.0000             nan     0.5231   -0.0000
  2800        0.0000             nan     0.5231   -0.0000
  2820        0.0000             nan     0.5231   -0.0000
  2840        0.0000             nan     0.5231   -0.0000
  2860        0.0000             nan     0.5231   -0.0000
  2880        0.0000             nan     0.5231   -0.0000
  2900        0.0000             nan     0.5231   -0.0000
  2920        0.0000             nan     0.5231    0.0000
  2940        0.0000             nan     0.5231   -0.0000
  2960        0.0000             nan     0.5231   -0.0000
  2980        0.0000             nan     0.5231   -0.0000
  3000        0.0000             nan     0.5231   -0.0000
  3020        0.0000             nan     0.5231   -0.0000
  3040        0.0000             nan     0.5231   -0.0000
  3060        0.0000             nan     0.5231   -0.0000
  3080        0.0000             nan     0.5231   -0.0000
  3100        0.0000             nan     0.5231   -0.0000
  3120        0.0000             nan     0.5231   -0.0000
  3140        0.0000             nan     0.5231   -0.0000
  3160        0.0000             nan     0.5231   -0.0000
  3180        0.0000             nan     0.5231   -0.0000
  3200        0.0000             nan     0.5231    0.0000
  3220        0.0000             nan     0.5231   -0.0000
  3240        0.0000             nan     0.5231   -0.0000
  3260        0.0000             nan     0.5231   -0.0000
  3280        0.0000             nan     0.5231   -0.0000
  3300        0.0000             nan     0.5231   -0.0000
  3320        0.0000             nan     0.5231   -0.0000
  3340        0.0000             nan     0.5231   -0.0000
  3360        0.0000             nan     0.5231   -0.0000
  3380        0.0000             nan     0.5231   -0.0000
  3400        0.0000             nan     0.5231   -0.0000
  3420        0.0000             nan     0.5231   -0.0000
  3440        0.0000             nan     0.5231   -0.0000
  3460        0.0000             nan     0.5231   -0.0000
  3480        0.0000             nan     0.5231   -0.0000
  3500        0.0000             nan     0.5231   -0.0000
  3520        0.0000             nan     0.5231   -0.0000
  3540        0.0000             nan     0.5231   -0.0000
  3560        0.0000             nan     0.5231   -0.0000
  3580        0.0000             nan     0.5231   -0.0000
  3600        0.0000             nan     0.5231   -0.0000
  3620        0.0000             nan     0.5231   -0.0000
  3640        0.0000             nan     0.5231   -0.0000
  3660        0.0000             nan     0.5231   -0.0000
  3680        0.0000             nan     0.5231   -0.0000
  3700        0.0000             nan     0.5231   -0.0000
  3720        0.0000             nan     0.5231   -0.0000
  3740        0.0000             nan     0.5231   -0.0000
  3760        0.0000             nan     0.5231   -0.0000
  3780        0.0000             nan     0.5231   -0.0000
  3800        0.0000             nan     0.5231   -0.0000
  3820        0.0000             nan     0.5231   -0.0000
  3840        0.0000             nan     0.5231   -0.0000
  3860        0.0000             nan     0.5231   -0.0000
  3880        0.0000             nan     0.5231   -0.0000
  3900        0.0000             nan     0.5231   -0.0000
  3920        0.0000             nan     0.5231   -0.0000
  3940        0.0000             nan     0.5231   -0.0000
  3960        0.0000             nan     0.5231   -0.0000
  3980        0.0000             nan     0.5231   -0.0000
  4000        0.0000             nan     0.5231   -0.0000
  4020        0.0000             nan     0.5231   -0.0000
  4040        0.0000             nan     0.5231   -0.0000
  4060        0.0000             nan     0.5231   -0.0000
  4080        0.0000             nan     0.5231   -0.0000
  4100        0.0000             nan     0.5231   -0.0000
  4120        0.0000             nan     0.5231   -0.0000
  4140        0.0000             nan     0.5231   -0.0000
  4160        0.0000             nan     0.5231   -0.0000
  4180        0.0000             nan     0.5231   -0.0000
  4200        0.0000             nan     0.5231    0.0000
  4220        0.0000             nan     0.5231   -0.0000
  4240        0.0000             nan     0.5231   -0.0000
  4260        0.0000             nan     0.5231   -0.0000
  4280        0.0000             nan     0.5231   -0.0000
  4300        0.0000             nan     0.5231   -0.0000
  4320        0.0000             nan     0.5231   -0.0000
  4340        0.0000             nan     0.5231   -0.0000
  4360        0.0000             nan     0.5231   -0.0000
  4380        0.0000             nan     0.5231    0.0000
  4383        0.0000             nan     0.5231   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.6545             nan     0.5261    0.1458
     2        0.5719             nan     0.5261    0.1020
     3        0.5314             nan     0.5261   -0.0632
     4        0.5382             nan     0.5261   -0.0496
     5        0.5033             nan     0.5261    0.0161
     6        0.5073             nan     0.5261   -0.0739
     7        0.5062             nan     0.5261   -0.0474
     8        0.4818             nan     0.5261   -0.0051
     9        0.4701             nan     0.5261    0.0004
    10        0.4791             nan     0.5261   -0.0460
    20        0.4416             nan     0.5261   -0.0236
    40        0.3973             nan     0.5261   -0.0116
    60        0.3685             nan     0.5261   -0.0655
    80        0.2550             nan     0.5261   -0.0121
   100        0.2571             nan     0.5261   -0.0489
   120        0.2182             nan     0.5261   -0.0192
   140        0.1875             nan     0.5261   -0.0204
   160        0.1778             nan     0.5261   -0.0205
   180        0.1450             nan     0.5261   -0.0169
   200        0.1354             nan     0.5261   -0.0087
   220        0.1281             nan     0.5261   -0.0129
   240        0.1289             nan     0.5261   -0.0240
   260        0.1045             nan     0.5261   -0.0070
   280        0.0976             nan     0.5261   -0.0115
   300        0.0968             nan     0.5261   -0.0201
   320        0.0891             nan     0.5261   -0.0055
   340        0.0813             nan     0.5261   -0.0086
   360        0.0679             nan     0.5261   -0.0084
   380        0.0670             nan     0.5261   -0.0105
   400        0.0606             nan     0.5261   -0.0034
   420        0.0591             nan     0.5261   -0.0024
   440        0.0525             nan     0.5261   -0.0042
   460        0.0459             nan     0.5261   -0.0044
   480        0.0431             nan     0.5261   -0.0016
   500        0.0443             nan     0.5261   -0.0064
   520        0.0420             nan     0.5261   -0.0164
   540        0.0358             nan     0.5261   -0.0040
   560        0.0351             nan     0.5261   -0.0099
   580        0.0280             nan     0.5261   -0.0003
   600        0.0287             nan     0.5261   -0.0057
   620        0.0272             nan     0.5261   -0.0050
   640        0.0232             nan     0.5261   -0.0004
   660        0.0226             nan     0.5261   -0.0045
   680        0.0209             nan     0.5261    0.0006
   700        0.0202             nan     0.5261   -0.0022
   720        0.0188             nan     0.5261   -0.0007
   740        0.0167             nan     0.5261   -0.0044
   760        0.0147             nan     0.5261   -0.0020
   780        0.0139             nan     0.5261   -0.0008
   800        0.0116             nan     0.5261   -0.0005
   820        0.0105             nan     0.5261   -0.0007
   840        0.0100             nan     0.5261   -0.0005
   860        0.0088             nan     0.5261   -0.0006
   880        0.0083             nan     0.5261   -0.0006
   900        0.0081             nan     0.5261   -0.0007
   920        0.0071             nan     0.5261   -0.0010
   940        0.0061             nan     0.5261   -0.0007
   960        0.0061             nan     0.5261   -0.0011
   980        0.0049             nan     0.5261   -0.0006
  1000        0.0049             nan     0.5261   -0.0001
  1020        0.0044             nan     0.5261   -0.0005
  1040        0.0041             nan     0.5261   -0.0002
  1060        0.0037             nan     0.5261   -0.0004
  1080        0.0033             nan     0.5261   -0.0003
  1100        0.0033             nan     0.5261   -0.0002
  1120        0.0030             nan     0.5261   -0.0001
  1140        0.0030             nan     0.5261   -0.0002
  1160        0.0025             nan     0.5261   -0.0003
  1180        0.0025             nan     0.5261   -0.0001
  1200        0.0022             nan     0.5261   -0.0002
  1220        0.0022             nan     0.5261   -0.0001
  1240        0.0022             nan     0.5261    0.0001
  1260        0.0020             nan     0.5261   -0.0003
  1280        0.0019             nan     0.5261   -0.0001
  1300        0.0017             nan     0.5261   -0.0001
  1320        0.0016             nan     0.5261   -0.0000
  1340        0.0014             nan     0.5261   -0.0001
  1360        0.0015             nan     0.5261   -0.0003
  1380        0.0016             nan     0.5261   -0.0004
  1400        0.0014             nan     0.5261   -0.0003
  1420        0.0013             nan     0.5261   -0.0001
  1440        0.0013             nan     0.5261   -0.0003
  1460        0.0012             nan     0.5261   -0.0000
  1480        0.0011             nan     0.5261   -0.0001
  1500        0.0010             nan     0.5261   -0.0000
  1520        0.0008             nan     0.5261   -0.0001
  1540        0.0008             nan     0.5261   -0.0001
  1560        0.0008             nan     0.5261   -0.0001
  1580        0.0008             nan     0.5261   -0.0001
  1600        0.0008             nan     0.5261   -0.0001
  1620        0.0007             nan     0.5261   -0.0001
  1640        0.0006             nan     0.5261   -0.0000
  1660        0.0006             nan     0.5261    0.0000
  1680        0.0005             nan     0.5261   -0.0001
  1700        0.0005             nan     0.5261   -0.0001
  1720        0.0005             nan     0.5261   -0.0000
  1740        0.0005             nan     0.5261   -0.0001
  1760        0.0005             nan     0.5261   -0.0000
  1780        0.0005             nan     0.5261   -0.0000
  1800        0.0004             nan     0.5261   -0.0001
  1820        0.0004             nan     0.5261   -0.0000
  1840        0.0004             nan     0.5261   -0.0000
  1860        0.0004             nan     0.5261   -0.0000
  1880        0.0004             nan     0.5261   -0.0001
  1900        0.0004             nan     0.5261   -0.0000
  1920        0.0003             nan     0.5261    0.0000
  1940        0.0004             nan     0.5261   -0.0000
  1960        0.0003             nan     0.5261   -0.0000
  1980        0.0003             nan     0.5261   -0.0001
  2000        0.0003             nan     0.5261   -0.0000
  2020        0.0003             nan     0.5261   -0.0001
  2040        0.0003             nan     0.5261   -0.0000
  2060        0.0003             nan     0.5261   -0.0000
  2080        0.0003             nan     0.5261   -0.0001
  2100        0.0002             nan     0.5261   -0.0000
  2120        0.0002             nan     0.5261   -0.0000
  2140        0.0002             nan     0.5261   -0.0000
  2160        0.0002             nan     0.5261   -0.0000
  2180        0.0002             nan     0.5261   -0.0000
  2200        0.0002             nan     0.5261    0.0000
  2220        0.0002             nan     0.5261   -0.0000
  2240        0.0002             nan     0.5261   -0.0000
  2260        0.0002             nan     0.5261   -0.0000
  2280        0.0002             nan     0.5261   -0.0000
  2300        0.0002             nan     0.5261   -0.0000
  2320        0.0002             nan     0.5261   -0.0000
  2340        0.0001             nan     0.5261   -0.0000
  2360        0.0001             nan     0.5261    0.0000
  2380        0.0001             nan     0.5261   -0.0000
  2400        0.0001             nan     0.5261   -0.0000
  2420        0.0001             nan     0.5261   -0.0000
  2440        0.0001             nan     0.5261   -0.0000
  2460        0.0001             nan     0.5261   -0.0000
  2480        0.0001             nan     0.5261   -0.0000
  2500        0.0001             nan     0.5261   -0.0000
  2520        0.0001             nan     0.5261   -0.0000
  2540        0.0001             nan     0.5261   -0.0000
  2560        0.0001             nan     0.5261   -0.0000
  2580        0.0001             nan     0.5261   -0.0000
  2600        0.0001             nan     0.5261   -0.0000
  2620        0.0001             nan     0.5261    0.0000
  2640        0.0001             nan     0.5261   -0.0000
  2660        0.0001             nan     0.5261   -0.0000
  2680        0.0001             nan     0.5261   -0.0000
  2700        0.0001             nan     0.5261   -0.0000
  2720        0.0001             nan     0.5261   -0.0000
  2740        0.0001             nan     0.5261   -0.0000
  2760        0.0001             nan     0.5261   -0.0000
  2780        0.0001             nan     0.5261   -0.0000
  2800        0.0001             nan     0.5261   -0.0000
  2820        0.0001             nan     0.5261   -0.0000
  2840        0.0001             nan     0.5261   -0.0000
  2860        0.0001             nan     0.5261   -0.0000
  2880        0.0001             nan     0.5261   -0.0000
  2900        0.0001             nan     0.5261   -0.0000
  2920        0.0001             nan     0.5261   -0.0000
  2940        0.0001             nan     0.5261   -0.0000
  2960        0.0001             nan     0.5261   -0.0000
  2980        0.0001             nan     0.5261    0.0000
  3000        0.0001             nan     0.5261   -0.0000
  3020        0.0001             nan     0.5261   -0.0000
  3032        0.0001             nan     0.5261   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.4477             nan     0.5987    0.2026
     2        0.4439             nan     0.5987   -0.0862
     3        0.4463             nan     0.5987   -0.0638
     4        0.4732             nan     0.5987   -0.1180
     5        0.4219             nan     0.5987    0.0514
     6        0.3905             nan     0.5987   -0.1519
     7        0.3172             nan     0.5987    0.0502
     8        0.3223             nan     0.5987   -0.0697
     9        0.3114             nan     0.5987   -0.0727
    10        0.3182             nan     0.5987   -0.0582
    20        0.2665             nan     0.5987   -0.0449
    40        0.0754             nan     0.5987   -0.0082
    60        0.0302             nan     0.5987   -0.0119
    80        0.0093             nan     0.5987   -0.0001
   100        0.0037             nan     0.5987   -0.0009
   120        0.0013             nan     0.5987   -0.0005
   140        0.0005             nan     0.5987   -0.0002
   160        0.0001             nan     0.5987   -0.0000
   180        0.0000             nan     0.5987   -0.0000
   200        0.0000             nan     0.5987   -0.0000
   220        0.0000             nan     0.5987   -0.0000
   240        0.0000             nan     0.5987   -0.0000
   260        0.0000             nan     0.5987   -0.0000
   280        0.0000             nan     0.5987   -0.0000
   300        0.0000             nan     0.5987   -0.0000
   320        0.0000             nan     0.5987   -0.0000
   340        0.0000             nan     0.5987   -0.0000
   360        0.0000             nan     0.5987   -0.0000
   380        0.0000             nan     0.5987   -0.0000
   400        0.0000             nan     0.5987   -0.0000
   420        0.0000             nan     0.5987   -0.0000
   440        0.0000             nan     0.5987   -0.0000
   460        0.0000             nan     0.5987    0.0000
   480        0.0000             nan     0.5987   -0.0000
   500        0.0000             nan     0.5987   -0.0000
   520        0.0000             nan     0.5987   -0.0000
   540        0.0000             nan     0.5987   -0.0000
   560        0.0000             nan     0.5987   -0.0000
   580        0.0000             nan     0.5987   -0.0000
   600        0.0000             nan     0.5987   -0.0000
   620        0.0000             nan     0.5987   -0.0000
   640        0.0000             nan     0.5987   -0.0000
   660        0.0000             nan     0.5987   -0.0000
   680        0.0000             nan     0.5987   -0.0000
   700        0.0000             nan     0.5987   -0.0000
   720        0.0000             nan     0.5987   -0.0000
   740        0.0000             nan     0.5987   -0.0000
   760        0.0000             nan     0.5987   -0.0000
   780        0.0000             nan     0.5987   -0.0000
   800        0.0000             nan     0.5987   -0.0000
   820        0.0000             nan     0.5987   -0.0000
   840        0.0000             nan     0.5987   -0.0000
   860        0.0000             nan     0.5987   -0.0000
   880        0.0000             nan     0.5987   -0.0000
   900        0.0000             nan     0.5987   -0.0000
   920        0.0000             nan     0.5987   -0.0000
   940        0.0000             nan     0.5987   -0.0000
   960        0.0000             nan     0.5987   -0.0000
   980        0.0000             nan     0.5987   -0.0000
  1000        0.0000             nan     0.5987   -0.0000
  1020        0.0000             nan     0.5987   -0.0000
  1040        0.0000             nan     0.5987   -0.0000
  1060        0.0000             nan     0.5987   -0.0000
  1080        0.0000             nan     0.5987   -0.0000
  1100        0.0000             nan     0.5987   -0.0000
  1120        0.0000             nan     0.5987    0.0000
  1140        0.0000             nan     0.5987   -0.0000
  1160        0.0000             nan     0.5987   -0.0000
  1170        0.0000             nan     0.5987   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5535             nan     0.0299    0.0116
     2        0.5362             nan     0.0299    0.0116
     3        0.5221             nan     0.0299    0.0122
     4        0.5109             nan     0.0299    0.0117
     5        0.4981             nan     0.0299    0.0112
     6        0.4870             nan     0.0299    0.0093
     7        0.4750             nan     0.0299    0.0097
     8        0.4657             nan     0.0299    0.0079
     9        0.4532             nan     0.0299    0.0077
    10        0.4441             nan     0.0299    0.0082
    20        0.3762             nan     0.0299    0.0036
    40        0.3188             nan     0.0299    0.0014
    60        0.2814             nan     0.0299   -0.0038
    80        0.2607             nan     0.0299    0.0011
   100        0.2420             nan     0.0299   -0.0003
   120        0.2296             nan     0.0299   -0.0002
   140        0.2192             nan     0.0299    0.0002
   160        0.2096             nan     0.0299   -0.0025
   180        0.2016             nan     0.0299   -0.0010
   200        0.1958             nan     0.0299   -0.0009
   220        0.1892             nan     0.0299   -0.0015
   240        0.1862             nan     0.0299   -0.0020
   260        0.1818             nan     0.0299   -0.0011
   280        0.1781             nan     0.0299   -0.0017
   300        0.1737             nan     0.0299   -0.0010
   320        0.1691             nan     0.0299   -0.0009
   340        0.1651             nan     0.0299   -0.0002
   360        0.1617             nan     0.0299   -0.0015
   380        0.1570             nan     0.0299   -0.0012
   400        0.1520             nan     0.0299   -0.0013
   420        0.1469             nan     0.0299   -0.0009
   440        0.1432             nan     0.0299   -0.0014

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5493             nan     0.0409    0.0105
     2        0.5259             nan     0.0409    0.0137
     3        0.5072             nan     0.0409    0.0148
     4        0.4884             nan     0.0409    0.0123
     5        0.4768             nan     0.0409    0.0125
     6        0.4685             nan     0.0409    0.0103
     7        0.4584             nan     0.0409    0.0113
     8        0.4472             nan     0.0409    0.0104
     9        0.4369             nan     0.0409    0.0094
    10        0.4283             nan     0.0409    0.0104
    20        0.3691             nan     0.0409    0.0029
    40        0.3109             nan     0.0409   -0.0005
    60        0.2742             nan     0.0409   -0.0002
    80        0.2555             nan     0.0409   -0.0027
   100        0.2417             nan     0.0409    0.0004
   120        0.2322             nan     0.0409   -0.0018
   140        0.2268             nan     0.0409   -0.0027
   160        0.2176             nan     0.0409   -0.0031
   180        0.2119             nan     0.0409   -0.0032
   200        0.2072             nan     0.0409   -0.0027
   220        0.2016             nan     0.0409   -0.0004
   240        0.1981             nan     0.0409   -0.0015
   260        0.1923             nan     0.0409   -0.0005
   280        0.1878             nan     0.0409   -0.0009
   300        0.1835             nan     0.0409   -0.0007
   320        0.1794             nan     0.0409    0.0001
   340        0.1760             nan     0.0409   -0.0018
   360        0.1721             nan     0.0409   -0.0010
   380        0.1671             nan     0.0409   -0.0018
   400        0.1630             nan     0.0409   -0.0014
   420        0.1594             nan     0.0409   -0.0010
   440        0.1560             nan     0.0409   -0.0013
   460        0.1547             nan     0.0409   -0.0010
   480        0.1504             nan     0.0409    0.0002
   500        0.1467             nan     0.0409   -0.0021
   520        0.1433             nan     0.0409   -0.0023
   540        0.1405             nan     0.0409   -0.0013
   560        0.1377             nan     0.0409   -0.0010
   580        0.1348             nan     0.0409   -0.0005
   600        0.1330             nan     0.0409   -0.0009
   620        0.1296             nan     0.0409   -0.0007
   640        0.1276             nan     0.0409   -0.0011
   660        0.1250             nan     0.0409   -0.0011
   680        0.1237             nan     0.0409   -0.0003
   700        0.1217             nan     0.0409   -0.0013
   720        0.1185             nan     0.0409   -0.0009
   740        0.1164             nan     0.0409   -0.0009
   760        0.1146             nan     0.0409   -0.0007
   780        0.1130             nan     0.0409   -0.0020
   800        0.1117             nan     0.0409   -0.0004
   820        0.1092             nan     0.0409   -0.0009
   840        0.1072             nan     0.0409   -0.0010
   860        0.1062             nan     0.0409    0.0001
   880        0.1040             nan     0.0409   -0.0012
   900        0.1027             nan     0.0409   -0.0013
   920        0.1007             nan     0.0409   -0.0010
   940        0.1001             nan     0.0409   -0.0009
   960        0.0987             nan     0.0409   -0.0001
   980        0.0962             nan     0.0409   -0.0005
  1000        0.0953             nan     0.0409   -0.0010
  1020        0.0938             nan     0.0409   -0.0009
  1040        0.0928             nan     0.0409   -0.0006
  1060        0.0918             nan     0.0409   -0.0004
  1080        0.0903             nan     0.0409   -0.0008
  1100        0.0892             nan     0.0409   -0.0007
  1120        0.0877             nan     0.0409    0.0003
  1140        0.0860             nan     0.0409   -0.0003
  1160        0.0848             nan     0.0409   -0.0005
  1180        0.0838             nan     0.0409   -0.0006
  1200        0.0820             nan     0.0409   -0.0005
  1220        0.0806             nan     0.0409   -0.0004
  1240        0.0798             nan     0.0409   -0.0011
  1260        0.0787             nan     0.0409   -0.0004
  1280        0.0779             nan     0.0409   -0.0004
  1300        0.0764             nan     0.0409   -0.0003
  1320        0.0757             nan     0.0409   -0.0003
  1340        0.0750             nan     0.0409   -0.0003
  1360        0.0743             nan     0.0409   -0.0015
  1380        0.0740             nan     0.0409   -0.0007
  1400        0.0731             nan     0.0409   -0.0002
  1420        0.0729             nan     0.0409   -0.0001
  1440        0.0719             nan     0.0409   -0.0005
  1460        0.0712             nan     0.0409   -0.0002
  1480        0.0708             nan     0.0409   -0.0000
  1500        0.0699             nan     0.0409   -0.0007
  1520        0.0690             nan     0.0409   -0.0005
  1540        0.0680             nan     0.0409   -0.0002
  1560        0.0672             nan     0.0409   -0.0006
  1580        0.0660             nan     0.0409   -0.0011
  1600        0.0650             nan     0.0409   -0.0006
  1620        0.0643             nan     0.0409   -0.0006
  1640        0.0638             nan     0.0409   -0.0007
  1660        0.0629             nan     0.0409   -0.0004
  1680        0.0619             nan     0.0409   -0.0000
  1700        0.0609             nan     0.0409   -0.0004
  1720        0.0599             nan     0.0409   -0.0003
  1740        0.0597             nan     0.0409   -0.0003
  1760        0.0587             nan     0.0409   -0.0008
  1780        0.0580             nan     0.0409   -0.0009
  1800        0.0571             nan     0.0409   -0.0003
  1820        0.0569             nan     0.0409   -0.0005
  1840        0.0557             nan     0.0409   -0.0004
  1860        0.0548             nan     0.0409   -0.0001
  1880        0.0543             nan     0.0409   -0.0003
  1900        0.0536             nan     0.0409   -0.0007
  1920        0.0530             nan     0.0409   -0.0003
  1940        0.0521             nan     0.0409   -0.0005
  1960        0.0516             nan     0.0409   -0.0002
  1980        0.0510             nan     0.0409   -0.0003
  2000        0.0507             nan     0.0409   -0.0004
  2020        0.0502             nan     0.0409   -0.0006
  2040        0.0496             nan     0.0409   -0.0003
  2060        0.0494             nan     0.0409   -0.0005
  2080        0.0493             nan     0.0409   -0.0001
  2100        0.0488             nan     0.0409   -0.0007
  2120        0.0485             nan     0.0409   -0.0005
  2140        0.0479             nan     0.0409   -0.0002
  2160        0.0471             nan     0.0409   -0.0004
  2180        0.0464             nan     0.0409   -0.0003
  2200        0.0459             nan     0.0409   -0.0002
  2220        0.0454             nan     0.0409   -0.0002
  2240        0.0454             nan     0.0409   -0.0006
  2260        0.0450             nan     0.0409   -0.0007
  2280        0.0444             nan     0.0409   -0.0004
  2300        0.0439             nan     0.0409   -0.0005
  2320        0.0435             nan     0.0409    0.0001
  2340        0.0433             nan     0.0409   -0.0001
  2360        0.0424             nan     0.0409   -0.0005
  2380        0.0421             nan     0.0409   -0.0003
  2400        0.0420             nan     0.0409   -0.0003
  2420        0.0417             nan     0.0409   -0.0004
  2440        0.0409             nan     0.0409   -0.0002
  2460        0.0404             nan     0.0409   -0.0002
  2480        0.0404             nan     0.0409   -0.0006
  2500        0.0397             nan     0.0409   -0.0003
  2520        0.0393             nan     0.0409   -0.0004
  2540        0.0389             nan     0.0409   -0.0004
  2560        0.0383             nan     0.0409   -0.0002
  2580        0.0381             nan     0.0409   -0.0001
  2600        0.0374             nan     0.0409   -0.0002
  2620        0.0371             nan     0.0409   -0.0002
  2640        0.0367             nan     0.0409   -0.0002
  2660        0.0365             nan     0.0409   -0.0003
  2680        0.0361             nan     0.0409   -0.0004
  2700        0.0358             nan     0.0409   -0.0002
  2720        0.0356             nan     0.0409   -0.0002
  2740        0.0352             nan     0.0409   -0.0001
  2760        0.0348             nan     0.0409   -0.0001
  2780        0.0344             nan     0.0409   -0.0002
  2800        0.0338             nan     0.0409   -0.0004
  2820        0.0334             nan     0.0409   -0.0002
  2840        0.0332             nan     0.0409   -0.0002
  2860        0.0326             nan     0.0409   -0.0001
  2880        0.0323             nan     0.0409   -0.0004
  2900        0.0321             nan     0.0409   -0.0002
  2920        0.0318             nan     0.0409   -0.0002
  2940        0.0313             nan     0.0409   -0.0003
  2960        0.0311             nan     0.0409   -0.0001
  2980        0.0309             nan     0.0409   -0.0002
  3000        0.0305             nan     0.0409   -0.0002
  3020        0.0302             nan     0.0409   -0.0002
  3040        0.0300             nan     0.0409   -0.0001
  3060        0.0298             nan     0.0409   -0.0000
  3080        0.0295             nan     0.0409   -0.0002
  3100        0.0293             nan     0.0409   -0.0001
  3120        0.0291             nan     0.0409   -0.0003
  3140        0.0289             nan     0.0409   -0.0002
  3160        0.0287             nan     0.0409   -0.0005
  3180        0.0286             nan     0.0409   -0.0001
  3200        0.0281             nan     0.0409   -0.0002
  3220        0.0278             nan     0.0409   -0.0002
  3240        0.0274             nan     0.0409   -0.0005
  3260        0.0270             nan     0.0409   -0.0001
  3280        0.0269             nan     0.0409   -0.0001
  3300        0.0268             nan     0.0409   -0.0002
  3320        0.0264             nan     0.0409   -0.0003
  3340        0.0262             nan     0.0409   -0.0002
  3360        0.0261             nan     0.0409   -0.0004
  3380        0.0257             nan     0.0409   -0.0002
  3400        0.0255             nan     0.0409   -0.0002
  3420        0.0256             nan     0.0409   -0.0001
  3440        0.0252             nan     0.0409   -0.0001
  3460        0.0251             nan     0.0409   -0.0001
  3480        0.0248             nan     0.0409   -0.0001
  3500        0.0245             nan     0.0409   -0.0001
  3520        0.0244             nan     0.0409   -0.0001
  3540        0.0245             nan     0.0409   -0.0002
  3560        0.0243             nan     0.0409   -0.0002
  3580        0.0239             nan     0.0409   -0.0001
  3600        0.0237             nan     0.0409   -0.0002
  3620        0.0235             nan     0.0409   -0.0002
  3624        0.0234             nan     0.0409   -0.0001

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5378             nan     0.0628    0.0199
     2        0.5215             nan     0.0628    0.0204
     3        0.5048             nan     0.0628    0.0187
     4        0.4827             nan     0.0628    0.0215
     5        0.4682             nan     0.0628    0.0181
     6        0.4546             nan     0.0628    0.0159
     7        0.4437             nan     0.0628    0.0128
     8        0.4300             nan     0.0628    0.0134
     9        0.4148             nan     0.0628    0.0084
    10        0.4044             nan     0.0628    0.0108
    20        0.3362             nan     0.0628    0.0027
    40        0.2712             nan     0.0628   -0.0018
    60        0.2473             nan     0.0628   -0.0012
    80        0.2270             nan     0.0628   -0.0011
   100        0.2179             nan     0.0628   -0.0003
   120        0.2092             nan     0.0628   -0.0034
   140        0.2018             nan     0.0628   -0.0004
   160        0.1956             nan     0.0628   -0.0022
   180        0.1898             nan     0.0628   -0.0020
   200        0.1800             nan     0.0628   -0.0011
   220        0.1741             nan     0.0628   -0.0035
   240        0.1655             nan     0.0628    0.0005
   260        0.1593             nan     0.0628   -0.0025
   280        0.1539             nan     0.0628   -0.0007
   300        0.1479             nan     0.0628   -0.0015
   320        0.1447             nan     0.0628   -0.0029
   340        0.1401             nan     0.0628   -0.0003
   360        0.1351             nan     0.0628   -0.0012
   380        0.1316             nan     0.0628   -0.0017
   400        0.1267             nan     0.0628    0.0001
   420        0.1227             nan     0.0628   -0.0008
   440        0.1220             nan     0.0628   -0.0002
   460        0.1197             nan     0.0628   -0.0015
   480        0.1169             nan     0.0628   -0.0017
   500        0.1115             nan     0.0628   -0.0002
   520        0.1079             nan     0.0628   -0.0005
   540        0.1042             nan     0.0628   -0.0019
   560        0.1027             nan     0.0628   -0.0015
   562        0.1025             nan     0.0628   -0.0007

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.4921             nan     0.1134    0.0770
     2        0.4125             nan     0.1134    0.0462
     3        0.3458             nan     0.1134    0.0386
     4        0.2988             nan     0.1134    0.0420
     5        0.2573             nan     0.1134    0.0279
     6        0.2372             nan     0.1134    0.0233
     7        0.2143             nan     0.1134    0.0220
     8        0.1891             nan     0.1134    0.0047
     9        0.1784             nan     0.1134    0.0085
    10        0.1738             nan     0.1134    0.0014
    20        0.1184             nan     0.1134   -0.0051
    40        0.0740             nan     0.1134   -0.0020
    60        0.0483             nan     0.1134   -0.0010
    80        0.0328             nan     0.1134   -0.0018
   100        0.0232             nan     0.1134   -0.0011
   120        0.0148             nan     0.1134    0.0001
   140        0.0102             nan     0.1134   -0.0001
   160        0.0077             nan     0.1134   -0.0003
   180        0.0052             nan     0.1134   -0.0003
   200        0.0035             nan     0.1134   -0.0001
   220        0.0025             nan     0.1134   -0.0000
   240        0.0018             nan     0.1134   -0.0001
   260        0.0015             nan     0.1134   -0.0000
   280        0.0010             nan     0.1134   -0.0000
   300        0.0007             nan     0.1134   -0.0000
   320        0.0005             nan     0.1134    0.0000
   340        0.0004             nan     0.1134   -0.0000
   360        0.0003             nan     0.1134   -0.0000
   380        0.0002             nan     0.1134   -0.0000
   400        0.0002             nan     0.1134   -0.0000
   420        0.0001             nan     0.1134   -0.0000
   440        0.0001             nan     0.1134   -0.0000
   460        0.0001             nan     0.1134   -0.0000
   480        0.0000             nan     0.1134   -0.0000
   500        0.0000             nan     0.1134   -0.0000
   520        0.0000             nan     0.1134   -0.0000
   540        0.0000             nan     0.1134   -0.0000
   560        0.0000             nan     0.1134   -0.0000
   580        0.0000             nan     0.1134   -0.0000
   600        0.0000             nan     0.1134   -0.0000
   620        0.0000             nan     0.1134   -0.0000
   640        0.0000             nan     0.1134   -0.0000
   660        0.0000             nan     0.1134   -0.0000
   680        0.0000             nan     0.1134   -0.0000
   700        0.0000             nan     0.1134   -0.0000
   720        0.0000             nan     0.1134   -0.0000
   740        0.0000             nan     0.1134   -0.0000
   760        0.0000             nan     0.1134   -0.0000
   780        0.0000             nan     0.1134   -0.0000
   800        0.0000             nan     0.1134   -0.0000
   820        0.0000             nan     0.1134   -0.0000
   840        0.0000             nan     0.1134   -0.0000
   860        0.0000             nan     0.1134   -0.0000
   880        0.0000             nan     0.1134   -0.0000
   900        0.0000             nan     0.1134   -0.0000
   920        0.0000             nan     0.1134   -0.0000
   940        0.0000             nan     0.1134    0.0000
   960        0.0000             nan     0.1134   -0.0000
   980        0.0000             nan     0.1134   -0.0000
  1000        0.0000             nan     0.1134   -0.0000
  1020        0.0000             nan     0.1134   -0.0000
  1040        0.0000             nan     0.1134   -0.0000
  1060        0.0000             nan     0.1134   -0.0000
  1080        0.0000             nan     0.1134   -0.0000
  1100        0.0000             nan     0.1134   -0.0000
  1120        0.0000             nan     0.1134   -0.0000
  1140        0.0000             nan     0.1134   -0.0000
  1160        0.0000             nan     0.1134   -0.0000
  1180        0.0000             nan     0.1134   -0.0000
  1200        0.0000             nan     0.1134   -0.0000
  1220        0.0000             nan     0.1134   -0.0000
  1240        0.0000             nan     0.1134   -0.0000
  1260        0.0000             nan     0.1134   -0.0000
  1280        0.0000             nan     0.1134   -0.0000
  1300        0.0000             nan     0.1134   -0.0000
  1320        0.0000             nan     0.1134   -0.0000
  1340        0.0000             nan     0.1134   -0.0000
  1360        0.0000             nan     0.1134   -0.0000
  1380        0.0000             nan     0.1134   -0.0000
  1400        0.0000             nan     0.1134   -0.0000
  1420        0.0000             nan     0.1134   -0.0000
  1440        0.0000             nan     0.1134   -0.0000
  1460        0.0000             nan     0.1134   -0.0000
  1480        0.0000             nan     0.1134   -0.0000
  1500        0.0000             nan     0.1134   -0.0000
  1520        0.0000             nan     0.1134   -0.0000
  1540        0.0000             nan     0.1134   -0.0000
  1560        0.0000             nan     0.1134   -0.0000
  1580        0.0000             nan     0.1134   -0.0000
  1600        0.0000             nan     0.1134   -0.0000
  1620        0.0000             nan     0.1134   -0.0000
  1640        0.0000             nan     0.1134    0.0000
  1660        0.0000             nan     0.1134   -0.0000
  1680        0.0000             nan     0.1134   -0.0000
  1700        0.0000             nan     0.1134   -0.0000
  1720        0.0000             nan     0.1134    0.0000
  1740        0.0000             nan     0.1134   -0.0000
  1760        0.0000             nan     0.1134   -0.0000
  1780        0.0000             nan     0.1134   -0.0000
  1800        0.0000             nan     0.1134   -0.0000
  1820        0.0000             nan     0.1134   -0.0000
  1840        0.0000             nan     0.1134   -0.0000
  1860        0.0000             nan     0.1134   -0.0000
  1880        0.0000             nan     0.1134   -0.0000
  1900        0.0000             nan     0.1134   -0.0000
  1920        0.0000             nan     0.1134   -0.0000
  1940        0.0000             nan     0.1134   -0.0000
  1960        0.0000             nan     0.1134   -0.0000
  1980        0.0000             nan     0.1134   -0.0000
  2000        0.0000             nan     0.1134   -0.0000
  2020        0.0000             nan     0.1134   -0.0000
  2040        0.0000             nan     0.1134   -0.0000
  2060        0.0000             nan     0.1134   -0.0000
  2080        0.0000             nan     0.1134   -0.0000
  2100        0.0000             nan     0.1134   -0.0000
  2120        0.0000             nan     0.1134   -0.0000
  2140        0.0000             nan     0.1134   -0.0000
  2160        0.0000             nan     0.1134   -0.0000
  2180        0.0000             nan     0.1134   -0.0000
  2200        0.0000             nan     0.1134   -0.0000
  2220        0.0000             nan     0.1134   -0.0000
  2240        0.0000             nan     0.1134   -0.0000
  2260        0.0000             nan     0.1134   -0.0000
  2280        0.0000             nan     0.1134   -0.0000
  2300        0.0000             nan     0.1134   -0.0000
  2320        0.0000             nan     0.1134   -0.0000
  2340        0.0000             nan     0.1134   -0.0000
  2360        0.0000             nan     0.1134   -0.0000
  2380        0.0000             nan     0.1134   -0.0000
  2400        0.0000             nan     0.1134   -0.0000
  2420        0.0000             nan     0.1134   -0.0000
  2440        0.0000             nan     0.1134   -0.0000
  2460        0.0000             nan     0.1134   -0.0000
  2480        0.0000             nan     0.1134   -0.0000
  2500        0.0000             nan     0.1134   -0.0000
  2520        0.0000             nan     0.1134    0.0000
  2540        0.0000             nan     0.1134   -0.0000
  2560        0.0000             nan     0.1134   -0.0000
  2580        0.0000             nan     0.1134   -0.0000
  2600        0.0000             nan     0.1134   -0.0000
  2620        0.0000             nan     0.1134   -0.0000
  2640        0.0000             nan     0.1134   -0.0000
  2660        0.0000             nan     0.1134    0.0000
  2680        0.0000             nan     0.1134    0.0000
  2700        0.0000             nan     0.1134   -0.0000
  2720        0.0000             nan     0.1134   -0.0000
  2740        0.0000             nan     0.1134   -0.0000
  2760        0.0000             nan     0.1134   -0.0000
  2780        0.0000             nan     0.1134   -0.0000
  2800        0.0000             nan     0.1134   -0.0000
  2820        0.0000             nan     0.1134   -0.0000
  2840        0.0000             nan     0.1134   -0.0000
  2860        0.0000             nan     0.1134   -0.0000
  2880        0.0000             nan     0.1134   -0.0000
  2900        0.0000             nan     0.1134    0.0000
  2920        0.0000             nan     0.1134   -0.0000
  2940        0.0000             nan     0.1134   -0.0000
  2960        0.0000             nan     0.1134   -0.0000
  2980        0.0000             nan     0.1134   -0.0000
  3000        0.0000             nan     0.1134   -0.0000
  3020        0.0000             nan     0.1134   -0.0000
  3040        0.0000             nan     0.1134   -0.0000
  3060        0.0000             nan     0.1134   -0.0000
  3080        0.0000             nan     0.1134   -0.0000
  3100        0.0000             nan     0.1134   -0.0000
  3120        0.0000             nan     0.1134   -0.0000
  3140        0.0000             nan     0.1134   -0.0000
  3160        0.0000             nan     0.1134   -0.0000
  3180        0.0000             nan     0.1134   -0.0000
  3200        0.0000             nan     0.1134   -0.0000
  3220        0.0000             nan     0.1134   -0.0000
  3240        0.0000             nan     0.1134   -0.0000
  3260        0.0000             nan     0.1134   -0.0000
  3280        0.0000             nan     0.1134   -0.0000
  3300        0.0000             nan     0.1134   -0.0000
  3320        0.0000             nan     0.1134   -0.0000
  3340        0.0000             nan     0.1134   -0.0000
  3360        0.0000             nan     0.1134   -0.0000
  3380        0.0000             nan     0.1134   -0.0000
  3400        0.0000             nan     0.1134   -0.0000
  3420        0.0000             nan     0.1134   -0.0000
  3440        0.0000             nan     0.1134   -0.0000
  3460        0.0000             nan     0.1134   -0.0000
  3480        0.0000             nan     0.1134   -0.0000
  3500        0.0000             nan     0.1134    0.0000
  3520        0.0000             nan     0.1134   -0.0000
  3540        0.0000             nan     0.1134   -0.0000
  3560        0.0000             nan     0.1134   -0.0000
  3572        0.0000             nan     0.1134   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5006             nan     0.1520    0.0721
     2        0.4389             nan     0.1520    0.0697
     3        0.3842             nan     0.1520    0.0571
     4        0.3362             nan     0.1520    0.0394
     5        0.3105             nan     0.1520    0.0312
     6        0.2836             nan     0.1520    0.0229
     7        0.2483             nan     0.1520   -0.0038
     8        0.2358             nan     0.1520    0.0050
     9        0.2249             nan     0.1520   -0.0125
    10        0.2099             nan     0.1520    0.0044
    20        0.1658             nan     0.1520   -0.0077
    40        0.1198             nan     0.1520   -0.0026
    60        0.0949             nan     0.1520   -0.0040
    80        0.0778             nan     0.1520   -0.0002
   100        0.0676             nan     0.1520   -0.0034
   120        0.0541             nan     0.1520   -0.0030
   140        0.0462             nan     0.1520   -0.0022
   160        0.0380             nan     0.1520   -0.0013
   180        0.0321             nan     0.1520   -0.0023
   200        0.0263             nan     0.1520   -0.0006
   220        0.0226             nan     0.1520   -0.0010
   240        0.0163             nan     0.1520   -0.0006
   260        0.0134             nan     0.1520   -0.0005
   280        0.0114             nan     0.1520   -0.0005
   300        0.0102             nan     0.1520   -0.0003
   320        0.0100             nan     0.1520   -0.0004
   340        0.0081             nan     0.1520   -0.0001
   360        0.0073             nan     0.1520   -0.0002
   380        0.0060             nan     0.1520   -0.0000
   400        0.0052             nan     0.1520    0.0000
   420        0.0045             nan     0.1520   -0.0001
   440        0.0041             nan     0.1520   -0.0001
   460        0.0036             nan     0.1520   -0.0002
   480        0.0031             nan     0.1520   -0.0002
   500        0.0028             nan     0.1520   -0.0001
   520        0.0024             nan     0.1520   -0.0000
   540        0.0022             nan     0.1520   -0.0001
   560        0.0019             nan     0.1520   -0.0000
   580        0.0016             nan     0.1520   -0.0001
   600        0.0014             nan     0.1520   -0.0000
   620        0.0013             nan     0.1520   -0.0001
   640        0.0012             nan     0.1520   -0.0001
   660        0.0010             nan     0.1520   -0.0000
   680        0.0009             nan     0.1520   -0.0000
   700        0.0008             nan     0.1520   -0.0000
   720        0.0007             nan     0.1520   -0.0000
   740        0.0007             nan     0.1520   -0.0000
   760        0.0006             nan     0.1520   -0.0000
   780        0.0005             nan     0.1520   -0.0000
   800        0.0004             nan     0.1520   -0.0000
   820        0.0004             nan     0.1520   -0.0000
   840        0.0003             nan     0.1520   -0.0000
   860        0.0003             nan     0.1520   -0.0000
   880        0.0002             nan     0.1520   -0.0000
   900        0.0002             nan     0.1520   -0.0000
   920        0.0002             nan     0.1520   -0.0000
   940        0.0002             nan     0.1520   -0.0000
   960        0.0002             nan     0.1520   -0.0000
   980        0.0002             nan     0.1520   -0.0000
  1000        0.0001             nan     0.1520   -0.0000
  1020        0.0001             nan     0.1520   -0.0000
  1040        0.0001             nan     0.1520   -0.0000
  1060        0.0001             nan     0.1520   -0.0000
  1080        0.0001             nan     0.1520    0.0000
  1100        0.0001             nan     0.1520   -0.0000
  1120        0.0001             nan     0.1520   -0.0000
  1140        0.0001             nan     0.1520   -0.0000
  1160        0.0001             nan     0.1520   -0.0000
  1180        0.0000             nan     0.1520   -0.0000
  1200        0.0000             nan     0.1520   -0.0000
  1220        0.0000             nan     0.1520   -0.0000
  1240        0.0000             nan     0.1520   -0.0000
  1254        0.0000             nan     0.1520   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3807             nan     0.3279    0.1369
     2        0.3046             nan     0.3279    0.0746
     3        0.2543             nan     0.3279    0.0532
     4        0.2563             nan     0.3279   -0.0490
     5        0.2255             nan     0.3279    0.0331
     6        0.1991             nan     0.3279    0.0215
     7        0.1914             nan     0.3279    0.0035
     8        0.1898             nan     0.3279   -0.0220
     9        0.1918             nan     0.3279   -0.0403
    10        0.1686             nan     0.3279    0.0087
    20        0.1183             nan     0.3279   -0.0091
    40        0.0940             nan     0.3279   -0.0117
    60        0.0762             nan     0.3279   -0.0126
    80        0.0480             nan     0.3279   -0.0012
   100        0.0389             nan     0.3279   -0.0067
   120        0.0278             nan     0.3279   -0.0003
   140        0.0215             nan     0.3279   -0.0021
   160        0.0187             nan     0.3279   -0.0018
   180        0.0144             nan     0.3279   -0.0013
   200        0.0110             nan     0.3279   -0.0010
   220        0.0083             nan     0.3279   -0.0004
   240        0.0071             nan     0.3279   -0.0009
   260        0.0058             nan     0.3279   -0.0004
   280        0.0049             nan     0.3279   -0.0005
   300        0.0042             nan     0.3279   -0.0004
   320        0.0036             nan     0.3279   -0.0003
   340        0.0032             nan     0.3279   -0.0001
   360        0.0028             nan     0.3279   -0.0003
   380        0.0023             nan     0.3279   -0.0001
   400        0.0018             nan     0.3279   -0.0002
   420        0.0016             nan     0.3279   -0.0001
   440        0.0014             nan     0.3279   -0.0001
   460        0.0011             nan     0.3279   -0.0001
   480        0.0009             nan     0.3279   -0.0000
   500        0.0007             nan     0.3279   -0.0001
   520        0.0007             nan     0.3279   -0.0000
   540        0.0006             nan     0.3279   -0.0001
   560        0.0005             nan     0.3279   -0.0001
   580        0.0004             nan     0.3279   -0.0000
   600        0.0003             nan     0.3279   -0.0000
   620        0.0003             nan     0.3279   -0.0000
   640        0.0002             nan     0.3279   -0.0000
   660        0.0002             nan     0.3279   -0.0000
   680        0.0002             nan     0.3279   -0.0000
   700        0.0001             nan     0.3279   -0.0000
   720        0.0001             nan     0.3279   -0.0000
   740        0.0001             nan     0.3279   -0.0000
   760        0.0001             nan     0.3279   -0.0000
   780        0.0001             nan     0.3279    0.0000
   800        0.0001             nan     0.3279   -0.0000
   820        0.0001             nan     0.3279   -0.0000
   840        0.0001             nan     0.3279   -0.0000
   860        0.0000             nan     0.3279   -0.0000
   880        0.0000             nan     0.3279   -0.0000
   900        0.0000             nan     0.3279   -0.0000
   920        0.0000             nan     0.3279   -0.0000
   940        0.0000             nan     0.3279    0.0000
   960        0.0000             nan     0.3279   -0.0000
   980        0.0000             nan     0.3279   -0.0000
  1000        0.0000             nan     0.3279   -0.0000
  1020        0.0000             nan     0.3279   -0.0000
  1040        0.0000             nan     0.3279   -0.0000
  1060        0.0000             nan     0.3279   -0.0000
  1080        0.0000             nan     0.3279   -0.0000
  1100        0.0000             nan     0.3279   -0.0000
  1120        0.0000             nan     0.3279   -0.0000
  1140        0.0000             nan     0.3279   -0.0000
  1160        0.0000             nan     0.3279   -0.0000
  1180        0.0000             nan     0.3279   -0.0000
  1200        0.0000             nan     0.3279   -0.0000
  1220        0.0000             nan     0.3279   -0.0000
  1240        0.0000             nan     0.3279   -0.0000
  1260        0.0000             nan     0.3279   -0.0000
  1280        0.0000             nan     0.3279   -0.0000
  1300        0.0000             nan     0.3279   -0.0000
  1320        0.0000             nan     0.3279   -0.0000
  1340        0.0000             nan     0.3279   -0.0000
  1360        0.0000             nan     0.3279   -0.0000
  1380        0.0000             nan     0.3279   -0.0000
  1400        0.0000             nan     0.3279   -0.0000
  1420        0.0000             nan     0.3279   -0.0000
  1440        0.0000             nan     0.3279   -0.0000
  1460        0.0000             nan     0.3279    0.0000
  1480        0.0000             nan     0.3279   -0.0000
  1500        0.0000             nan     0.3279   -0.0000
  1520        0.0000             nan     0.3279   -0.0000
  1540        0.0000             nan     0.3279   -0.0000
  1560        0.0000             nan     0.3279   -0.0000
  1580        0.0000             nan     0.3279   -0.0000
  1600        0.0000             nan     0.3279   -0.0000
  1620        0.0000             nan     0.3279   -0.0000
  1640        0.0000             nan     0.3279   -0.0000
  1660        0.0000             nan     0.3279   -0.0000
  1680        0.0000             nan     0.3279   -0.0000
  1700        0.0000             nan     0.3279   -0.0000
  1720        0.0000             nan     0.3279   -0.0000
  1740        0.0000             nan     0.3279   -0.0000
  1760        0.0000             nan     0.3279   -0.0000
  1780        0.0000             nan     0.3279    0.0000
  1800        0.0000             nan     0.3279   -0.0000
  1820        0.0000             nan     0.3279   -0.0000
  1840        0.0000             nan     0.3279   -0.0000
  1860        0.0000             nan     0.3279   -0.0000
  1880        0.0000             nan     0.3279   -0.0000
  1900        0.0000             nan     0.3279   -0.0000
  1920        0.0000             nan     0.3279   -0.0000
  1940        0.0000             nan     0.3279   -0.0000
  1960        0.0000             nan     0.3279   -0.0000
  1980        0.0000             nan     0.3279   -0.0000
  2000        0.0000             nan     0.3279   -0.0000
  2020        0.0000             nan     0.3279   -0.0000
  2040        0.0000             nan     0.3279   -0.0000
  2060        0.0000             nan     0.3279   -0.0000
  2080        0.0000             nan     0.3279   -0.0000
  2100        0.0000             nan     0.3279   -0.0000
  2120        0.0000             nan     0.3279   -0.0000
  2140        0.0000             nan     0.3279   -0.0000
  2160        0.0000             nan     0.3279   -0.0000
  2180        0.0000             nan     0.3279   -0.0000
  2200        0.0000             nan     0.3279   -0.0000
  2220        0.0000             nan     0.3279   -0.0000
  2240        0.0000             nan     0.3279   -0.0000
  2260        0.0000             nan     0.3279   -0.0000
  2280        0.0000             nan     0.3279   -0.0000
  2300        0.0000             nan     0.3279   -0.0000
  2320        0.0000             nan     0.3279   -0.0000
  2340        0.0000             nan     0.3279   -0.0000
  2360        0.0000             nan     0.3279   -0.0000
  2380        0.0000             nan     0.3279   -0.0000
  2400        0.0000             nan     0.3279   -0.0000
  2420        0.0000             nan     0.3279   -0.0000
  2440        0.0000             nan     0.3279   -0.0000
  2460        0.0000             nan     0.3279   -0.0000
  2480        0.0000             nan     0.3279   -0.0000
  2500        0.0000             nan     0.3279   -0.0000
  2520        0.0000             nan     0.3279   -0.0000
  2540        0.0000             nan     0.3279   -0.0000
  2560        0.0000             nan     0.3279   -0.0000
  2580        0.0000             nan     0.3279   -0.0000
  2600        0.0000             nan     0.3279   -0.0000
  2620        0.0000             nan     0.3279   -0.0000
  2640        0.0000             nan     0.3279   -0.0000
  2660        0.0000             nan     0.3279   -0.0000
  2680        0.0000             nan     0.3279   -0.0000
  2700        0.0000             nan     0.3279   -0.0000
  2720        0.0000             nan     0.3279   -0.0000
  2740        0.0000             nan     0.3279   -0.0000
  2760        0.0000             nan     0.3279   -0.0000
  2780        0.0000             nan     0.3279   -0.0000
  2800        0.0000             nan     0.3279   -0.0000
  2820        0.0000             nan     0.3279   -0.0000
  2840        0.0000             nan     0.3279   -0.0000
  2860        0.0000             nan     0.3279   -0.0000
  2880        0.0000             nan     0.3279   -0.0000
  2900        0.0000             nan     0.3279   -0.0000
  2920        0.0000             nan     0.3279   -0.0000
  2940        0.0000             nan     0.3279   -0.0000
  2960        0.0000             nan     0.3279   -0.0000
  2980        0.0000             nan     0.3279   -0.0000
  3000        0.0000             nan     0.3279   -0.0000
  3020        0.0000             nan     0.3279   -0.0000
  3040        0.0000             nan     0.3279   -0.0000
  3060        0.0000             nan     0.3279   -0.0000
  3080        0.0000             nan     0.3279   -0.0000
  3100        0.0000             nan     0.3279    0.0000
  3120        0.0000             nan     0.3279   -0.0000
  3140        0.0000             nan     0.3279   -0.0000
  3160        0.0000             nan     0.3279   -0.0000
  3180        0.0000             nan     0.3279   -0.0000
  3200        0.0000             nan     0.3279   -0.0000
  3220        0.0000             nan     0.3279   -0.0000
  3240        0.0000             nan     0.3279   -0.0000
  3260        0.0000             nan     0.3279   -0.0000
  3280        0.0000             nan     0.3279   -0.0000
  3300        0.0000             nan     0.3279   -0.0000
  3320        0.0000             nan     0.3279   -0.0000
  3340        0.0000             nan     0.3279   -0.0000
  3360        0.0000             nan     0.3279    0.0000
  3380        0.0000             nan     0.3279   -0.0000
  3400        0.0000             nan     0.3279   -0.0000
  3420        0.0000             nan     0.3279   -0.0000
  3440        0.0000             nan     0.3279   -0.0000
  3460        0.0000             nan     0.3279   -0.0000
  3480        0.0000             nan     0.3279   -0.0000
  3500        0.0000             nan     0.3279   -0.0000
  3520        0.0000             nan     0.3279   -0.0000
  3540        0.0000             nan     0.3279   -0.0000
  3560        0.0000             nan     0.3279   -0.0000
  3580        0.0000             nan     0.3279   -0.0000
  3600        0.0000             nan     0.3279   -0.0000
  3620        0.0000             nan     0.3279   -0.0000
  3640        0.0000             nan     0.3279   -0.0000
  3660        0.0000             nan     0.3279   -0.0000
  3680        0.0000             nan     0.3279   -0.0000
  3700        0.0000             nan     0.3279   -0.0000
  3720        0.0000             nan     0.3279   -0.0000
  3740        0.0000             nan     0.3279   -0.0000
  3760        0.0000             nan     0.3279   -0.0000
  3780        0.0000             nan     0.3279   -0.0000
  3800        0.0000             nan     0.3279    0.0000
  3820        0.0000             nan     0.3279   -0.0000
  3840        0.0000             nan     0.3279   -0.0000
  3860        0.0000             nan     0.3279   -0.0000
  3880        0.0000             nan     0.3279   -0.0000
  3900        0.0000             nan     0.3279   -0.0000
  3920        0.0000             nan     0.3279   -0.0000
  3940        0.0000             nan     0.3279   -0.0000
  3960        0.0000             nan     0.3279   -0.0000
  3980        0.0000             nan     0.3279   -0.0000
  4000        0.0000             nan     0.3279   -0.0000
  4020        0.0000             nan     0.3279   -0.0000
  4040        0.0000             nan     0.3279   -0.0000
  4060        0.0000             nan     0.3279   -0.0000
  4080        0.0000             nan     0.3279   -0.0000
  4100        0.0000             nan     0.3279   -0.0000
  4120        0.0000             nan     0.3279   -0.0000
  4140        0.0000             nan     0.3279   -0.0000
  4160        0.0000             nan     0.3279   -0.0000
  4180        0.0000             nan     0.3279    0.0000
  4200        0.0000             nan     0.3279   -0.0000
  4220        0.0000             nan     0.3279   -0.0000
  4240        0.0000             nan     0.3279   -0.0000
  4260        0.0000             nan     0.3279   -0.0000
  4280        0.0000             nan     0.3279   -0.0000
  4300        0.0000             nan     0.3279   -0.0000
  4320        0.0000             nan     0.3279   -0.0000
  4340        0.0000             nan     0.3279   -0.0000
  4360        0.0000             nan     0.3279   -0.0000
  4380        0.0000             nan     0.3279   -0.0000
  4400        0.0000             nan     0.3279   -0.0000
  4420        0.0000             nan     0.3279   -0.0000
  4440        0.0000             nan     0.3279   -0.0000
  4460        0.0000             nan     0.3279   -0.0000
  4480        0.0000             nan     0.3279   -0.0000
  4500        0.0000             nan     0.3279   -0.0000
  4520        0.0000             nan     0.3279   -0.0000
  4540        0.0000             nan     0.3279   -0.0000
  4560        0.0000             nan     0.3279   -0.0000
  4580        0.0000             nan     0.3279   -0.0000
  4600        0.0000             nan     0.3279   -0.0000
  4620        0.0000             nan     0.3279   -0.0000
  4640        0.0000             nan     0.3279   -0.0000
  4660        0.0000             nan     0.3279   -0.0000
  4665        0.0000             nan     0.3279   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3811             nan     0.4488    0.1483
     2        0.3199             nan     0.4488    0.0326
     3        0.2923             nan     0.4488    0.0011
     4        0.2882             nan     0.4488   -0.0259
     5        0.2588             nan     0.4488   -0.0143
     6        0.2610             nan     0.4488   -0.0195
     7        0.2808             nan     0.4488   -0.0801
     8        0.2720             nan     0.4488   -0.0285
     9        0.2676             nan     0.4488   -0.0212
    10        0.2695             nan     0.4488   -0.0517
    20        0.2397             nan     0.4488   -0.0355
    40        0.1406             nan     0.4488   -0.0142
    60        0.0995             nan     0.4488   -0.0129
    80        0.0666             nan     0.4488   -0.0002
   100        0.0547             nan     0.4488   -0.0007
   120        0.0456             nan     0.4488   -0.0014
   140        0.0382             nan     0.4488   -0.0023
   160        0.0290             nan     0.4488   -0.0001
   180        0.0251             nan     0.4488    0.0004
   200        0.0226             nan     0.4488   -0.0021
   220        0.0207             nan     0.4488   -0.0021
   240        0.0155             nan     0.4488   -0.0029
   260        0.0131             nan     0.4488   -0.0016
   280        0.0101             nan     0.4488   -0.0004
   300        0.0087             nan     0.4488   -0.0005
   320        0.0086             nan     0.4488   -0.0006
   340        0.0070             nan     0.4488   -0.0013
   360        0.0057             nan     0.4488   -0.0004
   380        0.0053             nan     0.4488   -0.0009
   400        0.0045             nan     0.4488   -0.0003
   420        0.0041             nan     0.4488   -0.0003
   440        0.0039             nan     0.4488   -0.0003
   460        0.0032             nan     0.4488   -0.0004
   480        0.0029             nan     0.4488   -0.0001
   500        0.0026             nan     0.4488   -0.0004
   520        0.0021             nan     0.4488   -0.0002
   540        0.0018             nan     0.4488   -0.0001
   560        0.0016             nan     0.4488   -0.0001
   580        0.0013             nan     0.4488   -0.0003
   600        0.0012             nan     0.4488   -0.0001
   620        0.0011             nan     0.4488   -0.0001
   640        0.0009             nan     0.4488    0.0000
   660        0.0008             nan     0.4488   -0.0001
   680        0.0007             nan     0.4488   -0.0001
   700        0.0007             nan     0.4488   -0.0000
   720        0.0006             nan     0.4488   -0.0000
   740        0.0005             nan     0.4488   -0.0000
   760        0.0004             nan     0.4488   -0.0000
   780        0.0003             nan     0.4488   -0.0000
   800        0.0003             nan     0.4488   -0.0000
   820        0.0003             nan     0.4488   -0.0000
   840        0.0002             nan     0.4488   -0.0000
   860        0.0002             nan     0.4488   -0.0000
   880        0.0002             nan     0.4488   -0.0000
   900        0.0001             nan     0.4488   -0.0000
   920        0.0001             nan     0.4488   -0.0000
   940        0.0001             nan     0.4488   -0.0000
   960        0.0001             nan     0.4488   -0.0000
   980        0.0001             nan     0.4488   -0.0000
  1000        0.0001             nan     0.4488   -0.0000
  1020        0.0001             nan     0.4488   -0.0000
  1040        0.0001             nan     0.4488   -0.0000
  1060        0.0001             nan     0.4488   -0.0000
  1080        0.0000             nan     0.4488   -0.0000
  1100        0.0000             nan     0.4488   -0.0000
  1120        0.0000             nan     0.4488   -0.0000
  1140        0.0000             nan     0.4488   -0.0000
  1160        0.0000             nan     0.4488   -0.0000
  1180        0.0000             nan     0.4488   -0.0000
  1200        0.0000             nan     0.4488   -0.0000
  1220        0.0000             nan     0.4488   -0.0000
  1240        0.0000             nan     0.4488   -0.0000
  1260        0.0000             nan     0.4488   -0.0000
  1280        0.0000             nan     0.4488   -0.0000
  1300        0.0000             nan     0.4488   -0.0000
  1320        0.0000             nan     0.4488   -0.0000
  1340        0.0000             nan     0.4488   -0.0000
  1360        0.0000             nan     0.4488   -0.0000
  1380        0.0000             nan     0.4488   -0.0000
  1400        0.0000             nan     0.4488   -0.0000
  1420        0.0000             nan     0.4488   -0.0000
  1431        0.0000             nan     0.4488   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3973             nan     0.5231    0.1932
     2        0.3587             nan     0.5231    0.0469
     3        0.3075             nan     0.5231   -0.0218
     4        0.2909             nan     0.5231    0.0001
     5        0.2801             nan     0.5231   -0.0041
     6        0.2750             nan     0.5231   -0.0267
     7        0.2749             nan     0.5231   -0.0270
     8        0.2840             nan     0.5231   -0.0607
     9        0.2882             nan     0.5231   -0.0253
    10        0.2878             nan     0.5231   -0.0614
    20        0.2362             nan     0.5231   -0.0319
    40        0.1352             nan     0.5231    0.0018
    60        0.1044             nan     0.5231   -0.0059
    80        0.0954             nan     0.5231   -0.0046
   100        0.0844             nan     0.5231   -0.0275
   120        0.0648             nan     0.5231   -0.0055
   140        0.0529             nan     0.5231   -0.0050
   160        0.0443             nan     0.5231   -0.0014
   180        0.0387             nan     0.5231   -0.0077
   200        0.0368             nan     0.5231   -0.0028
   220        0.0294             nan     0.5231   -0.0051
   240        0.0222             nan     0.5231   -0.0025
   260        0.0181             nan     0.5231   -0.0015
   280        0.0171             nan     0.5231   -0.0033
   300        0.0137             nan     0.5231   -0.0002
   320        0.0132             nan     0.5231   -0.0013
   340        0.0124             nan     0.5231   -0.0025
   360        0.0107             nan     0.5231   -0.0002
   380        0.0091             nan     0.5231   -0.0002
   400        0.0086             nan     0.5231   -0.0017
   420        0.0078             nan     0.5231   -0.0006
   440        0.0058             nan     0.5231   -0.0008
   460        0.0047             nan     0.5231   -0.0003
   480        0.0038             nan     0.5231   -0.0002
   500        0.0035             nan     0.5231   -0.0006
   520        0.0032             nan     0.5231   -0.0003
   540        0.0031             nan     0.5231   -0.0001
   560        0.0029             nan     0.5231   -0.0001
   580        0.0026             nan     0.5231   -0.0003
   600        0.0026             nan     0.5231   -0.0002
   620        0.0024             nan     0.5231   -0.0003
   640        0.0024             nan     0.5231   -0.0003
   660        0.0020             nan     0.5231   -0.0001
   680        0.0018             nan     0.5231   -0.0002
   700        0.0019             nan     0.5231   -0.0003
   720        0.0015             nan     0.5231    0.0001
   740        0.0013             nan     0.5231   -0.0001
   760        0.0011             nan     0.5231   -0.0002
   780        0.0010             nan     0.5231   -0.0001
   800        0.0010             nan     0.5231    0.0000
   820        0.0009             nan     0.5231   -0.0001
   840        0.0007             nan     0.5231   -0.0001
   860        0.0006             nan     0.5231   -0.0001
   880        0.0006             nan     0.5231   -0.0001
   900        0.0005             nan     0.5231   -0.0000
   920        0.0005             nan     0.5231   -0.0001
   940        0.0004             nan     0.5231   -0.0001
   960        0.0004             nan     0.5231   -0.0000
   980        0.0004             nan     0.5231   -0.0000
  1000        0.0004             nan     0.5231   -0.0000
  1020        0.0003             nan     0.5231   -0.0001
  1040        0.0003             nan     0.5231   -0.0000
  1060        0.0002             nan     0.5231   -0.0000
  1080        0.0002             nan     0.5231   -0.0000
  1100        0.0002             nan     0.5231   -0.0000
  1120        0.0002             nan     0.5231   -0.0000
  1140        0.0002             nan     0.5231   -0.0000
  1160        0.0002             nan     0.5231   -0.0000
  1180        0.0001             nan     0.5231   -0.0000
  1200        0.0001             nan     0.5231   -0.0000
  1220        0.0001             nan     0.5231   -0.0000
  1240        0.0001             nan     0.5231   -0.0000
  1260        0.0001             nan     0.5231   -0.0000
  1280        0.0001             nan     0.5231   -0.0000
  1300        0.0001             nan     0.5231   -0.0000
  1320        0.0001             nan     0.5231   -0.0000
  1340        0.0001             nan     0.5231   -0.0000
  1360        0.0001             nan     0.5231   -0.0000
  1380        0.0001             nan     0.5231   -0.0000
  1400        0.0001             nan     0.5231   -0.0000
  1420        0.0000             nan     0.5231   -0.0000
  1440        0.0000             nan     0.5231   -0.0000
  1460        0.0000             nan     0.5231    0.0000
  1480        0.0000             nan     0.5231   -0.0000
  1500        0.0000             nan     0.5231   -0.0000
  1520        0.0000             nan     0.5231   -0.0000
  1540        0.0000             nan     0.5231   -0.0000
  1560        0.0000             nan     0.5231   -0.0000
  1580        0.0000             nan     0.5231   -0.0000
  1600        0.0000             nan     0.5231   -0.0000
  1620        0.0000             nan     0.5231   -0.0000
  1640        0.0000             nan     0.5231   -0.0000
  1660        0.0000             nan     0.5231   -0.0000
  1680        0.0000             nan     0.5231   -0.0000
  1700        0.0000             nan     0.5231    0.0000
  1720        0.0000             nan     0.5231   -0.0000
  1740        0.0000             nan     0.5231   -0.0000
  1760        0.0000             nan     0.5231   -0.0000
  1780        0.0000             nan     0.5231   -0.0000
  1800        0.0000             nan     0.5231   -0.0000
  1820        0.0000             nan     0.5231   -0.0000
  1840        0.0000             nan     0.5231   -0.0000
  1860        0.0000             nan     0.5231   -0.0000
  1880        0.0000             nan     0.5231   -0.0000
  1900        0.0000             nan     0.5231   -0.0000
  1920        0.0000             nan     0.5231   -0.0000
  1940        0.0000             nan     0.5231   -0.0000
  1960        0.0000             nan     0.5231   -0.0000
  1980        0.0000             nan     0.5231   -0.0000
  2000        0.0000             nan     0.5231   -0.0000
  2020        0.0000             nan     0.5231   -0.0000
  2040        0.0000             nan     0.5231   -0.0000
  2060        0.0000             nan     0.5231   -0.0000
  2080        0.0000             nan     0.5231    0.0000
  2100        0.0000             nan     0.5231   -0.0000
  2120        0.0000             nan     0.5231   -0.0000
  2140        0.0000             nan     0.5231   -0.0000
  2160        0.0000             nan     0.5231   -0.0000
  2180        0.0000             nan     0.5231    0.0000
  2200        0.0000             nan     0.5231   -0.0000
  2220        0.0000             nan     0.5231   -0.0000
  2240        0.0000             nan     0.5231   -0.0000
  2260        0.0000             nan     0.5231   -0.0000
  2280        0.0000             nan     0.5231   -0.0000
  2300        0.0000             nan     0.5231   -0.0000
  2320        0.0000             nan     0.5231   -0.0000
  2340        0.0000             nan     0.5231   -0.0000
  2360        0.0000             nan     0.5231   -0.0000
  2380        0.0000             nan     0.5231   -0.0000
  2400        0.0000             nan     0.5231   -0.0000
  2420        0.0000             nan     0.5231   -0.0000
  2440        0.0000             nan     0.5231   -0.0000
  2460        0.0000             nan     0.5231   -0.0000
  2480        0.0000             nan     0.5231   -0.0000
  2500        0.0000             nan     0.5231   -0.0000
  2520        0.0000             nan     0.5231   -0.0000
  2540        0.0000             nan     0.5231   -0.0000
  2560        0.0000             nan     0.5231   -0.0000
  2580        0.0000             nan     0.5231   -0.0000
  2600        0.0000             nan     0.5231    0.0000
  2620        0.0000             nan     0.5231   -0.0000
  2640        0.0000             nan     0.5231   -0.0000
  2660        0.0000             nan     0.5231   -0.0000
  2680        0.0000             nan     0.5231   -0.0000
  2700        0.0000             nan     0.5231   -0.0000
  2720        0.0000             nan     0.5231   -0.0000
  2740        0.0000             nan     0.5231   -0.0000
  2760        0.0000             nan     0.5231   -0.0000
  2780        0.0000             nan     0.5231   -0.0000
  2800        0.0000             nan     0.5231   -0.0000
  2820        0.0000             nan     0.5231   -0.0000
  2840        0.0000             nan     0.5231   -0.0000
  2860        0.0000             nan     0.5231   -0.0000
  2880        0.0000             nan     0.5231   -0.0000
  2900        0.0000             nan     0.5231   -0.0000
  2920        0.0000             nan     0.5231   -0.0000
  2940        0.0000             nan     0.5231    0.0000
  2960        0.0000             nan     0.5231   -0.0000
  2980        0.0000             nan     0.5231   -0.0000
  3000        0.0000             nan     0.5231   -0.0000
  3020        0.0000             nan     0.5231   -0.0000
  3040        0.0000             nan     0.5231   -0.0000
  3060        0.0000             nan     0.5231   -0.0000
  3080        0.0000             nan     0.5231   -0.0000
  3100        0.0000             nan     0.5231   -0.0000
  3120        0.0000             nan     0.5231   -0.0000
  3140        0.0000             nan     0.5231   -0.0000
  3160        0.0000             nan     0.5231   -0.0000
  3180        0.0000             nan     0.5231   -0.0000
  3200        0.0000             nan     0.5231   -0.0000
  3220        0.0000             nan     0.5231   -0.0000
  3240        0.0000             nan     0.5231   -0.0000
  3260        0.0000             nan     0.5231   -0.0000
  3280        0.0000             nan     0.5231   -0.0000
  3300        0.0000             nan     0.5231   -0.0000
  3320        0.0000             nan     0.5231   -0.0000
  3340        0.0000             nan     0.5231    0.0000
  3360        0.0000             nan     0.5231   -0.0000
  3380        0.0000             nan     0.5231   -0.0000
  3400        0.0000             nan     0.5231   -0.0000
  3420        0.0000             nan     0.5231   -0.0000
  3440        0.0000             nan     0.5231   -0.0000
  3460        0.0000             nan     0.5231   -0.0000
  3480        0.0000             nan     0.5231   -0.0000
  3500        0.0000             nan     0.5231   -0.0000
  3520        0.0000             nan     0.5231   -0.0000
  3540        0.0000             nan     0.5231   -0.0000
  3560        0.0000             nan     0.5231   -0.0000
  3580        0.0000             nan     0.5231   -0.0000
  3600        0.0000             nan     0.5231   -0.0000
  3620        0.0000             nan     0.5231   -0.0000
  3640        0.0000             nan     0.5231   -0.0000
  3660        0.0000             nan     0.5231   -0.0000
  3680        0.0000             nan     0.5231   -0.0000
  3700        0.0000             nan     0.5231   -0.0000
  3720        0.0000             nan     0.5231   -0.0000
  3740        0.0000             nan     0.5231   -0.0000
  3760        0.0000             nan     0.5231    0.0000
  3780        0.0000             nan     0.5231   -0.0000
  3800        0.0000             nan     0.5231   -0.0000
  3820        0.0000             nan     0.5231   -0.0000
  3840        0.0000             nan     0.5231   -0.0000
  3860        0.0000             nan     0.5231   -0.0000
  3880        0.0000             nan     0.5231   -0.0000
  3900        0.0000             nan     0.5231   -0.0000
  3920        0.0000             nan     0.5231    0.0000
  3940        0.0000             nan     0.5231   -0.0000
  3960        0.0000             nan     0.5231   -0.0000
  3980        0.0000             nan     0.5231   -0.0000
  4000        0.0000             nan     0.5231   -0.0000
  4020        0.0000             nan     0.5231   -0.0000
  4040        0.0000             nan     0.5231   -0.0000
  4060        0.0000             nan     0.5231   -0.0000
  4080        0.0000             nan     0.5231   -0.0000
  4100        0.0000             nan     0.5231   -0.0000
  4120        0.0000             nan     0.5231   -0.0000
  4140        0.0000             nan     0.5231   -0.0000
  4160        0.0000             nan     0.5231   -0.0000
  4180        0.0000             nan     0.5231   -0.0000
  4200        0.0000             nan     0.5231   -0.0000
  4220        0.0000             nan     0.5231   -0.0000
  4240        0.0000             nan     0.5231   -0.0000
  4260        0.0000             nan     0.5231   -0.0000
  4280        0.0000             nan     0.5231   -0.0000
  4300        0.0000             nan     0.5231   -0.0000
  4320        0.0000             nan     0.5231   -0.0000
  4340        0.0000             nan     0.5231   -0.0000
  4360        0.0000             nan     0.5231   -0.0000
  4380        0.0000             nan     0.5231   -0.0000
  4383        0.0000             nan     0.5231   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.4228             nan     0.5261    0.1946
     2        0.3110             nan     0.5261    0.0613
     3        0.2980             nan     0.5261   -0.0394
     4        0.2891             nan     0.5261   -0.0129
     5        0.2805             nan     0.5261   -0.0301
     6        0.2771             nan     0.5261   -0.0188
     7        0.2793             nan     0.5261   -0.0246
     8        0.2782             nan     0.5261   -0.0172
     9        0.2772             nan     0.5261   -0.0236
    10        0.2806             nan     0.5261   -0.0273
    20        0.2353             nan     0.5261   -0.0267
    40        0.1884             nan     0.5261   -0.0274
    60        0.1597             nan     0.5261   -0.0136
    80        0.1347             nan     0.5261   -0.0291
   100        0.1318             nan     0.5261   -0.0359
   120        0.1084             nan     0.5261   -0.0136
   140        0.0952             nan     0.5261   -0.0064
   160        0.0848             nan     0.5261   -0.0099
   180        0.0731             nan     0.5261   -0.0038
   200        0.0649             nan     0.5261    0.0006
   220        0.0550             nan     0.5261   -0.0012
   240        0.0498             nan     0.5261   -0.0026
   260        0.0421             nan     0.5261   -0.0018
   280        0.0393             nan     0.5261   -0.0029
   300        0.0367             nan     0.5261   -0.0026
   320        0.0329             nan     0.5261   -0.0022
   340        0.0338             nan     0.5261   -0.0054
   360        0.0263             nan     0.5261   -0.0011
   380        0.0259             nan     0.5261   -0.0050
   400        0.0237             nan     0.5261   -0.0024
   420        0.0203             nan     0.5261    0.0003
   440        0.0201             nan     0.5261   -0.0030
   460        0.0173             nan     0.5261   -0.0014
   480        0.0173             nan     0.5261   -0.0005
   500        0.0159             nan     0.5261    0.0004
   520        0.0135             nan     0.5261   -0.0010
   540        0.0125             nan     0.5261   -0.0007
   560        0.0114             nan     0.5261    0.0001
   580        0.0105             nan     0.5261   -0.0019
   600        0.0093             nan     0.5261   -0.0002
   620        0.0082             nan     0.5261   -0.0011
   640        0.0078             nan     0.5261   -0.0006
   660        0.0075             nan     0.5261   -0.0004
   680        0.0070             nan     0.5261   -0.0023
   700        0.0060             nan     0.5261   -0.0003
   720        0.0057             nan     0.5261   -0.0004
   740        0.0057             nan     0.5261   -0.0013
   760        0.0051             nan     0.5261   -0.0003
   780        0.0051             nan     0.5261   -0.0002
   800        0.0046             nan     0.5261   -0.0001
   820        0.0043             nan     0.5261   -0.0012
   840        0.0040             nan     0.5261   -0.0002
   860        0.0035             nan     0.5261   -0.0001
   880        0.0032             nan     0.5261   -0.0006
   900        0.0030             nan     0.5261   -0.0004
   920        0.0026             nan     0.5261   -0.0004
   940        0.0025             nan     0.5261   -0.0001
   960        0.0025             nan     0.5261   -0.0005
   980        0.0022             nan     0.5261   -0.0001
  1000        0.0022             nan     0.5261   -0.0003
  1020        0.0022             nan     0.5261   -0.0005
  1040        0.0018             nan     0.5261   -0.0001
  1060        0.0017             nan     0.5261   -0.0003
  1080        0.0015             nan     0.5261   -0.0000
  1100        0.0013             nan     0.5261   -0.0003
  1120        0.0014             nan     0.5261   -0.0002
  1140        0.0013             nan     0.5261   -0.0003
  1160        0.0012             nan     0.5261   -0.0001
  1180        0.0011             nan     0.5261   -0.0001
  1200        0.0011             nan     0.5261   -0.0001
  1220        0.0010             nan     0.5261   -0.0001
  1240        0.0009             nan     0.5261   -0.0001
  1260        0.0010             nan     0.5261   -0.0001
  1280        0.0009             nan     0.5261   -0.0001
  1300        0.0008             nan     0.5261   -0.0001
  1320        0.0007             nan     0.5261   -0.0001
  1340        0.0006             nan     0.5261   -0.0001
  1360        0.0006             nan     0.5261   -0.0001
  1380        0.0006             nan     0.5261   -0.0000
  1400        0.0005             nan     0.5261   -0.0001
  1420        0.0005             nan     0.5261   -0.0001
  1440        0.0005             nan     0.5261   -0.0000
  1460        0.0005             nan     0.5261   -0.0000
  1480        0.0004             nan     0.5261   -0.0000
  1500        0.0004             nan     0.5261   -0.0000
  1520        0.0004             nan     0.5261   -0.0001
  1540        0.0004             nan     0.5261   -0.0000
  1560        0.0003             nan     0.5261   -0.0001
  1580        0.0003             nan     0.5261   -0.0000
  1600        0.0003             nan     0.5261   -0.0000
  1620        0.0002             nan     0.5261   -0.0000
  1640        0.0002             nan     0.5261   -0.0000
  1660        0.0002             nan     0.5261   -0.0000
  1680        0.0002             nan     0.5261   -0.0000
  1700        0.0002             nan     0.5261   -0.0000
  1720        0.0002             nan     0.5261   -0.0000
  1740        0.0002             nan     0.5261   -0.0000
  1760        0.0002             nan     0.5261   -0.0000
  1780        0.0002             nan     0.5261   -0.0000
  1800        0.0002             nan     0.5261   -0.0000
  1820        0.0002             nan     0.5261   -0.0000
  1840        0.0001             nan     0.5261   -0.0000
  1860        0.0001             nan     0.5261   -0.0000
  1880        0.0001             nan     0.5261   -0.0000
  1900        0.0001             nan     0.5261   -0.0000
  1920        0.0001             nan     0.5261   -0.0000
  1940        0.0001             nan     0.5261   -0.0000
  1960        0.0001             nan     0.5261   -0.0000
  1980        0.0001             nan     0.5261   -0.0000
  2000        0.0001             nan     0.5261   -0.0000
  2020        0.0001             nan     0.5261   -0.0000
  2040        0.0001             nan     0.5261   -0.0000
  2060        0.0001             nan     0.5261   -0.0000
  2080        0.0001             nan     0.5261   -0.0000
  2100        0.0001             nan     0.5261   -0.0000
  2120        0.0001             nan     0.5261   -0.0000
  2140        0.0001             nan     0.5261   -0.0000
  2160        0.0001             nan     0.5261   -0.0000
  2180        0.0001             nan     0.5261   -0.0000
  2200        0.0001             nan     0.5261   -0.0000
  2220        0.0000             nan     0.5261   -0.0000
  2240        0.0000             nan     0.5261   -0.0000
  2260        0.0000             nan     0.5261   -0.0000
  2280        0.0000             nan     0.5261   -0.0000
  2300        0.0000             nan     0.5261   -0.0000
  2320        0.0000             nan     0.5261   -0.0000
  2340        0.0000             nan     0.5261   -0.0000
  2360        0.0000             nan     0.5261   -0.0000
  2380        0.0000             nan     0.5261   -0.0000
  2400        0.0000             nan     0.5261   -0.0000
  2420        0.0000             nan     0.5261   -0.0000
  2440        0.0000             nan     0.5261   -0.0000
  2460        0.0000             nan     0.5261   -0.0000
  2480        0.0000             nan     0.5261   -0.0000
  2500        0.0000             nan     0.5261   -0.0000
  2520        0.0000             nan     0.5261    0.0000
  2540        0.0000             nan     0.5261   -0.0000
  2560        0.0000             nan     0.5261   -0.0000
  2580        0.0000             nan     0.5261   -0.0000
  2600        0.0000             nan     0.5261   -0.0000
  2620        0.0000             nan     0.5261   -0.0000
  2640        0.0000             nan     0.5261   -0.0000
  2660        0.0000             nan     0.5261   -0.0000
  2680        0.0000             nan     0.5261   -0.0000
  2700        0.0000             nan     0.5261   -0.0000
  2720        0.0000             nan     0.5261   -0.0000
  2740        0.0000             nan     0.5261   -0.0000
  2760        0.0000             nan     0.5261   -0.0000
  2780        0.0000             nan     0.5261   -0.0000
  2800        0.0000             nan     0.5261   -0.0000
  2820        0.0000             nan     0.5261    0.0000
  2840        0.0000             nan     0.5261   -0.0000
  2860        0.0000             nan     0.5261   -0.0000
  2880        0.0000             nan     0.5261   -0.0000
  2900        0.0000             nan     0.5261   -0.0000
  2920        0.0000             nan     0.5261   -0.0000
  2940        0.0000             nan     0.5261   -0.0000
  2960        0.0000             nan     0.5261   -0.0000
  2980        0.0000             nan     0.5261   -0.0000
  3000        0.0000             nan     0.5261   -0.0000
  3020        0.0000             nan     0.5261   -0.0000
  3032        0.0000             nan     0.5261   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2299             nan     0.5987    0.2210
     2        0.1340             nan     0.5987    0.0084
     3        0.1280             nan     0.5987   -0.0040
     4        0.1155             nan     0.5987   -0.0025
     5        0.0946             nan     0.5987    0.0085
     6        0.0780             nan     0.5987   -0.0124
     7        0.0771             nan     0.5987   -0.0192
     8        0.0723             nan     0.5987   -0.0058
     9        0.0666             nan     0.5987   -0.0138
    10        0.0657             nan     0.5987   -0.0261
    20        0.0281             nan     0.5987   -0.0131
    40        0.0082             nan     0.5987   -0.0010
    60        0.0016             nan     0.5987   -0.0000
    80        0.0007             nan     0.5987   -0.0003
   100        0.0003             nan     0.5987   -0.0000
   120        0.0001             nan     0.5987   -0.0000
   140        0.0000             nan     0.5987   -0.0000
   160        0.0000             nan     0.5987   -0.0000
   180        0.0000             nan     0.5987   -0.0000
   200        0.0000             nan     0.5987   -0.0000
   220        0.0000             nan     0.5987   -0.0000
   240        0.0000             nan     0.5987   -0.0000
   260        0.0000             nan     0.5987   -0.0000
   280        0.0000             nan     0.5987   -0.0000
   300        0.0000             nan     0.5987   -0.0000
   320        0.0000             nan     0.5987   -0.0000
   340        0.0000             nan     0.5987   -0.0000
   360        0.0000             nan     0.5987   -0.0000
   380        0.0000             nan     0.5987   -0.0000
   400        0.0000             nan     0.5987   -0.0000
   420        0.0000             nan     0.5987   -0.0000
   440        0.0000             nan     0.5987   -0.0000
   460        0.0000             nan     0.5987   -0.0000
   480        0.0000             nan     0.5987    0.0000
   500        0.0000             nan     0.5987    0.0000
   520        0.0000             nan     0.5987   -0.0000
   540        0.0000             nan     0.5987   -0.0000
   560        0.0000             nan     0.5987    0.0000
   580        0.0000             nan     0.5987   -0.0000
   600        0.0000             nan     0.5987   -0.0000
   620        0.0000             nan     0.5987    0.0000
   640        0.0000             nan     0.5987   -0.0000
   660        0.0000             nan     0.5987   -0.0000
   680        0.0000             nan     0.5987   -0.0000
   700        0.0000             nan     0.5987   -0.0000
   720        0.0000             nan     0.5987   -0.0000
   740        0.0000             nan     0.5987    0.0000
   760        0.0000             nan     0.5987   -0.0000
   780        0.0000             nan     0.5987   -0.0000
   800        0.0000             nan     0.5987   -0.0000
   820        0.0000             nan     0.5987   -0.0000
   840        0.0000             nan     0.5987   -0.0000
   860        0.0000             nan     0.5987   -0.0000
   880        0.0000             nan     0.5987   -0.0000
   900        0.0000             nan     0.5987   -0.0000
   920        0.0000             nan     0.5987   -0.0000
   940        0.0000             nan     0.5987    0.0000
   960        0.0000             nan     0.5987   -0.0000
   980        0.0000             nan     0.5987   -0.0000
  1000        0.0000             nan     0.5987   -0.0000
  1020        0.0000             nan     0.5987   -0.0000
  1040        0.0000             nan     0.5987   -0.0000
  1060        0.0000             nan     0.5987   -0.0000
  1080        0.0000             nan     0.5987   -0.0000
  1100        0.0000             nan     0.5987    0.0000
  1120        0.0000             nan     0.5987   -0.0000
  1140        0.0000             nan     0.5987   -0.0000
  1160        0.0000             nan     0.5987   -0.0000
  1170        0.0000             nan     0.5987    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.8111             nan     0.0299    0.0068
     2        0.8028             nan     0.0299    0.0062
     3        0.7875             nan     0.0299    0.0113
     4        0.7719             nan     0.0299    0.0124
     5        0.7606             nan     0.0299    0.0125
     6        0.7574             nan     0.0299    0.0047
     7        0.7392             nan     0.0299    0.0088
     8        0.7274             nan     0.0299    0.0103
     9        0.7133             nan     0.0299    0.0081
    10        0.7040             nan     0.0299    0.0086
    20        0.6385             nan     0.0299   -0.0015
    40        0.5704             nan     0.0299    0.0025
    60        0.5423             nan     0.0299   -0.0047
    80        0.5138             nan     0.0299   -0.0025
   100        0.4950             nan     0.0299   -0.0006
   120        0.4690             nan     0.0299   -0.0015
   140        0.4573             nan     0.0299   -0.0041
   160        0.4425             nan     0.0299   -0.0013
   180        0.4306             nan     0.0299   -0.0032
   200        0.4189             nan     0.0299   -0.0022
   220        0.4123             nan     0.0299   -0.0023
   240        0.4031             nan     0.0299   -0.0006
   260        0.3945             nan     0.0299   -0.0008
   280        0.3902             nan     0.0299   -0.0026
   300        0.3815             nan     0.0299   -0.0034
   320        0.3736             nan     0.0299   -0.0011
   340        0.3696             nan     0.0299   -0.0032
   360        0.3640             nan     0.0299   -0.0030
   380        0.3556             nan     0.0299   -0.0021
   400        0.3518             nan     0.0299   -0.0010
   420        0.3474             nan     0.0299   -0.0025
   440        0.3420             nan     0.0299   -0.0010

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.7239             nan     0.1134    0.0757
     2        0.6664             nan     0.1134    0.0726
     3        0.6206             nan     0.1134    0.0406
     4        0.5722             nan     0.1134    0.0517
     5        0.5447             nan     0.1134    0.0350
     6        0.4896             nan     0.1134    0.0157
     7        0.4759             nan     0.1134    0.0190
     8        0.4469             nan     0.1134    0.0161
     9        0.4296             nan     0.1134    0.0201
    10        0.4174             nan     0.1134    0.0125
    20        0.2985             nan     0.1134   -0.0093
    40        0.2163             nan     0.1134    0.0022
    60        0.1628             nan     0.1134   -0.0045
    80        0.1256             nan     0.1134   -0.0037
   100        0.0900             nan     0.1134   -0.0040
   120        0.0657             nan     0.1134   -0.0012
   140        0.0539             nan     0.1134   -0.0018
   160        0.0443             nan     0.1134   -0.0010
   180        0.0396             nan     0.1134   -0.0005
   200        0.0287             nan     0.1134   -0.0004
   220        0.0232             nan     0.1134   -0.0009
   240        0.0206             nan     0.1134   -0.0004
   260        0.0162             nan     0.1134   -0.0009
   280        0.0122             nan     0.1134   -0.0002
   300        0.0102             nan     0.1134   -0.0005
   320        0.0088             nan     0.1134   -0.0001
   340        0.0079             nan     0.1134   -0.0002
   360        0.0067             nan     0.1134   -0.0003
   380        0.0046             nan     0.1134   -0.0001
   400        0.0037             nan     0.1134   -0.0001
   420        0.0029             nan     0.1134   -0.0001
   440        0.0022             nan     0.1134   -0.0001
   460        0.0018             nan     0.1134   -0.0001
   480        0.0014             nan     0.1134   -0.0000
   500        0.0012             nan     0.1134   -0.0000
   520        0.0009             nan     0.1134   -0.0000
   540        0.0008             nan     0.1134   -0.0000
   560        0.0006             nan     0.1134   -0.0000
   580        0.0006             nan     0.1134   -0.0000
   600        0.0005             nan     0.1134   -0.0000
   620        0.0004             nan     0.1134   -0.0000
   640        0.0003             nan     0.1134   -0.0000
   660        0.0002             nan     0.1134   -0.0000
   680        0.0002             nan     0.1134   -0.0000
   700        0.0002             nan     0.1134   -0.0000
   720        0.0001             nan     0.1134   -0.0000
   740        0.0001             nan     0.1134   -0.0000
   760        0.0001             nan     0.1134   -0.0000
   780        0.0001             nan     0.1134    0.0000
   800        0.0001             nan     0.1134   -0.0000
   820        0.0001             nan     0.1134    0.0000
   840        0.0001             nan     0.1134   -0.0000
   860        0.0000             nan     0.1134   -0.0000
   880        0.0000             nan     0.1134   -0.0000
   900        0.0000             nan     0.1134   -0.0000
   920        0.0000             nan     0.1134   -0.0000
   940        0.0000             nan     0.1134   -0.0000
   960        0.0000             nan     0.1134   -0.0000
   980        0.0000             nan     0.1134   -0.0000
  1000        0.0000             nan     0.1134   -0.0000
  1020        0.0000             nan     0.1134   -0.0000
  1040        0.0000             nan     0.1134   -0.0000
  1060        0.0000             nan     0.1134   -0.0000
  1080        0.0000             nan     0.1134   -0.0000
  1100        0.0000             nan     0.1134   -0.0000
  1120        0.0000             nan     0.1134    0.0000
  1140        0.0000             nan     0.1134   -0.0000
  1160        0.0000             nan     0.1134   -0.0000
  1180        0.0000             nan     0.1134   -0.0000
  1200        0.0000             nan     0.1134   -0.0000
  1220        0.0000             nan     0.1134   -0.0000
  1240        0.0000             nan     0.1134   -0.0000
  1260        0.0000             nan     0.1134    0.0000
  1280        0.0000             nan     0.1134   -0.0000
  1300        0.0000             nan     0.1134   -0.0000
  1320        0.0000             nan     0.1134   -0.0000
  1340        0.0000             nan     0.1134   -0.0000
  1360        0.0000             nan     0.1134   -0.0000
  1380        0.0000             nan     0.1134   -0.0000
  1400        0.0000             nan     0.1134   -0.0000
  1420        0.0000             nan     0.1134    0.0000
  1440        0.0000             nan     0.1134   -0.0000
  1460        0.0000             nan     0.1134   -0.0000
  1480        0.0000             nan     0.1134   -0.0000
  1500        0.0000             nan     0.1134   -0.0000
  1520        0.0000             nan     0.1134   -0.0000
  1540        0.0000             nan     0.1134   -0.0000
  1560        0.0000             nan     0.1134   -0.0000
  1580        0.0000             nan     0.1134   -0.0000
  1600        0.0000             nan     0.1134   -0.0000
  1620        0.0000             nan     0.1134   -0.0000
  1640        0.0000             nan     0.1134   -0.0000
  1660        0.0000             nan     0.1134   -0.0000
  1680        0.0000             nan     0.1134   -0.0000
  1700        0.0000             nan     0.1134   -0.0000
  1720        0.0000             nan     0.1134   -0.0000
  1740        0.0000             nan     0.1134   -0.0000
  1760        0.0000             nan     0.1134   -0.0000
  1780        0.0000             nan     0.1134   -0.0000
  1800        0.0000             nan     0.1134    0.0000
  1820        0.0000             nan     0.1134   -0.0000
  1840        0.0000             nan     0.1134   -0.0000
  1860        0.0000             nan     0.1134   -0.0000
  1880        0.0000             nan     0.1134   -0.0000
  1900        0.0000             nan     0.1134   -0.0000
  1920        0.0000             nan     0.1134   -0.0000
  1940        0.0000             nan     0.1134   -0.0000
  1960        0.0000             nan     0.1134   -0.0000
  1980        0.0000             nan     0.1134   -0.0000
  2000        0.0000             nan     0.1134   -0.0000
  2020        0.0000             nan     0.1134   -0.0000
  2040        0.0000             nan     0.1134   -0.0000
  2060        0.0000             nan     0.1134   -0.0000
  2080        0.0000             nan     0.1134   -0.0000
  2100        0.0000             nan     0.1134   -0.0000
  2120        0.0000             nan     0.1134   -0.0000
  2140        0.0000             nan     0.1134   -0.0000
  2160        0.0000             nan     0.1134   -0.0000
  2180        0.0000             nan     0.1134   -0.0000
  2200        0.0000             nan     0.1134   -0.0000
  2220        0.0000             nan     0.1134   -0.0000
  2240        0.0000             nan     0.1134   -0.0000
  2260        0.0000             nan     0.1134   -0.0000
  2280        0.0000             nan     0.1134   -0.0000
  2300        0.0000             nan     0.1134   -0.0000
  2320        0.0000             nan     0.1134   -0.0000
  2340        0.0000             nan     0.1134   -0.0000
  2360        0.0000             nan     0.1134   -0.0000
  2380        0.0000             nan     0.1134   -0.0000
  2400        0.0000             nan     0.1134   -0.0000
  2420        0.0000             nan     0.1134   -0.0000
  2440        0.0000             nan     0.1134   -0.0000
  2460        0.0000             nan     0.1134   -0.0000
  2480        0.0000             nan     0.1134    0.0000
  2500        0.0000             nan     0.1134   -0.0000
  2520        0.0000             nan     0.1134   -0.0000
  2540        0.0000             nan     0.1134   -0.0000
  2560        0.0000             nan     0.1134   -0.0000
  2580        0.0000             nan     0.1134   -0.0000
  2600        0.0000             nan     0.1134   -0.0000
  2620        0.0000             nan     0.1134   -0.0000
  2640        0.0000             nan     0.1134   -0.0000
  2660        0.0000             nan     0.1134   -0.0000
  2680        0.0000             nan     0.1134   -0.0000
  2700        0.0000             nan     0.1134   -0.0000
  2720        0.0000             nan     0.1134   -0.0000
  2740        0.0000             nan     0.1134   -0.0000
  2760        0.0000             nan     0.1134   -0.0000
  2780        0.0000             nan     0.1134   -0.0000
  2800        0.0000             nan     0.1134   -0.0000
  2820        0.0000             nan     0.1134   -0.0000
  2840        0.0000             nan     0.1134   -0.0000
  2860        0.0000             nan     0.1134   -0.0000
  2880        0.0000             nan     0.1134   -0.0000
  2900        0.0000             nan     0.1134   -0.0000
  2920        0.0000             nan     0.1134   -0.0000
  2940        0.0000             nan     0.1134   -0.0000
  2960        0.0000             nan     0.1134   -0.0000
  2980        0.0000             nan     0.1134   -0.0000
  3000        0.0000             nan     0.1134   -0.0000
  3020        0.0000             nan     0.1134   -0.0000
  3040        0.0000             nan     0.1134   -0.0000
  3060        0.0000             nan     0.1134   -0.0000
  3080        0.0000             nan     0.1134   -0.0000
  3100        0.0000             nan     0.1134   -0.0000
  3120        0.0000             nan     0.1134   -0.0000
  3140        0.0000             nan     0.1134   -0.0000
  3160        0.0000             nan     0.1134   -0.0000
  3180        0.0000             nan     0.1134   -0.0000
  3200        0.0000             nan     0.1134   -0.0000
  3220        0.0000             nan     0.1134   -0.0000
  3240        0.0000             nan     0.1134   -0.0000
  3260        0.0000             nan     0.1134   -0.0000
  3280        0.0000             nan     0.1134   -0.0000
  3300        0.0000             nan     0.1134   -0.0000
  3320        0.0000             nan     0.1134   -0.0000
  3340        0.0000             nan     0.1134   -0.0000
  3360        0.0000             nan     0.1134   -0.0000
  3380        0.0000             nan     0.1134   -0.0000
  3400        0.0000             nan     0.1134   -0.0000
  3420        0.0000             nan     0.1134   -0.0000
  3440        0.0000             nan     0.1134   -0.0000
  3460        0.0000             nan     0.1134   -0.0000
  3480        0.0000             nan     0.1134   -0.0000
  3500        0.0000             nan     0.1134   -0.0000
  3520        0.0000             nan     0.1134   -0.0000
  3540        0.0000             nan     0.1134   -0.0000
  3560        0.0000             nan     0.1134   -0.0000
  3572        0.0000             nan     0.1134    0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.7230             nan     0.1520    0.0888
     2        0.6699             nan     0.1520    0.0652
     3        0.5924             nan     0.1520    0.0214
     4        0.5522             nan     0.1520    0.0238
     5        0.5468             nan     0.1520    0.0063
     6        0.5260             nan     0.1520    0.0201
     7        0.4923             nan     0.1520    0.0160
     8        0.4804             nan     0.1520    0.0021
     9        0.4815             nan     0.1520   -0.0065
    10        0.4699             nan     0.1520   -0.0021
    20        0.3408             nan     0.1520   -0.0096
    40        0.2807             nan     0.1520    0.0024
    60        0.2165             nan     0.1520   -0.0027
    80        0.1822             nan     0.1520   -0.0035
   100        0.1577             nan     0.1520   -0.0051
   120        0.1259             nan     0.1520   -0.0032
   140        0.1086             nan     0.1520   -0.0018
   160        0.0928             nan     0.1520   -0.0100
   180        0.0781             nan     0.1520   -0.0051
   200        0.0634             nan     0.1520   -0.0026
   220        0.0553             nan     0.1520   -0.0011
   240        0.0475             nan     0.1520   -0.0013
   260        0.0443             nan     0.1520   -0.0026
   280        0.0381             nan     0.1520   -0.0016
   300        0.0327             nan     0.1520   -0.0000
   320        0.0272             nan     0.1520   -0.0009
   340        0.0275             nan     0.1520   -0.0007
   360        0.0256             nan     0.1520    0.0000
   380        0.0226             nan     0.1520   -0.0004
   400        0.0194             nan     0.1520   -0.0007
   420        0.0161             nan     0.1520   -0.0009
   440        0.0143             nan     0.1520   -0.0005
   460        0.0124             nan     0.1520   -0.0003
   480        0.0110             nan     0.1520   -0.0011
   500        0.0104             nan     0.1520   -0.0002
   520        0.0091             nan     0.1520   -0.0002
   540        0.0079             nan     0.1520   -0.0003
   560        0.0071             nan     0.1520   -0.0003
   580        0.0064             nan     0.1520   -0.0001
   600        0.0055             nan     0.1520   -0.0002
   620        0.0052             nan     0.1520   -0.0002
   640        0.0047             nan     0.1520   -0.0002
   660        0.0039             nan     0.1520   -0.0001
   680        0.0032             nan     0.1520   -0.0001
   700        0.0030             nan     0.1520   -0.0000
   720        0.0025             nan     0.1520   -0.0001
   740        0.0021             nan     0.1520   -0.0001
   760        0.0018             nan     0.1520   -0.0001
   780        0.0015             nan     0.1520   -0.0000
   800        0.0013             nan     0.1520   -0.0000
   820        0.0012             nan     0.1520   -0.0000
   840        0.0010             nan     0.1520   -0.0000
   860        0.0009             nan     0.1520   -0.0000
   880        0.0008             nan     0.1520   -0.0001
   900        0.0007             nan     0.1520   -0.0000
   920        0.0006             nan     0.1520   -0.0000
   940        0.0005             nan     0.1520   -0.0000
   960        0.0005             nan     0.1520   -0.0000
   980        0.0004             nan     0.1520   -0.0000
  1000        0.0004             nan     0.1520   -0.0000
  1020        0.0004             nan     0.1520   -0.0000
  1040        0.0003             nan     0.1520   -0.0000
  1060        0.0003             nan     0.1520   -0.0000
  1080        0.0002             nan     0.1520   -0.0000
  1100        0.0002             nan     0.1520   -0.0000
  1120        0.0002             nan     0.1520   -0.0000
  1140        0.0002             nan     0.1520   -0.0000
  1160        0.0001             nan     0.1520   -0.0000
  1180        0.0001             nan     0.1520   -0.0000
  1200        0.0001             nan     0.1520   -0.0000
  1220        0.0001             nan     0.1520   -0.0000
  1240        0.0001             nan     0.1520   -0.0000
  1254        0.0001             nan     0.1520   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5868             nan     0.3279    0.0578
     2        0.5156             nan     0.3279    0.0749
     3        0.4491             nan     0.3279    0.0738
     4        0.4171             nan     0.3279    0.0299
     5        0.3942             nan     0.3279    0.0310
     6        0.3530             nan     0.3279    0.0131
     7        0.3405             nan     0.3279    0.0117
     8        0.3344             nan     0.3279   -0.0219
     9        0.3266             nan     0.3279    0.0042
    10        0.3248             nan     0.3279   -0.0045
    20        0.2245             nan     0.3279   -0.0236
    40        0.1496             nan     0.3279   -0.0086
    60        0.1028             nan     0.3279   -0.0033
    80        0.0866             nan     0.3279   -0.0073
   100        0.0782             nan     0.3279   -0.0067
   120        0.0594             nan     0.3279   -0.0026
   140        0.0466             nan     0.3279   -0.0053
   160        0.0374             nan     0.3279   -0.0014
   180        0.0283             nan     0.3279   -0.0012
   200        0.0235             nan     0.3279   -0.0010
   220        0.0205             nan     0.3279   -0.0013
   240        0.0169             nan     0.3279    0.0001
   260        0.0148             nan     0.3279   -0.0012
   280        0.0113             nan     0.3279   -0.0008
   300        0.0094             nan     0.3279   -0.0016
   320        0.0079             nan     0.3279   -0.0007
   340        0.0072             nan     0.3279   -0.0003
   360        0.0055             nan     0.3279   -0.0001
   380        0.0042             nan     0.3279   -0.0004
   400        0.0036             nan     0.3279   -0.0002
   420        0.0033             nan     0.3279   -0.0002
   440        0.0026             nan     0.3279   -0.0000
   460        0.0024             nan     0.3279   -0.0001
   480        0.0021             nan     0.3279   -0.0002
   500        0.0016             nan     0.3279   -0.0002
   520        0.0013             nan     0.3279   -0.0001
   540        0.0010             nan     0.3279   -0.0001
   560        0.0009             nan     0.3279   -0.0001
   580        0.0008             nan     0.3279   -0.0000
   600        0.0006             nan     0.3279   -0.0001
   620        0.0005             nan     0.3279   -0.0000
   640        0.0004             nan     0.3279   -0.0000
   660        0.0004             nan     0.3279   -0.0000
   680        0.0003             nan     0.3279   -0.0000
   700        0.0003             nan     0.3279   -0.0000
   720        0.0002             nan     0.3279   -0.0000
   740        0.0002             nan     0.3279   -0.0000
   760        0.0001             nan     0.3279   -0.0000
   780        0.0001             nan     0.3279   -0.0000
   800        0.0001             nan     0.3279   -0.0000
   820        0.0001             nan     0.3279   -0.0000
   840        0.0001             nan     0.3279   -0.0000
   860        0.0001             nan     0.3279   -0.0000
   880        0.0000             nan     0.3279   -0.0000
   900        0.0000             nan     0.3279   -0.0000
   920        0.0000             nan     0.3279   -0.0000
   940        0.0000             nan     0.3279    0.0000
   960        0.0000             nan     0.3279   -0.0000
   980        0.0000             nan     0.3279   -0.0000
  1000        0.0000             nan     0.3279   -0.0000
  1020        0.0000             nan     0.3279   -0.0000
  1040        0.0000             nan     0.3279   -0.0000
  1060        0.0000             nan     0.3279   -0.0000
  1080        0.0000             nan     0.3279   -0.0000
  1100        0.0000             nan     0.3279   -0.0000
  1120        0.0000             nan     0.3279   -0.0000
  1140        0.0000             nan     0.3279   -0.0000
  1160        0.0000             nan     0.3279   -0.0000
  1180        0.0000             nan     0.3279   -0.0000
  1200        0.0000             nan     0.3279   -0.0000
  1220        0.0000             nan     0.3279   -0.0000
  1240        0.0000             nan     0.3279   -0.0000
  1260        0.0000             nan     0.3279   -0.0000
  1280        0.0000             nan     0.3279   -0.0000
  1300        0.0000             nan     0.3279   -0.0000
  1320        0.0000             nan     0.3279   -0.0000
  1340        0.0000             nan     0.3279   -0.0000
  1360        0.0000             nan     0.3279   -0.0000
  1380        0.0000             nan     0.3279   -0.0000
  1400        0.0000             nan     0.3279   -0.0000
  1420        0.0000             nan     0.3279   -0.0000
  1440        0.0000             nan     0.3279   -0.0000
  1460        0.0000             nan     0.3279   -0.0000
  1480        0.0000             nan     0.3279   -0.0000
  1500        0.0000             nan     0.3279   -0.0000
  1520        0.0000             nan     0.3279   -0.0000
  1540        0.0000             nan     0.3279   -0.0000
  1560        0.0000             nan     0.3279   -0.0000
  1580        0.0000             nan     0.3279   -0.0000
  1600        0.0000             nan     0.3279   -0.0000
  1620        0.0000             nan     0.3279   -0.0000
  1640        0.0000             nan     0.3279   -0.0000
  1660        0.0000             nan     0.3279   -0.0000
  1680        0.0000             nan     0.3279   -0.0000
  1700        0.0000             nan     0.3279   -0.0000
  1720        0.0000             nan     0.3279   -0.0000
  1740        0.0000             nan     0.3279   -0.0000
  1760        0.0000             nan     0.3279   -0.0000
  1780        0.0000             nan     0.3279   -0.0000
  1800        0.0000             nan     0.3279   -0.0000
  1820        0.0000             nan     0.3279   -0.0000
  1840        0.0000             nan     0.3279   -0.0000
  1860        0.0000             nan     0.3279   -0.0000
  1880        0.0000             nan     0.3279   -0.0000
  1900        0.0000             nan     0.3279   -0.0000
  1920        0.0000             nan     0.3279   -0.0000
  1940        0.0000             nan     0.3279   -0.0000
  1960        0.0000             nan     0.3279   -0.0000
  1980        0.0000             nan     0.3279   -0.0000
  2000        0.0000             nan     0.3279   -0.0000
  2020        0.0000             nan     0.3279   -0.0000
  2040        0.0000             nan     0.3279   -0.0000
  2060        0.0000             nan     0.3279   -0.0000
  2080        0.0000             nan     0.3279   -0.0000
  2100        0.0000             nan     0.3279   -0.0000
  2120        0.0000             nan     0.3279   -0.0000
  2140        0.0000             nan     0.3279   -0.0000
  2160        0.0000             nan     0.3279   -0.0000
  2180        0.0000             nan     0.3279   -0.0000
  2200        0.0000             nan     0.3279   -0.0000
  2220        0.0000             nan     0.3279   -0.0000
  2240        0.0000             nan     0.3279   -0.0000
  2260        0.0000             nan     0.3279   -0.0000
  2280        0.0000             nan     0.3279   -0.0000
  2300        0.0000             nan     0.3279   -0.0000
  2320        0.0000             nan     0.3279   -0.0000
  2340        0.0000             nan     0.3279   -0.0000
  2360        0.0000             nan     0.3279   -0.0000
  2380        0.0000             nan     0.3279   -0.0000
  2400        0.0000             nan     0.3279   -0.0000
  2420        0.0000             nan     0.3279   -0.0000
  2440        0.0000             nan     0.3279   -0.0000
  2460        0.0000             nan     0.3279   -0.0000
  2480        0.0000             nan     0.3279   -0.0000
  2500        0.0000             nan     0.3279   -0.0000
  2520        0.0000             nan     0.3279   -0.0000
  2540        0.0000             nan     0.3279   -0.0000
  2560        0.0000             nan     0.3279   -0.0000
  2580        0.0000             nan     0.3279   -0.0000
  2600        0.0000             nan     0.3279   -0.0000
  2620        0.0000             nan     0.3279   -0.0000
  2640        0.0000             nan     0.3279    0.0000
  2660        0.0000             nan     0.3279   -0.0000
  2680        0.0000             nan     0.3279   -0.0000
  2700        0.0000             nan     0.3279   -0.0000
  2720        0.0000             nan     0.3279   -0.0000
  2740        0.0000             nan     0.3279   -0.0000
  2760        0.0000             nan     0.3279   -0.0000
  2780        0.0000             nan     0.3279   -0.0000
  2800        0.0000             nan     0.3279   -0.0000
  2820        0.0000             nan     0.3279   -0.0000
  2840        0.0000             nan     0.3279   -0.0000
  2860        0.0000             nan     0.3279   -0.0000
  2880        0.0000             nan     0.3279   -0.0000
  2900        0.0000             nan     0.3279   -0.0000
  2920        0.0000             nan     0.3279   -0.0000
  2940        0.0000             nan     0.3279   -0.0000
  2960        0.0000             nan     0.3279   -0.0000
  2980        0.0000             nan     0.3279   -0.0000
  3000        0.0000             nan     0.3279   -0.0000
  3020        0.0000             nan     0.3279   -0.0000
  3040        0.0000             nan     0.3279   -0.0000
  3060        0.0000             nan     0.3279   -0.0000
  3080        0.0000             nan     0.3279   -0.0000
  3100        0.0000             nan     0.3279   -0.0000
  3120        0.0000             nan     0.3279   -0.0000
  3140        0.0000             nan     0.3279   -0.0000
  3160        0.0000             nan     0.3279   -0.0000
  3180        0.0000             nan     0.3279   -0.0000
  3200        0.0000             nan     0.3279   -0.0000
  3220        0.0000             nan     0.3279   -0.0000
  3240        0.0000             nan     0.3279   -0.0000
  3260        0.0000             nan     0.3279   -0.0000
  3280        0.0000             nan     0.3279   -0.0000
  3300        0.0000             nan     0.3279   -0.0000
  3320        0.0000             nan     0.3279   -0.0000
  3340        0.0000             nan     0.3279   -0.0000
  3360        0.0000             nan     0.3279   -0.0000
  3380        0.0000             nan     0.3279   -0.0000
  3400        0.0000             nan     0.3279   -0.0000
  3420        0.0000             nan     0.3279   -0.0000
  3440        0.0000             nan     0.3279   -0.0000
  3460        0.0000             nan     0.3279   -0.0000
  3480        0.0000             nan     0.3279   -0.0000
  3500        0.0000             nan     0.3279   -0.0000
  3520        0.0000             nan     0.3279   -0.0000
  3540        0.0000             nan     0.3279    0.0000
  3560        0.0000             nan     0.3279   -0.0000
  3580        0.0000             nan     0.3279   -0.0000
  3600        0.0000             nan     0.3279   -0.0000
  3620        0.0000             nan     0.3279   -0.0000
  3640        0.0000             nan     0.3279   -0.0000
  3660        0.0000             nan     0.3279   -0.0000
  3680        0.0000             nan     0.3279   -0.0000
  3700        0.0000             nan     0.3279   -0.0000
  3720        0.0000             nan     0.3279   -0.0000
  3740        0.0000             nan     0.3279   -0.0000
  3760        0.0000             nan     0.3279   -0.0000
  3780        0.0000             nan     0.3279   -0.0000
  3800        0.0000             nan     0.3279   -0.0000
  3820        0.0000             nan     0.3279   -0.0000
  3840        0.0000             nan     0.3279   -0.0000
  3860        0.0000             nan     0.3279   -0.0000
  3880        0.0000             nan     0.3279    0.0000
  3900        0.0000             nan     0.3279   -0.0000
  3920        0.0000             nan     0.3279   -0.0000
  3940        0.0000             nan     0.3279   -0.0000
  3960        0.0000             nan     0.3279   -0.0000
  3980        0.0000             nan     0.3279   -0.0000
  4000        0.0000             nan     0.3279   -0.0000
  4020        0.0000             nan     0.3279   -0.0000
  4040        0.0000             nan     0.3279   -0.0000
  4060        0.0000             nan     0.3279   -0.0000
  4080        0.0000             nan     0.3279   -0.0000
  4100        0.0000             nan     0.3279   -0.0000
  4120        0.0000             nan     0.3279   -0.0000
  4140        0.0000             nan     0.3279   -0.0000
  4160        0.0000             nan     0.3279   -0.0000
  4180        0.0000             nan     0.3279   -0.0000
  4200        0.0000             nan     0.3279   -0.0000
  4220        0.0000             nan     0.3279    0.0000
  4240        0.0000             nan     0.3279   -0.0000
  4260        0.0000             nan     0.3279   -0.0000
  4280        0.0000             nan     0.3279   -0.0000
  4300        0.0000             nan     0.3279   -0.0000
  4320        0.0000             nan     0.3279   -0.0000
  4340        0.0000             nan     0.3279   -0.0000
  4360        0.0000             nan     0.3279   -0.0000
  4380        0.0000             nan     0.3279   -0.0000
  4400        0.0000             nan     0.3279    0.0000
  4420        0.0000             nan     0.3279   -0.0000
  4440        0.0000             nan     0.3279   -0.0000
  4460        0.0000             nan     0.3279   -0.0000
  4480        0.0000             nan     0.3279    0.0000
  4500        0.0000             nan     0.3279   -0.0000
  4520        0.0000             nan     0.3279   -0.0000
  4540        0.0000             nan     0.3279   -0.0000
  4560        0.0000             nan     0.3279   -0.0000
  4580        0.0000             nan     0.3279   -0.0000
  4600        0.0000             nan     0.3279   -0.0000
  4620        0.0000             nan     0.3279   -0.0000
  4640        0.0000             nan     0.3279   -0.0000
  4660        0.0000             nan     0.3279   -0.0000
  4665        0.0000             nan     0.3279   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.6562             nan     0.4488    0.1556
     2        0.6079             nan     0.4488    0.0500
     3        0.5346             nan     0.4488    0.0806
     4        0.5392             nan     0.4488   -0.0556
     5        0.4921             nan     0.4488    0.0301
     6        0.5110             nan     0.4488   -0.0522
     7        0.5115             nan     0.4488   -0.0139
     8        0.5214             nan     0.4488   -0.0462
     9        0.4873             nan     0.4488   -0.0729
    10        0.4785             nan     0.4488   -0.0169
    20        0.3674             nan     0.4488   -0.0173
    40        0.3154             nan     0.4488   -0.0171
    60        0.2451             nan     0.4488   -0.0246
    80        0.1999             nan     0.4488   -0.0083
   100        0.1456             nan     0.4488   -0.0187
   120        0.1481             nan     0.4488   -0.0206
   140        0.1102             nan     0.4488    0.0031
   160        0.1021             nan     0.4488    0.0014
   180        0.0945             nan     0.4488   -0.0091
   200        0.0839             nan     0.4488   -0.0067
   220        0.0721             nan     0.4488   -0.0074
   240        0.0605             nan     0.4488   -0.0140
   260        0.0538             nan     0.4488   -0.0040
   280        0.0440             nan     0.4488   -0.0014
   300        0.0352             nan     0.4488   -0.0019
   320        0.0291             nan     0.4488   -0.0026
   340        0.0251             nan     0.4488   -0.0018
   360        0.0194             nan     0.4488   -0.0011
   380        0.0165             nan     0.4488   -0.0029
   400        0.0131             nan     0.4488   -0.0023
   420        0.0109             nan     0.4488   -0.0001
   440        0.0096             nan     0.4488   -0.0005
   460        0.0088             nan     0.4488   -0.0001
   480        0.0082             nan     0.4488   -0.0006
   500        0.0067             nan     0.4488   -0.0007
   520        0.0061             nan     0.4488   -0.0011
   540        0.0054             nan     0.4488   -0.0003
   560        0.0047             nan     0.4488   -0.0003
   580        0.0037             nan     0.4488   -0.0001
   600        0.0035             nan     0.4488   -0.0007
   620        0.0028             nan     0.4488   -0.0002
   640        0.0026             nan     0.4488   -0.0001
   660        0.0022             nan     0.4488   -0.0002
   680        0.0019             nan     0.4488   -0.0002
   700        0.0015             nan     0.4488   -0.0000
   720        0.0014             nan     0.4488   -0.0000
   740        0.0012             nan     0.4488   -0.0001
   760        0.0010             nan     0.4488   -0.0001
   780        0.0009             nan     0.4488   -0.0002
   800        0.0008             nan     0.4488   -0.0001
   820        0.0007             nan     0.4488   -0.0002
   840        0.0007             nan     0.4488   -0.0000
   860        0.0006             nan     0.4488   -0.0000
   880        0.0005             nan     0.4488   -0.0000
   900        0.0005             nan     0.4488   -0.0001
   920        0.0004             nan     0.4488   -0.0000
   940        0.0003             nan     0.4488   -0.0000
   960        0.0003             nan     0.4488   -0.0000
   980        0.0002             nan     0.4488   -0.0000
  1000        0.0002             nan     0.4488   -0.0000
  1020        0.0002             nan     0.4488   -0.0001
  1040        0.0002             nan     0.4488   -0.0000
  1060        0.0001             nan     0.4488   -0.0000
  1080        0.0001             nan     0.4488   -0.0000
  1100        0.0001             nan     0.4488   -0.0000
  1120        0.0001             nan     0.4488   -0.0000
  1140        0.0001             nan     0.4488   -0.0000
  1160        0.0001             nan     0.4488   -0.0000
  1180        0.0001             nan     0.4488   -0.0000
  1200        0.0001             nan     0.4488   -0.0000
  1220        0.0000             nan     0.4488   -0.0000
  1240        0.0000             nan     0.4488   -0.0000
  1260        0.0000             nan     0.4488   -0.0000
  1280        0.0000             nan     0.4488    0.0000
  1300        0.0000             nan     0.4488   -0.0000
  1320        0.0000             nan     0.4488   -0.0000
  1340        0.0000             nan     0.4488   -0.0000
  1360        0.0000             nan     0.4488   -0.0000
  1380        0.0000             nan     0.4488   -0.0000
  1400        0.0000             nan     0.4488   -0.0000
  1420        0.0000             nan     0.4488   -0.0000
  1431        0.0000             nan     0.4488   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.6426             nan     0.5231    0.1873
     2        0.5720             nan     0.5231    0.0339
     3        0.4820             nan     0.5231    0.0031
     4        0.4753             nan     0.5231   -0.0135
     5        0.4501             nan     0.5231   -0.0107
     6        0.4604             nan     0.5231   -0.0585
     7        0.4516             nan     0.5231   -0.0012
     8        0.4272             nan     0.5231   -0.0179
     9        0.4273             nan     0.5231   -0.0196
    10        0.4311             nan     0.5231   -0.0301
    20        0.4153             nan     0.5231   -0.0163
    40        0.3536             nan     0.5231   -0.0108
    60        0.2960             nan     0.5231   -0.0317
    80        0.2619             nan     0.5231   -0.0108
   100        0.1870             nan     0.5231   -0.0413
   120        0.1661             nan     0.5231   -0.0103
   140        0.1610             nan     0.5231   -0.0154
   160        0.1537             nan     0.5231   -0.0174
   180        0.1361             nan     0.5231   -0.0109
   200        0.1321             nan     0.5231   -0.0220
   220        0.1053             nan     0.5231   -0.0108
   240        0.0987             nan     0.5231   -0.0099
   260        0.0885             nan     0.5231   -0.0182
   280        0.0890             nan     0.5231   -0.0014
   300        0.0779             nan     0.5231   -0.0248
   320        0.0708             nan     0.5231   -0.0089
   340        0.0642             nan     0.5231   -0.0107
   360        0.0568             nan     0.5231   -0.0086
   380        0.0499             nan     0.5231   -0.0049
   400        0.0473             nan     0.5231   -0.0055
   420        0.0397             nan     0.5231   -0.0044
   440        0.0390             nan     0.5231   -0.0031
   460        0.0347             nan     0.5231   -0.0016
   480        0.0316             nan     0.5231   -0.0041
   500        0.0268             nan     0.5231   -0.0053
   520        0.0229             nan     0.5231   -0.0015
   540        0.0223             nan     0.5231   -0.0008
   560        0.0204             nan     0.5231   -0.0022
   580        0.0199             nan     0.5231   -0.0019
   600        0.0166             nan     0.5231   -0.0030
   620        0.0135             nan     0.5231   -0.0009
   640        0.0109             nan     0.5231   -0.0003
   660        0.0103             nan     0.5231   -0.0033
   680        0.0078             nan     0.5231   -0.0002
   700        0.0072             nan     0.5231   -0.0004
   720        0.0068             nan     0.5231    0.0003
   740        0.0057             nan     0.5231   -0.0005
   760        0.0049             nan     0.5231   -0.0004
   780        0.0044             nan     0.5231   -0.0001
   800        0.0045             nan     0.5231   -0.0008
   820        0.0038             nan     0.5231   -0.0003
   840        0.0037             nan     0.5231   -0.0002
   860        0.0036             nan     0.5231   -0.0007
   880        0.0032             nan     0.5231   -0.0004
   900        0.0031             nan     0.5231   -0.0000
   920        0.0028             nan     0.5231   -0.0009
   940        0.0022             nan     0.5231   -0.0002
   960        0.0021             nan     0.5231   -0.0003
   980        0.0019             nan     0.5231   -0.0006
  1000        0.0016             nan     0.5231   -0.0001
  1020        0.0017             nan     0.5231   -0.0001
  1040        0.0016             nan     0.5231   -0.0005
  1060        0.0012             nan     0.5231   -0.0001
  1080        0.0012             nan     0.5231   -0.0000
  1100        0.0011             nan     0.5231   -0.0002
  1120        0.0009             nan     0.5231   -0.0001
  1140        0.0008             nan     0.5231   -0.0001
  1160        0.0008             nan     0.5231   -0.0002
  1180        0.0007             nan     0.5231   -0.0001
  1200        0.0006             nan     0.5231   -0.0000
  1220        0.0007             nan     0.5231   -0.0001
  1240        0.0005             nan     0.5231   -0.0000
  1260        0.0005             nan     0.5231   -0.0000
  1280        0.0004             nan     0.5231    0.0000
  1300        0.0004             nan     0.5231   -0.0000
  1320        0.0004             nan     0.5231   -0.0000
  1340        0.0003             nan     0.5231   -0.0000
  1360        0.0003             nan     0.5231   -0.0000
  1380        0.0003             nan     0.5231   -0.0000
  1400        0.0002             nan     0.5231   -0.0000
  1420        0.0002             nan     0.5231   -0.0000
  1440        0.0002             nan     0.5231   -0.0000
  1460        0.0002             nan     0.5231   -0.0000
  1480        0.0001             nan     0.5231   -0.0000
  1500        0.0001             nan     0.5231   -0.0000
  1520        0.0001             nan     0.5231   -0.0000
  1540        0.0001             nan     0.5231   -0.0000
  1560        0.0001             nan     0.5231    0.0000
  1580        0.0001             nan     0.5231   -0.0000
  1600        0.0001             nan     0.5231   -0.0000
  1620        0.0001             nan     0.5231   -0.0000
  1640        0.0001             nan     0.5231   -0.0000
  1660        0.0001             nan     0.5231   -0.0000
  1680        0.0001             nan     0.5231   -0.0000
  1700        0.0001             nan     0.5231   -0.0000
  1720        0.0000             nan     0.5231   -0.0000
  1740        0.0000             nan     0.5231   -0.0000
  1760        0.0000             nan     0.5231   -0.0000
  1780        0.0000             nan     0.5231   -0.0000
  1800        0.0000             nan     0.5231   -0.0000
  1820        0.0000             nan     0.5231   -0.0000
  1840        0.0000             nan     0.5231   -0.0000
  1860        0.0000             nan     0.5231   -0.0000
  1880        0.0000             nan     0.5231    0.0000
  1900        0.0000             nan     0.5231   -0.0000
  1920        0.0000             nan     0.5231    0.0000
  1940        0.0000             nan     0.5231   -0.0000
  1960        0.0000             nan     0.5231   -0.0000
  1980        0.0000             nan     0.5231   -0.0000
  2000        0.0000             nan     0.5231   -0.0000
  2020        0.0000             nan     0.5231   -0.0000
  2040        0.0000             nan     0.5231   -0.0000
  2060        0.0000             nan     0.5231   -0.0000
  2080        0.0000             nan     0.5231   -0.0000
  2100        0.0000             nan     0.5231   -0.0000
  2120        0.0000             nan     0.5231   -0.0000
  2140        0.0000             nan     0.5231   -0.0000
  2160        0.0000             nan     0.5231   -0.0000
  2180        0.0000             nan     0.5231   -0.0000
  2200        0.0000             nan     0.5231   -0.0000
  2220        0.0000             nan     0.5231   -0.0000
  2240        0.0000             nan     0.5231   -0.0000
  2260        0.0000             nan     0.5231   -0.0000
  2280        0.0000             nan     0.5231   -0.0000
  2300        0.0000             nan     0.5231   -0.0000
  2320        0.0000             nan     0.5231    0.0000
  2340        0.0000             nan     0.5231   -0.0000
  2360        0.0000             nan     0.5231   -0.0000
  2380        0.0000             nan     0.5231    0.0000
  2400        0.0000             nan     0.5231   -0.0000
  2420        0.0000             nan     0.5231   -0.0000
  2440        0.0000             nan     0.5231   -0.0000
  2460        0.0000             nan     0.5231   -0.0000
  2480        0.0000             nan     0.5231   -0.0000
  2500        0.0000             nan     0.5231   -0.0000
  2520        0.0000             nan     0.5231   -0.0000
  2540        0.0000             nan     0.5231   -0.0000
  2560        0.0000             nan     0.5231   -0.0000
  2580        0.0000             nan     0.5231   -0.0000
  2600        0.0000             nan     0.5231   -0.0000
  2620        0.0000             nan     0.5231   -0.0000
  2640        0.0000             nan     0.5231   -0.0000
  2660        0.0000             nan     0.5231   -0.0000
  2680        0.0000             nan     0.5231   -0.0000
  2700        0.0000             nan     0.5231   -0.0000
  2720        0.0000             nan     0.5231   -0.0000
  2740        0.0000             nan     0.5231   -0.0000
  2760        0.0000             nan     0.5231   -0.0000
  2780        0.0000             nan     0.5231   -0.0000
  2800        0.0000             nan     0.5231   -0.0000
  2820        0.0000             nan     0.5231   -0.0000
  2840        0.0000             nan     0.5231   -0.0000
  2860        0.0000             nan     0.5231   -0.0000
  2880        0.0000             nan     0.5231   -0.0000
  2900        0.0000             nan     0.5231   -0.0000
  2920        0.0000             nan     0.5231   -0.0000
  2940        0.0000             nan     0.5231   -0.0000
  2960        0.0000             nan     0.5231   -0.0000
  2980        0.0000             nan     0.5231   -0.0000
  3000        0.0000             nan     0.5231   -0.0000
  3020        0.0000             nan     0.5231   -0.0000
  3040        0.0000             nan     0.5231   -0.0000
  3060        0.0000             nan     0.5231   -0.0000
  3080        0.0000             nan     0.5231   -0.0000
  3100        0.0000             nan     0.5231   -0.0000
  3120        0.0000             nan     0.5231   -0.0000
  3140        0.0000             nan     0.5231   -0.0000
  3160        0.0000             nan     0.5231   -0.0000
  3180        0.0000             nan     0.5231   -0.0000
  3200        0.0000             nan     0.5231   -0.0000
  3220        0.0000             nan     0.5231   -0.0000
  3240        0.0000             nan     0.5231   -0.0000
  3260        0.0000             nan     0.5231   -0.0000
  3280        0.0000             nan     0.5231   -0.0000
  3300        0.0000             nan     0.5231   -0.0000
  3320        0.0000             nan     0.5231   -0.0000
  3340        0.0000             nan     0.5231   -0.0000
  3360        0.0000             nan     0.5231   -0.0000
  3380        0.0000             nan     0.5231   -0.0000
  3400        0.0000             nan     0.5231   -0.0000
  3420        0.0000             nan     0.5231   -0.0000
  3440        0.0000             nan     0.5231   -0.0000
  3460        0.0000             nan     0.5231    0.0000
  3480        0.0000             nan     0.5231   -0.0000
  3500        0.0000             nan     0.5231   -0.0000
  3520        0.0000             nan     0.5231   -0.0000
  3540        0.0000             nan     0.5231   -0.0000
  3560        0.0000             nan     0.5231   -0.0000
  3580        0.0000             nan     0.5231   -0.0000
  3600        0.0000             nan     0.5231   -0.0000
  3620        0.0000             nan     0.5231   -0.0000
  3640        0.0000             nan     0.5231   -0.0000
  3660        0.0000             nan     0.5231   -0.0000
  3680        0.0000             nan     0.5231   -0.0000
  3700        0.0000             nan     0.5231   -0.0000
  3720        0.0000             nan     0.5231   -0.0000
  3740        0.0000             nan     0.5231   -0.0000
  3760        0.0000             nan     0.5231   -0.0000
  3780        0.0000             nan     0.5231   -0.0000
  3800        0.0000             nan     0.5231   -0.0000
  3820        0.0000             nan     0.5231   -0.0000
  3840        0.0000             nan     0.5231   -0.0000
  3860        0.0000             nan     0.5231   -0.0000
  3880        0.0000             nan     0.5231   -0.0000
  3900        0.0000             nan     0.5231   -0.0000
  3920        0.0000             nan     0.5231   -0.0000
  3940        0.0000             nan     0.5231   -0.0000
  3960        0.0000             nan     0.5231   -0.0000
  3980        0.0000             nan     0.5231   -0.0000
  4000        0.0000             nan     0.5231   -0.0000
  4020        0.0000             nan     0.5231   -0.0000
  4040        0.0000             nan     0.5231    0.0000
  4060        0.0000             nan     0.5231   -0.0000
  4080        0.0000             nan     0.5231   -0.0000
  4100        0.0000             nan     0.5231   -0.0000
  4120        0.0000             nan     0.5231   -0.0000
  4140        0.0000             nan     0.5231    0.0000
  4160        0.0000             nan     0.5231   -0.0000
  4180        0.0000             nan     0.5231   -0.0000
  4200        0.0000             nan     0.5231   -0.0000
  4220        0.0000             nan     0.5231   -0.0000
  4240        0.0000             nan     0.5231   -0.0000
  4260        0.0000             nan     0.5231   -0.0000
  4280        0.0000             nan     0.5231   -0.0000
  4300        0.0000             nan     0.5231   -0.0000
  4320        0.0000             nan     0.5231   -0.0000
  4340        0.0000             nan     0.5231   -0.0000
  4360        0.0000             nan     0.5231   -0.0000
  4380        0.0000             nan     0.5231   -0.0000
  4383        0.0000             nan     0.5231   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.4958             nan     0.5987    0.3148
     2        0.4543             nan     0.5987    0.0035
     3        0.4000             nan     0.5987    0.0563
     4        0.3772             nan     0.5987   -0.0501
     5        0.3311             nan     0.5987   -0.0234
     6        0.3407             nan     0.5987   -0.0676
     7        0.3132             nan     0.5987   -0.1024
     8        0.2923             nan     0.5987   -0.0464
     9        0.2576             nan     0.5987   -0.0255
    10        0.2282             nan     0.5987   -0.0126
    20        0.1996             nan     0.5987   -0.0122
    40        0.0745             nan     0.5987   -0.0162
    60        0.0296             nan     0.5987   -0.0097
    80        0.0078             nan     0.5987   -0.0014
   100        0.0035             nan     0.5987    0.0001
   120        0.0014             nan     0.5987   -0.0004
   140        0.0005             nan     0.5987   -0.0001
   160        0.0001             nan     0.5987   -0.0000
   180        0.0000             nan     0.5987   -0.0000
   200        0.0000             nan     0.5987   -0.0000
   220        0.0000             nan     0.5987   -0.0000
   240        0.0000             nan     0.5987   -0.0000
   260        0.0000             nan     0.5987   -0.0000
   280        0.0000             nan     0.5987   -0.0000
   300        0.0000             nan     0.5987   -0.0000
   320        0.0000             nan     0.5987   -0.0000
   340        0.0000             nan     0.5987   -0.0000
   360        0.0000             nan     0.5987   -0.0000
   380        0.0000             nan     0.5987   -0.0000
   400        0.0000             nan     0.5987   -0.0000
   420        0.0000             nan     0.5987   -0.0000
   440        0.0000             nan     0.5987   -0.0000
   460        0.0000             nan     0.5987   -0.0000
   480        0.0000             nan     0.5987   -0.0000
   500        0.0000             nan     0.5987   -0.0000
   520        0.0000             nan     0.5987   -0.0000
   540        0.0000             nan     0.5987   -0.0000
   560        0.0000             nan     0.5987   -0.0000
   580        0.0000             nan     0.5987   -0.0000
   600        0.0000             nan     0.5987   -0.0000
   620        0.0000             nan     0.5987   -0.0000
   640        0.0000             nan     0.5987   -0.0000
   660        0.0000             nan     0.5987   -0.0000
   680        0.0000             nan     0.5987   -0.0000
   700        0.0000             nan     0.5987   -0.0000
   720        0.0000             nan     0.5987   -0.0000
   740        0.0000             nan     0.5987   -0.0000
   760        0.0000             nan     0.5987   -0.0000
   780        0.0000             nan     0.5987    0.0000
   800        0.0000             nan     0.5987   -0.0000
   820        0.0000             nan     0.5987   -0.0000
   840        0.0000             nan     0.5987    0.0000
   860        0.0000             nan     0.5987   -0.0000
   880        0.0000             nan     0.5987   -0.0000
   900        0.0000             nan     0.5987   -0.0000
   920        0.0000             nan     0.5987   -0.0000
   940        0.0000             nan     0.5987   -0.0000
   960        0.0000             nan     0.5987   -0.0000
   980        0.0000             nan     0.5987   -0.0000
  1000        0.0000             nan     0.5987   -0.0000
  1020        0.0000             nan     0.5987   -0.0000
  1040        0.0000             nan     0.5987   -0.0000
  1060        0.0000             nan     0.5987   -0.0000
  1080        0.0000             nan     0.5987   -0.0000
  1100        0.0000             nan     0.5987   -0.0000
  1120        0.0000             nan     0.5987   -0.0000
  1140        0.0000             nan     0.5987   -0.0000
  1160        0.0000             nan     0.5987    0.0000
  1170        0.0000             nan     0.5987   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.7000             nan     0.0299    0.0184
     2        0.6799             nan     0.0299    0.0196
     3        0.6581             nan     0.0299    0.0197
     4        0.6409             nan     0.0299    0.0180
     5        0.6267             nan     0.0299    0.0160
     6        0.6107             nan     0.0299    0.0165
     7        0.5980             nan     0.0299    0.0149
     8        0.5846             nan     0.0299    0.0143
     9        0.5697             nan     0.0299    0.0142
    10        0.5556             nan     0.0299    0.0133
    20        0.4588             nan     0.0299    0.0059
    40        0.3566             nan     0.0299    0.0019
    60        0.3182             nan     0.0299   -0.0001
    80        0.2902             nan     0.0299   -0.0006
   100        0.2708             nan     0.0299   -0.0012
   120        0.2576             nan     0.0299   -0.0013
   140        0.2465             nan     0.0299   -0.0009
   160        0.2423             nan     0.0299   -0.0016
   180        0.2304             nan     0.0299   -0.0017
   200        0.2236             nan     0.0299   -0.0004
   220        0.2154             nan     0.0299   -0.0023
   240        0.2066             nan     0.0299   -0.0016
   260        0.1993             nan     0.0299   -0.0009
   280        0.1915             nan     0.0299   -0.0014
   300        0.1854             nan     0.0299   -0.0015
   320        0.1816             nan     0.0299   -0.0010
   340        0.1773             nan     0.0299   -0.0007
   360        0.1734             nan     0.0299   -0.0010
   380        0.1650             nan     0.0299   -0.0007
   400        0.1592             nan     0.0299   -0.0005
   420        0.1553             nan     0.0299   -0.0012
   440        0.1529             nan     0.0299   -0.0008

Stochastic Gradient Boosting 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  shrinkage   interaction.depth  n.minobsinnode  n.trees  RMSE       Rsquared  
  0.02993102  10                 11               440     0.6736025  0.38048713
  0.04085569   4                 12              3624     0.9093590  0.16341480
  0.06283354  10                 12               562     0.7702092  0.29912781
  0.11344961   6                  6              3572     0.7061233  0.42495317
  0.14819805   6                 13              1924           NaN         NaN
  0.15201052   6                  8              1254     0.7330376  0.39421653
  0.18399097   1                 15              2332           NaN         NaN
  0.26662093   9                 22               213           NaN         NaN
  0.27487828   4                 22               122           NaN         NaN
  0.31420339   6                 13              4760           NaN         NaN
  0.32788732   1                  7              4665     0.8489610  0.31484681
  0.38919749   8                 19              2425           NaN         NaN
  0.39123275   6                 14               376           NaN         NaN
  0.39669898   5                 24              3383           NaN         NaN
  0.44203649   8                 18               647           NaN         NaN
  0.44877295   5                 10              1431     1.0825225  0.14511243
  0.52311142   3                 11              4383     1.0278591  0.14968672
  0.52605483   4                 12              3032     1.2568581  0.04821217
  0.58491709   7                 20              2168           NaN         NaN
  0.59873225   8                  6              1170     0.7557515  0.42297777
  MAE        Selected
  0.4282893  *       
  0.6141648          
  0.4903793          
  0.4896707          
        NaN          
  0.5043246          
        NaN          
        NaN          
        NaN          
        NaN          
  0.5877773          
        NaN          
        NaN          
        NaN          
        NaN          
  0.8199771          
  0.7231400          
  0.9176009          
        NaN          
  0.5696161          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were n.trees = 440, interaction.depth =
 10, shrinkage = 0.02993102 and n.minobsinnode = 11.
[1] "Sat Mar 10 01:57:36 2018"
Error in relative.influence(object, n.trees = numTrees) : 
  could not find function "relative.influence"
In addition: There were 35 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:00:56 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "gbm_h2o"                 
Multivariate Adaptive Regression Splines 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results:

  RMSE       Rsquared   MAE      
  0.1904614  0.9754148  0.1077846

Tuning parameter 'degree' was held constant at a value of 1
[1] "Sat Mar 10 02:01:09 2018"
Generalized Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results:

  RMSE      Rsquared   MAE      
  0.481729  0.7225825  0.3427649

[1] "Sat Mar 10 02:01:22 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
2: model fit failed for Fold1: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
3: model fit failed for Fold1: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
4: model fit failed for Fold2: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
5: model fit failed for Fold2: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
6: model fit failed for Fold2: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
7: model fit failed for Fold3: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
8: model fit failed for Fold3: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
9: model fit failed for Fold3: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
2: model fit failed for Fold1: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
3: model fit failed for Fold1: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
4: model fit failed for Fold2: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
5: model fit failed for Fold2: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
6: model fit failed for Fold2: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
7: model fit failed for Fold3: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
8: model fit failed for Fold3: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
9: model fit failed for Fold3: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3     NA's   :3    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:01:36 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "glm.nb"                  
Boosted Generalized Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  mstop  prune  RMSE       Rsquared   MAE        Selected
   25    yes    0.4126271  0.7848624  0.2801364  *       
   43    no     0.4174831  0.7766277  0.2906589          
   76    no     0.4291615  0.7645329  0.3006941          
   88    no     0.4335502  0.7606681  0.3050720          
  113    no     0.4430997  0.7526962  0.3135136          
  130    no     0.4478455  0.7490669  0.3168517          
  234    no     0.4675576  0.7336956  0.3321969          
  251    no     0.4694572  0.7321397  0.3336437          
  287    yes    0.4243753  0.7709058  0.2965641          
  385    no     0.4777798  0.7256661  0.3398949          
  434    no     0.4790831  0.7246491  0.3408941          
  467    yes    0.4243753  0.7709058  0.2965641          
  485    no     0.4800160  0.7239009  0.3415672          
  607    yes    0.4243753  0.7709058  0.2965641          
  677    yes    0.4243753  0.7709058  0.2965641          
  715    no     0.4814844  0.7227701  0.3426007          
  725    yes    0.4243753  0.7709058  0.2965641          
  877    yes    0.4243753  0.7709058  0.2965641          
  934    yes    0.4243753  0.7709058  0.2965641          
  953    no     0.4816953  0.7226077  0.3427440          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 25 and prune = yes.
[1] "Sat Mar 10 02:01:51 2018"
glmnet 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  alpha       lambda       RMSE       Rsquared   MAE        Selected
  0.02424925  0.030195207  0.4756803  0.7235834  0.3301526          
  0.04247987  1.453893129  0.6381489  0.7204888  0.3423418          
  0.07509305  0.088448974  0.4640929  0.7309814  0.3056071          
  0.08797359  3.992967173  0.7757634  0.7878733  0.4505979          
  0.11240018  6.217233295  0.8351266  0.8322841  0.5001089          
  0.12932486  1.038737428  0.6015072  0.7773535  0.3132320          
  0.23385692  0.746644554  0.5637781  0.7870123  0.2844853          
  0.25065253  0.094534792  0.4435760  0.7537517  0.2896722          
  0.28608022  0.038277602  0.4595892  0.7379444  0.3167439          
  0.38469318  0.089493030  0.4334147  0.7646120  0.2842065          
  0.43352645  0.475110425  0.5171953  0.7878733  0.2511132          
  0.46643031  0.001047751  0.4792928  0.7241252  0.3403044          
  0.48499598  1.283975499  0.7814788  0.7878733  0.4551110          
  0.60646832  0.025151034  0.4554891  0.7433460  0.3174105          
  0.67666045  0.041817010  0.4382418  0.7591960  0.3022969          
  0.71440735  0.176288109  0.4326181  0.7869440  0.2522842          
  0.72492907  0.021709656  0.4553160  0.7438756  0.3182536          
  0.87664305  0.006481914  0.4723775  0.7293288  0.3340751          
  0.93305532  0.001748798  0.4789528  0.7245290  0.3403298          
  0.95202160  0.148222230  0.4275723  0.7876951  0.2549870  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alpha = 0.9520216 and lambda
 = 0.1482222.
[1] "Sat Mar 10 02:02:05 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:05:25 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "glmnet_h2o"              
Start:  AIC=75.89
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V6    1    8.590  73.892
- V7    1    8.601  73.957
- V4    1    8.630  74.124
- V9    1    8.878  75.574
- V3    1    8.896  75.677
<none>       8.590  75.889
- V8    1    8.948  75.974
- V10   1    9.035  76.465
- V5    1    9.178  77.266
- V2    1   35.106 145.686

Step:  AIC=73.89
.outcome ~ V2 + V3 + V4 + V5 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V7    1    8.602  71.960
- V4    1    8.630  72.125
- V9    1    8.879  73.574
- V3    1    8.903  73.713
<none>       8.590  73.892
- V8    1    8.949  73.975
- V10   1    9.036  74.469
- V5    1    9.301  75.944
- V2    1   35.155 143.756

Step:  AIC=71.96
.outcome ~ V2 + V3 + V4 + V5 + V8 + V9 + V10

       Df Deviance     AIC
- V4    1    8.638  70.175
- V9    1    8.899  71.693
- V3    1    8.907  71.736
<none>       8.602  71.960
- V8    1    8.955  72.012
- V10   1    9.038  72.483
- V5    1    9.301  73.944
- V2    1   35.201 141.823

Step:  AIC=70.18
.outcome ~ V2 + V3 + V5 + V8 + V9 + V10

       Df Deviance     AIC
- V9    1    8.949  69.978
<none>       8.638  70.175
- V3    1    8.992  70.224
- V8    1    9.000  70.269
- V10   1    9.083  70.733
- V5    1    9.323  72.067
- V2    1   35.246 139.889

Step:  AIC=69.98
.outcome ~ V2 + V3 + V5 + V8 + V10

       Df Deviance     AIC
- V3    1    9.234  69.574
<none>       8.949  69.978
- V8    1    9.324  70.073
- V10   1    9.404  70.505
- V5    1    9.803  72.628
- V2    1   35.757 138.623

Step:  AIC=69.57
.outcome ~ V2 + V5 + V8 + V10

       Df Deviance     AIC
<none>       9.234  69.574
- V8    1    9.627  69.699
- V10   1    9.644  69.793
- V5    1   10.164  72.472
- V2    1   36.195 137.244
Start:  AIC=43.84
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V8    1   4.5826  41.845
- V10   1   4.5828  41.846
- V5    1   4.5839  41.858
- V3    1   4.5842  41.862
- V4    1   4.5915  41.943
- V7    1   4.5985  42.021
- V6    1   4.7342  43.504
<none>      4.5824  43.842
- V9    1   4.7841  44.039
- V2    1  18.7232 113.627

Step:  AIC=41.84
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9 + V10

       Df Deviance     AIC
- V10   1   4.5829  39.848
- V5    1   4.5840  39.860
- V3    1   4.5848  39.868
- V4    1   4.5915  39.944
- V7    1   4.5986  40.022
- V6    1   4.7342  41.504
<none>      4.5826  41.845
- V9    1   4.7842  42.040
- V2    1  19.5822 113.914

Step:  AIC=39.85
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9

       Df Deviance     AIC
- V5    1   4.5842  37.862
- V3    1   4.5857  37.878
- V4    1   4.5916  37.944
- V7    1   4.5986  38.022
- V6    1   4.7342  39.504
<none>      4.5829  39.848
- V9    1   4.7929  40.133
- V2    1  24.0183 122.328

Step:  AIC=37.86
.outcome ~ V2 + V3 + V4 + V6 + V7 + V9

       Df Deviance     AIC
- V3    1   4.5874  35.897
- V4    1   4.5925  35.954
- V7    1   4.5987  36.023
- V6    1   4.7349  37.511
<none>      4.5842  37.862
- V9    1   4.7958  38.164
- V2    1  24.2048 120.723

Step:  AIC=35.9
.outcome ~ V2 + V4 + V6 + V7 + V9

       Df Deviance     AIC
- V4    1   4.5939  33.970
- V7    1   4.6026  34.066
- V6    1   4.7362  35.526
<none>      4.5874  35.897
- V9    1   4.7994  36.201
- V2    1  24.7976 119.957

Step:  AIC=33.97
.outcome ~ V2 + V6 + V7 + V9

       Df Deviance     AIC
- V7    1   4.6066  32.110
- V6    1   4.7406  33.572
<none>      4.5939  33.970
- V9    1   4.8013  34.222
- V2    1  25.0873 118.549

Step:  AIC=32.11
.outcome ~ V2 + V6 + V9

       Df Deviance     AIC
- V6    1   4.7514  31.690
<none>      4.6066  32.110
- V9    1   4.8090  32.304
- V2    1  25.0990 116.573

Step:  AIC=31.69
.outcome ~ V2 + V9

       Df Deviance     AIC
- V9    1   4.8869  31.123
<none>      4.7514  31.690
- V2    1  27.7670 119.725

Step:  AIC=31.12
.outcome ~ V2

       Df Deviance     AIC
<none>      4.8869  31.123
- V2    1  28.7312 119.466
Start:  AIC=70.24
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V6    1    7.720  68.486
- V8    1    7.767  68.786
- V9    1    7.801  69.005
- V10   1    7.863  69.403
- V7    1    7.990  70.201
<none>       7.683  70.240
- V4    1    8.206  71.534
- V3    1    8.477  73.163
- V5    1    8.684  74.369
- V2    1   34.778 143.741

Step:  AIC=68.49
.outcome ~ V2 + V3 + V4 + V5 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V8    1    7.833  67.207
- V9    1    7.857  67.364
- V10   1    7.912  67.712
- V7    1    8.022  68.401
<none>       7.720  68.486
- V4    1    8.212  69.573
- V3    1    8.483  71.195
- V5    1    8.817  73.125
- V2    1   35.286 142.467

Step:  AIC=67.21
.outcome ~ V2 + V3 + V4 + V5 + V7 + V9 + V10

       Df Deviance     AIC
- V9    1    8.006  66.300
- V10   1    8.027  66.433
- V7    1    8.064  66.664
<none>       7.833  67.207
- V4    1    8.322  68.238
- V3    1    8.617  69.980
- V5    1    8.867  71.412
- V2    1   35.329 140.529

Step:  AIC=66.3
.outcome ~ V2 + V3 + V4 + V5 + V7 + V10

       Df Deviance     AIC
- V10   1    8.225  65.651
- V7    1    8.310  66.167
<none>       8.006  66.300
- V4    1    8.566  67.685
- V3    1    8.708  68.502
- V5    1    9.300  71.793
- V2    1   37.138 141.024

Step:  AIC=65.65
.outcome ~ V2 + V3 + V4 + V5 + V7

       Df Deviance     AIC
- V7    1    8.545  65.560
<none>       8.225  65.651
- V3    1    8.851  67.319
- V4    1    8.922  67.721
- V5    1    9.508  70.902
- V2    1   37.158 139.052

Step:  AIC=65.56
.outcome ~ V2 + V3 + V4 + V5

       Df Deviance     AIC
<none>       8.545  65.560
- V3    1    9.038  66.365
- V4    1    9.073  66.558
- V5    1    9.657  69.678
- V2    1   37.202 137.111
Start:  AIC=94.72
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V7    1   11.587  92.736
- V6    1   11.589  92.745
- V4    1   11.615  92.919
- V8    1   11.690  93.410
- V10   1   11.857  94.485
- V3    1   11.890  94.693
<none>      11.585  94.724
- V9    1   11.911  94.827
- V5    1   12.117  96.135
- V2    1   47.043 199.224

Step:  AIC=92.74
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V9 + V10

       Df Deviance     AIC
- V6    1   11.591  90.759
- V4    1   11.617  90.931
- V8    1   11.691  91.410
- V10   1   11.857  92.485
- V3    1   11.890  92.693
<none>      11.587  92.736
- V9    1   11.919  92.881
- V5    1   12.118  94.138
- V2    1   47.086 197.292

Step:  AIC=90.76
.outcome ~ V2 + V3 + V4 + V5 + V8 + V9 + V10

       Df Deviance     AIC
- V4    1   11.621  88.954
- V8    1   11.692  89.419
- V10   1   11.859  90.500
- V3    1   11.893  90.712
<none>      11.591  90.759
- V9    1   11.919  90.881
- V5    1   12.124  92.179
- V2    1   48.274 197.186

Step:  AIC=88.95
.outcome ~ V2 + V3 + V5 + V8 + V9 + V10

       Df Deviance     AIC
- V8    1   11.720  87.602
- V3    1   11.896  88.737
- V10   1   11.909  88.816
<none>      11.621  88.954
- V9    1   11.945  89.045
- V5    1   12.152  90.349
- V2    1   48.506 195.552

Step:  AIC=87.6
.outcome ~ V2 + V3 + V5 + V9 + V10

       Df Deviance     AIC
- V10   1   11.995  87.364
- V3    1   12.030  87.586
<none>      11.720  87.602
- V9    1   12.060  87.772
- V5    1   12.249  88.955
- V2    1   49.729 195.444

Step:  AIC=87.36
.outcome ~ V2 + V3 + V5 + V9

       Df Deviance     AIC
- V3    1   12.221  86.781
<none>      11.995  87.364
- V9    1   12.397  87.872
- V5    1   12.489  88.429
- V2    1   51.114 195.531

Step:  AIC=86.78
.outcome ~ V2 + V5 + V9

       Df Deviance     AIC
<none>      12.221  86.781
- V9    1   12.568  86.909
- V5    1   12.767  88.102
- V2    1   52.234 195.178
Generalized Linear Model with Stepwise Feature Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results:

  RMSE       Rsquared   MAE      
  0.4604502  0.7419246  0.3340261

[1] "Sat Mar 10 02:05:38 2018"
Independent Component Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  n.comp  RMSE       Rsquared   MAE        Selected
  1       0.7496067  0.2378958  0.4627885          
  2       0.7142936  0.3037674  0.4642118          
  3       0.7161699  0.3006642  0.4699306          
  4       0.6163828  0.4861686  0.4300791          
  5       0.6044929  0.4982636  0.4042360          
  6       0.6100616  0.4837248  0.4225191          
  7       0.5817127  0.5333414  0.4175425          
  8       0.5693425  0.5644271  0.4040074          
  9       0.4817290  0.7225825  0.3427649  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was n.comp = 9.
[1] "Sat Mar 10 02:05:54 2018"
Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      0.5319793  0.6098653  0.3526162          
  2      0.4941861  0.7031898  0.3577221          
  3      0.4821393  0.7197698  0.3495981          
  4      0.4794583  0.7253505  0.3411739  *       
  5      0.4817030  0.7222679  0.3427851          
  6      0.4817229  0.7225773  0.3427614          
  7      0.4817290  0.7225825  0.3427674          
  8      0.4817319  0.7225795  0.3427676          
  9      0.4817290  0.7225825  0.3427649          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 4.
[1] "Sat Mar 10 02:06:07 2018"
Error in MSEP(object) : could not find function "MSEP"
k-Nearest Neighbors 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  kmax  distance    kernel        RMSE       Rsquared   MAE        Selected
   1    1.14241462  biweight      0.7545371  0.2478291  0.4670877          
   2    1.50022828  cos           0.7011662  0.3182179  0.4267201          
   2    2.43228951  biweight      0.7313134  0.2750989  0.4551115          
   3    2.76864489  rectangular   0.7177664  0.3087188  0.4297520          
   3    2.91606293  rectangular   0.7269317  0.2917134  0.4396682          
   4    2.32034562  cos           0.7260088  0.3064089  0.4358332          
   6    2.21042158  gaussian      0.7050137  0.3857714  0.3902462          
   7    1.52238212  epanechnikov  0.6935915  0.3981202  0.3903246          
   8    1.22137944  cos           0.6609200  0.4768962  0.3629487          
  10    1.50413519  triangular    0.6923007  0.4173746  0.3855888          
  11    2.05992341  gaussian      0.7304586  0.3251724  0.4184561          
  12    0.02342561  epanechnikov  0.6866579  0.4696404  0.3802621          
  13    2.39091177  cos           0.7066778  0.3841239  0.3981894          
  16    1.08156035  gaussian      0.6545133  0.5282357  0.3665141  *       
  17    1.25082317  cos           0.6603125  0.4979749  0.3625998          
  18    1.72984779  triangular    0.6958808  0.4118399  0.3935944          
  19    1.03257246  rectangular   0.6791450  0.4745520  0.3712021          
  22    0.63014668  inv           0.6717544  0.4161821  0.3843523          
  24    0.19397978  triweight     0.7434992  0.3825158  0.4149918          
  24    1.67211548  triweight     0.7111402  0.3808451  0.3973969          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were kmax = 16, distance = 1.08156
 and kernel = gaussian.
[1] "Sat Mar 10 02:06:22 2018"
k-Nearest Neighbors 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  k   RMSE       Rsquared   MAE        Selected
   1  0.7817348  0.2462104  0.4963118          
   2  0.7279793  0.2744653  0.4637982          
   3  0.7243011  0.2841933  0.4368448          
   4  0.7112192  0.3204815  0.4107429          
   6  0.6806242  0.4289058  0.3855763          
   7  0.6946224  0.4018661  0.3850979          
   8  0.6949009  0.4033808  0.3879641          
  10  0.6722719  0.5002129  0.3773117  *       
  11  0.6757895  0.5124448  0.3685282          
  12  0.6774376  0.5460623  0.3717569          
  13  0.6869090  0.5333255  0.3787609          
  16  0.7011551  0.5437011  0.3856899          
  17  0.7085351  0.5214547  0.3856637          
  18  0.7167660  0.5145458  0.3907696          
  19  0.7158849  0.4917420  0.3949297          
  22  0.7232167  0.5089248  0.3950945          
  24  0.7346359  0.4725083  0.4039918          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 10.
[1] "Sat Mar 10 02:06:35 2018"
Polynomial Kernel Regularized Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  lambda        degree  RMSE         Rsquared    MAE          Selected
  1.322045e-05  2         0.8552029  0.08428127    0.5039478          
  1.630795e-05  3         0.8403227  0.22234453    0.5032337  *       
  2.373915e-05  2         0.8552016  0.08428464    0.5039459          
  2.753391e-05  3         0.8403227  0.22234460    0.5032337          
  3.647547e-05  3         0.8403227  0.22234466    0.5032337          
  4.432250e-05  3         0.8403227  0.22234471    0.5032337          
  1.476674e-04  3         0.8403227  0.22234537    0.5032337          
  1.791689e-04  2         0.8551818  0.08433434    0.5039183          
  2.694022e-04  2         0.8551703  0.08436317    0.5039022          
  8.384282e-04  2         0.8550986  0.08454449    0.5038021          
  1.471066e-03  3         0.8403227  0.22235383    0.5032338          
  2.148580e-03  1       138.3963985  0.20418632  113.2128730          
  2.660602e-03  3         0.8403227  0.22236144    0.5032338          
  1.077312e-02  2         0.8539869  0.08758445    0.5022674          
  2.417143e-02  2         0.8528014  0.09133266    0.5006802          
  3.732817e-02  2         0.8518701  0.09465602    0.4996139          
  4.213523e-02  2         0.8515719  0.09578815    0.4994154          
  2.416658e-01  1         1.6943298  0.19254809    1.2442703          
  4.626756e-01  1         1.2016018  0.18196873    0.8157047          
  5.755830e-01  2         0.8426727  0.09952677    0.4952306          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 1.630795e-05 and degree = 3.
[1] "Sat Mar 10 02:06:48 2018"

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.311416469  0.017221060 -0.061381075 -0.032924497  0.027075129 -0.006016014 
          V8           V9          V10 
-0.025054120 -0.014152118  0.050280590 

 Quartiles of Marginal Effects:
 
           V2          V3           V4          V5          V6           V7
25% 0.1889023 -0.02952613 -0.115545011 -0.08628018 -0.02105379 -0.052805405
50% 0.2516402  0.01403410 -0.046505679 -0.03832708  0.02990416 -0.005086846
75% 0.4174668  0.07321898  0.005900942  0.04076638  0.08812591  0.040883983
             V8          V9         V10
25% -0.09726320 -0.05633315 -0.03718215
50% -0.01077227 -0.01198066  0.05168442
75%  0.04148641  0.04948254  0.11972705

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.415115540  0.025819041 -0.085765442 -0.041002896  0.043942504 -0.002565912 
          V8           V9          V10 
 0.001025018 -0.023477400  0.076469666 

 Quartiles of Marginal Effects:
 
           V2          V3           V4          V5          V6           V7
25% 0.2547559 -0.02016240 -0.173877221 -0.13851759 -0.01448303 -0.067144123
50% 0.3351079  0.02613681 -0.066081543 -0.05082462  0.05110546 -0.000421956
75% 0.5671007  0.09586223 -0.007697398  0.03315068  0.15818037  0.052825972
             V8          V9         V10
25% -0.07829992 -0.08731696 -0.02117296
50%  0.02366719 -0.01810038  0.06693455
75%  0.08121166  0.07544093  0.20041298

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.730907317  0.063867696 -0.028192436  0.072668706  0.009162892  0.005747060 
          V8           V9          V10 
-0.108193127 -0.076950318 -0.056852030 

 Quartiles of Marginal Effects:
 
           V2         V3          V4         V5          V6           V7
25% 0.6920774 0.05442988 -0.03881846 0.04884016 -0.01114518 -0.012981933
50% 0.7367987 0.06672269 -0.02618886 0.07223603  0.01247306  0.004663085
75% 0.7630111 0.07200146 -0.01820521 0.09342569  0.02760238  0.019287331
             V8          V9         V10
25% -0.12663429 -0.10108642 -0.07418851
50% -0.11522001 -0.07572524 -0.05673640
75% -0.08182011 -0.05858218 -0.03705983

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.526665144  0.031561056 -0.141042267 -0.053837021  0.146557149  0.005094858 
          V8           V9          V10 
 0.101867985 -0.033935508  0.139085884 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5         V6          V7
25% 0.2333652 -0.07314070 -0.21706098 -0.22610877 -0.1167236 -0.11042736
50% 0.4453940  0.03093283 -0.15772241 -0.04553334  0.1708096  0.00711745
75% 0.7586407  0.11182128 -0.06403212  0.11705443  0.4101201  0.14358047
             V8           V9         V10
25% -0.04113312 -0.179666064 -0.07997192
50%  0.07121792  0.007932476  0.06466683
75%  0.27179874  0.114770356  0.36872743

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.58625422  0.02858985 -0.12031975 -0.01941143  0.08909207  0.03353222 
         V8          V9         V10 
 0.01164555 -0.03608271  0.09371119 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6          V7
25% 0.4233441 -0.03424180 -0.18934050 -0.15076843 -0.05677317 -0.06695073
50% 0.5158148  0.05203714 -0.11210767 -0.04108533  0.09036812  0.01098152
75% 0.7735206  0.09592070 -0.08413432  0.10608690  0.25584279  0.15758307
              V8          V9         V10
25% -0.109705763 -0.14386160 -0.10079783
50%  0.009299771 -0.01048822  0.06736384
75%  0.128140959  0.09686297  0.26811146

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.0303322765 -0.0033589816 -0.0009413921  0.0008505714  0.0018596951 
           V7            V8            V9           V10 
 0.0010164534 -0.0115012896 -0.0047221819  0.0035320511 

 Quartiles of Marginal Effects:
 
            V2           V3            V4           V5          V6          V7
25% 0.03032035 -0.003365404 -0.0009468964 0.0008429260 0.001855310 0.001009724
50% 0.03034044 -0.003360401 -0.0009412618 0.0008507215 0.001861787 0.001015525
75% 0.03035028 -0.003354368 -0.0009382352 0.0008591504 0.001867136 0.001022102
             V8           V9         V10
25% -0.01151377 -0.004729376 0.003524470
50% -0.01150428 -0.004724935 0.003536028
75% -0.01149742 -0.004715230 0.003541477

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.6774380212  0.0432726823 -0.0238958637  0.0736785492  0.0005292686 
           V7            V8            V9           V10 
 0.0006117429 -0.1153257655 -0.0741275429 -0.0563943163 

 Quartiles of Marginal Effects:
 
           V2         V3          V4         V5           V6            V7
25% 0.6674134 0.04126088 -0.02643693 0.06864548 -0.003301457 -0.0030300871
50% 0.6812076 0.04351524 -0.02370904 0.07458776  0.001133187  0.0005629669
75% 0.6866029 0.04485100 -0.02149168 0.07932655  0.004019228  0.0030306369
            V8          V9         V10
25% -0.1208259 -0.07966435 -0.05988196
50% -0.1165705 -0.07442517 -0.05655056
75% -0.1095745 -0.07007141 -0.05230569

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.0500307991  0.0023909552 -0.0094555224  0.0022848645  0.0005476927 
           V7            V8            V9           V10 
-0.0052971677 -0.0130320431  0.0025823256  0.0041480313 

 Quartiles of Marginal Effects:
 
             V2           V3           V4            V5           V6
25% 0.003290619 -0.009986526 -0.014558402 -0.0083979449 -0.022420656
50% 0.026219779 -0.001296041 -0.002151420  0.0008779672 -0.001596245
75% 0.061051209  0.017529077  0.009578325  0.0204155989  0.015082651
               V7           V8           V9          V10
25% -0.0141983768 -0.033147364 -0.019106800 -0.016594574
50%  0.0006017143 -0.003809803  0.002714042  0.002291116
75%  0.0142072019  0.009355701  0.011683565  0.026675888

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.528554593  0.035002847 -0.129300488 -0.043340867  0.116266828  0.004066067 
          V8           V9          V10 
 0.071434231 -0.028025660  0.124377709 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6          V7
25% 0.2666361 -0.03412975 -0.20419551 -0.18460080 -0.07923434 -0.09309291
50% 0.4351821  0.04486107 -0.14146883 -0.04021703  0.14311480  0.01312746
75% 0.7548706  0.09949518 -0.07753591  0.09213690  0.34741260  0.12009088
             V8          V9         V10
25% -0.05684462 -0.16740192 -0.05712506
50%  0.05572633  0.00686399  0.04200803
75%  0.22919189  0.09953414  0.34217932

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.682673979  0.041697480 -0.023607548  0.077938815 -0.002428286 -0.001409729 
          V8           V9          V10 
-0.116142975 -0.074645566 -0.061341945 

 Quartiles of Marginal Effects:
 
           V2         V3          V4         V5           V6           V7
25% 0.6822270 0.04160865 -0.02371883 0.07770903 -0.002598635 -0.001567138
50% 0.6828478 0.04170871 -0.02359097 0.07797845 -0.002401967 -0.001409398
75% 0.6830814 0.04176384 -0.02349193 0.07819555 -0.002277247 -0.001305717
            V8          V9         V10
25% -0.1163868 -0.07488926 -0.06149219
50% -0.1162000 -0.07466237 -0.06135177
75% -0.1158990 -0.07446425 -0.06116736

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.174216554 -0.012497866 -0.006074603  0.008369369  0.006751101  0.005302515 
          V8           V9          V10 
-0.057858753 -0.025482983  0.011684885 

 Quartiles of Marginal Effects:
 
           V2          V3           V4          V5          V6          V7
25% 0.1738945 -0.01261837 -0.006203913 0.008189662 0.006622618 0.005169791
50% 0.1743887 -0.01252586 -0.006073078 0.008387229 0.006802968 0.005283578
75% 0.1746059 -0.01239065 -0.006001976 0.008582009 0.006913990 0.005413206
             V8          V9        V10
25% -0.05810763 -0.02563270 0.01152365
50% -0.05792930 -0.02554324 0.01175946
75% -0.05776935 -0.02532702 0.01188300

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 1.204737e-02  8.475736e-04 -3.491861e-03  2.547576e-03 -1.205184e-04 
           V7            V8            V9           V10 
-2.870804e-03 -3.540729e-03 -2.857981e-05 -7.088233e-04 

 Quartiles of Marginal Effects:
 
               V2            V3            V4            V5            V6
25% -0.0003352924 -0.0020858360 -0.0033490736 -0.0016830493 -0.0044339784
50%  0.0034959947  0.0001931873 -0.0002032707  0.0002123392 -0.0001980929
75%  0.0167034679  0.0044886593  0.0028384775  0.0057624538  0.0036507729
               V7            V8            V9           V10
25% -0.0033515201 -0.0058982091 -0.0049860957 -0.0030734754
50%  0.0001533911 -0.0005409453  0.0001321379  0.0003448619
75%  0.0021139525  0.0026090472  0.0028964412  0.0066787658

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.281799579  0.015054999 -0.054820146 -0.029501394  0.023308061 -0.007086727 
          V8           V9          V10 
-0.028352697 -0.010904673  0.044400356 

 Quartiles of Marginal Effects:
 
           V2           V3           V4          V5          V6           V7
25% 0.1553771 -0.033308932 -0.103748187 -0.07128623 -0.02200611 -0.049346826
50% 0.2434051  0.009964816 -0.039323583 -0.02808409  0.03115211  0.008302907
75% 0.3748131  0.069690293  0.005982803  0.02907091  0.07933510  0.035315546
             V8          V9         V10
25% -0.09348321 -0.05270679 -0.02569831
50% -0.01223699 -0.00281176  0.03904857
75%  0.03441233  0.04105528  0.10399441

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.491256986  0.014074159 -0.020818866  0.033677575  0.009791054  0.009040493 
          V8           V9          V10 
-0.106797140 -0.057473125 -0.008623897 

 Quartiles of Marginal Effects:
 
           V2          V3          V4         V5           V6           V7
25% 0.4561390 0.005128878 -0.03060524 0.01741830 -0.004452179 -0.003660418
50% 0.5004155 0.011662122 -0.02139790 0.03673625  0.010133738  0.007896545
75% 0.5228285 0.022044277 -0.01119718 0.05242763  0.022545179  0.014944377
             V8          V9          V10
25% -0.12836871 -0.07224918 -0.024191966
50% -0.11057305 -0.05784091 -0.006442686
75% -0.09133548 -0.04738322  0.005037335

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.602755858  0.047630452 -0.059002178  0.009528944  0.038777689  0.030002639 
          V8           V9          V10 
-0.073642185 -0.061009203  0.025355905 

 Quartiles of Marginal Effects:
 
           V2         V3          V4          V5          V6          V7
25% 0.4830367 0.01162533 -0.09771873 -0.07082638 -0.02255168 -0.03877408
50% 0.5867559 0.06145650 -0.06507117 -0.01687821  0.06443365  0.02035816
75% 0.7012493 0.08281071 -0.02759533  0.09355250  0.11375322  0.08906445
               V8          V9         V10
25% -0.1494583887 -0.11499251 -0.08323914
50% -0.0766792693 -0.05498622  0.03520817
75%  0.0002413794  0.01742010  0.11297033

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.55804088  0.02985319 -0.12245372 -0.02869092  0.09362837  0.02354209 
         V8          V9         V10 
 0.02739105 -0.03200080  0.10327190 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6            V7
25% 0.3913820 -0.02841345 -0.18137398 -0.15739841 -0.05959578 -0.0618590633
50% 0.4718151  0.04958896 -0.13182865 -0.04576328  0.10800414 -0.0009619742
75% 0.7643217  0.10043104 -0.06638187  0.08948757  0.28463518  0.1418898137
             V8           V9         V10
25% -0.11473674 -0.152536486 -0.07507968
50%  0.02761959  0.001173018  0.07189295
75%  0.15659907  0.101817429  0.28818721

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.522092460  0.040362143 -0.130231253 -0.048111050  0.126454769 -0.004314432 
          V8           V9          V10 
 0.090699682 -0.032383204  0.129338776 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6           V7
25% 0.2403996 -0.03365279 -0.21022087 -0.19516900 -0.08978976 -0.112584986
50% 0.4322089  0.04772542 -0.14718042 -0.03056252  0.15268706  0.007123412
75% 0.7451018  0.12647200 -0.06863398  0.11117528  0.37451697  0.122294485
             V8           V9         V10
25% -0.03803245 -0.174176395 -0.05023062
50%  0.09078811  0.006180737  0.07149416
75%  0.24129237  0.102243170  0.34699231

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.468376017  0.030825437 -0.099462323 -0.041125293  0.059403377 -0.000538938 
          V8           V9          V10 
 0.021353703 -0.025826588  0.092596158 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6          V7
25% 0.2925389 -0.04075207 -0.17742467 -0.15485792 -0.04251261 -0.07725079
50% 0.4075563  0.03592808 -0.08275731 -0.05031511  0.08459152  0.01053894
75% 0.6336774  0.09928191 -0.03781079  0.05088613  0.20126799  0.09682013
             V8          V9         V10
25% -0.09128996 -0.11370687 -0.03077511
50%  0.05128660 -0.01055434  0.08227100
75%  0.11461026  0.08579344  0.24914468

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.758055703  0.068664515 -0.027934259  0.081164491  0.006735723  0.002581927 
          V8           V9          V10 
-0.107531344 -0.078436466 -0.067498625 

 Quartiles of Marginal Effects:
 
           V2         V3          V4         V5           V6           V7
25% 0.7252073 0.06081326 -0.03657056 0.06119036 -0.009888581 -0.011644153
50% 0.7628339 0.07141827 -0.02636612 0.07895931  0.010832573  0.001255902
75% 0.7829763 0.07568053 -0.01977094 0.09817208  0.022133465  0.014139081
             V8          V9         V10
25% -0.12158798 -0.09903702 -0.08061350
50% -0.11291212 -0.07621128 -0.06788966
75% -0.08535898 -0.06212614 -0.05114043

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.357763028  0.020893161 -0.072120444 -0.037586339  0.033566871 -0.004488711 
          V8           V9          V10 
-0.015857613 -0.018940050  0.061056483 

 Quartiles of Marginal Effects:
 
           V2          V3           V4          V5           V6           V7
25% 0.2273538 -0.01928297 -0.143138161 -0.10160592 -0.005965622 -0.058636306
50% 0.2825767  0.01741507 -0.056511145 -0.04683370  0.032324553  0.001112432
75% 0.4741856  0.08234707  0.001801298  0.03749706  0.115170250  0.040332967
             V8          V9         V10
25% -0.09168677 -0.07250107 -0.02924094
50%  0.01091936 -0.02195242  0.06028733
75%  0.05664971  0.05469098  0.14745716

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.363938189  0.001650727  0.038004738  0.013011906  0.020715836  0.014601427 
          V8           V9          V10 
 0.013792886 -0.060059215  0.036479920 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5           V6           V7
25% 0.1308675 -0.04283401 -0.01194619 -0.04362736 -0.050178414 -0.030113024
50% 0.3186715  0.01191608  0.02982194  0.02558270  0.001985573  0.009943769
75% 0.5453899  0.05464851  0.07769325  0.06711694  0.120250000  0.095149045
             V8          V9         V10
25% -0.02902900 -0.14878836 -0.03385481
50%  0.01265115 -0.06971666  0.03802826
75%  0.05834792  0.03598907  0.09605110

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.46372771  0.01116050  0.05463588  0.02062463  0.01199012  0.03654504 
         V8          V9         V10 
 0.03414174 -0.06908438  0.03535767 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5           V6          V7
25% 0.1935622 -0.08163572 -0.01950403 -0.04670422 -0.084881893 -0.03830597
50% 0.4803933  0.01668147  0.04775380  0.02438919  0.001394549  0.02956919
75% 0.6646658  0.09143033  0.10380516  0.08175946  0.099912422  0.13339751
             V8          V9         V10
25% -0.03949625 -0.19382859 -0.04509215
50%  0.02794382 -0.07277625  0.04169389
75%  0.11959614  0.06927930  0.10103180

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.641111835 -0.012354730  0.023034590  0.005462792  0.063179806  0.033281225 
          V8           V9          V10 
-0.015746350 -0.075416994  0.024097572 

 Quartiles of Marginal Effects:
 
           V2           V3         V4          V5         V6         V7
25% 0.6196592 -0.023900936 0.01431089 -0.01116785 0.05280960 0.02369901
50% 0.6455195 -0.012210474 0.01924262  0.01030938 0.06500640 0.03181244
75% 0.6636858 -0.002733979 0.02912067  0.01655287 0.07107797 0.04149255
              V8          V9        V10
25% -0.028755698 -0.08806364 0.01502345
50% -0.009820022 -0.07738177 0.02290973
75% -0.002696660 -0.05715527 0.03336632

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.599642698  0.015819280  0.065734427  0.006074369 -0.027487601  0.084052513 
          V8           V9          V10 
 0.040512761 -0.084597454 -0.027239859 

 Quartiles of Marginal Effects:
 
           V2           V3          V4           V5          V6          V7
25% 0.3182155 -0.086652578 -0.05188885 -0.063285699 -0.15259739 -0.05542667
50% 0.5717542  0.004730584  0.07330784  0.009170736 -0.03331795  0.06196763
75% 0.8817887  0.119920639  0.16087481  0.079767761  0.07066388  0.22164591
             V8         V9         V10
25% -0.04173843 -0.3771968 -0.16528104
50%  0.09431422 -0.1208475 -0.02381507
75%  0.20454959  0.2038564  0.13028178

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.587057192 -0.003615418  0.060398051  0.006532993  0.004467776  0.083166875 
          V8           V9          V10 
 0.007861493 -0.063619261 -0.008846538 

 Quartiles of Marginal Effects:
 
           V2           V3          V4          V5           V6          V7
25% 0.3233670 -0.066285144 0.003884092 -0.07596955 -0.078470377 -0.01256133
50% 0.5853178  0.008659135 0.064503308  0.01366159  0.009188737  0.05609899
75% 0.8318055  0.055686651 0.106899915  0.08166134  0.069074980  0.16658959
             V8          V9         V10
25% -0.09628025 -0.22435719 -0.07234551
50%  0.03043353 -0.06215078 -0.01423160
75%  0.15272242  0.12645507  0.06318654

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.027565885 -0.005426355  0.002140617  0.002047817  0.007247973 -0.001081001 
          V8           V9          V10 
-0.004299336 -0.005607572  0.010114434 

 Quartiles of Marginal Effects:
 
            V2           V3          V4          V5          V6           V7
25% 0.02755186 -0.005433966 0.002135673 0.002042349 0.007243788 -0.001088889
50% 0.02756953 -0.005428070 0.002142015 0.002050307 0.007248898 -0.001081887
75% 0.02758702 -0.005417907 0.002146285 0.002054664 0.007256386 -0.001076361
              V8           V9        V10
25% -0.004310603 -0.005617014 0.01010265
50% -0.004298339 -0.005611141 0.01011996
75% -0.004291938 -0.005599354 0.01012754

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.56763173 -0.02703357  0.02247075  0.01335559  0.07131537  0.02120317 
         V8          V9         V10 
-0.02921542 -0.08455001  0.05675337 

 Quartiles of Marginal Effects:
 
           V2          V3         V4         V5         V6         V7
25% 0.5648583 -0.02884516 0.02045717 0.01050929 0.06931447 0.01939124
50% 0.5670664 -0.02724198 0.02169976 0.01364625 0.07164400 0.02082751
75% 0.5732754 -0.02453219 0.02428668 0.01635604 0.07323104 0.02315545
             V8          V9        V10
25% -0.03149059 -0.08761169 0.05434818
50% -0.02814840 -0.08493124 0.05686450
75% -0.02646292 -0.08132443 0.05931681

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.084595947 -0.009317902  0.008228818  0.004395839  0.004747560  0.000761698 
          V8           V9          V10 
 0.002539717 -0.012676082  0.007552998 

 Quartiles of Marginal Effects:
 
             V2           V3           V4            V5            V6
25% 0.005945018 -0.035348260 -0.009227553 -0.0213399550 -0.0133229017
50% 0.034626375 -0.003581594  0.001812333  0.0008315388 -0.0002275609
75% 0.086432697  0.016501695  0.022346350  0.0284046538  0.0178669076
              V7           V8           V9           V10
25% -0.003400233 -0.016702374 -0.032343139 -0.0153481080
50%  0.002351337  0.001368794 -0.001366518  0.0007446563
75%  0.018445406  0.013595540  0.016290791  0.0304834772

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.58108776  0.02200138  0.06935463  0.01597439 -0.01831270  0.07592986 
         V8          V9         V10 
 0.04498037 -0.07619173 -0.00415056 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5         V6          V7
25% 0.2737850 -0.09204417 -0.02501754 -0.04688469 -0.1503801 -0.04389993
50% 0.5885073  0.03083695  0.07799506  0.02294302 -0.0165993  0.08241409
75% 0.8808657  0.12276280  0.15025500  0.08084682  0.0683375  0.19639784
             V8          V9          V10
25% -0.02903004 -0.30793358 -0.106940894
50%  0.07907626 -0.09878123 -0.008101784
75%  0.18840345  0.15802077  0.114894276

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.56513597 -0.02831276  0.02151823  0.01474695  0.07220287  0.01911235 
         V8          V9         V10 
-0.02935783 -0.08570771  0.05964511 

 Quartiles of Marginal Effects:
 
           V2          V3         V4         V5         V6         V7
25% 0.5650254 -0.02838957 0.02142965 0.01462827 0.07211536 0.01903665
50% 0.5651009 -0.02832318 0.02148603 0.01475944 0.07221768 0.01909643
75% 0.5653859 -0.02820368 0.02159678 0.01487899 0.07228808 0.01919986
             V8          V9        V10
25% -0.02944912 -0.08583728 0.05953192
50% -0.02931197 -0.08573121 0.05964859
75% -0.02923995 -0.08557199 0.05976113

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.153791259 -0.026726627  0.011092684  0.009976370  0.036302043 -0.002205789 
          V8           V9          V10 
-0.021875468 -0.031735770  0.049462111 

 Quartiles of Marginal Effects:
 
           V2          V3         V4          V5         V6           V7
25% 0.1534944 -0.02687733 0.01099052 0.009843306 0.03622437 -0.002365053
50% 0.1538974 -0.02676873 0.01111381 0.010010403 0.03630694 -0.002227055
75% 0.1542465 -0.02656018 0.01121071 0.010149702 0.03646595 -0.002111448
             V8          V9        V10
25% -0.02209856 -0.03194155 0.04923149
50% -0.02184917 -0.03177949 0.04956948
75% -0.02171266 -0.03154628 0.04972076

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.0262754136 -0.0040451529  0.0027496258  0.0017426137  0.0009549956 
           V7            V8            V9           V10 
-0.0001895493  0.0022467363 -0.0045438321  0.0020944359 

 Quartiles of Marginal Effects:
 
               V2            V3            V4            V5            V6
25% -6.004418e-05 -0.0077521720 -0.0012534464 -4.179027e-03 -0.0050192319
50%  4.573582e-03 -0.0003803021  0.0004069829  6.806827e-05  0.0001778471
75%  2.092664e-02  0.0035142102  0.0046450565  7.467706e-03  0.0093088873
               V7            V8            V9           V10
25% -0.0019233266 -0.0063995497 -5.199347e-03 -0.0026650425
50%  0.0007569897 -0.0001092597 -3.688188e-05 -0.0002468965
75%  0.0066221356  0.0041945916  4.570631e-03  0.0097768058

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.3356568227 -0.0006125372  0.0339071170  0.0117846530  0.0205779433 
           V7            V8            V9           V10 
 0.0105999832  0.0110261248 -0.0564044463  0.0351825352 

 Quartiles of Marginal Effects:
 
           V2           V3          V4          V5            V6          V7
25% 0.1166032 -0.045940787 -0.01002169 -0.04690214 -0.0437151722 -0.02854944
50% 0.2696037  0.008381567  0.02731355  0.02386463  0.0003685682  0.00781502
75% 0.4814063  0.051027112  0.07589349  0.06197165  0.1076654331  0.08672945
             V8          V9         V10
25% -0.02373549 -0.13688722 -0.03378210
50%  0.01161659 -0.05053405  0.03695064
75%  0.04718702  0.03158300  0.08421103

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.42779854 -0.03760904  0.02646803  0.01354674  0.06589352  0.01686183 
         V8          V9         V10 
-0.03746977 -0.07612086  0.07237792 

 Quartiles of Marginal Effects:
 
           V2          V3         V4          V5         V6          V7
25% 0.4071111 -0.04581066 0.01870010 0.003329778 0.05859641 0.006928564
50% 0.4290389 -0.04006897 0.02661088 0.014404514 0.06540881 0.015538397
75% 0.4540371 -0.02575581 0.03547073 0.027229769 0.07611090 0.026466848
             V8          V9        V10
25% -0.04992100 -0.09064078 0.05868288
50% -0.03514149 -0.07642758 0.07620469
75% -0.02478291 -0.06199821 0.08488894

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.584288638 -0.010336300  0.042841114 -0.001988670  0.041565402  0.060785752 
          V8           V9          V10 
-0.011716573 -0.063248713  0.008491194 

 Quartiles of Marginal Effects:
 
           V2           V3         V4           V5           V6         V7
25% 0.4673668 -0.058862670 0.01373643 -0.060248251 -0.001078938 0.01444236
50% 0.6020700 -0.003362223 0.03650683  0.007712339  0.042639536 0.05159936
75% 0.7085032  0.033817470 0.06982957  0.045614686  0.080725401 0.11213957
             V8           V9          V10
25% -0.08044378 -0.127402944 -0.020569764
50%  0.01873381 -0.070955460  0.003196778
75%  0.06576239  0.004628618  0.041782896

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.574823650  0.008014311  0.063275195  0.009954124  0.001013404  0.075633242 
          V8           V9          V10 
 0.016544033 -0.066809316 -0.004611252 

 Quartiles of Marginal Effects:
 
           V2          V3           V4          V5            V6          V7
25% 0.2963892 -0.04821515 -0.007018857 -0.05189780 -0.0907335468 -0.01962774
50% 0.5705493  0.01769703  0.058111794  0.01951446  0.0006948651  0.07069492
75% 0.8442448  0.07898214  0.113830352  0.07324942  0.0783771013  0.17372714
             V8          V9         V10
25% -0.09757650 -0.23082326 -0.08441236
50%  0.04267388 -0.08124602 -0.01245487
75%  0.17008036  0.13689432  0.08147390

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.5932251653  0.0264146749  0.0722288659  0.0188786809 -0.0323846374 
           V7            V8            V9           V10 
 0.0833198456  0.0666733607 -0.0804040223  0.0002610059 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6          V7
25% 0.2930624 -0.10994083 -0.05699966 -0.04673258 -0.15165944 -0.05600203
50% 0.5818146  0.03221604  0.08011630  0.01955997 -0.04354023  0.08900887
75% 0.8736304  0.16008953  0.16941750  0.09447290  0.07587146  0.21313971
             V8         V9         V10
25% -0.03774092 -0.3554294 -0.11051849
50%  0.11733262 -0.1115770 -0.01020819
75%  0.19992073  0.1834169  0.14124863

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.513952301  0.017976775  0.063366215  0.023018992  0.002791068  0.051052183 
          V8           V9          V10 
 0.044052329 -0.070608992  0.027853978 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5           V6          V7
25% 0.2266919 -0.08416603 -0.01120930 -0.03462240 -0.111220978 -0.03281209
50% 0.5285540  0.03429890  0.05860175  0.02966391  0.007967723  0.03685261
75% 0.7660691  0.11090408  0.11739388  0.09304134  0.091329237  0.16064871
             V8          V9         V10
25% -0.04444143 -0.22947371 -0.05899294
50%  0.06475049 -0.08629433  0.02465318
75%  0.14049303  0.09709624  0.08237575

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.661005448 -0.008707614  0.020785198  0.005299498  0.062437867  0.031705555 
          V8           V9          V10 
-0.012309164 -0.074502365  0.018694403 

 Quartiles of Marginal Effects:
 
           V2           V3         V4           V5         V6         V7
25% 0.6431797 -0.017655531 0.01334322 -0.009456758 0.05354710 0.02393215
50% 0.6644676 -0.008281208 0.01865467  0.010056763 0.06327027 0.02975498
75% 0.6824281 -0.001098835 0.02620322  0.014492149 0.06938555 0.03845600
              V8          V9        V10
25% -0.022371413 -0.08403896 0.01107182
50% -0.008286543 -0.07518553 0.01834336
75% -0.001236981 -0.06044302 0.02553589

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.408881638  0.005550745  0.045011633  0.016232154  0.018424115  0.022990418 
          V8           V9          V10 
 0.021809318 -0.065029026  0.037366883 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5           V6          V7
25% 0.1500574 -0.03896434 -0.01416226 -0.03417207 -0.067598499 -0.03553790
50% 0.4060920  0.00717010  0.04330283  0.01554186 -0.003830048  0.01496786
75% 0.6050119  0.05096681  0.09112299  0.07623100  0.112883696  0.11046035
             V8          V9         V10
25% -0.03749355 -0.16925193 -0.04313684
50%  0.01422716 -0.06103329  0.04012721
75%  0.08622475  0.04867823  0.10295391

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.350957174  0.009329997  0.007001350  0.032011097  0.015144775 -0.037829439 
          V8           V9          V10 
-0.046149656 -0.047385892 -0.049268237 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6          V7
25% 0.2057155 -0.04918026 -0.03933258 -0.01811860 -0.03977509 -0.10206885
50% 0.3116067 -0.00490083  0.01324901  0.01638828  0.01856911 -0.02906161
75% 0.4562786  0.03102165  0.06905871  0.05737410  0.08526619  0.02127612
             V8          V9         V10
25% -0.08351433 -0.12310976 -0.09887556
50% -0.02280738 -0.04514390 -0.04319966
75%  0.03078378  0.01389999  0.03746051

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.479607343  0.025395124  0.004384409  0.037882437  0.016717308 -0.062430671 
          V8           V9          V10 
-0.043126164 -0.061629258 -0.087608924 

 Quartiles of Marginal Effects:
 
           V2          V3           V4          V5          V6          V7
25% 0.3294819 -0.02791026 -0.068014946 -0.02526672 -0.05884350 -0.16979964
50% 0.4373873  0.02342840 -0.005581235  0.02301424  0.02548771 -0.04610348
75% 0.6277344  0.05581568  0.082681244  0.09521304  0.10280099  0.01616979
             V8          V9         V10
25% -0.10147405 -0.16249026 -0.17870734
50% -0.01281815 -0.07822694 -0.06030066
75%  0.06274733  0.04136376  0.01821888

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.769094490  0.093182621  0.090044033  0.106083597 -0.005494387 -0.060395844 
          V8           V9          V10 
-0.044269730 -0.064933061 -0.057316017 

 Quartiles of Marginal Effects:
 
           V2         V3         V4         V5            V6          V7
25% 0.7185665 0.08446482 0.06948515 0.08180075 -0.0169235250 -0.07507740
50% 0.7738002 0.09139412 0.08442431 0.10900266  0.0002210929 -0.06257451
75% 0.8130684 0.10106611 0.10716639 0.12739121  0.0143016240 -0.05230365
             V8          V9         V10
25% -0.05997949 -0.08450565 -0.07678836
50% -0.04404243 -0.06515491 -0.05919315
75% -0.02540218 -0.04972789 -0.04144760

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.662903743 -0.013375351 -0.059509780  0.038011997  0.070139693 -0.101693436 
          V8           V9          V10 
 0.008792268 -0.020954960 -0.145025343 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6          V7
25% 0.2477347 -0.09512174 -0.23072724 -0.08840797 -0.13601750 -0.26348377
50% 0.6211975 -0.01627705 -0.03286407  0.03744734  0.04342637 -0.09505749
75% 1.0109489  0.05701334  0.14303543  0.20801902  0.25310553  0.03913727
             V8          V9         V10
25% -0.20194334 -0.16112867 -0.39128889
50%  0.06433454 -0.02641674 -0.13454985
75%  0.20645287  0.15128670  0.07928846

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.69469106  0.02863838  0.02771573  0.04723084  0.03476871 -0.09575763 
         V8          V9         V10 
-0.01057159 -0.06675736 -0.09441555 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6           V7
25% 0.4873192 -0.04242378 -0.12438615 -0.03793694 -0.10082555 -0.178050421
50% 0.6331241  0.02858322  0.01514368  0.03843111  0.05409377 -0.086722514
75% 0.9041639  0.08060635  0.15115962  0.14204020  0.14898106 -0.002720499
             V8          V9         V10
25% -0.09130101 -0.19928466 -0.25355400
50% -0.01527099 -0.07498200 -0.06248941
75%  0.10837173  0.04888511  0.02739503

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.0315429713 -0.0054801489  0.0057031575  0.0052355652  0.0017340147 
           V7            V8            V9           V10 
 0.0002953893 -0.0017194350 -0.0095598072 -0.0009280659 

 Quartiles of Marginal Effects:
 
            V2           V3          V4          V5          V6           V7
25% 0.03152254 -0.005486936 0.005699232 0.005227604 0.001728127 0.0002891629
50% 0.03154473 -0.005482664 0.005706162 0.005237957 0.001736152 0.0002968019
75% 0.03157142 -0.005472758 0.005710948 0.005244956 0.001740165 0.0003008835
              V8           V9           V10
25% -0.001725408 -0.009572248 -0.0009350704
50% -0.001718303 -0.009561452 -0.0009279395
75% -0.001713378 -0.009549800 -0.0009220403

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.706317165  0.064460551  0.080196384  0.104709108 -0.002992035 -0.048451576 
          V8           V9          V10 
-0.040671159 -0.075176665 -0.047444551 

 Quartiles of Marginal Effects:
 
           V2         V3         V4         V5           V6          V7
25% 0.6929535 0.06212175 0.07617393 0.09892739 -0.005515471 -0.05191135
50% 0.7087332 0.06454554 0.07907234 0.10627743 -0.001744092 -0.04917265
75% 0.7178380 0.06618271 0.08373522 0.11075190  0.001270211 -0.04594722
             V8          V9         V10
25% -0.04502396 -0.07935970 -0.05105164
50% -0.04009528 -0.07623405 -0.04700793
75% -0.03761066 -0.07145039 -0.04459004

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 5.475567e-02 -2.394227e-04 -2.586911e-05  5.041926e-03  3.254732e-04 
           V7            V8            V9           V10 
-2.054356e-05 -1.052557e-02 -1.941114e-03 -2.041234e-03 

 Quartiles of Marginal Effects:
 
            V2           V3           V4           V5            V6
25% 0.01202602 -0.013055048 -0.014858376 -0.009103863 -0.0152310824
50% 0.03427915 -0.002167563 -0.000361134  0.001748073  0.0005385359
75% 0.06867190  0.007359200  0.018838570  0.011691565  0.0172441342
              V7           V8          V9           V10
25% -0.021712033 -0.026670072 -0.02354950 -0.0200872449
50%  0.001805077 -0.003535006 -0.00176063  0.0009268349
75%  0.011996451  0.008121556  0.01006352  0.0142684512

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.655124355  0.008438359 -0.034321717  0.038488761  0.046645639 -0.101260494 
          V8           V9          V10 
-0.003979231 -0.049250375 -0.141045217 

 Quartiles of Marginal Effects:
 
           V2            V3          V4          V5          V6           V7
25% 0.3316096 -0.0632142165 -0.19316044 -0.07424304 -0.11996443 -0.222323225
50% 0.5962704 -0.0006631623 -0.03772395  0.03291498  0.04593693 -0.086616375
75% 0.9413565  0.0755883689  0.11642621  0.19145845  0.21589802  0.007758937
             V8          V9         V10
25% -0.13988274 -0.18713515 -0.34900853
50%  0.03556256 -0.06709675 -0.09935605
75%  0.16378158  0.11891660  0.04124881

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.709935995  0.063498697  0.079908939  0.107586056 -0.004516767 -0.049174363 
          V8           V9          V10 
-0.040795662 -0.076485991 -0.048526193 

 Quartiles of Marginal Effects:
 
           V2         V3         V4        V5           V6          V7
25% 0.7093455 0.06339382 0.07973424 0.1073223 -0.004636645 -0.04932893
50% 0.7100485 0.06349611 0.07986318 0.1076578 -0.004467404 -0.04920208
75% 0.7104566 0.06357239 0.08006167 0.1078563 -0.004323941 -0.04906944
             V8          V9         V10
25% -0.04099507 -0.07666963 -0.04868143
50% -0.04077089 -0.07653993 -0.04850632
75% -0.04066371 -0.07631917 -0.04840473

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.180886085 -0.020385536  0.027878681  0.029309874  0.008777501 -0.000459081 
          V8           V9          V10 
-0.009914618 -0.046646697 -0.005293715 

 Quartiles of Marginal Effects:
 
           V2          V3         V4         V5          V6            V7
25% 0.1803839 -0.02053080 0.02777777 0.02910963 0.008654091 -0.0006071973
50% 0.1809245 -0.02045166 0.02793137 0.02935443 0.008824448 -0.0004230025
75% 0.1814687 -0.02024814 0.02802909 0.02951946 0.008905471 -0.0003480236
              V8          V9          V10
25% -0.010081500 -0.04689399 -0.005457051
50% -0.009898056 -0.04667385 -0.005283639
75% -0.009781410 -0.04644719 -0.005148776

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.0136589898  0.0005183716 -0.0003843453  0.0009889174 -0.0001863721 
           V7            V8            V9           V10 
 0.0002556447 -0.0027969473  0.0011803886 -0.0002540250 

 Quartiles of Marginal Effects:
 
             V2            V3            V4            V5            V6
25% 0.000438124 -1.957286e-03 -0.0044763008 -3.013646e-03 -5.350358e-03
50% 0.003948441  1.334664e-05  0.0001080432 -7.574293e-05 -2.605871e-05
75% 0.016714393  3.064221e-03  0.0044910966  4.040491e-03  6.397832e-03
               V7            V8            V9           V10
25% -0.0075658056 -0.0063788093 -8.030165e-03 -5.569925e-03
50%  0.0001390666 -0.0004228469 -1.696620e-07  9.827239e-05
75%  0.0016563012  0.0020199259  3.055699e-03  6.164518e-03

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.316019661  0.005693263  0.006433437  0.029019183  0.014064385 -0.031388057 
          V8           V9          V10 
-0.044470195 -0.042537102 -0.040468530 

 Quartiles of Marginal Effects:
 
           V2           V3          V4          V5          V6          V7
25% 0.1781892 -0.048304798 -0.03741852 -0.02115243 -0.03310670 -0.07914144
50% 0.2734819 -0.006241291  0.01105129  0.01643033  0.01411584 -0.02292205
75% 0.4183984  0.023891116  0.06964220  0.05674727  0.07777710  0.01463744
             V8          V9         V10
25% -0.07933558 -0.11407831 -0.08857096
50% -0.02237988 -0.04611396 -0.03263919
75%  0.01997552  0.01739675  0.03008842

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.51727934  0.01606809  0.06004238  0.07174107  0.01215769 -0.02128562 
         V8          V9         V10 
-0.03066548 -0.07675454 -0.02496682 

 Quartiles of Marginal Effects:
 
           V2          V3         V4         V5         V6          V7
25% 0.4693456 0.006832411 0.05008209 0.05351179 0.00379286 -0.03352747
50% 0.5247510 0.015958992 0.05993793 0.07589748 0.01454415 -0.02166515
75% 0.5589154 0.023130460 0.07034273 0.08772451 0.02471341 -0.01168460
             V8          V9         V10
25% -0.04307043 -0.09303376 -0.03502083
50% -0.02709251 -0.07937249 -0.02452487
75% -0.01661212 -0.06578633 -0.01487415

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.67510570  0.06032774  0.06787131  0.07084921  0.01396571 -0.05646113 
         V8          V9         V10 
-0.03700963 -0.06494225 -0.06392520 

 Quartiles of Marginal Effects:
 
           V2         V3          V4          V5          V6          V7
25% 0.5322940 0.02749624 0.003879699 0.003174081 -0.03507271 -0.09927323
50% 0.6850538 0.05735359 0.065481015 0.071924826  0.03078319 -0.06295311
75% 0.8016865 0.09242300 0.109847020 0.131600953  0.07189124 -0.02327306
             V8          V9          V10
25% -0.08864825 -0.11962817 -0.122566335
50% -0.01903185 -0.07276105 -0.060651269
75%  0.03208783 -0.01683147  0.002798634

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.672924829  0.024381745  0.009615756  0.045266361  0.032187931 -0.097856979 
          V8           V9          V10 
-0.007539776 -0.064930110 -0.106854405 

 Quartiles of Marginal Effects:
 
           V2          V3           V4          V5          V6           V7
25% 0.4397496 -0.04292748 -0.146946390 -0.04076044 -0.09685731 -0.191894757
50% 0.6200927  0.01526407  0.008500478  0.03413800  0.03338197 -0.085386794
75% 0.8834243  0.09334140  0.133059247  0.13727286  0.16555814 -0.008169892
              V8          V9         V10
25% -0.086286787 -0.19155927 -0.28329282
50%  0.005125967 -0.07545462 -0.07776364
75%  0.130331256  0.07269298  0.03245809

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.6487033031  0.0006082107 -0.0568008980  0.0337153823  0.0631200025 
           V7            V8            V9           V10 
-0.0951265814 -0.0063895262 -0.0322612720 -0.1650132682 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6          V7
25% 0.2656304 -0.07470116 -0.22777839 -0.09875298 -0.10716091 -0.23189888
50% 0.5872220 -0.01056700 -0.06519206  0.02068214  0.06533506 -0.06313608
75% 0.9490722  0.07589206  0.08390311  0.20193653  0.23032748  0.02581225
             V8         V9         V10
25% -0.19160367 -0.1968686 -0.39925199
50%  0.03235287 -0.0552587 -0.13988626
75%  0.18362751  0.1445488  0.03571391

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.553474173  0.030475265 -0.001432445  0.037648490  0.019025714 -0.075545057 
          V8           V9          V10 
-0.033346762 -0.065696860 -0.109969937 

 Quartiles of Marginal Effects:
 
           V2          V3           V4          V5          V6          V7
25% 0.3726335 -0.02180812 -0.101509962 -0.04315136 -0.07391212 -0.18658839
50% 0.4962637  0.02944577 -0.004107853  0.02924223  0.02337834 -0.06355603
75% 0.6889621  0.06624210  0.099009146  0.08147110  0.09724611  0.01473837
              V8          V9         V10
25% -0.122507768 -0.17038856 -0.22215322
50%  0.003403484 -0.06560349 -0.08181520
75%  0.078250517  0.03706201  0.01080648

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.795540547  0.102305052  0.093502371  0.111412187 -0.009476306 -0.065799935 
          V8           V9          V10 
-0.045778133 -0.062855813 -0.061279405 

 Quartiles of Marginal Effects:
 
           V2         V3         V4         V5           V6          V7
25% 0.7549232 0.09604226 0.07560810 0.09124558 -0.020038777 -0.07731595
50% 0.7989628 0.10088432 0.08767212 0.11336626 -0.004224104 -0.06783219
75% 0.8304500 0.10887182 0.10829905 0.12809677  0.006621411 -0.05870214
             V8          V9         V10
25% -0.05830228 -0.07996855 -0.07808421
50% -0.04559676 -0.06260545 -0.06319106
75% -0.02952421 -0.04959918 -0.04708368

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.406667716  0.016383828  0.006722213  0.035657905  0.016114680 -0.048757280 
          V8           V9          V10 
-0.046900256 -0.054349282 -0.065234716 

 Quartiles of Marginal Effects:
 
           V2           V3           V4          V5          V6          V7
25% 0.2563900 -0.042554300 -0.042439376 -0.01334843 -0.04743422 -0.13813560
50% 0.3791009  0.005858896  0.007757178  0.01865583  0.02733444 -0.03559677
75% 0.5244345  0.043365698  0.087439064  0.07451030  0.09359973  0.01129010
             V8           V9         V10
25% -0.08702123 -0.135942125 -0.13863277
50% -0.01752604 -0.049435725 -0.06074756
75%  0.02921365  0.009344012  0.02398097

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 7.062405e-01  3.439108e-02  1.915414e-02  7.089103e-02  1.787452e-02 
           V7            V8            V9           V10 
-6.120724e-05 -5.402955e-02 -7.673643e-02 -3.649931e-02 

 Quartiles of Marginal Effects:
 
           V2         V3         V4         V5         V6            V7
25% 0.7057460 0.03431317 0.01897517 0.07064835 0.01773509 -2.424139e-04
50% 0.7063430 0.03438648 0.01911532 0.07091160 0.01787020 -9.267162e-05
75% 0.7067123 0.03447717 0.01933808 0.07118947 0.01804086  4.822639e-05
             V8          V9         V10
25% -0.05425571 -0.07691629 -0.03666340
50% -0.05404892 -0.07674739 -0.03650208
75% -0.05380445 -0.07658022 -0.03634350
Radial Basis Function Kernel Regularized Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  lambda        sigma        RMSE       Rsquared    MAE        Selected
  1.322045e-05   299.764711  0.7451086  0.36607898  0.5294738          
  1.630795e-05     5.714093  0.7147544  0.32043507  0.3982788          
  2.373915e-05    99.929940  0.7174661  0.37784718  0.5003522          
  2.753391e-05     2.034574  0.8087831  0.14173698  0.4560986          
  3.647547e-05     1.293946  0.8293390  0.08939288  0.4823810          
  4.432250e-05     8.057603  0.6887552  0.36589767  0.4063271          
  1.476674e-04    11.292009  0.6697839  0.40013863  0.4145005          
  1.791689e-04    93.359210  0.6843387  0.41439604  0.4809137          
  2.694022e-04   235.230742  0.6013877  0.53241518  0.4370023          
  8.384282e-04    98.738476  0.6245282  0.49482091  0.4476470          
  1.471066e-03    17.924022  0.6533181  0.43075905  0.4266653          
  2.148580e-03  9306.060021  0.4738233  0.71823606  0.2981036  *       
  2.660602e-03     6.488102  0.7040809  0.33917472  0.4003931          
  1.077312e-02   361.342901  0.4747341  0.71683843  0.3229468          
  2.417143e-02   214.899685  0.4768471  0.71231332  0.3174930          
  3.732817e-02    49.378317  0.5338631  0.63088551  0.3604217          
  4.213523e-02   419.987838  0.4759341  0.71518919  0.2985877          
  2.416658e-01  1444.788999  0.7069939  0.66141307  0.3962662          
  4.626756e-01  5512.648434  0.8163958  0.64045920  0.4840607          
  5.755830e-01    58.953921  0.5306333  0.67824983  0.2902303          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 0.00214858 and sigma
 = 9306.06.
[1] "Sat Mar 10 02:07:03 2018"
Least Angle Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  fraction    RMSE       Rsquared   MAE        Selected
  0.02424925  0.8149370  0.7878733  0.4830555          
  0.04247987  0.7953865  0.7878733  0.4665426          
  0.07509305  0.7608015  0.7878733  0.4387855          
  0.08797359  0.7472960  0.7878733  0.4282217          
  0.11240018  0.7219551  0.7878733  0.4081885          
  0.12932486  0.7046275  0.7878733  0.3943079          
  0.23385692  0.6032820  0.7878733  0.3146384          
  0.25065253  0.5882191  0.7878733  0.3030058          
  0.28608022  0.5579929  0.7878733  0.2810178          
  0.38469318  0.4892925  0.7878733  0.2615545          
  0.43352645  0.4669522  0.7877225  0.2644903          
  0.46643031  0.4565096  0.7867322  0.2654890          
  0.48499598  0.4516507  0.7861357  0.2660837          
  0.60646832  0.4389218  0.7768921  0.2749436          
  0.67666045  0.4358660  0.7708897  0.2849903  *       
  0.71440735  0.4359318  0.7669755  0.2929316          
  0.72492907  0.4361082  0.7658707  0.2952960          
  0.87664305  0.4567335  0.7433787  0.3205416          
  0.93305532  0.4683762  0.7325188  0.3311507          
  0.95202160  0.4721366  0.7295314  0.3345803          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.6766605.
[1] "Sat Mar 10 02:07:15 2018"
Least Angle Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  step  RMSE       Rsquared   MAE        Selected
  1     0.8411550        NaN  0.5052625          
  2     0.4118502  0.7878733  0.2674652          
  3     0.4118823  0.7871133  0.2751899          
  4     0.4100110  0.7826686  0.2834933  *       
  5     0.4143539  0.7770872  0.2898943          
  6     0.4151969  0.7757037  0.2916937          
  7     0.4250090  0.7674773  0.2994451          
  8     0.4399937  0.7560483  0.3123484          
  9     0.4519696  0.7469226  0.3211244          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was step = 4.
[1] "Sat Mar 10 02:07:28 2018"
The lasso 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  fraction    RMSE       Rsquared   MAE        Selected
  0.02424925  0.8149370  0.7878733  0.4830555          
  0.04247987  0.7953865  0.7878733  0.4665426          
  0.07509305  0.7608015  0.7878733  0.4387855          
  0.08797359  0.7472960  0.7878733  0.4282217          
  0.11240018  0.7219551  0.7878733  0.4081885          
  0.12932486  0.7046275  0.7878733  0.3943079          
  0.23385692  0.6032820  0.7878733  0.3146384          
  0.25065253  0.5882191  0.7878733  0.3030058          
  0.28608022  0.5579929  0.7878733  0.2810178          
  0.38469318  0.4892925  0.7878733  0.2615545          
  0.43352645  0.4669522  0.7877225  0.2644903          
  0.46643031  0.4565096  0.7867322  0.2654890          
  0.48499598  0.4516507  0.7861357  0.2660837          
  0.60646832  0.4389218  0.7768921  0.2749436          
  0.67666045  0.4358660  0.7708897  0.2849903  *       
  0.71440735  0.4359318  0.7669755  0.2929316          
  0.72492907  0.4361082  0.7658707  0.2952960          
  0.87664305  0.4567335  0.7433787  0.3205416          
  0.93305532  0.4683762  0.7325188  0.3311507          
  0.95202160  0.4721366  0.7295314  0.3345803          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.6766605.
[1] "Sat Mar 10 02:07:40 2018"
Linear Regression with Backwards Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  nvmax  RMSE       Rsquared   MAE        Selected
  2      0.4288383  0.7764015  0.3102380  *       
  3      0.4472241  0.7534905  0.3131182          
  4      0.4627366  0.7385377  0.3308193          
  5      0.4822704  0.7211918  0.3475144          
  6      0.4776844  0.7234080  0.3355489          
  7      0.4749558  0.7252060  0.3382866          
  8      0.4789002  0.7262929  0.3397734          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sat Mar 10 02:07:52 2018"
Linear Regression with Forward Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  nvmax  RMSE       Rsquared   MAE        Selected
  2      0.4288383  0.7764015  0.3102380  *       
  3      0.4472241  0.7534905  0.3131182          
  4      0.4627366  0.7385377  0.3308193          
  5      0.4822704  0.7211918  0.3475144          
  6      0.4776844  0.7234080  0.3355489          
  7      0.4749558  0.7252060  0.3382866          
  8      0.4789002  0.7262929  0.3397734          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sat Mar 10 02:08:04 2018"
Linear Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results:

  RMSE      Rsquared   MAE      
  0.481729  0.7225825  0.3427649

Tuning parameter 'intercept' was held constant at a value of TRUE
[1] "Sat Mar 10 02:08:16 2018"
Start:  AIC=-70.84
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V6    1    0.0006  8.590 -72.840
- V7    1    0.0116  8.601 -72.775
- V4    1    0.0398  8.630 -72.607
- V9    1    0.2885  8.878 -71.158
- V3    1    0.3065  8.896 -71.055
<none>               8.590 -70.843
- V8    1    0.3585  8.948 -70.758
- V10   1    0.4450  9.035 -70.267
- V5    1    0.5882  9.178 -69.465
- V2    1   26.5165 35.106  -1.046

Step:  AIC=-72.84
.outcome ~ V2 + V3 + V4 + V5 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V7    1    0.0115  8.602 -74.771
- V4    1    0.0394  8.630 -74.606
- V9    1    0.2880  8.879 -73.158
- V3    1    0.3123  8.903 -73.019
<none>               8.590 -72.840
- V8    1    0.3582  8.949 -72.757
- V10   1    0.4452  9.036 -72.263
- V5    1    0.7104  9.301 -70.788
- V2    1   26.5641 35.155  -2.976

Step:  AIC=-74.77
.outcome ~ V2 + V3 + V4 + V5 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V4    1    0.0363  8.638 -76.556
- V9    1    0.2972  8.899 -75.039
- V3    1    0.3047  8.907 -74.996
<none>               8.602 -74.771
- V8    1    0.3530  8.955 -74.720
- V10   1    0.4362  9.038 -74.249
- V5    1    0.6989  9.301 -72.787
- V2    1   26.5988 35.201  -4.909

Step:  AIC=-76.56
.outcome ~ V2 + V3 + V5 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V9    1    0.3108  8.949 -76.754
<none>               8.638 -76.556
- V3    1    0.3541  8.992 -76.507
- V8    1    0.3620  9.000 -76.463
- V10   1    0.4442  9.083 -75.999
- V5    1    0.6849  9.323 -74.665
- V2    1   26.6082 35.246  -6.842

Step:  AIC=-76.75
.outcome ~ V2 + V3 + V5 + V8 + V10

       Df Sum of Sq    RSS     AIC
- V3    1    0.2844  9.234 -77.158
<none>               8.949 -76.754
- V8    1    0.3754  9.324 -76.658
- V10   1    0.4545  9.404 -76.227
- V5    1    0.8544  9.803 -74.104
- V2    1   26.8084 35.757  -8.108

Step:  AIC=-77.16
.outcome ~ V2 + V5 + V8 + V10

       Df Sum of Sq    RSS     AIC
<none>               9.234 -77.158
- V8    1    0.3930  9.627 -77.032
- V10   1    0.4108  9.644 -76.938
- V5    1    0.9309 10.164 -74.259
- V2    1   26.9619 36.195  -9.488
Start:  AIC=-102.89
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq     RSS      AIC
- V8    1    0.0002  4.5826 -104.887
- V10   1    0.0003  4.5828 -104.886
- V5    1    0.0015  4.5839 -104.873
- V3    1    0.0018  4.5842 -104.870
- V4    1    0.0091  4.5915 -104.788
- V7    1    0.0161  4.5985 -104.711
- V6    1    0.1517  4.7342 -103.228
<none>               4.5824 -102.890
- V9    1    0.2017  4.7841 -102.692
- V2    1   14.1408 18.7232  -33.105

Step:  AIC=-104.89
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9 + V10

       Df Sum of Sq     RSS      AIC
- V10   1    0.0003  4.5829 -106.884
- V5    1    0.0014  4.5840 -106.872
- V3    1    0.0021  4.5848 -106.863
- V4    1    0.0089  4.5915 -106.788
- V7    1    0.0160  4.5986 -106.709
- V6    1    0.1515  4.7342 -105.228
<none>               4.5826 -104.887
- V9    1    0.2016  4.7842 -104.691
- V2    1   14.9995 19.5822  -32.817

Step:  AIC=-106.88
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9

       Df Sum of Sq     RSS      AIC
- V5    1    0.0013  4.5842 -108.869
- V3    1    0.0028  4.5857 -108.853
- V4    1    0.0086  4.5916 -108.788
- V7    1    0.0157  4.5986 -108.709
- V6    1    0.1513  4.7342 -107.228
<none>               4.5829 -106.884
- V9    1    0.2100  4.7929 -106.599
- V2    1   19.4353 24.0183  -24.404

Step:  AIC=-108.87
.outcome ~ V2 + V3 + V4 + V6 + V7 + V9

       Df Sum of Sq     RSS      AIC
- V3    1    0.0031  4.5874 -110.835
- V4    1    0.0083  4.5925 -110.777
- V7    1    0.0145  4.5987 -110.709
- V6    1    0.1506  4.7349 -109.221
<none>               4.5842 -108.869
- V9    1    0.2116  4.7958 -108.568
- V2    1   19.6206 24.2048  -26.009

Step:  AIC=-110.83
.outcome ~ V2 + V4 + V6 + V7 + V9

       Df Sum of Sq     RSS      AIC
- V4    1    0.0065  4.5939 -112.762
- V7    1    0.0152  4.6026 -112.666
- V6    1    0.1489  4.7362 -111.206
<none>               4.5874 -110.835
- V9    1    0.2120  4.7994 -110.530
- V2    1   20.2103 24.7976  -26.775

Step:  AIC=-112.76
.outcome ~ V2 + V6 + V7 + V9

       Df Sum of Sq     RSS      AIC
- V7    1    0.0127  4.6066 -114.621
- V6    1    0.1467  4.7406 -113.159
<none>               4.5939 -112.762
- V9    1    0.2074  4.8013 -112.510
- V2    1   20.4934 25.0873  -28.183

Step:  AIC=-114.62
.outcome ~ V2 + V6 + V9

       Df Sum of Sq     RSS      AIC
- V6    1    0.1449  4.7514 -115.042
<none>               4.6066 -114.621
- V9    1    0.2024  4.8090 -114.428
- V2    1   20.4924 25.0990  -30.159

Step:  AIC=-115.04
.outcome ~ V2 + V9

       Df Sum of Sq     RSS      AIC
- V9    1    0.1354  4.8869 -115.609
<none>               4.7514 -115.042
- V2    1   23.0156 27.7670  -27.007

Step:  AIC=-115.61
.outcome ~ V2

       Df Sum of Sq     RSS      AIC
<none>               4.8869 -115.609
- V2    1    23.844 28.7312  -27.266
Start:  AIC=-73.65
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V6    1    0.0379  7.720 -75.408
- V8    1    0.0844  7.767 -75.108
- V9    1    0.1184  7.801 -74.889
- V10   1    0.1808  7.863 -74.490
- V7    1    0.3074  7.990 -73.692
<none>               7.683 -73.654
- V4    1    0.5232  8.206 -72.359
- V3    1    0.7949  8.477 -70.731
- V5    1    1.0018  8.684 -69.525
- V2    1   27.0951 34.778  -0.152

Step:  AIC=-75.41
.outcome ~ V2 + V3 + V4 + V5 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V8    1    0.1121  7.833 -76.687
- V9    1    0.1368  7.857 -76.530
- V10   1    0.1916  7.912 -76.182
- V7    1    0.3014  8.022 -75.493
<none>               7.720 -75.408
- V4    1    0.4917  8.212 -74.320
- V3    1    0.7624  8.483 -72.699
- V5    1    1.0963  8.817 -70.768
- V2    1   27.5655 35.286  -1.427

Step:  AIC=-76.69
.outcome ~ V2 + V3 + V4 + V5 + V7 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V9    1    0.1731  8.006 -77.593
- V10   1    0.1944  8.027 -77.461
- V7    1    0.2315  8.064 -77.230
<none>               7.833 -76.687
- V4    1    0.4894  8.322 -75.656
- V3    1    0.7845  8.617 -73.914
- V5    1    1.0349  8.867 -72.482
- V2    1   27.4969 35.329  -3.365

Step:  AIC=-77.59
.outcome ~ V2 + V3 + V4 + V5 + V7 + V10

       Df Sum of Sq    RSS     AIC
- V10   1    0.2192  8.225 -78.243
- V7    1    0.3046  8.310 -77.726
<none>               8.006 -77.593
- V4    1    0.5606  8.566 -76.209
- V3    1    0.7018  8.708 -75.392
- V5    1    1.2943  9.300 -72.100
- V2    1   29.1321 37.138  -2.869

Step:  AIC=-78.24
.outcome ~ V2 + V3 + V4 + V5 + V7

       Df Sum of Sq    RSS     AIC
- V7    1    0.3202  8.545 -78.334
<none>               8.225 -78.243
- V3    1    0.6261  8.851 -76.575
- V4    1    0.6976  8.922 -76.173
- V5    1    1.2836  9.508 -72.992
- V2    1   28.9333 37.158  -4.842

Step:  AIC=-78.33
.outcome ~ V2 + V3 + V4 + V5

       Df Sum of Sq    RSS     AIC
<none>               8.545 -78.334
- V3    1    0.4931  9.038 -77.528
- V4    1    0.5280  9.073 -77.336
- V5    1    1.1123  9.657 -74.215
- V2    1   28.6572 37.202  -6.783
Start:  AIC=-122.96
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS      AIC
- V7    1     0.002 11.587 -124.943
- V6    1     0.003 11.589 -124.934
- V4    1     0.030 11.615 -124.760
- V8    1     0.105 11.690 -124.269
- V10   1     0.272 11.857 -123.194
- V3    1     0.304 11.890 -122.985
<none>              11.585 -122.955
- V9    1     0.325 11.911 -122.852
- V5    1     0.532 12.117 -121.544
- V2    1    35.458 47.043  -18.454

Step:  AIC=-124.94
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V9 + V10

       Df Sum of Sq    RSS      AIC
- V6    1     0.003 11.591 -126.920
- V4    1     0.030 11.617 -126.747
- V8    1     0.103 11.691 -126.268
- V10   1     0.270 11.857 -125.194
- V3    1     0.302 11.890 -124.985
<none>              11.587 -124.943
- V9    1     0.332 11.919 -124.798
- V5    1     0.530 12.118 -123.541
- V2    1    35.498 47.086  -20.386

Step:  AIC=-126.92
.outcome ~ V2 + V3 + V4 + V5 + V8 + V9 + V10

       Df Sum of Sq    RSS      AIC
- V4    1     0.030 11.621 -128.724
- V8    1     0.101 11.692 -128.260
- V10   1     0.269 11.859 -127.178
- V3    1     0.302 11.893 -126.966
<none>              11.591 -126.920
- V9    1     0.328 11.919 -126.797
- V5    1     0.534 12.124 -125.500
- V2    1    36.683 48.274  -20.492

Step:  AIC=-128.72
.outcome ~ V2 + V3 + V5 + V8 + V9 + V10

       Df Sum of Sq    RSS      AIC
- V8    1     0.099 11.720 -130.077
- V3    1     0.276 11.896 -128.942
- V10   1     0.288 11.909 -128.863
<none>              11.621 -128.724
- V9    1     0.324 11.945 -128.633
- V5    1     0.531 12.152 -127.329
- V2    1    36.886 48.506  -22.127

Step:  AIC=-130.08
.outcome ~ V2 + V3 + V5 + V9 + V10

       Df Sum of Sq    RSS      AIC
- V10   1     0.275 11.995 -130.314
- V3    1     0.310 12.030 -130.093
<none>              11.720 -130.077
- V9    1     0.340 12.060 -129.906
- V5    1     0.529 12.249 -128.723
- V2    1    38.009 49.729  -22.235

Step:  AIC=-130.31
.outcome ~ V2 + V3 + V5 + V9

       Df Sum of Sq    RSS      AIC
- V3    1     0.226 12.221 -130.898
<none>              11.995 -130.314
- V9    1     0.402 12.397 -129.806
- V5    1     0.494 12.489 -129.249
- V2    1    39.119 51.114  -22.147

Step:  AIC=-130.9
.outcome ~ V2 + V5 + V9

       Df Sum of Sq    RSS     AIC
<none>              12.221 -130.90
- V9    1     0.347 12.568 -130.77
- V5    1     0.546 12.767 -129.58
- V2    1    40.013 52.234  -22.50
Linear Regression with Stepwise Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results:

  RMSE       Rsquared   MAE      
  0.4604502  0.7419246  0.3340261

[1] "Sat Mar 10 02:08:29 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:09:08 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "logreg"                  
Error : package RWeka is required
Error : package RWeka is required
Error : package RWeka is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:09:21 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "M5"                      
Error : package RWeka is required
Error : package RWeka is required
Error : package RWeka is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:09:34 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "M5Rules"                 
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:09:46 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "mlpKerasDecay"           
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:09:58 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "mlpKerasDropout"         
Multi-Step Adaptive MCP-Net 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  alphas      nsteps  scale      RMSE       Rsquared   MAE        Selected
  0.07182432   5      1.9645969  0.4114922  0.7878733  0.3011243  *       
  0.08823188   9      1.9129023  0.4116204  0.7878733  0.3016448          
  0.11758374   6      2.6930264  0.4117669  0.7878733  0.3022159          
  0.12917623  10      0.4311208  0.4118076  0.7878733  0.3023704          
  0.15116016  10      0.6371048  0.4118685  0.7878733  0.3025985          
  0.16639237   8      3.0110799  0.4119017  0.7878733  0.3027214          
  0.26047123   8      3.9920633  0.4120232  0.7878733  0.3031628          
  0.27558728   6      1.1953914  0.4120352  0.7878733  0.3032057          
  0.30747220   5      3.0532530  0.4120568  0.7878733  0.3032824          
  0.39622386   6      1.1715237  0.4120988  0.7878733  0.3034310          
  0.44017381   8      3.9055744  0.4121135  0.7878733  0.3034825          
  0.46978728   2      1.3956029  0.4121218  0.7878733  0.3035117          
  0.48649638   9      2.6802848  0.4121261  0.7878733  0.3035266          
  0.59582149   5      3.5370712  0.4121482  0.7878733  0.3036037          
  0.65899441   5      2.7272474  0.4121576  0.7878733  0.3036365          
  0.69296661   7      0.9539834  0.4121620  0.7878733  0.3036517          
  0.70243616   5      0.4995139  0.4121632  0.7878733  0.3036557          
  0.83897874   3      3.5186441  0.4121769  0.7878733  0.3037031          
  0.88974979   2      2.2964565  0.4121809  0.7878733  0.3037170          
  0.90681944   7      2.2107892  0.4121821  0.7878733  0.3037213          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alphas = 0.07182432, nsteps = 5
 and scale = 1.964597.
[1] "Sat Mar 10 02:10:16 2018"
# weights:  111
initial  value 74.763085 
iter  10 value 41.841925
iter  20 value 41.811771
iter  30 value 41.702517
iter  30 value 41.702517
iter  30 value 41.702517
final  value 41.702517 
converged
# weights:  56
initial  value 71.304817 
iter  10 value 39.971135
iter  20 value 39.267556
iter  20 value 39.267556
iter  20 value 39.267556
final  value 39.267556 
converged
# weights:  155
initial  value 67.161695 
iter  10 value 40.873468
iter  20 value 40.203192
iter  30 value 33.741693
iter  40 value 33.567431
iter  50 value 33.549128
iter  60 value 33.539166
iter  70 value 33.531569
iter  80 value 33.529272
iter  90 value 33.527387
iter 100 value 33.526129
final  value 33.526129 
stopped after 100 iterations
# weights:  12
initial  value 65.342811 
iter  10 value 40.803831
iter  20 value 40.785100
iter  30 value 40.763495
iter  40 value 40.737416
iter  50 value 40.691352
iter  60 value 40.475720
iter  70 value 34.406655
iter  80 value 33.779138
iter  90 value 33.722575
iter 100 value 33.659009
final  value 33.659009 
stopped after 100 iterations
# weights:  67
initial  value 54.822433 
iter  10 value 40.762328
iter  20 value 35.713618
iter  30 value 33.734880
iter  40 value 33.535637
iter  50 value 33.496837
iter  60 value 33.488670
iter  70 value 33.485416
iter  80 value 33.483744
iter  90 value 33.482449
iter 100 value 33.481385
final  value 33.481385 
stopped after 100 iterations
# weights:  210
initial  value 73.814196 
iter  10 value 40.686336
iter  20 value 40.683429
iter  30 value 40.682921
iter  40 value 40.682074
iter  50 value 40.680416
iter  60 value 40.676079
iter  70 value 40.652802
iter  80 value 36.694847
iter  90 value 33.366655
iter 100 value 33.272049
final  value 33.272049 
stopped after 100 iterations
# weights:  166
initial  value 51.020662 
iter  10 value 40.725421
iter  20 value 40.676571
iter  30 value 33.904040
iter  40 value 33.489623
iter  50 value 33.396840
iter  60 value 33.347345
iter  70 value 33.320944
iter  80 value 33.304289
iter  90 value 33.292421
iter 100 value 33.284822
final  value 33.284822 
stopped after 100 iterations
# weights:  23
initial  value 61.870173 
iter  10 value 49.203047
iter  20 value 49.163255
iter  20 value 49.163255
iter  20 value 49.163255
final  value 49.163255 
converged
# weights:  67
initial  value 57.727020 
iter  10 value 40.969539
iter  20 value 34.603838
iter  30 value 34.079760
iter  40 value 34.065281
iter  50 value 34.058036
iter  60 value 34.057156
iter  70 value 34.056797
iter  80 value 34.056673
iter  90 value 34.056593
iter 100 value 34.056526
final  value 34.056526 
stopped after 100 iterations
# weights:  111
initial  value 65.394693 
iter  10 value 40.685309
final  value 40.685305 
converged
# weights:  199
initial  value 74.096306 
iter  10 value 40.696659
iter  20 value 40.686742
iter  30 value 40.682367
iter  40 value 40.674947
iter  50 value 40.582558
iter  60 value 33.720862
iter  70 value 33.307368
iter  80 value 33.272476
iter  90 value 33.260284
iter 100 value 33.234740
final  value 33.234740 
stopped after 100 iterations
# weights:  34
initial  value 75.034897 
iter  10 value 51.444929
iter  20 value 51.385360
final  value 51.385356 
converged
# weights:  12
initial  value 58.274073 
iter  10 value 44.041056
final  value 44.039341 
converged
# weights:  221
initial  value 57.075826 
iter  10 value 40.789594
iter  20 value 34.407735
iter  30 value 34.155398
iter  40 value 34.134653
iter  50 value 34.132183
iter  60 value 34.132005
iter  70 value 34.131984
final  value 34.131982 
converged
# weights:  166
initial  value 84.006012 
iter  10 value 38.083484
iter  20 value 34.382779
iter  30 value 34.353799
iter  40 value 34.347790
iter  50 value 34.347511
final  value 34.347503 
converged
# weights:  89
initial  value 66.829362 
iter  10 value 41.117908
iter  20 value 35.937912
iter  30 value 34.100281
iter  40 value 34.005674
iter  50 value 33.996783
iter  60 value 33.995250
iter  70 value 33.995052
iter  80 value 33.994859
iter  90 value 33.994676
iter 100 value 33.994562
final  value 33.994562 
stopped after 100 iterations
# weights:  23
initial  value 60.009593 
iter  10 value 41.005248
iter  20 value 36.041250
iter  30 value 34.267705
iter  40 value 34.180531
iter  50 value 34.178457
iter  60 value 34.177860
iter  70 value 34.177800
final  value 34.177796 
converged
# weights:  100
initial  value 53.285392 
iter  10 value 38.841605
iter  20 value 37.302811
final  value 37.302233 
converged
# weights:  144
initial  value 49.968823 
iter  10 value 40.698984
iter  20 value 36.147440
iter  30 value 33.493768
iter  40 value 33.416764
iter  50 value 33.374299
iter  60 value 33.359384
iter  70 value 33.353985
iter  80 value 33.351222
iter  90 value 33.350218
iter 100 value 33.349166
final  value 33.349166 
stopped after 100 iterations
# weights:  34
initial  value 55.576797 
iter  10 value 41.210880
final  value 41.210390 
converged
# weights:  111
initial  value 63.104995 
iter  10 value 30.772570
iter  20 value 30.550073
iter  20 value 30.550073
iter  20 value 30.550073
final  value 30.550073 
converged
# weights:  56
initial  value 60.432128 
iter  10 value 28.273888
final  value 28.214874 
converged
# weights:  155
initial  value 37.176814 
iter  10 value 29.390774
iter  20 value 24.687454
iter  30 value 22.846093
iter  40 value 22.739428
iter  50 value 22.726542
iter  60 value 22.719730
iter  70 value 22.713942
iter  80 value 22.710789
iter  90 value 22.697396
iter 100 value 22.643933
final  value 22.643933 
stopped after 100 iterations
# weights:  12
initial  value 38.659938 
iter  10 value 29.338967
iter  20 value 27.248309
iter  30 value 22.934953
iter  40 value 22.824463
iter  50 value 22.817123
iter  60 value 22.814887
final  value 22.814840 
converged
# weights:  67
initial  value 43.641326 
iter  10 value 29.357042
iter  20 value 24.541271
iter  30 value 22.823204
iter  40 value 22.717936
iter  50 value 22.690436
iter  60 value 22.680495
iter  70 value 22.672982
iter  80 value 22.667342
iter  90 value 22.665715
iter 100 value 22.665149
final  value 22.665149 
stopped after 100 iterations
# weights:  210
initial  value 60.620365 
iter  10 value 29.370375
iter  20 value 29.356790
iter  30 value 29.319360
iter  40 value 24.130476
iter  50 value 22.581530
iter  60 value 22.446729
iter  70 value 22.406768
iter  80 value 22.388684
iter  90 value 22.382684
iter 100 value 22.381596
final  value 22.381596 
stopped after 100 iterations
# weights:  166
initial  value 55.906316 
iter  10 value 29.437472
iter  20 value 29.201413
iter  30 value 23.321280
iter  40 value 22.674131
iter  50 value 22.581102
iter  60 value 22.546539
iter  70 value 22.527270
iter  80 value 22.514306
iter  90 value 22.508481
iter 100 value 22.505151
final  value 22.505151 
stopped after 100 iterations
# weights:  23
initial  value 59.032441 
iter  10 value 37.691122
final  value 37.685378 
converged
# weights:  67
initial  value 59.020222 
iter  10 value 29.696866
iter  20 value 28.867536
iter  30 value 23.356160
iter  40 value 23.254685
iter  50 value 23.155700
iter  60 value 23.108943
iter  70 value 23.086930
iter  80 value 23.082719
iter  90 value 23.080839
iter 100 value 23.080500
final  value 23.080500 
stopped after 100 iterations
# weights:  111
initial  value 52.510310 
iter  10 value 29.369158
iter  20 value 29.368885
iter  30 value 29.368585
iter  40 value 29.368241
iter  50 value 29.367814
iter  60 value 29.367235
iter  70 value 29.366359
iter  80 value 29.364815
iter  90 value 29.361322
iter 100 value 29.347788
final  value 29.347788 
stopped after 100 iterations
# weights:  199
initial  value 30.204470 
iter  10 value 29.331049
iter  20 value 23.046856
iter  30 value 22.438305
iter  40 value 22.379499
iter  50 value 22.365201
iter  60 value 22.357275
iter  70 value 22.355656
iter  80 value 22.354065
iter  90 value 22.352625
iter 100 value 22.351841
final  value 22.351841 
stopped after 100 iterations
# weights:  34
initial  value 90.307228 
iter  10 value 39.867517
final  value 39.851170 
converged
# weights:  12
initial  value 49.651600 
iter  10 value 32.695288
final  value 32.695175 
converged
# weights:  221
initial  value 69.463541 
iter  10 value 29.622749
iter  20 value 23.493407
iter  30 value 23.324039
iter  40 value 23.313589
iter  50 value 23.312383
iter  60 value 23.312312
final  value 23.312307 
converged
# weights:  166
initial  value 56.940082 
iter  10 value 29.492083
iter  20 value 24.157836
iter  30 value 24.023245
iter  40 value 24.019276
iter  50 value 24.017812
iter  60 value 24.017576
iter  70 value 24.017371
iter  80 value 24.017353
final  value 24.017353 
converged
# weights:  89
initial  value 69.200985 
iter  10 value 29.681390
iter  20 value 24.635079
iter  30 value 23.208551
iter  40 value 23.184103
iter  50 value 23.180738
iter  60 value 23.179092
iter  70 value 23.178181
iter  80 value 23.177735
iter  90 value 23.177573
iter 100 value 23.177503
final  value 23.177503 
stopped after 100 iterations
# weights:  23
initial  value 53.357722 
iter  10 value 29.552774
iter  20 value 24.655539
iter  30 value 23.508966
iter  40 value 23.372577
iter  50 value 23.337185
iter  60 value 23.326168
iter  70 value 23.324849
final  value 23.324839 
converged
# weights:  100
initial  value 65.484981 
iter  10 value 28.187644
iter  20 value 26.379713
final  value 26.379570 
converged
# weights:  144
initial  value 60.628005 
iter  10 value 29.454178
iter  20 value 29.001632
iter  30 value 22.831119
iter  40 value 22.573017
iter  50 value 22.535525
iter  60 value 22.517229
iter  70 value 22.505959
iter  80 value 22.499571
iter  90 value 22.494332
iter 100 value 22.491640
final  value 22.491640 
stopped after 100 iterations
# weights:  34
initial  value 44.158791 
iter  10 value 30.022389
final  value 30.020139 
converged
# weights:  111
initial  value 88.938441 
iter  10 value 43.222368
iter  20 value 43.020301
iter  20 value 43.020301
iter  20 value 43.020301
final  value 43.020301 
converged
# weights:  56
initial  value 54.297243 
iter  10 value 40.628478
final  value 40.446369 
converged
# weights:  155
initial  value 51.031987 
iter  10 value 41.843116
iter  20 value 40.578721
iter  30 value 34.417009
iter  40 value 34.352070
iter  50 value 34.330715
iter  60 value 34.322397
iter  70 value 34.319521
iter  80 value 34.318210
iter  90 value 34.317586
iter 100 value 34.317374
final  value 34.317374 
stopped after 100 iterations
# weights:  12
initial  value 68.957263 
iter  10 value 41.832729
iter  20 value 38.712470
iter  30 value 34.739124
iter  40 value 34.562689
iter  50 value 34.558178
iter  60 value 34.553195
iter  60 value 34.553195
iter  60 value 34.553195
final  value 34.553195 
converged
# weights:  67
initial  value 53.366614 
iter  10 value 41.834095
iter  20 value 35.072272
iter  30 value 34.450301
iter  40 value 34.424849
iter  50 value 34.418125
iter  60 value 34.414276
iter  70 value 34.412325
iter  80 value 34.411124
iter  90 value 34.409789
iter 100 value 34.407243
final  value 34.407243 
stopped after 100 iterations
# weights:  210
initial  value 58.157405 
iter  10 value 41.758356
iter  20 value 34.855119
iter  30 value 34.205780
iter  40 value 34.159663
iter  50 value 34.141369
iter  60 value 34.133688
iter  70 value 34.133096
iter  80 value 34.132230
iter  90 value 34.131629
iter 100 value 34.131354
final  value 34.131354 
stopped after 100 iterations
# weights:  166
initial  value 90.781729 
iter  10 value 41.788760
iter  20 value 41.764700
iter  30 value 41.713755
iter  40 value 36.344126
iter  50 value 34.434553
iter  60 value 34.335376
iter  70 value 34.275017
iter  80 value 34.251923
iter  90 value 34.243346
iter 100 value 34.238489
final  value 34.238489 
stopped after 100 iterations
# weights:  23
initial  value 70.564904 
iter  10 value 50.209254
iter  20 value 50.197128
iter  20 value 50.197128
iter  20 value 50.197128
final  value 50.197128 
converged
# weights:  67
initial  value 53.362003 
iter  10 value 41.971297
iter  20 value 36.074210
iter  30 value 35.016893
iter  40 value 35.008679
iter  50 value 35.005674
iter  60 value 35.004770
iter  70 value 35.004022
iter  80 value 35.003584
iter  90 value 35.003322
iter 100 value 35.003223
final  value 35.003223 
stopped after 100 iterations
# weights:  111
initial  value 67.484758 
iter  10 value 41.757309
iter  20 value 38.478082
iter  30 value 34.393465
iter  40 value 34.230843
iter  50 value 34.190747
iter  60 value 34.154357
iter  70 value 34.153888
iter  80 value 34.151713
iter  90 value 34.144629
iter 100 value 34.133608
final  value 34.133608 
stopped after 100 iterations
# weights:  199
initial  value 82.237940 
iter  10 value 41.767363
iter  20 value 41.720409
iter  30 value 39.693667
iter  40 value 34.327950
iter  50 value 34.194795
iter  60 value 34.174245
iter  70 value 34.170068
iter  80 value 34.164755
iter  90 value 34.150435
iter 100 value 34.128792
final  value 34.128792 
stopped after 100 iterations
# weights:  34
initial  value 88.722487 
iter  10 value 52.470645
iter  20 value 52.408628
iter  20 value 52.408628
iter  20 value 52.408628
final  value 52.408628 
converged
# weights:  12
initial  value 54.538444 
iter  10 value 45.163518
final  value 45.162646 
converged
# weights:  221
initial  value 58.826374 
iter  10 value 41.884516
iter  20 value 35.412603
iter  30 value 35.137182
iter  40 value 35.118446
iter  50 value 35.115383
iter  60 value 35.115143
iter  70 value 35.115114
final  value 35.115113 
converged
# weights:  166
initial  value 55.337943 
iter  10 value 41.786460
iter  20 value 35.888973
iter  30 value 35.845804
iter  40 value 35.844883
iter  50 value 35.844331
iter  60 value 35.844244
iter  70 value 35.840602
iter  80 value 35.553131
iter  90 value 35.489757
iter 100 value 35.488689
final  value 35.488689 
stopped after 100 iterations
# weights:  89
initial  value 71.051088 
iter  10 value 42.148054
iter  20 value 36.135559
iter  30 value 34.961259
iter  40 value 34.950665
iter  50 value 34.946315
iter  60 value 34.945775
iter  70 value 34.945658
iter  80 value 34.945620
iter  90 value 34.945604
final  value 34.945600 
converged
# weights:  23
initial  value 53.527022 
iter  10 value 41.774628
iter  20 value 35.540471
iter  30 value 35.129082
iter  40 value 35.113334
iter  50 value 35.102027
iter  60 value 35.099160
iter  70 value 35.097200
iter  80 value 35.096909
final  value 35.096877 
converged
# weights:  100
initial  value 62.067723 
iter  10 value 41.795937
iter  20 value 38.368087
iter  30 value 37.930424
iter  40 value 37.922729
final  value 37.922721 
converged
# weights:  144
initial  value 68.005208 
iter  10 value 41.820158
iter  20 value 37.636341
iter  30 value 35.290545
iter  40 value 34.722184
iter  50 value 34.598649
iter  60 value 34.527804
iter  70 value 34.446066
iter  80 value 34.343023
iter  90 value 34.263582
iter 100 value 34.231979
final  value 34.231979 
stopped after 100 iterations
# weights:  34
initial  value 69.714952 
iter  10 value 42.399074
final  value 42.395996 
converged
# weights:  221
initial  value 103.922263 
iter  10 value 56.180936
iter  20 value 46.975529
iter  30 value 46.068295
iter  40 value 45.979185
iter  50 value 45.958227
iter  60 value 45.955744
iter  70 value 45.955379
iter  80 value 45.955128
iter  90 value 45.955037
final  value 45.955033 
converged
Neural Network 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  size  decay         RMSE       Rsquared   MAE        Selected
   1    1.926767e-03  0.7669779  0.3521733  0.3807490          
   1    7.321145e-01  0.8535451  0.5864276  0.4870067          
   2    1.001052e-02  0.7699952  0.4011117  0.3781456          
   2    3.445798e+00  0.8995430  0.6265487  0.5412359          
   3    4.372112e-01  0.8232553  0.5437090  0.4559612          
   3    6.794005e+00  0.9234483  0.6205150  0.5709046          
   5    2.635379e-01  0.8058317  0.5100120  0.4379363          
   6    2.771781e-03  0.7704367  0.3769569  0.3872877          
   6    1.108573e-02  0.7685494  0.4089028  0.3737987          
   8    1.019226e-02  0.7679606  0.4115878  0.3733835          
   9    1.317792e-01  0.7902450  0.4792694  0.4189963          
  10    1.113913e-05  0.8298169  0.2823373  0.4655795          
  10    6.050950e-01  0.8359231  0.5694740  0.4695540          
  13    1.455864e-03  0.7715074  0.3569324  0.3961492          
  14    3.174288e-03  0.7677999  0.3854824  0.3801040          
  15    1.161836e-03  0.7701658  0.3728676  0.3857160          
  15    2.882011e-02  0.7698428  0.4367707  0.3752140          
  18    1.820930e-04  0.7689351  0.3401651  0.3909184          
  19    2.443203e-05  0.7739409  0.3192504  0.4041588          
  20    2.209179e-02  0.7659196  0.4280609  0.3660875  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 20 and decay = 0.02209179.
[1] "Sat Mar 10 02:11:02 2018"
Non-Negative Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results:

  RMSE       Rsquared   MAE      
  0.4750796  0.7480785  0.3227445

[1] "Sat Mar 10 02:11:14 2018"
Non-Informative Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results:

  RMSE      Rsquared  MAE      
  0.841155  NaN       0.5052625

[1] "Sat Mar 10 02:11:26 2018"
Parallel Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE         Selected
  1     0.7213889  0.4035451  0.42539698          
  2     0.6233656  0.6444363  0.36122057          
  3     0.5409847  0.7765515  0.29498473          
  4     0.4742380  0.8219564  0.25008665          
  5     0.4089231  0.8756295  0.20179614          
  6     0.3472361  0.9037736  0.15543272          
  7     0.3062349  0.9185435  0.13168538          
  8     0.2633457  0.9382191  0.10791466          
  9     0.2480118  0.9433502  0.09682643  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 02:11:43 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: Warning messages:
1: package 'mxnet' is not available (for R version 3.4.3) 
2: package 'mxnet' is not available (for R version 3.4.3) 
3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
# weights:  111
initial  value 73.849514 
iter  10 value 41.509701
final  value 41.490812 
converged
# weights:  56
initial  value 71.541973 
iter  10 value 39.140002
final  value 39.116153 
converged
# weights:  155
initial  value 68.727395 
iter  10 value 40.882508
iter  20 value 39.741860
iter  30 value 33.759498
iter  40 value 33.557022
iter  50 value 33.541304
iter  60 value 33.523721
iter  70 value 33.512088
iter  80 value 33.507228
iter  90 value 33.504534
iter 100 value 33.486631
final  value 33.486631 
stopped after 100 iterations
# weights:  12
initial  value 64.540730 
iter  10 value 40.799634
iter  20 value 40.784697
iter  30 value 40.768430
iter  40 value 40.750505
iter  50 value 40.724897
iter  60 value 40.669430
iter  70 value 40.213375
iter  80 value 34.022585
iter  90 value 33.723111
iter 100 value 33.657770
final  value 33.657770 
stopped after 100 iterations
# weights:  67
initial  value 55.036385 
iter  10 value 40.766372
iter  20 value 34.144107
iter  30 value 33.551993
iter  40 value 33.513340
iter  50 value 33.480976
iter  60 value 33.473376
iter  70 value 33.470960
iter  80 value 33.469965
iter  90 value 33.469307
iter 100 value 33.468697
final  value 33.468697 
stopped after 100 iterations
# weights:  210
initial  value 73.756029 
iter  10 value 40.686313
iter  20 value 40.685283
iter  30 value 40.685278
iter  40 value 40.685273
iter  50 value 40.685268
iter  60 value 40.685262
iter  70 value 40.685256
iter  80 value 40.685250
iter  90 value 40.685243
iter 100 value 40.685237
final  value 40.685237 
stopped after 100 iterations
# weights:  166
initial  value 51.431122 
iter  10 value 40.724357
iter  20 value 36.239457
iter  30 value 33.464478
iter  40 value 33.381325
iter  50 value 33.338487
iter  60 value 33.322561
iter  70 value 33.316514
iter  80 value 33.312847
iter  90 value 33.311244
iter 100 value 33.309868
final  value 33.309868 
stopped after 100 iterations
# weights:  23
initial  value 61.740164 
iter  10 value 49.114387
iter  20 value 49.108107
iter  20 value 49.108106
iter  20 value 49.108106
final  value 49.108106 
converged
# weights:  67
initial  value 58.553984 
iter  10 value 40.985409
iter  20 value 35.366769
iter  30 value 34.142056
iter  40 value 34.043094
iter  50 value 34.035935
iter  60 value 34.032469
iter  70 value 34.030920
iter  80 value 34.030605
iter  90 value 34.030317
iter 100 value 34.030193
final  value 34.030193 
stopped after 100 iterations
# weights:  111
initial  value 65.321464 
iter  10 value 40.685360
final  value 40.685350 
converged
# weights:  199
initial  value 73.216353 
iter  10 value 40.696898
iter  20 value 40.686474
iter  30 value 40.668909
iter  40 value 40.045543
iter  50 value 33.393540
iter  60 value 33.303673
iter  70 value 33.275247
iter  80 value 33.252987
iter  90 value 33.234339
iter 100 value 33.213492
final  value 33.213492 
stopped after 100 iterations
# weights:  34
initial  value 75.281140 
iter  10 value 51.420672
iter  20 value 51.361014
final  value 51.361012 
converged
# weights:  12
initial  value 58.625145 
iter  10 value 43.946192
final  value 43.946006 
converged
# weights:  221
initial  value 54.538145 
iter  10 value 40.767406
iter  20 value 34.326027
iter  30 value 34.119814
iter  40 value 34.097217
iter  50 value 34.094053
iter  60 value 34.093805
iter  70 value 34.093789
iter  70 value 34.093788
iter  70 value 34.093788
final  value 34.093788 
converged
# weights:  166
initial  value 84.455532 
iter  10 value 39.053674
iter  20 value 34.404448
iter  30 value 34.328669
iter  40 value 34.310317
iter  50 value 34.309347
iter  60 value 34.309297
iter  60 value 34.309297
iter  60 value 34.309297
final  value 34.309297 
converged
# weights:  89
initial  value 67.238871 
iter  10 value 41.133588
iter  20 value 36.581911
iter  30 value 34.017780
iter  40 value 33.986710
iter  50 value 33.975306
iter  60 value 33.972144
iter  70 value 33.971566
iter  80 value 33.971324
iter  90 value 33.971184
iter 100 value 33.971115
final  value 33.971115 
stopped after 100 iterations
# weights:  23
initial  value 60.087084 
iter  10 value 40.990409
iter  20 value 35.891361
iter  30 value 34.413004
iter  40 value 34.173277
iter  50 value 34.161110
iter  60 value 34.160373
iter  70 value 34.159117
iter  80 value 34.157997
iter  90 value 34.157913
final  value 34.157906 
converged
# weights:  100
initial  value 53.054358 
iter  10 value 37.387695
iter  20 value 37.178753
final  value 37.178744 
converged
# weights:  144
initial  value 48.786767 
iter  10 value 40.696213
iter  20 value 36.981739
iter  30 value 33.646079
iter  40 value 33.498758
iter  50 value 33.397356
iter  60 value 33.346416
iter  70 value 33.316039
iter  80 value 33.303028
iter  90 value 33.295736
iter 100 value 33.289421
final  value 33.289421 
stopped after 100 iterations
# weights:  34
initial  value 56.304822 
iter  10 value 41.058792
final  value 41.055792 
converged
# weights:  111
initial  value 64.888586 
iter  10 value 30.467436
iter  20 value 30.372534
final  value 30.372533 
converged
# weights:  56
initial  value 57.719126 
iter  10 value 28.128570
final  value 28.042948 
converged
# weights:  155
initial  value 34.727715 
iter  10 value 29.353184
iter  20 value 23.010713
iter  30 value 22.655339
iter  40 value 22.608886
iter  50 value 22.588084
iter  60 value 22.579039
iter  70 value 22.576577
iter  80 value 22.575718
iter  90 value 22.575286
iter 100 value 22.575090
final  value 22.575090 
stopped after 100 iterations
# weights:  12
initial  value 38.779844 
iter  10 value 29.135787
iter  20 value 23.209808
iter  30 value 22.855342
iter  40 value 22.818774
iter  50 value 22.814420
iter  60 value 22.812641
iter  60 value 22.812641
iter  60 value 22.812641
final  value 22.812641 
converged
# weights:  67
initial  value 47.306934 
iter  10 value 29.403685
iter  20 value 24.675555
iter  30 value 22.977470
iter  40 value 22.840394
iter  50 value 22.732724
iter  60 value 22.681818
iter  70 value 22.664531
iter  80 value 22.658637
iter  90 value 22.655400
iter 100 value 22.653733
final  value 22.653733 
stopped after 100 iterations
# weights:  210
initial  value 53.297189 
iter  10 value 29.370579
iter  20 value 28.167557
iter  30 value 22.703795
iter  40 value 22.542031
iter  50 value 22.426387
iter  60 value 22.392299
iter  70 value 22.386538
iter  80 value 22.382352
iter  90 value 22.381174
iter 100 value 22.380452
final  value 22.380452 
stopped after 100 iterations
# weights:  166
initial  value 49.179147 
iter  10 value 29.430369
iter  20 value 29.271692
iter  30 value 23.317046
iter  40 value 22.686433
iter  50 value 22.608096
iter  60 value 22.558952
iter  70 value 22.528867
iter  80 value 22.513784
iter  90 value 22.503481
iter 100 value 22.499488
final  value 22.499488 
stopped after 100 iterations
# weights:  23
initial  value 59.858362 
iter  10 value 37.639848
final  value 37.634959 
converged
# weights:  67
initial  value 59.506404 
iter  10 value 29.700300
iter  20 value 24.380086
iter  30 value 23.241604
iter  40 value 23.222012
iter  50 value 23.211408
iter  60 value 23.208677
iter  70 value 23.208287
iter  80 value 23.208221
iter  90 value 23.208186
iter 100 value 23.208175
final  value 23.208175 
stopped after 100 iterations
# weights:  111
initial  value 54.469319 
iter  10 value 29.369171
iter  20 value 29.369006
iter  30 value 29.368833
iter  40 value 29.368638
iter  50 value 29.368402
iter  60 value 29.368096
iter  70 value 29.367659
iter  80 value 29.366946
iter  90 value 29.365518
iter 100 value 29.361334
final  value 29.361334 
stopped after 100 iterations
# weights:  199
initial  value 31.874727 
iter  10 value 29.383503
iter  20 value 29.383363
iter  30 value 29.383219
iter  40 value 29.383072
iter  50 value 29.382919
iter  60 value 29.382759
iter  70 value 29.382590
iter  80 value 29.382409
iter  90 value 29.382210
iter 100 value 29.381986
final  value 29.381986 
stopped after 100 iterations
# weights:  34
initial  value 92.209105 
iter  10 value 39.883982
final  value 39.829240 
converged
# weights:  12
initial  value 50.168656 
iter  10 value 32.593128
final  value 32.592502 
converged
# weights:  221
initial  value 76.326911 
iter  10 value 30.176423
iter  20 value 25.623243
iter  30 value 23.744361
iter  40 value 23.740026
iter  50 value 23.604498
iter  60 value 23.449475
iter  70 value 23.446463
iter  80 value 23.446047
iter  90 value 23.445821
iter  90 value 23.445820
iter 100 value 23.441048
final  value 23.441048 
stopped after 100 iterations
# weights:  166
initial  value 54.564061 
iter  10 value 29.452992
iter  20 value 24.147306
iter  30 value 23.959398
iter  40 value 23.953536
iter  50 value 23.952618
iter  60 value 23.952453
final  value 23.952447 
converged
# weights:  89
initial  value 67.785167 
iter  10 value 29.716579
iter  20 value 23.980339
iter  30 value 23.173937
iter  40 value 23.154775
iter  50 value 23.152344
iter  60 value 23.151716
iter  70 value 23.151614
iter  80 value 23.151571
iter  90 value 23.151563
iter 100 value 23.151555
final  value 23.151555 
stopped after 100 iterations
# weights:  23
initial  value 52.921659 
iter  10 value 29.506214
iter  20 value 24.413870
iter  30 value 23.368972
iter  40 value 23.332618
iter  50 value 23.317051
iter  60 value 23.309176
iter  70 value 23.307187
iter  80 value 23.307139
final  value 23.307138 
converged
# weights:  100
initial  value 63.506966 
iter  10 value 27.372429
iter  20 value 26.236979
final  value 26.236724 
converged
# weights:  144
initial  value 59.352526 
iter  10 value 29.448004
iter  20 value 26.616538
iter  30 value 23.248959
iter  40 value 22.865013
iter  50 value 22.717530
iter  60 value 22.625047
iter  70 value 22.571785
iter  80 value 22.549856
iter  90 value 22.542759
iter 100 value 22.538488
final  value 22.538488 
stopped after 100 iterations
# weights:  34
initial  value 45.417628 
iter  10 value 29.849801
final  value 29.849252 
converged
# weights:  111
initial  value 87.297589 
iter  10 value 42.948376
iter  20 value 42.823851
final  value 42.823849 
converged
# weights:  56
initial  value 53.832048 
iter  10 value 40.236962
final  value 40.221853 
converged
# weights:  155
initial  value 51.964648 
iter  10 value 41.853584
iter  20 value 35.150123
iter  30 value 34.516884
iter  40 value 34.460185
iter  50 value 34.448374
iter  60 value 34.441194
iter  70 value 34.436494
iter  80 value 34.432903
iter  90 value 34.430654
iter 100 value 34.429176
final  value 34.429176 
stopped after 100 iterations
# weights:  12
initial  value 69.096051 
iter  10 value 41.847722
iter  20 value 41.502399
iter  30 value 35.328844
iter  40 value 34.601182
iter  50 value 34.561873
iter  60 value 34.555264
iter  70 value 34.550545
final  value 34.550458 
converged
# weights:  67
initial  value 53.863517 
iter  10 value 41.834273
iter  20 value 35.214672
iter  30 value 34.400107
iter  40 value 34.357688
iter  50 value 34.349527
iter  60 value 34.346617
iter  70 value 34.345160
iter  80 value 34.344302
iter  90 value 34.343828
iter 100 value 34.343606
final  value 34.343606 
stopped after 100 iterations
# weights:  210
initial  value 60.820176 
iter  10 value 41.758321
iter  20 value 36.230779
iter  30 value 34.226556
iter  40 value 34.163615
iter  50 value 34.153787
iter  60 value 34.141175
iter  70 value 34.139273
iter  80 value 34.137614
iter  90 value 34.136078
iter 100 value 34.135843
final  value 34.135843 
stopped after 100 iterations
# weights:  166
initial  value 91.590559 
iter  10 value 41.785909
iter  20 value 41.762228
iter  30 value 41.680245
iter  40 value 34.908010
iter  50 value 34.351573
iter  60 value 34.278408
iter  70 value 34.260870
iter  80 value 34.248594
iter  90 value 34.231621
iter 100 value 34.228931
final  value 34.228931 
stopped after 100 iterations
# weights:  23
initial  value 71.386348 
iter  10 value 50.246531
iter  20 value 50.139803
iter  20 value 50.139803
iter  20 value 50.139803
final  value 50.139803 
converged
# weights:  67
initial  value 54.673094 
iter  10 value 41.962135
iter  20 value 36.076443
iter  30 value 34.992155
iter  40 value 34.973533
iter  50 value 34.970966
iter  60 value 34.969835
iter  70 value 34.969192
iter  80 value 34.968918
iter  90 value 34.968790
final  value 34.968779 
converged
# weights:  111
initial  value 66.522299 
iter  10 value 41.757325
iter  20 value 41.709626
iter  30 value 35.720498
iter  40 value 34.225339
iter  50 value 34.101411
iter  60 value 34.083481
iter  70 value 34.082599
iter  80 value 34.082398
iter  90 value 34.082291
iter  90 value 34.082290
iter  90 value 34.082290
final  value 34.082290 
converged
# weights:  199
initial  value 79.637199 
iter  10 value 41.768419
iter  20 value 37.945589
iter  30 value 34.450932
iter  40 value 34.183938
iter  50 value 34.149319
iter  60 value 34.124228
iter  70 value 34.110366
iter  80 value 34.106555
iter  90 value 34.105257
iter 100 value 34.104377
final  value 34.104377 
stopped after 100 iterations
# weights:  34
initial  value 88.422671 
iter  10 value 52.430152
final  value 52.382827 
converged
# weights:  12
initial  value 54.694957 
iter  10 value 45.038958
final  value 45.038272 
converged
# weights:  221
initial  value 56.952410 
iter  10 value 41.816269
iter  20 value 35.151246
iter  30 value 35.028531
iter  40 value 35.019519
iter  50 value 35.018593
iter  60 value 35.018551
final  value 35.018549 
converged
# weights:  166
initial  value 53.235530 
iter  10 value 41.381935
iter  20 value 35.835192
iter  30 value 35.766150
iter  40 value 35.764005
iter  50 value 35.763374
iter  60 value 35.763062
iter  70 value 35.763031
iter  80 value 35.763024
final  value 35.763021 
converged
# weights:  89
initial  value 69.917579 
iter  10 value 42.110695
iter  20 value 35.545777
iter  30 value 34.984150
iter  40 value 34.915406
iter  50 value 34.911654
iter  60 value 34.911035
iter  70 value 34.910917
iter  80 value 34.910865
iter  90 value 34.910839
iter 100 value 34.910579
final  value 34.910579 
stopped after 100 iterations
# weights:  23
initial  value 52.067118 
iter  10 value 41.768970
iter  20 value 35.787699
iter  30 value 35.258167
iter  40 value 35.102504
iter  50 value 35.079686
iter  60 value 35.076871
iter  70 value 35.075121
iter  80 value 35.074231
iter  90 value 35.074156
final  value 35.074150 
converged
# weights:  100
initial  value 60.590483 
iter  10 value 41.122154
iter  20 value 38.244098
final  value 38.243765 
converged
# weights:  144
initial  value 66.819701 
iter  10 value 41.821598
iter  20 value 35.599005
iter  30 value 34.377564
iter  40 value 34.280589
iter  50 value 34.240607
iter  60 value 34.230714
iter  70 value 34.226412
iter  80 value 34.223998
iter  90 value 34.221980
iter 100 value 34.221315
final  value 34.221315 
stopped after 100 iterations
# weights:  34
initial  value 68.935184 
iter  10 value 42.178271
final  value 42.175582 
converged
# weights:  221
initial  value 98.649597 
iter  10 value 56.210836
iter  20 value 48.947096
iter  30 value 46.151965
iter  40 value 45.932815
iter  50 value 45.899824
iter  60 value 45.892688
iter  70 value 45.891722
iter  80 value 45.891504
iter  90 value 45.891414
iter 100 value 45.891344
final  value 45.891344 
stopped after 100 iterations
Neural Networks with Feature Extraction 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  size  decay         RMSE       Rsquared   MAE        Selected
   1    1.926767e-03  0.7673669  0.3529732  0.3818433          
   1    7.321145e-01  0.8520655  0.5831480  0.4853660          
   2    1.001052e-02  0.7721966  0.3950419  0.3796690          
   2    3.445798e+00  0.8982807  0.6387067  0.5402134          
   3    4.372112e-01  0.8213791  0.5386525  0.4533478          
   3    6.794005e+00  0.9227862  0.6372408  0.5703110          
   5    2.635379e-01  0.8044166  0.5056506  0.4358420          
   6    2.771781e-03  0.7733692  0.3643392  0.3926610          
   6    1.108573e-02  0.7710819  0.4030458  0.3776492          
   8    1.019226e-02  0.7695131  0.4052853  0.3748904          
   9    1.317792e-01  0.7907115  0.4751330  0.4186929          
  10    1.113913e-05  0.8247009  0.3788878  0.4553841          
  10    6.050950e-01  0.8324680  0.5636705  0.4662314          
  13    1.455864e-03  0.7693298  0.3691513  0.3846670          
  14    3.174288e-03  0.7728809  0.3779195  0.3895301          
  15    1.161836e-03  0.7764073  0.3470486  0.4036813          
  15    2.882011e-02  0.7700708  0.4357507  0.3753281          
  18    1.820930e-04  0.7950184  0.2743710  0.4233143          
  19    2.443203e-05  0.8066419  0.4190546  0.4428059          
  20    2.209179e-02  0.7662367  0.4264602  0.3680974  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 20 and decay = 0.02209179.
[1] "Sat Mar 10 02:11:58 2018"
Principal Component Analysis 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared    MAE        Selected
  1      0.8242952  0.07463476  0.5110917          
  2      0.7824626  0.18599640  0.4811248          
  3      0.7215823  0.27599763  0.4461409          
  4      0.6763881  0.35600540  0.4419490          
  5      0.5857750  0.53791147  0.4074728          
  6      0.5846829  0.52853038  0.4113093  *       
  8      0.6324385  0.45795570  0.4398848          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 6.
[1] "Sat Mar 10 02:12:11 2018"
____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.159394118050113) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.147361438535154) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 7 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 7 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0129319851752371) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 7 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 7 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.00156170753762126) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 7 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 7 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 7 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 7 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.137328227190301) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 7 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.159394118050113) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.147361438535154) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0833882110659033) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0814252960961312) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0129319851752371) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0688381643034518) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.101492141606286) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.00156170753762126) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.111474365182221) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.115323185734451) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.100275679072365) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.137328227190301) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0721040236297995) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.159394118050113) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.147361438535154) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 8 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 8 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0129319851752371) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 8 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 8 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.00156170753762126) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 8 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 8 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.115323185734451) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 8 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.137328227190301) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 8 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0420097786933184) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

Partial Least Squares Generalized Linear Models  

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  nt  alpha.pvals.expli  RMSE       Rsquared   MAE        Selected
  1   0.076160975        0.5115417  0.6573505  0.3262526          
  1   0.100015219        0.5115417  0.6573505  0.3262526          
  1   0.162152634        0.5115417  0.6573505  0.3262526          
  1   0.184576326        0.5349366  0.6178822  0.3533112          
  2   0.154689708        0.4264974  0.7759520  0.3106216          
  2   0.194404196        0.4370000  0.7610302  0.3225069          
  3   0.081425296        0.4257634  0.7683736  0.3053468          
  3   0.101492142        0.4215310  0.7749201  0.3040867          
  3   0.147361439        0.4252006  0.7774401  0.3086483          
  4   0.100275679        0.4215310  0.7749201  0.3040867          
  4   0.137328227        0.4252006  0.7774401  0.3086483          
  5   0.001561708        0.4597160  0.7339313  0.3279224          
  5   0.159394118        0.4264974  0.7759520  0.3106216          
  6   0.072104024        0.4257634  0.7683736  0.3053468          
  7   0.068838164        0.4257634  0.7683736  0.3053468          
  7   0.083388211        0.4257634  0.7683736  0.3053468          
  7   0.115323186        0.4496635  0.7470567  0.3219897          
  8   0.042009779        0.4188341  0.7770565  0.3093083  *       
  9   0.012931985        0.4429480  0.7573864  0.3188510          
  9   0.111474365        0.4215310  0.7749201  0.3040867          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nt = 8 and alpha.pvals.expli
 = 0.04200978.
[1] "Sat Mar 10 02:12:48 2018"
Projection Pursuit Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  nterms  RMSE       Rsquared   MAE         Selected
   1      0.2406015  0.9590578  0.10280590          
   2      0.2278963  0.9609346  0.09315980          
   3      0.2270080  0.9617423  0.09487507          
   4      0.2268382  0.9615530  0.09225679          
   5      0.2252506  0.9610878  0.09235380          
   6      0.2231249  0.9615117  0.09151528          
   7      0.2224561  0.9615681  0.09094542          
   8      0.2210117  0.9618868  0.09047597  *       
   9      0.2214792  0.9618579  0.09072154          
  10      0.2215858  0.9618149  0.09108141          
  11      0.2216014  0.9618297  0.09115584          
  12      0.2216304  0.9618472  0.09103492          
  13      0.2216209  0.9618717  0.09098064          
  14      0.2215510  0.9618936  0.09092195          
  15      0.2216072  0.9618555  0.09095643          
  16      0.2215977  0.9618482  0.09095588          
  17      0.2215927  0.9618498  0.09095416          
  18      0.2215925  0.9618479  0.09094951          
  19      0.2215947  0.9618472  0.09095619          
  20      0.2215979  0.9618469  0.09095700          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nterms = 8.
[1] "Sat Mar 10 02:13:01 2018"
Quantile Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE        Selected
  1     1.1400368  0.2961824  0.9119814          
  2     1.1701493  0.4217728  0.9055031          
  3     1.0309210  0.5279991  0.7604973          
  4     1.0365777  0.5493363  0.7208026          
  5     1.0210563  0.5973283  0.6774403          
  6     0.9190619  0.6457007  0.5584528          
  7     0.8804396  0.6670731  0.5010694          
  8     0.8352379  0.6999317  0.4306100          
  9     0.6848171  0.7907301  0.3489692  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 02:13:36 2018"
Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  min.node.size  mtry  splitrule   RMSE       Rsquared   MAE        Selected
   1             4     extratrees  0.4592676  0.8502752  0.2241818          
   1             8     extratrees  0.3000542  0.9218624  0.1328682          
   2             5     extratrees  0.4188453  0.8751122  0.1961822          
   2             9     variance    0.2473662  0.9403565  0.0940024  *       
   3             7     maxstat     0.4049458  0.8175776  0.1804612          
   3             9     variance    0.2479990  0.9423469  0.0932242          
   5             7     maxstat     0.4074121  0.8143169  0.1869077          
   6             4     maxstat     0.5770349  0.6696997  0.2734382          
   6             5     variance    0.4046576  0.8779067  0.1970180          
   8             5     variance    0.4258862  0.8613597  0.2133911          
   9             7     maxstat     0.4332248  0.7831211  0.1974647          
  10             1     variance    0.7386750  0.3442418  0.4440610          
  10             8     extratrees  0.3341749  0.8989578  0.1529993          
  13             4     maxstat     0.6305859  0.5617349  0.3051494          
  14             4     extratrees  0.5326526  0.8183402  0.2613350          
  15             4     variance    0.5154143  0.7815747  0.2791444          
  15             6     variance    0.3801813  0.8897846  0.1844305          
  18             2     maxstat     0.7322863  0.4088448  0.3817418          
  19             1     extratrees  0.7323141  0.6919374  0.4177575          
  20             6     extratrees  0.4754021  0.8288924  0.2229975          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 9, splitrule = variance
 and min.node.size = 2.
[1] "Sat Mar 10 02:13:53 2018"
Error in code$varImp(object$finalModel, ...) : 
  No importance values available
In addition: Warning messages:
1: package 'gpls' is not available (for R version 3.4.3) 
2: package 'rPython' is not available (for R version 3.4.3) 
Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  predFixed  minNode  RMSE       Rsquared   MAE         Selected
  1           8       0.7259543  0.3908366  0.43174658          
  1          11       0.7297574  0.3829830  0.44086275          
  1          17       0.7534256  0.3292797  0.45752408          
  1          19       0.7501927  0.3497271  0.45444894          
  2          16       0.6682602  0.5443950  0.40079792          
  2          20       0.6795743  0.5513787  0.40978992          
  3           9       0.5706290  0.7075028  0.31902503          
  3          11       0.5700022  0.7102269  0.31740068          
  3          15       0.5743844  0.7215383  0.33268114          
  4          11       0.5048587  0.7869873  0.27303614          
  4          14       0.5055505  0.7802186  0.27704219          
  5           1       0.3621577  0.9042565  0.16960690          
  5          16       0.4533479  0.8447561  0.23744342          
  6           8       0.3569957  0.8876048  0.16515165          
  7           7       0.2956848  0.9129178  0.12153879          
  7           9       0.3085500  0.9106359  0.13041428          
  7          12       0.3198395  0.9118942  0.13831752          
  8           5       0.2596735  0.9421105  0.10204525          
  9           2       0.1936182  0.9666147  0.05994849  *       
  9          12       0.2583536  0.9262995  0.10029206          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were predFixed = 9 and minNode = 2.
[1] "Sat Mar 10 02:16:36 2018"
Relaxed Lasso 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  phi          lambda       RMSE       Rsquared   MAE        Selected
  0.007808538   1.08913894  0.4903922  0.7318379  0.3343448          
  0.064659926  31.75257136  0.8494033        NaN  0.4810733          
  0.210048893  21.12057031  0.8494033        NaN  0.4810733          
  0.344190822   7.05486271  0.4309050  0.7878733  0.2928228          
  0.360520118   2.99674993  0.4500846  0.7677516  0.3027097          
  0.380804874   0.04457744  0.4830528  0.7357088  0.3316320          
  0.407126480   0.29578758  0.4815362  0.7374791  0.3291823          
  0.416941055   4.97711352  0.4391596  0.7800779  0.2912483          
  0.500076094   0.06437384  0.4805579  0.7376552  0.3299125          
  0.501378395   0.60327799  0.4794190  0.7398696  0.3262694          
  0.507460708   0.22896836  0.4790953  0.7395649  0.3273815          
  0.557371826  36.41769050  0.8494033        NaN  0.4810733          
  0.576615929   6.53825391  0.4303236  0.7878733  0.2834301  *       
  0.686641136   0.85861984  0.4750302  0.7436919  0.3229533          
  0.736807193   0.20279419  0.4737261  0.7441936  0.3232653          
  0.773448540   0.09526610  0.4740350  0.7431894  0.3263705          
  0.796970590   1.24554473  0.4644444  0.7540301  0.3108364          
  0.810763171   0.05085568  0.4740415  0.7430432  0.3255059          
  0.922881632   0.07065458  0.4718269  0.7448890  0.3240234          
  0.972020978   0.08429723  0.4701002  0.7464176  0.3237949          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 6.538254 and phi = 0.5766159.
[1] "Sat Mar 10 02:16:50 2018"
Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE         Selected
  1     0.7195303  0.4124748  0.43156373          
  2     0.6065488  0.6686950  0.35420318          
  3     0.5240655  0.7770817  0.29128829          
  4     0.4806052  0.8179235  0.25825259          
  5     0.3978391  0.8861779  0.19906218          
  6     0.3510716  0.9035937  0.15693579          
  7     0.3174161  0.9148001  0.13592594          
  8     0.2743839  0.9308384  0.11195810          
  9     0.2538157  0.9418845  0.09705155  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 02:17:06 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Ridge Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  lambda        RMSE       Rsquared   MAE        Selected
  1.397962e-05  0.4817281  0.7225828  0.3427638          
  1.798371e-05  0.4817278  0.7225829  0.3427635          
  2.822008e-05  0.4817272  0.7225831  0.3427627          
  3.371643e-05  0.4817268  0.7225832  0.3427623          
  4.724992e-05  0.4817259  0.7225835  0.3427613          
  5.969654e-05  0.4817251  0.7225838  0.3427603          
  2.530122e-04  0.4817126  0.7225878  0.3427454          
  3.190915e-04  0.4817084  0.7225892  0.3427404          
  5.205726e-04  0.4816954  0.7225933  0.3427249          
  2.033101e-03  0.4815990  0.7226235  0.3426092          
  3.991708e-03  0.4814767  0.7226597  0.3424609          
  6.289004e-03  0.4813370  0.7226982  0.3422891          
  8.127854e-03  0.4812280  0.7227260  0.3421532          
  4.353213e-02  0.4795525  0.7228193  0.3400224          
  1.148055e-01  0.4779084  0.7213551  0.3393325          
  1.933945e-01  0.4776422  0.7185596  0.3399579  *       
  2.236528e-01  0.4777981  0.7173488  0.3403723          
  1.819107e+00  0.5044954  0.6767710  0.3696673          
  3.965810e+00  0.5226786  0.6601187  0.3875551          
  5.153824e+00  0.5282158  0.6556680  0.3926110          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was lambda = 0.1933945.
[1] "Sat Mar 10 02:17:19 2018"
Robust Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  intercept  psi           RMSE       Rsquared   MAE        Selected
  FALSE      psi.huber     0.4805228  0.7553559  0.2794214          
  FALSE      psi.hampel    0.4791358  0.7668735  0.2736979          
  FALSE      psi.bisquare  0.5589809  0.7634378  0.2571437          
   TRUE      psi.huber     0.4645301  0.7558039  0.2705196          
   TRUE      psi.hampel    0.4599181  0.7664041  0.2753432  *       
   TRUE      psi.bisquare  0.5358511  0.7754198  0.2495346          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were intercept = TRUE and psi = psi.hampel.
[1] "Sat Mar 10 02:17:32 2018"
CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  cp           RMSE       Rsquared   MAE        Selected
  0.000000000  0.4345955  0.7562182  0.2444730  *       
  0.002537292  0.4348497  0.7558618  0.2475807          
  0.017932421  0.4510349  0.7353710  0.2915409          
  0.062922604  0.4713248  0.7145172  0.3144994          
  0.156432803  0.5442553  0.6055993  0.3547234          
  0.566340726  0.7599060  0.4356413  0.4848988          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.
[1] "Sat Mar 10 02:17:45 2018"
CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results:

  RMSE       Rsquared  MAE      
  0.4375177  0.751644  0.2550466

[1] "Sat Mar 10 02:17:57 2018"
CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  maxdepth  RMSE       Rsquared   MAE        Selected
  1         0.5714356  0.5659736  0.3795837          
  2         0.4713248  0.7145172  0.3144994          
  3         0.4441793  0.7453209  0.2779445          
  4         0.4375177  0.7516440  0.2550466  *       
  5         0.4375177  0.7516440  0.2550466          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was maxdepth = 4.
[1] "Sat Mar 10 02:18:10 2018"
Quantile Regression with LASSO penalty 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  lambda        RMSE       Rsquared   MAE        Selected
  1.397962e-05  0.4854477  0.7564929  0.2658730          
  1.798371e-05  0.4854477  0.7564929  0.2658730          
  2.822008e-05  0.4854477  0.7564929  0.2658730          
  3.371643e-05  0.4854477  0.7564929  0.2658730          
  4.724992e-05  0.4854477  0.7564929  0.2658730          
  5.969654e-05  0.4854477  0.7564929  0.2658730          
  2.530122e-04  0.4856741  0.7560325  0.2656893          
  3.190915e-04  0.4856741  0.7560325  0.2656893          
  5.205726e-04  0.4856741  0.7560325  0.2656893          
  2.033101e-03  0.4818825  0.7638195  0.2587184  *       
  3.991708e-03  0.4880591  0.7592859  0.2555882          
  6.289004e-03  0.4890718  0.7586012  0.2549059          
  8.127854e-03  0.4904156  0.7566622  0.2551683          
  4.353213e-02  0.5085871  0.7799972  0.2431688          
  1.148055e-01  0.5596126  0.7840410  0.2472337          
  1.933945e-01  0.5956527  0.7878733  0.2622946          
  2.236528e-01  0.6227106  0.7878733  0.2762944          
  1.819107e+00  0.8479415        NaN  0.4813800          
  3.965810e+00  0.8479415        NaN  0.4813800          
  5.153824e+00  0.8479415        NaN  0.4813800          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was lambda = 0.002033101.
[1] "Sat Mar 10 02:18:23 2018"
Non-Convex Penalized Quantile Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  lambda        penalty  RMSE       Rsquared   MAE        Selected
  1.397962e-05  MCP      0.4854477  0.7564929  0.2658730          
  1.798371e-05  SCAD     0.4854477  0.7564929  0.2658730          
  2.822008e-05  SCAD     0.4854477  0.7564929  0.2658730          
  3.371643e-05  SCAD     0.4854477  0.7564929  0.2658730          
  4.724992e-05  SCAD     0.4854477  0.7564929  0.2658730          
  5.969654e-05  SCAD     0.4854477  0.7564929  0.2658730          
  2.530122e-04  SCAD     0.4854477  0.7564929  0.2658730          
  3.190915e-04  SCAD     0.4856741  0.7560325  0.2656893          
  5.205726e-04  MCP      0.4856741  0.7560325  0.2656893          
  2.033101e-03  SCAD     0.4821275  0.7605225  0.2623562          
  3.991708e-03  SCAD     0.4827650  0.7596049  0.2629255          
  6.289004e-03  MCP      0.4818204  0.7588340  0.2620591          
  8.127854e-03  SCAD     0.4874965  0.7590670  0.2563289          
  4.353213e-02  MCP      0.4896685  0.7739891  0.2436849          
  1.148055e-01  MCP      0.4729247  0.7864728  0.2340333  *       
  1.933945e-01  SCAD     0.5663711  0.7878733  0.2476584          
  2.236528e-01  MCP      0.5657160  0.7878733  0.2473859          
  1.819107e+00  MCP      0.8479415        NaN  0.4813800          
  3.965810e+00  MCP      0.8479415        NaN  0.4813800          
  5.153824e+00  SCAD     0.8479415        NaN  0.4813800          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 0.1148055 and penalty = MCP.
[1] "Sat Mar 10 02:18:36 2018"
Regularized Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  mtry  coefReg      coefImp     RMSE       Rsquared   MAE         Selected
  1     0.380804874  0.45722585  0.7183499  0.4254899  0.42574370          
  1     0.500076094  0.65147371  0.7204802  0.4288674  0.43034914          
  1     0.810763171  0.44344063  0.7237023  0.4059897  0.43179702          
  1     0.922881632  0.04829887  0.7158486  0.4311865  0.42730559          
  2     0.773448540  0.73628797  0.6226358  0.6606667  0.35858517          
  2     0.972020978  0.10322795  0.6193768  0.6485098  0.35731164          
  3     0.407126480  0.74753413  0.5434525  0.7504475  0.30082455          
  3     0.507460708  0.25210437  0.5381722  0.7647813  0.29934713          
  3     0.736807193  0.99788356  0.5863235  0.7216102  0.33829312          
  4     0.501378395  0.24573965  0.4714633  0.8346620  0.24770109          
  4     0.686641136  0.97481985  0.4559753  0.8422064  0.24072914          
  5     0.007808538  0.30549411  0.4010607  0.8841747  0.20125826          
  5     0.796970590  0.64807594  0.4013298  0.8812112  0.20086700          
  6     0.360520118  0.87655231  0.3385321  0.9137301  0.15727855          
  7     0.344190822  0.06653705  0.2986823  0.9273449  0.12571846          
  7     0.416941055  0.66059929  0.2981873  0.9249823  0.12048073          
  7     0.576615929  0.18772890  0.3128844  0.9192050  0.13504943          
  8     0.210048893  0.87163842  0.2497234  0.9477509  0.09551329          
  9     0.064659926  0.54572173  0.2331885  0.9564524  0.08461873  *       
  9     0.557371826  0.52287711  0.2351657  0.9573192  0.08412285          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 9, coefReg = 0.06465993
 and coefImp = 0.5457217.
[1] "Sat Mar 10 02:19:01 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 15 warnings (use warnings() to see them)
Regularized Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  mtry  coefReg      RMSE       Rsquared   MAE         Selected
  1     0.380804874  0.7066373  0.4578282  0.42274846          
  1     0.500076094  0.7242979  0.3840183  0.44014668          
  1     0.810763171  0.7098850  0.4437820  0.42404277          
  1     0.922881632  0.7136857  0.4234699  0.42954160          
  2     0.773448540  0.6171054  0.6530514  0.35829415          
  2     0.972020978  0.6192673  0.6550956  0.35737455          
  3     0.407126480  0.5307158  0.7793838  0.29153531          
  3     0.507460708  0.5449668  0.7546716  0.29919517          
  3     0.736807193  0.5369649  0.7698222  0.29385558          
  4     0.501378395  0.4630212  0.8407960  0.24068863          
  4     0.686641136  0.4821671  0.8232511  0.25581962          
  5     0.007808538  0.4041450  0.8685644  0.20410432          
  5     0.796970590  0.4094955  0.8701956  0.20198635          
  6     0.360520118  0.3529910  0.9068252  0.16135476          
  7     0.344190822  0.3049662  0.9243540  0.13394847          
  7     0.416941055  0.3116287  0.9148342  0.13180234          
  7     0.576615929  0.3100230  0.9215081  0.13202305          
  8     0.210048893  0.2615023  0.9420562  0.09924115          
  9     0.064659926  0.2501832  0.9493801  0.08890143          
  9     0.557371826  0.2283508  0.9568030  0.08136688  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 9 and coefReg = 0.5573718.
[1] "Sat Mar 10 02:19:19 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
In addition: Warning messages:
1: model fit failed for Resample11: parameter=none Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
2: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:19:36 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "rvmLinear"               
Error in .local(object, ...) : test vector does not match model !
In addition: There were 22 warnings (use warnings() to see them)
Error in .local(object, ...) : test vector does not match model !
In addition: There were 13 warnings (use warnings() to see them)
Error in .local(object, ...) : test vector does not match model !
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:20:14 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "rvmPoly"                 
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:20:35 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "rvmRadial"               
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  12%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  22%  |                                                                              |================                                                      |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  27%  |                                                                              |=====================                                                 |  29%  |                                                                              |======================                                                |  31%  |                                                                              |=======================                                               |  33%  |                                                                              |=========================                                             |  35%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |==============================                                        |  43%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  59%  |                                                                              |===========================================                           |  61%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  65%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  69%  |                                                                              |=================================================                     |  71%  |                                                                              |===================================================                   |  73%  |                                                                              |====================================================                  |  75%  |                                                                              |======================================================                |  76%  |                                                                              |=======================================================               |  78%  |                                                                              |========================================================              |  80%  |                                                                              |==========================================================            |  82%  |                                                                              |===========================================================           |  84%  |                                                                              |============================================================          |  86%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  12%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  22%  |                                                                              |================                                                      |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  27%  |                                                                              |=====================                                                 |  29%  |                                                                              |======================                                                |  31%  |                                                                              |=======================                                               |  33%  |                                                                              |=========================                                             |  35%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |==============================                                        |  43%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  59%  |                                                                              |===========================================                           |  61%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  65%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  69%  |                                                                              |=================================================                     |  71%  |                                                                              |===================================================                   |  73%  |                                                                              |====================================================                  |  75%  |                                                                              |======================================================                |  76%  |                                                                              |=======================================================               |  78%  |                                                                              |========================================================              |  80%  |                                                                              |==========================================================            |  82%  |                                                                              |===========================================================           |  84%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |======                                                                |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  12%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  26%  |                                                                              |====================                                                  |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  40%  |                                                                              |=============================                                         |  42%  |                                                                              |===============================                                       |  44%  |                                                                              |================================                                      |  46%  |                                                                              |==================================                                    |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  52%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  56%  |                                                                              |=========================================                             |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |===========================================                           |  62%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |================================================                      |  68%  |                                                                              |=================================================                     |  70%  |                                                                              |==================================================                    |  72%  |                                                                              |====================================================                  |  74%  |                                                                              |=====================================================                 |  76%  |                                                                              |=======================================================               |  78%  |                                                                              |========================================================              |  80%  |                                                                              |=========================================================             |  82%  |                                                                              |===========================================================           |  84%  |                                                                              |============================================================          |  86%  |                                                                              |==============================================================        |  88%  |                                                                              |===============================================================       |  90%  |                                                                              |================================================================      |  92%  |                                                                              |==================================================================    |  94%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%  |                                                                              |=================================================                     |  70%  |                                                                              |==================================================                    |  71%  |                                                                              |===================================================                   |  72%
Subtractive Clustering and Fuzzy c-Means Rules 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  r.a  eps.high   eps.low     RMSE       Rsquared   MAE        Selected
   1   0.8530876  0.56277206  0.6770085  0.3809193  0.4022804  *       
   2   0.3023985  0.09579729        NaN        NaN        NaN          
   2   0.7027288  0.61452110        NaN        NaN        NaN          
   2   0.9362425  0.08302782        NaN        NaN        NaN          
   5   0.4155194  0.06499828        NaN        NaN        NaN          
   8   0.1418253  0.02480845        NaN        NaN        NaN          
   8   0.6815394  0.35698786        NaN        NaN        NaN          
   9   0.5577921  0.26771344        NaN        NaN        NaN          
  10   0.9563114  0.54879782        NaN        NaN        NaN          
  11   0.6137218  0.44640667        NaN        NaN        NaN          
  12   0.8565993  0.50558477        NaN        NaN        NaN          
  13   0.4645267  0.44470203        NaN        NaN        NaN          
  13   0.4897420  0.37911598        NaN        NaN        NaN          
  14   0.7878542  0.63601380        NaN        NaN        NaN          
  14   0.8671686  0.10052552        NaN        NaN        NaN          
  15   0.7003478  0.30509437        NaN        NaN        NaN          
  16   0.9973779  0.18923016        NaN        NaN        NaN          
  17   0.8836726  0.68246128        NaN        NaN        NaN          
  19   0.9582079  0.01654310        NaN        NaN        NaN          
  20   0.9528072  0.29815651        NaN        NaN        NaN          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were r.a = 1, eps.high = 0.8530876
 and eps.low = 0.5627721.
[1] "Sat Mar 10 02:21:07 2018"
Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      0.5319793  0.6098653  0.3526162          
  2      0.4941861  0.7031898  0.3577221          
  3      0.4821393  0.7197698  0.3495981          
  4      0.4794583  0.7253505  0.3411739  *       
  5      0.4817030  0.7222679  0.3427851          
  6      0.4817229  0.7225773  0.3427614          
  8      0.4817319  0.7225795  0.3427676          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 4.
[1] "Sat Mar 10 02:21:19 2018"
Error in mvrValstats(object = object, estimate = "train") : 
  could not find function "mvrValstats"
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Spike and Slab Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  vars  RMSE       Rsquared   MAE        Selected
  1     0.4093521  0.7878733  0.2707959          
  2     0.4085717  0.7865190  0.2800451          
  3     0.4083868  0.7848047  0.2863510  *       
  4     0.4113386  0.7815741  0.2909991          
  5     0.4120802  0.7810187  0.2918351          
  6     0.4189123  0.7757481  0.2981260          
  7     0.4270818  0.7697043  0.3052430          
  8     0.4331818  0.7652661  0.3099918          
  9     0.4438630  0.7569099  0.3177921          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was vars = 3.
[1] "Sat Mar 10 02:21:43 2018"
Sparse Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  kappa       eta          K  RMSE       Rsquared   MAE        Selected
  0.01212462  0.380804874  5  0.4815794  0.7226765  0.3426462          
  0.02123993  0.810763171  4  0.4758371  0.7231680  0.3385152          
  0.03754652  0.500076094  6  0.4817131  0.7225902  0.3427438          
  0.04398679  0.922881632  1  0.4121882  0.7878733  0.3037423          
  0.05620009  0.972020978  1  0.4121882  0.7878733  0.3037423  *       
  0.06466243  0.773448540  7  0.4815325  0.7226790  0.3429006          
  0.11692846  0.736807193  9  0.4817290  0.7225825  0.3427649          
  0.12532627  0.507460708  3  0.4593084  0.7392021  0.3255036          
  0.14304011  0.407126480  7  0.4817317  0.7225772  0.3427676          
  0.19234659  0.501378395  3  0.4593084  0.7392021  0.3255036          
  0.21676323  0.686641136  9  0.4817290  0.7225825  0.3427649          
  0.23321516  0.007808538  3  0.4743994  0.7280606  0.3390498          
  0.24249799  0.796970590  6  0.4822303  0.7215018  0.3433145          
  0.30323416  0.360520118  8  0.4817290  0.7225822  0.3427650          
  0.33833023  0.416941055  6  0.4817131  0.7225902  0.3427438          
  0.35720367  0.576615929  2  0.4396869  0.7630349  0.3156815          
  0.36246453  0.344190822  1  0.4886592  0.7049839  0.3351375          
  0.43832152  0.210048893  8  0.4817290  0.7225822  0.3427650          
  0.46652766  0.064659926  5  0.4815794  0.7226765  0.3426462          
  0.47601080  0.557371826  5  0.4813856  0.7228580  0.3427515          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were K = 1, eta = 0.972021 and kappa
 = 0.05620009.
[1] "Sat Mar 10 02:21:57 2018"
Error in rowMeans(x) : 'x' must be an array of at least two dimensions
Error in confusionMatrix.default(p[, 1], p[, 2]) : 
  the data cannot have more levels than the reference
Supervised Principal Component Analysis 

76 samples
 9 predictor

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 76, 76, 76, 76, 76, 76, ... 
Resampling results across tuning parameters:

  n.components  threshold  RMSE       Rsquared   MAE        Selected
  1             0.1        0.8349795  0.0941827  0.5010036          
  1             0.5        0.8099233  0.1657121  0.5017227          
  1             0.9        0.7737208  0.2747153  0.5010189          
  2             0.1        0.8119164  0.1619603  0.5089545          
  2             0.5        0.7858768  0.2304740  0.4978570          
  2             0.9        0.6335080  0.5062714  0.4192520          
  3             0.1        0.7700793  0.2553973  0.4963046          
  3             0.5        0.7426908  0.3177929  0.4822159          
  3             0.9        0.5542371  0.6516530  0.3815327  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were threshold = 0.9 and n.components = 3.
[1] "Sat Mar 10 02:22:16 2018"
Error : 'x' should be a character matrix with a single column for string kernel methods
In addition: Warning messages:
1: predictions failed for Resample01: n.components=3, threshold=0.9 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
2: predictions failed for Resample03: n.components=3, threshold=0.9 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:22:28 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "svmBoundrangeString"     
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:22:40 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "svmExpoString"           
Support Vector Machines with Linear Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  C             RMSE       Rsquared   MAE        Selected
    0.04021113  0.5245200  0.7655511  0.2455254          
    0.04860330  0.5167581  0.7691703  0.2420021          
    0.06822270  0.5019012  0.7767595  0.2414386          
    0.07799924  0.4963546  0.7791112  0.2403105          
    0.10055124  0.4912925  0.7762095  0.2419087          
    0.11989745  0.4877827  0.7727698  0.2453288          
    0.35548334  0.4737058  0.7737419  0.2479273          
    0.42331045  0.4699054  0.7767428  0.2478736          
    0.61183023  0.4693357  0.7767107  0.2486242          
    1.70573984  0.4695305  0.7736620  0.2531902          
    2.83411206  0.4692917  0.7740968  0.2526421          
    3.99018229  0.4691367  0.7734282  0.2539556          
    4.83977422  0.4676715  0.7729508  0.2552366  *       
   17.11304756  0.4687070  0.7690627  0.2600229          
   35.50392921  0.4687342  0.7690423  0.2600973          
   52.56789945  0.4687762  0.7690517  0.2600818          
   58.64499358  0.4686332  0.7691581  0.2600028          
  283.98004022  0.4686911  0.7691634  0.2599274          
  510.52218658  0.4686950  0.7694153  0.2598767          
  621.80740128  0.4686100  0.7692657  0.2598842          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was C = 4.839774.
[1] "Sat Mar 10 02:23:25 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Linear Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  cost          RMSE       Rsquared   MAE        Selected
    0.04021113  0.5245200  0.7655511  0.2455254          
    0.04860330  0.5167581  0.7691703  0.2420021          
    0.06822270  0.5019012  0.7767595  0.2414386          
    0.07799924  0.4963546  0.7791112  0.2403105          
    0.10055124  0.4912925  0.7762095  0.2419087          
    0.11989745  0.4877827  0.7727698  0.2453288          
    0.35548334  0.4737058  0.7737419  0.2479273          
    0.42331045  0.4699054  0.7767428  0.2478736          
    0.61183023  0.4693357  0.7767107  0.2486242          
    1.70573984  0.4695305  0.7736620  0.2531902          
    2.83411206  0.4692917  0.7740968  0.2526421          
    3.99018229  0.4691367  0.7734282  0.2539556          
    4.83977422  0.4676715  0.7729508  0.2552366  *       
   17.11304756  0.4687070  0.7690627  0.2600229          
   35.50392921  0.4687342  0.7690423  0.2600973          
   52.56789945  0.4687762  0.7690517  0.2600818          
   58.64499358  0.4686332  0.7691581  0.2600028          
  283.98004022  0.4686911  0.7691634  0.2599274          
  510.52218658  0.4686950  0.7694153  0.2598767          
  621.80740128  0.4686100  0.7692657  0.2598842          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cost = 4.839774.
[1] "Sat Mar 10 02:24:09 2018"
L2 Regularized Support Vector Machine (dual) with Linear Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  cost          Loss  RMSE       Rsquared   MAE        Selected
  1.366769e-03  L1    0.7884601  0.6071870  0.4229100          
  1.759764e-03  L2    0.7995782  0.6483043  0.4335248          
  2.765701e-03  L2    0.7738727  0.6748331  0.4118056          
  3.306388e-03  L2    0.7606002  0.6787653  0.4022351          
  4.638917e-03  L2    0.7294664  0.6874956  0.3777372          
  5.865611e-03  L2    0.7070306  0.6920933  0.3588786          
  2.498383e-02  L2    0.5548533  0.7517316  0.2511806          
  3.153397e-02  L2    0.5380271  0.7577292  0.2472525          
  5.153171e-02  L1    0.4871223  0.6951174  0.2985367          
  2.022012e-01  L2    0.4728505  0.7750935  0.2452333          
  3.979141e-01  L2    0.4707495  0.7747519  0.2488423          
  6.278998e-01  L1    0.4798866  0.7234979  0.3428778          
  8.122072e-01  L2    0.4697580  0.7734356  0.2507587          
  4.375253e+00  L1    0.4841032  0.7238791  0.3494070          
  1.157716e+01  L1    0.4846035  0.7238980  0.3501845          
  1.953713e+01  L2    0.4608698  0.7656598  0.2652724  *       
  2.260518e+01  L1    0.4849010  0.7234833  0.3505631          
  1.851898e+02  L1    0.4859296  0.7237930  0.3512985          
  4.048115e+02  L1    0.4859418  0.7237933  0.3513153          
  5.265519e+02  L2    0.4929683  0.7308718  0.3754418          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were cost = 19.53713 and Loss = L2.
[1] "Sat Mar 10 02:24:23 2018"
Support Vector Machines with Polynomial Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  degree  scale         C             RMSE       Rsquared   MAE        Selected
  1       1.043898e-03  3.626022e+00  0.7454503  0.6919147  0.3892524          
  1       1.439427e-03  7.418276e+01  0.4896343  0.7753338  0.2437080          
  1       4.476292e-03  2.732417e+01  0.4868012  0.7719467  0.2457745          
  1       4.898515e-03  4.297488e-01  0.7883763  0.6827238  0.4246433          
  1       8.050763e-02  1.001713e+03  0.4687321  0.7690319  0.2601205          
  1       1.259140e-01  6.599659e+01  0.4669051  0.7722970  0.2561612  *       
  1       1.985546e-01  3.141839e+00  0.4682982  0.7769747  0.2488848          
  1       7.802310e-01  5.163464e-02  0.5246407  0.7654985  0.2455496          
  1       1.421387e+00  9.140515e-02  0.4836816  0.7711230  0.2469025          
  2       1.100002e-05  7.486751e-01  0.8500465  0.6616418  0.4809097          
  2       8.149444e-04  2.837123e+02  0.4715099  0.7725536  0.2540099          
  2       4.548015e-03  4.022306e-01  0.7485856  0.6930997  0.3915687          
  2       4.364206e-02  7.881343e+02  0.7040892  0.4058841  0.5012057          
  2       1.677896e-01  2.637573e+01  0.7043735  0.3985209  0.4995202          
  3       2.201731e-05  9.099744e+00  0.8313018  0.6762712  0.4646002          
  3       1.298600e-04  2.695813e+02  0.4901906  0.7759907  0.2430113          
  3       6.676762e-04  6.241583e-02  0.8463760  0.6748406  0.4778941          
  3       1.622621e-03  3.004368e+01  0.4797068  0.7759288  0.2463561          
  3       9.008340e-03  7.175884e+00  0.4921694  0.7447242  0.2707751          
  3       1.139353e-02  2.200546e-01  0.6777713  0.7134897  0.3344323          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 1, scale = 0.125914 and C
 = 65.99659.
[1] "Sat Mar 10 02:24:39 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Support Vector Machines with Radial Basis Function Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  sigma       C             RMSE       Rsquared   MAE        Selected
  0.01425575  332.33958350  0.6464886  0.4604043  0.4543403          
  0.01517160   21.73570693  0.5304728  0.6444501  0.3419197  *       
  0.02702187    0.29806100  0.6729591  0.6754954  0.3274824          
  0.02786992  545.14410046  0.6424441  0.4601518  0.4432874          
  0.03888326    0.06211172  0.7861026  0.6307898  0.4209917          
  0.04074482    1.08055842  0.5823458  0.6669190  0.2825235          
  0.04234721    5.14328950  0.5809243  0.5670342  0.3651449          
  0.04654045    1.56985679  0.5896076  0.6220156  0.3060708          
  0.04989589   17.69014860  0.6487794  0.4442333  0.4301547          
  0.05203482    0.63924781  0.6116133  0.6466558  0.2944893          
  0.05522190    0.14435331  0.7241049  0.6167897  0.3631429          
  0.06773121   10.32376677  0.6590921  0.4252583  0.4230884          
  0.09029083    0.14780378  0.7374170  0.5562917  0.3728613          
  0.11329970    0.66680243  0.6751466  0.4933238  0.3400762          
  0.12461726  193.32371591  0.7017931  0.3430295  0.4247895          
  0.12934615    0.12507261  0.7675770  0.4729810  0.3977957          
  0.15514363   31.53454354  0.7172843  0.3113205  0.4212791          
  0.23287509   12.47302474  0.7520408  0.2497996  0.4256051          
  0.24167542    0.55023376  0.7660801  0.3204428  0.4002086          
  0.25644749    0.30390283  0.7903491  0.2945466  0.4160857          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were sigma = 0.0151716 and C = 21.73571.
[1] "Sat Mar 10 02:24:52 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  C             RMSE       Rsquared   MAE        Selected
    0.04021113  0.8057614  0.5700968  0.4375608          
    0.04860330  0.8011239  0.5421670  0.4327039          
    0.06822270  0.7810532  0.5670738  0.4143618          
    0.07799924  0.7800032  0.5173884  0.4110157          
    0.10055124  0.7569108  0.5732665  0.3908675          
    0.11989745  0.7470968  0.5575578  0.3814256          
    0.35548334  0.6592452  0.6197145  0.3147209          
    0.42331045  0.6544463  0.5913107  0.3148779          
    0.61183023  0.6512005  0.5505471  0.3220689          
    1.70573984  0.6122329  0.5644085  0.3297768  *       
    2.83411206  0.6320179  0.4814833  0.3785139          
    3.99018229  0.6495605  0.4419453  0.3992017          
    4.83977422  0.6530096  0.4340496  0.4118130          
   17.11304756  0.6725495  0.4043046  0.4272884          
   35.50392921  0.6644028  0.4172098  0.4266783          
   52.56789945  0.6628683  0.4209341  0.4321187          
   58.64499358  0.6663037  0.4151965  0.4284093          
  283.98004022  0.6682056  0.4112460  0.4315118          
  510.52218658  0.6669053  0.4124720  0.4318457          
  621.80740128  0.6650862  0.4157498  0.4303786          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was C = 1.70574.
[1] "Sat Mar 10 02:25:05 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  sigma       C             RMSE       Rsquared   MAE        Selected
  0.01425575  332.33958350  0.6464886  0.4604043  0.4543403          
  0.01517160   21.73570693  0.5304728  0.6444501  0.3419197  *       
  0.02702187    0.29806100  0.6729591  0.6754954  0.3274824          
  0.02786992  545.14410046  0.6424441  0.4601518  0.4432874          
  0.03888326    0.06211172  0.7861026  0.6307898  0.4209917          
  0.04074482    1.08055842  0.5823458  0.6669190  0.2825235          
  0.04234721    5.14328950  0.5809243  0.5670342  0.3651449          
  0.04654045    1.56985679  0.5896076  0.6220156  0.3060708          
  0.04989589   17.69014860  0.6487794  0.4442333  0.4301547          
  0.05203482    0.63924781  0.6116133  0.6466558  0.2944893          
  0.05522190    0.14435331  0.7241049  0.6167897  0.3631429          
  0.06773121   10.32376677  0.6590921  0.4252583  0.4230884          
  0.09029083    0.14780378  0.7374170  0.5562917  0.3728613          
  0.11329970    0.66680243  0.6751466  0.4933238  0.3400762          
  0.12461726  193.32371591  0.7017931  0.3430295  0.4247895          
  0.12934615    0.12507261  0.7675770  0.4729810  0.3977957          
  0.15514363   31.53454354  0.7172843  0.3113205  0.4212791          
  0.23287509   12.47302474  0.7520408  0.2497996  0.4256051          
  0.24167542    0.55023376  0.7660801  0.3204428  0.4002086          
  0.25644749    0.30390283  0.7903491  0.2945466  0.4160857          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were sigma = 0.0151716 and C = 21.73571.
[1] "Sat Mar 10 02:25:20 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:25:32 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "svmSpectrumString"       
Bagged CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results:

  RMSE       Rsquared   MAE      
  0.4047574  0.7862012  0.1924589

[1] "Sat Mar 10 02:25:45 2018"
Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 51, 50 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      0.5319793  0.6098653  0.3526162          
  2      0.4941861  0.7031898  0.3577221          
  3      0.4821393  0.7197698  0.3495981          
  4      0.4794583  0.7253505  0.3411739  *       
  5      0.4817030  0.7222679  0.3427851          
  6      0.4817229  0.7225773  0.3427614          
  8      0.4817319  0.7225795  0.3427676          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 4.
[1] "Sat Mar 10 02:26:14 2018"
Error in mvrValstats(object = object, estimate = "train") : 
  could not find function "mvrValstats"
Error : package xgboost is required
Error : package xgboost is required
Error : package xgboost is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:26:26 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "xgbDART"                 
Error : package xgboost is required
Error : package xgboost is required
Error : package xgboost is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:26:39 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "xgbLinear"               
Error : package xgboost is required
Error : package xgboost is required
Error : package xgboost is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:26:51 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "xgbTree"                 
Error : package kohonen is required
Error : package kohonen is required
Error : package kohonen is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:27:03 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "asis"                     "HOPPER"                  
 [9] "14th20hp3cv"              "xyf"                     
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  No variation for for: V11, V12
Fitting Repeat 1 

# weights:  210
initial  value 9.436102 
iter  10 value 3.171049
iter  20 value 3.149381
iter  30 value 3.145987
iter  40 value 3.143689
iter  50 value 3.142515
iter  60 value 3.142174
iter  70 value 3.142085
iter  70 value 3.142085
iter  70 value 3.142085
final  value 3.142085 
converged
Fitting Repeat 2 

# weights:  210
initial  value 8.236487 
iter  10 value 3.176117
iter  20 value 3.151541
iter  30 value 3.145879
iter  40 value 3.143362
iter  50 value 3.142785
iter  60 value 3.142616
iter  70 value 3.142612
iter  70 value 3.142612
iter  70 value 3.142612
final  value 3.142612 
converged
Fitting Repeat 3 

# weights:  210
initial  value 8.325656 
iter  10 value 3.169056
iter  20 value 3.147801
iter  30 value 3.145255
iter  40 value 3.142995
iter  50 value 3.142619
final  value 3.142612 
converged
Fitting Repeat 4 

# weights:  210
initial  value 14.665596 
iter  10 value 3.195495
iter  20 value 3.158017
iter  30 value 3.147450
iter  40 value 3.142993
iter  50 value 3.142622
iter  60 value 3.142615
iter  70 value 3.142613
final  value 3.142612 
converged
Fitting Repeat 5 

# weights:  210
initial  value 10.059330 
iter  10 value 3.174190
iter  20 value 3.148714
iter  30 value 3.146196
iter  40 value 3.145525
iter  50 value 3.145352
iter  60 value 3.145343
iter  60 value 3.145343
iter  60 value 3.145343
final  value 3.145343 
converged
Fitting Repeat 1 

# weights:  78
initial  value 5.918545 
iter  10 value 0.149157
iter  20 value 0.125619
iter  30 value 0.116997
iter  40 value 0.113980
iter  50 value 0.112826
iter  60 value 0.112024
iter  70 value 0.111536
iter  80 value 0.111040
iter  90 value 0.110860
iter 100 value 0.110791
final  value 0.110791 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  78
initial  value 5.380371 
iter  10 value 0.276324
iter  20 value 0.151471
iter  30 value 0.134922
iter  40 value 0.126663
iter  50 value 0.124457
iter  60 value 0.118847
iter  70 value 0.116373
iter  80 value 0.114084
iter  90 value 0.112952
iter 100 value 0.111618
final  value 0.111618 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  78
initial  value 4.892591 
iter  10 value 0.413958
iter  20 value 0.198761
iter  30 value 0.152945
iter  40 value 0.135277
iter  50 value 0.122143
iter  60 value 0.118852
iter  70 value 0.116697
iter  80 value 0.115759
iter  90 value 0.114988
iter 100 value 0.114098
final  value 0.114098 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  78
initial  value 5.235820 
iter  10 value 0.242645
iter  20 value 0.133180
iter  30 value 0.121666
iter  40 value 0.116066
iter  50 value 0.113960
iter  60 value 0.112958
iter  70 value 0.112020
iter  80 value 0.111687
iter  90 value 0.111535
iter 100 value 0.111453
final  value 0.111453 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  78
initial  value 5.842672 
iter  10 value 0.633404
iter  20 value 0.134941
iter  30 value 0.122516
iter  40 value 0.117674
iter  50 value 0.115057
iter  60 value 0.113801
iter  70 value 0.112856
iter  80 value 0.111875
iter  90 value 0.111241
iter 100 value 0.111017
final  value 0.111017 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 5.352320 
iter  10 value 0.423152
iter  20 value 0.311638
iter  30 value 0.299898
iter  40 value 0.297258
iter  50 value 0.295125
iter  60 value 0.294099
iter  70 value 0.292801
iter  80 value 0.291965
iter  90 value 0.291259
iter 100 value 0.290234
final  value 0.290234 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  177
initial  value 9.386221 
iter  10 value 0.558517
iter  20 value 0.365704
iter  30 value 0.331225
iter  40 value 0.324234
iter  50 value 0.319942
iter  60 value 0.317042
iter  70 value 0.314996
iter  80 value 0.313866
iter  90 value 0.313108
iter 100 value 0.312690
final  value 0.312690 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  177
initial  value 6.975908 
iter  10 value 0.494517
iter  20 value 0.321278
iter  30 value 0.298999
iter  40 value 0.295285
iter  50 value 0.292516
iter  60 value 0.290618
iter  70 value 0.289794
iter  80 value 0.289583
iter  90 value 0.289418
iter 100 value 0.289312
final  value 0.289312 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  177
initial  value 7.902555 
iter  10 value 0.436726
iter  20 value 0.316897
iter  30 value 0.302497
iter  40 value 0.299499
iter  50 value 0.296157
iter  60 value 0.293306
iter  70 value 0.292150
iter  80 value 0.291317
iter  90 value 0.290891
iter 100 value 0.290537
final  value 0.290537 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  177
initial  value 4.723347 
iter  10 value 0.436111
iter  20 value 0.301053
iter  30 value 0.279352
iter  40 value 0.277009
iter  50 value 0.275499
iter  60 value 0.274087
iter  70 value 0.273586
iter  80 value 0.273384
iter  90 value 0.273287
iter 100 value 0.273264
final  value 0.273264 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  199
initial  value 4.836872 
iter  10 value 0.252793
iter  20 value 0.182260
iter  30 value 0.165705
iter  40 value 0.162990
iter  50 value 0.161867
iter  60 value 0.160768
iter  70 value 0.160061
iter  80 value 0.159508
iter  90 value 0.159163
iter 100 value 0.158878
final  value 0.158878 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 6.996214 
iter  10 value 0.236374
iter  20 value 0.183410
iter  30 value 0.167431
iter  40 value 0.163347
iter  50 value 0.161532
iter  60 value 0.160225
iter  70 value 0.159532
iter  80 value 0.159239
iter  90 value 0.159024
iter 100 value 0.158820
final  value 0.158820 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 5.070559 
iter  10 value 0.255716
iter  20 value 0.195007
iter  30 value 0.170050
iter  40 value 0.165437
iter  50 value 0.163727
iter  60 value 0.161949
iter  70 value 0.160439
iter  80 value 0.159529
iter  90 value 0.159234
iter 100 value 0.159026
final  value 0.159026 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 13.964992 
iter  10 value 0.262272
iter  20 value 0.195610
iter  30 value 0.171920
iter  40 value 0.164994
iter  50 value 0.163359
iter  60 value 0.162047
iter  70 value 0.161071
iter  80 value 0.160289
iter  90 value 0.159591
iter 100 value 0.159261
final  value 0.159261 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 5.808523 
iter  10 value 0.295407
iter  20 value 0.196139
iter  30 value 0.173549
iter  40 value 0.165674
iter  50 value 0.163839
iter  60 value 0.161781
iter  70 value 0.160674
iter  80 value 0.159744
iter  90 value 0.159269
iter 100 value 0.158963
final  value 0.158963 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  144
initial  value 8.413688 
iter  10 value 2.987268
iter  20 value 2.931008
iter  30 value 2.918701
iter  40 value 2.911643
iter  50 value 2.908685
iter  60 value 2.908448
iter  70 value 2.908421
final  value 2.908417 
converged
Fitting Repeat 2 

# weights:  144
initial  value 7.397400 
iter  10 value 2.970197
iter  20 value 2.926857
iter  30 value 2.915696
iter  40 value 2.913532
iter  50 value 2.913166
iter  60 value 2.913094
iter  70 value 2.913089
final  value 2.913088 
converged
Fitting Repeat 3 

# weights:  144
initial  value 7.647365 
iter  10 value 2.963032
iter  20 value 2.931268
iter  30 value 2.922387
iter  40 value 2.916428
iter  50 value 2.912615
iter  60 value 2.908704
iter  70 value 2.908429
iter  80 value 2.908417
final  value 2.908417 
converged
Fitting Repeat 4 

# weights:  144
initial  value 11.002424 
iter  10 value 2.948081
iter  20 value 2.916573
iter  30 value 2.911817
iter  40 value 2.908626
iter  50 value 2.908424
iter  60 value 2.908418
final  value 2.908417 
converged
Fitting Repeat 5 

# weights:  144
initial  value 6.682657 
iter  10 value 2.936738
iter  20 value 2.920759
iter  30 value 2.915491
iter  40 value 2.913540
iter  50 value 2.913116
iter  60 value 2.913089
final  value 2.913088 
converged
Fitting Repeat 1 

# weights:  34
initial  value 4.908015 
iter  10 value 0.344625
iter  20 value 0.094815
iter  30 value 0.069049
iter  40 value 0.059098
iter  50 value 0.052373
iter  60 value 0.048328
iter  70 value 0.044693
iter  80 value 0.041927
iter  90 value 0.039380
iter 100 value 0.034637
final  value 0.034637 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  34
initial  value 6.394975 
iter  10 value 0.824982
iter  20 value 0.135609
iter  30 value 0.064250
iter  40 value 0.044087
iter  50 value 0.031459
iter  60 value 0.030185
iter  70 value 0.029814
iter  80 value 0.029636
iter  90 value 0.029338
iter 100 value 0.028512
final  value 0.028512 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  34
initial  value 6.702404 
iter  10 value 0.291846
iter  20 value 0.088040
iter  30 value 0.071675
iter  40 value 0.056802
iter  50 value 0.049392
iter  60 value 0.042601
iter  70 value 0.039090
iter  80 value 0.037446
iter  90 value 0.034207
iter 100 value 0.029925
final  value 0.029925 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  34
initial  value 4.595851 
iter  10 value 0.229256
iter  20 value 0.110437
iter  30 value 0.072309
iter  40 value 0.061634
iter  50 value 0.054791
iter  60 value 0.046076
iter  70 value 0.036515
iter  80 value 0.034812
iter  90 value 0.032469
iter 100 value 0.030859
final  value 0.030859 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  34
initial  value 4.423892 
iter  10 value 0.142151
iter  20 value 0.084554
iter  30 value 0.072334
iter  40 value 0.067743
iter  50 value 0.060493
iter  60 value 0.057617
iter  70 value 0.055383
iter  80 value 0.053542
iter  90 value 0.048708
iter 100 value 0.044248
final  value 0.044248 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  133
initial  value 6.434375 
iter  10 value 1.144473
iter  20 value 1.011658
iter  30 value 1.002000
iter  40 value 0.989567
iter  50 value 0.982982
iter  60 value 0.976522
iter  70 value 0.972735
iter  80 value 0.971053
iter  90 value 0.970738
iter 100 value 0.970615
final  value 0.970615 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  133
initial  value 6.664332 
iter  10 value 1.140738
iter  20 value 0.990757
iter  30 value 0.979295
iter  40 value 0.973646
iter  50 value 0.972697
iter  60 value 0.972024
iter  70 value 0.971466
iter  80 value 0.970666
iter  90 value 0.970586
iter 100 value 0.970578
final  value 0.970578 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 4.585026 
iter  10 value 1.079397
iter  20 value 0.985999
iter  30 value 0.980108
iter  40 value 0.975674
iter  50 value 0.972378
iter  60 value 0.970926
iter  70 value 0.970661
iter  80 value 0.970616
iter  90 value 0.970574
iter 100 value 0.970572
final  value 0.970572 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  133
initial  value 8.631938 
iter  10 value 1.222007
iter  20 value 1.002521
iter  30 value 0.989183
iter  40 value 0.982024
iter  50 value 0.977925
iter  60 value 0.973864
iter  70 value 0.971847
iter  80 value 0.971374
iter  90 value 0.971073
iter 100 value 0.970874
final  value 0.970874 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  133
initial  value 4.877485 
iter  10 value 1.174998
iter  20 value 0.995902
iter  30 value 0.982230
iter  40 value 0.973684
iter  50 value 0.970989
iter  60 value 0.969326
iter  70 value 0.969158
iter  80 value 0.969063
iter  90 value 0.969018
iter 100 value 0.969012
final  value 0.969012 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  221
initial  value 124.763316 
iter  10 value 4.395283
iter  20 value 4.214028
final  value 4.214028 
converged
Fitting Repeat 2 

# weights:  221
initial  value 126.850693 
iter  10 value 5.183441
iter  20 value 5.055683
final  value 5.055645 
converged
Fitting Repeat 3 

# weights:  221
initial  value 113.380264 
iter  10 value 5.287139
iter  20 value 4.569406
final  value 4.569363 
converged
Fitting Repeat 4 

# weights:  221
initial  value 124.278592 
iter  10 value 4.827080
iter  20 value 4.825704
iter  20 value 4.825704
iter  20 value 4.825704
final  value 4.825704 
converged
Fitting Repeat 5 

# weights:  221
initial  value 123.114355 
iter  10 value 4.751344
iter  20 value 4.714062
final  value 4.714061 
converged
Fitting Repeat 1 

# weights:  89
initial  value 6.275248 
iter  10 value 3.649839
iter  20 value 3.629171
iter  30 value 3.626993
iter  40 value 3.626926
final  value 3.626925 
converged
Fitting Repeat 2 

# weights:  89
initial  value 7.803502 
iter  10 value 3.667362
iter  20 value 3.635052
iter  30 value 3.627904
iter  40 value 3.626400
final  value 3.626343 
converged
Fitting Repeat 3 

# weights:  89
initial  value 6.964918 
iter  10 value 3.672443
iter  20 value 3.644285
iter  30 value 3.633353
iter  40 value 3.628880
iter  50 value 3.626998
iter  60 value 3.626925
final  value 3.626925 
converged
Fitting Repeat 4 

# weights:  89
initial  value 8.417432 
iter  10 value 3.694153
iter  20 value 3.646292
iter  30 value 3.632007
iter  40 value 3.627496
iter  50 value 3.626962
final  value 3.626925 
converged
Fitting Repeat 5 

# weights:  89
initial  value 8.215845 
iter  10 value 3.639946
iter  20 value 3.627575
iter  30 value 3.626365
iter  40 value 3.626343
final  value 3.626343 
converged
Fitting Repeat 1 

# weights:  111
initial  value 5.843491 
iter  10 value 0.738030
iter  20 value 0.542859
iter  30 value 0.530554
iter  40 value 0.527785
iter  50 value 0.523918
iter  60 value 0.522034
iter  70 value 0.521290
iter  80 value 0.519328
iter  90 value 0.518428
iter 100 value 0.518061
final  value 0.518061 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  111
initial  value 4.675174 
iter  10 value 0.718524
iter  20 value 0.499301
iter  30 value 0.480962
iter  40 value 0.477316
iter  50 value 0.475992
iter  60 value 0.474859
iter  70 value 0.474241
iter  80 value 0.473449
iter  90 value 0.472638
iter 100 value 0.472129
final  value 0.472129 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  111
initial  value 7.036854 
iter  10 value 0.958260
iter  20 value 0.540803
iter  30 value 0.513107
iter  40 value 0.507082
iter  50 value 0.504173
iter  60 value 0.501819
iter  70 value 0.501326
iter  80 value 0.501029
iter  90 value 0.500784
iter 100 value 0.500102
final  value 0.500102 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  111
initial  value 4.347281 
iter  10 value 0.799235
iter  20 value 0.539006
iter  30 value 0.497256
iter  40 value 0.490214
iter  50 value 0.486937
iter  60 value 0.481762
iter  70 value 0.479266
iter  80 value 0.477919
iter  90 value 0.476884
iter 100 value 0.476726
final  value 0.476726 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  111
initial  value 3.789659 
iter  10 value 0.620199
iter  20 value 0.496087
iter  30 value 0.490535
iter  40 value 0.486674
iter  50 value 0.484317
iter  60 value 0.483349
iter  70 value 0.483084
iter  80 value 0.482979
iter  90 value 0.482925
iter 100 value 0.482919
final  value 0.482919 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  111
initial  value 64.217180 
iter  10 value 4.548903
iter  20 value 4.452834
final  value 4.452832 
converged
Fitting Repeat 2 

# weights:  111
initial  value 71.003956 
iter  10 value 4.791214
iter  20 value 4.452843
final  value 4.452832 
converged
Fitting Repeat 3 

# weights:  111
initial  value 71.028105 
iter  10 value 4.526909
iter  20 value 4.452835
final  value 4.452832 
converged
Fitting Repeat 4 

# weights:  111
initial  value 69.820496 
iter  10 value 4.562413
iter  20 value 4.452835
final  value 4.452832 
converged
Fitting Repeat 5 

# weights:  111
initial  value 58.570745 
iter  10 value 4.472524
iter  20 value 4.452833
final  value 4.452832 
converged
Fitting Repeat 1 

# weights:  111
initial  value 83.800134 
iter  10 value 4.775566
iter  20 value 4.453086
final  value 4.453086 
converged
Fitting Repeat 2 

# weights:  111
initial  value 84.561595 
iter  10 value 4.457979
final  value 4.453086 
converged
Fitting Repeat 3 

# weights:  111
initial  value 92.246974 
iter  10 value 4.679238
iter  20 value 4.453094
final  value 4.453086 
converged
Fitting Repeat 4 

# weights:  111
initial  value 69.687769 
iter  10 value 4.469885
iter  20 value 4.453086
final  value 4.453086 
converged
Fitting Repeat 5 

# weights:  111
initial  value 98.989529 
iter  10 value 4.537690
iter  20 value 4.453087
final  value 4.453086 
converged
Fitting Repeat 1 

# weights:  56
initial  value 12.312856 
iter  10 value 4.452427
final  value 4.452421 
converged
Fitting Repeat 2 

# weights:  56
initial  value 11.725859 
iter  10 value 4.452429
final  value 4.452421 
converged
Fitting Repeat 3 

# weights:  56
initial  value 13.445991 
iter  10 value 4.452443
final  value 4.452421 
converged
Fitting Repeat 4 

# weights:  56
initial  value 14.790167 
iter  10 value 4.452705
final  value 4.452421 
converged
Fitting Repeat 5 

# weights:  56
initial  value 14.924847 
iter  10 value 4.452423
final  value 4.452421 
converged
Fitting Repeat 1 

# weights:  111
initial  value 14.556277 
iter  10 value 4.452487
final  value 4.452192 
converged
Fitting Repeat 2 

# weights:  111
initial  value 14.264085 
iter  10 value 4.454299
final  value 4.452192 
converged
Fitting Repeat 3 

# weights:  111
initial  value 14.251175 
iter  10 value 4.453158
final  value 4.452192 
converged
Fitting Repeat 4 

# weights:  111
initial  value 13.507557 
iter  10 value 4.452312
final  value 4.452192 
converged
Fitting Repeat 5 

# weights:  111
initial  value 16.882326 
iter  10 value 4.452688
final  value 4.452192 
converged
Fitting Repeat 1 

# weights:  67
initial  value 15.626336 
iter  10 value 4.452948
final  value 4.452407 
converged
Fitting Repeat 2 

# weights:  67
initial  value 14.911438 
iter  10 value 4.452556
final  value 4.452407 
converged
Fitting Repeat 3 

# weights:  67
initial  value 14.241944 
iter  10 value 4.452496
final  value 4.452407 
converged
Fitting Repeat 4 

# weights:  67
initial  value 14.571150 
iter  10 value 4.452514
final  value 4.452407 
converged
Fitting Repeat 5 

# weights:  67
initial  value 13.230238 
iter  10 value 4.452418
final  value 4.452407 
converged
Fitting Repeat 1 

# weights:  89
initial  value 5.927790 
iter  10 value 2.145152
iter  20 value 2.111201
iter  30 value 2.105265
iter  40 value 2.091511
iter  50 value 2.082621
iter  60 value 2.081647
iter  70 value 2.081614
final  value 2.081608 
converged
Fitting Repeat 2 

# weights:  89
initial  value 9.359278 
iter  10 value 2.106483
iter  20 value 2.050519
iter  30 value 2.044398
iter  40 value 2.041788
iter  50 value 2.039677
iter  60 value 2.039404
iter  70 value 2.039395
final  value 2.039394 
converged
Fitting Repeat 3 

# weights:  89
initial  value 8.699657 
iter  10 value 2.117013
iter  20 value 2.074132
iter  30 value 2.067432
iter  40 value 2.066382
iter  50 value 2.066277
iter  60 value 2.066243
iter  70 value 2.066241
final  value 2.066240 
converged
Fitting Repeat 4 

# weights:  89
initial  value 7.206996 
iter  10 value 2.167639
iter  20 value 2.073990
iter  30 value 2.023574
iter  40 value 2.013200
iter  50 value 2.010917
iter  60 value 2.010432
iter  70 value 2.010369
iter  80 value 2.010357
final  value 2.010357 
converged
Fitting Repeat 5 

# weights:  89
initial  value 9.800140 
iter  10 value 2.017628
iter  20 value 1.970736
iter  30 value 1.963144
iter  40 value 1.960332
iter  50 value 1.959656
iter  60 value 1.959649
final  value 1.959648 
converged
Fitting Repeat 1 

# weights:  122
initial  value 31.630490 
iter  10 value 4.453746
final  value 4.452380 
converged
Fitting Repeat 2 

# weights:  122
initial  value 34.889370 
iter  10 value 4.535010
iter  20 value 4.452380
iter  20 value 4.452380
iter  20 value 4.452380
final  value 4.452380 
converged
Fitting Repeat 3 

# weights:  122
initial  value 30.992052 
iter  10 value 4.452520
final  value 4.452380 
converged
Fitting Repeat 4 

# weights:  122
initial  value 28.091142 
iter  10 value 4.454243
final  value 4.452380 
converged
Fitting Repeat 5 

# weights:  122
initial  value 33.768178 
iter  10 value 4.455173
final  value 4.452380 
converged
Fitting Repeat 1 

# weights:  111
initial  value 9.808703 
iter  10 value 3.761094
iter  20 value 3.730512
iter  30 value 3.722132
iter  40 value 3.720901
iter  50 value 3.720755
final  value 3.720748 
converged
Fitting Repeat 2 

# weights:  111
initial  value 6.688448 
iter  10 value 3.377066
iter  20 value 3.365696
iter  30 value 3.363485
iter  40 value 3.362062
iter  50 value 3.361324
final  value 3.361315 
converged
Fitting Repeat 3 

# weights:  111
initial  value 12.433562 
iter  10 value 3.139118
iter  20 value 3.112444
iter  30 value 3.107025
iter  40 value 3.105787
iter  50 value 3.105725
final  value 3.105724 
converged
Fitting Repeat 4 

# weights:  111
initial  value 8.820760 
iter  10 value 3.804583
iter  20 value 3.782147
iter  30 value 3.781216
iter  40 value 3.781165
final  value 3.781165 
converged
Fitting Repeat 5 

# weights:  111
initial  value 10.228987 
iter  10 value 3.567791
iter  20 value 3.525351
iter  30 value 3.517959
iter  40 value 3.517476
iter  50 value 3.517461
final  value 3.517460 
converged
Fitting Repeat 1 

# weights:  199
initial  value 13.029824 
iter  10 value 0.505353
iter  20 value 0.349256
iter  30 value 0.335323
iter  40 value 0.330894
iter  50 value 0.327604
iter  60 value 0.326162
iter  70 value 0.324895
iter  80 value 0.324167
iter  90 value 0.324037
iter 100 value 0.323883
final  value 0.323883 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 4.569647 
iter  10 value 0.503720
iter  20 value 0.351760
iter  30 value 0.337289
iter  40 value 0.333077
iter  50 value 0.329967
iter  60 value 0.328639
iter  70 value 0.327426
iter  80 value 0.326873
iter  90 value 0.326514
iter 100 value 0.325978
final  value 0.325978 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 11.269424 
iter  10 value 0.524780
iter  20 value 0.363671
iter  30 value 0.333286
iter  40 value 0.329956
iter  50 value 0.327860
iter  60 value 0.326631
iter  70 value 0.325203
iter  80 value 0.324550
iter  90 value 0.324277
iter 100 value 0.323979
final  value 0.323979 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 5.725738 
iter  10 value 0.577389
iter  20 value 0.363006
iter  30 value 0.337954
iter  40 value 0.334079
iter  50 value 0.329283
iter  60 value 0.326210
iter  70 value 0.324809
iter  80 value 0.324034
iter  90 value 0.323556
iter 100 value 0.323345
final  value 0.323345 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 6.493589 
iter  10 value 0.484707
iter  20 value 0.347674
iter  30 value 0.332807
iter  40 value 0.330060
iter  50 value 0.326705
iter  60 value 0.324910
iter  70 value 0.324161
iter  80 value 0.323891
iter  90 value 0.323724
iter 100 value 0.323422
final  value 0.323422 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 16.211080 
iter  10 value 4.452496
iter  20 value 4.452100
final  value 4.452100 
converged
Fitting Repeat 2 

# weights:  177
initial  value 19.822377 
iter  10 value 4.452592
final  value 4.452100 
converged
Fitting Repeat 3 

# weights:  177
initial  value 15.333609 
iter  10 value 4.452451
iter  20 value 4.452100
final  value 4.452100 
converged
Fitting Repeat 4 

# weights:  177
initial  value 18.060333 
iter  10 value 4.452553
iter  20 value 4.452100
iter  20 value 4.452100
iter  20 value 4.452100
final  value 4.452100 
converged
Fitting Repeat 5 

# weights:  177
initial  value 17.009199 
iter  10 value 4.452640
iter  20 value 4.452100
iter  20 value 4.452100
iter  20 value 4.452100
final  value 4.452100 
converged
Fitting Repeat 1 

# weights:  210
initial  value 8.433472 
iter  10 value 3.153256
iter  20 value 3.132950
iter  30 value 3.126191
iter  40 value 3.124609
iter  50 value 3.124314
iter  60 value 3.124268
final  value 3.124266 
converged
Fitting Repeat 2 

# weights:  210
initial  value 9.925926 
iter  10 value 3.163490
iter  20 value 3.138872
iter  30 value 3.129618
iter  40 value 3.127276
iter  50 value 3.126920
iter  60 value 3.126205
iter  70 value 3.126171
final  value 3.126170 
converged
Fitting Repeat 3 

# weights:  210
initial  value 8.127867 
iter  10 value 3.152544
iter  20 value 3.132308
iter  30 value 3.126279
iter  40 value 3.124890
iter  50 value 3.123709
iter  60 value 3.122378
iter  70 value 3.122332
final  value 3.122330 
converged
Fitting Repeat 4 

# weights:  210
initial  value 9.659989 
iter  10 value 3.156392
iter  20 value 3.130858
iter  30 value 3.127359
iter  40 value 3.126445
iter  50 value 3.126195
iter  60 value 3.126172
final  value 3.126170 
converged
Fitting Repeat 5 

# weights:  210
initial  value 8.828886 
iter  10 value 3.156381
iter  20 value 3.134884
iter  30 value 3.127926
iter  40 value 3.125204
iter  50 value 3.124460
iter  60 value 3.124274
iter  70 value 3.124267
final  value 3.124266 
converged
Fitting Repeat 1 

# weights:  78
initial  value 6.957003 
iter  10 value 0.250060
iter  20 value 0.148368
iter  30 value 0.126796
iter  40 value 0.120932
iter  50 value 0.117942
iter  60 value 0.116946
iter  70 value 0.116478
iter  80 value 0.115660
iter  90 value 0.114858
iter 100 value 0.114170
final  value 0.114170 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  78
initial  value 4.587817 
iter  10 value 0.201174
iter  20 value 0.131395
iter  30 value 0.121394
iter  40 value 0.115935
iter  50 value 0.114391
iter  60 value 0.113103
iter  70 value 0.112733
iter  80 value 0.111987
iter  90 value 0.111156
iter 100 value 0.110747
final  value 0.110747 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  78
initial  value 4.349589 
iter  10 value 0.283554
iter  20 value 0.173176
iter  30 value 0.140718
iter  40 value 0.128535
iter  50 value 0.120876
iter  60 value 0.118129
iter  70 value 0.115870
iter  80 value 0.114109
iter  90 value 0.112481
iter 100 value 0.111656
final  value 0.111656 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  78
initial  value 4.130700 
iter  10 value 0.155094
iter  20 value 0.123789
iter  30 value 0.114154
iter  40 value 0.111951
iter  50 value 0.110922
iter  60 value 0.110549
iter  70 value 0.110285
iter  80 value 0.110202
iter  90 value 0.110162
iter 100 value 0.110147
final  value 0.110147 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  78
initial  value 5.033967 
iter  10 value 0.156293
iter  20 value 0.133230
iter  30 value 0.122383
iter  40 value 0.120437
iter  50 value 0.118714
iter  60 value 0.118105
iter  70 value 0.117113
iter  80 value 0.116245
iter  90 value 0.114715
iter 100 value 0.113299
final  value 0.113299 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 3.925734 
iter  10 value 0.448509
iter  20 value 0.302487
iter  30 value 0.280995
iter  40 value 0.277197
iter  50 value 0.273885
iter  60 value 0.272037
iter  70 value 0.270966
iter  80 value 0.269939
iter  90 value 0.269642
iter 100 value 0.269528
final  value 0.269528 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  177
initial  value 4.532992 
iter  10 value 0.443684
iter  20 value 0.309375
iter  30 value 0.287565
iter  40 value 0.285397
iter  50 value 0.283847
iter  60 value 0.282358
iter  70 value 0.281560
iter  80 value 0.281243
iter  90 value 0.281013
iter 100 value 0.280852
final  value 0.280852 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  177
initial  value 7.782243 
iter  10 value 0.432617
iter  20 value 0.304107
iter  30 value 0.288320
iter  40 value 0.285469
iter  50 value 0.282686
iter  60 value 0.280512
iter  70 value 0.279615
iter  80 value 0.279175
iter  90 value 0.278802
iter 100 value 0.278605
final  value 0.278605 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  177
initial  value 7.306563 
iter  10 value 0.448331
iter  20 value 0.320601
iter  30 value 0.302770
iter  40 value 0.300153
iter  50 value 0.297168
iter  60 value 0.293874
iter  70 value 0.292369
iter  80 value 0.292047
iter  90 value 0.291725
iter 100 value 0.291563
final  value 0.291563 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  177
initial  value 6.192264 
iter  10 value 0.443972
iter  20 value 0.327381
iter  30 value 0.310494
iter  40 value 0.308860
iter  50 value 0.307224
iter  60 value 0.305647
iter  70 value 0.304777
iter  80 value 0.304346
iter  90 value 0.304123
iter 100 value 0.303996
final  value 0.303996 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  199
initial  value 6.117628 
iter  10 value 0.242883
iter  20 value 0.187539
iter  30 value 0.169407
iter  40 value 0.164346
iter  50 value 0.163213
iter  60 value 0.161985
iter  70 value 0.161210
iter  80 value 0.160516
iter  90 value 0.160258
iter 100 value 0.160095
final  value 0.160095 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 5.340555 
iter  10 value 0.247989
iter  20 value 0.180088
iter  30 value 0.166948
iter  40 value 0.163889
iter  50 value 0.162815
iter  60 value 0.161607
iter  70 value 0.160817
iter  80 value 0.160380
iter  90 value 0.160130
iter 100 value 0.159985
final  value 0.159985 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 5.684391 
iter  10 value 0.250883
iter  20 value 0.191757
iter  30 value 0.170676
iter  40 value 0.164237
iter  50 value 0.163285
iter  60 value 0.162267
iter  70 value 0.161563
iter  80 value 0.161102
iter  90 value 0.160939
iter 100 value 0.160813
final  value 0.160813 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 4.799420 
iter  10 value 0.255235
iter  20 value 0.193506
iter  30 value 0.171486
iter  40 value 0.167035
iter  50 value 0.165064
iter  60 value 0.163618
iter  70 value 0.162452
iter  80 value 0.161825
iter  90 value 0.161373
iter 100 value 0.160979
final  value 0.160979 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 7.360232 
iter  10 value 0.257202
iter  20 value 0.191532
iter  30 value 0.170390
iter  40 value 0.164010
iter  50 value 0.162364
iter  60 value 0.161343
iter  70 value 0.160712
iter  80 value 0.160225
iter  90 value 0.159849
iter 100 value 0.159675
final  value 0.159675 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  144
initial  value 9.813478 
iter  10 value 2.995429
iter  20 value 2.924383
iter  30 value 2.906633
iter  40 value 2.903239
iter  50 value 2.902533
iter  60 value 2.902444
iter  70 value 2.902443
iter  70 value 2.902443
iter  70 value 2.902443
final  value 2.902443 
converged
Fitting Repeat 2 

# weights:  144
initial  value 7.883975 
iter  10 value 2.929780
iter  20 value 2.902128
iter  30 value 2.898649
iter  40 value 2.896442
iter  50 value 2.895164
iter  60 value 2.895131
final  value 2.895131 
converged
Fitting Repeat 3 

# weights:  144
initial  value 12.026068 
iter  10 value 2.954778
iter  20 value 2.916062
iter  30 value 2.902995
iter  40 value 2.898999
iter  50 value 2.898833
iter  60 value 2.898821
iter  70 value 2.898811
iter  80 value 2.897870
iter  90 value 2.895223
iter 100 value 2.895132
final  value 2.895132 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  144
initial  value 8.989381 
iter  10 value 2.938291
iter  20 value 2.913759
iter  30 value 2.908745
iter  40 value 2.901937
iter  50 value 2.900377
iter  60 value 2.899990
iter  70 value 2.899969
final  value 2.899968 
converged
Fitting Repeat 5 

# weights:  144
initial  value 14.191830 
iter  10 value 3.053149
iter  20 value 2.927989
iter  30 value 2.907233
iter  40 value 2.896367
iter  50 value 2.894797
iter  60 value 2.894758
final  value 2.894756 
converged
Fitting Repeat 1 

# weights:  34
initial  value 6.382343 
iter  10 value 1.090732
iter  20 value 0.111657
iter  30 value 0.082049
iter  40 value 0.061473
iter  50 value 0.055787
iter  60 value 0.052394
iter  70 value 0.044836
iter  80 value 0.040274
iter  90 value 0.038399
iter 100 value 0.037004
final  value 0.037004 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  34
initial  value 5.725988 
iter  10 value 0.229818
iter  20 value 0.076576
iter  30 value 0.051524
iter  40 value 0.046116
iter  50 value 0.040395
iter  60 value 0.034035
iter  70 value 0.030294
iter  80 value 0.029079
iter  90 value 0.028482
iter 100 value 0.027458
final  value 0.027458 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  34
initial  value 5.903553 
iter  10 value 0.426967
iter  20 value 0.080606
iter  30 value 0.060273
iter  40 value 0.047211
iter  50 value 0.043724
iter  60 value 0.040315
iter  70 value 0.038348
iter  80 value 0.035122
iter  90 value 0.033529
iter 100 value 0.031323
final  value 0.031323 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  34
initial  value 5.242075 
iter  10 value 0.474402
iter  20 value 0.101942
iter  30 value 0.074250
iter  40 value 0.063461
iter  50 value 0.059804
iter  60 value 0.054035
iter  70 value 0.051875
iter  80 value 0.048495
iter  90 value 0.045200
iter 100 value 0.041297
final  value 0.041297 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  34
initial  value 7.745884 
iter  10 value 0.104743
iter  20 value 0.060692
iter  30 value 0.052845
iter  40 value 0.044906
iter  50 value 0.040834
iter  60 value 0.038837
iter  70 value 0.038175
iter  80 value 0.037452
iter  90 value 0.037024
iter 100 value 0.035884
final  value 0.035884 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  133
initial  value 10.814817 
iter  10 value 1.260777
iter  20 value 1.021387
iter  30 value 1.000231
iter  40 value 0.991462
iter  50 value 0.986624
iter  60 value 0.983243
iter  70 value 0.977193
iter  80 value 0.976447
iter  90 value 0.975926
iter 100 value 0.975799
final  value 0.975799 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  133
initial  value 4.782182 
iter  10 value 1.079570
iter  20 value 0.992600
iter  30 value 0.984414
iter  40 value 0.977903
iter  50 value 0.975703
iter  60 value 0.975254
iter  70 value 0.974949
iter  80 value 0.974922
iter  90 value 0.974902
iter 100 value 0.974902
final  value 0.974902 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 12.163963 
iter  10 value 1.215936
iter  20 value 0.995345
iter  30 value 0.984135
iter  40 value 0.978179
iter  50 value 0.977034
iter  60 value 0.975987
iter  70 value 0.975785
iter  80 value 0.975739
iter  90 value 0.975704
iter 100 value 0.975702
final  value 0.975702 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  133
initial  value 5.819018 
iter  10 value 1.239492
iter  20 value 0.995611
iter  30 value 0.985729
iter  40 value 0.979368
iter  50 value 0.977631
iter  60 value 0.976965
iter  70 value 0.976817
iter  80 value 0.976801
iter  90 value 0.976076
iter 100 value 0.975403
final  value 0.975403 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  133
initial  value 5.204959 
iter  10 value 1.152908
iter  20 value 0.995381
iter  30 value 0.989553
iter  40 value 0.984477
iter  50 value 0.981526
iter  60 value 0.980704
iter  70 value 0.980380
iter  80 value 0.980309
iter  90 value 0.980224
iter 100 value 0.980212
final  value 0.980212 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  221
initial  value 124.694874 
iter  10 value 4.846047
iter  20 value 4.763421
final  value 4.763419 
converged
Fitting Repeat 2 

# weights:  221
initial  value 123.729303 
iter  10 value 5.259459
iter  20 value 4.587698
final  value 4.587676 
converged
Fitting Repeat 3 

# weights:  221
initial  value 136.443421 
iter  10 value 5.183998
iter  20 value 5.015242
final  value 5.015242 
converged
Fitting Repeat 4 

# weights:  221
initial  value 117.340706 
iter  10 value 4.491359
iter  20 value 4.393338
final  value 4.393324 
converged
Fitting Repeat 5 

# weights:  221
initial  value 115.804666 
iter  10 value 4.839770
iter  20 value 4.790394
final  value 4.790391 
converged
Fitting Repeat 1 

# weights:  89
initial  value 7.671991 
iter  10 value 3.649820
iter  20 value 3.620992
iter  30 value 3.618864
iter  40 value 3.606383
iter  50 value 3.603518
iter  60 value 3.603425
final  value 3.603424 
converged
Fitting Repeat 2 

# weights:  89
initial  value 6.757869 
iter  10 value 3.620277
iter  20 value 3.604303
iter  30 value 3.602362
iter  40 value 3.602286
final  value 3.602284 
converged
Fitting Repeat 3 

# weights:  89
initial  value 7.646156 
iter  10 value 3.660214
iter  20 value 3.615154
iter  30 value 3.604283
iter  40 value 3.603438
final  value 3.603424 
converged
Fitting Repeat 4 

# weights:  89
initial  value 7.309792 
iter  10 value 3.649385
iter  20 value 3.607058
iter  30 value 3.602771
iter  40 value 3.602311
iter  50 value 3.602284
final  value 3.602284 
converged
Fitting Repeat 5 

# weights:  89
initial  value 15.823877 
iter  10 value 3.739596
iter  20 value 3.607836
iter  30 value 3.603591
iter  40 value 3.603425
final  value 3.603424 
converged
Fitting Repeat 1 

# weights:  111
initial  value 4.965172 
iter  10 value 0.705361
iter  20 value 0.523798
iter  30 value 0.497651
iter  40 value 0.492810
iter  50 value 0.489513
iter  60 value 0.486761
iter  70 value 0.486071
iter  80 value 0.485650
iter  90 value 0.485478
iter 100 value 0.485410
final  value 0.485410 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  111
initial  value 5.276387 
iter  10 value 0.618575
iter  20 value 0.503340
iter  30 value 0.496584
iter  40 value 0.492529
iter  50 value 0.489843
iter  60 value 0.488292
iter  70 value 0.487514
iter  80 value 0.486596
iter  90 value 0.485882
iter 100 value 0.485574
final  value 0.485574 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  111
initial  value 4.852634 
iter  10 value 0.581167
iter  20 value 0.514003
iter  30 value 0.505017
iter  40 value 0.500462
iter  50 value 0.497970
iter  60 value 0.495828
iter  70 value 0.494021
iter  80 value 0.492406
iter  90 value 0.492001
iter 100 value 0.491709
final  value 0.491709 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  111
initial  value 4.884321 
iter  10 value 0.646735
iter  20 value 0.551387
iter  30 value 0.540130
iter  40 value 0.535741
iter  50 value 0.533701
iter  60 value 0.532083
iter  70 value 0.531698
iter  80 value 0.531666
iter  90 value 0.531651
iter 100 value 0.531641
final  value 0.531641 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  111
initial  value 7.715375 
iter  10 value 0.649117
iter  20 value 0.506161
iter  30 value 0.491291
iter  40 value 0.485191
iter  50 value 0.483921
iter  60 value 0.482854
iter  70 value 0.482261
iter  80 value 0.482082
iter  90 value 0.481804
iter 100 value 0.481510
final  value 0.481510 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  111
initial  value 63.304599 
iter  10 value 4.395555
final  value 4.394064 
converged
Fitting Repeat 2 

# weights:  111
initial  value 59.228573 
iter  10 value 4.420918
iter  20 value 4.394064
final  value 4.394064 
converged
Fitting Repeat 3 

# weights:  111
initial  value 65.440613 
iter  10 value 4.494935
iter  20 value 4.394067
final  value 4.394064 
converged
Fitting Repeat 4 

# weights:  111
initial  value 52.934845 
iter  10 value 4.414453
iter  20 value 4.394064
final  value 4.394064 
converged
Fitting Repeat 5 

# weights:  111
initial  value 59.960742 
iter  10 value 4.395420
final  value 4.394064 
converged
Fitting Repeat 1 

# weights:  111
initial  value 80.476867 
iter  10 value 4.440938
iter  20 value 4.394146
final  value 4.394145 
converged
Fitting Repeat 2 

# weights:  111
initial  value 82.541121 
iter  10 value 4.545985
iter  20 value 4.394145
final  value 4.394145 
converged
Fitting Repeat 3 

# weights:  111
initial  value 79.747922 
iter  10 value 4.426404
iter  20 value 4.394146
final  value 4.394145 
converged
Fitting Repeat 4 

# weights:  111
initial  value 96.843380 
iter  10 value 4.423452
iter  20 value 4.394147
final  value 4.394145 
converged
Fitting Repeat 5 

# weights:  111
initial  value 99.863540 
iter  10 value 4.444019
iter  20 value 4.394146
final  value 4.394145 
converged
Fitting Repeat 1 

# weights:  56
initial  value 15.953306 
iter  10 value 4.394082
final  value 4.393931 
converged
Fitting Repeat 2 

# weights:  56
initial  value 12.123680 
iter  10 value 4.393975
final  value 4.393931 
converged
Fitting Repeat 3 

# weights:  56
initial  value 12.571331 
iter  10 value 4.393933
final  value 4.393931 
converged
Fitting Repeat 4 

# weights:  56
initial  value 15.837050 
iter  10 value 4.393993
final  value 4.393931 
converged
Fitting Repeat 5 

# weights:  56
initial  value 12.783721 
iter  10 value 4.394019
final  value 4.393931 
converged
Fitting Repeat 1 

# weights:  111
initial  value 22.724530 
iter  10 value 4.394194
final  value 4.393857 
converged
Fitting Repeat 2 

# weights:  111
initial  value 13.899885 
iter  10 value 4.393961
final  value 4.393857 
converged
Fitting Repeat 3 

# weights:  111
initial  value 16.993434 
iter  10 value 4.393901
final  value 4.393857 
converged
Fitting Repeat 4 

# weights:  111
initial  value 16.525574 
iter  10 value 4.393876
final  value 4.393857 
converged
Fitting Repeat 5 

# weights:  111
initial  value 16.395556 
iter  10 value 4.400358
final  value 4.393857 
converged
Fitting Repeat 1 

# weights:  67
initial  value 15.245258 
iter  10 value 4.393940
final  value 4.393926 
converged
Fitting Repeat 2 

# weights:  67
initial  value 14.244989 
iter  10 value 4.393954
final  value 4.393926 
converged
Fitting Repeat 3 

# weights:  67
initial  value 15.317002 
iter  10 value 4.393955
final  value 4.393926 
converged
Fitting Repeat 4 

# weights:  67
initial  value 18.682898 
iter  10 value 4.394112
final  value 4.393926 
converged
Fitting Repeat 5 

# weights:  67
initial  value 15.403985 
iter  10 value 4.393960
final  value 4.393926 
converged
Fitting Repeat 1 

# weights:  89
initial  value 4.921004 
iter  10 value 1.857340
iter  20 value 1.831636
iter  30 value 1.829804
iter  40 value 1.829051
iter  50 value 1.828513
iter  60 value 1.828265
iter  70 value 1.828210
iter  80 value 1.828207
final  value 1.828206 
converged
Fitting Repeat 2 

# weights:  89
initial  value 5.478695 
iter  10 value 2.040387
iter  20 value 1.983370
iter  30 value 1.973923
iter  40 value 1.973267
iter  50 value 1.973187
iter  60 value 1.973183
final  value 1.973183 
converged
Fitting Repeat 3 

# weights:  89
initial  value 10.782706 
iter  10 value 2.241533
iter  20 value 2.171508
iter  30 value 2.162504
iter  40 value 2.161847
iter  50 value 2.161744
iter  60 value 2.161741
iter  60 value 2.161741
iter  60 value 2.161741
final  value 2.161741 
converged
Fitting Repeat 4 

# weights:  89
initial  value 5.782959 
iter  10 value 1.934263
iter  20 value 1.856163
iter  30 value 1.841096
iter  40 value 1.837600
iter  50 value 1.836545
iter  60 value 1.836532
iter  70 value 1.836525
iter  80 value 1.834602
iter  90 value 1.832639
iter 100 value 1.832374
final  value 1.832374 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 6.045954 
iter  10 value 2.128928
iter  20 value 2.091464
iter  30 value 2.079722
iter  40 value 2.076049
iter  50 value 2.075591
iter  60 value 2.075573
final  value 2.075573 
converged
Fitting Repeat 1 

# weights:  122
initial  value 32.278938 
iter  10 value 4.395029
final  value 4.393918 
converged
Fitting Repeat 2 

# weights:  122
initial  value 28.831027 
iter  10 value 4.400932
final  value 4.393918 
converged
Fitting Repeat 3 

# weights:  122
initial  value 29.014822 
iter  10 value 4.396239
final  value 4.393918 
converged
Fitting Repeat 4 

# weights:  122
initial  value 28.874827 
iter  10 value 4.396005
final  value 4.393918 
converged
Fitting Repeat 5 

# weights:  122
initial  value 31.600157 
iter  10 value 4.400664
final  value 4.393918 
converged
Fitting Repeat 1 

# weights:  111
initial  value 7.762477 
iter  10 value 3.801911
iter  20 value 3.741653
iter  30 value 3.731540
iter  40 value 3.727783
iter  50 value 3.726478
iter  60 value 3.726409
final  value 3.726409 
converged
Fitting Repeat 2 

# weights:  111
initial  value 6.736623 
iter  10 value 3.419056
iter  20 value 3.409275
iter  30 value 3.406693
iter  40 value 3.406561
final  value 3.406559 
converged
Fitting Repeat 3 

# weights:  111
initial  value 6.667823 
iter  10 value 3.072028
iter  20 value 3.063440
iter  30 value 3.058273
iter  40 value 3.055505
iter  50 value 3.055441
final  value 3.055441 
converged
Fitting Repeat 4 

# weights:  111
initial  value 7.219814 
iter  10 value 3.260904
iter  20 value 3.256405
iter  30 value 3.253980
iter  40 value 3.252824
iter  50 value 3.252062
iter  60 value 3.252030
final  value 3.252030 
converged
Fitting Repeat 5 

# weights:  111
initial  value 8.243422 
iter  10 value 3.704238
iter  20 value 3.671018
iter  30 value 3.660508
iter  40 value 3.660125
final  value 3.660124 
converged
Fitting Repeat 1 

# weights:  199
initial  value 7.809566 
iter  10 value 0.505972
iter  20 value 0.363924
iter  30 value 0.336325
iter  40 value 0.331218
iter  50 value 0.329318
iter  60 value 0.328389
iter  70 value 0.327341
iter  80 value 0.326546
iter  90 value 0.326223
iter 100 value 0.325954
final  value 0.325954 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 10.073668 
iter  10 value 0.501890
iter  20 value 0.353333
iter  30 value 0.336958
iter  40 value 0.331572
iter  50 value 0.328227
iter  60 value 0.326779
iter  70 value 0.326090
iter  80 value 0.325847
iter  90 value 0.325682
iter 100 value 0.325617
final  value 0.325617 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 4.363847 
iter  10 value 0.469997
iter  20 value 0.338795
iter  30 value 0.329586
iter  40 value 0.328302
iter  50 value 0.327232
iter  60 value 0.326585
iter  70 value 0.325972
iter  80 value 0.325736
iter  90 value 0.325642
iter 100 value 0.325544
final  value 0.325544 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 11.468759 
iter  10 value 0.526438
iter  20 value 0.360892
iter  30 value 0.336573
iter  40 value 0.334056
iter  50 value 0.330961
iter  60 value 0.328823
iter  70 value 0.327628
iter  80 value 0.327114
iter  90 value 0.326910
iter 100 value 0.326573
final  value 0.326573 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 4.848182 
iter  10 value 0.507579
iter  20 value 0.351879
iter  30 value 0.336870
iter  40 value 0.334067
iter  50 value 0.331441
iter  60 value 0.329232
iter  70 value 0.328197
iter  80 value 0.327069
iter  90 value 0.326294
iter 100 value 0.325889
final  value 0.325889 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 14.464471 
iter  10 value 4.398164
iter  20 value 4.393827
final  value 4.393827 
converged
Fitting Repeat 2 

# weights:  177
initial  value 15.286030 
iter  10 value 4.393944
final  value 4.393827 
converged
Fitting Repeat 3 

# weights:  177
initial  value 19.491848 
iter  10 value 4.394631
iter  20 value 4.393827
iter  20 value 4.393827
iter  20 value 4.393827
final  value 4.393827 
converged
Fitting Repeat 4 

# weights:  177
initial  value 17.773317 
iter  10 value 4.399904
final  value 4.393827 
converged
Fitting Repeat 5 

# weights:  177
initial  value 15.683362 
iter  10 value 4.393900
final  value 4.393827 
converged
Fitting Repeat 1 

# weights:  210
initial  value 10.966321 
iter  10 value 2.953702
iter  20 value 2.943231
iter  30 value 2.941778
iter  40 value 2.941051
iter  50 value 2.940829
iter  60 value 2.939266
iter  70 value 2.936708
iter  80 value 2.936624
iter  90 value 2.936603
final  value 2.936602 
converged
Fitting Repeat 2 

# weights:  210
initial  value 11.966525 
iter  10 value 2.958690
iter  20 value 2.946246
iter  30 value 2.940272
iter  40 value 2.937692
iter  50 value 2.936749
iter  60 value 2.936645
iter  70 value 2.936634
final  value 2.936633 
converged
Fitting Repeat 3 

# weights:  210
initial  value 11.558457 
iter  10 value 2.948923
iter  20 value 2.938846
iter  30 value 2.936800
iter  40 value 2.935364
iter  50 value 2.934921
iter  60 value 2.934917
final  value 2.934917 
converged
Fitting Repeat 4 

# weights:  210
initial  value 9.605649 
iter  10 value 2.961226
iter  20 value 2.945891
iter  30 value 2.941319
iter  40 value 2.937542
iter  50 value 2.936853
iter  60 value 2.936672
iter  70 value 2.936636
final  value 2.936633 
converged
Fitting Repeat 5 

# weights:  210
initial  value 8.420188 
iter  10 value 2.958212
iter  20 value 2.941304
iter  30 value 2.938344
iter  40 value 2.937183
iter  50 value 2.936650
iter  60 value 2.936635
iter  70 value 2.936633
final  value 2.936633 
converged
Fitting Repeat 1 

# weights:  78
initial  value 3.929580 
iter  10 value 0.173831
iter  20 value 0.120808
iter  30 value 0.107655
iter  40 value 0.099871
iter  50 value 0.096746
iter  60 value 0.093621
iter  70 value 0.091857
iter  80 value 0.089805
iter  90 value 0.088850
iter 100 value 0.087548
final  value 0.087548 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  78
initial  value 4.530765 
iter  10 value 0.206440
iter  20 value 0.120112
iter  30 value 0.105299
iter  40 value 0.097874
iter  50 value 0.092246
iter  60 value 0.091332
iter  70 value 0.090657
iter  80 value 0.089866
iter  90 value 0.089564
iter 100 value 0.089170
final  value 0.089170 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  78
initial  value 7.694198 
iter  10 value 0.486524
iter  20 value 0.109819
iter  30 value 0.094838
iter  40 value 0.090789
iter  50 value 0.088002
iter  60 value 0.087491
iter  70 value 0.086919
iter  80 value 0.086398
iter  90 value 0.086062
iter 100 value 0.085925
final  value 0.085925 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  78
initial  value 6.894943 
iter  10 value 0.147829
iter  20 value 0.106633
iter  30 value 0.093817
iter  40 value 0.090374
iter  50 value 0.088784
iter  60 value 0.087754
iter  70 value 0.087016
iter  80 value 0.086209
iter  90 value 0.085813
iter 100 value 0.085545
final  value 0.085545 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  78
initial  value 5.413392 
iter  10 value 0.184686
iter  20 value 0.118795
iter  30 value 0.098478
iter  40 value 0.092391
iter  50 value 0.089724
iter  60 value 0.088213
iter  70 value 0.087713
iter  80 value 0.087159
iter  90 value 0.086806
iter 100 value 0.086645
final  value 0.086645 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 6.690401 
iter  10 value 0.369366
iter  20 value 0.272833
iter  30 value 0.261698
iter  40 value 0.259314
iter  50 value 0.257863
iter  60 value 0.256823
iter  70 value 0.255836
iter  80 value 0.255465
iter  90 value 0.255314
iter 100 value 0.255206
final  value 0.255206 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  177
initial  value 9.539644 
iter  10 value 0.424191
iter  20 value 0.267437
iter  30 value 0.253039
iter  40 value 0.251451
iter  50 value 0.250220
iter  60 value 0.249193
iter  70 value 0.248523
iter  80 value 0.248375
iter  90 value 0.248342
iter 100 value 0.248325
final  value 0.248325 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  177
initial  value 8.270793 
iter  10 value 0.470301
iter  20 value 0.319299
iter  30 value 0.289359
iter  40 value 0.284837
iter  50 value 0.280903
iter  60 value 0.279462
iter  70 value 0.278401
iter  80 value 0.277479
iter  90 value 0.277123
iter 100 value 0.276939
final  value 0.276939 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  177
initial  value 5.495775 
iter  10 value 0.407213
iter  20 value 0.290657
iter  30 value 0.276736
iter  40 value 0.274977
iter  50 value 0.273692
iter  60 value 0.272619
iter  70 value 0.272256
iter  80 value 0.271961
iter  90 value 0.271670
iter 100 value 0.271482
final  value 0.271482 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  177
initial  value 9.041231 
iter  10 value 0.582297
iter  20 value 0.310645
iter  30 value 0.277613
iter  40 value 0.269851
iter  50 value 0.262459
iter  60 value 0.259944
iter  70 value 0.259007
iter  80 value 0.258267
iter  90 value 0.257496
iter 100 value 0.257267
final  value 0.257267 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  199
initial  value 4.495506 
iter  10 value 0.292365
iter  20 value 0.173853
iter  30 value 0.145820
iter  40 value 0.138696
iter  50 value 0.136352
iter  60 value 0.134948
iter  70 value 0.134324
iter  80 value 0.134049
iter  90 value 0.133867
iter 100 value 0.133663
final  value 0.133663 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 4.683349 
iter  10 value 0.220439
iter  20 value 0.154791
iter  30 value 0.140409
iter  40 value 0.137575
iter  50 value 0.136763
iter  60 value 0.135621
iter  70 value 0.134783
iter  80 value 0.134251
iter  90 value 0.133916
iter 100 value 0.133554
final  value 0.133554 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 4.474196 
iter  10 value 0.255846
iter  20 value 0.172188
iter  30 value 0.145511
iter  40 value 0.138150
iter  50 value 0.136901
iter  60 value 0.135797
iter  70 value 0.134706
iter  80 value 0.134109
iter  90 value 0.133708
iter 100 value 0.133408
final  value 0.133408 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 6.212448 
iter  10 value 0.235569
iter  20 value 0.162109
iter  30 value 0.145463
iter  40 value 0.138683
iter  50 value 0.136474
iter  60 value 0.134938
iter  70 value 0.134318
iter  80 value 0.133997
iter  90 value 0.133590
iter 100 value 0.133341
final  value 0.133341 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 4.358817 
iter  10 value 0.225459
iter  20 value 0.157627
iter  30 value 0.139714
iter  40 value 0.136164
iter  50 value 0.135499
iter  60 value 0.134844
iter  70 value 0.134349
iter  80 value 0.133941
iter  90 value 0.133748
iter 100 value 0.133670
final  value 0.133670 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  144
initial  value 8.296086 
iter  10 value 2.764036
iter  20 value 2.736370
iter  30 value 2.725727
iter  40 value 2.723088
iter  50 value 2.722605
iter  60 value 2.722594
final  value 2.722592 
converged
Fitting Repeat 2 

# weights:  144
initial  value 8.910704 
iter  10 value 2.794775
iter  20 value 2.743967
iter  30 value 2.726764
iter  40 value 2.723895
iter  50 value 2.722038
iter  60 value 2.721628
iter  70 value 2.721620
iter  70 value 2.721620
iter  70 value 2.721620
final  value 2.721620 
converged
Fitting Repeat 3 

# weights:  144
initial  value 7.938285 
iter  10 value 2.758058
iter  20 value 2.737709
iter  30 value 2.728655
iter  40 value 2.726332
iter  50 value 2.723745
iter  60 value 2.721995
iter  70 value 2.721662
final  value 2.721620 
converged
Fitting Repeat 4 

# weights:  144
initial  value 9.253616 
iter  10 value 2.741681
iter  20 value 2.734018
iter  30 value 2.732344
iter  40 value 2.731815
iter  50 value 2.731776
iter  60 value 2.731769
final  value 2.731768 
converged
Fitting Repeat 5 

# weights:  144
initial  value 9.376978 
iter  10 value 2.786141
iter  20 value 2.730015
iter  30 value 2.724244
iter  40 value 2.722793
iter  50 value 2.722602
iter  60 value 2.722592
final  value 2.722591 
converged
Fitting Repeat 1 

# weights:  34
initial  value 4.525335 
iter  10 value 0.230009
iter  20 value 0.039066
iter  30 value 0.029823
iter  40 value 0.024414
iter  50 value 0.021511
iter  60 value 0.021048
iter  70 value 0.020842
iter  80 value 0.020529
iter  90 value 0.020239
iter 100 value 0.019917
final  value 0.019917 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  34
initial  value 6.251932 
iter  10 value 0.327652
iter  20 value 0.048682
iter  30 value 0.030375
iter  40 value 0.026178
iter  50 value 0.024334
iter  60 value 0.023218
iter  70 value 0.022526
iter  80 value 0.022221
iter  90 value 0.021535
iter 100 value 0.021164
final  value 0.021164 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  34
initial  value 5.286980 
iter  10 value 0.310257
iter  20 value 0.045484
iter  30 value 0.043101
iter  40 value 0.041190
iter  50 value 0.038691
iter  60 value 0.035839
iter  70 value 0.033372
iter  80 value 0.032000
iter  90 value 0.030353
iter 100 value 0.028495
final  value 0.028495 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  34
initial  value 6.058141 
iter  10 value 0.249901
iter  20 value 0.055332
iter  30 value 0.040412
iter  40 value 0.036931
iter  50 value 0.034411
iter  60 value 0.031137
iter  70 value 0.030371
iter  80 value 0.029694
iter  90 value 0.028829
iter 100 value 0.026862
final  value 0.026862 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  34
initial  value 5.489241 
iter  10 value 1.175232
iter  20 value 0.072286
iter  30 value 0.041486
iter  40 value 0.035481
iter  50 value 0.030083
iter  60 value 0.022479
iter  70 value 0.021374
iter  80 value 0.020603
iter  90 value 0.020362
iter 100 value 0.020059
final  value 0.020059 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  133
initial  value 7.617194 
iter  10 value 0.994713
iter  20 value 0.928775
iter  30 value 0.923282
iter  40 value 0.918094
iter  50 value 0.916132
iter  60 value 0.913754
iter  70 value 0.913279
iter  80 value 0.913035
iter  90 value 0.912970
iter 100 value 0.912956
final  value 0.912956 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  133
initial  value 7.797263 
iter  10 value 1.102894
iter  20 value 0.925811
iter  30 value 0.918453
iter  40 value 0.915342
iter  50 value 0.914140
iter  60 value 0.913778
iter  70 value 0.913149
iter  80 value 0.913044
iter  90 value 0.912980
iter 100 value 0.912949
final  value 0.912949 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 7.028503 
iter  10 value 1.046945
iter  20 value 0.932977
iter  30 value 0.922737
iter  40 value 0.919837
iter  50 value 0.916113
iter  60 value 0.914265
iter  70 value 0.911954
iter  80 value 0.911842
iter  90 value 0.911778
iter 100 value 0.911740
final  value 0.911740 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  133
initial  value 7.041357 
iter  10 value 1.123749
iter  20 value 0.952511
iter  30 value 0.936602
iter  40 value 0.924618
iter  50 value 0.920929
iter  60 value 0.918587
iter  70 value 0.917856
iter  80 value 0.917489
iter  90 value 0.917268
iter 100 value 0.917227
final  value 0.917227 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  133
initial  value 8.773217 
iter  10 value 1.068341
iter  20 value 0.934264
iter  30 value 0.926239
iter  40 value 0.919511
iter  50 value 0.917899
iter  60 value 0.915829
iter  70 value 0.914494
iter  80 value 0.913853
iter  90 value 0.913182
iter 100 value 0.913060
final  value 0.913060 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  221
initial  value 122.917037 
iter  10 value 4.046076
iter  20 value 4.043631
final  value 4.043630 
converged
Fitting Repeat 2 

# weights:  221
initial  value 127.296625 
iter  10 value 4.293973
iter  20 value 3.816812
final  value 3.816810 
converged
Fitting Repeat 3 

# weights:  221
initial  value 123.702946 
iter  10 value 4.387830
iter  20 value 3.506256
final  value 3.505686 
converged
Fitting Repeat 4 

# weights:  221
initial  value 124.783080 
iter  10 value 3.817796
iter  20 value 3.711900
final  value 3.711900 
converged
Fitting Repeat 5 

# weights:  221
initial  value 130.978462 
iter  10 value 3.862080
iter  20 value 3.830651
final  value 3.830649 
converged
Fitting Repeat 1 

# weights:  89
initial  value 6.534427 
iter  10 value 3.374844
iter  20 value 3.350170
iter  30 value 3.349113
iter  40 value 3.349068
final  value 3.349067 
converged
Fitting Repeat 2 

# weights:  89
initial  value 7.204378 
iter  10 value 3.396359
iter  20 value 3.364533
iter  30 value 3.359058
iter  40 value 3.355773
iter  50 value 3.349727
iter  60 value 3.348396
final  value 3.348395 
converged
Fitting Repeat 3 

# weights:  89
initial  value 6.114903 
iter  10 value 3.376350
iter  20 value 3.352001
iter  30 value 3.349673
iter  40 value 3.349090
iter  50 value 3.349068
final  value 3.349068 
converged
Fitting Repeat 4 

# weights:  89
initial  value 6.772818 
iter  10 value 3.375162
iter  20 value 3.356107
iter  30 value 3.349783
iter  40 value 3.348472
iter  50 value 3.348400
final  value 3.348395 
converged
Fitting Repeat 5 

# weights:  89
initial  value 9.092192 
iter  10 value 3.375811
iter  20 value 3.361467
iter  30 value 3.350661
iter  40 value 3.348862
iter  50 value 3.348415
final  value 3.348395 
converged
Fitting Repeat 1 

# weights:  111
initial  value 10.188470 
iter  10 value 0.720230
iter  20 value 0.464605
iter  30 value 0.456889
iter  40 value 0.453090
iter  50 value 0.449888
iter  60 value 0.447489
iter  70 value 0.446939
iter  80 value 0.446504
iter  90 value 0.446340
iter 100 value 0.446292
final  value 0.446292 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  111
initial  value 8.166386 
iter  10 value 0.674221
iter  20 value 0.475668
iter  30 value 0.468162
iter  40 value 0.465882
iter  50 value 0.462980
iter  60 value 0.462229
iter  70 value 0.461803
iter  80 value 0.461591
iter  90 value 0.461531
iter 100 value 0.461521
final  value 0.461521 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  111
initial  value 4.319129 
iter  10 value 0.545833
iter  20 value 0.484554
iter  30 value 0.481435
iter  40 value 0.478465
iter  50 value 0.476827
iter  60 value 0.476151
iter  70 value 0.476059
iter  80 value 0.475998
iter  90 value 0.475967
iter 100 value 0.475961
final  value 0.475961 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  111
initial  value 7.913332 
iter  10 value 0.648077
iter  20 value 0.474657
iter  30 value 0.466250
iter  40 value 0.464156
iter  50 value 0.461614
iter  60 value 0.460577
iter  70 value 0.459823
iter  80 value 0.459323
iter  90 value 0.458763
iter 100 value 0.458582
final  value 0.458582 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  111
initial  value 3.877611 
iter  10 value 0.587384
iter  20 value 0.482046
iter  30 value 0.471072
iter  40 value 0.467230
iter  50 value 0.462760
iter  60 value 0.459093
iter  70 value 0.458017
iter  80 value 0.456645
iter  90 value 0.456286
iter 100 value 0.456181
final  value 0.456181 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  111
initial  value 59.136563 
iter  10 value 4.031750
iter  20 value 3.910069
final  value 3.910069 
converged
Fitting Repeat 2 

# weights:  111
initial  value 58.333635 
iter  10 value 3.918586
iter  20 value 3.910069
iter  20 value 3.910069
iter  20 value 3.910069
final  value 3.910069 
converged
Fitting Repeat 3 

# weights:  111
initial  value 56.826103 
iter  10 value 3.925415
final  value 3.910069 
converged
Fitting Repeat 4 

# weights:  111
initial  value 58.958459 
iter  10 value 3.924382
final  value 3.910069 
converged
Fitting Repeat 5 

# weights:  111
initial  value 58.841221 
iter  10 value 3.915850
iter  20 value 3.910070
final  value 3.910069 
converged
Fitting Repeat 1 

# weights:  111
initial  value 82.987954 
iter  10 value 3.930841
iter  20 value 3.910258
final  value 3.910258 
converged
Fitting Repeat 2 

# weights:  111
initial  value 108.426783 
iter  10 value 4.207637
iter  20 value 3.910259
final  value 3.910258 
converged
Fitting Repeat 3 

# weights:  111
initial  value 88.558729 
iter  10 value 3.922295
iter  20 value 3.910258
final  value 3.910258 
converged
Fitting Repeat 4 

# weights:  111
initial  value 80.203699 
iter  10 value 4.036973
iter  20 value 3.910260
final  value 3.910258 
converged
Fitting Repeat 5 

# weights:  111
initial  value 93.989698 
iter  10 value 3.952482
iter  20 value 3.910258
final  value 3.910258 
converged
Fitting Repeat 1 

# weights:  56
initial  value 15.316416 
iter  10 value 3.909939
final  value 3.909765 
converged
Fitting Repeat 2 

# weights:  56
initial  value 15.183262 
iter  10 value 3.909794
final  value 3.909765 
converged
Fitting Repeat 3 

# weights:  56
initial  value 12.471178 
iter  10 value 3.909944
final  value 3.909765 
converged
Fitting Repeat 4 

# weights:  56
initial  value 11.808637 
iter  10 value 3.909775
final  value 3.909765 
converged
Fitting Repeat 5 

# weights:  56
initial  value 12.386453 
iter  10 value 3.909795
final  value 3.909765 
converged
Fitting Repeat 1 

# weights:  111
initial  value 15.813356 
iter  10 value 3.909628
final  value 3.909596 
converged
Fitting Repeat 2 

# weights:  111
initial  value 14.438509 
iter  10 value 3.909693
final  value 3.909596 
converged
Fitting Repeat 3 

# weights:  111
initial  value 18.771756 
iter  10 value 3.910136
final  value 3.909596 
converged
Fitting Repeat 4 

# weights:  111
initial  value 16.449515 
iter  10 value 3.909648
final  value 3.909596 
converged
Fitting Repeat 5 

# weights:  111
initial  value 13.526365 
iter  10 value 3.909648
final  value 3.909596 
converged
Fitting Repeat 1 

# weights:  67
initial  value 14.149731 
iter  10 value 3.909812
final  value 3.909754 
converged
Fitting Repeat 2 

# weights:  67
initial  value 17.396231 
iter  10 value 3.909768
final  value 3.909754 
converged
Fitting Repeat 3 

# weights:  67
initial  value 16.129694 
iter  10 value 3.909996
final  value 3.909754 
converged
Fitting Repeat 4 

# weights:  67
initial  value 15.275030 
iter  10 value 3.909780
final  value 3.909754 
converged
Fitting Repeat 5 

# weights:  67
initial  value 13.368502 
iter  10 value 3.909760
final  value 3.909754 
converged
Fitting Repeat 1 

# weights:  89
initial  value 4.755394 
iter  10 value 1.862792
iter  20 value 1.823080
iter  30 value 1.810618
iter  40 value 1.806383
iter  50 value 1.805043
iter  60 value 1.804659
iter  70 value 1.804630
iter  80 value 1.804617
final  value 1.804616 
converged
Fitting Repeat 2 

# weights:  89
initial  value 6.876484 
iter  10 value 1.903791
iter  20 value 1.874109
iter  30 value 1.868419
iter  40 value 1.861831
iter  50 value 1.860422
iter  60 value 1.860421
iter  60 value 1.860421
iter  60 value 1.860421
final  value 1.860421 
converged
Fitting Repeat 3 

# weights:  89
initial  value 5.738147 
iter  10 value 1.897008
iter  20 value 1.846723
iter  30 value 1.836830
iter  40 value 1.835087
iter  50 value 1.834373
iter  60 value 1.834031
iter  70 value 1.833719
iter  80 value 1.833554
iter  90 value 1.833528
iter 100 value 1.833488
final  value 1.833488 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 7.631581 
iter  10 value 1.949304
iter  20 value 1.923515
iter  30 value 1.919439
iter  40 value 1.918775
iter  50 value 1.918375
iter  60 value 1.918345
final  value 1.918345 
converged
Fitting Repeat 5 

# weights:  89
initial  value 5.594229 
iter  10 value 1.962262
iter  20 value 1.936578
iter  30 value 1.916883
iter  40 value 1.915411
iter  50 value 1.914883
iter  60 value 1.914872
iter  70 value 1.914868
final  value 1.914867 
converged
Fitting Repeat 1 

# weights:  122
initial  value 28.944260 
iter  10 value 3.910758
final  value 3.909733 
converged
Fitting Repeat 2 

# weights:  122
initial  value 32.343699 
iter  10 value 3.910328
final  value 3.909733 
converged
Fitting Repeat 3 

# weights:  122
initial  value 30.137919 
iter  10 value 3.911012
final  value 3.909733 
converged
Fitting Repeat 4 

# weights:  122
initial  value 28.981385 
iter  10 value 3.914345
final  value 3.909733 
converged
Fitting Repeat 5 

# weights:  122
initial  value 38.520248 
iter  10 value 3.910097
final  value 3.909733 
converged
Fitting Repeat 1 

# weights:  111
initial  value 6.623510 
iter  10 value 3.309190
iter  20 value 3.286103
iter  30 value 3.277048
iter  40 value 3.276343
iter  50 value 3.276336
final  value 3.276335 
converged
Fitting Repeat 2 

# weights:  111
initial  value 5.614453 
iter  10 value 2.712614
iter  20 value 2.710507
iter  30 value 2.709823
iter  40 value 2.709533
iter  50 value 2.708536
iter  60 value 2.708314
final  value 2.708309 
converged
Fitting Repeat 3 

# weights:  111
initial  value 8.448674 
iter  10 value 3.403841
iter  20 value 3.391863
iter  30 value 3.386667
iter  40 value 3.385900
iter  50 value 3.385878
final  value 3.385877 
converged
Fitting Repeat 4 

# weights:  111
initial  value 11.810838 
iter  10 value 3.442473
iter  20 value 3.405344
iter  30 value 3.394630
iter  40 value 3.388935
iter  50 value 3.388091
final  value 3.388087 
converged
Fitting Repeat 5 

# weights:  111
initial  value 7.708568 
iter  10 value 3.493575
iter  20 value 3.479519
iter  30 value 3.476258
iter  40 value 3.475148
iter  50 value 3.475125
final  value 3.475124 
converged
Fitting Repeat 1 

# weights:  199
initial  value 4.425824 
iter  10 value 0.504453
iter  20 value 0.331792
iter  30 value 0.307953
iter  40 value 0.300306
iter  50 value 0.296656
iter  60 value 0.295501
iter  70 value 0.294071
iter  80 value 0.293008
iter  90 value 0.292621
iter 100 value 0.292288
final  value 0.292288 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 7.269683 
iter  10 value 0.575536
iter  20 value 0.344919
iter  30 value 0.311709
iter  40 value 0.305221
iter  50 value 0.300981
iter  60 value 0.297737
iter  70 value 0.295624
iter  80 value 0.294542
iter  90 value 0.293879
iter 100 value 0.293490
final  value 0.293490 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 4.570711 
iter  10 value 0.443053
iter  20 value 0.317748
iter  30 value 0.305074
iter  40 value 0.298498
iter  50 value 0.296182
iter  60 value 0.294024
iter  70 value 0.293089
iter  80 value 0.292658
iter  90 value 0.292482
iter 100 value 0.292388
final  value 0.292388 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 4.852450 
iter  10 value 0.462822
iter  20 value 0.319428
iter  30 value 0.300672
iter  40 value 0.299248
iter  50 value 0.296898
iter  60 value 0.295091
iter  70 value 0.293890
iter  80 value 0.293333
iter  90 value 0.293080
iter 100 value 0.292600
final  value 0.292600 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 4.713049 
iter  10 value 0.538010
iter  20 value 0.324730
iter  30 value 0.299126
iter  40 value 0.296755
iter  50 value 0.294773
iter  60 value 0.293797
iter  70 value 0.292838
iter  80 value 0.292531
iter  90 value 0.292325
iter 100 value 0.292151
final  value 0.292151 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 20.103183 
iter  10 value 3.930690
iter  20 value 3.909529
final  value 3.909529 
converged
Fitting Repeat 2 

# weights:  177
initial  value 18.898591 
iter  10 value 3.910592
final  value 3.909529 
converged
Fitting Repeat 3 

# weights:  177
initial  value 18.396836 
iter  10 value 3.909714
final  value 3.909529 
converged
Fitting Repeat 4 

# weights:  177
initial  value 15.432464 
iter  10 value 3.909719
final  value 3.909529 
converged
Fitting Repeat 5 

# weights:  177
initial  value 16.730054 
iter  10 value 3.909718
final  value 3.909529 
converged
Fitting Repeat 1 

# weights:  34
initial  value 6.609925 
iter  10 value 0.380260
iter  20 value 0.095426
iter  30 value 0.084688
iter  40 value 0.077420
iter  50 value 0.074089
iter  60 value 0.072304
iter  70 value 0.069032
iter  80 value 0.066715
iter  90 value 0.065067
iter 100 value 0.063965
final  value 0.063965 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  34
initial  value 6.595631 
iter  10 value 0.208511
iter  20 value 0.067357
iter  30 value 0.056568
iter  40 value 0.043778
iter  50 value 0.038703
iter  60 value 0.036378
iter  70 value 0.033039
iter  80 value 0.031790
iter  90 value 0.031157
iter 100 value 0.030383
final  value 0.030383 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  34
initial  value 7.250469 
iter  10 value 0.344191
iter  20 value 0.068641
iter  30 value 0.038486
iter  40 value 0.030437
iter  50 value 0.029371
iter  60 value 0.029284
iter  70 value 0.029071
iter  80 value 0.028972
iter  90 value 0.028793
iter 100 value 0.028134
final  value 0.028134 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  34
initial  value 9.077415 
iter  10 value 0.897008
iter  20 value 0.088084
iter  30 value 0.064044
iter  40 value 0.041903
iter  50 value 0.033304
iter  60 value 0.029061
iter  70 value 0.028644
iter  80 value 0.028362
iter  90 value 0.027875
iter 100 value 0.027494
final  value 0.027494 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  34
initial  value 6.566847 
iter  10 value 0.274461
iter  20 value 0.111253
iter  30 value 0.085245
iter  40 value 0.077026
iter  50 value 0.071825
iter  60 value 0.069616
iter  70 value 0.068567
iter  80 value 0.067325
iter  90 value 0.066911
iter 100 value 0.065626
final  value 0.065626 
stopped after 100 iterations
Model Averaged Neural Network 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  size  decay        bag    RMSE        Rsquared   MAE         Selected
   3    0.000107316  FALSE  0.02978697  0.9904535  0.02318791  *       
   5    0.880642313  FALSE  0.28972985  0.9244134  0.25141664          
   6    0.933051753  FALSE  0.28973170  0.9242392  0.25141846          
   7    0.001347284  FALSE  0.03656088  0.9868920  0.02986538          
   8    0.062220260   TRUE  0.11537519  0.9729876  0.09789236          
   8    0.149971978  FALSE  0.19047932  0.9638371  0.16491848          
  10    0.011324397   TRUE  0.04424044  0.9901280  0.03231584          
  10    0.159493552   TRUE  0.21283041  0.9647954  0.18498958          
  10    0.561950384  FALSE  0.28975773  0.9241889  0.25144390          
  10    3.154983040  FALSE  0.28968133  0.9244493  0.25136758          
  10    4.593273730  FALSE  0.28965197  0.9241848  0.25133684          
  11    1.253977888  FALSE  0.28973554  0.9243999  0.25142206          
  12    0.024664092  FALSE  0.05732719  0.9912762  0.04403848          
  13    0.104343923  FALSE  0.14708280  0.9780015  0.12633458          
  16    0.005924272   TRUE  0.03603214  0.9889403  0.02777826          
  16    0.393247400  FALSE  0.28976893  0.9240665  0.25145473          
  18    0.002660376  FALSE  0.03403166  0.9890963  0.02730878          
  18    0.006789566  FALSE  0.03541692  0.9904509  0.02646751          
  19    0.119107524  FALSE  0.15954017  0.9756133  0.13775436          
  20    3.216321273   TRUE  0.29002121  0.9190313  0.25138951          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 3, decay = 0.000107316 and
 bag = FALSE.
[1] "Sat Mar 10 02:27:29 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:27:44 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "bag"                     
Bagged MARS 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  degree  nprune  RMSE         Rsquared   MAE           Selected
  1       2       0.056050616  0.9751905  0.0428198847          
  1       4       0.001450835  0.9999727  0.0005534367          
  2       2       0.060514361  0.9645311  0.0454707790          
  2       3       0.001452996  0.9999727  0.0005444161          
  2       4       0.001441235  0.9999730  0.0005180921  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nprune = 4 and degree = 2.
[1] "Sat Mar 10 02:28:11 2018"
Bagged MARS using gCV Pruning 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results:

  RMSE         Rsquared  MAE         
  0.001440869  0.999973  0.0005260458

Tuning parameter 'degree' was held constant at a value of 1
[1] "Sat Mar 10 02:28:58 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :5     NA's   :5     NA's   :5    
Error : Stopping
In addition: There were 16 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 13 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:29:43 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "bam"                     
bartMachine initializing with 95 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 186.7/477.6MB
Iteration 200/1250  mem: 152.8/477.6MB
Iteration 300/1250  mem: 122/477.6MB
Iteration 400/1250  mem: 173.3/477.6MB
Iteration 500/1250  mem: 149.9/477.6MB
Iteration 600/1250  mem: 202.8/477.6MB
Iteration 700/1250  mem: 175.4/477.6MB
Iteration 800/1250  mem: 228.9/477.6MB
Iteration 900/1250  mem: 203.6/477.6MB
Iteration 1000/1250  mem: 169.3/477.6MB
Iteration 1100/1250  mem: 222.2/477.6MB
Iteration 1200/1250  mem: 188.2/477.6MB
done building BART in 1.156 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 38 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 237.6/477.6MB
Iteration 200/1250  mem: 257/477.6MB
Iteration 300/1250  mem: 276.3/477.6MB
Iteration 400/1250  mem: 195.8/477.6MB
Iteration 500/1250  mem: 213.4/477.6MB
Iteration 600/1250  mem: 232.8/477.6MB
Iteration 700/1250  mem: 252.2/477.6MB
Iteration 800/1250  mem: 269.9/477.6MB
Iteration 900/1250  mem: 289.3/477.6MB
Iteration 1000/1250  mem: 210/477.6MB
Iteration 1100/1250  mem: 228.2/477.6MB
Iteration 1200/1250  mem: 248.5/477.6MB
done building BART in 0.389 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 80 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 304.1/477.6MB
Iteration 200/1250  mem: 238.5/477.6MB
Iteration 300/1250  mem: 279.8/477.6MB
Iteration 400/1250  mem: 211/477.6MB
Iteration 500/1250  mem: 252.1/477.6MB
Iteration 600/1250  mem: 293.1/477.6MB
Iteration 700/1250  mem: 43.8/477.6MB
Iteration 800/1250  mem: 85.9/477.6MB
Iteration 900/1250  mem: 125.9/477.6MB
Iteration 1000/1250  mem: 59.1/477.6MB
Iteration 1100/1250  mem: 98/477.6MB
Iteration 1200/1250  mem: 139.1/477.6MB
done building BART in 0.935 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 89 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 98.1/477.6MB
Iteration 200/1250  mem: 142.8/477.6MB
Iteration 300/1250  mem: 64.8/477.6MB
Iteration 400/1250  mem: 109.7/477.6MB
Iteration 500/1250  mem: 152.7/477.6MB
Iteration 600/1250  mem: 75.1/477.6MB
Iteration 700/1250  mem: 118.5/477.6MB
Iteration 800/1250  mem: 161.9/477.6MB
Iteration 900/1250  mem: 85.7/477.6MB
Iteration 1000/1250  mem: 128.9/477.6MB
Iteration 1100/1250  mem: 174.4/477.6MB
Iteration 1200/1250  mem: 94.2/477.6MB
done building BART in 0.911 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 65 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 156.4/477.6MB
Iteration 200/1250  mem: 190.3/477.6MB
Iteration 300/1250  mem: 99.3/477.6MB
Iteration 400/1250  mem: 131.6/477.6MB
Iteration 500/1250  mem: 163.9/477.6MB
Iteration 600/1250  mem: 198.5/477.6MB
Iteration 700/1250  mem: 105.8/477.6MB
Iteration 800/1250  mem: 140/477.6MB
Iteration 900/1250  mem: 171.8/477.6MB
Iteration 1000/1250  mem: 206/477.6MB
Iteration 1100/1250  mem: 114.3/477.6MB
Iteration 1200/1250  mem: 147.7/477.6MB
done building BART in 0.678 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 19 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 175.7/477.6MB
Iteration 200/1250  mem: 186.6/477.6MB
Iteration 300/1250  mem: 197.5/477.6MB
Iteration 400/1250  mem: 205.8/477.6MB
Iteration 500/1250  mem: 216.7/477.6MB
Iteration 600/1250  mem: 224.9/477.6MB
Iteration 700/1250  mem: 235.9/477.6MB
Iteration 800/1250  mem: 244.1/477.6MB
Iteration 900/1250  mem: 126.6/477.6MB
Iteration 1000/1250  mem: 135.4/477.6MB
Iteration 1100/1250  mem: 146.3/477.6MB
Iteration 1200/1250  mem: 155/477.6MB
done building BART in 0.204 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 61 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 196.2/477.6MB
Iteration 200/1250  mem: 226.3/477.6MB
Iteration 300/1250  mem: 126.6/477.6MB
Iteration 400/1250  mem: 159.5/477.6MB
Iteration 500/1250  mem: 190.1/477.6MB
Iteration 600/1250  mem: 220.7/477.6MB
Iteration 700/1250  mem: 251.3/477.6MB
Iteration 800/1250  mem: 151.5/477.6MB
Iteration 900/1250  mem: 181.6/477.6MB
Iteration 1000/1250  mem: 214.2/477.6MB
Iteration 1100/1250  mem: 244.3/477.6MB
Iteration 1200/1250  mem: 146.4/477.6MB
done building BART in 0.661 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 100 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 223.1/477.6MB
Iteration 200/1250  mem: 279.7/477.6MB
Iteration 300/1250  mem: 197.4/477.6MB
Iteration 400/1250  mem: 250.1/477.6MB
Iteration 500/1250  mem: 177.2/477.6MB
Iteration 600/1250  mem: 231.4/477.6MB
Iteration 700/1250  mem: 285.7/477.6MB
Iteration 800/1250  mem: 213.5/477.6MB
Iteration 900/1250  mem: 268.7/477.6MB
Iteration 1000/1250  mem: 200.3/477.6MB
Iteration 1100/1250  mem: 253.1/477.6MB
Iteration 1200/1250  mem: 308.6/477.6MB
done building BART in 1.172 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 45 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 245/477.6MB
Iteration 200/1250  mem: 271.9/477.6MB
Iteration 300/1250  mem: 296.2/477.6MB
Iteration 400/1250  mem: 320.4/477.6MB
Iteration 500/1250  mem: 344.7/477.6MB
Iteration 600/1250  mem: 245.3/477.6MB
Iteration 700/1250  mem: 269/477.6MB
Iteration 800/1250  mem: 295.3/477.6MB
Iteration 900/1250  mem: 319/477.6MB
Iteration 1000/1250  mem: 342.7/477.6MB
Iteration 1100/1250  mem: 250.7/477.6MB
Iteration 1200/1250  mem: 274.5/477.6MB
done building BART in 0.523 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 54 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 321.1/477.6MB
Iteration 200/1250  mem: 347.8/477.6MB
Iteration 300/1250  mem: 39.1/477.6MB
Iteration 400/1250  mem: 65.3/477.6MB
Iteration 500/1250  mem: 93.7/477.6MB
Iteration 600/1250  mem: 120/477.6MB
Iteration 700/1250  mem: 148.4/477.6MB
Iteration 800/1250  mem: 49.7/477.6MB
Iteration 900/1250  mem: 77.9/477.6MB
Iteration 1000/1250  mem: 106.1/477.6MB
Iteration 1100/1250  mem: 131.9/477.6MB
Iteration 1200/1250  mem: 160/477.6MB
done building BART in 0.643 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 53 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 86.8/477.6MB
Iteration 200/1250  mem: 122.3/477.6MB
Iteration 300/1250  mem: 156.4/477.6MB
Iteration 400/1250  mem: 67.3/477.6MB
Iteration 500/1250  mem: 101.1/477.6MB
Iteration 600/1250  mem: 136.3/477.6MB
Iteration 700/1250  mem: 170.1/477.6MB
Iteration 800/1250  mem: 90.2/477.6MB
Iteration 900/1250  mem: 124.4/477.6MB
Iteration 1000/1250  mem: 156.9/477.6MB
Iteration 1100/1250  mem: 189.4/477.6MB
Iteration 1200/1250  mem: 118.4/477.6MB
done building BART in 0.717 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 53 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 169.9/477.6MB
Iteration 200/1250  mem: 199.7/477.6MB
Iteration 300/1250  mem: 229.5/477.6MB
Iteration 400/1250  mem: 143.8/477.6MB
Iteration 500/1250  mem: 172/477.6MB
Iteration 600/1250  mem: 200.1/477.6MB
Iteration 700/1250  mem: 228.3/477.6MB
Iteration 800/1250  mem: 149.5/477.6MB
Iteration 900/1250  mem: 176.7/477.6MB
Iteration 1000/1250  mem: 206.2/477.6MB
Iteration 1100/1250  mem: 235.7/477.6MB
Iteration 1200/1250  mem: 154.7/477.6MB
done building BART in 0.635 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 32 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 190.1/477.6MB
Iteration 200/1250  mem: 205/477.6MB
Iteration 300/1250  mem: 222.4/477.6MB
Iteration 400/1250  mem: 239.8/477.6MB
Iteration 500/1250  mem: 254.7/477.6MB
Iteration 600/1250  mem: 155.8/477.6MB
Iteration 700/1250  mem: 171.5/477.6MB
Iteration 800/1250  mem: 187.1/477.6MB
Iteration 900/1250  mem: 202.8/477.6MB
Iteration 1000/1250  mem: 218.4/477.6MB
Iteration 1100/1250  mem: 234.1/477.6MB
Iteration 1200/1250  mem: 252/477.6MB
done building BART in 0.316 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 53 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 174.3/477.6MB
Iteration 200/1250  mem: 202/477.6MB
Iteration 300/1250  mem: 229.7/477.6MB
Iteration 400/1250  mem: 256/477.6MB
Iteration 500/1250  mem: 282.3/477.6MB
Iteration 600/1250  mem: 188.4/477.6MB
Iteration 700/1250  mem: 214.1/477.6MB
Iteration 800/1250  mem: 241.7/477.6MB
Iteration 900/1250  mem: 267.5/477.6MB
Iteration 1000/1250  mem: 295.1/477.6MB
Iteration 1100/1250  mem: 202.4/477.6MB
Iteration 1200/1250  mem: 230/477.6MB
done building BART in 0.545 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 33 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 264.7/477.6MB
Iteration 200/1250  mem: 280.2/477.6MB
Iteration 300/1250  mem: 298.4/477.6MB
Iteration 400/1250  mem: 194.1/477.6MB
Iteration 500/1250  mem: 209.4/477.6MB
Iteration 600/1250  mem: 226.7/477.6MB
Iteration 700/1250  mem: 244/477.6MB
Iteration 800/1250  mem: 261.2/477.6MB
Iteration 900/1250  mem: 276.6/477.6MB
Iteration 1000/1250  mem: 293.9/477.6MB
Iteration 1100/1250  mem: 311.2/477.6MB
Iteration 1200/1250  mem: 207.2/477.6MB
done building BART in 0.367 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 42 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 240.7/477.6MB
Iteration 200/1250  mem: 264.3/477.6MB
Iteration 300/1250  mem: 285.3/477.6MB
Iteration 400/1250  mem: 306.2/477.6MB
Iteration 500/1250  mem: 203.8/477.6MB
Iteration 600/1250  mem: 225.3/477.6MB
Iteration 700/1250  mem: 246.8/477.6MB
Iteration 800/1250  mem: 268.3/477.6MB
Iteration 900/1250  mem: 289.9/477.6MB
Iteration 1000/1250  mem: 311.4/477.6MB
Iteration 1100/1250  mem: 330.5/477.6MB
Iteration 1200/1250  mem: 231.1/477.6MB
done building BART in 0.419 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 59 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 277.4/477.6MB
Iteration 200/1250  mem: 306.9/477.6MB
Iteration 300/1250  mem: 336.4/477.6MB
Iteration 400/1250  mem: 238.8/477.6MB
Iteration 500/1250  mem: 269.6/477.6MB
Iteration 600/1250  mem: 298/477.6MB
Iteration 700/1250  mem: 328.8/477.6MB
Iteration 800/1250  mem: 234.6/477.6MB
Iteration 900/1250  mem: 262.4/477.6MB
Iteration 1000/1250  mem: 292.7/477.6MB
Iteration 1100/1250  mem: 323.1/477.6MB
Iteration 1200/1250  mem: 353.4/477.6MB
done building BART in 0.627 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 53 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 271.3/477.6MB
Iteration 200/1250  mem: 298.7/477.6MB
Iteration 300/1250  mem: 326.1/477.6MB
Iteration 400/1250  mem: 353.5/477.6MB
Iteration 500/1250  mem: 38.3/477.6MB
Iteration 600/1250  mem: 65.9/477.6MB
Iteration 700/1250  mem: 93.5/477.6MB
Iteration 800/1250  mem: 121.1/477.6MB
Iteration 900/1250  mem: 148.7/477.6MB
Iteration 1000/1250  mem: 45.4/477.6MB
Iteration 1100/1250  mem: 73/477.6MB
Iteration 1200/1250  mem: 100.7/477.6MB
done building BART in 0.674 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 166.2/477.6MB
Iteration 200/1250  mem: 82.1/477.6MB
Iteration 300/1250  mem: 129.2/477.6MB
Iteration 400/1250  mem: 174.8/477.6MB
Iteration 500/1250  mem: 84.9/477.6MB
Iteration 600/1250  mem: 132.1/477.6MB
Iteration 700/1250  mem: 177.5/477.6MB
Iteration 800/1250  mem: 91.5/477.6MB
Iteration 900/1250  mem: 136.7/477.6MB
Iteration 1000/1250  mem: 184.2/477.6MB
Iteration 1100/1250  mem: 97/477.6MB
Iteration 1200/1250  mem: 141.6/477.6MB
done building BART in 0.928 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 78 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 211.5/477.6MB
Iteration 200/1250  mem: 116/477.6MB
Iteration 300/1250  mem: 154.8/477.6MB
Iteration 400/1250  mem: 195.6/477.6MB
Iteration 500/1250  mem: 234.4/477.6MB
Iteration 600/1250  mem: 136.6/477.6MB
Iteration 700/1250  mem: 176.4/477.6MB
Iteration 800/1250  mem: 213.9/477.6MB
Iteration 900/1250  mem: 120.2/477.6MB
Iteration 1000/1250  mem: 158.9/477.6MB
Iteration 1100/1250  mem: 197.5/477.6MB
Iteration 1200/1250  mem: 238.7/477.6MB
done building BART in 0.857 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 95 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 181/477.6MB
Iteration 200/1250  mem: 234.1/477.6MB
Iteration 300/1250  mem: 143.9/477.6MB
Iteration 400/1250  mem: 196/477.6MB
Iteration 500/1250  mem: 248.1/477.6MB
Iteration 600/1250  mem: 170.5/477.6MB
Iteration 700/1250  mem: 222.9/477.6MB
Iteration 800/1250  mem: 275.3/477.6MB
Iteration 900/1250  mem: 199/477.6MB
Iteration 1000/1250  mem: 250.4/477.6MB
Iteration 1100/1250  mem: 195/477.6MB
Iteration 1200/1250  mem: 246.6/477.6MB
done building BART in 1.122 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 38 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 297.2/477.6MB
Iteration 200/1250  mem: 207.1/477.6MB
Iteration 300/1250  mem: 226.3/477.6MB
Iteration 400/1250  mem: 245.5/477.6MB
Iteration 500/1250  mem: 264.6/477.6MB
Iteration 600/1250  mem: 283.8/477.6MB
Iteration 700/1250  mem: 303/477.6MB
Iteration 800/1250  mem: 206.5/477.6MB
Iteration 900/1250  mem: 224.8/477.6MB
Iteration 1000/1250  mem: 244.8/477.6MB
Iteration 1100/1250  mem: 263.1/477.6MB
Iteration 1200/1250  mem: 283.2/477.6MB
done building BART in 0.429 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 80 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 225/477.6MB
Iteration 200/1250  mem: 266.7/477.6MB
Iteration 300/1250  mem: 307/477.6MB
Iteration 400/1250  mem: 225.9/477.6MB
Iteration 500/1250  mem: 267.5/477.6MB
Iteration 600/1250  mem: 307.2/477.6MB
Iteration 700/1250  mem: 232.3/477.6MB
Iteration 800/1250  mem: 271/477.6MB
Iteration 900/1250  mem: 311.8/477.6MB
Iteration 1000/1250  mem: 352.2/477.6MB
Iteration 1100/1250  mem: 276.2/477.6MB
Iteration 1200/1250  mem: 317.5/477.6MB
done building BART in 0.843 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 89 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 65.9/477.6MB
Iteration 200/1250  mem: 109.6/477.6MB
Iteration 300/1250  mem: 153.3/477.6MB
Iteration 400/1250  mem: 68/477.6MB
Iteration 500/1250  mem: 110.4/477.6MB
Iteration 600/1250  mem: 154.9/477.6MB
Iteration 700/1250  mem: 71.7/477.6MB
Iteration 800/1250  mem: 116.1/477.6MB
Iteration 900/1250  mem: 158.3/477.6MB
Iteration 1000/1250  mem: 75.5/477.6MB
Iteration 1100/1250  mem: 118.4/477.6MB
Iteration 1200/1250  mem: 161.3/477.6MB
done building BART in 1.024 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 65 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 94.9/477.6MB
Iteration 200/1250  mem: 128.1/477.6MB
Iteration 300/1250  mem: 159.8/477.6MB
Iteration 400/1250  mem: 193/477.6MB
Iteration 500/1250  mem: 95.5/477.6MB
Iteration 600/1250  mem: 127.6/477.6MB
Iteration 700/1250  mem: 159.6/477.6MB
Iteration 800/1250  mem: 191.6/477.6MB
Iteration 900/1250  mem: 101.3/477.6MB
Iteration 1000/1250  mem: 132.3/477.6MB
Iteration 1100/1250  mem: 165.5/477.6MB
Iteration 1200/1250  mem: 196.5/477.6MB
done building BART in 0.676 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 19 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 101/477.6MB
Iteration 200/1250  mem: 109.6/477.6MB
Iteration 300/1250  mem: 119.7/477.6MB
Iteration 400/1250  mem: 129.7/477.6MB
Iteration 500/1250  mem: 139.8/477.6MB
Iteration 600/1250  mem: 148.5/477.6MB
Iteration 700/1250  mem: 158.5/477.6MB
Iteration 800/1250  mem: 168.6/477.6MB
Iteration 900/1250  mem: 177.2/477.6MB
Iteration 1000/1250  mem: 187.3/477.6MB
Iteration 1100/1250  mem: 197.4/477.6MB
Iteration 1200/1250  mem: 206/477.6MB
done building BART in 0.199 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 61 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 118.6/477.6MB
Iteration 200/1250  mem: 149/477.6MB
Iteration 300/1250  mem: 181/477.6MB
Iteration 400/1250  mem: 211.4/477.6MB
Iteration 500/1250  mem: 241.8/477.6MB
Iteration 600/1250  mem: 140.6/477.6MB
Iteration 700/1250  mem: 170.9/477.6MB
Iteration 800/1250  mem: 203.1/477.6MB
Iteration 900/1250  mem: 233.4/477.6MB
Iteration 1000/1250  mem: 134.7/477.6MB
Iteration 1100/1250  mem: 164.9/477.6MB
Iteration 1200/1250  mem: 197.4/477.6MB
done building BART in 0.635 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 100 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 141.5/477.6MB
Iteration 200/1250  mem: 197/477.6MB
Iteration 300/1250  mem: 252.4/477.6MB
Iteration 400/1250  mem: 175/477.6MB
Iteration 500/1250  mem: 229.2/477.6MB
Iteration 600/1250  mem: 159.4/477.6MB
Iteration 700/1250  mem: 213.7/477.6MB
Iteration 800/1250  mem: 268.1/477.6MB
Iteration 900/1250  mem: 198.5/477.6MB
Iteration 1000/1250  mem: 253.2/477.6MB
Iteration 1100/1250  mem: 305.2/477.6MB
Iteration 1200/1250  mem: 247.2/477.6MB
done building BART in 1.216 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 45 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 301.8/477.6MB
Iteration 200/1250  mem: 208.5/477.6MB
Iteration 300/1250  mem: 231.3/477.6MB
Iteration 400/1250  mem: 256/477.6MB
Iteration 500/1250  mem: 278.9/477.6MB
Iteration 600/1250  mem: 303.5/477.6MB
Iteration 700/1250  mem: 326.4/477.6MB
Iteration 800/1250  mem: 231.6/477.6MB
Iteration 900/1250  mem: 254.2/477.6MB
Iteration 1000/1250  mem: 278.9/477.6MB
Iteration 1100/1250  mem: 301.6/477.6MB
Iteration 1200/1250  mem: 326.3/477.6MB
done building BART in 0.508 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 54 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 254.9/477.6MB
Iteration 200/1250  mem: 282.2/477.6MB
Iteration 300/1250  mem: 308.1/477.6MB
Iteration 400/1250  mem: 335.4/477.6MB
Iteration 500/1250  mem: 237.6/477.6MB
Iteration 600/1250  mem: 264.6/477.6MB
Iteration 700/1250  mem: 291.6/477.6MB
Iteration 800/1250  mem: 318.6/477.6MB
Iteration 900/1250  mem: 345.7/477.6MB
Iteration 1000/1250  mem: 251.4/477.6MB
Iteration 1100/1250  mem: 279.2/477.6MB
Iteration 1200/1250  mem: 304.8/477.6MB
done building BART in 0.581 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 53 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 353.1/477.6MB
Iteration 200/1250  mem: 32.8/477.6MB
Iteration 300/1250  mem: 65.3/477.6MB
Iteration 400/1250  mem: 97.9/477.6MB
Iteration 500/1250  mem: 130.4/477.6MB
Iteration 600/1250  mem: 47/477.6MB
Iteration 700/1250  mem: 81.6/477.6MB
Iteration 800/1250  mem: 111.8/477.6MB
Iteration 900/1250  mem: 146.4/477.6MB
Iteration 1000/1250  mem: 66/477.6MB
Iteration 1100/1250  mem: 98.6/477.6MB
Iteration 1200/1250  mem: 131.3/477.6MB
done building BART in 0.765 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 53 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 180.7/477.6MB
Iteration 200/1250  mem: 91.5/477.6MB
Iteration 300/1250  mem: 119.4/477.6MB
Iteration 400/1250  mem: 147.3/477.6MB
Iteration 500/1250  mem: 175.2/477.6MB
Iteration 600/1250  mem: 203.1/477.6MB
Iteration 700/1250  mem: 112.3/477.6MB
Iteration 800/1250  mem: 140.7/477.6MB
Iteration 900/1250  mem: 169.2/477.6MB
Iteration 1000/1250  mem: 195.6/477.6MB
Iteration 1100/1250  mem: 110.7/477.6MB
Iteration 1200/1250  mem: 139.7/477.6MB
done building BART in 0.604 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 32 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 172.4/477.6MB
Iteration 200/1250  mem: 190.4/477.6MB
Iteration 300/1250  mem: 205.9/477.6MB
Iteration 400/1250  mem: 221.4/477.6MB
Iteration 500/1250  mem: 117.9/477.6MB
Iteration 600/1250  mem: 134.5/477.6MB
Iteration 700/1250  mem: 149/477.6MB
Iteration 800/1250  mem: 165.6/477.6MB
Iteration 900/1250  mem: 180.2/477.6MB
Iteration 1000/1250  mem: 196.8/477.6MB
Iteration 1100/1250  mem: 213.4/477.6MB
Iteration 1200/1250  mem: 227.9/477.6MB
done building BART in 0.333 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 53 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 147.8/477.6MB
Iteration 200/1250  mem: 174.2/477.6MB
Iteration 300/1250  mem: 200.7/477.6MB
Iteration 400/1250  mem: 227.1/477.6MB
Iteration 500/1250  mem: 253.6/477.6MB
Iteration 600/1250  mem: 153.9/477.6MB
Iteration 700/1250  mem: 180.3/477.6MB
Iteration 800/1250  mem: 206.7/477.6MB
Iteration 900/1250  mem: 233.1/477.6MB
Iteration 1000/1250  mem: 259.5/477.6MB
Iteration 1100/1250  mem: 162/477.6MB
Iteration 1200/1250  mem: 187.8/477.6MB
done building BART in 0.54 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 33 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 222.5/477.6MB
Iteration 200/1250  mem: 241.1/477.6MB
Iteration 300/1250  mem: 257.1/477.6MB
Iteration 400/1250  mem: 147.9/477.6MB
Iteration 500/1250  mem: 163.9/477.6MB
Iteration 600/1250  mem: 180/477.6MB
Iteration 700/1250  mem: 196/477.6MB
Iteration 800/1250  mem: 212.1/477.6MB
Iteration 900/1250  mem: 230.1/477.6MB
Iteration 1000/1250  mem: 246.2/477.6MB
Iteration 1100/1250  mem: 262.2/477.6MB
Iteration 1200/1250  mem: 278.3/477.6MB
done building BART in 0.334 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 42 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 188.2/477.6MB
Iteration 200/1250  mem: 209.8/477.6MB
Iteration 300/1250  mem: 231.3/477.6MB
Iteration 400/1250  mem: 252.9/477.6MB
Iteration 500/1250  mem: 274.4/477.6MB
Iteration 600/1250  mem: 169.3/477.6MB
Iteration 700/1250  mem: 190.3/477.6MB
Iteration 800/1250  mem: 211.2/477.6MB
Iteration 900/1250  mem: 229.5/477.6MB
Iteration 1000/1250  mem: 250.4/477.6MB
Iteration 1100/1250  mem: 274/477.6MB
Iteration 1200/1250  mem: 294.9/477.6MB
done building BART in 0.459 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 59 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 214.1/477.6MB
Iteration 200/1250  mem: 244.3/477.6MB
Iteration 300/1250  mem: 274.5/477.6MB
Iteration 400/1250  mem: 304.8/477.6MB
Iteration 500/1250  mem: 202.8/477.6MB
Iteration 600/1250  mem: 232.3/477.6MB
Iteration 700/1250  mem: 261.8/477.6MB
Iteration 800/1250  mem: 291.3/477.6MB
Iteration 900/1250  mem: 194.1/477.6MB
Iteration 1000/1250  mem: 224.4/477.6MB
Iteration 1100/1250  mem: 252/477.6MB
Iteration 1200/1250  mem: 282.3/477.6MB
done building BART in 0.578 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 53 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 328.2/477.6MB
Iteration 200/1250  mem: 224/477.6MB
Iteration 300/1250  mem: 250.1/477.6MB
Iteration 400/1250  mem: 278/477.6MB
Iteration 500/1250  mem: 304.1/477.6MB
Iteration 600/1250  mem: 331.9/477.6MB
Iteration 700/1250  mem: 226.7/477.6MB
Iteration 800/1250  mem: 252.5/477.6MB
Iteration 900/1250  mem: 278.3/477.6MB
Iteration 1000/1250  mem: 306.3/477.6MB
Iteration 1100/1250  mem: 332.1/477.6MB
Iteration 1200/1250  mem: 227.8/477.6MB
done building BART in 0.555 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 292.4/477.6MB
Iteration 200/1250  mem: 338.1/477.6MB
Iteration 300/1250  mem: 247.6/477.6MB
Iteration 400/1250  mem: 290.5/477.6MB
Iteration 500/1250  mem: 336/477.6MB
Iteration 600/1250  mem: 248.7/477.6MB
Iteration 700/1250  mem: 292.7/477.6MB
Iteration 800/1250  mem: 339.5/477.6MB
Iteration 900/1250  mem: 53.1/477.6MB
Iteration 1000/1250  mem: 97.8/477.6MB
Iteration 1100/1250  mem: 142.6/477.6MB
Iteration 1200/1250  mem: 187.3/477.6MB
done building BART in 1.057 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 78 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 123.6/477.6MB
Iteration 200/1250  mem: 164/477.6MB
Iteration 300/1250  mem: 204.3/477.6MB
Iteration 400/1250  mem: 106.3/477.6MB
Iteration 500/1250  mem: 144.1/477.6MB
Iteration 600/1250  mem: 181.9/477.6MB
Iteration 700/1250  mem: 88/477.6MB
Iteration 800/1250  mem: 126.7/477.6MB
Iteration 900/1250  mem: 165.4/477.6MB
Iteration 1000/1250  mem: 206.9/477.6MB
Iteration 1100/1250  mem: 110.5/477.6MB
Iteration 1200/1250  mem: 149.8/477.6MB
done building BART in 0.778 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 95 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 228.5/477.6MB
Iteration 200/1250  mem: 149.3/477.6MB
Iteration 300/1250  mem: 203.5/477.6MB
Iteration 400/1250  mem: 125.4/477.6MB
Iteration 500/1250  mem: 180.4/477.6MB
Iteration 600/1250  mem: 235.3/477.6MB
Iteration 700/1250  mem: 164.6/477.6MB
Iteration 800/1250  mem: 218.7/477.6MB
Iteration 900/1250  mem: 272.7/477.6MB
Iteration 1000/1250  mem: 201.6/477.6MB
Iteration 1100/1250  mem: 258.3/477.6MB
Iteration 1200/1250  mem: 211.2/477.6MB
done building BART in 1.244 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 38 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 263/477.6MB
Iteration 200/1250  mem: 284.3/477.6MB
Iteration 300/1250  mem: 197.2/477.6MB
Iteration 400/1250  mem: 215.2/477.6MB
Iteration 500/1250  mem: 234.8/477.6MB
Iteration 600/1250  mem: 254.5/477.6MB
Iteration 700/1250  mem: 274.1/477.6MB
Iteration 800/1250  mem: 293.7/477.6MB
Iteration 900/1250  mem: 203/477.6MB
Iteration 1000/1250  mem: 222/477.6MB
Iteration 1100/1250  mem: 240.9/477.6MB
Iteration 1200/1250  mem: 261.7/477.6MB
done building BART in 0.404 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 80 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 208.6/477.6MB
Iteration 200/1250  mem: 249.8/477.6MB
Iteration 300/1250  mem: 291/477.6MB
Iteration 400/1250  mem: 214.6/477.6MB
Iteration 500/1250  mem: 256/477.6MB
Iteration 600/1250  mem: 295.5/477.6MB
Iteration 700/1250  mem: 225.3/477.6MB
Iteration 800/1250  mem: 265.2/477.6MB
Iteration 900/1250  mem: 305.1/477.6MB
Iteration 1000/1250  mem: 231.3/477.6MB
Iteration 1100/1250  mem: 273.6/477.6MB
Iteration 1200/1250  mem: 313.7/477.6MB
done building BART in 0.853 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 89 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 272/477.6MB
Iteration 200/1250  mem: 317.1/477.6MB
Iteration 300/1250  mem: 237.6/477.6MB
Iteration 400/1250  mem: 280.1/477.6MB
Iteration 500/1250  mem: 324.3/477.6MB
Iteration 600/1250  mem: 249.1/477.6MB
Iteration 700/1250  mem: 292.8/477.6MB
Iteration 800/1250  mem: 336.5/477.6MB
Iteration 900/1250  mem: 258.7/477.6MB
Iteration 1000/1250  mem: 301.6/477.6MB
Iteration 1100/1250  mem: 346.7/477.6MB
Iteration 1200/1250  mem: 266/477.6MB
done building BART in 0.935 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 65 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 327.7/477.6MB
Iteration 200/1250  mem: 361.5/477.6MB
Iteration 300/1250  mem: 270.9/477.6MB
Iteration 400/1250  mem: 302.5/477.6MB
Iteration 500/1250  mem: 336.3/477.6MB
Iteration 600/1250  mem: 370/477.6MB
Iteration 700/1250  mem: 50.8/477.6MB
Iteration 800/1250  mem: 84.9/477.6MB
Iteration 900/1250  mem: 116.5/477.6MB
Iteration 1000/1250  mem: 150.5/477.6MB
Iteration 1100/1250  mem: 62.2/477.6MB
Iteration 1200/1250  mem: 94.8/477.6MB
done building BART in 0.775 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 19 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 121.9/477.6MB
Iteration 200/1250  mem: 132.5/477.6MB
Iteration 300/1250  mem: 141.9/477.6MB
Iteration 400/1250  mem: 152.5/477.6MB
Iteration 500/1250  mem: 161.8/477.6MB
Iteration 600/1250  mem: 171.2/477.6MB
Iteration 700/1250  mem: 181.8/477.6MB
Iteration 800/1250  mem: 66.7/477.6MB
Iteration 900/1250  mem: 76.7/477.6MB
Iteration 1000/1250  mem: 86.8/477.6MB
Iteration 1100/1250  mem: 95.5/477.6MB
Iteration 1200/1250  mem: 105.5/477.6MB
done building BART in 0.198 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 61 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 147.9/477.6MB
Iteration 200/1250  mem: 179.2/477.6MB
Iteration 300/1250  mem: 79.1/477.6MB
Iteration 400/1250  mem: 109.7/477.6MB
Iteration 500/1250  mem: 140.3/477.6MB
Iteration 600/1250  mem: 170.9/477.6MB
Iteration 700/1250  mem: 201.5/477.6MB
Iteration 800/1250  mem: 106.6/477.6MB
Iteration 900/1250  mem: 136.4/477.6MB
Iteration 1000/1250  mem: 166.2/477.6MB
Iteration 1100/1250  mem: 197.9/477.6MB
Iteration 1200/1250  mem: 98.3/477.6MB
done building BART in 0.645 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 100 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 176.6/477.6MB
Iteration 200/1250  mem: 99.2/477.6MB
Iteration 300/1250  mem: 152.5/477.6MB
Iteration 400/1250  mem: 205.9/477.6MB
Iteration 500/1250  mem: 132.9/477.6MB
Iteration 600/1250  mem: 185.8/477.6MB
Iteration 700/1250  mem: 241.3/477.6MB
Iteration 800/1250  mem: 170.3/477.6MB
Iteration 900/1250  mem: 225.7/477.6MB
Iteration 1000/1250  mem: 157.6/477.6MB
Iteration 1100/1250  mem: 211.2/477.6MB
Iteration 1200/1250  mem: 264.8/477.6MB
done building BART in 1.153 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 45 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 201.1/477.6MB
Iteration 200/1250  mem: 228/477.6MB
Iteration 300/1250  mem: 255/477.6MB
Iteration 400/1250  mem: 281.9/477.6MB
Iteration 500/1250  mem: 183/477.6MB
Iteration 600/1250  mem: 210.7/477.6MB
Iteration 700/1250  mem: 235.9/477.6MB
Iteration 800/1250  mem: 263.7/477.6MB
Iteration 900/1250  mem: 288.9/477.6MB
Iteration 1000/1250  mem: 199.5/477.6MB
Iteration 1100/1250  mem: 225/477.6MB
Iteration 1200/1250  mem: 252.9/477.6MB
done building BART in 0.528 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 54 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 298.7/477.6MB
Iteration 200/1250  mem: 207.5/477.6MB
Iteration 300/1250  mem: 233.1/477.6MB
Iteration 400/1250  mem: 260.5/477.6MB
Iteration 500/1250  mem: 287.9/477.6MB
Iteration 600/1250  mem: 315.3/477.6MB
Iteration 700/1250  mem: 218.5/477.6MB
Iteration 800/1250  mem: 244/477.6MB
Iteration 900/1250  mem: 271.6/477.6MB
Iteration 1000/1250  mem: 299.3/477.6MB
Iteration 1100/1250  mem: 326.9/477.6MB
Iteration 1200/1250  mem: 228.6/477.6MB
done building BART in 0.57 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 53 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 278.4/477.6MB
Iteration 200/1250  mem: 310.4/477.6MB
Iteration 300/1250  mem: 345/477.6MB
Iteration 400/1250  mem: 251.4/477.6MB
Iteration 500/1250  mem: 282.5/477.6MB
Iteration 600/1250  mem: 313.6/477.6MB
Iteration 700/1250  mem: 347.1/477.6MB
Iteration 800/1250  mem: 264.5/477.6MB
Iteration 900/1250  mem: 296.9/477.6MB
Iteration 1000/1250  mem: 329.4/477.6MB
Iteration 1100/1250  mem: 361.8/477.6MB
Iteration 1200/1250  mem: 279.1/477.6MB
done building BART in 0.673 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 53 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 330.5/477.6MB
Iteration 200/1250  mem: 358.5/477.6MB
Iteration 300/1250  mem: 386.5/477.6MB
Iteration 400/1250  mem: 81.4/477.6MB
Iteration 500/1250  mem: 110/477.6MB
Iteration 600/1250  mem: 138.5/477.6MB
Iteration 700/1250  mem: 167.1/477.6MB
Iteration 800/1250  mem: 80.7/477.6MB
Iteration 900/1250  mem: 108.8/477.6MB
Iteration 1000/1250  mem: 136.8/477.6MB
Iteration 1100/1250  mem: 164.9/477.6MB
Iteration 1200/1250  mem: 195.3/477.6MB
done building BART in 0.751 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 32 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 114.5/477.6MB
Iteration 200/1250  mem: 132.5/477.6MB
Iteration 300/1250  mem: 147.9/477.6MB
Iteration 400/1250  mem: 165.9/477.6MB
Iteration 500/1250  mem: 181.3/477.6MB
Iteration 600/1250  mem: 196.8/477.6MB
Iteration 700/1250  mem: 93.5/477.6MB
Iteration 800/1250  mem: 108.4/477.6MB
Iteration 900/1250  mem: 125.8/477.6MB
Iteration 1000/1250  mem: 140.7/477.6MB
Iteration 1100/1250  mem: 158.1/477.6MB
Iteration 1200/1250  mem: 173/477.6MB
done building BART in 0.33 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 53 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 211.8/477.6MB
Iteration 200/1250  mem: 116.4/477.6MB
Iteration 300/1250  mem: 142.9/477.6MB
Iteration 400/1250  mem: 169.4/477.6MB
Iteration 500/1250  mem: 195.9/477.6MB
Iteration 600/1250  mem: 222.4/477.6MB
Iteration 700/1250  mem: 126.1/477.6MB
Iteration 800/1250  mem: 152/477.6MB
Iteration 900/1250  mem: 179.5/477.6MB
Iteration 1000/1250  mem: 205.5/477.6MB
Iteration 1100/1250  mem: 233/477.6MB
Iteration 1200/1250  mem: 137.8/477.6MB
done building BART in 0.538 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 33 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 174.1/477.6MB
Iteration 200/1250  mem: 189.9/477.6MB
Iteration 300/1250  mem: 208.4/477.6MB
Iteration 400/1250  mem: 224.3/477.6MB
Iteration 500/1250  mem: 240.1/477.6MB
Iteration 600/1250  mem: 133/477.6MB
Iteration 700/1250  mem: 149.2/477.6MB
Iteration 800/1250  mem: 165.3/477.6MB
Iteration 900/1250  mem: 183.8/477.6MB
Iteration 1000/1250  mem: 200/477.6MB
Iteration 1100/1250  mem: 216.2/477.6MB
Iteration 1200/1250  mem: 232.3/477.6MB
done building BART in 0.342 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 42 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 143/477.6MB
Iteration 200/1250  mem: 164.5/477.6MB
Iteration 300/1250  mem: 185.9/477.6MB
Iteration 400/1250  mem: 207.3/477.6MB
Iteration 500/1250  mem: 228.7/477.6MB
Iteration 600/1250  mem: 249.3/477.6MB
Iteration 700/1250  mem: 144.4/477.6MB
Iteration 800/1250  mem: 165.4/477.6MB
Iteration 900/1250  mem: 186.3/477.6MB
Iteration 1000/1250  mem: 207.3/477.6MB
Iteration 1100/1250  mem: 229.8/477.6MB
Iteration 1200/1250  mem: 250.8/477.6MB
done building BART in 0.424 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 59 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 170.9/477.6MB
Iteration 200/1250  mem: 201.2/477.6MB
Iteration 300/1250  mem: 231.5/477.6MB
Iteration 400/1250  mem: 261.9/477.6MB
Iteration 500/1250  mem: 160.6/477.6MB
Iteration 600/1250  mem: 192/477.6MB
Iteration 700/1250  mem: 221.4/477.6MB
Iteration 800/1250  mem: 250.8/477.6MB
Iteration 900/1250  mem: 280.3/477.6MB
Iteration 1000/1250  mem: 182/477.6MB
Iteration 1100/1250  mem: 213.4/477.6MB
Iteration 1200/1250  mem: 242.5/477.6MB
done building BART in 0.607 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 53 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 289.8/477.6MB
Iteration 200/1250  mem: 188.9/477.6MB
Iteration 300/1250  mem: 215.3/477.6MB
Iteration 400/1250  mem: 243.5/477.6MB
Iteration 500/1250  mem: 269.9/477.6MB
Iteration 600/1250  mem: 298.2/477.6MB
Iteration 700/1250  mem: 193.3/477.6MB
Iteration 800/1250  mem: 219.8/477.6MB
Iteration 900/1250  mem: 248.5/477.6MB
Iteration 1000/1250  mem: 275/477.6MB
Iteration 1100/1250  mem: 301.5/477.6MB
Iteration 1200/1250  mem: 200.1/477.6MB
done building BART in 0.583 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 265.5/477.6MB
Iteration 200/1250  mem: 311.4/477.6MB
Iteration 300/1250  mem: 219.5/477.6MB
Iteration 400/1250  mem: 265.6/477.6MB
Iteration 500/1250  mem: 314.4/477.6MB
Iteration 600/1250  mem: 228.3/477.6MB
Iteration 700/1250  mem: 272.7/477.6MB
Iteration 800/1250  mem: 320/477.6MB
Iteration 900/1250  mem: 234/477.6MB
Iteration 1000/1250  mem: 279.1/477.6MB
Iteration 1100/1250  mem: 324.1/477.6MB
Iteration 1200/1250  mem: 369.2/477.6MB
done building BART in 0.993 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 78 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 304/477.6MB
Iteration 200/1250  mem: 344.4/477.6MB
Iteration 300/1250  mem: 245.8/477.6MB
Iteration 400/1250  mem: 285.5/477.6MB
Iteration 500/1250  mem: 325.1/477.6MB
Iteration 600/1250  mem: 364.8/477.6MB
Iteration 700/1250  mem: 273.5/477.6MB
Iteration 800/1250  mem: 312/477.6MB
Iteration 900/1250  mem: 350.5/477.6MB
Iteration 1000/1250  mem: 391.7/477.6MB
Iteration 1100/1250  mem: 296.8/477.6MB
Iteration 1200/1250  mem: 336/477.6MB
done building BART in 0.81 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 32 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 383.2/477.6MB
Iteration 200/1250  mem: 403.5/477.6MB
Iteration 300/1250  mem: 290.6/477.6MB
Iteration 400/1250  mem: 312.6/477.6MB
Iteration 500/1250  mem: 334.5/477.6MB
Iteration 600/1250  mem: 356.4/477.6MB
Iteration 700/1250  mem: 378.4/477.6MB
Iteration 800/1250  mem: 400.3/477.6MB
Iteration 900/1250  mem: 284.3/477.6MB
Iteration 1000/1250  mem: 305.2/477.6MB
Iteration 1100/1250  mem: 326/477.6MB
Iteration 1200/1250  mem: 349.1/477.6MB
done building BART in 0.441 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
Bayesian Additive Regression Trees 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  num_trees  k          alpha      beta        nu        RMSE        Rsquared 
   19        0.8588869  0.9776504  3.15550651  1.390544  0.04662357  0.9828572
   32        4.1206663  0.9758435  3.77532782  1.577527  0.03621735  0.9891385
   33        4.1415881  0.9946486  2.81053264  1.552505  0.04115974  0.9835516
   38        1.7745493  0.9697173  0.11151007  4.994620  0.07280357  0.9463472
   42        3.1616098  0.9373079  2.96385017  3.538999  0.06062255  0.9634125
   45        3.4800084  0.9992416  0.83675617  4.112558  0.06183405  0.9604337
   53        3.5022859  0.9487581  2.70372548  2.556118  0.04150015  0.9825824
   53        3.9580816  0.9759093  3.75855318  3.898480  0.04758444  0.9758805
   53        4.5824975  0.9684913  0.51831500  1.175012  0.06887594  0.9504671
   53        4.7184353  0.9900513  1.23896606  4.263606  0.06529588  0.9585211
   54        2.5450126  0.9078872  3.46846058  4.670264  0.05612111  0.9722472
   59        4.2485749  0.9972976  3.08043793  3.343040  0.03949427  0.9875143
   61        2.8267209  0.9775277  3.38496231  4.200447  0.06596128  0.9574168
   65        3.3487226  0.9872196  2.62918030  4.500610  0.05148040  0.9778297
   78        3.8288882  0.9738321  2.55971872  3.094269  0.04765006  0.9779068
   80        2.3105291  0.9169199  2.22114696  2.606814  0.06974067  0.9432181
   89        2.0207858  0.9710631  0.08829393  4.785482  0.05886869  0.9631173
   90        2.3598683  0.9530371  2.49125472  1.145612  0.06325779  0.9581048
   95        3.3966160  0.9540295  0.61869510  3.150043  0.07590533  0.9363693
  100        4.5894662  0.9472942  1.04424453  2.986723  0.05832518  0.9665488
  MAE         Selected
  0.03775795          
  0.02918155  *       
  0.03302853          
  0.05744718          
  0.04703609          
  0.05110364          
  0.03573676          
  0.03681652          
  0.05486493          
  0.05084907          
  0.04658493          
  0.03230728          
  0.05415787          
  0.04025256          
  0.03929122          
  0.05375194          
  0.04718788          
  0.04931955          
  0.05821507          
  0.04552525          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were num_trees = 32, k = 4.120666, alpha
 = 0.9758435, beta = 3.775328 and nu = 1.577527.
[1] "Sat Mar 10 02:32:06 2018"
Error in investigate_var_importance(object, plot = FALSE) : 
  could not find function "investigate_var_importance"
Error : package arm is required
Error : package arm is required
Error : package arm is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:32:20 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "bayesglm"                
t=100, m=2
t=200, m=2
t=300, m=1
t=400, m=2
t=500, m=2
t=600, m=1
t=700, m=1
t=800, m=1
t=900, m=1
t=100, m=1
t=200, m=1
t=300, m=1
t=400, m=1
t=500, m=1
t=600, m=1
t=700, m=1
t=800, m=1
t=900, m=1
t=100, m=1
t=200, m=1
t=300, m=2
t=400, m=1
t=500, m=1
t=600, m=2
t=700, m=1
t=800, m=1
t=900, m=1
t=100, m=1
t=200, m=1
t=300, m=1
t=400, m=1
t=500, m=1
t=600, m=1
t=700, m=1
t=800, m=1
t=900, m=1
The Bayesian lasso 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  sparsity   RMSE         Rsquared  MAE          Selected
  0.1071961  0.001892763  0.999974  0.001292028          
  0.2441936  0.001892763  0.999974  0.001292028          
  0.2578249  0.001892763  0.999974  0.001292028          
  0.3169125  0.001892763  0.999974  0.001292028          
  0.3585754  0.001892763  0.999974  0.001292028          
  0.3906604  0.001892763  0.999974  0.001292028          
  0.4758149  0.001892763  0.999974  0.001292028          
  0.4767434  0.001892763  0.999974  0.001292028          
  0.4805985  0.001892763  0.999974  0.001292028          
  0.4812563  0.001892763  0.999974  0.001292028          
  0.4872827  0.001892763  0.999974  0.001292028          
  0.5485869  0.001892763  0.999974  0.001292028          
  0.5663104  0.001892763  0.999974  0.001292028          
  0.6049106  0.001892763  0.999974  0.001292028          
  0.7542918  0.001892763  0.999974  0.001292028          
  0.7697237  0.001892763  0.999974  0.001292028          
  0.8690024  0.001892763  0.999974  0.001292028          
  0.8799880  0.001892763  0.999974  0.001292028          
  0.9344600  0.001892763  0.999974  0.001292028          
  0.9931335  0.001892763  0.999974  0.001292028  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was sparsity = 0.9931335.
[1] "Sat Mar 10 02:32:35 2018"
t=100, m=1
t=200, m=1
t=300, m=1
t=400, m=2
t=500, m=1
t=600, m=1
t=700, m=1
t=800, m=1
t=900, m=1
t=100, m=1
t=200, m=1
t=300, m=1
t=400, m=1
t=500, m=1
t=600, m=1
t=700, m=1
t=800, m=1
t=900, m=1
t=100, m=1
t=200, m=1
t=300, m=2
t=400, m=1
t=500, m=1
t=600, m=2
t=700, m=1
t=800, m=1
t=900, m=1
t=100, m=1
t=200, m=1
t=300, m=2
t=400, m=1
t=500, m=1
t=600, m=1
t=700, m=1
t=800, m=2
t=900, m=1
Bayesian Ridge Regression (Model Averaged) 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results:

  RMSE         Rsquared   MAE         
  0.001578701  0.9999739  0.0009496099

[1] "Sat Mar 10 02:32:49 2018"
t=100, m=1
t=200, m=1
t=300, m=1
t=400, m=1
t=500, m=1
t=600, m=1
t=700, m=1
t=800, m=1
t=900, m=1
t=100, m=1
t=200, m=1
t=300, m=1
t=400, m=1
t=500, m=1
t=600, m=1
t=700, m=1
t=800, m=1
t=900, m=1
t=100, m=1
t=200, m=1
t=300, m=1
t=400, m=1
t=500, m=1
t=600, m=1
t=700, m=1
t=800, m=1
t=900, m=1
t=100, m=1
t=200, m=1
t=300, m=1
t=400, m=1
t=500, m=1
t=600, m=1
t=700, m=1
t=800, m=1
t=900, m=1
Bayesian Ridge Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results:

  RMSE         Rsquared   MAE        
  0.002687836  0.9999744  0.002225875

[1] "Sat Mar 10 02:33:04 2018"
Boosted Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  nu         mstop  RMSE        Rsquared   MAE         Selected
  0.1038947   54    0.19566595  0.8296428  0.16473460          
  0.2135910  159    0.09970631  0.8966116  0.08064814          
  0.2430901  434    0.09348818  0.9043315  0.07218213          
  0.2778014  385    0.09354621  0.9042174  0.07220420          
  0.2837122  440    0.09338359  0.9045081  0.07208029          
  0.3058925  244    0.09396997  0.9034213  0.07266975          
  0.3396412  283    0.09357510  0.9041407  0.07225386          
  0.3797609  179    0.09449709  0.9023960  0.07319469          
  0.4021770  302    0.09332783  0.9046971  0.07204730          
  0.4079146  467    0.09324669  0.9048488  0.07194645          
  0.4179050  195    0.09364819  0.9039230  0.07235314          
  0.4205739  240    0.09339204  0.9045007  0.07210903          
  0.4597008  377    0.09326217  0.9048219  0.07196424          
  0.4751782  238    0.09346546  0.9043744  0.07215195          
  0.4946558  122    0.09523517  0.9011136  0.07368291          
  0.4971623  129    0.09489458  0.9016635  0.07334372          
  0.5099793  274    0.09330472  0.9047102  0.07201961          
  0.5499832  238    0.09331922  0.9046745  0.07203233          
  0.5508180  496    0.09323302  0.9048959  0.07193352  *       
  0.5662685  241    0.09331586  0.9047137  0.07201381          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 496 and nu = 0.550818.
[1] "Sat Mar 10 02:34:37 2018"
Conditional Inference Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE         Selected
  1     0.27296640  0.7570225  0.23457531          
  3     0.21936806  0.8420302  0.18344099          
  4     0.18921047  0.8724877  0.15474698          
  5     0.14574559  0.9058007  0.11666663          
  6     0.11732750  0.9295321  0.09362250          
  7     0.09572323  0.9463174  0.07767543          
  8     0.07877421  0.9588029  0.06404189          
  9     0.06765592  0.9682417  0.05578785  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 02:34:55 2018"
Error in varimp(object, ...) : could not find function "varimp"
Conditional Inference Tree 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  mincriterion  RMSE        Rsquared   MAE         Selected
  0.1071961     0.09225525  0.9241262  0.08003644          
  0.2441936     0.09225525  0.9241262  0.08003644          
  0.2578249     0.09225525  0.9241262  0.08003644          
  0.3169125     0.09225525  0.9241262  0.08003644          
  0.3585754     0.09225525  0.9241262  0.08003644          
  0.3906604     0.09225525  0.9241262  0.08003644          
  0.4758149     0.09225525  0.9241262  0.08003644          
  0.4767434     0.09225525  0.9241262  0.08003644          
  0.4805985     0.09225525  0.9241262  0.08003644          
  0.4812563     0.09225525  0.9241262  0.08003644          
  0.4872827     0.09225525  0.9241262  0.08003644          
  0.5485869     0.09225525  0.9241262  0.08003644          
  0.5663104     0.09225525  0.9241262  0.08003644          
  0.6049106     0.09225525  0.9241262  0.08003644          
  0.7542918     0.09225525  0.9241262  0.08003644          
  0.7697237     0.09225525  0.9241262  0.08003644          
  0.8690024     0.09225525  0.9241262  0.08003644          
  0.8799880     0.09225525  0.9241262  0.08003644          
  0.9344600     0.09225525  0.9241262  0.08003644          
  0.9931335     0.09225525  0.9241262  0.08003644  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mincriterion = 0.9931335.
[1] "Sat Mar 10 02:35:12 2018"
Conditional Inference Tree 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  maxdepth  mincriterion  RMSE        Rsquared   MAE         Selected
   2        0.1717774     0.09225525  0.9241262  0.08003644  *       
   4        0.8241333     0.09225525  0.9241262  0.08003644          
   4        0.8283176     0.09225525  0.9241262  0.08003644          
   5        0.3549099     0.09225525  0.9241262  0.08003644          
   6        0.6323220     0.09225525  0.9241262  0.08003644          
   6        0.6960017     0.09225525  0.9241262  0.08003644          
   8        0.5090025     0.09225525  0.9241262  0.08003644          
   8        0.7004572     0.09225525  0.9241262  0.08003644          
   8        0.7916163     0.09225525  0.9241262  0.08003644          
   8        0.9164995     0.09225525  0.9241262  0.08003644          
   8        0.9436871     0.09225525  0.9241262  0.08003644          
   9        0.5653442     0.09225525  0.9241262  0.08003644          
   9        0.8497150     0.09225525  0.9241262  0.08003644          
  10        0.6697445     0.09225525  0.9241262  0.08003644          
  12        0.4621058     0.09225525  0.9241262  0.08003644          
  12        0.7657776     0.09225525  0.9241262  0.08003644          
  14        0.4041572     0.09225525  0.9241262  0.08003644          
  14        0.4719737     0.09225525  0.9241262  0.08003644          
  15        0.6793232     0.09225525  0.9241262  0.08003644          
  15        0.9178932     0.09225525  0.9241262  0.08003644          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were maxdepth = 2 and mincriterion
 = 0.1717774.
[1] "Sat Mar 10 02:35:28 2018"
Cubist 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  committees  neighbors  RMSE         Rsquared   MAE           Selected
  18          1          0.001819069  0.9999618  0.0006808632          
  36          3          0.001466280  0.9999738  0.0006732766  *       
  41          8          0.001486550  0.9999724  0.0007016679          
  47          7          0.001478115  0.9999726  0.0006911852          
  48          8          0.001486550  0.9999724  0.0007016679          
  51          4          0.001495827  0.9999726  0.0007315202          
  57          5          0.001484714  0.9999726  0.0007068677          
  64          3          0.001466280  0.9999738  0.0006732766          
  67          6          0.001484271  0.9999728  0.0007035940          
  68          9          0.001480019  0.9999725  0.0006807692          
  70          3          0.001466280  0.9999738  0.0006732766          
  71          4          0.001495827  0.9999726  0.0007315202          
  77          7          0.001478115  0.9999726  0.0006911852          
  80          4          0.001495827  0.9999726  0.0007315202          
  83          2          0.001641008  0.9999685  0.0007282275          
  85          5          0.001484714  0.9999726  0.0007068677          
  92          4          0.001495827  0.9999726  0.0007315202          
  92          9          0.001485453  0.9999724  0.0006954147          
  95          4          0.001495827  0.9999726  0.0007315202          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were committees = 36 and neighbors = 3.
[1] "Sat Mar 10 02:35:50 2018"
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
Stacked AutoEncoder Deep Neural Network 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  layer1  layer2  layer3  hidden_dropout  visible_dropout  RMSE     
   4       5      16      0.55221364      0.1946762        0.3427230
   6      17      16      0.66068237      0.2208537        0.3581265
   6      17      19      0.49184321      0.2173507        0.4492583
   8       8      15      0.01951426      0.6992468        0.3857594
   8      14       9      0.51867378      0.4954598        0.3481293
   9      15      20      0.14643233      0.5757581        0.7828167
  11      11       3      0.60698060      0.6538369        0.3577666
  11      15      11      0.47315196      0.3578565        0.4967202
  11      17      16      0.65774681      0.5457872        0.3331899
  11      19      15      0.09070512      0.1645017        0.7247831
  11      19      19      0.21681906      0.5969048        0.6017316
  12      12      16      0.59236840      0.5880626        0.9581855
  12      18      20      0.53907664      0.4680257        0.5941029
  13      14      18      0.46010655      0.6300855        0.9946700
  16      10       5      0.38870072      0.3649540        0.3414113
  16      16      16      0.44795078      0.4331977        0.5798607
  18       9      15      0.01545144      0.6699675        0.3296433
  18      10      12      0.43596958      0.1603857        0.3226141
  19      14      12      0.10827164      0.4410060        0.8364708
  20      19      10      0.18274279      0.4181412        0.3094755
  Rsquared    MAE        Selected
  0.30111882  0.2830125          
  0.20938432  0.2959224          
  0.15734799  0.3753144          
  0.08350717  0.3254364          
  0.01988742  0.2889512          
  0.04828259  0.7300906          
  0.06808218  0.2961279          
  0.01646710  0.4330351          
  0.03049575  0.2753235          
  0.12162904  0.6768355          
  0.23212100  0.5278240          
  0.18640832  0.9264261          
  0.20842570  0.5361859          
  0.32369221  0.9421076          
  0.28760152  0.2891787          
  0.11565801  0.5095363          
  0.05631908  0.2767223          
  0.18412054  0.2703093          
  0.18525442  0.7829346          
  0.10033989  0.2575948  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were layer1 = 20, layer2 = 19, layer3 =
 10, hidden_dropout = 0.1827428 and visible_dropout = 0.4181412.
[1] "Sat Mar 10 02:36:06 2018"
Multivariate Adaptive Regression Spline 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  degree  nprune  RMSE         Rsquared  MAE           Selected
  1       2       0.105578860  0.869631  0.0844191648          
  1       4       0.001442387  0.999973  0.0005323059  *       
  2       2       0.105578860  0.869631  0.0844191648          
  2       3       0.001442387  0.999973  0.0005323059          
  2       4       0.001442387  0.999973  0.0005323059          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nprune = 4 and degree = 1.
[1] "Sat Mar 10 02:36:20 2018"
Extreme Learning Machine 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  nhid  actfun   RMSE         Rsquared   MAE          Selected
   3    sin      0.293931403  0.2332326  0.238494016          
   5    tansig   0.240814598  0.3705612  0.203562479          
   7    purelin  0.158402136  0.6985268  0.128656400          
   7    radbas   0.237072090  0.3556421  0.196464206          
   8    purelin  0.141316377  0.7932065  0.116731219          
  10    purelin  0.001687096  0.9999678  0.001003681  *       
  10    tansig   0.149018189  0.7337069  0.118601888          
  11    purelin  0.001687096  0.9999678  0.001003681          
  11    tansig   0.145313631  0.7284978  0.113412637          
  12    purelin  0.001687096  0.9999678  0.001003681          
  15    radbas   0.236273499  0.5093338  0.188006656          
  15    tansig   0.085471839  0.9000701  0.066830551          
  17    radbas   0.258298615  0.3965727  0.209242665          
  18    purelin  0.001687096  0.9999678  0.001003681          
  19    tansig   0.073189935  0.9402070  0.057952953          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nhid = 10 and actfun = purelin.
[1] "Sat Mar 10 02:36:34 2018"
Elasticnet 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  lambda        fraction   RMSE        Rsquared   MAE         Selected
  4.397204e-05  0.1717774  0.23970412  0.9999740  0.20801428          
  2.918513e-04  0.8241333  0.04956065  0.9999740  0.04301612          
  3.523297e-04  0.8283176  0.04834211  0.9999740  0.04195824          
  7.970308e-04  0.3549099  0.18632530  0.9999740  0.16169713          
  1.417269e-03  0.6323220  0.10547042  0.9999740  0.09153624          
  2.207814e-03  0.6960017  0.08689945  0.9999740  0.07541994          
  7.159615e-03  0.7916163  0.05860384  0.9999740  0.05085606          
  7.252039e-03  0.9164995  0.02214662  0.9999740  0.01922140          
  7.648764e-03  0.7004572  0.08515672  0.9999740  0.07390069          
  7.718587e-03  0.9436871  0.01415107  0.9999740  0.01229552  *       
  8.388733e-03  0.5090025  0.14101085  0.9999740  0.12237011          
  1.956688e-02  0.8497150  0.03933659  0.9999740  0.03411755          
  2.499554e-02  0.5653442  0.12244594  0.9999740  0.10625101          
  4.260531e-02  0.6697445  0.08889304  0.9999740  0.07712320          
  3.355449e-01  0.7657776  0.02320778  0.9998858  0.01996518          
  4.152813e-01  0.4621058  0.12329291  0.9999740  0.10691753          
  1.636871e+00  0.4041572  0.11331079  0.9999740  0.09822131          
  1.905145e+00  0.4719737  0.07969350  0.9999740  0.06902516          
  4.043524e+00  0.6793232  0.03891979  0.9847131  0.03068148          
  9.094966e+00  0.9178932  0.06953888  0.9463905  0.05972783          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were fraction = 0.9436871 and lambda
 = 0.007718587.
[1] "Sat Mar 10 02:36:49 2018"
Error : package evtree is required
In addition: There were 45 warnings (use warnings() to see them)
Error : package evtree is required
Error : package evtree is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:37:03 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "evtree"                  
Random Forest by Randomization 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  mtry  numRandomCuts  RMSE        Rsquared   MAE         Selected
  1      5             0.20079747  0.7548363  0.16454163          
  3      9             0.10541730  0.9272231  0.07719467          
  3     21             0.11072223  0.9039159  0.08196459          
  4     16             0.08294852  0.9489709  0.05954640          
  4     18             0.08416026  0.9463611  0.06070829          
  5     13             0.06356466  0.9699312  0.04466473          
  5     18             0.06561982  0.9666886  0.04554815          
  5     20             0.06821178  0.9624630  0.04866230          
  5     22             0.06802204  0.9635124  0.04750046          
  5     23             0.06549851  0.9660756  0.04600156          
  5     24             0.06741849  0.9625172  0.04751195          
  6     15             0.05174711  0.9787492  0.03603577          
  6     17             0.04728954  0.9828691  0.03285044          
  7     12             0.03248073  0.9918926  0.02194125          
  7     20             0.03504932  0.9896761  0.02442633          
  8     11             0.02093355  0.9965657  0.01429505          
  8     12             0.02231362  0.9960564  0.01528205          
  9     17             0.01507046  0.9980015  0.01036158  *       
  9     23             0.01663101  0.9975774  0.01176222          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 9 and numRandomCuts = 17.
[1] "Sat Mar 10 02:37:39 2018"
Ridge Regression with Variable Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  lambda        k  RMSE         Rsquared   MAE           Selected
  4.397204e-05  2  0.001444030  0.9999732  0.0007370936          
  2.918513e-04  8  0.001657691  0.9999687  0.0009850749          
  3.523297e-04  8  0.001657810  0.9999687  0.0009878670          
  7.970308e-04  4  0.001381745  0.9999807  0.0008386020  *       
  1.417269e-03  6  0.001443043  0.9999766  0.0007783298          
  2.207814e-03  7          NaN        NaN           NaN          
  7.159615e-03  8          NaN        NaN           NaN          
  7.252039e-03  9          NaN        NaN           NaN          
  7.648764e-03  7          NaN        NaN           NaN          
  7.718587e-03  9          NaN        NaN           NaN          
  8.388733e-03  5          NaN        NaN           NaN          
  1.956688e-02  8          NaN        NaN           NaN          
  2.499554e-02  6          NaN        NaN           NaN          
  4.260531e-02  7          NaN        NaN           NaN          
  3.355449e-01  7          NaN        NaN           NaN          
  4.152813e-01  5          NaN        NaN           NaN          
  1.636871e+00  4          NaN        NaN           NaN          
  1.905145e+00  5          NaN        NaN           NaN          
  4.043524e+00  7          NaN        NaN           NaN          
  9.094966e+00  9          NaN        NaN           NaN          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were k = 4 and lambda = 0.0007970308.
[1] "Sat Mar 10 02:37:54 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 13 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:38:11 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "gam"                     
Boosted Generalized Additive Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  mstop  prune  RMSE         Rsquared   MAE           Selected
  108    yes    0.001564356  0.9999702  0.0008880482  *       
  245    no     0.001743626  0.9999652  0.0011613471          
  258    no     0.001751405  0.9999649  0.0011713803          
  317    yes    0.001567139  0.9999700  0.0008936538          
  359    no     0.001801214  0.9999631  0.0012520781          
  391    no     0.001815531  0.9999625  0.0012727542          
  476    no     0.001842230  0.9999615  0.0013144841          
  477    no     0.001842747  0.9999615  0.0013150087          
  481    no     0.001845764  0.9999614  0.0013180678          
  482    no     0.001846025  0.9999614  0.0013182705          
  488    no     0.001848483  0.9999613  0.0013209864          
  549    no     0.001865143  0.9999605  0.0013444704          
  567    no     0.001869312  0.9999604  0.0013504792          
  605    no     0.001879284  0.9999600  0.0013630688          
  755    no     0.001911160  0.9999585  0.0013994620          
  770    yes    0.001567139  0.9999700  0.0008936538          
  870    yes    0.001567139  0.9999700  0.0008936538          
  880    yes    0.001567139  0.9999700  0.0008936538          
  935    no     0.001945174  0.9999571  0.0014298667          
  994    no     0.001954720  0.9999567  0.0014385130          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 108 and prune = yes.
[1] "Sat Mar 10 02:38:44 2018"
Generalized Additive Model using Splines 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  df         RMSE         Rsquared   MAE          Selected
  0.5359805  0.001687096  0.9999678  0.001003681  *       
  1.2209680  0.001692636  0.9999677  0.001016904          
  1.2891243  0.001696149  0.9999676  0.001023159          
  1.5845626  0.001720127  0.9999670  0.001065753          
  1.7928770  0.001743996  0.9999664  0.001101502          
  1.9533021  0.001764779  0.9999658  0.001133256          
  2.3790747  0.001821473  0.9999639  0.001235196          
  2.3837168  0.001822061  0.9999639  0.001236311          
  2.4029927  0.001824505  0.9999638  0.001240940          
  2.4062815  0.001824918  0.9999638  0.001241724          
  2.4364136  0.001828681  0.9999637  0.001248753          
  2.7429347  0.001863559  0.9999624  0.001311606          
  2.8315521  0.001872467  0.9999621  0.001326926          
  3.0245531  0.001890224  0.9999614  0.001356403          
  3.7714589  0.001943141  0.9999589  0.001438542          
  3.8486186  0.001947931  0.9999586  0.001444975          
  4.3450120  0.001980323  0.9999568  0.001483934          
  4.3999400  0.001984189  0.9999566  0.001487597          
  4.6723000  0.002004942  0.9999555  0.001514370          
  4.9656676  0.002030382  0.9999541  0.001546844          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was df = 0.5359805.
[1] "Sat Mar 10 02:39:04 2018"
Error in .local(object, ...) : test vector does not match model !
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:39:22 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "gaussprLinear"           
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:39:45 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "gaussprPoly"             
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:40:05 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "gaussprRadial"           
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0546             nan     0.2843    0.0323
     2        0.0378             nan     0.2843    0.0140
     3        0.0272             nan     0.2843    0.0119
     4        0.0191             nan     0.2843    0.0081
     5        0.0165             nan     0.2843    0.0027
     6        0.0134             nan     0.2843    0.0026
     7        0.0112             nan     0.2843    0.0014
     8        0.0099             nan     0.2843    0.0010
     9        0.0095             nan     0.2843   -0.0002
    10        0.0086             nan     0.2843    0.0004
    20        0.0068             nan     0.2843   -0.0007
    40        0.0049             nan     0.2843   -0.0003
    60        0.0037             nan     0.2843   -0.0003
    80        0.0029             nan     0.2843   -0.0002
   100        0.0022             nan     0.2843   -0.0002
   120        0.0018             nan     0.2843   -0.0001
   140        0.0014             nan     0.2843   -0.0000
   160        0.0012             nan     0.2843   -0.0000
   180        0.0009             nan     0.2843    0.0000
   200        0.0008             nan     0.2843   -0.0000
   220        0.0007             nan     0.2843   -0.0001
   240        0.0006             nan     0.2843   -0.0000
   260        0.0005             nan     0.2843   -0.0000
   280        0.0004             nan     0.2843   -0.0000
   300        0.0004             nan     0.2843   -0.0000
   320        0.0004             nan     0.2843   -0.0000
   340        0.0003             nan     0.2843   -0.0000
   360        0.0003             nan     0.2843   -0.0000
   380        0.0003             nan     0.2843   -0.0000
   400        0.0002             nan     0.2843   -0.0000
   420        0.0002             nan     0.2843   -0.0000
   440        0.0002             nan     0.2843   -0.0000
   460        0.0002             nan     0.2843   -0.0000
   480        0.0001             nan     0.2843   -0.0000
   500        0.0001             nan     0.2843   -0.0000
   520        0.0001             nan     0.2843   -0.0000
   540        0.0001             nan     0.2843   -0.0000
   560        0.0001             nan     0.2843   -0.0000
   580        0.0001             nan     0.2843   -0.0000
   600        0.0001             nan     0.2843   -0.0000
   620        0.0001             nan     0.2843   -0.0000
   640        0.0001             nan     0.2843   -0.0000
   660        0.0001             nan     0.2843   -0.0000
   680        0.0001             nan     0.2843   -0.0000
   700        0.0000             nan     0.2843   -0.0000
   720        0.0000             nan     0.2843   -0.0000
   740        0.0000             nan     0.2843   -0.0000
   760        0.0000             nan     0.2843   -0.0000
   780        0.0000             nan     0.2843   -0.0000
   800        0.0000             nan     0.2843   -0.0000
   820        0.0000             nan     0.2843   -0.0000
   840        0.0000             nan     0.2843   -0.0000
   860        0.0000             nan     0.2843   -0.0000
   880        0.0000             nan     0.2843    0.0000
   900        0.0000             nan     0.2843   -0.0000
   920        0.0000             nan     0.2843   -0.0000
   940        0.0000             nan     0.2843   -0.0000
   960        0.0000             nan     0.2843   -0.0000
   980        0.0000             nan     0.2843   -0.0000
  1000        0.0000             nan     0.2843   -0.0000
  1020        0.0000             nan     0.2843   -0.0000
  1040        0.0000             nan     0.2843   -0.0000
  1060        0.0000             nan     0.2843   -0.0000
  1080        0.0000             nan     0.2843   -0.0000
  1100        0.0000             nan     0.2843   -0.0000
  1120        0.0000             nan     0.2843   -0.0000
  1140        0.0000             nan     0.2843   -0.0000
  1160        0.0000             nan     0.2843   -0.0000
  1180        0.0000             nan     0.2843   -0.0000
  1200        0.0000             nan     0.2843   -0.0000
  1220        0.0000             nan     0.2843   -0.0000
  1240        0.0000             nan     0.2843   -0.0000
  1260        0.0000             nan     0.2843   -0.0000
  1280        0.0000             nan     0.2843   -0.0000
  1300        0.0000             nan     0.2843   -0.0000
  1320        0.0000             nan     0.2843   -0.0000
  1340        0.0000             nan     0.2843   -0.0000
  1360        0.0000             nan     0.2843   -0.0000
  1380        0.0000             nan     0.2843   -0.0000
  1400        0.0000             nan     0.2843   -0.0000
  1420        0.0000             nan     0.2843   -0.0000
  1440        0.0000             nan     0.2843   -0.0000
  1460        0.0000             nan     0.2843   -0.0000
  1480        0.0000             nan     0.2843   -0.0000
  1500        0.0000             nan     0.2843   -0.0000
  1520        0.0000             nan     0.2843   -0.0000
  1540        0.0000             nan     0.2843   -0.0000
  1560        0.0000             nan     0.2843   -0.0000
  1580        0.0000             nan     0.2843   -0.0000
  1600        0.0000             nan     0.2843   -0.0000
  1620        0.0000             nan     0.2843   -0.0000
  1640        0.0000             nan     0.2843   -0.0000
  1660        0.0000             nan     0.2843   -0.0000
  1680        0.0000             nan     0.2843   -0.0000
  1700        0.0000             nan     0.2843   -0.0000
  1720        0.0000             nan     0.2843   -0.0000
  1740        0.0000             nan     0.2843   -0.0000
  1760        0.0000             nan     0.2843   -0.0000
  1780        0.0000             nan     0.2843   -0.0000
  1800        0.0000             nan     0.2843   -0.0000
  1820        0.0000             nan     0.2843   -0.0000
  1840        0.0000             nan     0.2843   -0.0000
  1860        0.0000             nan     0.2843   -0.0000
  1880        0.0000             nan     0.2843   -0.0000
  1900        0.0000             nan     0.2843   -0.0000
  1920        0.0000             nan     0.2843   -0.0000
  1940        0.0000             nan     0.2843   -0.0000
  1960        0.0000             nan     0.2843   -0.0000
  1980        0.0000             nan     0.2843   -0.0000
  2000        0.0000             nan     0.2843   -0.0000
  2020        0.0000             nan     0.2843   -0.0000
  2040        0.0000             nan     0.2843   -0.0000
  2060        0.0000             nan     0.2843   -0.0000
  2080        0.0000             nan     0.2843   -0.0000
  2100        0.0000             nan     0.2843   -0.0000
  2120        0.0000             nan     0.2843   -0.0000
  2140        0.0000             nan     0.2843   -0.0000
  2160        0.0000             nan     0.2843   -0.0000
  2180        0.0000             nan     0.2843   -0.0000
  2200        0.0000             nan     0.2843   -0.0000
  2220        0.0000             nan     0.2843   -0.0000
  2240        0.0000             nan     0.2843   -0.0000
  2260        0.0000             nan     0.2843   -0.0000
  2280        0.0000             nan     0.2843   -0.0000
  2300        0.0000             nan     0.2843   -0.0000
  2320        0.0000             nan     0.2843   -0.0000
  2340        0.0000             nan     0.2843   -0.0000
  2360        0.0000             nan     0.2843   -0.0000
  2380        0.0000             nan     0.2843   -0.0000
  2400        0.0000             nan     0.2843   -0.0000
  2420        0.0000             nan     0.2843   -0.0000
  2440        0.0000             nan     0.2843   -0.0000
  2460        0.0000             nan     0.2843   -0.0000
  2480        0.0000             nan     0.2843   -0.0000
  2500        0.0000             nan     0.2843   -0.0000
  2520        0.0000             nan     0.2843   -0.0000
  2540        0.0000             nan     0.2843   -0.0000
  2560        0.0000             nan     0.2843   -0.0000
  2580        0.0000             nan     0.2843   -0.0000
  2600        0.0000             nan     0.2843   -0.0000
  2620        0.0000             nan     0.2843   -0.0000
  2640        0.0000             nan     0.2843   -0.0000
  2660        0.0000             nan     0.2843   -0.0000
  2680        0.0000             nan     0.2843   -0.0000
  2700        0.0000             nan     0.2843   -0.0000
  2720        0.0000             nan     0.2843   -0.0000
  2740        0.0000             nan     0.2843   -0.0000
  2760        0.0000             nan     0.2843   -0.0000
  2780        0.0000             nan     0.2843   -0.0000
  2800        0.0000             nan     0.2843   -0.0000
  2820        0.0000             nan     0.2843   -0.0000
  2840        0.0000             nan     0.2843   -0.0000
  2860        0.0000             nan     0.2843   -0.0000
  2880        0.0000             nan     0.2843   -0.0000
  2900        0.0000             nan     0.2843   -0.0000
  2920        0.0000             nan     0.2843   -0.0000
  2940        0.0000             nan     0.2843   -0.0000
  2960        0.0000             nan     0.2843   -0.0000
  2980        0.0000             nan     0.2843   -0.0000
  3000        0.0000             nan     0.2843   -0.0000
  3020        0.0000             nan     0.2843   -0.0000
  3040        0.0000             nan     0.2843   -0.0000
  3060        0.0000             nan     0.2843   -0.0000
  3080        0.0000             nan     0.2843   -0.0000
  3100        0.0000             nan     0.2843   -0.0000
  3120        0.0000             nan     0.2843   -0.0000
  3140        0.0000             nan     0.2843   -0.0000
  3160        0.0000             nan     0.2843   -0.0000
  3180        0.0000             nan     0.2843   -0.0000
  3200        0.0000             nan     0.2843   -0.0000
  3220        0.0000             nan     0.2843   -0.0000
  3240        0.0000             nan     0.2843   -0.0000
  3260        0.0000             nan     0.2843   -0.0000
  3280        0.0000             nan     0.2843   -0.0000
  3300        0.0000             nan     0.2843   -0.0000
  3320        0.0000             nan     0.2843   -0.0000
  3340        0.0000             nan     0.2843   -0.0000
  3360        0.0000             nan     0.2843   -0.0000
  3380        0.0000             nan     0.2843   -0.0000
  3400        0.0000             nan     0.2843   -0.0000
  3420        0.0000             nan     0.2843   -0.0000
  3440        0.0000             nan     0.2843   -0.0000
  3460        0.0000             nan     0.2843   -0.0000
  3480        0.0000             nan     0.2843   -0.0000
  3500        0.0000             nan     0.2843   -0.0000
  3520        0.0000             nan     0.2843   -0.0000
  3540        0.0000             nan     0.2843   -0.0000
  3560        0.0000             nan     0.2843   -0.0000
  3580        0.0000             nan     0.2843   -0.0000
  3600        0.0000             nan     0.2843   -0.0000
  3620        0.0000             nan     0.2843   -0.0000
  3640        0.0000             nan     0.2843   -0.0000
  3660        0.0000             nan     0.2843   -0.0000
  3680        0.0000             nan     0.2843   -0.0000
  3700        0.0000             nan     0.2843   -0.0000
  3720        0.0000             nan     0.2843   -0.0000
  3740        0.0000             nan     0.2843   -0.0000
  3760        0.0000             nan     0.2843   -0.0000
  3780        0.0000             nan     0.2843   -0.0000
  3800        0.0000             nan     0.2843   -0.0000
  3820        0.0000             nan     0.2843   -0.0000
  3840        0.0000             nan     0.2843   -0.0000
  3860        0.0000             nan     0.2843   -0.0000
  3880        0.0000             nan     0.2843   -0.0000
  3900        0.0000             nan     0.2843   -0.0000
  3920        0.0000             nan     0.2843   -0.0000
  3940        0.0000             nan     0.2843    0.0000
  3960        0.0000             nan     0.2843   -0.0000
  3980        0.0000             nan     0.2843   -0.0000
  4000        0.0000             nan     0.2843   -0.0000
  4020        0.0000             nan     0.2843   -0.0000
  4040        0.0000             nan     0.2843   -0.0000
  4060        0.0000             nan     0.2843   -0.0000
  4080        0.0000             nan     0.2843   -0.0000
  4100        0.0000             nan     0.2843   -0.0000
  4120        0.0000             nan     0.2843   -0.0000
  4140        0.0000             nan     0.2843   -0.0000
  4160        0.0000             nan     0.2843   -0.0000
  4180        0.0000             nan     0.2843   -0.0000
  4200        0.0000             nan     0.2843   -0.0000
  4220        0.0000             nan     0.2843   -0.0000
  4240        0.0000             nan     0.2843   -0.0000
  4260        0.0000             nan     0.2843   -0.0000
  4280        0.0000             nan     0.2843   -0.0000
  4300        0.0000             nan     0.2843   -0.0000
  4320        0.0000             nan     0.2843   -0.0000
  4340        0.0000             nan     0.2843   -0.0000
  4360        0.0000             nan     0.2843   -0.0000
  4380        0.0000             nan     0.2843   -0.0000
  4400        0.0000             nan     0.2843   -0.0000
  4420        0.0000             nan     0.2843   -0.0000
  4440        0.0000             nan     0.2843   -0.0000
  4460        0.0000             nan     0.2843   -0.0000
  4480        0.0000             nan     0.2843   -0.0000
  4500        0.0000             nan     0.2843   -0.0000
  4520        0.0000             nan     0.2843   -0.0000
  4540        0.0000             nan     0.2843   -0.0000
  4560        0.0000             nan     0.2843   -0.0000
  4580        0.0000             nan     0.2843   -0.0000
  4600        0.0000             nan     0.2843   -0.0000
  4620        0.0000             nan     0.2843   -0.0000
  4640        0.0000             nan     0.2843   -0.0000
  4660        0.0000             nan     0.2843   -0.0000
  4680        0.0000             nan     0.2843   -0.0000
  4700        0.0000             nan     0.2843   -0.0000
  4720        0.0000             nan     0.2843   -0.0000
  4740        0.0000             nan     0.2843   -0.0000
  4760        0.0000             nan     0.2843   -0.0000
  4780        0.0000             nan     0.2843   -0.0000
  4800        0.0000             nan     0.2843   -0.0000
  4820        0.0000             nan     0.2843    0.0000
  4840        0.0000             nan     0.2843   -0.0000
  4860        0.0000             nan     0.2843   -0.0000
  4880        0.0000             nan     0.2843   -0.0000
  4900        0.0000             nan     0.2843   -0.0000
  4920        0.0000             nan     0.2843   -0.0000
  4940        0.0000             nan     0.2843   -0.0000
  4960        0.0000             nan     0.2843   -0.0000
  4965        0.0000             nan     0.2843   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0550             nan     0.3246    0.0373
     2        0.0349             nan     0.3246    0.0199
     3        0.0193             nan     0.3246    0.0146
     4        0.0114             nan     0.3246    0.0064
     5        0.0082             nan     0.3246    0.0033
     6        0.0070             nan     0.3246    0.0011
     7        0.0057             nan     0.3246    0.0007
     8        0.0050             nan     0.3246    0.0004
     9        0.0038             nan     0.3246    0.0009
    10        0.0035             nan     0.3246    0.0000
    20        0.0020             nan     0.3246   -0.0002
    40        0.0012             nan     0.3246   -0.0000
    60        0.0008             nan     0.3246   -0.0000
    80        0.0005             nan     0.3246   -0.0000
   100        0.0003             nan     0.3246   -0.0000
   120        0.0002             nan     0.3246   -0.0001
   140        0.0001             nan     0.3246   -0.0000
   160        0.0001             nan     0.3246   -0.0000
   180        0.0001             nan     0.3246   -0.0000
   200        0.0000             nan     0.3246   -0.0000
   220        0.0000             nan     0.3246   -0.0000
   240        0.0000             nan     0.3246   -0.0000
   260        0.0000             nan     0.3246   -0.0000
   280        0.0000             nan     0.3246   -0.0000
   300        0.0000             nan     0.3246   -0.0000
   320        0.0000             nan     0.3246   -0.0000
   340        0.0000             nan     0.3246   -0.0000
   360        0.0000             nan     0.3246   -0.0000
   380        0.0000             nan     0.3246   -0.0000
   400        0.0000             nan     0.3246   -0.0000
   420        0.0000             nan     0.3246    0.0000
   440        0.0000             nan     0.3246   -0.0000
   460        0.0000             nan     0.3246   -0.0000
   480        0.0000             nan     0.3246    0.0000
   500        0.0000             nan     0.3246   -0.0000
   520        0.0000             nan     0.3246   -0.0000
   540        0.0000             nan     0.3246   -0.0000
   560        0.0000             nan     0.3246   -0.0000
   580        0.0000             nan     0.3246   -0.0000
   600        0.0000             nan     0.3246   -0.0000
   620        0.0000             nan     0.3246   -0.0000
   640        0.0000             nan     0.3246   -0.0000
   660        0.0000             nan     0.3246   -0.0000
   680        0.0000             nan     0.3246   -0.0000
   700        0.0000             nan     0.3246   -0.0000
   720        0.0000             nan     0.3246   -0.0000
   740        0.0000             nan     0.3246   -0.0000
   760        0.0000             nan     0.3246   -0.0000
   780        0.0000             nan     0.3246   -0.0000
   800        0.0000             nan     0.3246   -0.0000
   820        0.0000             nan     0.3246   -0.0000
   840        0.0000             nan     0.3246   -0.0000
   860        0.0000             nan     0.3246   -0.0000
   880        0.0000             nan     0.3246   -0.0000
   900        0.0000             nan     0.3246   -0.0000
   920        0.0000             nan     0.3246   -0.0000
   940        0.0000             nan     0.3246   -0.0000
   960        0.0000             nan     0.3246   -0.0000
   980        0.0000             nan     0.3246   -0.0000
  1000        0.0000             nan     0.3246   -0.0000
  1020        0.0000             nan     0.3246   -0.0000
  1040        0.0000             nan     0.3246   -0.0000
  1060        0.0000             nan     0.3246   -0.0000
  1080        0.0000             nan     0.3246   -0.0000
  1100        0.0000             nan     0.3246   -0.0000
  1120        0.0000             nan     0.3246   -0.0000
  1140        0.0000             nan     0.3246   -0.0000
  1160        0.0000             nan     0.3246   -0.0000
  1180        0.0000             nan     0.3246   -0.0000
  1200        0.0000             nan     0.3246   -0.0000
  1220        0.0000             nan     0.3246   -0.0000
  1240        0.0000             nan     0.3246   -0.0000
  1260        0.0000             nan     0.3246   -0.0000
  1280        0.0000             nan     0.3246   -0.0000
  1300        0.0000             nan     0.3246   -0.0000
  1320        0.0000             nan     0.3246   -0.0000
  1340        0.0000             nan     0.3246   -0.0000
  1360        0.0000             nan     0.3246   -0.0000
  1380        0.0000             nan     0.3246   -0.0000
  1400        0.0000             nan     0.3246   -0.0000
  1420        0.0000             nan     0.3246   -0.0000
  1440        0.0000             nan     0.3246   -0.0000
  1460        0.0000             nan     0.3246   -0.0000
  1480        0.0000             nan     0.3246   -0.0000
  1500        0.0000             nan     0.3246   -0.0000
  1520        0.0000             nan     0.3246   -0.0000
  1540        0.0000             nan     0.3246   -0.0000
  1560        0.0000             nan     0.3246   -0.0000
  1580        0.0000             nan     0.3246   -0.0000
  1600        0.0000             nan     0.3246   -0.0000
  1620        0.0000             nan     0.3246   -0.0000
  1640        0.0000             nan     0.3246   -0.0000
  1660        0.0000             nan     0.3246   -0.0000
  1680        0.0000             nan     0.3246   -0.0000
  1700        0.0000             nan     0.3246   -0.0000
  1720        0.0000             nan     0.3246   -0.0000
  1740        0.0000             nan     0.3246   -0.0000
  1760        0.0000             nan     0.3246   -0.0000
  1780        0.0000             nan     0.3246   -0.0000
  1800        0.0000             nan     0.3246   -0.0000
  1820        0.0000             nan     0.3246   -0.0000
  1840        0.0000             nan     0.3246   -0.0000
  1860        0.0000             nan     0.3246   -0.0000
  1880        0.0000             nan     0.3246   -0.0000
  1900        0.0000             nan     0.3246   -0.0000
  1920        0.0000             nan     0.3246   -0.0000
  1940        0.0000             nan     0.3246   -0.0000
  1960        0.0000             nan     0.3246   -0.0000
  1980        0.0000             nan     0.3246   -0.0000
  2000        0.0000             nan     0.3246   -0.0000
  2020        0.0000             nan     0.3246   -0.0000
  2040        0.0000             nan     0.3246   -0.0000
  2060        0.0000             nan     0.3246   -0.0000
  2080        0.0000             nan     0.3246   -0.0000
  2100        0.0000             nan     0.3246   -0.0000
  2120        0.0000             nan     0.3246   -0.0000
  2140        0.0000             nan     0.3246   -0.0000
  2160        0.0000             nan     0.3246   -0.0000
  2180        0.0000             nan     0.3246    0.0000
  2200        0.0000             nan     0.3246   -0.0000
  2220        0.0000             nan     0.3246   -0.0000
  2240        0.0000             nan     0.3246   -0.0000
  2260        0.0000             nan     0.3246   -0.0000
  2280        0.0000             nan     0.3246   -0.0000
  2300        0.0000             nan     0.3246   -0.0000
  2320        0.0000             nan     0.3246   -0.0000
  2340        0.0000             nan     0.3246   -0.0000
  2360        0.0000             nan     0.3246   -0.0000
  2380        0.0000             nan     0.3246   -0.0000
  2400        0.0000             nan     0.3246    0.0000
  2420        0.0000             nan     0.3246   -0.0000
  2440        0.0000             nan     0.3246   -0.0000
  2460        0.0000             nan     0.3246    0.0000
  2480        0.0000             nan     0.3246   -0.0000
  2500        0.0000             nan     0.3246   -0.0000
  2520        0.0000             nan     0.3246   -0.0000
  2540        0.0000             nan     0.3246   -0.0000
  2560        0.0000             nan     0.3246   -0.0000
  2580        0.0000             nan     0.3246   -0.0000
  2600        0.0000             nan     0.3246   -0.0000
  2620        0.0000             nan     0.3246   -0.0000
  2640        0.0000             nan     0.3246   -0.0000
  2660        0.0000             nan     0.3246   -0.0000
  2680        0.0000             nan     0.3246   -0.0000
  2700        0.0000             nan     0.3246   -0.0000
  2720        0.0000             nan     0.3246   -0.0000
  2740        0.0000             nan     0.3246   -0.0000
  2760        0.0000             nan     0.3246   -0.0000
  2780        0.0000             nan     0.3246   -0.0000
  2800        0.0000             nan     0.3246   -0.0000
  2820        0.0000             nan     0.3246   -0.0000
  2840        0.0000             nan     0.3246   -0.0000
  2860        0.0000             nan     0.3246   -0.0000
  2880        0.0000             nan     0.3246   -0.0000
  2900        0.0000             nan     0.3246   -0.0000
  2920        0.0000             nan     0.3246   -0.0000
  2940        0.0000             nan     0.3246   -0.0000
  2960        0.0000             nan     0.3246   -0.0000
  2980        0.0000             nan     0.3246   -0.0000
  3000        0.0000             nan     0.3246   -0.0000
  3020        0.0000             nan     0.3246   -0.0000
  3040        0.0000             nan     0.3246   -0.0000
  3060        0.0000             nan     0.3246   -0.0000
  3080        0.0000             nan     0.3246   -0.0000
  3100        0.0000             nan     0.3246   -0.0000
  3120        0.0000             nan     0.3246   -0.0000
  3140        0.0000             nan     0.3246   -0.0000
  3160        0.0000             nan     0.3246   -0.0000
  3180        0.0000             nan     0.3246   -0.0000
  3200        0.0000             nan     0.3246   -0.0000
  3220        0.0000             nan     0.3246   -0.0000
  3240        0.0000             nan     0.3246   -0.0000
  3260        0.0000             nan     0.3246   -0.0000
  3280        0.0000             nan     0.3246   -0.0000
  3300        0.0000             nan     0.3246   -0.0000
  3320        0.0000             nan     0.3246   -0.0000
  3340        0.0000             nan     0.3246   -0.0000
  3360        0.0000             nan     0.3246   -0.0000
  3380        0.0000             nan     0.3246   -0.0000
  3400        0.0000             nan     0.3246   -0.0000
  3420        0.0000             nan     0.3246   -0.0000
  3440        0.0000             nan     0.3246   -0.0000
  3460        0.0000             nan     0.3246   -0.0000
  3480        0.0000             nan     0.3246   -0.0000
  3500        0.0000             nan     0.3246   -0.0000
  3520        0.0000             nan     0.3246   -0.0000
  3540        0.0000             nan     0.3246   -0.0000
  3560        0.0000             nan     0.3246   -0.0000
  3580        0.0000             nan     0.3246   -0.0000
  3600        0.0000             nan     0.3246   -0.0000
  3620        0.0000             nan     0.3246   -0.0000
  3640        0.0000             nan     0.3246   -0.0000
  3660        0.0000             nan     0.3246   -0.0000
  3680        0.0000             nan     0.3246   -0.0000
  3700        0.0000             nan     0.3246   -0.0000
  3720        0.0000             nan     0.3246   -0.0000
  3740        0.0000             nan     0.3246   -0.0000
  3760        0.0000             nan     0.3246   -0.0000
  3780        0.0000             nan     0.3246   -0.0000
  3800        0.0000             nan     0.3246    0.0000
  3820        0.0000             nan     0.3246   -0.0000
  3840        0.0000             nan     0.3246   -0.0000
  3860        0.0000             nan     0.3246   -0.0000
  3880        0.0000             nan     0.3246   -0.0000
  3900        0.0000             nan     0.3246   -0.0000
  3920        0.0000             nan     0.3246   -0.0000
  3940        0.0000             nan     0.3246    0.0000
  3960        0.0000             nan     0.3246   -0.0000
  3980        0.0000             nan     0.3246   -0.0000
  4000        0.0000             nan     0.3246   -0.0000
  4020        0.0000             nan     0.3246   -0.0000
  4040        0.0000             nan     0.3246   -0.0000
  4060        0.0000             nan     0.3246   -0.0000
  4080        0.0000             nan     0.3246   -0.0000
  4100        0.0000             nan     0.3246   -0.0000
  4120        0.0000             nan     0.3246   -0.0000
  4140        0.0000             nan     0.3246   -0.0000
  4160        0.0000             nan     0.3246   -0.0000
  4180        0.0000             nan     0.3246   -0.0000
  4200        0.0000             nan     0.3246   -0.0000
  4220        0.0000             nan     0.3246   -0.0000
  4240        0.0000             nan     0.3246   -0.0000
  4260        0.0000             nan     0.3246   -0.0000
  4280        0.0000             nan     0.3246   -0.0000
  4300        0.0000             nan     0.3246   -0.0000
  4320        0.0000             nan     0.3246   -0.0000
  4340        0.0000             nan     0.3246   -0.0000
  4360        0.0000             nan     0.3246   -0.0000
  4380        0.0000             nan     0.3246   -0.0000
  4400        0.0000             nan     0.3246   -0.0000
  4420        0.0000             nan     0.3246   -0.0000
  4440        0.0000             nan     0.3246   -0.0000
  4460        0.0000             nan     0.3246   -0.0000
  4480        0.0000             nan     0.3246   -0.0000
  4500        0.0000             nan     0.3246   -0.0000
  4520        0.0000             nan     0.3246   -0.0000
  4540        0.0000             nan     0.3246    0.0000
  4560        0.0000             nan     0.3246   -0.0000
  4580        0.0000             nan     0.3246   -0.0000
  4600        0.0000             nan     0.3246   -0.0000
  4620        0.0000             nan     0.3246   -0.0000
  4640        0.0000             nan     0.3246   -0.0000
  4660        0.0000             nan     0.3246   -0.0000
  4672        0.0000             nan     0.3246   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0378             nan     0.4113    0.0487
     2        0.0198             nan     0.4113    0.0213
     3        0.0093             nan     0.4113    0.0089
     4        0.0082             nan     0.4113   -0.0003
     5        0.0055             nan     0.4113    0.0016
     6        0.0052             nan     0.4113   -0.0001
     7        0.0043             nan     0.4113   -0.0000
     8        0.0036             nan     0.4113   -0.0004
     9        0.0034             nan     0.4113   -0.0002
    10        0.0031             nan     0.4113   -0.0003
    20        0.0016             nan     0.4113    0.0000
    40        0.0006             nan     0.4113   -0.0001
    60        0.0003             nan     0.4113   -0.0001
    80        0.0002             nan     0.4113   -0.0000
   100        0.0001             nan     0.4113   -0.0000
   120        0.0001             nan     0.4113   -0.0000
   140        0.0000             nan     0.4113   -0.0000
   160        0.0000             nan     0.4113   -0.0000
   180        0.0000             nan     0.4113   -0.0000
   200        0.0000             nan     0.4113   -0.0000
   220        0.0000             nan     0.4113   -0.0000
   240        0.0000             nan     0.4113   -0.0000
   260        0.0000             nan     0.4113   -0.0000
   280        0.0000             nan     0.4113   -0.0000
   300        0.0000             nan     0.4113   -0.0000
   320        0.0000             nan     0.4113   -0.0000
   340        0.0000             nan     0.4113   -0.0000
   360        0.0000             nan     0.4113   -0.0000
   380        0.0000             nan     0.4113   -0.0000
   400        0.0000             nan     0.4113   -0.0000
   420        0.0000             nan     0.4113   -0.0000
   440        0.0000             nan     0.4113   -0.0000
   460        0.0000             nan     0.4113   -0.0000
   480        0.0000             nan     0.4113   -0.0000
   500        0.0000             nan     0.4113   -0.0000
   520        0.0000             nan     0.4113   -0.0000
   540        0.0000             nan     0.4113   -0.0000
   560        0.0000             nan     0.4113   -0.0000
   580        0.0000             nan     0.4113   -0.0000
   600        0.0000             nan     0.4113   -0.0000
   620        0.0000             nan     0.4113   -0.0000
   640        0.0000             nan     0.4113   -0.0000
   660        0.0000             nan     0.4113   -0.0000
   680        0.0000             nan     0.4113   -0.0000
   700        0.0000             nan     0.4113   -0.0000
   720        0.0000             nan     0.4113    0.0000
   740        0.0000             nan     0.4113   -0.0000
   760        0.0000             nan     0.4113   -0.0000
   780        0.0000             nan     0.4113   -0.0000
   800        0.0000             nan     0.4113   -0.0000
   820        0.0000             nan     0.4113   -0.0000
   840        0.0000             nan     0.4113   -0.0000
   860        0.0000             nan     0.4113   -0.0000
   880        0.0000             nan     0.4113   -0.0000
   900        0.0000             nan     0.4113   -0.0000
   920        0.0000             nan     0.4113   -0.0000
   940        0.0000             nan     0.4113   -0.0000
   960        0.0000             nan     0.4113   -0.0000
   980        0.0000             nan     0.4113   -0.0000
  1000        0.0000             nan     0.4113   -0.0000
  1020        0.0000             nan     0.4113   -0.0000
  1040        0.0000             nan     0.4113   -0.0000
  1060        0.0000             nan     0.4113   -0.0000
  1080        0.0000             nan     0.4113   -0.0000
  1100        0.0000             nan     0.4113   -0.0000
  1120        0.0000             nan     0.4113    0.0000
  1140        0.0000             nan     0.4113   -0.0000
  1160        0.0000             nan     0.4113   -0.0000
  1180        0.0000             nan     0.4113   -0.0000
  1200        0.0000             nan     0.4113    0.0000
  1220        0.0000             nan     0.4113   -0.0000
  1240        0.0000             nan     0.4113   -0.0000
  1260        0.0000             nan     0.4113   -0.0000
  1280        0.0000             nan     0.4113   -0.0000
  1300        0.0000             nan     0.4113   -0.0000
  1320        0.0000             nan     0.4113   -0.0000
  1340        0.0000             nan     0.4113   -0.0000
  1360        0.0000             nan     0.4113   -0.0000
  1380        0.0000             nan     0.4113   -0.0000
  1400        0.0000             nan     0.4113   -0.0000
  1420        0.0000             nan     0.4113   -0.0000
  1440        0.0000             nan     0.4113   -0.0000
  1460        0.0000             nan     0.4113   -0.0000
  1480        0.0000             nan     0.4113   -0.0000
  1500        0.0000             nan     0.4113   -0.0000
  1520        0.0000             nan     0.4113    0.0000
  1540        0.0000             nan     0.4113   -0.0000
  1560        0.0000             nan     0.4113   -0.0000
  1580        0.0000             nan     0.4113   -0.0000
  1600        0.0000             nan     0.4113   -0.0000
  1620        0.0000             nan     0.4113   -0.0000
  1640        0.0000             nan     0.4113   -0.0000
  1660        0.0000             nan     0.4113   -0.0000
  1680        0.0000             nan     0.4113    0.0000
  1700        0.0000             nan     0.4113   -0.0000
  1720        0.0000             nan     0.4113   -0.0000
  1740        0.0000             nan     0.4113   -0.0000
  1760        0.0000             nan     0.4113   -0.0000
  1780        0.0000             nan     0.4113   -0.0000
  1800        0.0000             nan     0.4113   -0.0000
  1820        0.0000             nan     0.4113   -0.0000
  1840        0.0000             nan     0.4113   -0.0000
  1860        0.0000             nan     0.4113   -0.0000
  1880        0.0000             nan     0.4113   -0.0000
  1900        0.0000             nan     0.4113   -0.0000
  1920        0.0000             nan     0.4113   -0.0000
  1940        0.0000             nan     0.4113   -0.0000
  1960        0.0000             nan     0.4113    0.0000
  1980        0.0000             nan     0.4113   -0.0000
  2000        0.0000             nan     0.4113   -0.0000
  2020        0.0000             nan     0.4113    0.0000
  2040        0.0000             nan     0.4113   -0.0000
  2060        0.0000             nan     0.4113   -0.0000
  2080        0.0000             nan     0.4113   -0.0000
  2100        0.0000             nan     0.4113   -0.0000
  2120        0.0000             nan     0.4113   -0.0000
  2140        0.0000             nan     0.4113   -0.0000
  2160        0.0000             nan     0.4113   -0.0000
  2180        0.0000             nan     0.4113   -0.0000
  2200        0.0000             nan     0.4113   -0.0000
  2220        0.0000             nan     0.4113   -0.0000
  2240        0.0000             nan     0.4113   -0.0000
  2260        0.0000             nan     0.4113   -0.0000
  2280        0.0000             nan     0.4113   -0.0000
  2300        0.0000             nan     0.4113   -0.0000
  2320        0.0000             nan     0.4113   -0.0000
  2340        0.0000             nan     0.4113   -0.0000
  2360        0.0000             nan     0.4113   -0.0000
  2380        0.0000             nan     0.4113   -0.0000
  2384        0.0000             nan     0.4113   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0371             nan     0.4186    0.0563
     2        0.0140             nan     0.4186    0.0215
     3        0.0060             nan     0.4186    0.0084
     4        0.0028             nan     0.4186    0.0025
     5        0.0017             nan     0.4186    0.0008
     6        0.0014             nan     0.4186    0.0001
     7        0.0011             nan     0.4186   -0.0000
     8        0.0011             nan     0.4186   -0.0003
     9        0.0009             nan     0.4186    0.0000
    10        0.0008             nan     0.4186    0.0000
    20        0.0002             nan     0.4186    0.0000
    40        0.0000             nan     0.4186   -0.0000
    60        0.0000             nan     0.4186   -0.0000
    80        0.0000             nan     0.4186   -0.0000
   100        0.0000             nan     0.4186   -0.0000
   120        0.0000             nan     0.4186   -0.0000
   140        0.0000             nan     0.4186   -0.0000
   160        0.0000             nan     0.4186   -0.0000
   180        0.0000             nan     0.4186   -0.0000
   200        0.0000             nan     0.4186   -0.0000
   220        0.0000             nan     0.4186   -0.0000
   240        0.0000             nan     0.4186   -0.0000
   260        0.0000             nan     0.4186    0.0000
   280        0.0000             nan     0.4186   -0.0000
   300        0.0000             nan     0.4186   -0.0000
   320        0.0000             nan     0.4186   -0.0000
   340        0.0000             nan     0.4186   -0.0000
   360        0.0000             nan     0.4186   -0.0000
   380        0.0000             nan     0.4186   -0.0000
   400        0.0000             nan     0.4186   -0.0000
   420        0.0000             nan     0.4186   -0.0000
   440        0.0000             nan     0.4186   -0.0000
   460        0.0000             nan     0.4186   -0.0000
   480        0.0000             nan     0.4186   -0.0000
   500        0.0000             nan     0.4186   -0.0000
   520        0.0000             nan     0.4186   -0.0000
   540        0.0000             nan     0.4186   -0.0000
   560        0.0000             nan     0.4186   -0.0000
   580        0.0000             nan     0.4186    0.0000
   600        0.0000             nan     0.4186   -0.0000
   620        0.0000             nan     0.4186   -0.0000
   640        0.0000             nan     0.4186   -0.0000
   660        0.0000             nan     0.4186   -0.0000
   680        0.0000             nan     0.4186   -0.0000
   700        0.0000             nan     0.4186   -0.0000
   720        0.0000             nan     0.4186   -0.0000
   740        0.0000             nan     0.4186   -0.0000
   760        0.0000             nan     0.4186   -0.0000
   780        0.0000             nan     0.4186   -0.0000
   800        0.0000             nan     0.4186   -0.0000
   820        0.0000             nan     0.4186   -0.0000
   840        0.0000             nan     0.4186   -0.0000
   860        0.0000             nan     0.4186   -0.0000
   880        0.0000             nan     0.4186   -0.0000
   900        0.0000             nan     0.4186   -0.0000
   920        0.0000             nan     0.4186   -0.0000
   940        0.0000             nan     0.4186   -0.0000
   960        0.0000             nan     0.4186    0.0000
   980        0.0000             nan     0.4186   -0.0000
  1000        0.0000             nan     0.4186   -0.0000
  1020        0.0000             nan     0.4186   -0.0000
  1040        0.0000             nan     0.4186   -0.0000
  1060        0.0000             nan     0.4186    0.0000
  1080        0.0000             nan     0.4186   -0.0000
  1100        0.0000             nan     0.4186   -0.0000
  1120        0.0000             nan     0.4186    0.0000
  1140        0.0000             nan     0.4186   -0.0000
  1160        0.0000             nan     0.4186   -0.0000
  1180        0.0000             nan     0.4186   -0.0000
  1200        0.0000             nan     0.4186   -0.0000
  1220        0.0000             nan     0.4186    0.0000
  1240        0.0000             nan     0.4186   -0.0000
  1260        0.0000             nan     0.4186   -0.0000
  1280        0.0000             nan     0.4186   -0.0000
  1300        0.0000             nan     0.4186   -0.0000
  1320        0.0000             nan     0.4186   -0.0000
  1340        0.0000             nan     0.4186   -0.0000
  1360        0.0000             nan     0.4186   -0.0000
  1380        0.0000             nan     0.4186   -0.0000
  1400        0.0000             nan     0.4186   -0.0000
  1420        0.0000             nan     0.4186   -0.0000
  1440        0.0000             nan     0.4186   -0.0000
  1460        0.0000             nan     0.4186   -0.0000
  1480        0.0000             nan     0.4186   -0.0000
  1500        0.0000             nan     0.4186   -0.0000
  1520        0.0000             nan     0.4186   -0.0000
  1540        0.0000             nan     0.4186   -0.0000
  1560        0.0000             nan     0.4186   -0.0000
  1580        0.0000             nan     0.4186   -0.0000
  1585        0.0000             nan     0.4186   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0325             nan     0.4267    0.0516
     2        0.0121             nan     0.4267    0.0147
     3        0.0056             nan     0.4267    0.0069
     4        0.0029             nan     0.4267    0.0026
     5        0.0019             nan     0.4267    0.0008
     6        0.0015             nan     0.4267    0.0000
     7        0.0013             nan     0.4267   -0.0003
     8        0.0010             nan     0.4267    0.0001
     9        0.0009             nan     0.4267   -0.0001
    10        0.0008             nan     0.4267   -0.0002
    20        0.0004             nan     0.4267   -0.0000
    40        0.0001             nan     0.4267   -0.0000
    60        0.0000             nan     0.4267    0.0000
    80        0.0000             nan     0.4267   -0.0000
   100        0.0000             nan     0.4267   -0.0000
   120        0.0000             nan     0.4267   -0.0000
   140        0.0000             nan     0.4267   -0.0000
   160        0.0000             nan     0.4267   -0.0000
   180        0.0000             nan     0.4267   -0.0000
   200        0.0000             nan     0.4267   -0.0000
   220        0.0000             nan     0.4267   -0.0000
   240        0.0000             nan     0.4267   -0.0000
   260        0.0000             nan     0.4267    0.0000
   280        0.0000             nan     0.4267   -0.0000
   300        0.0000             nan     0.4267   -0.0000
   320        0.0000             nan     0.4267   -0.0000
   340        0.0000             nan     0.4267   -0.0000
   360        0.0000             nan     0.4267   -0.0000
   380        0.0000             nan     0.4267   -0.0000
   400        0.0000             nan     0.4267   -0.0000
   420        0.0000             nan     0.4267   -0.0000
   440        0.0000             nan     0.4267    0.0000
   460        0.0000             nan     0.4267   -0.0000
   480        0.0000             nan     0.4267   -0.0000
   500        0.0000             nan     0.4267   -0.0000
   520        0.0000             nan     0.4267   -0.0000
   540        0.0000             nan     0.4267   -0.0000
   560        0.0000             nan     0.4267   -0.0000
   580        0.0000             nan     0.4267   -0.0000
   600        0.0000             nan     0.4267   -0.0000
   620        0.0000             nan     0.4267   -0.0000
   640        0.0000             nan     0.4267   -0.0000
   660        0.0000             nan     0.4267   -0.0000
   680        0.0000             nan     0.4267   -0.0000
   700        0.0000             nan     0.4267   -0.0000
   720        0.0000             nan     0.4267   -0.0000
   740        0.0000             nan     0.4267   -0.0000
   760        0.0000             nan     0.4267   -0.0000
   780        0.0000             nan     0.4267   -0.0000
   800        0.0000             nan     0.4267   -0.0000
   820        0.0000             nan     0.4267   -0.0000
   840        0.0000             nan     0.4267   -0.0000
   860        0.0000             nan     0.4267   -0.0000
   880        0.0000             nan     0.4267   -0.0000
   900        0.0000             nan     0.4267   -0.0000
   920        0.0000             nan     0.4267   -0.0000
   940        0.0000             nan     0.4267   -0.0000
   960        0.0000             nan     0.4267   -0.0000
   980        0.0000             nan     0.4267   -0.0000
  1000        0.0000             nan     0.4267   -0.0000
  1020        0.0000             nan     0.4267   -0.0000
  1040        0.0000             nan     0.4267   -0.0000
  1060        0.0000             nan     0.4267   -0.0000
  1080        0.0000             nan     0.4267   -0.0000
  1100        0.0000             nan     0.4267   -0.0000
  1120        0.0000             nan     0.4267   -0.0000
  1140        0.0000             nan     0.4267    0.0000
  1160        0.0000             nan     0.4267   -0.0000
  1180        0.0000             nan     0.4267   -0.0000
  1200        0.0000             nan     0.4267   -0.0000
  1220        0.0000             nan     0.4267   -0.0000
  1240        0.0000             nan     0.4267   -0.0000
  1260        0.0000             nan     0.4267   -0.0000
  1280        0.0000             nan     0.4267    0.0000
  1300        0.0000             nan     0.4267   -0.0000
  1320        0.0000             nan     0.4267   -0.0000
  1340        0.0000             nan     0.4267    0.0000
  1360        0.0000             nan     0.4267   -0.0000
  1380        0.0000             nan     0.4267   -0.0000
  1400        0.0000             nan     0.4267   -0.0000
  1420        0.0000             nan     0.4267   -0.0000
  1440        0.0000             nan     0.4267   -0.0000
  1460        0.0000             nan     0.4267   -0.0000
  1480        0.0000             nan     0.4267   -0.0000
  1500        0.0000             nan     0.4267   -0.0000
  1520        0.0000             nan     0.4267   -0.0000
  1540        0.0000             nan     0.4267   -0.0000
  1560        0.0000             nan     0.4267   -0.0000
  1580        0.0000             nan     0.4267   -0.0000
  1600        0.0000             nan     0.4267   -0.0000
  1620        0.0000             nan     0.4267   -0.0000
  1640        0.0000             nan     0.4267   -0.0000
  1660        0.0000             nan     0.4267    0.0000
  1680        0.0000             nan     0.4267   -0.0000
  1700        0.0000             nan     0.4267   -0.0000
  1720        0.0000             nan     0.4267   -0.0000
  1740        0.0000             nan     0.4267   -0.0000
  1760        0.0000             nan     0.4267   -0.0000
  1780        0.0000             nan     0.4267   -0.0000
  1800        0.0000             nan     0.4267   -0.0000
  1820        0.0000             nan     0.4267   -0.0000
  1840        0.0000             nan     0.4267   -0.0000
  1860        0.0000             nan     0.4267   -0.0000
  1880        0.0000             nan     0.4267   -0.0000
  1900        0.0000             nan     0.4267   -0.0000
  1920        0.0000             nan     0.4267   -0.0000
  1940        0.0000             nan     0.4267   -0.0000
  1960        0.0000             nan     0.4267   -0.0000
  1980        0.0000             nan     0.4267   -0.0000
  2000        0.0000             nan     0.4267   -0.0000
  2020        0.0000             nan     0.4267   -0.0000
  2040        0.0000             nan     0.4267   -0.0000
  2060        0.0000             nan     0.4267   -0.0000
  2080        0.0000             nan     0.4267   -0.0000
  2100        0.0000             nan     0.4267   -0.0000
  2120        0.0000             nan     0.4267   -0.0000
  2140        0.0000             nan     0.4267   -0.0000
  2160        0.0000             nan     0.4267   -0.0000
  2180        0.0000             nan     0.4267   -0.0000
  2200        0.0000             nan     0.4267   -0.0000
  2220        0.0000             nan     0.4267   -0.0000
  2240        0.0000             nan     0.4267   -0.0000
  2260        0.0000             nan     0.4267   -0.0000
  2280        0.0000             nan     0.4267   -0.0000
  2300        0.0000             nan     0.4267   -0.0000
  2320        0.0000             nan     0.4267   -0.0000
  2340        0.0000             nan     0.4267   -0.0000
  2360        0.0000             nan     0.4267   -0.0000
  2380        0.0000             nan     0.4267   -0.0000
  2400        0.0000             nan     0.4267   -0.0000
  2420        0.0000             nan     0.4267   -0.0000
  2440        0.0000             nan     0.4267   -0.0000
  2460        0.0000             nan     0.4267   -0.0000
  2480        0.0000             nan     0.4267   -0.0000
  2500        0.0000             nan     0.4267   -0.0000
  2520        0.0000             nan     0.4267   -0.0000
  2540        0.0000             nan     0.4267   -0.0000
  2560        0.0000             nan     0.4267   -0.0000
  2580        0.0000             nan     0.4267   -0.0000
  2600        0.0000             nan     0.4267   -0.0000
  2620        0.0000             nan     0.4267   -0.0000
  2640        0.0000             nan     0.4267   -0.0000
  2660        0.0000             nan     0.4267    0.0000
  2680        0.0000             nan     0.4267    0.0000
  2700        0.0000             nan     0.4267   -0.0000
  2720        0.0000             nan     0.4267   -0.0000
  2740        0.0000             nan     0.4267   -0.0000
  2760        0.0000             nan     0.4267   -0.0000
  2780        0.0000             nan     0.4267   -0.0000
  2800        0.0000             nan     0.4267   -0.0000
  2820        0.0000             nan     0.4267   -0.0000
  2840        0.0000             nan     0.4267   -0.0000
  2860        0.0000             nan     0.4267   -0.0000
  2880        0.0000             nan     0.4267   -0.0000
  2900        0.0000             nan     0.4267   -0.0000
  2920        0.0000             nan     0.4267   -0.0000
  2940        0.0000             nan     0.4267   -0.0000
  2960        0.0000             nan     0.4267   -0.0000
  2980        0.0000             nan     0.4267   -0.0000
  3000        0.0000             nan     0.4267   -0.0000
  3020        0.0000             nan     0.4267   -0.0000
  3040        0.0000             nan     0.4267   -0.0000
  3060        0.0000             nan     0.4267   -0.0000
  3080        0.0000             nan     0.4267   -0.0000
  3100        0.0000             nan     0.4267   -0.0000
  3120        0.0000             nan     0.4267   -0.0000
  3140        0.0000             nan     0.4267   -0.0000
  3160        0.0000             nan     0.4267   -0.0000
  3180        0.0000             nan     0.4267   -0.0000
  3200        0.0000             nan     0.4267   -0.0000
  3220        0.0000             nan     0.4267   -0.0000
  3240        0.0000             nan     0.4267   -0.0000
  3260        0.0000             nan     0.4267   -0.0000
  3280        0.0000             nan     0.4267   -0.0000
  3300        0.0000             nan     0.4267   -0.0000
  3320        0.0000             nan     0.4267   -0.0000
  3340        0.0000             nan     0.4267   -0.0000
  3360        0.0000             nan     0.4267   -0.0000
  3380        0.0000             nan     0.4267   -0.0000
  3400        0.0000             nan     0.4267   -0.0000
  3420        0.0000             nan     0.4267   -0.0000
  3440        0.0000             nan     0.4267   -0.0000
  3460        0.0000             nan     0.4267   -0.0000
  3480        0.0000             nan     0.4267   -0.0000
  3500        0.0000             nan     0.4267   -0.0000
  3520        0.0000             nan     0.4267    0.0000
  3540        0.0000             nan     0.4267    0.0000
  3560        0.0000             nan     0.4267   -0.0000
  3580        0.0000             nan     0.4267   -0.0000
  3600        0.0000             nan     0.4267   -0.0000
  3620        0.0000             nan     0.4267   -0.0000
  3640        0.0000             nan     0.4267   -0.0000
  3660        0.0000             nan     0.4267    0.0000
  3680        0.0000             nan     0.4267   -0.0000
  3700        0.0000             nan     0.4267   -0.0000
  3720        0.0000             nan     0.4267   -0.0000
  3740        0.0000             nan     0.4267   -0.0000
  3760        0.0000             nan     0.4267   -0.0000
  3780        0.0000             nan     0.4267   -0.0000
  3800        0.0000             nan     0.4267   -0.0000
  3820        0.0000             nan     0.4267   -0.0000
  3840        0.0000             nan     0.4267   -0.0000
  3860        0.0000             nan     0.4267    0.0000
  3880        0.0000             nan     0.4267   -0.0000
  3900        0.0000             nan     0.4267   -0.0000
  3920        0.0000             nan     0.4267   -0.0000
  3940        0.0000             nan     0.4267   -0.0000
  3960        0.0000             nan     0.4267   -0.0000
  3980        0.0000             nan     0.4267   -0.0000
  4000        0.0000             nan     0.4267   -0.0000
  4020        0.0000             nan     0.4267   -0.0000
  4040        0.0000             nan     0.4267   -0.0000
  4060        0.0000             nan     0.4267   -0.0000
  4080        0.0000             nan     0.4267    0.0000
  4100        0.0000             nan     0.4267   -0.0000
  4120        0.0000             nan     0.4267   -0.0000
  4140        0.0000             nan     0.4267   -0.0000
  4160        0.0000             nan     0.4267   -0.0000
  4180        0.0000             nan     0.4267   -0.0000
  4200        0.0000             nan     0.4267   -0.0000
  4220        0.0000             nan     0.4267   -0.0000
  4240        0.0000             nan     0.4267   -0.0000
  4260        0.0000             nan     0.4267   -0.0000
  4280        0.0000             nan     0.4267   -0.0000
  4300        0.0000             nan     0.4267    0.0000
  4320        0.0000             nan     0.4267   -0.0000
  4340        0.0000             nan     0.4267   -0.0000
  4345        0.0000             nan     0.4267   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0368             nan     0.5404    0.0500
     2        0.0191             nan     0.5404    0.0143
     3        0.0194             nan     0.5404   -0.0029
     4        0.0155             nan     0.5404    0.0036
     5        0.0156             nan     0.5404   -0.0030
     6        0.0142             nan     0.5404   -0.0003
     7        0.0132             nan     0.5404    0.0005
     8        0.0129             nan     0.5404   -0.0020
     9        0.0110             nan     0.5404    0.0020
    10        0.0112             nan     0.5404   -0.0025
    20        0.0082             nan     0.5404   -0.0021
    40        0.0057             nan     0.5404   -0.0008
    60        0.0047             nan     0.5404   -0.0006
    80        0.0032             nan     0.5404   -0.0002
   100        0.0026             nan     0.5404   -0.0001
   120        0.0023             nan     0.5404   -0.0002
   140        0.0023             nan     0.5404   -0.0005
   160        0.0016             nan     0.5404   -0.0001
   180        0.0015             nan     0.5404   -0.0002
   200        0.0013             nan     0.5404   -0.0003
   220        0.0010             nan     0.5404   -0.0001
   240        0.0009             nan     0.5404   -0.0001
   260        0.0007             nan     0.5404    0.0000
   280        0.0006             nan     0.5404    0.0001
   300        0.0006             nan     0.5404   -0.0001
   320        0.0005             nan     0.5404   -0.0001
   340        0.0004             nan     0.5404   -0.0001
   360        0.0003             nan     0.5404   -0.0001
   380        0.0003             nan     0.5404   -0.0000
   400        0.0002             nan     0.5404   -0.0000
   420        0.0002             nan     0.5404   -0.0000
   440        0.0002             nan     0.5404   -0.0000
   460        0.0002             nan     0.5404   -0.0000
   480        0.0001             nan     0.5404   -0.0000
   500        0.0001             nan     0.5404   -0.0000
   520        0.0001             nan     0.5404   -0.0000
   540        0.0001             nan     0.5404   -0.0000
   560        0.0001             nan     0.5404   -0.0000
   580        0.0001             nan     0.5404   -0.0000
   600        0.0001             nan     0.5404   -0.0000
   620        0.0001             nan     0.5404   -0.0000
   640        0.0001             nan     0.5404   -0.0000
   660        0.0000             nan     0.5404   -0.0000
   680        0.0000             nan     0.5404   -0.0000
   700        0.0000             nan     0.5404   -0.0000
   720        0.0000             nan     0.5404   -0.0000
   740        0.0000             nan     0.5404   -0.0000
   760        0.0000             nan     0.5404   -0.0000
   780        0.0000             nan     0.5404   -0.0000
   800        0.0000             nan     0.5404   -0.0000
   820        0.0000             nan     0.5404   -0.0000
   840        0.0000             nan     0.5404   -0.0000
   860        0.0000             nan     0.5404   -0.0000
   880        0.0000             nan     0.5404   -0.0000
   900        0.0000             nan     0.5404   -0.0000
   920        0.0000             nan     0.5404   -0.0000
   940        0.0000             nan     0.5404   -0.0000
   960        0.0000             nan     0.5404   -0.0000
   980        0.0000             nan     0.5404    0.0000
  1000        0.0000             nan     0.5404   -0.0000
  1020        0.0000             nan     0.5404   -0.0000
  1040        0.0000             nan     0.5404   -0.0000
  1060        0.0000             nan     0.5404   -0.0000
  1080        0.0000             nan     0.5404   -0.0000
  1100        0.0000             nan     0.5404   -0.0000
  1120        0.0000             nan     0.5404   -0.0000
  1140        0.0000             nan     0.5404   -0.0000
  1160        0.0000             nan     0.5404   -0.0000
  1180        0.0000             nan     0.5404   -0.0000
  1200        0.0000             nan     0.5404   -0.0000
  1220        0.0000             nan     0.5404   -0.0000
  1240        0.0000             nan     0.5404   -0.0000
  1260        0.0000             nan     0.5404   -0.0000
  1280        0.0000             nan     0.5404   -0.0000
  1300        0.0000             nan     0.5404    0.0000
  1320        0.0000             nan     0.5404   -0.0000
  1340        0.0000             nan     0.5404   -0.0000
  1360        0.0000             nan     0.5404   -0.0000
  1380        0.0000             nan     0.5404   -0.0000
  1400        0.0000             nan     0.5404   -0.0000
  1420        0.0000             nan     0.5404   -0.0000
  1440        0.0000             nan     0.5404    0.0000
  1460        0.0000             nan     0.5404   -0.0000
  1480        0.0000             nan     0.5404   -0.0000
  1500        0.0000             nan     0.5404   -0.0000
  1520        0.0000             nan     0.5404   -0.0000
  1540        0.0000             nan     0.5404   -0.0000
  1560        0.0000             nan     0.5404   -0.0000
  1580        0.0000             nan     0.5404   -0.0000
  1600        0.0000             nan     0.5404   -0.0000
  1620        0.0000             nan     0.5404   -0.0000
  1640        0.0000             nan     0.5404   -0.0000
  1660        0.0000             nan     0.5404   -0.0000
  1680        0.0000             nan     0.5404   -0.0000
  1700        0.0000             nan     0.5404   -0.0000
  1720        0.0000             nan     0.5404   -0.0000
  1740        0.0000             nan     0.5404   -0.0000
  1760        0.0000             nan     0.5404   -0.0000
  1780        0.0000             nan     0.5404   -0.0000
  1800        0.0000             nan     0.5404   -0.0000
  1820        0.0000             nan     0.5404   -0.0000
  1840        0.0000             nan     0.5404   -0.0000
  1860        0.0000             nan     0.5404   -0.0000
  1880        0.0000             nan     0.5404   -0.0000
  1900        0.0000             nan     0.5404   -0.0000
  1920        0.0000             nan     0.5404   -0.0000
  1940        0.0000             nan     0.5404   -0.0000
  1960        0.0000             nan     0.5404   -0.0000
  1980        0.0000             nan     0.5404   -0.0000
  2000        0.0000             nan     0.5404   -0.0000
  2020        0.0000             nan     0.5404   -0.0000
  2040        0.0000             nan     0.5404   -0.0000
  2060        0.0000             nan     0.5404   -0.0000
  2080        0.0000             nan     0.5404   -0.0000
  2100        0.0000             nan     0.5404   -0.0000
  2120        0.0000             nan     0.5404   -0.0000
  2140        0.0000             nan     0.5404   -0.0000
  2160        0.0000             nan     0.5404   -0.0000
  2180        0.0000             nan     0.5404   -0.0000
  2200        0.0000             nan     0.5404   -0.0000
  2220        0.0000             nan     0.5404   -0.0000
  2240        0.0000             nan     0.5404   -0.0000
  2260        0.0000             nan     0.5404   -0.0000
  2280        0.0000             nan     0.5404   -0.0000
  2300        0.0000             nan     0.5404   -0.0000
  2320        0.0000             nan     0.5404   -0.0000
  2340        0.0000             nan     0.5404   -0.0000
  2360        0.0000             nan     0.5404   -0.0000
  2380        0.0000             nan     0.5404   -0.0000
  2400        0.0000             nan     0.5404   -0.0000
  2406        0.0000             nan     0.5404   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0358             nan     0.5955    0.0609
     2        0.0179             nan     0.5955    0.0161
     3        0.0139             nan     0.5955    0.0011
     4        0.0093             nan     0.5955    0.0037
     5        0.0078             nan     0.5955    0.0008
     6        0.0073             nan     0.5955    0.0005
     7        0.0079             nan     0.5955   -0.0035
     8        0.0071             nan     0.5955   -0.0014
     9        0.0073             nan     0.5955   -0.0017
    10        0.0072             nan     0.5955   -0.0017
    20        0.0041             nan     0.5955   -0.0006
    40        0.0028             nan     0.5955   -0.0004
    60        0.0019             nan     0.5955   -0.0003
    80        0.0016             nan     0.5955    0.0000
   100        0.0013             nan     0.5955   -0.0001
   120        0.0010             nan     0.5955   -0.0001
   140        0.0007             nan     0.5955   -0.0003
   160        0.0006             nan     0.5955   -0.0001
   180        0.0005             nan     0.5955   -0.0000
   200        0.0004             nan     0.5955   -0.0001
   220        0.0003             nan     0.5955    0.0000
   240        0.0002             nan     0.5955   -0.0000
   260        0.0002             nan     0.5955   -0.0001
   280        0.0002             nan     0.5955   -0.0000
   300        0.0001             nan     0.5955   -0.0000
   320        0.0001             nan     0.5955   -0.0000
   340        0.0001             nan     0.5955   -0.0000
   360        0.0001             nan     0.5955   -0.0000
   380        0.0001             nan     0.5955   -0.0000
   400        0.0000             nan     0.5955   -0.0000
   420        0.0000             nan     0.5955   -0.0000
   440        0.0000             nan     0.5955    0.0000
   460        0.0000             nan     0.5955   -0.0000
   480        0.0000             nan     0.5955   -0.0000
   500        0.0000             nan     0.5955   -0.0000
   520        0.0000             nan     0.5955   -0.0000
   540        0.0000             nan     0.5955   -0.0000
   560        0.0000             nan     0.5955   -0.0000
   580        0.0000             nan     0.5955   -0.0000
   600        0.0000             nan     0.5955   -0.0000
   620        0.0000             nan     0.5955   -0.0000
   640        0.0000             nan     0.5955   -0.0000
   660        0.0000             nan     0.5955   -0.0000
   680        0.0000             nan     0.5955   -0.0000
   700        0.0000             nan     0.5955   -0.0000
   720        0.0000             nan     0.5955   -0.0000
   740        0.0000             nan     0.5955    0.0000
   760        0.0000             nan     0.5955   -0.0000
   780        0.0000             nan     0.5955   -0.0000
   800        0.0000             nan     0.5955   -0.0000
   820        0.0000             nan     0.5955    0.0000
   840        0.0000             nan     0.5955   -0.0000
   860        0.0000             nan     0.5955   -0.0000
   880        0.0000             nan     0.5955   -0.0000
   900        0.0000             nan     0.5955   -0.0000
   920        0.0000             nan     0.5955   -0.0000
   940        0.0000             nan     0.5955   -0.0000
   960        0.0000             nan     0.5955   -0.0000
   980        0.0000             nan     0.5955   -0.0000
  1000        0.0000             nan     0.5955   -0.0000
  1020        0.0000             nan     0.5955   -0.0000
  1040        0.0000             nan     0.5955   -0.0000
  1060        0.0000             nan     0.5955   -0.0000
  1080        0.0000             nan     0.5955   -0.0000
  1100        0.0000             nan     0.5955   -0.0000
  1120        0.0000             nan     0.5955   -0.0000
  1140        0.0000             nan     0.5955   -0.0000
  1160        0.0000             nan     0.5955   -0.0000
  1180        0.0000             nan     0.5955   -0.0000
  1200        0.0000             nan     0.5955   -0.0000
  1220        0.0000             nan     0.5955   -0.0000
  1240        0.0000             nan     0.5955   -0.0000
  1260        0.0000             nan     0.5955   -0.0000
  1280        0.0000             nan     0.5955   -0.0000
  1300        0.0000             nan     0.5955    0.0000
  1320        0.0000             nan     0.5955   -0.0000
  1340        0.0000             nan     0.5955    0.0000
  1360        0.0000             nan     0.5955   -0.0000
  1380        0.0000             nan     0.5955   -0.0000
  1400        0.0000             nan     0.5955   -0.0000
  1420        0.0000             nan     0.5955   -0.0000
  1440        0.0000             nan     0.5955   -0.0000
  1460        0.0000             nan     0.5955   -0.0000
  1480        0.0000             nan     0.5955   -0.0000
  1500        0.0000             nan     0.5955   -0.0000
  1520        0.0000             nan     0.5955   -0.0000
  1540        0.0000             nan     0.5955   -0.0000
  1560        0.0000             nan     0.5955   -0.0000
  1580        0.0000             nan     0.5955   -0.0000
  1600        0.0000             nan     0.5955   -0.0000
  1620        0.0000             nan     0.5955   -0.0000
  1640        0.0000             nan     0.5955   -0.0000
  1660        0.0000             nan     0.5955    0.0000
  1680        0.0000             nan     0.5955   -0.0000
  1700        0.0000             nan     0.5955   -0.0000
  1720        0.0000             nan     0.5955   -0.0000
  1740        0.0000             nan     0.5955   -0.0000
  1760        0.0000             nan     0.5955   -0.0000
  1780        0.0000             nan     0.5955   -0.0000
  1800        0.0000             nan     0.5955   -0.0000
  1820        0.0000             nan     0.5955   -0.0000
  1840        0.0000             nan     0.5955   -0.0000
  1860        0.0000             nan     0.5955   -0.0000
  1880        0.0000             nan     0.5955   -0.0000
  1900        0.0000             nan     0.5955   -0.0000
  1920        0.0000             nan     0.5955   -0.0000
  1940        0.0000             nan     0.5955   -0.0000
  1953        0.0000             nan     0.5955   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0534             nan     0.2843    0.0287
     2        0.0396             nan     0.2843    0.0134
     3        0.0294             nan     0.2843    0.0120
     4        0.0201             nan     0.2843    0.0086
     5        0.0155             nan     0.2843    0.0036
     6        0.0128             nan     0.2843    0.0030
     7        0.0106             nan     0.2843    0.0017
     8        0.0086             nan     0.2843    0.0013
     9        0.0064             nan     0.2843    0.0005
    10        0.0064             nan     0.2843   -0.0006
    20        0.0045             nan     0.2843   -0.0002
    40        0.0033             nan     0.2843   -0.0002
    60        0.0028             nan     0.2843   -0.0001
    80        0.0019             nan     0.2843   -0.0001
   100        0.0016             nan     0.2843   -0.0001
   120        0.0013             nan     0.2843   -0.0000
   140        0.0012             nan     0.2843   -0.0000
   160        0.0012             nan     0.2843   -0.0001
   180        0.0009             nan     0.2843   -0.0000
   200        0.0008             nan     0.2843   -0.0001
   220        0.0007             nan     0.2843   -0.0001
   240        0.0006             nan     0.2843   -0.0000
   260        0.0006             nan     0.2843   -0.0001
   280        0.0005             nan     0.2843   -0.0000
   300        0.0004             nan     0.2843   -0.0001
   320        0.0004             nan     0.2843   -0.0000
   340        0.0004             nan     0.2843   -0.0000
   360        0.0003             nan     0.2843   -0.0000
   380        0.0003             nan     0.2843   -0.0000
   400        0.0003             nan     0.2843   -0.0000
   420        0.0003             nan     0.2843   -0.0000
   440        0.0002             nan     0.2843   -0.0000
   460        0.0002             nan     0.2843   -0.0000
   480        0.0002             nan     0.2843   -0.0000
   500        0.0002             nan     0.2843   -0.0000
   520        0.0002             nan     0.2843   -0.0000
   540        0.0001             nan     0.2843   -0.0000
   560        0.0001             nan     0.2843   -0.0000
   580        0.0001             nan     0.2843   -0.0000
   600        0.0001             nan     0.2843   -0.0000
   620        0.0001             nan     0.2843   -0.0000
   640        0.0001             nan     0.2843   -0.0000
   660        0.0001             nan     0.2843   -0.0000
   680        0.0001             nan     0.2843   -0.0000
   700        0.0001             nan     0.2843   -0.0000
   720        0.0001             nan     0.2843   -0.0000
   740        0.0001             nan     0.2843   -0.0000
   760        0.0001             nan     0.2843   -0.0000
   780        0.0001             nan     0.2843   -0.0000
   800        0.0001             nan     0.2843   -0.0000
   820        0.0001             nan     0.2843   -0.0000
   840        0.0001             nan     0.2843   -0.0000
   860        0.0000             nan     0.2843   -0.0000
   880        0.0000             nan     0.2843   -0.0000
   900        0.0000             nan     0.2843   -0.0000
   920        0.0000             nan     0.2843   -0.0000
   940        0.0000             nan     0.2843   -0.0000
   960        0.0000             nan     0.2843   -0.0000
   980        0.0000             nan     0.2843   -0.0000
  1000        0.0000             nan     0.2843   -0.0000
  1020        0.0000             nan     0.2843   -0.0000
  1040        0.0000             nan     0.2843   -0.0000
  1060        0.0000             nan     0.2843   -0.0000
  1080        0.0000             nan     0.2843   -0.0000
  1100        0.0000             nan     0.2843   -0.0000
  1120        0.0000             nan     0.2843   -0.0000
  1140        0.0000             nan     0.2843   -0.0000
  1160        0.0000             nan     0.2843   -0.0000
  1180        0.0000             nan     0.2843   -0.0000
  1200        0.0000             nan     0.2843   -0.0000
  1220        0.0000             nan     0.2843   -0.0000
  1240        0.0000             nan     0.2843   -0.0000
  1260        0.0000             nan     0.2843   -0.0000
  1280        0.0000             nan     0.2843   -0.0000
  1300        0.0000             nan     0.2843   -0.0000
  1320        0.0000             nan     0.2843   -0.0000
  1340        0.0000             nan     0.2843   -0.0000
  1360        0.0000             nan     0.2843   -0.0000
  1380        0.0000             nan     0.2843   -0.0000
  1400        0.0000             nan     0.2843   -0.0000
  1420        0.0000             nan     0.2843   -0.0000
  1440        0.0000             nan     0.2843   -0.0000
  1460        0.0000             nan     0.2843   -0.0000
  1480        0.0000             nan     0.2843   -0.0000
  1500        0.0000             nan     0.2843   -0.0000
  1520        0.0000             nan     0.2843   -0.0000
  1540        0.0000             nan     0.2843   -0.0000
  1560        0.0000             nan     0.2843   -0.0000
  1580        0.0000             nan     0.2843   -0.0000
  1600        0.0000             nan     0.2843   -0.0000
  1620        0.0000             nan     0.2843   -0.0000
  1640        0.0000             nan     0.2843   -0.0000
  1660        0.0000             nan     0.2843   -0.0000
  1680        0.0000             nan     0.2843   -0.0000
  1700        0.0000             nan     0.2843   -0.0000
  1720        0.0000             nan     0.2843   -0.0000
  1740        0.0000             nan     0.2843   -0.0000
  1760        0.0000             nan     0.2843   -0.0000
  1780        0.0000             nan     0.2843   -0.0000
  1800        0.0000             nan     0.2843   -0.0000
  1820        0.0000             nan     0.2843   -0.0000
  1840        0.0000             nan     0.2843   -0.0000
  1860        0.0000             nan     0.2843   -0.0000
  1880        0.0000             nan     0.2843   -0.0000
  1900        0.0000             nan     0.2843   -0.0000
  1920        0.0000             nan     0.2843   -0.0000
  1940        0.0000             nan     0.2843   -0.0000
  1960        0.0000             nan     0.2843   -0.0000
  1980        0.0000             nan     0.2843   -0.0000
  2000        0.0000             nan     0.2843   -0.0000
  2020        0.0000             nan     0.2843   -0.0000
  2040        0.0000             nan     0.2843   -0.0000
  2060        0.0000             nan     0.2843   -0.0000
  2080        0.0000             nan     0.2843   -0.0000
  2100        0.0000             nan     0.2843   -0.0000
  2120        0.0000             nan     0.2843   -0.0000
  2140        0.0000             nan     0.2843   -0.0000
  2160        0.0000             nan     0.2843   -0.0000
  2180        0.0000             nan     0.2843    0.0000
  2200        0.0000             nan     0.2843   -0.0000
  2220        0.0000             nan     0.2843   -0.0000
  2240        0.0000             nan     0.2843   -0.0000
  2260        0.0000             nan     0.2843   -0.0000
  2280        0.0000             nan     0.2843   -0.0000
  2300        0.0000             nan     0.2843   -0.0000
  2320        0.0000             nan     0.2843   -0.0000
  2340        0.0000             nan     0.2843   -0.0000
  2360        0.0000             nan     0.2843   -0.0000
  2380        0.0000             nan     0.2843   -0.0000
  2400        0.0000             nan     0.2843   -0.0000
  2420        0.0000             nan     0.2843   -0.0000
  2440        0.0000             nan     0.2843   -0.0000
  2460        0.0000             nan     0.2843   -0.0000
  2480        0.0000             nan     0.2843   -0.0000
  2500        0.0000             nan     0.2843   -0.0000
  2520        0.0000             nan     0.2843   -0.0000
  2540        0.0000             nan     0.2843   -0.0000
  2560        0.0000             nan     0.2843   -0.0000
  2580        0.0000             nan     0.2843   -0.0000
  2600        0.0000             nan     0.2843   -0.0000
  2620        0.0000             nan     0.2843   -0.0000
  2640        0.0000             nan     0.2843   -0.0000
  2660        0.0000             nan     0.2843   -0.0000
  2680        0.0000             nan     0.2843   -0.0000
  2700        0.0000             nan     0.2843   -0.0000
  2720        0.0000             nan     0.2843   -0.0000
  2740        0.0000             nan     0.2843   -0.0000
  2760        0.0000             nan     0.2843   -0.0000
  2780        0.0000             nan     0.2843   -0.0000
  2800        0.0000             nan     0.2843   -0.0000
  2820        0.0000             nan     0.2843   -0.0000
  2840        0.0000             nan     0.2843   -0.0000
  2860        0.0000             nan     0.2843   -0.0000
  2880        0.0000             nan     0.2843   -0.0000
  2900        0.0000             nan     0.2843   -0.0000
  2920        0.0000             nan     0.2843   -0.0000
  2940        0.0000             nan     0.2843   -0.0000
  2960        0.0000             nan     0.2843   -0.0000
  2980        0.0000             nan     0.2843   -0.0000
  3000        0.0000             nan     0.2843   -0.0000
  3020        0.0000             nan     0.2843   -0.0000
  3040        0.0000             nan     0.2843   -0.0000
  3060        0.0000             nan     0.2843    0.0000
  3080        0.0000             nan     0.2843   -0.0000
  3100        0.0000             nan     0.2843   -0.0000
  3120        0.0000             nan     0.2843   -0.0000
  3140        0.0000             nan     0.2843   -0.0000
  3160        0.0000             nan     0.2843   -0.0000
  3180        0.0000             nan     0.2843   -0.0000
  3200        0.0000             nan     0.2843   -0.0000
  3220        0.0000             nan     0.2843   -0.0000
  3240        0.0000             nan     0.2843   -0.0000
  3260        0.0000             nan     0.2843   -0.0000
  3280        0.0000             nan     0.2843   -0.0000
  3300        0.0000             nan     0.2843   -0.0000
  3320        0.0000             nan     0.2843   -0.0000
  3340        0.0000             nan     0.2843   -0.0000
  3360        0.0000             nan     0.2843   -0.0000
  3380        0.0000             nan     0.2843   -0.0000
  3400        0.0000             nan     0.2843   -0.0000
  3420        0.0000             nan     0.2843   -0.0000
  3440        0.0000             nan     0.2843   -0.0000
  3460        0.0000             nan     0.2843   -0.0000
  3480        0.0000             nan     0.2843   -0.0000
  3500        0.0000             nan     0.2843   -0.0000
  3520        0.0000             nan     0.2843   -0.0000
  3540        0.0000             nan     0.2843   -0.0000
  3560        0.0000             nan     0.2843   -0.0000
  3580        0.0000             nan     0.2843   -0.0000
  3600        0.0000             nan     0.2843   -0.0000
  3620        0.0000             nan     0.2843   -0.0000
  3640        0.0000             nan     0.2843   -0.0000
  3660        0.0000             nan     0.2843   -0.0000
  3680        0.0000             nan     0.2843   -0.0000
  3700        0.0000             nan     0.2843   -0.0000
  3720        0.0000             nan     0.2843   -0.0000
  3740        0.0000             nan     0.2843   -0.0000
  3760        0.0000             nan     0.2843   -0.0000
  3780        0.0000             nan     0.2843   -0.0000
  3800        0.0000             nan     0.2843   -0.0000
  3820        0.0000             nan     0.2843   -0.0000
  3840        0.0000             nan     0.2843   -0.0000
  3860        0.0000             nan     0.2843   -0.0000
  3880        0.0000             nan     0.2843   -0.0000
  3900        0.0000             nan     0.2843   -0.0000
  3920        0.0000             nan     0.2843   -0.0000
  3940        0.0000             nan     0.2843   -0.0000
  3960        0.0000             nan     0.2843    0.0000
  3980        0.0000             nan     0.2843   -0.0000
  4000        0.0000             nan     0.2843   -0.0000
  4020        0.0000             nan     0.2843   -0.0000
  4040        0.0000             nan     0.2843   -0.0000
  4060        0.0000             nan     0.2843   -0.0000
  4080        0.0000             nan     0.2843   -0.0000
  4100        0.0000             nan     0.2843   -0.0000
  4120        0.0000             nan     0.2843   -0.0000
  4140        0.0000             nan     0.2843   -0.0000
  4160        0.0000             nan     0.2843   -0.0000
  4180        0.0000             nan     0.2843   -0.0000
  4200        0.0000             nan     0.2843   -0.0000
  4220        0.0000             nan     0.2843   -0.0000
  4240        0.0000             nan     0.2843   -0.0000
  4260        0.0000             nan     0.2843   -0.0000
  4280        0.0000             nan     0.2843   -0.0000
  4300        0.0000             nan     0.2843   -0.0000
  4320        0.0000             nan     0.2843   -0.0000
  4340        0.0000             nan     0.2843   -0.0000
  4360        0.0000             nan     0.2843   -0.0000
  4380        0.0000             nan     0.2843   -0.0000
  4400        0.0000             nan     0.2843   -0.0000
  4420        0.0000             nan     0.2843   -0.0000
  4440        0.0000             nan     0.2843   -0.0000
  4460        0.0000             nan     0.2843   -0.0000
  4480        0.0000             nan     0.2843   -0.0000
  4500        0.0000             nan     0.2843   -0.0000
  4520        0.0000             nan     0.2843   -0.0000
  4540        0.0000             nan     0.2843   -0.0000
  4560        0.0000             nan     0.2843   -0.0000
  4580        0.0000             nan     0.2843   -0.0000
  4600        0.0000             nan     0.2843   -0.0000
  4620        0.0000             nan     0.2843   -0.0000
  4640        0.0000             nan     0.2843   -0.0000
  4660        0.0000             nan     0.2843   -0.0000
  4680        0.0000             nan     0.2843   -0.0000
  4700        0.0000             nan     0.2843   -0.0000
  4720        0.0000             nan     0.2843   -0.0000
  4740        0.0000             nan     0.2843   -0.0000
  4760        0.0000             nan     0.2843   -0.0000
  4780        0.0000             nan     0.2843   -0.0000
  4800        0.0000             nan     0.2843   -0.0000
  4820        0.0000             nan     0.2843   -0.0000
  4840        0.0000             nan     0.2843   -0.0000
  4860        0.0000             nan     0.2843   -0.0000
  4880        0.0000             nan     0.2843   -0.0000
  4900        0.0000             nan     0.2843   -0.0000
  4920        0.0000             nan     0.2843   -0.0000
  4940        0.0000             nan     0.2843   -0.0000
  4960        0.0000             nan     0.2843   -0.0000
  4965        0.0000             nan     0.2843   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0541             nan     0.3246    0.0349
     2        0.0303             nan     0.3246    0.0260
     3        0.0156             nan     0.3246    0.0103
     4        0.0114             nan     0.3246    0.0026
     5        0.0070             nan     0.3246    0.0028
     6        0.0066             nan     0.3246    0.0003
     7        0.0054             nan     0.3246    0.0011
     8        0.0041             nan     0.3246    0.0011
     9        0.0039             nan     0.3246   -0.0002
    10        0.0032             nan     0.3246    0.0001
    20        0.0018             nan     0.3246    0.0000
    40        0.0012             nan     0.3246   -0.0000
    60        0.0007             nan     0.3246   -0.0001
    80        0.0006             nan     0.3246   -0.0000
   100        0.0004             nan     0.3246   -0.0001
   120        0.0003             nan     0.3246    0.0000
   140        0.0002             nan     0.3246   -0.0000
   160        0.0002             nan     0.3246   -0.0000
   180        0.0001             nan     0.3246   -0.0000
   200        0.0001             nan     0.3246   -0.0000
   220        0.0001             nan     0.3246   -0.0000
   240        0.0000             nan     0.3246   -0.0000
   260        0.0000             nan     0.3246   -0.0000
   280        0.0000             nan     0.3246   -0.0000
   300        0.0000             nan     0.3246   -0.0000
   320        0.0000             nan     0.3246   -0.0000
   340        0.0000             nan     0.3246   -0.0000
   360        0.0000             nan     0.3246   -0.0000
   380        0.0000             nan     0.3246   -0.0000
   400        0.0000             nan     0.3246   -0.0000
   420        0.0000             nan     0.3246   -0.0000
   440        0.0000             nan     0.3246   -0.0000
   460        0.0000             nan     0.3246   -0.0000
   480        0.0000             nan     0.3246   -0.0000
   500        0.0000             nan     0.3246   -0.0000
   520        0.0000             nan     0.3246   -0.0000
   540        0.0000             nan     0.3246   -0.0000
   560        0.0000             nan     0.3246   -0.0000
   580        0.0000             nan     0.3246   -0.0000
   600        0.0000             nan     0.3246    0.0000
   620        0.0000             nan     0.3246   -0.0000
   640        0.0000             nan     0.3246   -0.0000
   660        0.0000             nan     0.3246    0.0000
   680        0.0000             nan     0.3246   -0.0000
   700        0.0000             nan     0.3246   -0.0000
   720        0.0000             nan     0.3246   -0.0000
   740        0.0000             nan     0.3246   -0.0000
   760        0.0000             nan     0.3246   -0.0000
   780        0.0000             nan     0.3246   -0.0000
   800        0.0000             nan     0.3246   -0.0000
   820        0.0000             nan     0.3246   -0.0000
   840        0.0000             nan     0.3246   -0.0000
   860        0.0000             nan     0.3246   -0.0000
   880        0.0000             nan     0.3246   -0.0000
   900        0.0000             nan     0.3246   -0.0000
   920        0.0000             nan     0.3246   -0.0000
   940        0.0000             nan     0.3246   -0.0000
   960        0.0000             nan     0.3246   -0.0000
   980        0.0000             nan     0.3246   -0.0000
  1000        0.0000             nan     0.3246   -0.0000
  1020        0.0000             nan     0.3246   -0.0000
  1040        0.0000             nan     0.3246   -0.0000
  1060        0.0000             nan     0.3246   -0.0000
  1080        0.0000             nan     0.3246   -0.0000
  1100        0.0000             nan     0.3246   -0.0000
  1120        0.0000             nan     0.3246   -0.0000
  1140        0.0000             nan     0.3246   -0.0000
  1160        0.0000             nan     0.3246   -0.0000
  1180        0.0000             nan     0.3246   -0.0000
  1200        0.0000             nan     0.3246   -0.0000
  1220        0.0000             nan     0.3246   -0.0000
  1240        0.0000             nan     0.3246   -0.0000
  1260        0.0000             nan     0.3246   -0.0000
  1280        0.0000             nan     0.3246   -0.0000
  1300        0.0000             nan     0.3246   -0.0000
  1320        0.0000             nan     0.3246   -0.0000
  1340        0.0000             nan     0.3246   -0.0000
  1360        0.0000             nan     0.3246   -0.0000
  1380        0.0000             nan     0.3246   -0.0000
  1400        0.0000             nan     0.3246   -0.0000
  1420        0.0000             nan     0.3246   -0.0000
  1440        0.0000             nan     0.3246   -0.0000
  1460        0.0000             nan     0.3246   -0.0000
  1480        0.0000             nan     0.3246   -0.0000
  1500        0.0000             nan     0.3246   -0.0000
  1520        0.0000             nan     0.3246   -0.0000
  1540        0.0000             nan     0.3246   -0.0000
  1560        0.0000             nan     0.3246   -0.0000
  1580        0.0000             nan     0.3246   -0.0000
  1600        0.0000             nan     0.3246   -0.0000
  1620        0.0000             nan     0.3246   -0.0000
  1640        0.0000             nan     0.3246   -0.0000
  1660        0.0000             nan     0.3246   -0.0000
  1680        0.0000             nan     0.3246   -0.0000
  1700        0.0000             nan     0.3246   -0.0000
  1720        0.0000             nan     0.3246   -0.0000
  1740        0.0000             nan     0.3246   -0.0000
  1760        0.0000             nan     0.3246   -0.0000
  1780        0.0000             nan     0.3246   -0.0000
  1800        0.0000             nan     0.3246   -0.0000
  1820        0.0000             nan     0.3246   -0.0000
  1840        0.0000             nan     0.3246   -0.0000
  1860        0.0000             nan     0.3246   -0.0000
  1880        0.0000             nan     0.3246   -0.0000
  1900        0.0000             nan     0.3246   -0.0000
  1920        0.0000             nan     0.3246   -0.0000
  1940        0.0000             nan     0.3246   -0.0000
  1960        0.0000             nan     0.3246   -0.0000
  1980        0.0000             nan     0.3246   -0.0000
  2000        0.0000             nan     0.3246   -0.0000
  2020        0.0000             nan     0.3246   -0.0000
  2040        0.0000             nan     0.3246   -0.0000
  2060        0.0000             nan     0.3246   -0.0000
  2080        0.0000             nan     0.3246   -0.0000
  2100        0.0000             nan     0.3246   -0.0000
  2120        0.0000             nan     0.3246   -0.0000
  2140        0.0000             nan     0.3246    0.0000
  2160        0.0000             nan     0.3246   -0.0000
  2180        0.0000             nan     0.3246    0.0000
  2200        0.0000             nan     0.3246   -0.0000
  2220        0.0000             nan     0.3246   -0.0000
  2240        0.0000             nan     0.3246   -0.0000
  2260        0.0000             nan     0.3246   -0.0000
  2280        0.0000             nan     0.3246   -0.0000
  2300        0.0000             nan     0.3246   -0.0000
  2320        0.0000             nan     0.3246   -0.0000
  2340        0.0000             nan     0.3246   -0.0000
  2360        0.0000             nan     0.3246   -0.0000
  2380        0.0000             nan     0.3246   -0.0000
  2400        0.0000             nan     0.3246   -0.0000
  2420        0.0000             nan     0.3246   -0.0000
  2440        0.0000             nan     0.3246   -0.0000
  2460        0.0000             nan     0.3246   -0.0000
  2480        0.0000             nan     0.3246   -0.0000
  2500        0.0000             nan     0.3246   -0.0000
  2520        0.0000             nan     0.3246    0.0000
  2540        0.0000             nan     0.3246   -0.0000
  2560        0.0000             nan     0.3246   -0.0000
  2580        0.0000             nan     0.3246   -0.0000
  2600        0.0000             nan     0.3246   -0.0000
  2620        0.0000             nan     0.3246   -0.0000
  2640        0.0000             nan     0.3246   -0.0000
  2660        0.0000             nan     0.3246   -0.0000
  2680        0.0000             nan     0.3246   -0.0000
  2700        0.0000             nan     0.3246   -0.0000
  2720        0.0000             nan     0.3246   -0.0000
  2740        0.0000             nan     0.3246   -0.0000
  2760        0.0000             nan     0.3246   -0.0000
  2780        0.0000             nan     0.3246   -0.0000
  2800        0.0000             nan     0.3246   -0.0000
  2820        0.0000             nan     0.3246   -0.0000
  2840        0.0000             nan     0.3246   -0.0000
  2860        0.0000             nan     0.3246   -0.0000
  2880        0.0000             nan     0.3246   -0.0000
  2900        0.0000             nan     0.3246   -0.0000
  2920        0.0000             nan     0.3246   -0.0000
  2940        0.0000             nan     0.3246   -0.0000
  2960        0.0000             nan     0.3246   -0.0000
  2980        0.0000             nan     0.3246   -0.0000
  3000        0.0000             nan     0.3246   -0.0000
  3020        0.0000             nan     0.3246    0.0000
  3040        0.0000             nan     0.3246    0.0000
  3060        0.0000             nan     0.3246   -0.0000
  3080        0.0000             nan     0.3246   -0.0000
  3100        0.0000             nan     0.3246    0.0000
  3120        0.0000             nan     0.3246   -0.0000
  3140        0.0000             nan     0.3246   -0.0000
  3160        0.0000             nan     0.3246   -0.0000
  3180        0.0000             nan     0.3246   -0.0000
  3200        0.0000             nan     0.3246   -0.0000
  3220        0.0000             nan     0.3246   -0.0000
  3240        0.0000             nan     0.3246   -0.0000
  3260        0.0000             nan     0.3246   -0.0000
  3280        0.0000             nan     0.3246   -0.0000
  3300        0.0000             nan     0.3246   -0.0000
  3320        0.0000             nan     0.3246   -0.0000
  3340        0.0000             nan     0.3246   -0.0000
  3360        0.0000             nan     0.3246   -0.0000
  3380        0.0000             nan     0.3246   -0.0000
  3400        0.0000             nan     0.3246   -0.0000
  3420        0.0000             nan     0.3246   -0.0000
  3440        0.0000             nan     0.3246   -0.0000
  3460        0.0000             nan     0.3246   -0.0000
  3480        0.0000             nan     0.3246   -0.0000
  3500        0.0000             nan     0.3246   -0.0000
  3520        0.0000             nan     0.3246   -0.0000
  3540        0.0000             nan     0.3246   -0.0000
  3560        0.0000             nan     0.3246   -0.0000
  3580        0.0000             nan     0.3246   -0.0000
  3600        0.0000             nan     0.3246   -0.0000
  3620        0.0000             nan     0.3246   -0.0000
  3640        0.0000             nan     0.3246   -0.0000
  3660        0.0000             nan     0.3246   -0.0000
  3680        0.0000             nan     0.3246   -0.0000
  3700        0.0000             nan     0.3246   -0.0000
  3720        0.0000             nan     0.3246   -0.0000
  3740        0.0000             nan     0.3246   -0.0000
  3760        0.0000             nan     0.3246    0.0000
  3780        0.0000             nan     0.3246   -0.0000
  3800        0.0000             nan     0.3246   -0.0000
  3820        0.0000             nan     0.3246   -0.0000
  3840        0.0000             nan     0.3246   -0.0000
  3860        0.0000             nan     0.3246   -0.0000
  3880        0.0000             nan     0.3246   -0.0000
  3900        0.0000             nan     0.3246   -0.0000
  3920        0.0000             nan     0.3246   -0.0000
  3940        0.0000             nan     0.3246   -0.0000
  3960        0.0000             nan     0.3246   -0.0000
  3980        0.0000             nan     0.3246   -0.0000
  4000        0.0000             nan     0.3246   -0.0000
  4020        0.0000             nan     0.3246   -0.0000
  4040        0.0000             nan     0.3246   -0.0000
  4060        0.0000             nan     0.3246   -0.0000
  4080        0.0000             nan     0.3246   -0.0000
  4100        0.0000             nan     0.3246   -0.0000
  4120        0.0000             nan     0.3246   -0.0000
  4140        0.0000             nan     0.3246   -0.0000
  4160        0.0000             nan     0.3246   -0.0000
  4180        0.0000             nan     0.3246   -0.0000
  4200        0.0000             nan     0.3246   -0.0000
  4220        0.0000             nan     0.3246   -0.0000
  4240        0.0000             nan     0.3246   -0.0000
  4260        0.0000             nan     0.3246   -0.0000
  4280        0.0000             nan     0.3246   -0.0000
  4300        0.0000             nan     0.3246   -0.0000
  4320        0.0000             nan     0.3246   -0.0000
  4340        0.0000             nan     0.3246   -0.0000
  4360        0.0000             nan     0.3246   -0.0000
  4380        0.0000             nan     0.3246   -0.0000
  4400        0.0000             nan     0.3246   -0.0000
  4420        0.0000             nan     0.3246   -0.0000
  4440        0.0000             nan     0.3246   -0.0000
  4460        0.0000             nan     0.3246   -0.0000
  4480        0.0000             nan     0.3246   -0.0000
  4500        0.0000             nan     0.3246   -0.0000
  4520        0.0000             nan     0.3246   -0.0000
  4540        0.0000             nan     0.3246   -0.0000
  4560        0.0000             nan     0.3246   -0.0000
  4580        0.0000             nan     0.3246   -0.0000
  4600        0.0000             nan     0.3246   -0.0000
  4620        0.0000             nan     0.3246   -0.0000
  4640        0.0000             nan     0.3246   -0.0000
  4660        0.0000             nan     0.3246   -0.0000
  4672        0.0000             nan     0.3246   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0385             nan     0.4113    0.0483
     2        0.0173             nan     0.4113    0.0254
     3        0.0088             nan     0.4113    0.0082
     4        0.0065             nan     0.4113    0.0024
     5        0.0046             nan     0.4113    0.0017
     6        0.0034             nan     0.4113    0.0003
     7        0.0032             nan     0.4113   -0.0004
     8        0.0027             nan     0.4113    0.0001
     9        0.0026             nan     0.4113   -0.0005
    10        0.0023             nan     0.4113   -0.0002
    20        0.0014             nan     0.4113   -0.0002
    40        0.0006             nan     0.4113   -0.0001
    60        0.0003             nan     0.4113   -0.0000
    80        0.0001             nan     0.4113   -0.0000
   100        0.0001             nan     0.4113   -0.0000
   120        0.0000             nan     0.4113   -0.0000
   140        0.0000             nan     0.4113   -0.0000
   160        0.0000             nan     0.4113   -0.0000
   180        0.0000             nan     0.4113   -0.0000
   200        0.0000             nan     0.4113   -0.0000
   220        0.0000             nan     0.4113   -0.0000
   240        0.0000             nan     0.4113   -0.0000
   260        0.0000             nan     0.4113   -0.0000
   280        0.0000             nan     0.4113   -0.0000
   300        0.0000             nan     0.4113   -0.0000
   320        0.0000             nan     0.4113   -0.0000
   340        0.0000             nan     0.4113   -0.0000
   360        0.0000             nan     0.4113   -0.0000
   380        0.0000             nan     0.4113   -0.0000
   400        0.0000             nan     0.4113   -0.0000
   420        0.0000             nan     0.4113   -0.0000
   440        0.0000             nan     0.4113   -0.0000
   460        0.0000             nan     0.4113   -0.0000
   480        0.0000             nan     0.4113   -0.0000
   500        0.0000             nan     0.4113   -0.0000
   520        0.0000             nan     0.4113   -0.0000
   540        0.0000             nan     0.4113   -0.0000
   560        0.0000             nan     0.4113   -0.0000
   580        0.0000             nan     0.4113   -0.0000
   600        0.0000             nan     0.4113   -0.0000
   620        0.0000             nan     0.4113    0.0000
   640        0.0000             nan     0.4113   -0.0000
   660        0.0000             nan     0.4113   -0.0000
   680        0.0000             nan     0.4113   -0.0000
   700        0.0000             nan     0.4113   -0.0000
   720        0.0000             nan     0.4113   -0.0000
   740        0.0000             nan     0.4113   -0.0000
   760        0.0000             nan     0.4113   -0.0000
   780        0.0000             nan     0.4113   -0.0000
   800        0.0000             nan     0.4113   -0.0000
   820        0.0000             nan     0.4113   -0.0000
   840        0.0000             nan     0.4113   -0.0000
   860        0.0000             nan     0.4113   -0.0000
   880        0.0000             nan     0.4113   -0.0000
   900        0.0000             nan     0.4113   -0.0000
   920        0.0000             nan     0.4113   -0.0000
   940        0.0000             nan     0.4113   -0.0000
   960        0.0000             nan     0.4113    0.0000
   980        0.0000             nan     0.4113   -0.0000
  1000        0.0000             nan     0.4113   -0.0000
  1020        0.0000             nan     0.4113   -0.0000
  1040        0.0000             nan     0.4113   -0.0000
  1060        0.0000             nan     0.4113   -0.0000
  1080        0.0000             nan     0.4113   -0.0000
  1100        0.0000             nan     0.4113   -0.0000
  1120        0.0000             nan     0.4113   -0.0000
  1140        0.0000             nan     0.4113   -0.0000
  1160        0.0000             nan     0.4113   -0.0000
  1180        0.0000             nan     0.4113   -0.0000
  1200        0.0000             nan     0.4113   -0.0000
  1220        0.0000             nan     0.4113   -0.0000
  1240        0.0000             nan     0.4113   -0.0000
  1260        0.0000             nan     0.4113   -0.0000
  1280        0.0000             nan     0.4113   -0.0000
  1300        0.0000             nan     0.4113   -0.0000
  1320        0.0000             nan     0.4113   -0.0000
  1340        0.0000             nan     0.4113   -0.0000
  1360        0.0000             nan     0.4113   -0.0000
  1380        0.0000             nan     0.4113   -0.0000
  1400        0.0000             nan     0.4113   -0.0000
  1420        0.0000             nan     0.4113   -0.0000
  1440        0.0000             nan     0.4113   -0.0000
  1460        0.0000             nan     0.4113   -0.0000
  1480        0.0000             nan     0.4113   -0.0000
  1500        0.0000             nan     0.4113   -0.0000
  1520        0.0000             nan     0.4113   -0.0000
  1540        0.0000             nan     0.4113   -0.0000
  1560        0.0000             nan     0.4113   -0.0000
  1580        0.0000             nan     0.4113   -0.0000
  1600        0.0000             nan     0.4113   -0.0000
  1620        0.0000             nan     0.4113   -0.0000
  1640        0.0000             nan     0.4113   -0.0000
  1660        0.0000             nan     0.4113   -0.0000
  1680        0.0000             nan     0.4113   -0.0000
  1700        0.0000             nan     0.4113    0.0000
  1720        0.0000             nan     0.4113   -0.0000
  1740        0.0000             nan     0.4113   -0.0000
  1760        0.0000             nan     0.4113   -0.0000
  1780        0.0000             nan     0.4113   -0.0000
  1800        0.0000             nan     0.4113   -0.0000
  1820        0.0000             nan     0.4113   -0.0000
  1840        0.0000             nan     0.4113   -0.0000
  1860        0.0000             nan     0.4113   -0.0000
  1880        0.0000             nan     0.4113   -0.0000
  1900        0.0000             nan     0.4113   -0.0000
  1920        0.0000             nan     0.4113   -0.0000
  1940        0.0000             nan     0.4113   -0.0000
  1960        0.0000             nan     0.4113   -0.0000
  1980        0.0000             nan     0.4113   -0.0000
  2000        0.0000             nan     0.4113   -0.0000
  2020        0.0000             nan     0.4113   -0.0000
  2040        0.0000             nan     0.4113   -0.0000
  2060        0.0000             nan     0.4113   -0.0000
  2080        0.0000             nan     0.4113   -0.0000
  2100        0.0000             nan     0.4113   -0.0000
  2120        0.0000             nan     0.4113   -0.0000
  2140        0.0000             nan     0.4113   -0.0000
  2160        0.0000             nan     0.4113   -0.0000
  2180        0.0000             nan     0.4113   -0.0000
  2200        0.0000             nan     0.4113   -0.0000
  2220        0.0000             nan     0.4113   -0.0000
  2240        0.0000             nan     0.4113   -0.0000
  2260        0.0000             nan     0.4113   -0.0000
  2280        0.0000             nan     0.4113   -0.0000
  2300        0.0000             nan     0.4113   -0.0000
  2320        0.0000             nan     0.4113   -0.0000
  2340        0.0000             nan     0.4113   -0.0000
  2360        0.0000             nan     0.4113   -0.0000
  2380        0.0000             nan     0.4113   -0.0000
  2384        0.0000             nan     0.4113   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0335             nan     0.4186    0.0569
     2        0.0129             nan     0.4186    0.0174
     3        0.0053             nan     0.4186    0.0057
     4        0.0027             nan     0.4186    0.0025
     5        0.0017             nan     0.4186    0.0006
     6        0.0012             nan     0.4186    0.0005
     7        0.0009             nan     0.4186    0.0001
     8        0.0007             nan     0.4186   -0.0000
     9        0.0006             nan     0.4186   -0.0001
    10        0.0006             nan     0.4186   -0.0001
    20        0.0002             nan     0.4186   -0.0000
    40        0.0001             nan     0.4186   -0.0000
    60        0.0000             nan     0.4186    0.0000
    80        0.0000             nan     0.4186   -0.0000
   100        0.0000             nan     0.4186   -0.0000
   120        0.0000             nan     0.4186   -0.0000
   140        0.0000             nan     0.4186   -0.0000
   160        0.0000             nan     0.4186   -0.0000
   180        0.0000             nan     0.4186   -0.0000
   200        0.0000             nan     0.4186   -0.0000
   220        0.0000             nan     0.4186   -0.0000
   240        0.0000             nan     0.4186   -0.0000
   260        0.0000             nan     0.4186   -0.0000
   280        0.0000             nan     0.4186   -0.0000
   300        0.0000             nan     0.4186    0.0000
   320        0.0000             nan     0.4186   -0.0000
   340        0.0000             nan     0.4186   -0.0000
   360        0.0000             nan     0.4186   -0.0000
   380        0.0000             nan     0.4186   -0.0000
   400        0.0000             nan     0.4186   -0.0000
   420        0.0000             nan     0.4186   -0.0000
   440        0.0000             nan     0.4186   -0.0000
   460        0.0000             nan     0.4186   -0.0000
   480        0.0000             nan     0.4186   -0.0000
   500        0.0000             nan     0.4186   -0.0000
   520        0.0000             nan     0.4186   -0.0000
   540        0.0000             nan     0.4186   -0.0000
   560        0.0000             nan     0.4186   -0.0000
   580        0.0000             nan     0.4186   -0.0000
   600        0.0000             nan     0.4186   -0.0000
   620        0.0000             nan     0.4186   -0.0000
   640        0.0000             nan     0.4186   -0.0000
   660        0.0000             nan     0.4186   -0.0000
   680        0.0000             nan     0.4186   -0.0000
   700        0.0000             nan     0.4186   -0.0000
   720        0.0000             nan     0.4186   -0.0000
   740        0.0000             nan     0.4186   -0.0000
   760        0.0000             nan     0.4186   -0.0000
   780        0.0000             nan     0.4186   -0.0000
   800        0.0000             nan     0.4186   -0.0000
   820        0.0000             nan     0.4186   -0.0000
   840        0.0000             nan     0.4186   -0.0000
   860        0.0000             nan     0.4186   -0.0000
   880        0.0000             nan     0.4186    0.0000
   900        0.0000             nan     0.4186   -0.0000
   920        0.0000             nan     0.4186   -0.0000
   940        0.0000             nan     0.4186    0.0000
   960        0.0000             nan     0.4186   -0.0000
   980        0.0000             nan     0.4186   -0.0000
  1000        0.0000             nan     0.4186   -0.0000
  1020        0.0000             nan     0.4186   -0.0000
  1040        0.0000             nan     0.4186   -0.0000
  1060        0.0000             nan     0.4186   -0.0000
  1080        0.0000             nan     0.4186   -0.0000
  1100        0.0000             nan     0.4186   -0.0000
  1120        0.0000             nan     0.4186   -0.0000
  1140        0.0000             nan     0.4186   -0.0000
  1160        0.0000             nan     0.4186   -0.0000
  1180        0.0000             nan     0.4186   -0.0000
  1200        0.0000             nan     0.4186   -0.0000
  1220        0.0000             nan     0.4186   -0.0000
  1240        0.0000             nan     0.4186   -0.0000
  1260        0.0000             nan     0.4186   -0.0000
  1280        0.0000             nan     0.4186   -0.0000
  1300        0.0000             nan     0.4186   -0.0000
  1320        0.0000             nan     0.4186   -0.0000
  1340        0.0000             nan     0.4186   -0.0000
  1360        0.0000             nan     0.4186   -0.0000
  1380        0.0000             nan     0.4186   -0.0000
  1400        0.0000             nan     0.4186   -0.0000
  1420        0.0000             nan     0.4186   -0.0000
  1440        0.0000             nan     0.4186   -0.0000
  1460        0.0000             nan     0.4186   -0.0000
  1480        0.0000             nan     0.4186   -0.0000
  1500        0.0000             nan     0.4186   -0.0000
  1520        0.0000             nan     0.4186   -0.0000
  1540        0.0000             nan     0.4186   -0.0000
  1560        0.0000             nan     0.4186   -0.0000
  1580        0.0000             nan     0.4186   -0.0000
  1585        0.0000             nan     0.4186   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0399             nan     0.4267    0.0595
     2        0.0170             nan     0.4267    0.0215
     3        0.0072             nan     0.4267    0.0071
     4        0.0033             nan     0.4267    0.0022
     5        0.0020             nan     0.4267    0.0009
     6        0.0014             nan     0.4267    0.0007
     7        0.0011             nan     0.4267    0.0001
     8        0.0009             nan     0.4267   -0.0002
     9        0.0008             nan     0.4267   -0.0001
    10        0.0008             nan     0.4267   -0.0001
    20        0.0003             nan     0.4267   -0.0000
    40        0.0001             nan     0.4267   -0.0000
    60        0.0000             nan     0.4267   -0.0000
    80        0.0000             nan     0.4267   -0.0000
   100        0.0000             nan     0.4267   -0.0000
   120        0.0000             nan     0.4267    0.0000
   140        0.0000             nan     0.4267   -0.0000
   160        0.0000             nan     0.4267   -0.0000
   180        0.0000             nan     0.4267   -0.0000
   200        0.0000             nan     0.4267   -0.0000
   220        0.0000             nan     0.4267   -0.0000
   240        0.0000             nan     0.4267    0.0000
   260        0.0000             nan     0.4267   -0.0000
   280        0.0000             nan     0.4267   -0.0000
   300        0.0000             nan     0.4267   -0.0000
   320        0.0000             nan     0.4267   -0.0000
   340        0.0000             nan     0.4267   -0.0000
   360        0.0000             nan     0.4267   -0.0000
   380        0.0000             nan     0.4267   -0.0000
   400        0.0000             nan     0.4267   -0.0000
   420        0.0000             nan     0.4267   -0.0000
   440        0.0000             nan     0.4267   -0.0000
   460        0.0000             nan     0.4267   -0.0000
   480        0.0000             nan     0.4267   -0.0000
   500        0.0000             nan     0.4267   -0.0000
   520        0.0000             nan     0.4267   -0.0000
   540        0.0000             nan     0.4267    0.0000
   560        0.0000             nan     0.4267   -0.0000
   580        0.0000             nan     0.4267   -0.0000
   600        0.0000             nan     0.4267   -0.0000
   620        0.0000             nan     0.4267   -0.0000
   640        0.0000             nan     0.4267   -0.0000
   660        0.0000             nan     0.4267   -0.0000
   680        0.0000             nan     0.4267   -0.0000
   700        0.0000             nan     0.4267   -0.0000
   720        0.0000             nan     0.4267    0.0000
   740        0.0000             nan     0.4267   -0.0000
   760        0.0000             nan     0.4267   -0.0000
   780        0.0000             nan     0.4267   -0.0000
   800        0.0000             nan     0.4267   -0.0000
   820        0.0000             nan     0.4267   -0.0000
   840        0.0000             nan     0.4267   -0.0000
   860        0.0000             nan     0.4267    0.0000
   880        0.0000             nan     0.4267   -0.0000
   900        0.0000             nan     0.4267   -0.0000
   920        0.0000             nan     0.4267   -0.0000
   940        0.0000             nan     0.4267   -0.0000
   960        0.0000             nan     0.4267    0.0000
   980        0.0000             nan     0.4267   -0.0000
  1000        0.0000             nan     0.4267   -0.0000
  1020        0.0000             nan     0.4267   -0.0000
  1040        0.0000             nan     0.4267   -0.0000
  1060        0.0000             nan     0.4267   -0.0000
  1080        0.0000             nan     0.4267   -0.0000
  1100        0.0000             nan     0.4267   -0.0000
  1120        0.0000             nan     0.4267   -0.0000
  1140        0.0000             nan     0.4267    0.0000
  1160        0.0000             nan     0.4267   -0.0000
  1180        0.0000             nan     0.4267   -0.0000
  1200        0.0000             nan     0.4267   -0.0000
  1220        0.0000             nan     0.4267   -0.0000
  1240        0.0000             nan     0.4267   -0.0000
  1260        0.0000             nan     0.4267   -0.0000
  1280        0.0000             nan     0.4267   -0.0000
  1300        0.0000             nan     0.4267   -0.0000
  1320        0.0000             nan     0.4267   -0.0000
  1340        0.0000             nan     0.4267   -0.0000
  1360        0.0000             nan     0.4267   -0.0000
  1380        0.0000             nan     0.4267   -0.0000
  1400        0.0000             nan     0.4267   -0.0000
  1420        0.0000             nan     0.4267   -0.0000
  1440        0.0000             nan     0.4267   -0.0000
  1460        0.0000             nan     0.4267   -0.0000
  1480        0.0000             nan     0.4267   -0.0000
  1500        0.0000             nan     0.4267   -0.0000
  1520        0.0000             nan     0.4267   -0.0000
  1540        0.0000             nan     0.4267   -0.0000
  1560        0.0000             nan     0.4267   -0.0000
  1580        0.0000             nan     0.4267   -0.0000
  1600        0.0000             nan     0.4267   -0.0000
  1620        0.0000             nan     0.4267   -0.0000
  1640        0.0000             nan     0.4267   -0.0000
  1660        0.0000             nan     0.4267   -0.0000
  1680        0.0000             nan     0.4267   -0.0000
  1700        0.0000             nan     0.4267   -0.0000
  1720        0.0000             nan     0.4267    0.0000
  1740        0.0000             nan     0.4267   -0.0000
  1760        0.0000             nan     0.4267   -0.0000
  1780        0.0000             nan     0.4267   -0.0000
  1800        0.0000             nan     0.4267   -0.0000
  1820        0.0000             nan     0.4267   -0.0000
  1840        0.0000             nan     0.4267   -0.0000
  1860        0.0000             nan     0.4267   -0.0000
  1880        0.0000             nan     0.4267   -0.0000
  1900        0.0000             nan     0.4267   -0.0000
  1920        0.0000             nan     0.4267   -0.0000
  1940        0.0000             nan     0.4267   -0.0000
  1960        0.0000             nan     0.4267   -0.0000
  1980        0.0000             nan     0.4267   -0.0000
  2000        0.0000             nan     0.4267   -0.0000
  2020        0.0000             nan     0.4267   -0.0000
  2040        0.0000             nan     0.4267   -0.0000
  2060        0.0000             nan     0.4267   -0.0000
  2080        0.0000             nan     0.4267   -0.0000
  2100        0.0000             nan     0.4267   -0.0000
  2120        0.0000             nan     0.4267   -0.0000
  2140        0.0000             nan     0.4267   -0.0000
  2160        0.0000             nan     0.4267   -0.0000
  2180        0.0000             nan     0.4267   -0.0000
  2200        0.0000             nan     0.4267   -0.0000
  2220        0.0000             nan     0.4267   -0.0000
  2240        0.0000             nan     0.4267   -0.0000
  2260        0.0000             nan     0.4267   -0.0000
  2280        0.0000             nan     0.4267   -0.0000
  2300        0.0000             nan     0.4267   -0.0000
  2320        0.0000             nan     0.4267   -0.0000
  2340        0.0000             nan     0.4267   -0.0000
  2360        0.0000             nan     0.4267   -0.0000
  2380        0.0000             nan     0.4267   -0.0000
  2400        0.0000             nan     0.4267   -0.0000
  2420        0.0000             nan     0.4267   -0.0000
  2440        0.0000             nan     0.4267   -0.0000
  2460        0.0000             nan     0.4267   -0.0000
  2480        0.0000             nan     0.4267   -0.0000
  2500        0.0000             nan     0.4267   -0.0000
  2520        0.0000             nan     0.4267   -0.0000
  2540        0.0000             nan     0.4267   -0.0000
  2560        0.0000             nan     0.4267   -0.0000
  2580        0.0000             nan     0.4267   -0.0000
  2600        0.0000             nan     0.4267   -0.0000
  2620        0.0000             nan     0.4267    0.0000
  2640        0.0000             nan     0.4267   -0.0000
  2660        0.0000             nan     0.4267   -0.0000
  2680        0.0000             nan     0.4267   -0.0000
  2700        0.0000             nan     0.4267   -0.0000
  2720        0.0000             nan     0.4267   -0.0000
  2740        0.0000             nan     0.4267   -0.0000
  2760        0.0000             nan     0.4267   -0.0000
  2780        0.0000             nan     0.4267   -0.0000
  2800        0.0000             nan     0.4267   -0.0000
  2820        0.0000             nan     0.4267   -0.0000
  2840        0.0000             nan     0.4267   -0.0000
  2860        0.0000             nan     0.4267   -0.0000
  2880        0.0000             nan     0.4267   -0.0000
  2900        0.0000             nan     0.4267   -0.0000
  2920        0.0000             nan     0.4267   -0.0000
  2940        0.0000             nan     0.4267   -0.0000
  2960        0.0000             nan     0.4267   -0.0000
  2980        0.0000             nan     0.4267   -0.0000
  3000        0.0000             nan     0.4267   -0.0000
  3020        0.0000             nan     0.4267   -0.0000
  3040        0.0000             nan     0.4267   -0.0000
  3060        0.0000             nan     0.4267   -0.0000
  3080        0.0000             nan     0.4267   -0.0000
  3100        0.0000             nan     0.4267   -0.0000
  3120        0.0000             nan     0.4267   -0.0000
  3140        0.0000             nan     0.4267   -0.0000
  3160        0.0000             nan     0.4267   -0.0000
  3180        0.0000             nan     0.4267   -0.0000
  3200        0.0000             nan     0.4267   -0.0000
  3220        0.0000             nan     0.4267   -0.0000
  3240        0.0000             nan     0.4267   -0.0000
  3260        0.0000             nan     0.4267   -0.0000
  3280        0.0000             nan     0.4267   -0.0000
  3300        0.0000             nan     0.4267   -0.0000
  3320        0.0000             nan     0.4267   -0.0000
  3340        0.0000             nan     0.4267   -0.0000
  3360        0.0000             nan     0.4267   -0.0000
  3380        0.0000             nan     0.4267   -0.0000
  3400        0.0000             nan     0.4267   -0.0000
  3420        0.0000             nan     0.4267   -0.0000
  3440        0.0000             nan     0.4267   -0.0000
  3460        0.0000             nan     0.4267   -0.0000
  3480        0.0000             nan     0.4267   -0.0000
  3500        0.0000             nan     0.4267   -0.0000
  3520        0.0000             nan     0.4267    0.0000
  3540        0.0000             nan     0.4267   -0.0000
  3560        0.0000             nan     0.4267   -0.0000
  3580        0.0000             nan     0.4267    0.0000
  3600        0.0000             nan     0.4267   -0.0000
  3620        0.0000             nan     0.4267   -0.0000
  3640        0.0000             nan     0.4267   -0.0000
  3660        0.0000             nan     0.4267   -0.0000
  3680        0.0000             nan     0.4267   -0.0000
  3700        0.0000             nan     0.4267   -0.0000
  3720        0.0000             nan     0.4267   -0.0000
  3740        0.0000             nan     0.4267   -0.0000
  3760        0.0000             nan     0.4267   -0.0000
  3780        0.0000             nan     0.4267   -0.0000
  3800        0.0000             nan     0.4267   -0.0000
  3820        0.0000             nan     0.4267    0.0000
  3840        0.0000             nan     0.4267   -0.0000
  3860        0.0000             nan     0.4267   -0.0000
  3880        0.0000             nan     0.4267   -0.0000
  3900        0.0000             nan     0.4267   -0.0000
  3920        0.0000             nan     0.4267   -0.0000
  3940        0.0000             nan     0.4267   -0.0000
  3960        0.0000             nan     0.4267   -0.0000
  3980        0.0000             nan     0.4267   -0.0000
  4000        0.0000             nan     0.4267   -0.0000
  4020        0.0000             nan     0.4267   -0.0000
  4040        0.0000             nan     0.4267   -0.0000
  4060        0.0000             nan     0.4267   -0.0000
  4080        0.0000             nan     0.4267   -0.0000
  4100        0.0000             nan     0.4267   -0.0000
  4120        0.0000             nan     0.4267   -0.0000
  4140        0.0000             nan     0.4267   -0.0000
  4160        0.0000             nan     0.4267   -0.0000
  4180        0.0000             nan     0.4267   -0.0000
  4200        0.0000             nan     0.4267   -0.0000
  4220        0.0000             nan     0.4267   -0.0000
  4240        0.0000             nan     0.4267   -0.0000
  4260        0.0000             nan     0.4267   -0.0000
  4280        0.0000             nan     0.4267   -0.0000
  4300        0.0000             nan     0.4267   -0.0000
  4320        0.0000             nan     0.4267   -0.0000
  4340        0.0000             nan     0.4267   -0.0000
  4345        0.0000             nan     0.4267   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0514             nan     0.5404    0.0215
     2        0.0202             nan     0.5404    0.0275
     3        0.0142             nan     0.5404    0.0065
     4        0.0118             nan     0.5404    0.0026
     5        0.0101             nan     0.5404    0.0011
     6        0.0091             nan     0.5404   -0.0015
     7        0.0080             nan     0.5404    0.0000
     8        0.0083             nan     0.5404   -0.0021
     9        0.0083             nan     0.5404   -0.0008
    10        0.0077             nan     0.5404   -0.0004
    20        0.0058             nan     0.5404   -0.0010
    40        0.0049             nan     0.5404   -0.0006
    60        0.0035             nan     0.5404   -0.0001
    80        0.0029             nan     0.5404   -0.0006
   100        0.0019             nan     0.5404   -0.0005
   120        0.0018             nan     0.5404   -0.0001
   140        0.0014             nan     0.5404   -0.0002
   160        0.0013             nan     0.5404   -0.0003
   180        0.0009             nan     0.5404   -0.0001
   200        0.0007             nan     0.5404   -0.0001
   220        0.0006             nan     0.5404   -0.0000
   240        0.0006             nan     0.5404   -0.0000
   260        0.0006             nan     0.5404    0.0000
   280        0.0005             nan     0.5404    0.0000
   300        0.0005             nan     0.5404   -0.0001
   320        0.0004             nan     0.5404   -0.0000
   340        0.0004             nan     0.5404   -0.0000
   360        0.0004             nan     0.5404   -0.0001
   380        0.0003             nan     0.5404   -0.0000
   400        0.0003             nan     0.5404   -0.0000
   420        0.0003             nan     0.5404   -0.0000
   440        0.0002             nan     0.5404   -0.0000
   460        0.0002             nan     0.5404   -0.0001
   480        0.0002             nan     0.5404   -0.0000
   500        0.0002             nan     0.5404   -0.0000
   520        0.0002             nan     0.5404   -0.0000
   540        0.0001             nan     0.5404   -0.0000
   560        0.0001             nan     0.5404   -0.0000
   580        0.0001             nan     0.5404    0.0000
   600        0.0001             nan     0.5404   -0.0000
   620        0.0001             nan     0.5404   -0.0000
   640        0.0001             nan     0.5404   -0.0000
   660        0.0001             nan     0.5404   -0.0000
   680        0.0001             nan     0.5404   -0.0000
   700        0.0001             nan     0.5404   -0.0000
   720        0.0001             nan     0.5404   -0.0000
   740        0.0001             nan     0.5404   -0.0000
   760        0.0001             nan     0.5404   -0.0000
   780        0.0001             nan     0.5404   -0.0000
   800        0.0001             nan     0.5404    0.0000
   820        0.0001             nan     0.5404   -0.0000
   840        0.0000             nan     0.5404   -0.0000
   860        0.0000             nan     0.5404   -0.0000
   880        0.0000             nan     0.5404   -0.0000
   900        0.0000             nan     0.5404   -0.0000
   920        0.0000             nan     0.5404   -0.0000
   940        0.0000             nan     0.5404   -0.0000
   960        0.0000             nan     0.5404   -0.0000
   980        0.0000             nan     0.5404   -0.0000
  1000        0.0000             nan     0.5404   -0.0000
  1020        0.0000             nan     0.5404   -0.0000
  1040        0.0000             nan     0.5404   -0.0000
  1060        0.0000             nan     0.5404   -0.0000
  1080        0.0000             nan     0.5404   -0.0000
  1100        0.0000             nan     0.5404   -0.0000
  1120        0.0000             nan     0.5404   -0.0000
  1140        0.0000             nan     0.5404   -0.0000
  1160        0.0000             nan     0.5404   -0.0000
  1180        0.0000             nan     0.5404   -0.0000
  1200        0.0000             nan     0.5404   -0.0000
  1220        0.0000             nan     0.5404   -0.0000
  1240        0.0000             nan     0.5404   -0.0000
  1260        0.0000             nan     0.5404   -0.0000
  1280        0.0000             nan     0.5404   -0.0000
  1300        0.0000             nan     0.5404   -0.0000
  1320        0.0000             nan     0.5404   -0.0000
  1340        0.0000             nan     0.5404   -0.0000
  1360        0.0000             nan     0.5404   -0.0000
  1380        0.0000             nan     0.5404   -0.0000
  1400        0.0000             nan     0.5404   -0.0000
  1420        0.0000             nan     0.5404   -0.0000
  1440        0.0000             nan     0.5404   -0.0000
  1460        0.0000             nan     0.5404   -0.0000
  1480        0.0000             nan     0.5404   -0.0000
  1500        0.0000             nan     0.5404   -0.0000
  1520        0.0000             nan     0.5404   -0.0000
  1540        0.0000             nan     0.5404   -0.0000
  1560        0.0000             nan     0.5404   -0.0000
  1580        0.0000             nan     0.5404   -0.0000
  1600        0.0000             nan     0.5404   -0.0000
  1620        0.0000             nan     0.5404   -0.0000
  1640        0.0000             nan     0.5404   -0.0000
  1660        0.0000             nan     0.5404   -0.0000
  1680        0.0000             nan     0.5404   -0.0000
  1700        0.0000             nan     0.5404   -0.0000
  1720        0.0000             nan     0.5404   -0.0000
  1740        0.0000             nan     0.5404   -0.0000
  1760        0.0000             nan     0.5404   -0.0000
  1780        0.0000             nan     0.5404   -0.0000
  1800        0.0000             nan     0.5404   -0.0000
  1820        0.0000             nan     0.5404   -0.0000
  1840        0.0000             nan     0.5404   -0.0000
  1860        0.0000             nan     0.5404    0.0000
  1880        0.0000             nan     0.5404   -0.0000
  1900        0.0000             nan     0.5404   -0.0000
  1920        0.0000             nan     0.5404   -0.0000
  1940        0.0000             nan     0.5404    0.0000
  1960        0.0000             nan     0.5404    0.0000
  1980        0.0000             nan     0.5404   -0.0000
  2000        0.0000             nan     0.5404   -0.0000
  2020        0.0000             nan     0.5404   -0.0000
  2040        0.0000             nan     0.5404   -0.0000
  2060        0.0000             nan     0.5404   -0.0000
  2080        0.0000             nan     0.5404   -0.0000
  2100        0.0000             nan     0.5404   -0.0000
  2120        0.0000             nan     0.5404   -0.0000
  2140        0.0000             nan     0.5404    0.0000
  2160        0.0000             nan     0.5404   -0.0000
  2180        0.0000             nan     0.5404   -0.0000
  2200        0.0000             nan     0.5404   -0.0000
  2220        0.0000             nan     0.5404   -0.0000
  2240        0.0000             nan     0.5404   -0.0000
  2260        0.0000             nan     0.5404   -0.0000
  2280        0.0000             nan     0.5404   -0.0000
  2300        0.0000             nan     0.5404   -0.0000
  2320        0.0000             nan     0.5404   -0.0000
  2340        0.0000             nan     0.5404   -0.0000
  2360        0.0000             nan     0.5404   -0.0000
  2380        0.0000             nan     0.5404   -0.0000
  2400        0.0000             nan     0.5404   -0.0000
  2406        0.0000             nan     0.5404   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0407             nan     0.5955    0.0585
     2        0.0182             nan     0.5955    0.0180
     3        0.0122             nan     0.5955    0.0051
     4        0.0123             nan     0.5955   -0.0044
     5        0.0109             nan     0.5955    0.0020
     6        0.0109             nan     0.5955   -0.0027
     7        0.0097             nan     0.5955   -0.0004
     8        0.0098             nan     0.5955   -0.0014
     9        0.0094             nan     0.5955   -0.0001
    10        0.0084             nan     0.5955    0.0003
    20        0.0050             nan     0.5955   -0.0004
    40        0.0032             nan     0.5955   -0.0002
    60        0.0025             nan     0.5955   -0.0006
    80        0.0024             nan     0.5955   -0.0005
   100        0.0014             nan     0.5955   -0.0002
   120        0.0011             nan     0.5955   -0.0002
   140        0.0008             nan     0.5955   -0.0001
   160        0.0006             nan     0.5955   -0.0001
   180        0.0005             nan     0.5955   -0.0001
   200        0.0004             nan     0.5955   -0.0002
   220        0.0003             nan     0.5955   -0.0001
   240        0.0003             nan     0.5955   -0.0000
   260        0.0003             nan     0.5955   -0.0000
   280        0.0002             nan     0.5955   -0.0001
   300        0.0002             nan     0.5955   -0.0000
   320        0.0002             nan     0.5955   -0.0000
   340        0.0001             nan     0.5955   -0.0000
   360        0.0001             nan     0.5955   -0.0000
   380        0.0001             nan     0.5955   -0.0000
   400        0.0001             nan     0.5955   -0.0000
   420        0.0000             nan     0.5955   -0.0000
   440        0.0000             nan     0.5955    0.0000
   460        0.0000             nan     0.5955   -0.0000
   480        0.0000             nan     0.5955   -0.0000
   500        0.0000             nan     0.5955   -0.0000
   520        0.0000             nan     0.5955   -0.0000
   540        0.0000             nan     0.5955   -0.0000
   560        0.0000             nan     0.5955   -0.0000
   580        0.0000             nan     0.5955   -0.0000
   600        0.0000             nan     0.5955   -0.0000
   620        0.0000             nan     0.5955   -0.0000
   640        0.0000             nan     0.5955   -0.0000
   660        0.0000             nan     0.5955   -0.0000
   680        0.0000             nan     0.5955    0.0000
   700        0.0000             nan     0.5955   -0.0000
   720        0.0000             nan     0.5955   -0.0000
   740        0.0000             nan     0.5955   -0.0000
   760        0.0000             nan     0.5955   -0.0000
   780        0.0000             nan     0.5955   -0.0000
   800        0.0000             nan     0.5955   -0.0000
   820        0.0000             nan     0.5955   -0.0000
   840        0.0000             nan     0.5955   -0.0000
   860        0.0000             nan     0.5955   -0.0000
   880        0.0000             nan     0.5955   -0.0000
   900        0.0000             nan     0.5955   -0.0000
   920        0.0000             nan     0.5955   -0.0000
   940        0.0000             nan     0.5955    0.0000
   960        0.0000             nan     0.5955   -0.0000
   980        0.0000             nan     0.5955   -0.0000
  1000        0.0000             nan     0.5955   -0.0000
  1020        0.0000             nan     0.5955   -0.0000
  1040        0.0000             nan     0.5955   -0.0000
  1060        0.0000             nan     0.5955   -0.0000
  1080        0.0000             nan     0.5955   -0.0000
  1100        0.0000             nan     0.5955   -0.0000
  1120        0.0000             nan     0.5955   -0.0000
  1140        0.0000             nan     0.5955    0.0000
  1160        0.0000             nan     0.5955   -0.0000
  1180        0.0000             nan     0.5955   -0.0000
  1200        0.0000             nan     0.5955   -0.0000
  1220        0.0000             nan     0.5955   -0.0000
  1240        0.0000             nan     0.5955   -0.0000
  1260        0.0000             nan     0.5955   -0.0000
  1280        0.0000             nan     0.5955   -0.0000
  1300        0.0000             nan     0.5955    0.0000
  1320        0.0000             nan     0.5955   -0.0000
  1340        0.0000             nan     0.5955   -0.0000
  1360        0.0000             nan     0.5955   -0.0000
  1380        0.0000             nan     0.5955   -0.0000
  1400        0.0000             nan     0.5955   -0.0000
  1420        0.0000             nan     0.5955   -0.0000
  1440        0.0000             nan     0.5955   -0.0000
  1460        0.0000             nan     0.5955   -0.0000
  1480        0.0000             nan     0.5955   -0.0000
  1500        0.0000             nan     0.5955   -0.0000
  1520        0.0000             nan     0.5955   -0.0000
  1540        0.0000             nan     0.5955   -0.0000
  1560        0.0000             nan     0.5955   -0.0000
  1580        0.0000             nan     0.5955   -0.0000
  1600        0.0000             nan     0.5955    0.0000
  1620        0.0000             nan     0.5955   -0.0000
  1640        0.0000             nan     0.5955   -0.0000
  1660        0.0000             nan     0.5955   -0.0000
  1680        0.0000             nan     0.5955    0.0000
  1700        0.0000             nan     0.5955   -0.0000
  1720        0.0000             nan     0.5955   -0.0000
  1740        0.0000             nan     0.5955   -0.0000
  1760        0.0000             nan     0.5955   -0.0000
  1780        0.0000             nan     0.5955   -0.0000
  1800        0.0000             nan     0.5955   -0.0000
  1820        0.0000             nan     0.5955   -0.0000
  1840        0.0000             nan     0.5955   -0.0000
  1860        0.0000             nan     0.5955   -0.0000
  1880        0.0000             nan     0.5955   -0.0000
  1900        0.0000             nan     0.5955   -0.0000
  1920        0.0000             nan     0.5955   -0.0000
  1940        0.0000             nan     0.5955   -0.0000
  1953        0.0000             nan     0.5955   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0483             nan     0.2843    0.0280
     2        0.0312             nan     0.2843    0.0129
     3        0.0211             nan     0.2843    0.0085
     4        0.0148             nan     0.2843    0.0058
     5        0.0115             nan     0.2843    0.0038
     6        0.0084             nan     0.2843    0.0028
     7        0.0070             nan     0.2843    0.0011
     8        0.0061             nan     0.2843    0.0011
     9        0.0055             nan     0.2843    0.0004
    10        0.0049             nan     0.2843    0.0004
    20        0.0039             nan     0.2843   -0.0002
    40        0.0026             nan     0.2843   -0.0002
    60        0.0019             nan     0.2843    0.0000
    80        0.0014             nan     0.2843   -0.0001
   100        0.0011             nan     0.2843   -0.0001
   120        0.0009             nan     0.2843   -0.0001
   140        0.0008             nan     0.2843   -0.0000
   160        0.0006             nan     0.2843   -0.0000
   180        0.0005             nan     0.2843   -0.0001
   200        0.0005             nan     0.2843   -0.0000
   220        0.0004             nan     0.2843   -0.0000
   240        0.0003             nan     0.2843   -0.0000
   260        0.0003             nan     0.2843   -0.0000
   280        0.0003             nan     0.2843   -0.0000
   300        0.0002             nan     0.2843   -0.0000
   320        0.0002             nan     0.2843   -0.0000
   340        0.0002             nan     0.2843   -0.0000
   360        0.0001             nan     0.2843   -0.0000
   380        0.0001             nan     0.2843   -0.0000
   400        0.0001             nan     0.2843   -0.0000
   420        0.0001             nan     0.2843   -0.0000
   440        0.0001             nan     0.2843   -0.0000
   460        0.0001             nan     0.2843   -0.0000
   480        0.0001             nan     0.2843   -0.0000
   500        0.0001             nan     0.2843   -0.0000
   520        0.0001             nan     0.2843    0.0000
   540        0.0001             nan     0.2843   -0.0000
   560        0.0001             nan     0.2843   -0.0000
   580        0.0001             nan     0.2843   -0.0000
   600        0.0001             nan     0.2843   -0.0000
   620        0.0001             nan     0.2843   -0.0000
   640        0.0000             nan     0.2843   -0.0000
   660        0.0000             nan     0.2843   -0.0000
   680        0.0000             nan     0.2843   -0.0000
   700        0.0000             nan     0.2843   -0.0000
   720        0.0000             nan     0.2843   -0.0000
   740        0.0000             nan     0.2843   -0.0000
   760        0.0000             nan     0.2843   -0.0000
   780        0.0000             nan     0.2843   -0.0000
   800        0.0000             nan     0.2843   -0.0000
   820        0.0000             nan     0.2843   -0.0000
   840        0.0000             nan     0.2843   -0.0000
   860        0.0000             nan     0.2843   -0.0000
   880        0.0000             nan     0.2843   -0.0000
   900        0.0000             nan     0.2843   -0.0000
   920        0.0000             nan     0.2843   -0.0000
   940        0.0000             nan     0.2843   -0.0000
   960        0.0000             nan     0.2843   -0.0000
   980        0.0000             nan     0.2843   -0.0000
  1000        0.0000             nan     0.2843   -0.0000
  1020        0.0000             nan     0.2843   -0.0000
  1040        0.0000             nan     0.2843   -0.0000
  1060        0.0000             nan     0.2843   -0.0000
  1080        0.0000             nan     0.2843    0.0000
  1100        0.0000             nan     0.2843   -0.0000
  1120        0.0000             nan     0.2843   -0.0000
  1140        0.0000             nan     0.2843   -0.0000
  1160        0.0000             nan     0.2843   -0.0000
  1180        0.0000             nan     0.2843   -0.0000
  1200        0.0000             nan     0.2843   -0.0000
  1220        0.0000             nan     0.2843   -0.0000
  1240        0.0000             nan     0.2843   -0.0000
  1260        0.0000             nan     0.2843   -0.0000
  1280        0.0000             nan     0.2843   -0.0000
  1300        0.0000             nan     0.2843   -0.0000
  1320        0.0000             nan     0.2843   -0.0000
  1340        0.0000             nan     0.2843   -0.0000
  1360        0.0000             nan     0.2843   -0.0000
  1380        0.0000             nan     0.2843   -0.0000
  1400        0.0000             nan     0.2843   -0.0000
  1420        0.0000             nan     0.2843   -0.0000
  1440        0.0000             nan     0.2843   -0.0000
  1460        0.0000             nan     0.2843   -0.0000
  1480        0.0000             nan     0.2843   -0.0000
  1500        0.0000             nan     0.2843   -0.0000
  1520        0.0000             nan     0.2843   -0.0000
  1540        0.0000             nan     0.2843   -0.0000
  1560        0.0000             nan     0.2843   -0.0000
  1580        0.0000             nan     0.2843   -0.0000
  1600        0.0000             nan     0.2843   -0.0000
  1620        0.0000             nan     0.2843   -0.0000
  1640        0.0000             nan     0.2843   -0.0000
  1660        0.0000             nan     0.2843   -0.0000
  1680        0.0000             nan     0.2843   -0.0000
  1700        0.0000             nan     0.2843   -0.0000
  1720        0.0000             nan     0.2843   -0.0000
  1740        0.0000             nan     0.2843   -0.0000
  1760        0.0000             nan     0.2843   -0.0000
  1780        0.0000             nan     0.2843   -0.0000
  1800        0.0000             nan     0.2843   -0.0000
  1820        0.0000             nan     0.2843   -0.0000
  1840        0.0000             nan     0.2843   -0.0000
  1860        0.0000             nan     0.2843   -0.0000
  1880        0.0000             nan     0.2843   -0.0000
  1900        0.0000             nan     0.2843   -0.0000
  1920        0.0000             nan     0.2843   -0.0000
  1940        0.0000             nan     0.2843   -0.0000
  1960        0.0000             nan     0.2843   -0.0000
  1980        0.0000             nan     0.2843   -0.0000
  2000        0.0000             nan     0.2843   -0.0000
  2020        0.0000             nan     0.2843   -0.0000
  2040        0.0000             nan     0.2843   -0.0000
  2060        0.0000             nan     0.2843   -0.0000
  2080        0.0000             nan     0.2843   -0.0000
  2100        0.0000             nan     0.2843   -0.0000
  2120        0.0000             nan     0.2843   -0.0000
  2140        0.0000             nan     0.2843   -0.0000
  2160        0.0000             nan     0.2843   -0.0000
  2180        0.0000             nan     0.2843   -0.0000
  2200        0.0000             nan     0.2843   -0.0000
  2220        0.0000             nan     0.2843   -0.0000
  2240        0.0000             nan     0.2843   -0.0000
  2260        0.0000             nan     0.2843   -0.0000
  2280        0.0000             nan     0.2843   -0.0000
  2300        0.0000             nan     0.2843   -0.0000
  2320        0.0000             nan     0.2843   -0.0000
  2340        0.0000             nan     0.2843   -0.0000
  2360        0.0000             nan     0.2843   -0.0000
  2380        0.0000             nan     0.2843   -0.0000
  2400        0.0000             nan     0.2843   -0.0000
  2420        0.0000             nan     0.2843   -0.0000
  2440        0.0000             nan     0.2843   -0.0000
  2460        0.0000             nan     0.2843   -0.0000
  2480        0.0000             nan     0.2843   -0.0000
  2500        0.0000             nan     0.2843   -0.0000
  2520        0.0000             nan     0.2843   -0.0000
  2540        0.0000             nan     0.2843   -0.0000
  2560        0.0000             nan     0.2843   -0.0000
  2580        0.0000             nan     0.2843   -0.0000
  2600        0.0000             nan     0.2843   -0.0000
  2620        0.0000             nan     0.2843   -0.0000
  2640        0.0000             nan     0.2843   -0.0000
  2660        0.0000             nan     0.2843   -0.0000
  2680        0.0000             nan     0.2843    0.0000
  2700        0.0000             nan     0.2843   -0.0000
  2720        0.0000             nan     0.2843   -0.0000
  2740        0.0000             nan     0.2843   -0.0000
  2760        0.0000             nan     0.2843   -0.0000
  2780        0.0000             nan     0.2843   -0.0000
  2800        0.0000             nan     0.2843   -0.0000
  2820        0.0000             nan     0.2843   -0.0000
  2840        0.0000             nan     0.2843   -0.0000
  2860        0.0000             nan     0.2843   -0.0000
  2880        0.0000             nan     0.2843   -0.0000
  2900        0.0000             nan     0.2843   -0.0000
  2920        0.0000             nan     0.2843   -0.0000
  2940        0.0000             nan     0.2843   -0.0000
  2960        0.0000             nan     0.2843   -0.0000
  2980        0.0000             nan     0.2843   -0.0000
  3000        0.0000             nan     0.2843   -0.0000
  3020        0.0000             nan     0.2843   -0.0000
  3040        0.0000             nan     0.2843   -0.0000
  3060        0.0000             nan     0.2843   -0.0000
  3080        0.0000             nan     0.2843   -0.0000
  3100        0.0000             nan     0.2843   -0.0000
  3120        0.0000             nan     0.2843   -0.0000
  3140        0.0000             nan     0.2843   -0.0000
  3160        0.0000             nan     0.2843   -0.0000
  3180        0.0000             nan     0.2843   -0.0000
  3200        0.0000             nan     0.2843   -0.0000
  3220        0.0000             nan     0.2843   -0.0000
  3240        0.0000             nan     0.2843   -0.0000
  3260        0.0000             nan     0.2843   -0.0000
  3280        0.0000             nan     0.2843   -0.0000
  3300        0.0000             nan     0.2843   -0.0000
  3320        0.0000             nan     0.2843   -0.0000
  3340        0.0000             nan     0.2843   -0.0000
  3360        0.0000             nan     0.2843   -0.0000
  3380        0.0000             nan     0.2843   -0.0000
  3400        0.0000             nan     0.2843   -0.0000
  3420        0.0000             nan     0.2843   -0.0000
  3440        0.0000             nan     0.2843   -0.0000
  3460        0.0000             nan     0.2843   -0.0000
  3480        0.0000             nan     0.2843   -0.0000
  3500        0.0000             nan     0.2843   -0.0000
  3520        0.0000             nan     0.2843   -0.0000
  3540        0.0000             nan     0.2843   -0.0000
  3560        0.0000             nan     0.2843   -0.0000
  3580        0.0000             nan     0.2843   -0.0000
  3600        0.0000             nan     0.2843   -0.0000
  3620        0.0000             nan     0.2843   -0.0000
  3640        0.0000             nan     0.2843   -0.0000
  3660        0.0000             nan     0.2843   -0.0000
  3680        0.0000             nan     0.2843   -0.0000
  3700        0.0000             nan     0.2843   -0.0000
  3720        0.0000             nan     0.2843   -0.0000
  3740        0.0000             nan     0.2843   -0.0000
  3760        0.0000             nan     0.2843   -0.0000
  3780        0.0000             nan     0.2843   -0.0000
  3800        0.0000             nan     0.2843   -0.0000
  3820        0.0000             nan     0.2843   -0.0000
  3840        0.0000             nan     0.2843   -0.0000
  3860        0.0000             nan     0.2843   -0.0000
  3880        0.0000             nan     0.2843   -0.0000
  3900        0.0000             nan     0.2843   -0.0000
  3920        0.0000             nan     0.2843   -0.0000
  3940        0.0000             nan     0.2843   -0.0000
  3960        0.0000             nan     0.2843   -0.0000
  3980        0.0000             nan     0.2843   -0.0000
  4000        0.0000             nan     0.2843   -0.0000
  4020        0.0000             nan     0.2843   -0.0000
  4040        0.0000             nan     0.2843   -0.0000
  4060        0.0000             nan     0.2843   -0.0000
  4080        0.0000             nan     0.2843   -0.0000
  4100        0.0000             nan     0.2843   -0.0000
  4120        0.0000             nan     0.2843   -0.0000
  4140        0.0000             nan     0.2843   -0.0000
  4160        0.0000             nan     0.2843   -0.0000
  4180        0.0000             nan     0.2843   -0.0000
  4200        0.0000             nan     0.2843   -0.0000
  4220        0.0000             nan     0.2843   -0.0000
  4240        0.0000             nan     0.2843   -0.0000
  4260        0.0000             nan     0.2843   -0.0000
  4280        0.0000             nan     0.2843   -0.0000
  4300        0.0000             nan     0.2843   -0.0000
  4320        0.0000             nan     0.2843   -0.0000
  4340        0.0000             nan     0.2843   -0.0000
  4360        0.0000             nan     0.2843   -0.0000
  4380        0.0000             nan     0.2843   -0.0000
  4400        0.0000             nan     0.2843   -0.0000
  4420        0.0000             nan     0.2843   -0.0000
  4440        0.0000             nan     0.2843   -0.0000
  4460        0.0000             nan     0.2843   -0.0000
  4480        0.0000             nan     0.2843   -0.0000
  4500        0.0000             nan     0.2843   -0.0000
  4520        0.0000             nan     0.2843   -0.0000
  4540        0.0000             nan     0.2843   -0.0000
  4560        0.0000             nan     0.2843   -0.0000
  4580        0.0000             nan     0.2843   -0.0000
  4600        0.0000             nan     0.2843   -0.0000
  4620        0.0000             nan     0.2843   -0.0000
  4640        0.0000             nan     0.2843   -0.0000
  4660        0.0000             nan     0.2843   -0.0000
  4680        0.0000             nan     0.2843   -0.0000
  4700        0.0000             nan     0.2843   -0.0000
  4720        0.0000             nan     0.2843   -0.0000
  4740        0.0000             nan     0.2843   -0.0000
  4760        0.0000             nan     0.2843   -0.0000
  4780        0.0000             nan     0.2843   -0.0000
  4800        0.0000             nan     0.2843   -0.0000
  4820        0.0000             nan     0.2843   -0.0000
  4840        0.0000             nan     0.2843   -0.0000
  4860        0.0000             nan     0.2843   -0.0000
  4880        0.0000             nan     0.2843   -0.0000
  4900        0.0000             nan     0.2843   -0.0000
  4920        0.0000             nan     0.2843   -0.0000
  4940        0.0000             nan     0.2843   -0.0000
  4960        0.0000             nan     0.2843   -0.0000
  4965        0.0000             nan     0.2843   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0415             nan     0.3246    0.0272
     2        0.0241             nan     0.3246    0.0206
     3        0.0148             nan     0.3246    0.0069
     4        0.0111             nan     0.3246    0.0049
     5        0.0083             nan     0.3246    0.0024
     6        0.0061             nan     0.3246    0.0019
     7        0.0049             nan     0.3246    0.0009
     8        0.0040             nan     0.3246    0.0004
     9        0.0035             nan     0.3246    0.0004
    10        0.0031             nan     0.3246    0.0003
    20        0.0020             nan     0.3246   -0.0003
    40        0.0012             nan     0.3246   -0.0002
    60        0.0007             nan     0.3246   -0.0001
    80        0.0004             nan     0.3246   -0.0000
   100        0.0003             nan     0.3246   -0.0000
   120        0.0002             nan     0.3246   -0.0000
   140        0.0001             nan     0.3246   -0.0000
   160        0.0001             nan     0.3246   -0.0000
   180        0.0001             nan     0.3246   -0.0000
   200        0.0000             nan     0.3246   -0.0000
   220        0.0000             nan     0.3246   -0.0000
   240        0.0000             nan     0.3246   -0.0000
   260        0.0000             nan     0.3246   -0.0000
   280        0.0000             nan     0.3246   -0.0000
   300        0.0000             nan     0.3246   -0.0000
   320        0.0000             nan     0.3246    0.0000
   340        0.0000             nan     0.3246   -0.0000
   360        0.0000             nan     0.3246   -0.0000
   380        0.0000             nan     0.3246   -0.0000
   400        0.0000             nan     0.3246   -0.0000
   420        0.0000             nan     0.3246   -0.0000
   440        0.0000             nan     0.3246   -0.0000
   460        0.0000             nan     0.3246   -0.0000
   480        0.0000             nan     0.3246   -0.0000
   500        0.0000             nan     0.3246   -0.0000
   520        0.0000             nan     0.3246   -0.0000
   540        0.0000             nan     0.3246   -0.0000
   560        0.0000             nan     0.3246   -0.0000
   580        0.0000             nan     0.3246   -0.0000
   600        0.0000             nan     0.3246   -0.0000
   620        0.0000             nan     0.3246   -0.0000
   640        0.0000             nan     0.3246   -0.0000
   660        0.0000             nan     0.3246   -0.0000
   680        0.0000             nan     0.3246   -0.0000
   700        0.0000             nan     0.3246   -0.0000
   720        0.0000             nan     0.3246   -0.0000
   740        0.0000             nan     0.3246   -0.0000
   760        0.0000             nan     0.3246    0.0000
   780        0.0000             nan     0.3246   -0.0000
   800        0.0000             nan     0.3246   -0.0000
   820        0.0000             nan     0.3246   -0.0000
   840        0.0000             nan     0.3246   -0.0000
   860        0.0000             nan     0.3246   -0.0000
   880        0.0000             nan     0.3246   -0.0000
   900        0.0000             nan     0.3246   -0.0000
   920        0.0000             nan     0.3246   -0.0000
   940        0.0000             nan     0.3246   -0.0000
   960        0.0000             nan     0.3246   -0.0000
   980        0.0000             nan     0.3246   -0.0000
  1000        0.0000             nan     0.3246   -0.0000
  1020        0.0000             nan     0.3246   -0.0000
  1040        0.0000             nan     0.3246   -0.0000
  1060        0.0000             nan     0.3246   -0.0000
  1080        0.0000             nan     0.3246   -0.0000
  1100        0.0000             nan     0.3246   -0.0000
  1120        0.0000             nan     0.3246   -0.0000
  1140        0.0000             nan     0.3246   -0.0000
  1160        0.0000             nan     0.3246   -0.0000
  1180        0.0000             nan     0.3246   -0.0000
  1200        0.0000             nan     0.3246   -0.0000
  1220        0.0000             nan     0.3246   -0.0000
  1240        0.0000             nan     0.3246   -0.0000
  1260        0.0000             nan     0.3246   -0.0000
  1280        0.0000             nan     0.3246   -0.0000
  1300        0.0000             nan     0.3246   -0.0000
  1320        0.0000             nan     0.3246   -0.0000
  1340        0.0000             nan     0.3246   -0.0000
  1360        0.0000             nan     0.3246    0.0000
  1380        0.0000             nan     0.3246   -0.0000
  1400        0.0000             nan     0.3246   -0.0000
  1420        0.0000             nan     0.3246   -0.0000
  1440        0.0000             nan     0.3246   -0.0000
  1460        0.0000             nan     0.3246   -0.0000
  1480        0.0000             nan     0.3246   -0.0000
  1500        0.0000             nan     0.3246   -0.0000
  1520        0.0000             nan     0.3246   -0.0000
  1540        0.0000             nan     0.3246   -0.0000
  1560        0.0000             nan     0.3246   -0.0000
  1580        0.0000             nan     0.3246   -0.0000
  1600        0.0000             nan     0.3246   -0.0000
  1620        0.0000             nan     0.3246   -0.0000
  1640        0.0000             nan     0.3246   -0.0000
  1660        0.0000             nan     0.3246   -0.0000
  1680        0.0000             nan     0.3246   -0.0000
  1700        0.0000             nan     0.3246   -0.0000
  1720        0.0000             nan     0.3246   -0.0000
  1740        0.0000             nan     0.3246   -0.0000
  1760        0.0000             nan     0.3246   -0.0000
  1780        0.0000             nan     0.3246   -0.0000
  1800        0.0000             nan     0.3246   -0.0000
  1820        0.0000             nan     0.3246   -0.0000
  1840        0.0000             nan     0.3246   -0.0000
  1860        0.0000             nan     0.3246   -0.0000
  1880        0.0000             nan     0.3246   -0.0000
  1900        0.0000             nan     0.3246   -0.0000
  1920        0.0000             nan     0.3246    0.0000
  1940        0.0000             nan     0.3246   -0.0000
  1960        0.0000             nan     0.3246   -0.0000
  1980        0.0000             nan     0.3246   -0.0000
  2000        0.0000             nan     0.3246   -0.0000
  2020        0.0000             nan     0.3246   -0.0000
  2040        0.0000             nan     0.3246   -0.0000
  2060        0.0000             nan     0.3246   -0.0000
  2080        0.0000             nan     0.3246   -0.0000
  2100        0.0000             nan     0.3246   -0.0000
  2120        0.0000             nan     0.3246   -0.0000
  2140        0.0000             nan     0.3246   -0.0000
  2160        0.0000             nan     0.3246   -0.0000
  2180        0.0000             nan     0.3246   -0.0000
  2200        0.0000             nan     0.3246   -0.0000
  2220        0.0000             nan     0.3246   -0.0000
  2240        0.0000             nan     0.3246   -0.0000
  2260        0.0000             nan     0.3246   -0.0000
  2280        0.0000             nan     0.3246   -0.0000
  2300        0.0000             nan     0.3246   -0.0000
  2320        0.0000             nan     0.3246    0.0000
  2340        0.0000             nan     0.3246   -0.0000
  2360        0.0000             nan     0.3246   -0.0000
  2380        0.0000             nan     0.3246   -0.0000
  2400        0.0000             nan     0.3246   -0.0000
  2420        0.0000             nan     0.3246   -0.0000
  2440        0.0000             nan     0.3246   -0.0000
  2460        0.0000             nan     0.3246   -0.0000
  2480        0.0000             nan     0.3246   -0.0000
  2500        0.0000             nan     0.3246   -0.0000
  2520        0.0000             nan     0.3246   -0.0000
  2540        0.0000             nan     0.3246   -0.0000
  2560        0.0000             nan     0.3246   -0.0000
  2580        0.0000             nan     0.3246   -0.0000
  2600        0.0000             nan     0.3246   -0.0000
  2620        0.0000             nan     0.3246   -0.0000
  2640        0.0000             nan     0.3246   -0.0000
  2660        0.0000             nan     0.3246   -0.0000
  2680        0.0000             nan     0.3246   -0.0000
  2700        0.0000             nan     0.3246   -0.0000
  2720        0.0000             nan     0.3246   -0.0000
  2740        0.0000             nan     0.3246   -0.0000
  2760        0.0000             nan     0.3246   -0.0000
  2780        0.0000             nan     0.3246   -0.0000
  2800        0.0000             nan     0.3246   -0.0000
  2820        0.0000             nan     0.3246   -0.0000
  2840        0.0000             nan     0.3246   -0.0000
  2860        0.0000             nan     0.3246   -0.0000
  2880        0.0000             nan     0.3246   -0.0000
  2900        0.0000             nan     0.3246   -0.0000
  2920        0.0000             nan     0.3246   -0.0000
  2940        0.0000             nan     0.3246   -0.0000
  2960        0.0000             nan     0.3246   -0.0000
  2980        0.0000             nan     0.3246   -0.0000
  3000        0.0000             nan     0.3246   -0.0000
  3020        0.0000             nan     0.3246   -0.0000
  3040        0.0000             nan     0.3246   -0.0000
  3060        0.0000             nan     0.3246   -0.0000
  3080        0.0000             nan     0.3246   -0.0000
  3100        0.0000             nan     0.3246   -0.0000
  3120        0.0000             nan     0.3246   -0.0000
  3140        0.0000             nan     0.3246   -0.0000
  3160        0.0000             nan     0.3246   -0.0000
  3180        0.0000             nan     0.3246   -0.0000
  3200        0.0000             nan     0.3246   -0.0000
  3220        0.0000             nan     0.3246   -0.0000
  3240        0.0000             nan     0.3246   -0.0000
  3260        0.0000             nan     0.3246   -0.0000
  3280        0.0000             nan     0.3246   -0.0000
  3300        0.0000             nan     0.3246   -0.0000
  3320        0.0000             nan     0.3246   -0.0000
  3340        0.0000             nan     0.3246   -0.0000
  3360        0.0000             nan     0.3246   -0.0000
  3380        0.0000             nan     0.3246   -0.0000
  3400        0.0000             nan     0.3246   -0.0000
  3420        0.0000             nan     0.3246   -0.0000
  3440        0.0000             nan     0.3246   -0.0000
  3460        0.0000             nan     0.3246   -0.0000
  3480        0.0000             nan     0.3246   -0.0000
  3500        0.0000             nan     0.3246   -0.0000
  3520        0.0000             nan     0.3246   -0.0000
  3540        0.0000             nan     0.3246   -0.0000
  3560        0.0000             nan     0.3246   -0.0000
  3580        0.0000             nan     0.3246   -0.0000
  3600        0.0000             nan     0.3246   -0.0000
  3620        0.0000             nan     0.3246   -0.0000
  3640        0.0000             nan     0.3246   -0.0000
  3660        0.0000             nan     0.3246    0.0000
  3680        0.0000             nan     0.3246   -0.0000
  3700        0.0000             nan     0.3246   -0.0000
  3720        0.0000             nan     0.3246   -0.0000
  3740        0.0000             nan     0.3246   -0.0000
  3760        0.0000             nan     0.3246   -0.0000
  3780        0.0000             nan     0.3246   -0.0000
  3800        0.0000             nan     0.3246   -0.0000
  3820        0.0000             nan     0.3246   -0.0000
  3840        0.0000             nan     0.3246   -0.0000
  3860        0.0000             nan     0.3246   -0.0000
  3880        0.0000             nan     0.3246   -0.0000
  3900        0.0000             nan     0.3246   -0.0000
  3920        0.0000             nan     0.3246   -0.0000
  3940        0.0000             nan     0.3246   -0.0000
  3960        0.0000             nan     0.3246   -0.0000
  3980        0.0000             nan     0.3246   -0.0000
  4000        0.0000             nan     0.3246   -0.0000
  4020        0.0000             nan     0.3246   -0.0000
  4040        0.0000             nan     0.3246   -0.0000
  4060        0.0000             nan     0.3246   -0.0000
  4080        0.0000             nan     0.3246   -0.0000
  4100        0.0000             nan     0.3246   -0.0000
  4120        0.0000             nan     0.3246   -0.0000
  4140        0.0000             nan     0.3246   -0.0000
  4160        0.0000             nan     0.3246   -0.0000
  4180        0.0000             nan     0.3246   -0.0000
  4200        0.0000             nan     0.3246   -0.0000
  4220        0.0000             nan     0.3246   -0.0000
  4240        0.0000             nan     0.3246   -0.0000
  4260        0.0000             nan     0.3246   -0.0000
  4280        0.0000             nan     0.3246   -0.0000
  4300        0.0000             nan     0.3246   -0.0000
  4320        0.0000             nan     0.3246   -0.0000
  4340        0.0000             nan     0.3246   -0.0000
  4360        0.0000             nan     0.3246   -0.0000
  4380        0.0000             nan     0.3246   -0.0000
  4400        0.0000             nan     0.3246   -0.0000
  4420        0.0000             nan     0.3246   -0.0000
  4440        0.0000             nan     0.3246   -0.0000
  4460        0.0000             nan     0.3246   -0.0000
  4480        0.0000             nan     0.3246   -0.0000
  4500        0.0000             nan     0.3246   -0.0000
  4520        0.0000             nan     0.3246   -0.0000
  4540        0.0000             nan     0.3246   -0.0000
  4560        0.0000             nan     0.3246   -0.0000
  4580        0.0000             nan     0.3246    0.0000
  4600        0.0000             nan     0.3246   -0.0000
  4620        0.0000             nan     0.3246   -0.0000
  4640        0.0000             nan     0.3246   -0.0000
  4660        0.0000             nan     0.3246   -0.0000
  4672        0.0000             nan     0.3246   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0380             nan     0.4113    0.0353
     2        0.0153             nan     0.4113    0.0234
     3        0.0086             nan     0.4113    0.0074
     4        0.0050             nan     0.4113    0.0020
     5        0.0038             nan     0.4113    0.0009
     6        0.0037             nan     0.4113   -0.0005
     7        0.0031             nan     0.4113    0.0001
     8        0.0026             nan     0.4113    0.0002
     9        0.0025             nan     0.4113   -0.0005
    10        0.0023             nan     0.4113   -0.0003
    20        0.0010             nan     0.4113   -0.0002
    40        0.0004             nan     0.4113   -0.0001
    60        0.0002             nan     0.4113   -0.0000
    80        0.0001             nan     0.4113   -0.0000
   100        0.0000             nan     0.4113   -0.0000
   120        0.0000             nan     0.4113   -0.0000
   140        0.0000             nan     0.4113   -0.0000
   160        0.0000             nan     0.4113   -0.0000
   180        0.0000             nan     0.4113   -0.0000
   200        0.0000             nan     0.4113   -0.0000
   220        0.0000             nan     0.4113   -0.0000
   240        0.0000             nan     0.4113   -0.0000
   260        0.0000             nan     0.4113   -0.0000
   280        0.0000             nan     0.4113   -0.0000
   300        0.0000             nan     0.4113   -0.0000
   320        0.0000             nan     0.4113   -0.0000
   340        0.0000             nan     0.4113   -0.0000
   360        0.0000             nan     0.4113   -0.0000
   380        0.0000             nan     0.4113   -0.0000
   400        0.0000             nan     0.4113   -0.0000
   420        0.0000             nan     0.4113   -0.0000
   440        0.0000             nan     0.4113   -0.0000
   460        0.0000             nan     0.4113   -0.0000
   480        0.0000             nan     0.4113   -0.0000
   500        0.0000             nan     0.4113   -0.0000
   520        0.0000             nan     0.4113   -0.0000
   540        0.0000             nan     0.4113   -0.0000
   560        0.0000             nan     0.4113   -0.0000
   580        0.0000             nan     0.4113   -0.0000
   600        0.0000             nan     0.4113   -0.0000
   620        0.0000             nan     0.4113   -0.0000
   640        0.0000             nan     0.4113   -0.0000
   660        0.0000             nan     0.4113   -0.0000
   680        0.0000             nan     0.4113   -0.0000
   700        0.0000             nan     0.4113   -0.0000
   720        0.0000             nan     0.4113   -0.0000
   740        0.0000             nan     0.4113   -0.0000
   760        0.0000             nan     0.4113   -0.0000
   780        0.0000             nan     0.4113   -0.0000
   800        0.0000             nan     0.4113   -0.0000
   820        0.0000             nan     0.4113   -0.0000
   840        0.0000             nan     0.4113   -0.0000
   860        0.0000             nan     0.4113   -0.0000
   880        0.0000             nan     0.4113   -0.0000
   900        0.0000             nan     0.4113   -0.0000
   920        0.0000             nan     0.4113   -0.0000
   940        0.0000             nan     0.4113   -0.0000
   960        0.0000             nan     0.4113   -0.0000
   980        0.0000             nan     0.4113   -0.0000
  1000        0.0000             nan     0.4113   -0.0000
  1020        0.0000             nan     0.4113   -0.0000
  1040        0.0000             nan     0.4113   -0.0000
  1060        0.0000             nan     0.4113   -0.0000
  1080        0.0000             nan     0.4113   -0.0000
  1100        0.0000             nan     0.4113   -0.0000
  1120        0.0000             nan     0.4113   -0.0000
  1140        0.0000             nan     0.4113   -0.0000
  1160        0.0000             nan     0.4113   -0.0000
  1180        0.0000             nan     0.4113    0.0000
  1200        0.0000             nan     0.4113   -0.0000
  1220        0.0000             nan     0.4113   -0.0000
  1240        0.0000             nan     0.4113   -0.0000
  1260        0.0000             nan     0.4113   -0.0000
  1280        0.0000             nan     0.4113   -0.0000
  1300        0.0000             nan     0.4113   -0.0000
  1320        0.0000             nan     0.4113   -0.0000
  1340        0.0000             nan     0.4113   -0.0000
  1360        0.0000             nan     0.4113    0.0000
  1380        0.0000             nan     0.4113   -0.0000
  1400        0.0000             nan     0.4113   -0.0000
  1420        0.0000             nan     0.4113    0.0000
  1440        0.0000             nan     0.4113   -0.0000
  1460        0.0000             nan     0.4113    0.0000
  1480        0.0000             nan     0.4113   -0.0000
  1500        0.0000             nan     0.4113   -0.0000
  1520        0.0000             nan     0.4113   -0.0000
  1540        0.0000             nan     0.4113   -0.0000
  1560        0.0000             nan     0.4113   -0.0000
  1580        0.0000             nan     0.4113   -0.0000
  1600        0.0000             nan     0.4113   -0.0000
  1620        0.0000             nan     0.4113   -0.0000
  1640        0.0000             nan     0.4113   -0.0000
  1660        0.0000             nan     0.4113   -0.0000
  1680        0.0000             nan     0.4113   -0.0000
  1700        0.0000             nan     0.4113   -0.0000
  1720        0.0000             nan     0.4113   -0.0000
  1740        0.0000             nan     0.4113    0.0000
  1760        0.0000             nan     0.4113   -0.0000
  1780        0.0000             nan     0.4113   -0.0000
  1800        0.0000             nan     0.4113   -0.0000
  1820        0.0000             nan     0.4113   -0.0000
  1840        0.0000             nan     0.4113   -0.0000
  1860        0.0000             nan     0.4113   -0.0000
  1880        0.0000             nan     0.4113   -0.0000
  1900        0.0000             nan     0.4113   -0.0000
  1920        0.0000             nan     0.4113   -0.0000
  1940        0.0000             nan     0.4113   -0.0000
  1960        0.0000             nan     0.4113   -0.0000
  1980        0.0000             nan     0.4113   -0.0000
  2000        0.0000             nan     0.4113   -0.0000
  2020        0.0000             nan     0.4113   -0.0000
  2040        0.0000             nan     0.4113   -0.0000
  2060        0.0000             nan     0.4113   -0.0000
  2080        0.0000             nan     0.4113   -0.0000
  2100        0.0000             nan     0.4113   -0.0000
  2120        0.0000             nan     0.4113   -0.0000
  2140        0.0000             nan     0.4113   -0.0000
  2160        0.0000             nan     0.4113   -0.0000
  2180        0.0000             nan     0.4113   -0.0000
  2200        0.0000             nan     0.4113   -0.0000
  2220        0.0000             nan     0.4113   -0.0000
  2240        0.0000             nan     0.4113   -0.0000
  2260        0.0000             nan     0.4113   -0.0000
  2280        0.0000             nan     0.4113   -0.0000
  2300        0.0000             nan     0.4113   -0.0000
  2320        0.0000             nan     0.4113   -0.0000
  2340        0.0000             nan     0.4113   -0.0000
  2360        0.0000             nan     0.4113    0.0000
  2380        0.0000             nan     0.4113   -0.0000
  2384        0.0000             nan     0.4113   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0297             nan     0.4186    0.0361
     2        0.0140             nan     0.4186    0.0143
     3        0.0057             nan     0.4186    0.0064
     4        0.0031             nan     0.4186    0.0027
     5        0.0021             nan     0.4186    0.0013
     6        0.0013             nan     0.4186    0.0003
     7        0.0012             nan     0.4186   -0.0000
     8        0.0011             nan     0.4186   -0.0001
     9        0.0011             nan     0.4186   -0.0002
    10        0.0010             nan     0.4186   -0.0002
    20        0.0004             nan     0.4186   -0.0001
    40        0.0001             nan     0.4186   -0.0000
    60        0.0000             nan     0.4186   -0.0000
    80        0.0000             nan     0.4186   -0.0000
   100        0.0000             nan     0.4186   -0.0000
   120        0.0000             nan     0.4186   -0.0000
   140        0.0000             nan     0.4186   -0.0000
   160        0.0000             nan     0.4186   -0.0000
   180        0.0000             nan     0.4186   -0.0000
   200        0.0000             nan     0.4186   -0.0000
   220        0.0000             nan     0.4186    0.0000
   240        0.0000             nan     0.4186   -0.0000
   260        0.0000             nan     0.4186   -0.0000
   280        0.0000             nan     0.4186   -0.0000
   300        0.0000             nan     0.4186   -0.0000
   320        0.0000             nan     0.4186    0.0000
   340        0.0000             nan     0.4186   -0.0000
   360        0.0000             nan     0.4186   -0.0000
   380        0.0000             nan     0.4186   -0.0000
   400        0.0000             nan     0.4186   -0.0000
   420        0.0000             nan     0.4186   -0.0000
   440        0.0000             nan     0.4186   -0.0000
   460        0.0000             nan     0.4186   -0.0000
   480        0.0000             nan     0.4186   -0.0000
   500        0.0000             nan     0.4186   -0.0000
   520        0.0000             nan     0.4186   -0.0000
   540        0.0000             nan     0.4186   -0.0000
   560        0.0000             nan     0.4186   -0.0000
   580        0.0000             nan     0.4186   -0.0000
   600        0.0000             nan     0.4186   -0.0000
   620        0.0000             nan     0.4186   -0.0000
   640        0.0000             nan     0.4186   -0.0000
   660        0.0000             nan     0.4186   -0.0000
   680        0.0000             nan     0.4186   -0.0000
   700        0.0000             nan     0.4186   -0.0000
   720        0.0000             nan     0.4186   -0.0000
   740        0.0000             nan     0.4186   -0.0000
   760        0.0000             nan     0.4186   -0.0000
   780        0.0000             nan     0.4186    0.0000
   800        0.0000             nan     0.4186   -0.0000
   820        0.0000             nan     0.4186   -0.0000
   840        0.0000             nan     0.4186   -0.0000
   860        0.0000             nan     0.4186   -0.0000
   880        0.0000             nan     0.4186    0.0000
   900        0.0000             nan     0.4186   -0.0000
   920        0.0000             nan     0.4186   -0.0000
   940        0.0000             nan     0.4186   -0.0000
   960        0.0000             nan     0.4186   -0.0000
   980        0.0000             nan     0.4186   -0.0000
  1000        0.0000             nan     0.4186   -0.0000
  1020        0.0000             nan     0.4186   -0.0000
  1040        0.0000             nan     0.4186   -0.0000
  1060        0.0000             nan     0.4186   -0.0000
  1080        0.0000             nan     0.4186   -0.0000
  1100        0.0000             nan     0.4186   -0.0000
  1120        0.0000             nan     0.4186   -0.0000
  1140        0.0000             nan     0.4186   -0.0000
  1160        0.0000             nan     0.4186   -0.0000
  1180        0.0000             nan     0.4186   -0.0000
  1200        0.0000             nan     0.4186   -0.0000
  1220        0.0000             nan     0.4186   -0.0000
  1240        0.0000             nan     0.4186   -0.0000
  1260        0.0000             nan     0.4186   -0.0000
  1280        0.0000             nan     0.4186    0.0000
  1300        0.0000             nan     0.4186   -0.0000
  1320        0.0000             nan     0.4186   -0.0000
  1340        0.0000             nan     0.4186   -0.0000
  1360        0.0000             nan     0.4186   -0.0000
  1380        0.0000             nan     0.4186   -0.0000
  1400        0.0000             nan     0.4186   -0.0000
  1420        0.0000             nan     0.4186   -0.0000
  1440        0.0000             nan     0.4186   -0.0000
  1460        0.0000             nan     0.4186   -0.0000
  1480        0.0000             nan     0.4186   -0.0000
  1500        0.0000             nan     0.4186   -0.0000
  1520        0.0000             nan     0.4186   -0.0000
  1540        0.0000             nan     0.4186   -0.0000
  1560        0.0000             nan     0.4186   -0.0000
  1580        0.0000             nan     0.4186   -0.0000
  1585        0.0000             nan     0.4186   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0278             nan     0.4267    0.0367
     2        0.0115             nan     0.4267    0.0162
     3        0.0054             nan     0.4267    0.0049
     4        0.0025             nan     0.4267    0.0025
     5        0.0019             nan     0.4267    0.0003
     6        0.0014             nan     0.4267    0.0000
     7        0.0012             nan     0.4267    0.0000
     8        0.0011             nan     0.4267   -0.0001
     9        0.0009             nan     0.4267   -0.0001
    10        0.0009             nan     0.4267   -0.0002
    20        0.0004             nan     0.4267   -0.0001
    40        0.0001             nan     0.4267   -0.0000
    60        0.0000             nan     0.4267   -0.0000
    80        0.0000             nan     0.4267    0.0000
   100        0.0000             nan     0.4267    0.0000
   120        0.0000             nan     0.4267   -0.0000
   140        0.0000             nan     0.4267   -0.0000
   160        0.0000             nan     0.4267   -0.0000
   180        0.0000             nan     0.4267   -0.0000
   200        0.0000             nan     0.4267   -0.0000
   220        0.0000             nan     0.4267    0.0000
   240        0.0000             nan     0.4267   -0.0000
   260        0.0000             nan     0.4267   -0.0000
   280        0.0000             nan     0.4267   -0.0000
   300        0.0000             nan     0.4267   -0.0000
   320        0.0000             nan     0.4267   -0.0000
   340        0.0000             nan     0.4267   -0.0000
   360        0.0000             nan     0.4267   -0.0000
   380        0.0000             nan     0.4267   -0.0000
   400        0.0000             nan     0.4267   -0.0000
   420        0.0000             nan     0.4267   -0.0000
   440        0.0000             nan     0.4267   -0.0000
   460        0.0000             nan     0.4267   -0.0000
   480        0.0000             nan     0.4267   -0.0000
   500        0.0000             nan     0.4267   -0.0000
   520        0.0000             nan     0.4267   -0.0000
   540        0.0000             nan     0.4267   -0.0000
   560        0.0000             nan     0.4267   -0.0000
   580        0.0000             nan     0.4267   -0.0000
   600        0.0000             nan     0.4267   -0.0000
   620        0.0000             nan     0.4267   -0.0000
   640        0.0000             nan     0.4267   -0.0000
   660        0.0000             nan     0.4267   -0.0000
   680        0.0000             nan     0.4267   -0.0000
   700        0.0000             nan     0.4267   -0.0000
   720        0.0000             nan     0.4267   -0.0000
   740        0.0000             nan     0.4267   -0.0000
   760        0.0000             nan     0.4267   -0.0000
   780        0.0000             nan     0.4267   -0.0000
   800        0.0000             nan     0.4267   -0.0000
   820        0.0000             nan     0.4267   -0.0000
   840        0.0000             nan     0.4267   -0.0000
   860        0.0000             nan     0.4267   -0.0000
   880        0.0000             nan     0.4267   -0.0000
   900        0.0000             nan     0.4267   -0.0000
   920        0.0000             nan     0.4267   -0.0000
   940        0.0000             nan     0.4267   -0.0000
   960        0.0000             nan     0.4267   -0.0000
   980        0.0000             nan     0.4267    0.0000
  1000        0.0000             nan     0.4267   -0.0000
  1020        0.0000             nan     0.4267   -0.0000
  1040        0.0000             nan     0.4267   -0.0000
  1060        0.0000             nan     0.4267   -0.0000
  1080        0.0000             nan     0.4267    0.0000
  1100        0.0000             nan     0.4267   -0.0000
  1120        0.0000             nan     0.4267   -0.0000
  1140        0.0000             nan     0.4267   -0.0000
  1160        0.0000             nan     0.4267    0.0000
  1180        0.0000             nan     0.4267   -0.0000
  1200        0.0000             nan     0.4267   -0.0000
  1220        0.0000             nan     0.4267   -0.0000
  1240        0.0000             nan     0.4267   -0.0000
  1260        0.0000             nan     0.4267   -0.0000
  1280        0.0000             nan     0.4267   -0.0000
  1300        0.0000             nan     0.4267   -0.0000
  1320        0.0000             nan     0.4267   -0.0000
  1340        0.0000             nan     0.4267   -0.0000
  1360        0.0000             nan     0.4267   -0.0000
  1380        0.0000             nan     0.4267   -0.0000
  1400        0.0000             nan     0.4267   -0.0000
  1420        0.0000             nan     0.4267   -0.0000
  1440        0.0000             nan     0.4267   -0.0000
  1460        0.0000             nan     0.4267   -0.0000
  1480        0.0000             nan     0.4267   -0.0000
  1500        0.0000             nan     0.4267   -0.0000
  1520        0.0000             nan     0.4267   -0.0000
  1540        0.0000             nan     0.4267   -0.0000
  1560        0.0000             nan     0.4267   -0.0000
  1580        0.0000             nan     0.4267    0.0000
  1600        0.0000             nan     0.4267   -0.0000
  1620        0.0000             nan     0.4267    0.0000
  1640        0.0000             nan     0.4267   -0.0000
  1660        0.0000             nan     0.4267   -0.0000
  1680        0.0000             nan     0.4267   -0.0000
  1700        0.0000             nan     0.4267   -0.0000
  1720        0.0000             nan     0.4267   -0.0000
  1740        0.0000             nan     0.4267   -0.0000
  1760        0.0000             nan     0.4267   -0.0000
  1780        0.0000             nan     0.4267   -0.0000
  1800        0.0000             nan     0.4267   -0.0000
  1820        0.0000             nan     0.4267   -0.0000
  1840        0.0000             nan     0.4267   -0.0000
  1860        0.0000             nan     0.4267   -0.0000
  1880        0.0000             nan     0.4267   -0.0000
  1900        0.0000             nan     0.4267    0.0000
  1920        0.0000             nan     0.4267   -0.0000
  1940        0.0000             nan     0.4267   -0.0000
  1960        0.0000             nan     0.4267   -0.0000
  1980        0.0000             nan     0.4267   -0.0000
  2000        0.0000             nan     0.4267   -0.0000
  2020        0.0000             nan     0.4267   -0.0000
  2040        0.0000             nan     0.4267   -0.0000
  2060        0.0000             nan     0.4267   -0.0000
  2080        0.0000             nan     0.4267   -0.0000
  2100        0.0000             nan     0.4267   -0.0000
  2120        0.0000             nan     0.4267   -0.0000
  2140        0.0000             nan     0.4267   -0.0000
  2160        0.0000             nan     0.4267   -0.0000
  2180        0.0000             nan     0.4267   -0.0000
  2200        0.0000             nan     0.4267   -0.0000
  2220        0.0000             nan     0.4267   -0.0000
  2240        0.0000             nan     0.4267   -0.0000
  2260        0.0000             nan     0.4267   -0.0000
  2280        0.0000             nan     0.4267   -0.0000
  2300        0.0000             nan     0.4267   -0.0000
  2320        0.0000             nan     0.4267   -0.0000
  2340        0.0000             nan     0.4267   -0.0000
  2360        0.0000             nan     0.4267   -0.0000
  2380        0.0000             nan     0.4267    0.0000
  2400        0.0000             nan     0.4267   -0.0000
  2420        0.0000             nan     0.4267   -0.0000
  2440        0.0000             nan     0.4267   -0.0000
  2460        0.0000             nan     0.4267    0.0000
  2480        0.0000             nan     0.4267   -0.0000
  2500        0.0000             nan     0.4267   -0.0000
  2520        0.0000             nan     0.4267   -0.0000
  2540        0.0000             nan     0.4267   -0.0000
  2560        0.0000             nan     0.4267   -0.0000
  2580        0.0000             nan     0.4267   -0.0000
  2600        0.0000             nan     0.4267    0.0000
  2620        0.0000             nan     0.4267   -0.0000
  2640        0.0000             nan     0.4267   -0.0000
  2660        0.0000             nan     0.4267   -0.0000
  2680        0.0000             nan     0.4267   -0.0000
  2700        0.0000             nan     0.4267   -0.0000
  2720        0.0000             nan     0.4267   -0.0000
  2740        0.0000             nan     0.4267   -0.0000
  2760        0.0000             nan     0.4267   -0.0000
  2780        0.0000             nan     0.4267   -0.0000
  2800        0.0000             nan     0.4267   -0.0000
  2820        0.0000             nan     0.4267   -0.0000
  2840        0.0000             nan     0.4267   -0.0000
  2860        0.0000             nan     0.4267   -0.0000
  2880        0.0000             nan     0.4267   -0.0000
  2900        0.0000             nan     0.4267   -0.0000
  2920        0.0000             nan     0.4267   -0.0000
  2940        0.0000             nan     0.4267   -0.0000
  2960        0.0000             nan     0.4267   -0.0000
  2980        0.0000             nan     0.4267   -0.0000
  3000        0.0000             nan     0.4267   -0.0000
  3020        0.0000             nan     0.4267   -0.0000
  3040        0.0000             nan     0.4267   -0.0000
  3060        0.0000             nan     0.4267   -0.0000
  3080        0.0000             nan     0.4267   -0.0000
  3100        0.0000             nan     0.4267   -0.0000
  3120        0.0000             nan     0.4267   -0.0000
  3140        0.0000             nan     0.4267   -0.0000
  3160        0.0000             nan     0.4267   -0.0000
  3180        0.0000             nan     0.4267   -0.0000
  3200        0.0000             nan     0.4267   -0.0000
  3220        0.0000             nan     0.4267   -0.0000
  3240        0.0000             nan     0.4267   -0.0000
  3260        0.0000             nan     0.4267    0.0000
  3280        0.0000             nan     0.4267   -0.0000
  3300        0.0000             nan     0.4267   -0.0000
  3320        0.0000             nan     0.4267   -0.0000
  3340        0.0000             nan     0.4267   -0.0000
  3360        0.0000             nan     0.4267   -0.0000
  3380        0.0000             nan     0.4267   -0.0000
  3400        0.0000             nan     0.4267   -0.0000
  3420        0.0000             nan     0.4267   -0.0000
  3440        0.0000             nan     0.4267   -0.0000
  3460        0.0000             nan     0.4267   -0.0000
  3480        0.0000             nan     0.4267   -0.0000
  3500        0.0000             nan     0.4267   -0.0000
  3520        0.0000             nan     0.4267   -0.0000
  3540        0.0000             nan     0.4267   -0.0000
  3560        0.0000             nan     0.4267   -0.0000
  3580        0.0000             nan     0.4267   -0.0000
  3600        0.0000             nan     0.4267   -0.0000
  3620        0.0000             nan     0.4267   -0.0000
  3640        0.0000             nan     0.4267   -0.0000
  3660        0.0000             nan     0.4267   -0.0000
  3680        0.0000             nan     0.4267   -0.0000
  3700        0.0000             nan     0.4267   -0.0000
  3720        0.0000             nan     0.4267   -0.0000
  3740        0.0000             nan     0.4267   -0.0000
  3760        0.0000             nan     0.4267   -0.0000
  3780        0.0000             nan     0.4267   -0.0000
  3800        0.0000             nan     0.4267   -0.0000
  3820        0.0000             nan     0.4267   -0.0000
  3840        0.0000             nan     0.4267   -0.0000
  3860        0.0000             nan     0.4267   -0.0000
  3880        0.0000             nan     0.4267   -0.0000
  3900        0.0000             nan     0.4267   -0.0000
  3920        0.0000             nan     0.4267   -0.0000
  3940        0.0000             nan     0.4267   -0.0000
  3960        0.0000             nan     0.4267   -0.0000
  3980        0.0000             nan     0.4267   -0.0000
  4000        0.0000             nan     0.4267   -0.0000
  4020        0.0000             nan     0.4267   -0.0000
  4040        0.0000             nan     0.4267   -0.0000
  4060        0.0000             nan     0.4267   -0.0000
  4080        0.0000             nan     0.4267   -0.0000
  4100        0.0000             nan     0.4267   -0.0000
  4120        0.0000             nan     0.4267   -0.0000
  4140        0.0000             nan     0.4267   -0.0000
  4160        0.0000             nan     0.4267   -0.0000
  4180        0.0000             nan     0.4267    0.0000
  4200        0.0000             nan     0.4267   -0.0000
  4220        0.0000             nan     0.4267   -0.0000
  4240        0.0000             nan     0.4267   -0.0000
  4260        0.0000             nan     0.4267   -0.0000
  4280        0.0000             nan     0.4267   -0.0000
  4300        0.0000             nan     0.4267   -0.0000
  4320        0.0000             nan     0.4267   -0.0000
  4340        0.0000             nan     0.4267   -0.0000
  4345        0.0000             nan     0.4267   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0312             nan     0.5404    0.0453
     2        0.0179             nan     0.5404    0.0146
     3        0.0177             nan     0.5404   -0.0067
     4        0.0111             nan     0.5404    0.0049
     5        0.0102             nan     0.5404    0.0001
     6        0.0089             nan     0.5404   -0.0008
     7        0.0071             nan     0.5404    0.0008
     8        0.0063             nan     0.5404    0.0009
     9        0.0061             nan     0.5404   -0.0005
    10        0.0059             nan     0.5404   -0.0004
    20        0.0046             nan     0.5404   -0.0012
    40        0.0024             nan     0.5404   -0.0004
    60        0.0020             nan     0.5404   -0.0002
    80        0.0014             nan     0.5404   -0.0003
   100        0.0012             nan     0.5404   -0.0003
   120        0.0009             nan     0.5404   -0.0002
   140        0.0008             nan     0.5404   -0.0001
   160        0.0005             nan     0.5404   -0.0000
   180        0.0005             nan     0.5404   -0.0000
   200        0.0004             nan     0.5404   -0.0000
   220        0.0003             nan     0.5404   -0.0001
   240        0.0003             nan     0.5404   -0.0000
   260        0.0002             nan     0.5404   -0.0000
   280        0.0002             nan     0.5404   -0.0000
   300        0.0001             nan     0.5404   -0.0000
   320        0.0001             nan     0.5404   -0.0000
   340        0.0001             nan     0.5404   -0.0000
   360        0.0001             nan     0.5404   -0.0000
   380        0.0001             nan     0.5404   -0.0000
   400        0.0001             nan     0.5404   -0.0000
   420        0.0001             nan     0.5404   -0.0000
   440        0.0001             nan     0.5404   -0.0000
   460        0.0000             nan     0.5404   -0.0000
   480        0.0000             nan     0.5404   -0.0000
   500        0.0000             nan     0.5404   -0.0000
   520        0.0000             nan     0.5404   -0.0000
   540        0.0000             nan     0.5404   -0.0000
   560        0.0000             nan     0.5404   -0.0000
   580        0.0000             nan     0.5404   -0.0000
   600        0.0000             nan     0.5404   -0.0000
   620        0.0000             nan     0.5404   -0.0000
   640        0.0000             nan     0.5404   -0.0000
   660        0.0000             nan     0.5404   -0.0000
   680        0.0000             nan     0.5404   -0.0000
   700        0.0000             nan     0.5404   -0.0000
   720        0.0000             nan     0.5404   -0.0000
   740        0.0000             nan     0.5404   -0.0000
   760        0.0000             nan     0.5404   -0.0000
   780        0.0000             nan     0.5404   -0.0000
   800        0.0000             nan     0.5404   -0.0000
   820        0.0000             nan     0.5404   -0.0000
   840        0.0000             nan     0.5404   -0.0000
   860        0.0000             nan     0.5404   -0.0000
   880        0.0000             nan     0.5404   -0.0000
   900        0.0000             nan     0.5404   -0.0000
   920        0.0000             nan     0.5404   -0.0000
   940        0.0000             nan     0.5404   -0.0000
   960        0.0000             nan     0.5404   -0.0000
   980        0.0000             nan     0.5404   -0.0000
  1000        0.0000             nan     0.5404   -0.0000
  1020        0.0000             nan     0.5404   -0.0000
  1040        0.0000             nan     0.5404   -0.0000
  1060        0.0000             nan     0.5404   -0.0000
  1080        0.0000             nan     0.5404   -0.0000
  1100        0.0000             nan     0.5404   -0.0000
  1120        0.0000             nan     0.5404   -0.0000
  1140        0.0000             nan     0.5404   -0.0000
  1160        0.0000             nan     0.5404    0.0000
  1180        0.0000             nan     0.5404   -0.0000
  1200        0.0000             nan     0.5404   -0.0000
  1220        0.0000             nan     0.5404   -0.0000
  1240        0.0000             nan     0.5404   -0.0000
  1260        0.0000             nan     0.5404   -0.0000
  1280        0.0000             nan     0.5404   -0.0000
  1300        0.0000             nan     0.5404    0.0000
  1320        0.0000             nan     0.5404   -0.0000
  1340        0.0000             nan     0.5404   -0.0000
  1360        0.0000             nan     0.5404   -0.0000
  1380        0.0000             nan     0.5404   -0.0000
  1400        0.0000             nan     0.5404   -0.0000
  1420        0.0000             nan     0.5404   -0.0000
  1440        0.0000             nan     0.5404   -0.0000
  1460        0.0000             nan     0.5404   -0.0000
  1480        0.0000             nan     0.5404   -0.0000
  1500        0.0000             nan     0.5404   -0.0000
  1520        0.0000             nan     0.5404   -0.0000
  1540        0.0000             nan     0.5404   -0.0000
  1560        0.0000             nan     0.5404   -0.0000
  1580        0.0000             nan     0.5404   -0.0000
  1600        0.0000             nan     0.5404   -0.0000
  1620        0.0000             nan     0.5404   -0.0000
  1640        0.0000             nan     0.5404   -0.0000
  1660        0.0000             nan     0.5404   -0.0000
  1680        0.0000             nan     0.5404   -0.0000
  1700        0.0000             nan     0.5404   -0.0000
  1720        0.0000             nan     0.5404   -0.0000
  1740        0.0000             nan     0.5404   -0.0000
  1760        0.0000             nan     0.5404   -0.0000
  1780        0.0000             nan     0.5404   -0.0000
  1800        0.0000             nan     0.5404   -0.0000
  1820        0.0000             nan     0.5404   -0.0000
  1840        0.0000             nan     0.5404   -0.0000
  1860        0.0000             nan     0.5404   -0.0000
  1880        0.0000             nan     0.5404    0.0000
  1900        0.0000             nan     0.5404   -0.0000
  1920        0.0000             nan     0.5404   -0.0000
  1940        0.0000             nan     0.5404   -0.0000
  1960        0.0000             nan     0.5404   -0.0000
  1980        0.0000             nan     0.5404   -0.0000
  2000        0.0000             nan     0.5404   -0.0000
  2020        0.0000             nan     0.5404   -0.0000
  2040        0.0000             nan     0.5404   -0.0000
  2060        0.0000             nan     0.5404   -0.0000
  2080        0.0000             nan     0.5404   -0.0000
  2100        0.0000             nan     0.5404   -0.0000
  2120        0.0000             nan     0.5404   -0.0000
  2140        0.0000             nan     0.5404   -0.0000
  2160        0.0000             nan     0.5404   -0.0000
  2180        0.0000             nan     0.5404   -0.0000
  2200        0.0000             nan     0.5404   -0.0000
  2220        0.0000             nan     0.5404   -0.0000
  2240        0.0000             nan     0.5404    0.0000
  2260        0.0000             nan     0.5404   -0.0000
  2280        0.0000             nan     0.5404   -0.0000
  2300        0.0000             nan     0.5404   -0.0000
  2320        0.0000             nan     0.5404   -0.0000
  2340        0.0000             nan     0.5404    0.0000
  2360        0.0000             nan     0.5404   -0.0000
  2380        0.0000             nan     0.5404   -0.0000
  2400        0.0000             nan     0.5404   -0.0000
  2406        0.0000             nan     0.5404   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0292             nan     0.5955    0.0489
     2        0.0182             nan     0.5955    0.0065
     3        0.0115             nan     0.5955    0.0079
     4        0.0084             nan     0.5955    0.0041
     5        0.0052             nan     0.5955    0.0018
     6        0.0053             nan     0.5955   -0.0009
     7        0.0048             nan     0.5955   -0.0006
     8        0.0039             nan     0.5955    0.0002
     9        0.0041             nan     0.5955   -0.0010
    10        0.0037             nan     0.5955   -0.0004
    20        0.0027             nan     0.5955   -0.0006
    40        0.0018             nan     0.5955   -0.0002
    60        0.0011             nan     0.5955   -0.0002
    80        0.0007             nan     0.5955   -0.0001
   100        0.0005             nan     0.5955   -0.0001
   120        0.0003             nan     0.5955   -0.0001
   140        0.0002             nan     0.5955   -0.0001
   160        0.0002             nan     0.5955   -0.0000
   180        0.0001             nan     0.5955   -0.0000
   200        0.0001             nan     0.5955   -0.0000
   220        0.0001             nan     0.5955   -0.0000
   240        0.0001             nan     0.5955   -0.0000
   260        0.0001             nan     0.5955   -0.0000
   280        0.0000             nan     0.5955   -0.0000
   300        0.0000             nan     0.5955    0.0000
   320        0.0000             nan     0.5955   -0.0000
   340        0.0000             nan     0.5955   -0.0000
   360        0.0000             nan     0.5955   -0.0000
   380        0.0000             nan     0.5955   -0.0000
   400        0.0000             nan     0.5955   -0.0000
   420        0.0000             nan     0.5955   -0.0000
   440        0.0000             nan     0.5955   -0.0000
   460        0.0000             nan     0.5955    0.0000
   480        0.0000             nan     0.5955   -0.0000
   500        0.0000             nan     0.5955   -0.0000
   520        0.0000             nan     0.5955   -0.0000
   540        0.0000             nan     0.5955   -0.0000
   560        0.0000             nan     0.5955   -0.0000
   580        0.0000             nan     0.5955   -0.0000
   600        0.0000             nan     0.5955   -0.0000
   620        0.0000             nan     0.5955   -0.0000
   640        0.0000             nan     0.5955   -0.0000
   660        0.0000             nan     0.5955   -0.0000
   680        0.0000             nan     0.5955   -0.0000
   700        0.0000             nan     0.5955   -0.0000
   720        0.0000             nan     0.5955   -0.0000
   740        0.0000             nan     0.5955   -0.0000
   760        0.0000             nan     0.5955   -0.0000
   780        0.0000             nan     0.5955   -0.0000
   800        0.0000             nan     0.5955   -0.0000
   820        0.0000             nan     0.5955   -0.0000
   840        0.0000             nan     0.5955   -0.0000
   860        0.0000             nan     0.5955   -0.0000
   880        0.0000             nan     0.5955   -0.0000
   900        0.0000             nan     0.5955    0.0000
   920        0.0000             nan     0.5955   -0.0000
   940        0.0000             nan     0.5955   -0.0000
   960        0.0000             nan     0.5955   -0.0000
   980        0.0000             nan     0.5955   -0.0000
  1000        0.0000             nan     0.5955   -0.0000
  1020        0.0000             nan     0.5955   -0.0000
  1040        0.0000             nan     0.5955   -0.0000
  1060        0.0000             nan     0.5955   -0.0000
  1080        0.0000             nan     0.5955   -0.0000
  1100        0.0000             nan     0.5955   -0.0000
  1120        0.0000             nan     0.5955   -0.0000
  1140        0.0000             nan     0.5955   -0.0000
  1160        0.0000             nan     0.5955   -0.0000
  1180        0.0000             nan     0.5955   -0.0000
  1200        0.0000             nan     0.5955   -0.0000
  1220        0.0000             nan     0.5955   -0.0000
  1240        0.0000             nan     0.5955   -0.0000
  1260        0.0000             nan     0.5955   -0.0000
  1280        0.0000             nan     0.5955   -0.0000
  1300        0.0000             nan     0.5955   -0.0000
  1320        0.0000             nan     0.5955   -0.0000
  1340        0.0000             nan     0.5955   -0.0000
  1360        0.0000             nan     0.5955   -0.0000
  1380        0.0000             nan     0.5955   -0.0000
  1400        0.0000             nan     0.5955   -0.0000
  1420        0.0000             nan     0.5955   -0.0000
  1440        0.0000             nan     0.5955   -0.0000
  1460        0.0000             nan     0.5955   -0.0000
  1480        0.0000             nan     0.5955   -0.0000
  1500        0.0000             nan     0.5955   -0.0000
  1520        0.0000             nan     0.5955   -0.0000
  1540        0.0000             nan     0.5955   -0.0000
  1560        0.0000             nan     0.5955   -0.0000
  1580        0.0000             nan     0.5955   -0.0000
  1600        0.0000             nan     0.5955   -0.0000
  1620        0.0000             nan     0.5955   -0.0000
  1640        0.0000             nan     0.5955   -0.0000
  1660        0.0000             nan     0.5955   -0.0000
  1680        0.0000             nan     0.5955   -0.0000
  1700        0.0000             nan     0.5955   -0.0000
  1720        0.0000             nan     0.5955   -0.0000
  1740        0.0000             nan     0.5955   -0.0000
  1760        0.0000             nan     0.5955    0.0000
  1780        0.0000             nan     0.5955   -0.0000
  1800        0.0000             nan     0.5955   -0.0000
  1820        0.0000             nan     0.5955    0.0000
  1840        0.0000             nan     0.5955   -0.0000
  1860        0.0000             nan     0.5955    0.0000
  1880        0.0000             nan     0.5955   -0.0000
  1900        0.0000             nan     0.5955   -0.0000
  1920        0.0000             nan     0.5955   -0.0000
  1940        0.0000             nan     0.5955   -0.0000
  1953        0.0000             nan     0.5955   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0300             nan     0.4186    0.0505
     2        0.0117             nan     0.4186    0.0209
     3        0.0056             nan     0.4186    0.0064
     4        0.0027             nan     0.4186    0.0030
     5        0.0015             nan     0.4186    0.0014
     6        0.0010             nan     0.4186    0.0005
     7        0.0007             nan     0.4186    0.0002
     8        0.0006             nan     0.4186    0.0001
     9        0.0006             nan     0.4186   -0.0000
    10        0.0005             nan     0.4186   -0.0000
    20        0.0002             nan     0.4186   -0.0001
    40        0.0000             nan     0.4186   -0.0000
    60        0.0000             nan     0.4186   -0.0000
    80        0.0000             nan     0.4186   -0.0000
   100        0.0000             nan     0.4186   -0.0000
   120        0.0000             nan     0.4186   -0.0000
   140        0.0000             nan     0.4186   -0.0000
   160        0.0000             nan     0.4186   -0.0000
   180        0.0000             nan     0.4186   -0.0000
   200        0.0000             nan     0.4186   -0.0000
   220        0.0000             nan     0.4186   -0.0000
   240        0.0000             nan     0.4186   -0.0000
   260        0.0000             nan     0.4186   -0.0000
   280        0.0000             nan     0.4186   -0.0000
   300        0.0000             nan     0.4186   -0.0000
   320        0.0000             nan     0.4186   -0.0000
   340        0.0000             nan     0.4186   -0.0000
   360        0.0000             nan     0.4186   -0.0000
   380        0.0000             nan     0.4186   -0.0000
   400        0.0000             nan     0.4186   -0.0000
   420        0.0000             nan     0.4186    0.0000
   440        0.0000             nan     0.4186   -0.0000
   460        0.0000             nan     0.4186    0.0000
   480        0.0000             nan     0.4186   -0.0000
   500        0.0000             nan     0.4186   -0.0000
   520        0.0000             nan     0.4186   -0.0000
   540        0.0000             nan     0.4186    0.0000
   560        0.0000             nan     0.4186   -0.0000
   580        0.0000             nan     0.4186   -0.0000
   600        0.0000             nan     0.4186   -0.0000
   620        0.0000             nan     0.4186   -0.0000
   640        0.0000             nan     0.4186   -0.0000
   660        0.0000             nan     0.4186   -0.0000
   680        0.0000             nan     0.4186    0.0000
   700        0.0000             nan     0.4186   -0.0000
   720        0.0000             nan     0.4186    0.0000
   740        0.0000             nan     0.4186   -0.0000
   760        0.0000             nan     0.4186   -0.0000
   780        0.0000             nan     0.4186   -0.0000
   800        0.0000             nan     0.4186   -0.0000
   820        0.0000             nan     0.4186   -0.0000
   840        0.0000             nan     0.4186   -0.0000
   860        0.0000             nan     0.4186   -0.0000
   880        0.0000             nan     0.4186   -0.0000
   900        0.0000             nan     0.4186   -0.0000
   920        0.0000             nan     0.4186   -0.0000
   940        0.0000             nan     0.4186    0.0000
   960        0.0000             nan     0.4186   -0.0000
   980        0.0000             nan     0.4186   -0.0000
  1000        0.0000             nan     0.4186   -0.0000
  1020        0.0000             nan     0.4186   -0.0000
  1040        0.0000             nan     0.4186   -0.0000
  1060        0.0000             nan     0.4186   -0.0000
  1080        0.0000             nan     0.4186   -0.0000
  1100        0.0000             nan     0.4186   -0.0000
  1120        0.0000             nan     0.4186   -0.0000
  1140        0.0000             nan     0.4186   -0.0000
  1160        0.0000             nan     0.4186   -0.0000
  1180        0.0000             nan     0.4186   -0.0000
  1200        0.0000             nan     0.4186   -0.0000
  1220        0.0000             nan     0.4186   -0.0000
  1240        0.0000             nan     0.4186   -0.0000
  1260        0.0000             nan     0.4186   -0.0000
  1280        0.0000             nan     0.4186   -0.0000
  1300        0.0000             nan     0.4186   -0.0000
  1320        0.0000             nan     0.4186   -0.0000
  1340        0.0000             nan     0.4186   -0.0000
  1360        0.0000             nan     0.4186   -0.0000
  1380        0.0000             nan     0.4186   -0.0000
  1400        0.0000             nan     0.4186   -0.0000
  1420        0.0000             nan     0.4186   -0.0000
  1440        0.0000             nan     0.4186   -0.0000
  1460        0.0000             nan     0.4186   -0.0000
  1480        0.0000             nan     0.4186   -0.0000
  1500        0.0000             nan     0.4186   -0.0000
  1520        0.0000             nan     0.4186   -0.0000
  1540        0.0000             nan     0.4186   -0.0000
  1560        0.0000             nan     0.4186   -0.0000
  1580        0.0000             nan     0.4186   -0.0000
  1585        0.0000             nan     0.4186   -0.0000

Stochastic Gradient Boosting 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  shrinkage   interaction.depth  n.minobsinnode  n.trees  RMSE        Rsquared 
  0.04824458   6                 23              2436            NaN        NaN
  0.10235022   5                 16              3848            NaN        NaN
  0.22447446   7                 20              1793            NaN        NaN
  0.28429214  10                 10              4965     0.11581257  0.8609165
  0.29306083   8                 19              2403            NaN        NaN
  0.31869216   5                 18              4400            NaN        NaN
  0.32463698   7                  8              4672     0.08373128  0.9220396
  0.41126291  10                  7              2384     0.08501159  0.9183038
  0.41860669   4                  5              1585     0.05246576  0.9718029
  0.42666813   5                  5              4345     0.05640354  0.9704213
  0.44325440   8                 18              3771            NaN        NaN
  0.45530253   9                 24              1221            NaN        NaN
  0.45569665   8                 24              2379            NaN        NaN
  0.46539077   6                 22              2831            NaN        NaN
  0.46612565   2                 21               536            NaN        NaN
  0.52344532   7                 18              3024            NaN        NaN
  0.54040709  10                 11              2406     0.13011183  0.8224462
  0.56794541   9                 19              1289            NaN        NaN
  0.58381237   9                 21              2743            NaN        NaN
  0.59545747   7                  9              1953     0.10712537  0.8778141
  MAE         Selected
         NaN          
         NaN          
         NaN          
  0.09603195          
         NaN          
         NaN          
  0.06488195          
  0.06928704          
  0.04286031  *       
  0.04441399          
         NaN          
         NaN          
         NaN          
         NaN          
         NaN          
         NaN          
  0.10569159          
         NaN          
         NaN          
  0.08400255          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were n.trees = 1585, interaction.depth =
 4, shrinkage = 0.4186067 and n.minobsinnode = 5.
[1] "Sat Mar 10 02:40:23 2018"
Error in relative.influence(object, n.trees = numTrees) : 
  could not find function "relative.influence"
In addition: There were 41 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:43:46 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "gbm_h2o"                 
Multivariate Adaptive Regression Splines 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results:

  RMSE         Rsquared  MAE         
  0.001442387  0.999973  0.0005323059

Tuning parameter 'degree' was held constant at a value of 1
[1] "Sat Mar 10 02:44:01 2018"
Generalized Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results:

  RMSE         Rsquared   MAE        
  0.001687096  0.9999678  0.001003681

[1] "Sat Mar 10 02:44:15 2018"
Timing stopped at: 0 0 0
Error : no valid set of coefficients has been found: please supply starting values
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Timing stopped at: 0.02 0 0.02
Error : no valid set of coefficients has been found: please supply starting values
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Timing stopped at: 0.02 0 0.02
Error : no valid set of coefficients has been found: please supply starting values
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:44:35 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "glm.nb"                  
Boosted Generalized Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  mstop  prune  RMSE         Rsquared   MAE           Selected
  108    yes    0.001476964  0.9999729  0.0007164874  *       
  245    no     0.001629452  0.9999694  0.0009414843          
  258    no     0.001635983  0.9999692  0.0009485142          
  317    yes    0.001476964  0.9999729  0.0007164874          
  359    no     0.001666697  0.9999684  0.0009822757          
  391    no     0.001671707  0.9999682  0.0009876606          
  476    no     0.001679977  0.9999680  0.0009962244          
  477    no     0.001680055  0.9999680  0.0009963253          
  481    no     0.001680262  0.9999680  0.0009964914          
  482    no     0.001680326  0.9999680  0.0009966087          
  488    no     0.001680729  0.9999680  0.0009970015          
  549    no     0.001683417  0.9999679  0.0009998762          
  567    no     0.001683966  0.9999679  0.0010004183          
  605    no     0.001684892  0.9999679  0.0010013887          
  755    no     0.001686526  0.9999678  0.0010030958          
  770    yes    0.001476964  0.9999729  0.0007164874          
  870    yes    0.001476964  0.9999729  0.0007164874          
  880    yes    0.001476964  0.9999729  0.0007164874          
  935    no     0.001686982  0.9999678  0.0010035636          
  994    no     0.001687030  0.9999678  0.0010036124          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 108 and prune = yes.
[1] "Sat Mar 10 02:44:52 2018"
glmnet 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  alpha      lambda      RMSE         Rsquared   MAE          Selected
  0.1071961  0.00459126  0.008918999  0.9999535  0.007781701  *       
  0.2441936  1.64004154  0.289776914        NaN  0.251461841          
  0.2578249  1.70305979  0.289776914        NaN  0.251461841          
  0.3169125  0.02391117  0.022746403  0.9999740  0.019746432          
  0.3585754  0.29122086  0.177347287  0.9999740  0.153900279          
  0.3906604  0.51692322  0.247972404  0.9999740  0.215183164          
  0.4758149  1.22349845  0.289776914        NaN  0.251461841          
  0.4767434  3.76981457  0.289776914        NaN  0.251461841          
  0.4805985  0.53809897  0.274207124  0.9999740  0.237946972          
  0.4812563  4.81632154  0.289776914        NaN  0.251461841          
  0.4872827  0.09585734  0.082191488  0.9999740  0.071328145          
  0.5485869  2.06521894  0.289776914        NaN  0.251461841          
  0.5663104  0.15926205  0.128938398  0.9999740  0.111893830          
  0.6049106  0.40801069  0.262661837  0.9999740  0.227928145          
  0.7542918  0.96936330  0.289776914        NaN  0.251461841          
  0.7697237  0.06282035  0.060038903  0.9999740  0.052102303          
  0.8690024  0.03726702  0.036797533  0.9999740  0.031926194          
  0.8799880  0.06866209  0.067000257  0.9999740  0.058144107          
  0.9344600  0.44479164  0.289776914        NaN  0.251461841          
  0.9931335  3.81745747  0.289776914        NaN  0.251461841          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alpha = 0.1071961 and lambda
 = 0.00459126.
[1] "Sat Mar 10 02:45:08 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:48:30 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "glmnet_h2o"              
Start:  AIC=-507.49
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V9    1   0.0001 -509.37
- V4    1   0.0001 -509.36
- V7    1   0.0001 -508.74
- V3    1   0.0001 -508.40
- V5    1   0.0001 -507.95
<none>      0.0001 -507.49
- V10   1   0.0001 -506.28
- V8    1   0.0001 -506.28
- V6    1   0.0001 -505.15
- V2    1   4.0506   35.55

Step:  AIC=-509.37
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V10

       Df Deviance     AIC
- V4    1   0.0001 -511.25
- V7    1   0.0001 -510.71
- V3    1   0.0001 -510.30
- V5    1   0.0001 -509.94
<none>      0.0001 -509.37
- V10   1   0.0001 -508.19
- V8    1   0.0001 -508.06
- V6    1   0.0001 -506.99
- V2    1   4.0757   33.87

Step:  AIC=-511.25
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V10

       Df Deviance     AIC
- V7    1   0.0001 -512.59
- V3    1   0.0001 -511.87
- V5    1   0.0001 -511.83
<none>      0.0001 -511.25
- V10   1   0.0001 -510.18
- V8    1   0.0001 -510.04
- V6    1   0.0001 -508.71
- V2    1   4.1929   33.31

Step:  AIC=-512.59
.outcome ~ V2 + V3 + V5 + V6 + V8 + V10

       Df Deviance     AIC
- V5    1   0.0001 -513.18
- V3    1   0.0001 -512.90
<none>      0.0001 -512.59
- V10   1   0.0001 -511.52
- V8    1   0.0001 -510.55
- V6    1   0.0001 -510.16
- V2    1   4.1929   31.31

Step:  AIC=-513.18
.outcome ~ V2 + V3 + V6 + V8 + V10

       Df Deviance     AIC
- V3    1   0.0001 -513.59
<none>      0.0001 -513.18
- V10   1   0.0001 -512.63
- V6    1   0.0001 -511.33
- V8    1   0.0001 -511.00
- V2    1   4.2456   29.95

Step:  AIC=-513.59
.outcome ~ V2 + V6 + V8 + V10

       Df Deviance     AIC
<none>      0.0001 -513.59
- V10   1   0.0001 -513.34
- V6    1   0.0001 -511.99
- V8    1   0.0001 -511.59
- V2    1   4.3152   28.78
Start:  AIC=-524.58
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V4    1   0.0001 -526.51
- V9    1   0.0001 -526.33
- V8    1   0.0001 -526.20
- V10   1   0.0001 -525.81
- V5    1   0.0001 -525.78
- V7    1   0.0001 -525.32
- V3    1   0.0001 -525.12
<none>      0.0001 -524.58
- V6    1   0.0001 -523.71
- V2    1   3.8500   33.70

Step:  AIC=-526.51
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V9    1   0.0001 -528.28
- V8    1   0.0001 -528.10
- V5    1   0.0001 -527.74
- V10   1   0.0001 -527.70
- V7    1   0.0001 -527.25
- V3    1   0.0001 -527.12
<none>      0.0001 -526.51
- V6    1   0.0001 -525.51
- V2    1   3.9093   32.46

Step:  AIC=-528.28
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V10

       Df Deviance     AIC
- V8    1   0.0001 -529.76
- V5    1   0.0001 -529.55
- V10   1   0.0001 -529.04
- V7    1   0.0001 -529.02
- V3    1   0.0001 -529.02
<none>      0.0001 -528.28
- V6    1   0.0001 -527.33
- V2    1   3.9346   30.78

Step:  AIC=-529.76
.outcome ~ V2 + V3 + V5 + V6 + V7 + V10

       Df Deviance     AIC
- V7    1   0.0001 -530.96
- V5    1   0.0001 -530.93
- V10   1   0.0001 -530.67
- V3    1   0.0001 -530.48
<none>      0.0001 -529.76
- V6    1   0.0001 -529.14
- V2    1   3.9612   29.12

Step:  AIC=-530.96
.outcome ~ V2 + V3 + V5 + V6 + V10

       Df Deviance     AIC
- V10   1   0.0001 -532.11
- V5    1   0.0001 -531.79
- V3    1   0.0001 -531.61
<none>      0.0001 -530.96
- V6    1   0.0001 -530.42
- V2    1   4.0340   28.03

Step:  AIC=-532.11
.outcome ~ V2 + V3 + V5 + V6

       Df Deviance     AIC
- V5    1   0.0001 -533.06
- V3    1   0.0001 -532.38
<none>      0.0001 -532.11
- V6    1   0.0001 -531.82
- V2    1   4.2346   28.46

Step:  AIC=-533.06
.outcome ~ V2 + V3 + V6

       Df Deviance     AIC
- V3    1   0.0001 -533.46
<none>      0.0001 -533.06
- V6    1   0.0001 -532.51
- V2    1   4.3307   27.58

Step:  AIC=-533.46
.outcome ~ V2 + V6

       Df Deviance     AIC
<none>      0.0001 -533.46
- V6    1   0.0001 -532.97
- V2    1   4.3496   25.80
Start:  AIC=-499.34
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V3    1   0.0001 -501.33
- V5    1   0.0001 -501.33
- V7    1   0.0001 -500.58
- V10   1   0.0001 -500.54
- V9    1   0.0001 -499.79
- V8    1   0.0001 -499.57
<none>      0.0001 -499.34
- V4    1   0.0001 -497.34
- V6    1   0.0001 -497.31
- V2    1   3.2928   24.99

Step:  AIC=-501.33
.outcome ~ V2 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V5    1   0.0001 -503.32
- V7    1   0.0001 -502.58
- V10   1   0.0001 -502.54
- V9    1   0.0001 -501.78
- V8    1   0.0001 -501.44
<none>      0.0001 -501.33
- V4    1   0.0001 -499.33
- V6    1   0.0001 -499.27
- V2    1   3.5709   27.12

Step:  AIC=-503.32
.outcome ~ V2 + V4 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V7    1   0.0001 -504.58
- V10   1   0.0001 -504.41
- V9    1   0.0001 -503.78
- V8    1   0.0001 -503.36
<none>      0.0001 -503.32
- V6    1   0.0001 -501.24
- V4    1   0.0001 -501.04
- V2    1   3.6660   26.46

Step:  AIC=-504.58
.outcome ~ V2 + V4 + V6 + V8 + V9 + V10

       Df Deviance     AIC
- V10   1   0.0001 -505.48
- V9    1   0.0001 -504.68
<none>      0.0001 -504.58
- V8    1   0.0001 -503.89
- V4    1   0.0001 -502.72
- V6    1   0.0001 -502.59
- V2    1   3.7055   25.01

Step:  AIC=-505.48
.outcome ~ V2 + V4 + V6 + V8 + V9

       Df Deviance     AIC
- V9    1   0.0001 -506.00
<none>      0.0001 -505.48
- V8    1   0.0001 -505.16
- V6    1   0.0001 -503.93
- V4    1   0.0001 -503.55
- V2    1   3.8705   25.23

Step:  AIC=-506
.outcome ~ V2 + V4 + V6 + V8

       Df Deviance     AIC
<none>      0.0001 -506.00
- V8    1   0.0001 -505.71
- V4    1   0.0001 -504.81
- V6    1   0.0001 -504.08
- V2    1   3.8728   23.26
Start:  AIC=-765.64
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V5    1   0.0001 -767.58
- V9    1   0.0001 -767.05
- V10   1   0.0001 -766.96
- V7    1   0.0001 -766.95
- V4    1   0.0001 -766.95
- V8    1   0.0001 -766.69
- V3    1   0.0001 -766.44
<none>      0.0001 -765.64
- V6    1   0.0001 -762.80
- V2    1   5.7202   39.09

Step:  AIC=-767.58
.outcome ~ V2 + V3 + V4 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V9    1   0.0001 -768.95
- V10   1   0.0001 -768.94
- V7    1   0.0001 -768.89
- V4    1   0.0001 -768.83
- V8    1   0.0001 -768.63
- V3    1   0.0001 -768.38
<none>      0.0001 -767.58
- V6    1   0.0001 -764.80
- V2    1   5.8274   38.50

Step:  AIC=-768.95
.outcome ~ V2 + V3 + V4 + V6 + V7 + V8 + V10

       Df Deviance     AIC
- V10   1   0.0001 -770.49
- V4    1   0.0001 -770.28
- V8    1   0.0001 -770.13
- V7    1   0.0001 -770.09
- V3    1   0.0001 -769.73
<none>      0.0001 -768.95
- V6    1   0.0002 -766.22
- V2    1   5.8673   37.02

Step:  AIC=-770.49
.outcome ~ V2 + V3 + V4 + V6 + V7 + V8

       Df Deviance     AIC
- V4    1   0.0001 -771.74
- V8    1   0.0001 -771.72
- V7    1   0.0001 -771.52
- V3    1   0.0001 -771.39
<none>      0.0001 -770.49
- V6    1   0.0002 -767.74
- V2    1   6.1250   38.28

Step:  AIC=-771.74
.outcome ~ V2 + V3 + V6 + V7 + V8

       Df Deviance     AIC
- V8    1   0.0001 -772.98
- V3    1   0.0001 -772.91
- V7    1   0.0001 -772.87
<none>      0.0001 -771.74
- V6    1   0.0002 -769.10
- V2    1   6.1760   36.91

Step:  AIC=-772.98
.outcome ~ V2 + V3 + V6 + V7

       Df Deviance     AIC
- V3    1   0.0001 -774.04
- V7    1   0.0001 -773.43
<none>      0.0001 -772.98
- V6    1   0.0002 -770.02
- V2    1   6.1869   35.05

Step:  AIC=-774.04
.outcome ~ V2 + V6 + V7

       Df Deviance     AIC
- V7    1   0.0002 -774.25
<none>      0.0001 -774.04
- V6    1   0.0002 -770.99
- V2    1   6.3291   34.78

Step:  AIC=-774.25
.outcome ~ V2 + V6

       Df Deviance     AIC
<none>      0.0002 -774.25
- V6    1   0.0002 -771.26
- V2    1   6.3502   33.03
Generalized Linear Model with Stepwise Feature Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results:

  RMSE       Rsquared   MAE         
  0.0015609  0.9999712  0.0008656243

[1] "Sat Mar 10 02:48:45 2018"
Independent Component Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  n.comp  RMSE         Rsquared    MAE          Selected
  1       0.282380761  0.05766912  0.249403422          
  3       0.210807607  0.46222002  0.173413678          
  4       0.164160528  0.68862076  0.136024544          
  5       0.161027080  0.70385596  0.134795157          
  6       0.157661140  0.70199132  0.124972513          
  7       0.144372251  0.76763654  0.118263061          
  8       0.053105898  0.96304474  0.042181413          
  9       0.001687096  0.99996782  0.001003681  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was n.comp = 9.
[1] "Sat Mar 10 02:49:02 2018"
Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  ncomp  RMSE         Rsquared   MAE          Selected
  1      0.082395168  0.9239946  0.070146270          
  3      0.011799208  0.9983129  0.010151307          
  4      0.003586359  0.9998485  0.002917955          
  5      0.001842350  0.9999614  0.001228023          
  6      0.001663375  0.9999674  0.001010627  *       
  7      0.001686104  0.9999679  0.001001770          
  8      0.001687072  0.9999678  0.001003645          
  9      0.001687096  0.9999678  0.001003681          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 6.
[1] "Sat Mar 10 02:49:16 2018"
Error in MSEP(object) : could not find function "MSEP"
k-Nearest Neighbors 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  kmax  distance   kernel        RMSE       Rsquared   MAE        Selected
   3    0.5153321  inv           0.1857950  0.5910333  0.1418311          
   7    2.4723998  inv           0.1912947  0.5976702  0.1562241          
   7    2.4849529  gaussian      0.1912214  0.5962611  0.1562289          
   8    1.0647296  cos           0.1753824  0.6626636  0.1390205          
   9    1.8969659  epanechnikov  0.1797796  0.6367689  0.1490179          
  10    2.0880051  gaussian      0.1794289  0.6480038  0.1490892          
  12    2.3748490  inv           0.1898898  0.5951389  0.1544805          
  12    2.7494985  cos           0.1820851  0.6219584  0.1492480          
  13    1.5270075  rectangular   0.1779972  0.6847514  0.1466626          
  13    2.1013716  biweight      0.1786519  0.6376397  0.1449644          
  13    2.8310612  gaussian      0.1836611  0.6409906  0.1534180          
  14    2.5491449  gaussian      0.1929067  0.5887433  0.1586087          
  15    1.6960326  inv           0.1830687  0.6986821  0.1557376          
  16    2.0092336  inv           0.1835651  0.6868592  0.1581697          
  19    2.2973329  cos           0.1840669  0.6321886  0.1538867          
  20    1.3863175  triangular    0.1798671  0.6855508  0.1481095          
  22    1.2124715  cos           0.1784507  0.6858412  0.1463199          
  22    1.4159210  triweight     0.1768897  0.6777510  0.1434631          
  24    2.0379696  triweight     0.1752503  0.6885957  0.1466621  *       
  25    2.7536797  biweight      0.1796521  0.6692073  0.1516907          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were kmax = 24, distance = 2.03797
 and kernel = triweight.
[1] "Sat Mar 10 02:49:33 2018"
k-Nearest Neighbors 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  k   RMSE       Rsquared   MAE        Selected
   3  0.1834029  0.6121053  0.1484366          
   7  0.1827993  0.6838981  0.1588559  *       
   8  0.1849045  0.7131445  0.1610169          
   9  0.1849122  0.7564028  0.1633014          
  10  0.1874518  0.7701973  0.1634371          
  12  0.1947718  0.7704744  0.1701253          
  13  0.1940584  0.7958346  0.1692451          
  14  0.1961425  0.7888007  0.1699175          
  15  0.1986252  0.8020150  0.1724733          
  16  0.1998438  0.8228338  0.1728526          
  19  0.2046150  0.8122687  0.1762263          
  20  0.2065787  0.8271623  0.1790727          
  22  0.2092562  0.8282486  0.1808322          
  24  0.2143553  0.8409671  0.1864790          
  25  0.2174837  0.8470937  0.1890573          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 7.
[1] "Sat Mar 10 02:49:48 2018"
Polynomial Kernel Regularized Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  lambda        degree  RMSE        Rsquared   MAE         Selected
  3.435425e-05  1       12.9401956  0.1004313  10.8917023          
  1.663290e-04  3        0.2892520  0.6756418   0.2510215          
  1.945917e-04  3        0.2892520  0.6756420   0.2510215          
  3.842046e-04  2        0.2850554  0.8357589   0.2472036          
  6.206932e-04  2        0.2850548  0.8358862   0.2472027          
  8.980533e-04  3        0.2892520  0.6756471   0.2510215          
  2.393728e-03  3        0.2892521  0.6756580   0.2510215          
  2.419451e-03  3        0.2892521  0.6756582   0.2510215          
  2.529256e-03  3        0.2892521  0.6756590   0.2510215          
  2.548482e-03  3        0.2892521  0.6756591   0.2510215          
  2.731578e-03  2        0.2850495  0.8369893   0.2471949          
  5.532669e-03  3        0.2892521  0.6756808   0.2510215          
  6.785035e-03  2        0.2850398  0.8389535   0.2471806          
  1.058164e-02  3        0.2892521  0.6757174   0.2510216          
  5.908250e-02  3        0.2892523  0.6760680   0.2510218          
  7.056976e-02  2        0.2849516  0.8554794   0.2470472          
  2.213156e-01  2        0.2849076  0.8662628   0.2469688          
  2.511539e-01  2        0.2849075  0.8672668   0.2469660  *       
  4.702188e-01  3        0.2892541  0.6789451   0.2510236          
  9.239907e-01  3        0.2892560  0.6819328   0.2510255          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 0.2511539 and degree = 2.
[1] "Sat Mar 10 02:50:03 2018"

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.6878666279  0.0046786912  0.0162354964  0.0102898133 -0.0009142764 
           V7            V8            V9           V10 
-0.0082026961  0.0003643498 -0.0116267322  0.0276647150 

 Quartiles of Marginal Effects:
 
           V2           V3          V4           V5           V6          V7
25% 0.6219369 -0.031603188 -0.01837820 -0.046527819 -0.035991971 -0.04865591
50% 0.6938754  0.004349065  0.01225708  0.008248801  0.003824643 -0.01899345
75% 0.7475633  0.053851036  0.06007969  0.069031684  0.023946824  0.03821146
              V8          V9         V10
25% -0.049574998 -0.05768208 -0.01557861
50%  0.005182964 -0.01210041  0.02131888
75%  0.041335483  0.03397296  0.07364595

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.9915356648  0.0017252466 -0.0005244277 -0.0007822864 -0.0016494857 
           V7            V8            V9           V10 
-0.0003287901 -0.0020468194 -0.0008663179 -0.0021225957 

 Quartiles of Marginal Effects:
 
           V2           V3            V4           V5           V6
25% 0.9873214 -0.001523456 -0.0037438166 -0.004380680 -0.005980915
50% 0.9923453  0.002320081 -0.0007077463 -0.001087127 -0.001994832
75% 0.9972820  0.005080593  0.0024575773  0.002544396  0.003191763
               V7           V8            V9           V10
25% -4.953618e-03 -0.007177024 -0.0045665169 -0.0056139513
50%  6.086729e-06 -0.001931729 -0.0008317263 -0.0032396547
75%  3.910541e-03  0.002543445  0.0031470410  0.0005647492

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.8721663860 -0.0046000504  0.0145036060  0.0084053740  0.0013350667 
           V7            V8            V9           V10 
-0.0008889521  0.0051671799 -0.0074959083  0.0184448818 

 Quartiles of Marginal Effects:
 
           V2           V3          V4           V5           V6           V7
25% 0.8593891 -0.011962989 0.009175229 -0.000232980 -0.005805173 -0.007742134
50% 0.8751140 -0.003407839 0.013354105  0.008876899  0.002171058 -0.001745107
75% 0.8843825  0.003218021 0.022020553  0.018820534  0.007305180  0.007746885
              V8            V9        V10
25% -0.004141697 -0.0164163783 0.01165415
50%  0.006162106 -0.0072773061 0.01800396
75%  0.012013691 -0.0006554222 0.02493131

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.6134255712 -0.0207720151  0.0370067392  0.0206926320  0.0071992045 
           V7            V8            V9           V10 
-0.0003632903  0.0189339342 -0.0197167749  0.0432629494 

 Quartiles of Marginal Effects:
 
           V2          V3         V4         V5          V6            V7
25% 0.6081589 -0.02305936 0.03440038 0.01738179 0.004478618 -0.0032132345
50% 0.6143775 -0.02059567 0.03734489 0.02105790 0.007419800 -0.0007510884
75% 0.6187300 -0.01718163 0.03985619 0.02540209 0.009871358  0.0028361925
            V8          V9        V10
25% 0.01566301 -0.02345365 0.04015853
50% 0.01939358 -0.01963804 0.04282770
75% 0.02213802 -0.01691764 0.04622708

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 8.486845e-01  1.416041e-02 -7.911878e-03 -3.800025e-05 -2.256987e-03 
           V7            V8            V9           V10 
-7.995569e-03 -4.924774e-03 -7.559336e-03  9.360522e-04 

 Quartiles of Marginal Effects:
 
           V2          V3          V4           V5           V6           V7
25% 0.7765653 -0.03015719 -0.06951088 -0.067897868 -0.038296143 -0.059303213
50% 0.8594638  0.01050904 -0.01202927 -0.004473209  0.007440823 -0.008872809
75% 0.9282668  0.06595191  0.05101972  0.059815925  0.024755037  0.037158465
               V8           V9          V10
25% -0.0616294553 -0.061433932 -0.043497733
50%  0.0007173736 -0.004913317 -0.006988317
75%  0.0425792012  0.042189738  0.059650197

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.9986529299  0.0006782620 -0.0002648039 -0.0009849754 -0.0015723783 
           V7            V8            V9           V10 
 0.0004487698 -0.0015570586 -0.0004732441 -0.0014423811 

 Quartiles of Marginal Effects:
 
           V2            V3            V4            V5            V6
25% 0.9974692 -0.0003358346 -1.206300e-03 -0.0017081178 -0.0029596965
50% 0.9988186  0.0006317639  2.421564e-05 -0.0012828212 -0.0020107860
75% 1.0000683  0.0018036585  6.815742e-04 -0.0003550252 -0.0003761993
               V7            V8            V9           V10
25% -0.0007828924 -0.0030643053 -0.0012805567 -0.0026189012
50%  0.0004861865 -0.0019409982 -0.0004726509 -0.0016004276
75%  0.0014186664 -0.0001723049  0.0002382646 -0.0007063537

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.9397249458  0.0090484807 -0.0020941334  0.0001215197 -0.0021015035 
           V7            V8            V9           V10 
-0.0046486572 -0.0043153302 -0.0031719146 -0.0047621152 

 Quartiles of Marginal Effects:
 
           V2          V3           V4           V5            V6           V7
25% 0.9122019 -0.01052857 -0.025296680 -0.027797939 -0.0254241873 -0.029322230
50% 0.9440655  0.01117103 -0.003859708  0.003914574  0.0004644106 -0.004924769
75% 0.9730516  0.03216240  0.017679330  0.023786421  0.0146037811  0.017564088
              V8           V9          V10
25% -0.032830687 -0.027926898 -0.025753623
50%  0.001238372 -0.002615011 -0.007320671
75%  0.019707175  0.018804086  0.020681475

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.0482699609  0.0022235041  0.0036254232  0.0067657262  0.0009219038 
           V7            V8            V9           V10 
-0.0037677134 -0.0013780230 -0.0065569151  0.0089809499 

 Quartiles of Marginal Effects:
 
             V2            V3           V4           V5           V6
25% 0.008026088 -2.213169e-02 -0.011057491 -0.013254866 -0.021139188
50% 0.027420560 -4.997459e-05  0.001980823  0.005698152  0.006224813
75% 0.080527803  2.301726e-02  0.015711879  0.020021250  0.025410035
              V7           V8           V9          V10
25% -0.019695881 -0.034629466 -0.028757292 -0.007492942
50% -0.003045984 -0.002925545 -0.004080722  0.008089308
75%  0.010732158  0.016741402  0.013089319  0.027155968

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 8.119008e-01  1.127527e-02 -1.338223e-02  7.735985e-05 -2.585295e-03 
           V7            V8            V9           V10 
-7.058932e-03 -3.511777e-03 -9.521874e-03  8.011269e-03 

 Quartiles of Marginal Effects:
 
           V2           V3          V4           V5           V6          V7
25% 0.7185869 -0.046613312 -0.09113380 -0.085460231 -0.041352947 -0.06583377
50% 0.8259602  0.007611211 -0.01762870 -0.001488282  0.008608296 -0.01961463
75% 0.9001922  0.072085345  0.06633104  0.067244362  0.030754226  0.04206779
               V8           V9          V10
25% -0.0738835726 -0.057965929 -0.041652701
50% -0.0003879767 -0.006986273 -0.002075801
75%  0.0552018367  0.052252323  0.081820295

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.9645328577  0.0058453261 -0.0013930888 -0.0001037647 -0.0019386245 
           V7            V8            V9           V10 
-0.0027608878 -0.0032027057 -0.0020165333 -0.0043294645 

 Quartiles of Marginal Effects:
 
           V2           V3           V4            V5           V6           V7
25% 0.9473589 -0.008179057 -0.014793006 -0.0163381973 -0.018269740 -0.018849581
50% 0.9661977  0.006269005 -0.001691130  0.0008920264 -0.001662232 -0.003229485
75% 0.9858469  0.019922178  0.009393781  0.0145072287  0.009303245  0.011513299
               V8           V9          V10
25% -2.195930e-02 -0.017792930 -0.016375356
50%  9.597803e-05 -0.001331608 -0.007037224
75%  1.284740e-02  0.012540233  0.010819238

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.092021921  0.008284804  0.004605668  0.012049135  0.002246168 -0.007573697 
          V8           V9          V10 
-0.001775084 -0.014333156  0.016355375 

 Quartiles of Marginal Effects:
 
            V2           V3           V4           V5          V6           V7
25% 0.01822176 -0.034518530 -0.019092256 -0.025891857 -0.03811476 -0.029137396
50% 0.05603257 -0.001725529  0.003455625  0.003561481  0.01130634 -0.006357786
75% 0.14980328  0.043302641  0.020520443  0.036584923  0.04959655  0.019543598
             V8          V9         V10
25% -0.06319831 -0.05318398 -0.01354349
50% -0.00637347 -0.01024578  0.01827371
75%  0.03027061  0.02556682  0.04513398

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.0446667236  0.0059608287  0.0035024618  0.0091082717  0.0016872554 
           V7            V8            V9           V10 
-0.0065301736 -0.0006407349 -0.0101847254  0.0082167418 

 Quartiles of Marginal Effects:
 
              V2            V3            V4           V5           V6
25% 0.0007448541 -0.0267771652 -0.0088068123 -0.009697108 -0.018321735
50% 0.0216505049  0.0002904498  0.0008114632  0.003536637  0.005131925
75% 0.0751711892  0.0293771990  0.0162001033  0.023585651  0.028558568
              V7           V8           V9          V10
25% -0.027579643 -0.035019251 -0.031658251 -0.010559766
50%  0.001065187 -0.004370835 -0.003001185  0.006528157
75%  0.009357830  0.019446570  0.012162338  0.034867676

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.4365081750  0.0226547471 -0.0003163094  0.0064871287 -0.0001886779 
           V7            V8            V9           V10 
-0.0082317941 -0.0051926751 -0.0230022309  0.0376215551 

 Quartiles of Marginal Effects:
 
           V2            V3           V4           V5          V6          V7
25% 0.2755058 -0.0614358219 -0.095769419 -0.099666677 -0.06916818 -0.10665300
50% 0.4128959 -0.0003899842 -0.007125121 -0.004638976  0.01627612 -0.01888526
75% 0.5623020  0.1084337113  0.076571942  0.090735730  0.06727674  0.06876236
             V8          V9         V10
25% -0.10549008 -0.11386377 -0.03974873
50% -0.01638980 -0.01193307  0.02897587
75%  0.08772323  0.07066214  0.12858991

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.566639322  0.023197807 -0.005579298  0.002813493 -0.001475831 -0.009512317 
          V8           V9          V10 
-0.006217597 -0.020511134  0.032591312 

 Quartiles of Marginal Effects:
 
           V2           V3          V4           V5           V6          V7
25% 0.4139726 -0.059804256 -0.09230803 -0.119152033 -0.059492219 -0.09760487
50% 0.5599901 -0.004634293 -0.01127935 -0.007397375  0.009786125 -0.03031252
75% 0.6844100  0.123201054  0.08629829  0.104225356  0.051470537  0.06936444
             V8          V9        V10
25% -0.10197660 -0.10130618 -0.0450717
50% -0.01183227 -0.01325140  0.0290949
75%  0.08729821  0.06565264  0.1393306

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.4183910947  0.0222574006  0.0003415019  0.0070623341  0.0000308733 
           V7            V8            V9           V10 
-0.0080466989 -0.0050158776 -0.0231344096  0.0377608128 

 Quartiles of Marginal Effects:
 
           V2           V3           V4           V5          V6          V7
25% 0.2587174 -0.065173574 -0.094017710 -0.095945168 -0.06978843 -0.10557224
50% 0.3906201 -0.001978496 -0.004703704 -0.004212564  0.01286545 -0.01578684
75% 0.5456333  0.105978228  0.076830735  0.087942898  0.06903946  0.06752655
             V8          V9         V10
25% -0.10566432 -0.11270897 -0.03842997
50% -0.02632494 -0.01275651  0.03095723
75%  0.08609483  0.06940496  0.12596975

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.8923885395  0.0055840583 -0.0111952629  0.0009746462 -0.0030003948 
           V7            V8            V9           V10 
-0.0045454956 -0.0009238129 -0.0045113443 -0.0002586796 

 Quartiles of Marginal Effects:
 
           V2           V3          V4           V5           V6           V7
25% 0.8255115 -0.035648933 -0.06676767 -0.050195797 -0.026508335 -0.043086321
50% 0.8977325  0.004578449 -0.01116996  0.005103533  0.006556253 -0.001492286
75% 0.9530959  0.040730904  0.03296378  0.045386174  0.026254072  0.036318231
               V8           V9          V10
25% -0.0411709299 -0.042606289 -0.036201555
50% -0.0005477393 -0.001305605 -0.006959356
75%  0.0426860603  0.038649246  0.044963227

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.323373234  0.019041996  0.003312202  0.010024092  0.001192950 -0.007281365 
          V8           V9          V10 
-0.004088017 -0.022807442  0.036250798 

 Quartiles of Marginal Effects:
 
           V2           V3           V4           V5          V6          V7
25% 0.1725594 -0.077094119 -0.067659486 -0.078241954 -0.06731133 -0.08593317
50% 0.2835001  0.006365386  0.006319995  0.002440151  0.01155348 -0.01821117
75% 0.4535434  0.089582732  0.069614508  0.078814982  0.07407286  0.05653006
             V8           V9         V10
25% -0.10048021 -0.118784802 -0.03697694
50% -0.03176691 -0.008786477  0.03702585
75%  0.07558069  0.066244745  0.10802521

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 8.041111e-01  1.266496e-02 -1.266499e-02  1.914422e-05 -2.466556e-03 
           V7            V8            V9           V10 
-7.578362e-03 -4.037782e-03 -1.000341e-02  8.427718e-03 

 Quartiles of Marginal Effects:
 
           V2          V3          V4           V5           V6          V7
25% 0.7096909 -0.04464980 -0.09027375 -0.088066389 -0.045132772 -0.06851294
50% 0.8199930  0.00664112 -0.01754583  0.000211277  0.009394918 -0.02007491
75% 0.8943908  0.07637441  0.06967842  0.069139648  0.031638392  0.04083561
              V8          V9          V10
25% -0.077301657 -0.06292464 -0.041673660
50% -0.001071017 -0.00712833 -0.001454356
75%  0.056665020  0.05271777  0.084496966

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.6990892115 -0.0155359309  0.0312847183  0.0175396121  0.0052888107 
           V7            V8            V9           V10 
-0.0001907336  0.0150169989 -0.0161672850  0.0377392759 

 Quartiles of Marginal Effects:
 
           V2           V3         V4         V5            V6            V7
25% 0.6878540 -0.020969721 0.02579575 0.01024112 -0.0006952123 -0.0063909465
50% 0.7010749 -0.014970711 0.03161569 0.01824763  0.0058294140 -0.0008925386
75% 0.7101641 -0.008328489 0.03742820 0.02727435  0.0107830369  0.0069344578
            V8          V9        V10
25% 0.00758172 -0.02426678 0.03132508
50% 0.01594141 -0.01530719 0.03706679
75% 0.02152105 -0.01031691 0.04380922

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.635503602  0.023445766 -0.003404147  0.001795189 -0.002570657 -0.011683889 
          V8           V9          V10 
-0.007518743 -0.017116009  0.024356971 

 Quartiles of Marginal Effects:
 
           V2          V3          V4           V5           V6          V7
25% 0.4960328 -0.04666145 -0.08392228 -0.115938779 -0.065591491 -0.08847235
50% 0.6286153  0.01482949 -0.01303025 -0.003646738  0.007392027 -0.03218687
75% 0.7547559  0.12299034  0.09134636  0.095790214  0.046298674  0.05830429
             V8          V9         V10
25% -0.09730508 -0.09962150 -0.04297594
50% -0.01487824 -0.01678862  0.01197741
75%  0.07955118  0.06291549  0.11240292

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.6656666854 -0.0006870412  0.0136556005  0.0103705593  0.0097050524 
           V7            V8            V9           V10 
 0.0193577781  0.0047388506 -0.0137345779  0.0458427882 

 Quartiles of Marginal Effects:
 
           V2           V3           V4            V5          V6          V7
25% 0.5570998 -0.038339928 -0.022030455 -0.0444591680 -0.03654421 -0.02969502
50% 0.6967051  0.005633133  0.003521795  0.0006009643  0.01219651  0.02724103
75% 0.7616506  0.046870146  0.052305109  0.0638983441  0.05230609  0.05924065
              V8          V9         V10
25% -0.037802455 -0.05782347 0.002935675
50%  0.004818155 -0.02130222 0.061108750
75%  0.049814124  0.03800114 0.085644231

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.9879235877  0.0016926082  0.0001826491  0.0005144665 -0.0022949542 
           V7            V8            V9           V10 
 0.0027907257  0.0029744498 -0.0003098413  0.0024142015 

 Quartiles of Marginal Effects:
 
           V2            V3            V4            V5           V6
25% 0.9804708 -0.0009710318 -0.0028804555 -0.0033371521 -0.005276602
50% 0.9900424  0.0019353651 -0.0006992894 -0.0003018933 -0.002771633
75% 0.9950545  0.0052165697  0.0020508446  0.0039880709  0.001008896
              V7           V8            V9           V10
25% -0.001122336 0.0001655807 -0.0042046098 -0.0004786404
50%  0.003433225 0.0026460437 -0.0002306388  0.0023810216
75%  0.006700926 0.0060260282  0.0032700602  0.0048915457

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.862460650 -0.006067553  0.012614961  0.012742238  0.010581654  0.011831656 
          V8           V9          V10 
-0.005608434 -0.008269885  0.029985022 

 Quartiles of Marginal Effects:
 
           V2           V3          V4          V5         V6          V7
25% 0.8424169 -0.012551242 0.006626894 0.003135813 0.00330179 0.003324981
50% 0.8687229 -0.004770928 0.010608042 0.011381880 0.01172935 0.012968162
75% 0.8787661  0.001747245 0.018590051 0.022417744 0.01707427 0.018899080
              V8            V9        V10
25% -0.012875405 -0.0153693254 0.02282640
50% -0.005321126 -0.0078979565 0.03346090
75%  0.001719185 -0.0004218277 0.03623526

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.60243062 -0.01715308  0.03110078  0.03430612  0.02723137  0.02565208 
         V8          V9         V10 
-0.02261188 -0.01252192  0.05853755 

 Quartiles of Marginal Effects:
 
           V2          V3         V4         V5         V6         V7
25% 0.5942433 -0.02022070 0.02849128 0.03031216 0.02443568 0.02225902
50% 0.6052445 -0.01672490 0.03044331 0.03408443 0.02798487 0.02638620
75% 0.6093748 -0.01378547 0.03356612 0.03870420 0.03023903 0.02878916
             V8           V9        V10
25% -0.02558679 -0.014968002 0.05525506
50% -0.02299981 -0.012114159 0.05982346
75% -0.01976383 -0.009286229 0.06172675

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.830868125  0.011281883 -0.014582987 -0.011221414 -0.023512830  0.031893552 
          V8           V9          V10 
 0.034487888 -0.001487117  0.020736693 

 Quartiles of Marginal Effects:
 
           V2           V3          V4          V5          V6          V7
25% 0.7279225 -0.030236028 -0.07147306 -0.05487555 -0.07218586 -0.01976098
50% 0.8551668  0.008737156 -0.01764856 -0.01591133 -0.02568965  0.02927059
75% 0.9418538  0.067731053  0.02590476  0.03656379  0.02436660  0.09488785
             V8           V9          V10
25% 0.001654046 -0.060375522 -0.004083034
50% 0.036568316 -0.004477609  0.022638760
75% 0.084145687  0.056903579  0.055926551

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.9972436382  0.0009497159  0.0004032490  0.0009812858 -0.0013097534 
           V7            V8            V9           V10 
 0.0011297393  0.0009261488  0.0002999494  0.0011794871 

 Quartiles of Marginal Effects:
 
           V2           V3            V4           V5            V6          V7
25% 0.9958974 0.0005535181 -0.0003145734 0.0002517728 -0.0025808733 0.000127523
50% 0.9974878 0.0009979931  0.0003652633 0.0007880111 -0.0013951891 0.001236917
75% 0.9987428 0.0015355197  0.0010802301 0.0018910584 -0.0002878794 0.002221122
              V8            V9          V10
25% 0.0004113925 -0.0002609704 0.0001536399
50% 0.0008805490  0.0005309348 0.0010084494
75% 0.0015797579  0.0009412201 0.0021465501

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.926427634  0.006389901 -0.004128835 -0.004057840 -0.010234827  0.014724371 
          V8           V9          V10 
 0.016051384 -0.003164530  0.009261033 

 Quartiles of Marginal Effects:
 
           V2          V3           V4           V5          V6           V7
25% 0.8843331 -0.01417376 -0.026894248 -0.025052818 -0.03231995 -0.006845468
50% 0.9429415  0.00635125 -0.009660379 -0.007130719 -0.01200453  0.014150050
75% 0.9689482  0.03029969  0.009880062  0.016048390  0.01036404  0.041837579
              V8           V9          V10
25% -0.001133906 -0.030116496 -0.004481530
50%  0.015581624 -0.002823747  0.008220329
75%  0.037495061  0.025859073  0.029520214

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.0510761229  0.0001312272  0.0004241415  0.0077576046  0.0029343982 
           V7            V8            V9           V10 
 0.0037853979  0.0038706025 -0.0043223915  0.0082428583 

 Quartiles of Marginal Effects:
 
             V2           V3           V4            V5           V6
25% 0.002795152 -0.019351684 -0.008969816 -0.0140003196 -0.013697597
50% 0.043313375  0.003006019  0.003036821  0.0007261276  0.006764003
75% 0.080305149  0.021783178  0.020247299  0.0223370252  0.020996376
              V7           V8           V9          V10
25% -0.009960437 -0.017646983 -0.033076560 -0.012243046
50%  0.005690971 -0.001948911  0.000739862  0.006530396
75%  0.027542230  0.020438317  0.014722592  0.039607277

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.798642787  0.011019793 -0.021404340 -0.015094240 -0.030049922  0.037606464 
          V8           V9          V10 
 0.042072800  0.002723272  0.024778879 

 Quartiles of Marginal Effects:
 
           V2           V3          V4          V5          V6          V7
25% 0.6732939 -0.036948120 -0.09431873 -0.07248292 -0.08967560 -0.01671216
50% 0.8313530  0.006045954 -0.01848522 -0.02199681 -0.02679661  0.03446746
75% 0.9409438  0.079289927  0.03240451  0.04126679  0.02425462  0.10962472
            V8           V9          V10
25% 0.00611131 -0.063424859 -0.006261449
50% 0.04856969  0.000138702  0.026248141
75% 0.10377317  0.072644478  0.068103722

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.955208439  0.004413263 -0.002163815 -0.002349497 -0.007013980  0.009825763 
          V8           V9          V10 
 0.010614904 -0.002001900  0.005675980 

 Quartiles of Marginal Effects:
 
           V2           V3           V4           V5           V6           V7
25% 0.9282757 -0.008130252 -0.015934254 -0.016202709 -0.020522642 -0.003278055
50% 0.9651600  0.004534954 -0.005663535 -0.004713634 -0.008961602  0.009727059
75% 0.9810195  0.019425962  0.006669899  0.010437980  0.006331501  0.027849683
              V8           V9          V10
25% 0.0009879462 -0.016935730 -0.002345501
50% 0.0100682023 -0.002173229  0.005189340
75% 0.0236931082  0.015860146  0.018049662

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 9.910259e-02  3.122560e-03  4.592026e-05  1.465401e-02  3.887643e-03 
           V7            V8            V9           V10 
 6.958162e-03  9.996421e-03 -9.446510e-03  1.558067e-02 

 Quartiles of Marginal Effects:
 
            V2           V3           V4           V5          V6          V7
25% 0.01099729 -0.031242616 -0.018167037 -0.028573281 -0.02413350 -0.01618064
50% 0.08132640  0.002452676  0.004831458 -0.001231243  0.01096759  0.01176067
75% 0.15693801  0.040316570  0.037465214  0.043747562  0.03711770  0.04976932
              V8            V9         V10
25% -0.026574439 -6.159736e-02 -0.02026086
50% -0.001120424  9.737441e-05  0.01142912
75%  0.038381401  2.771305e-02  0.07779861

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.0503872591  0.0038284623 -0.0002115168  0.0104395709  0.0016719356 
           V7            V8            V9           V10 
 0.0030359587  0.0070617006 -0.0072354267  0.0095455479 

 Quartiles of Marginal Effects:
 
             V2           V3           V4            V5          V6
25% 0.001072282 -0.016233321 -0.017977374 -0.0176910148 -0.01569583
50% 0.031627243  0.001383702  0.002102536 -0.0002179533  0.00476320
75% 0.081182155  0.033436869  0.023848856  0.0382001460  0.02699959
              V7           V8          V9          V10
25% -0.011862932 -0.018098288 -0.03836455 -0.014457811
50%  0.003768461  0.001494319  0.00224597  0.002163095
75%  0.028445634  0.017017131  0.01788373  0.050468202

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.436780590  0.010922042 -0.004296919  0.005997716 -0.005275245  0.031495273 
          V8           V9          V10 
 0.035040258 -0.010692806  0.042954093 

 Quartiles of Marginal Effects:
 
           V2           V3          V4           V5           V6          V7
25% 0.2298836 -0.068255737 -0.08603927 -0.105299513 -0.079655618 -0.03627350
50% 0.4339695  0.004154176 -0.01549905 -0.003095007  0.006421641  0.03504466
75% 0.6145734  0.100360103  0.10986060  0.080219460  0.082834260  0.10665908
             V8         V9         V10
25% -0.04369504 -0.1068342 -0.04138252
50%  0.02408432 -0.0272872  0.04269532
75%  0.10935757  0.1046066  0.12549744

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.559478051  0.014255687 -0.010312741 -0.002834574 -0.016228187  0.037535339 
          V8           V9          V10 
 0.043827516 -0.006926418  0.044039498 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5           V6          V7
25% 0.3513211 -0.07069701 -0.12623876 -0.08897139 -0.090520312 -0.04272108
50% 0.5565455  0.01087777 -0.01936332 -0.02191629 -0.004989305  0.04189413
75% 0.7552739  0.10386429  0.09286983  0.05271251  0.073851830  0.11986821
             V8           V9         V10
25% -0.03786607 -0.106112485 -0.03627191
50%  0.03341659 -0.009364555  0.04485541
75%  0.12580718  0.104627051  0.12013896

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.419556867  0.010306544 -0.003605466  0.007188378 -0.003894586  0.030484270 
          V8           V9          V10 
 0.033608358 -0.011068294  0.042257592 

 Quartiles of Marginal Effects:
 
           V2           V3          V4           V5          V6          V7
25% 0.2112242 -0.067161445 -0.08017950 -0.105660995 -0.08008761 -0.03709976
50% 0.4171996  0.002863062 -0.01415483 -0.001939818  0.00838799  0.03231255
75% 0.5995264  0.098303381  0.11188880  0.085377650  0.08573447  0.10396543
             V8          V9         V10
25% -0.04459615 -0.10508847 -0.04113303
50%  0.02321256 -0.02861246  0.04095915
75%  0.11112860  0.10208779  0.12737079

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.883446669  0.007340071 -0.019381754 -0.013825743 -0.026277850  0.029435532 
          V8           V9          V10 
 0.029847901  0.004170195  0.013857989 

 Quartiles of Marginal Effects:
 
           V2           V3          V4          V5           V6          V7
25% 0.8107894 -0.025116950 -0.05739590 -0.05022895 -0.063250372 -0.01291332
50% 0.9045447  0.008596807 -0.02504972 -0.02239119 -0.030668648  0.03122289
75% 0.9719808  0.050299808  0.01930145  0.02893683  0.006379762  0.08503010
             V8           V9         V10
25% 0.006294275 -0.047481384 -0.01304464
50% 0.029471063  0.001951088  0.02264996
75% 0.074963343  0.051603707  0.04200046

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.3282167150  0.0068357560 -0.0007429791  0.0127481874  0.0021586036 
           V7            V8            V9           V10 
 0.0246046225  0.0257646120 -0.0122783991  0.0368161979 

 Quartiles of Marginal Effects:
 
           V2           V3           V4           V5          V6          V7
25% 0.1208265 -0.059651104 -0.081554607 -0.095915276 -0.06534497 -0.03319927
50% 0.3190065  0.006046416 -0.008547605  0.007635542  0.02428591  0.02782555
75% 0.5117880  0.082532979  0.093385598  0.097990508  0.08334638  0.09243948
             V8          V9         V10
25% -0.04786293 -0.11115384 -0.03348148
50%  0.02095521 -0.03051018  0.04707551
75%  0.08387793  0.08600932  0.11339787

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.789755928  0.011632344 -0.020441198 -0.014473048 -0.029359522  0.037778409 
          V8           V9          V10 
 0.042387853  0.001785183  0.025878434 

 Quartiles of Marginal Effects:
 
           V2           V3          V4          V5          V6          V7
25% 0.6592778 -0.037158039 -0.09369943 -0.07154116 -0.09183728 -0.01627333
50% 0.8227741  0.005702504 -0.01578656 -0.02046136 -0.02672392  0.03368700
75% 0.9356425  0.082691732  0.03384993  0.04104556  0.02700225  0.11406463
             V8           V9          V10
25% 0.004295692 -0.065557333 -0.004503134
50% 0.047656632 -0.003184223  0.027861668
75% 0.104848615  0.071046556  0.070897621

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.68698526 -0.01446267  0.02670257  0.02798001  0.02348800  0.02197787 
         V8          V9         V10 
-0.01730667 -0.01246385  0.05260754 

 Quartiles of Marginal Effects:
 
           V2           V3         V4         V5         V6         V7
25% 0.6693918 -0.020583409 0.02131608 0.02014523 0.01738765 0.01458807
50% 0.6929617 -0.013452638 0.02502865 0.02727570 0.02510081 0.02346356
75% 0.7015615 -0.007458824 0.03166042 0.03722316 0.02965824 0.02853768
             V8           V9        V10
25% -0.02354108 -0.018051456 0.04578414
50% -0.01764777 -0.011759218 0.05536422
75% -0.01086872 -0.005631261 0.05898748

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.618355608  0.014509112 -0.009066807 -0.003756548 -0.016270597  0.035809269 
          V8           V9          V10 
 0.039672424 -0.007719444  0.041996482 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6          V7
25% 0.4340612 -0.06338466 -0.10694801 -0.08020483 -0.08944415 -0.04072293
50% 0.6235345  0.01165399 -0.01466557 -0.02400730 -0.01207741  0.04038317
75% 0.7926793  0.09237951  0.08001629  0.05565974  0.06510037  0.11931052
             V8          V9         V10
25% -0.02839633 -0.09681827 -0.02229708
50%  0.03438090 -0.01629698  0.04595358
75%  0.11764322  0.08789592  0.10675779

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.706858963 -0.029754222  0.015346373  0.018071736 -0.013699323  0.008456678 
          V8           V9          V10 
-0.011927412 -0.001556060  0.011179648 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6           V7
25% 0.6109771 -0.07652951 -0.01777537 -0.03635887 -0.05040339 -0.039544800
50% 0.7143708 -0.02848120  0.01981157  0.02850593 -0.01019811  0.001242117
75% 0.7969616  0.02339611  0.05328643  0.06686808  0.02019917  0.055287546
             V8           V9         V10
25% -0.04776408 -0.031535428 -0.04513644
50% -0.01277632 -0.004840023  0.01541013
75%  0.01587114  0.040276105  0.04741047

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.9944088718 -0.0004830872  0.0013221087 -0.0011775130 -0.0033294152 
           V7            V8            V9           V10 
 0.0009869096 -0.0022526018  0.0013202577 -0.0038614838 

 Quartiles of Marginal Effects:
 
           V2           V3           V4            V5            V6
25% 0.9873737 -0.004412219 -0.001441357 -0.0046528487 -0.0066690109
50% 0.9955257 -0.001118681  0.001057847 -0.0004734974 -0.0034931788
75% 1.0010129  0.003469890  0.004769836  0.0034093216 -0.0002683539
               V7            V8           V9          V10
25% -0.0024563741 -0.0053951758 -0.001883633 -0.008072789
50%  0.0009640851 -0.0024349484  0.001746604 -0.004226659
75%  0.0044629327 -0.0003050695  0.005108997 -0.001451980

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.8710780430 -0.0276908121  0.0105199002  0.0154097967 -0.0014299760 
           V7            V8            V9           V10 
 0.0116828079 -0.0089439071 -0.0006218942  0.0172576002 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5            V6         V7
25% 0.8550390 -0.03580633 0.004933427 0.006797811 -0.0080524143 0.00282437
50% 0.8723618 -0.02847897 0.011386341 0.017313963 -0.0006408429 0.01028872
75% 0.8862858 -0.01867260 0.017211631 0.024188061  0.0037024989 0.01928415
              V8           V9         V10
25% -0.016255724 -0.004935655 0.008887384
50% -0.009232072 -0.001420671 0.017703000
75% -0.004002804  0.005823955 0.024426881

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.605837317 -0.067362537  0.016350316  0.030085499  0.009585930  0.026823615 
          V8           V9          V10 
-0.017140452 -0.005514921  0.047592022 

 Quartiles of Marginal Effects:
 
           V2          V3         V4         V5          V6         V7
25% 0.5998031 -0.07110884 0.01380624 0.02594104 0.006812944 0.02302126
50% 0.6069310 -0.06799066 0.01677836 0.03102387 0.010089370 0.02618638
75% 0.6124556 -0.06396463 0.01898094 0.03380574 0.011911921 0.02997218
             V8           V9        V10
25% -0.02018124 -0.007256246 0.04407952
50% -0.01750826 -0.005763875 0.04835888
75% -0.01510482 -0.002487872 0.05066328

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.890734076  0.006663461 -0.002275494 -0.008686174 -0.015213290 -0.008797871 
          V8           V9          V10 
-0.008664391  0.004897526 -0.019668840 

 Quartiles of Marginal Effects:
 
           V2            V3           V4          V5          V6           V7
25% 0.7811678 -0.0463433624 -0.053624820 -0.06809277 -0.05108421 -0.052697877
50% 0.9119472  0.0007228426  0.002907778 -0.01114550 -0.01176671 -0.004447061
75% 0.9781593  0.0605230586  0.039322864  0.05455043  0.01923401  0.045315833
             V8          V9         V10
25% -0.06283752 -0.04668365 -0.07538865
50% -0.01315755  0.01695877 -0.01562587
75%  0.03099463  0.03743934  0.01009666

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.9988516502 -0.0005954392  0.0017284143 -0.0003099513 -0.0018172121 
           V7            V8            V9           V10 
 0.0014893751 -0.0017372326  0.0008132289 -0.0013315742 

 Quartiles of Marginal Effects:
 
           V2            V3           V4            V5           V6
25% 0.9971657 -0.0016370065 0.0006663054 -0.0011256450 -0.002809602
50% 0.9990312 -0.0004033096 0.0017917005 -0.0002039504 -0.001978794
75% 1.0005450  0.0005181878 0.0027924729  0.0005922816 -0.001088536
              V7            V8            V9           V10
25% 0.0004449605 -0.0029917213 -0.0001366429 -0.0022382904
50% 0.0015411357 -0.0018256096  0.0007189696 -0.0011867453
75% 0.0024064008 -0.0004736311  0.0021122992 -0.0007471496

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.960347018  0.001890818 -0.001610437 -0.005228089 -0.010526325 -0.004029643 
          V8           V9          V10 
-0.005095910  0.003988942 -0.015274171 

 Quartiles of Marginal Effects:
 
           V2           V3            V4           V5           V6           V7
25% 0.9156386 -0.018444294 -0.0224866977 -0.030893073 -0.025228872 -0.024384432
50% 0.9726964 -0.003308235 -0.0005208402 -0.006344768 -0.012249724 -0.001200285
75% 0.9964572  0.023913684  0.0164031207  0.024160155  0.004526906  0.023165775
             V8          V9          V10
25% -0.02836980 -0.01627787 -0.039932191
50% -0.00463947  0.00565712 -0.015068842
75%  0.01220644  0.02250734 -0.003390945

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.056536047 -0.004904767 -0.002330139  0.003447798 -0.001634708  0.002330558 
          V8           V9          V10 
-0.002097837 -0.001745704  0.004276075 

 Quartiles of Marginal Effects:
 
             V2          V3           V4            V5           V6
25% 0.009593648 -0.02277454 -0.014946306 -9.019313e-03 -0.018557102
50% 0.024033499 -0.00148137  0.001908571 -1.272047e-05  0.001146406
75% 0.072419640  0.01622178  0.012014387  1.567742e-02  0.018768374
              V7           V8           V9          V10
25% -0.007642679 -0.014016402 -0.014008736 -0.012211702
50%  0.007515997  0.003420018 -0.002265774  0.003776068
75%  0.019146897  0.013398945  0.013820029  0.026681265

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.859408660  0.009352750 -0.001583663 -0.009987243 -0.013323918 -0.010749969 
          V8           V9          V10 
-0.009470866  0.003488453 -0.015392940 

 Quartiles of Marginal Effects:
 
           V2            V3           V4          V5            V6           V7
25% 0.7186857 -0.0544705589 -0.060355640 -0.07699214 -0.0595825361 -0.067778477
50% 0.8714704  0.0008986137  0.003273239 -0.01452285 -0.0000815971 -0.005757107
75% 0.9690485  0.0685908983  0.053050435  0.06096514  0.0291822950  0.049630615
              V8          V9         V10
25% -0.073489382 -0.05694679 -0.07986442
50% -0.009248001  0.01562049 -0.01534456
75%  0.031081389  0.04809515  0.02314403

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.9777292589  0.0008815007 -0.0008814483 -0.0040093213 -0.0073545947 
           V7            V8            V9           V10 
-0.0020392931 -0.0039384357  0.0031316452 -0.0111868258 

 Quartiles of Marginal Effects:
 
           V2           V3           V4           V5           V6            V7
25% 0.9506864 -0.011210974 -0.012774265 -0.020226627 -0.016659220 -0.0139078157
50% 0.9838799 -0.001688105 -0.001147932 -0.004435949 -0.008607403  0.0004500221
75% 1.0002784  0.014244105  0.009333859  0.013864832  0.002924346  0.0142370706
              V8           V9          V10
25% -0.017689765 -0.009301923 -0.027956927
50% -0.004523102  0.003807205 -0.012119660
75%  0.006223883  0.016107112 -0.003123032

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.1095711473 -0.0043998690 -0.0044840366  0.0085327126 -0.0068425083 
           V7            V8            V9           V10 
 0.0026841044 -0.0049416296 -0.0006954979  0.0039285363 

 Quartiles of Marginal Effects:
 
            V2           V3           V4           V5          V6          V7
25% 0.01809462 -0.038841291 -0.029543750 -0.016826163 -0.03656503 -0.01576619
50% 0.05474882 -0.001890429  0.003950698  0.002767237 -0.00331213  0.01384123
75% 0.14175747  0.034155741  0.019474679  0.025683744  0.03068510  0.03407206
              V8           V9          V10
25% -0.028727675 -0.028150450 -0.022487527
50%  0.005688576 -0.001234819  0.005713153
75%  0.019161668  0.023891359  0.045248683

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.0551242350 -0.0021810240 -0.0051340739  0.0050985204 -0.0028389691 
           V7            V8            V9           V10 
 0.0016418307 -0.0024537414 -0.0004047038  0.0016331563 

 Quartiles of Marginal Effects:
 
             V2           V3          V4           V5           V6           V7
25% 0.004238158 -0.023068986 -0.01201191 -0.012292870 -0.016875101 -0.005088529
50% 0.016392878 -0.000929414  0.00102100 -0.001997984 -0.001622146  0.006971789
75% 0.066180954  0.016593755  0.01294355  0.011255340  0.019998091  0.016865937
              V8            V9          V10
25% -0.009621883 -0.0171989802 -0.008058588
50%  0.002359062 -0.0004803137  0.005209197
75%  0.014266827  0.0135629492  0.021181179

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.4930523157 -0.0008618229  0.0115127606  0.0123583981 -0.0269725978 
           V7            V8            V9           V10 
 0.0016813109 -0.0169772541  0.0021986840 -0.0011210854 

 Quartiles of Marginal Effects:
 
           V2           V3          V4          V5          V6          V7
25% 0.2730516 -0.100821426 -0.05996080 -0.05015539 -0.09484545 -0.08290462
50% 0.4269298  0.005442807  0.02103691  0.01004338 -0.01741765  0.01412486
75% 0.6509262  0.089358686  0.09541365  0.10393830  0.03955500  0.09339378
             V8           V9          V10
25% -0.07982737 -0.074754023 -0.074044749
50% -0.01641560 -0.002353477  0.004797572
75%  0.04157948  0.087688560  0.074088974

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.628520790  0.004525446  0.011408143  0.004965051 -0.027521109 -0.002840757 
          V8           V9          V10 
-0.016881024  0.003477201 -0.008571493 

 Quartiles of Marginal Effects:
 
           V2           V3          V4           V5          V6          V7
25% 0.4087216 -0.080815377 -0.05733034 -0.075189141 -0.10382566 -0.09046476
50% 0.5911550 -0.008201937  0.02457686 -0.001113532 -0.01732342  0.01168298
75% 0.8029818  0.098986788  0.09969188  0.106032322  0.03380895  0.10079977
             V8           V9          V10
25% -0.07167136 -0.086653434 -0.089721457
50% -0.02416544 -0.008758783 -0.002164657
75%  0.03801928  0.093115975  0.068296418

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.4736246694 -0.0016088792  0.0111243193  0.0130651722 -0.0265110618 
           V7            V8            V9           V10 
 0.0021522928 -0.0167726894  0.0019541059 -0.0001649691 

 Quartiles of Marginal Effects:
 
           V2            V3          V4          V5          V6          V7
25% 0.2565755 -0.1014182969 -0.06162395 -0.04924976 -0.09462476 -0.08236941
50% 0.4076752  0.0003884841  0.01737341  0.01297782 -0.01870650  0.01431588
75% 0.6267950  0.0887611418  0.09279089  0.10172726  0.03956471  0.09167929
             V8            V9          V10
25% -0.08209155 -0.0706347411 -0.071549730
50% -0.01715570 -0.0009365619  0.002525575
75%  0.04070663  0.0848183193  0.078009415

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.925798590  0.007336471 -0.005544346 -0.009769277 -0.005908889 -0.008840692 
          V8           V9          V10 
-0.006244840  0.003116626 -0.013042089 

 Quartiles of Marginal Effects:
 
           V2            V3           V4           V5           V6           V7
25% 0.8480592 -0.0290012537 -0.047387859 -0.055079318 -0.034317425 -0.041932594
50% 0.9446986  0.0006378195 -0.007721892 -0.009218956 -0.003330939 -0.006709891
75% 0.9893469  0.0479096305  0.034363137  0.041352723  0.027332915  0.030149727
              V8          V9          V10
25% -0.051384960 -0.03298970 -0.054489453
50% -0.008143826  0.00751071 -0.012287207
75%  0.023536415  0.03230979  0.007673858

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.369730179 -0.005140608  0.007780488  0.015128512 -0.022640178  0.003857004 
          V8           V9          V10 
-0.014833504  0.000522731  0.004061201 

 Quartiles of Marginal Effects:
 
           V2            V3          V4          V5          V6          V7
25% 0.1771539 -0.0972901691 -0.06053269 -0.05204179 -0.08943836 -0.06683598
50% 0.2812896 -0.0003885939  0.01064120  0.02437221 -0.01438869  0.01093581
75% 0.4904703  0.0883037397  0.06796102  0.08604465  0.04915250  0.08825734
             V8           V9           V10
25% -0.07909464 -0.059179482 -0.0537920745
50% -0.00968878 -0.004485221 -0.0006725007
75%  0.03997900  0.069223668  0.0807119163

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.8527868441  0.0091569760 -0.0009285442 -0.0095511185 -0.0147521049 
           V7            V8            V9           V10 
-0.0104903171 -0.0099211127  0.0037726949 -0.0162360074 

 Quartiles of Marginal Effects:
 
           V2           V3           V4          V5           V6           V7
25% 0.7088592 -0.055557264 -0.060946378 -0.07868099 -0.063112425 -0.068663284
50% 0.8629244 -0.001207884  0.003951882 -0.01175015 -0.004913845 -0.005657368
75% 0.9645816  0.070485702  0.054099670  0.06344009  0.028231668  0.049939473
              V8          V9         V10
25% -0.075939424 -0.05904287 -0.08228384
50% -0.009614193  0.01453459 -0.01600299
75%  0.030312092  0.05087397  0.02335968

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.692879089 -0.057689661  0.016137415  0.027679890  0.005736752  0.023589441 
          V8           V9          V10 
-0.015297355 -0.003966811  0.040097345 

 Quartiles of Marginal Effects:
 
           V2          V3         V4         V5            V6         V7
25% 0.6796253 -0.06521660 0.01096493 0.01931874 -0.0001315374 0.01555216
50% 0.6945680 -0.05849969 0.01668898 0.02972538  0.0065882248 0.02241399
75% 0.7067243 -0.05031243 0.02186538 0.03538554  0.0105463703 0.03037571
             V8           V9        V10
25% -0.02167667 -0.007479914 0.03269395
50% -0.01597612 -0.004423969 0.04180233
75% -0.01091616  0.002238915 0.04642025

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.692785240  0.002170107  0.010260402  0.003174514 -0.026353206 -0.003761233 
          V8           V9          V10 
-0.015547258  0.002710814 -0.012795178 

 Quartiles of Marginal Effects:
 
           V2           V3          V4           V5           V6           V7
25% 0.4927312 -0.075435611 -0.05896429 -0.081480342 -0.100017539 -0.085268532
50% 0.6757261 -0.007707598  0.01315112  0.009667083 -0.008817182  0.004271073
75% 0.8577827  0.096112334  0.08559876  0.097427839  0.034239767  0.081446735
             V8           V9         V10
25% -0.07205369 -0.076991228 -0.09478114
50% -0.01940998 -0.008688159 -0.01512090
75%  0.03513275  0.081060860  0.05177394

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.9991606060  0.0009407000  0.0006075020 -0.0000715192 -0.0015352487 
           V7            V8            V9           V10 
 0.0006435821 -0.0006809842  0.0002320846 -0.0005088308 

 Quartiles of Marginal Effects:
 
           V2            V3            V4            V5            V6
25% 0.9977133 -0.0001302121 -0.0006247603 -8.005499e-04 -0.0026848663
50% 0.9994734  0.0011052555  0.0007704713  6.978833e-05 -0.0017253172
75% 1.0006573  0.0022752511  0.0017030394  6.470550e-04 -0.0004696336
               V7            V8            V9           V10
25% -0.0004296491 -0.0015760761 -0.0008913270 -0.0015098527
50%  0.0007846718 -0.0007347136  0.0002865796 -0.0005477703
75%  0.0014997708  0.0004022996  0.0012389292  0.0005955351
Radial Basis Function Kernel Regularized Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  lambda        sigma        RMSE         Rsquared   MAE          Selected
  3.435425e-05  2055.372223  0.002311072  0.9999469  0.001604047  *       
  1.663290e-04     5.052042  0.169603202  0.7467137  0.132553287          
  1.945917e-04     4.861044  0.173780454  0.7341528  0.136473680          
  3.842046e-04   380.505146  0.005086007  0.9997826  0.003768556          
  6.206932e-04    29.560523  0.048858663  0.9774687  0.037503179          
  8.980533e-04    16.443462  0.074704501  0.9480990  0.056565318          
  2.393728e-03     6.816080  0.139096924  0.8283754  0.105793622          
  2.419451e-03     2.157754  0.253564651  0.4271975  0.215578399          
  2.529256e-03    15.782335  0.076947884  0.9452630  0.058223832          
  2.548482e-03     1.679778  0.268598243  0.3490984  0.230629834          
  2.731578e-03    92.042824  0.017295243  0.9975070  0.013199010          
  5.532669e-03     3.991536  0.195675908  0.6631289  0.157463830          
  6.785035e-03    54.780153  0.027568265  0.9936512  0.021138895          
  1.058164e-02    20.942180  0.062375574  0.9649853  0.047487571          
  5.908250e-02     8.647477  0.121424093  0.8732941  0.092074422          
  7.056976e-02   141.767505  0.038205188  0.9970171  0.031883189          
  2.213156e-01   241.752703  0.112150440  0.9831987  0.097074515          
  2.511539e-01   129.450979  0.087658332  0.9883219  0.075142677          
  4.702188e-01    19.173756  0.094852975  0.9573331  0.074439414          
  9.239907e-01     2.130233  0.268679370  0.4136107  0.231404901          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 3.435425e-05 and sigma
 = 2055.372.
[1] "Sat Mar 10 02:50:19 2018"
Least Angle Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  fraction   RMSE         Rsquared   MAE          Selected
  0.1071961  0.258529350  0.9999740  0.224348745          
  0.2441936  0.218594892  0.9999740  0.189697982          
  0.2578249  0.214621422  0.9999740  0.186250225          
  0.3169125  0.197397619  0.9999740  0.171305193          
  0.3585754  0.185253113  0.9999740  0.160767405          
  0.3906604  0.175900535  0.9999740  0.152652145          
  0.4758149  0.151078724  0.9999740  0.131114020          
  0.4767434  0.150808101  0.9999740  0.130879195          
  0.4805985  0.149684356  0.9999740  0.129904105          
  0.4812563  0.149492627  0.9999740  0.129737738          
  0.4872827  0.147735991  0.9999740  0.128213475          
  0.5485869  0.129866624  0.9999740  0.112707809          
  0.5663104  0.124700516  0.9999740  0.108225009          
  0.6049106  0.113449295  0.9999740  0.098461865          
  0.7542918  0.069910156  0.9999740  0.060678905          
  0.7697237  0.065412758  0.9999740  0.056775704          
  0.8690024  0.036485794  0.9999740  0.031665158          
  0.8799880  0.033286295  0.9999740  0.028886573          
  0.9344600  0.017436726  0.9999740  0.015156005          
  0.9931335  0.001494869  0.9999739  0.000844464  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.9931335.
[1] "Sat Mar 10 02:50:33 2018"
Least Angle Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  step  RMSE         Rsquared   MAE           Selected
  1     0.289776914        NaN  0.2514618411          
  3     0.001436887  0.9999742  0.0006769575          
  4     0.001426509  0.9999741  0.0006511333  *       
  5     0.001435701  0.9999738  0.0006471430          
  6     0.001445833  0.9999735  0.0006694138          
  7     0.001485220  0.9999728  0.0007465071          
  8     0.001606303  0.9999703  0.0009068613          
  9     0.001625320  0.9999698  0.0009323944          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was step = 4.
[1] "Sat Mar 10 02:50:46 2018"
The lasso 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  fraction   RMSE         Rsquared   MAE          Selected
  0.1071961  0.258529350  0.9999740  0.224348745          
  0.2441936  0.218594892  0.9999740  0.189697982          
  0.2578249  0.214621422  0.9999740  0.186250225          
  0.3169125  0.197397619  0.9999740  0.171305193          
  0.3585754  0.185253113  0.9999740  0.160767405          
  0.3906604  0.175900535  0.9999740  0.152652145          
  0.4758149  0.151078724  0.9999740  0.131114020          
  0.4767434  0.150808101  0.9999740  0.130879195          
  0.4805985  0.149684356  0.9999740  0.129904105          
  0.4812563  0.149492627  0.9999740  0.129737738          
  0.4872827  0.147735991  0.9999740  0.128213475          
  0.5485869  0.129866624  0.9999740  0.112707809          
  0.5663104  0.124700516  0.9999740  0.108225009          
  0.6049106  0.113449295  0.9999740  0.098461865          
  0.7542918  0.069910156  0.9999740  0.060678905          
  0.7697237  0.065412758  0.9999740  0.056775704          
  0.8690024  0.036485794  0.9999740  0.031665158          
  0.8799880  0.033286295  0.9999740  0.028886573          
  0.9344600  0.017436726  0.9999740  0.015156005          
  0.9931335  0.001494869  0.9999739  0.000844464  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.9931335.
[1] "Sat Mar 10 02:51:01 2018"
Linear Regression with Backwards Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  nvmax  RMSE         Rsquared   MAE           Selected
  2      0.001445086  0.9999732  0.0007385211  *       
  3      0.001510232  0.9999723  0.0008580215          
  4      0.001573028  0.9999706  0.0009012338          
  5      0.001623239  0.9999689  0.0009529321          
  6      0.001651031  0.9999687  0.0009723128          
  7      0.001671038  0.9999682  0.0009944281          
  8      0.001676020  0.9999680  0.0010025588          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sat Mar 10 02:51:15 2018"
Linear Regression with Forward Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  nvmax  RMSE         Rsquared   MAE           Selected
  2      0.001445086  0.9999732  0.0007385211  *       
  3      0.001510232  0.9999723  0.0008580215          
  4      0.001573028  0.9999706  0.0009012338          
  5      0.001623239  0.9999689  0.0009529321          
  6      0.001651031  0.9999687  0.0009723128          
  7      0.001671038  0.9999682  0.0009944281          
  8      0.001676020  0.9999680  0.0010025588          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sat Mar 10 02:51:28 2018"
Linear Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results:

  RMSE         Rsquared   MAE        
  0.001687096  0.9999678  0.001003681

Tuning parameter 'intercept' was held constant at a value of TRUE
[1] "Sat Mar 10 02:51:42 2018"
Start:  AIC=-654.22
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V9    1    0.0000 0.0001 -656.10
- V4    1    0.0000 0.0001 -656.09
- V7    1    0.0000 0.0001 -655.47
- V3    1    0.0000 0.0001 -655.14
- V5    1    0.0000 0.0001 -654.68
<none>              0.0001 -654.22
- V10   1    0.0000 0.0001 -653.02
- V8    1    0.0000 0.0001 -653.01
- V6    1    0.0000 0.0001 -651.88
- V2    1    4.0505 4.0506 -111.18

Step:  AIC=-656.1
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V10

       Df Sum of Sq    RSS     AIC
- V4    1    0.0000 0.0001 -657.98
- V7    1    0.0000 0.0001 -657.44
- V3    1    0.0000 0.0001 -657.03
- V5    1    0.0000 0.0001 -656.67
<none>              0.0001 -656.10
- V10   1    0.0000 0.0001 -654.92
- V8    1    0.0000 0.0001 -654.80
- V6    1    0.0000 0.0001 -653.72
- V2    1    4.0756 4.0757 -112.87

Step:  AIC=-657.98
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V10

       Df Sum of Sq    RSS     AIC
- V7    1    0.0000 0.0001 -659.32
- V3    1    0.0000 0.0001 -658.61
- V5    1    0.0000 0.0001 -658.56
<none>              0.0001 -657.98
- V10   1    0.0000 0.0001 -656.91
- V8    1    0.0000 0.0001 -656.78
- V6    1    0.0000 0.0001 -655.44
- V2    1    4.1928 4.1929 -113.42

Step:  AIC=-659.32
.outcome ~ V2 + V3 + V5 + V6 + V8 + V10

       Df Sum of Sq    RSS     AIC
- V5    1    0.0000 0.0001 -659.91
- V3    1    0.0000 0.0001 -659.63
<none>              0.0001 -659.32
- V10   1    0.0000 0.0001 -658.25
- V8    1    0.0000 0.0001 -657.29
- V6    1    0.0000 0.0001 -656.89
- V2    1    4.1928 4.1929 -115.42

Step:  AIC=-659.91
.outcome ~ V2 + V3 + V6 + V8 + V10

       Df Sum of Sq    RSS     AIC
- V3    1    0.0000 0.0001 -660.33
<none>              0.0001 -659.91
- V10   1    0.0000 0.0001 -659.36
- V6    1    0.0000 0.0001 -658.06
- V8    1    0.0000 0.0001 -657.73
- V2    1    4.2455 4.2456 -116.78

Step:  AIC=-660.33
.outcome ~ V2 + V6 + V8 + V10

       Df Sum of Sq    RSS     AIC
<none>              0.0001 -660.33
- V10   1    0.0000 0.0001 -660.07
- V6    1    0.0000 0.0001 -658.72
- V8    1    0.0000 0.0001 -658.32
- V2    1    4.3151 4.3152 -117.95
Start:  AIC=-668.47
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V4    1    0.0000 0.0001 -670.41
- V9    1    0.0000 0.0001 -670.23
- V8    1    0.0000 0.0001 -670.10
- V10   1    0.0000 0.0001 -669.70
- V5    1    0.0000 0.0001 -669.67
- V7    1    0.0000 0.0001 -669.21
- V3    1    0.0000 0.0001 -669.02
<none>              0.0001 -668.47
- V6    1    0.0000 0.0001 -667.60
- V2    1    3.8499 3.8500 -110.20

Step:  AIC=-670.41
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V9    1    0.0000 0.0001 -672.17
- V8    1    0.0000 0.0001 -671.99
- V5    1    0.0000 0.0001 -671.63
- V10   1    0.0000 0.0001 -671.60
- V7    1    0.0000 0.0001 -671.15
- V3    1    0.0000 0.0001 -671.01
<none>              0.0001 -670.41
- V6    1    0.0000 0.0001 -669.40
- V2    1    3.9092 3.9093 -111.43

Step:  AIC=-672.17
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V10

       Df Sum of Sq    RSS     AIC
- V8    1    0.0000 0.0001 -673.65
- V5    1    0.0000 0.0001 -673.45
- V10   1    0.0000 0.0001 -672.93
- V7    1    0.0000 0.0001 -672.92
- V3    1    0.0000 0.0001 -672.91
<none>              0.0001 -672.17
- V6    1    0.0000 0.0001 -671.22
- V2    1    3.9346 3.9346 -113.11

Step:  AIC=-673.65
.outcome ~ V2 + V3 + V5 + V6 + V7 + V10

       Df Sum of Sq    RSS     AIC
- V7    1    0.0000 0.0001 -674.85
- V5    1    0.0000 0.0001 -674.82
- V10   1    0.0000 0.0001 -674.56
- V3    1    0.0000 0.0001 -674.38
<none>              0.0001 -673.65
- V6    1    0.0000 0.0001 -673.03
- V2    1    3.9611 3.9612 -114.77

Step:  AIC=-674.85
.outcome ~ V2 + V3 + V5 + V6 + V10

       Df Sum of Sq    RSS     AIC
- V10   1    0.0000 0.0001 -676.01
- V5    1    0.0000 0.0001 -675.68
- V3    1    0.0000 0.0001 -675.51
<none>              0.0001 -674.85
- V6    1    0.0000 0.0001 -674.31
- V2    1    4.0339 4.0340 -115.86

Step:  AIC=-676.01
.outcome ~ V2 + V3 + V5 + V6

       Df Sum of Sq    RSS     AIC
- V5    1    0.0000 0.0001 -676.96
- V3    1    0.0000 0.0001 -676.28
<none>              0.0001 -676.01
- V6    1    0.0000 0.0001 -675.71
- V2    1    4.2346 4.2346 -115.44

Step:  AIC=-676.96
.outcome ~ V2 + V3 + V6

       Df Sum of Sq    RSS     AIC
- V3    1    0.0000 0.0001 -677.36
<none>              0.0001 -676.96
- V6    1    0.0000 0.0001 -676.40
- V2    1    4.3307 4.3307 -116.31

Step:  AIC=-677.36
.outcome ~ V2 + V6

       Df Sum of Sq    RSS     AIC
<none>              0.0001 -677.36
- V6    1    0.0000 0.0001 -676.87
- V2    1    4.3495 4.3496 -118.10
Start:  AIC=-646.07
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V3    1    0.0000 0.0001 -648.07
- V5    1    0.0000 0.0001 -648.06
- V7    1    0.0000 0.0001 -647.32
- V10   1    0.0000 0.0001 -647.27
- V9    1    0.0000 0.0001 -646.52
- V8    1    0.0000 0.0001 -646.30
<none>              0.0001 -646.07
- V4    1    0.0000 0.0001 -644.07
- V6    1    0.0000 0.0001 -644.04
- V2    1    3.2927 3.2928 -121.74

Step:  AIC=-648.07
.outcome ~ V2 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V5    1    0.0000 0.0001 -650.05
- V7    1    0.0000 0.0001 -649.32
- V10   1    0.0000 0.0001 -649.27
- V9    1    0.0000 0.0001 -648.51
- V8    1    0.0000 0.0001 -648.17
<none>              0.0001 -648.07
- V4    1    0.0000 0.0001 -646.06
- V6    1    0.0000 0.0001 -646.01
- V2    1    3.5708 3.5709 -119.61

Step:  AIC=-650.05
.outcome ~ V2 + V4 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V7    1    0.0000 0.0001 -651.31
- V10   1    0.0000 0.0001 -651.14
- V9    1    0.0000 0.0001 -650.51
- V8    1    0.0000 0.0001 -650.09
<none>              0.0001 -650.05
- V6    1    0.0000 0.0001 -647.97
- V4    1    0.0000 0.0001 -647.77
- V2    1    3.6659 3.6660 -120.27

Step:  AIC=-651.31
.outcome ~ V2 + V4 + V6 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V10   1    0.0000 0.0001 -652.21
- V9    1    0.0000 0.0001 -651.41
<none>              0.0001 -651.31
- V8    1    0.0000 0.0001 -650.62
- V4    1    0.0000 0.0001 -649.46
- V6    1    0.0000 0.0001 -649.32
- V2    1    3.7054 3.7055 -121.72

Step:  AIC=-652.21
.outcome ~ V2 + V4 + V6 + V8 + V9

       Df Sum of Sq    RSS     AIC
- V9    1    0.0000 0.0001 -652.74
<none>              0.0001 -652.21
- V8    1    0.0000 0.0001 -651.89
- V6    1    0.0000 0.0001 -650.66
- V4    1    0.0000 0.0001 -650.28
- V2    1    3.8704 3.8705 -121.50

Step:  AIC=-652.74
.outcome ~ V2 + V4 + V6 + V8

       Df Sum of Sq    RSS     AIC
<none>              0.0001 -652.74
- V8    1    0.0000 0.0001 -652.44
- V4    1    0.0000 0.0001 -651.54
- V6    1    0.0000 0.0001 -650.82
- V2    1    3.8727 3.8728 -123.47
Start:  AIC=-983.32
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V5    1    0.0000 0.0001 -985.26
- V9    1    0.0000 0.0001 -984.73
- V10   1    0.0000 0.0001 -984.64
- V7    1    0.0000 0.0001 -984.63
- V4    1    0.0000 0.0001 -984.63
- V8    1    0.0000 0.0001 -984.37
- V3    1    0.0000 0.0001 -984.11
<none>              0.0001 -983.32
- V6    1    0.0000 0.0001 -980.48
- V2    1    5.7201 5.7202 -178.59

Step:  AIC=-985.26
.outcome ~ V2 + V3 + V4 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V9    1    0.0000 0.0001 -986.62
- V10   1    0.0000 0.0001 -986.62
- V7    1    0.0000 0.0001 -986.57
- V4    1    0.0000 0.0001 -986.51
- V8    1    0.0000 0.0001 -986.31
- V3    1    0.0000 0.0001 -986.06
<none>              0.0001 -985.26
- V6    1    0.0000 0.0001 -982.48
- V2    1    5.8273 5.8274 -179.18

Step:  AIC=-986.62
.outcome ~ V2 + V3 + V4 + V6 + V7 + V8 + V10

       Df Sum of Sq    RSS     AIC
- V10   1    0.0000 0.0001 -988.17
- V4    1    0.0000 0.0001 -987.96
- V8    1    0.0000 0.0001 -987.81
- V7    1    0.0000 0.0001 -987.77
- V3    1    0.0000 0.0001 -987.40
<none>              0.0001 -986.62
- V6    1    0.0000 0.0002 -983.90
- V2    1    5.8672 5.8673 -180.66

Step:  AIC=-988.17
.outcome ~ V2 + V3 + V4 + V6 + V7 + V8

       Df Sum of Sq    RSS     AIC
- V4    1    0.0000 0.0001 -989.41
- V8    1    0.0000 0.0001 -989.40
- V7    1    0.0000 0.0001 -989.20
- V3    1    0.0000 0.0001 -989.06
<none>              0.0001 -988.17
- V6    1    0.0000 0.0002 -985.42
- V2    1    6.1248 6.1250 -179.40

Step:  AIC=-989.41
.outcome ~ V2 + V3 + V6 + V7 + V8

       Df Sum of Sq    RSS     AIC
- V8    1    0.0000 0.0001 -990.65
- V3    1    0.0000 0.0001 -990.59
- V7    1    0.0000 0.0001 -990.54
<none>              0.0001 -989.41
- V6    1    0.0000 0.0002 -986.78
- V2    1    6.1758 6.1760 -180.77

Step:  AIC=-990.65
.outcome ~ V2 + V3 + V6 + V7

       Df Sum of Sq    RSS     AIC
- V3    1    0.0000 0.0001 -991.71
- V7    1    0.0000 0.0001 -991.11
<none>              0.0001 -990.65
- V6    1    0.0000 0.0002 -987.70
- V2    1    6.1868 6.1869 -182.63

Step:  AIC=-991.71
.outcome ~ V2 + V6 + V7

       Df Sum of Sq    RSS     AIC
- V7    1     0.000 0.0002 -991.93
<none>              0.0001 -991.71
- V6    1     0.000 0.0002 -988.67
- V2    1     6.329 6.3291 -182.90

Step:  AIC=-991.93
.outcome ~ V2 + V6

       Df Sum of Sq    RSS     AIC
<none>              0.0002 -991.93
- V6    1    0.0000 0.0002 -988.94
- V2    1    6.3501 6.3502 -184.65
Linear Regression with Stepwise Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results:

  RMSE       Rsquared   MAE         
  0.0015609  0.9999712  0.0008656243

[1] "Sat Mar 10 02:51:57 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:52:31 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "logreg"                  
Error : package RWeka is required
Error : package RWeka is required
Error : package RWeka is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:52:45 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "M5"                      
Error : package RWeka is required
Error : package RWeka is required
Error : package RWeka is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:52:58 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "M5Rules"                 
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:53:11 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "mlpKerasDecay"           
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 02:53:24 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "mlpKerasDropout"         
Multi-Step Adaptive MCP-Net 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  alphas     nsteps  scale      RMSE         Rsquared  MAE           Selected
  0.1464765   3      3.1618885  0.001485532  0.999974  0.0008226451          
  0.2697742   9      3.0941310  0.001421345  0.999974  0.0006074520          
  0.2820424   9      3.7993243  0.001419352  0.999974  0.0005966788          
  0.3352213   5      2.8643992  0.001413471  0.999974  0.0005608671          
  0.3727179   7      1.6490471  0.001411020  0.999974  0.0005426900          
  0.4015944   8      3.9715618  0.001409698  0.999974  0.0005315225          
  0.4782334   9      3.0965984  0.001407600  0.999974  0.0005120190          
  0.4790690  10      2.8184239  0.001407585  0.999974  0.0005118794          
  0.4825387   8      2.0784276  0.001407523  0.999974  0.0005113050          
  0.4831307  10      3.6269225  0.001407513  0.999974  0.0005112078          
  0.4885545   6      0.5457716  0.001407421  0.999974  0.0005103283          
  0.5437282   9      3.8986584  0.001406718  0.999974  0.0005023784          
  0.5596794   7      3.1572878  0.001406577  0.999974  0.0005004659          
  0.5944196   8      3.5207345  0.001406342  0.999974  0.0004966958          
  0.7288626   8      3.0187045  0.001405989  0.999974  0.0004882394          
  0.7427514   6      0.8844964  0.001405983  0.999974  0.0004876694  *       
  0.8321022   5      2.9148672  0.001406017  0.999974  0.0004850008          
  0.8419892   6      2.2388908  0.001406026  0.999974  0.0004847540          
  0.8910140   8      2.2761080  0.001406083  0.999974  0.0004836110          
  0.9438202  10      2.0235318  0.001406160  0.999974  0.0004825396          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alphas = 0.7427514, nsteps = 6
 and scale = 0.8844964.
[1] "Sat Mar 10 02:53:44 2018"
# weights:  210
initial  value 8.815052 
iter  10 value 3.220901
iter  20 value 3.161650
iter  30 value 3.150014
iter  40 value 3.145892
iter  50 value 3.142920
iter  60 value 3.142174
iter  70 value 3.142097
iter  80 value 3.142086
final  value 3.142085 
converged
# weights:  78
initial  value 4.841950 
iter  10 value 0.402673
iter  20 value 0.162325
iter  30 value 0.137439
iter  40 value 0.128182
iter  50 value 0.123049
iter  60 value 0.118415
iter  70 value 0.115959
iter  80 value 0.114645
iter  90 value 0.113971
iter 100 value 0.113586
final  value 0.113586 
stopped after 100 iterations
# weights:  177
initial  value 4.847098 
iter  10 value 0.464616
iter  20 value 0.323531
iter  30 value 0.302951
iter  40 value 0.298844
iter  50 value 0.295773
iter  60 value 0.294446
iter  70 value 0.293621
iter  80 value 0.292818
iter  90 value 0.292168
iter 100 value 0.291532
final  value 0.291532 
stopped after 100 iterations
# weights:  199
initial  value 5.318488 
iter  10 value 0.255656
iter  20 value 0.188273
iter  30 value 0.167892
iter  40 value 0.163283
iter  50 value 0.162084
iter  60 value 0.161138
iter  70 value 0.160229
iter  80 value 0.159434
iter  90 value 0.159102
iter 100 value 0.158950
final  value 0.158950 
stopped after 100 iterations
# weights:  144
initial  value 7.205876 
iter  10 value 2.957136
iter  20 value 2.926564
iter  30 value 2.917628
iter  40 value 2.914534
iter  50 value 2.910151
iter  60 value 2.908476
iter  70 value 2.908418
final  value 2.908417 
converged
# weights:  34
initial  value 6.704978 
iter  10 value 0.279485
iter  20 value 0.085451
iter  30 value 0.062030
iter  40 value 0.047009
iter  50 value 0.035168
iter  60 value 0.032061
iter  70 value 0.030868
iter  80 value 0.030383
iter  90 value 0.029765
iter 100 value 0.028968
final  value 0.028968 
stopped after 100 iterations
# weights:  133
initial  value 4.914955 
iter  10 value 1.079058
iter  20 value 0.978723
iter  30 value 0.974742
iter  40 value 0.972023
iter  50 value 0.971520
iter  60 value 0.970802
iter  70 value 0.970613
iter  80 value 0.970595
iter  90 value 0.970577
iter 100 value 0.970574
final  value 0.970574 
stopped after 100 iterations
# weights:  221
initial  value 109.699403 
iter  10 value 4.534239
iter  20 value 4.452567
final  value 4.452563 
converged
# weights:  89
initial  value 7.245503 
iter  10 value 3.652858
iter  20 value 3.628163
iter  30 value 3.626981
iter  40 value 3.626925
final  value 3.626925 
converged
# weights:  111
initial  value 5.899334 
iter  10 value 0.788087
iter  20 value 0.539890
iter  30 value 0.523724
iter  40 value 0.517724
iter  50 value 0.513471
iter  60 value 0.508616
iter  70 value 0.507557
iter  80 value 0.507212
iter  90 value 0.507103
iter 100 value 0.507028
final  value 0.507028 
stopped after 100 iterations
# weights:  111
initial  value 67.663611 
iter  10 value 4.599026
iter  20 value 4.452892
final  value 4.452832 
converged
# weights:  111
initial  value 79.719616 
iter  10 value 4.476601
iter  20 value 4.453086
final  value 4.453086 
converged
# weights:  56
initial  value 12.991026 
iter  10 value 4.452422
final  value 4.452421 
converged
# weights:  111
initial  value 15.065176 
iter  10 value 4.452359
final  value 4.452192 
converged
# weights:  67
initial  value 15.158187 
iter  10 value 4.452417
final  value 4.452407 
converged
# weights:  89
initial  value 5.310520 
iter  10 value 2.111390
iter  20 value 2.065099
iter  30 value 2.056321
iter  40 value 2.053895
iter  50 value 2.053038
iter  60 value 2.052985
iter  70 value 2.052982
final  value 2.052982 
converged
# weights:  122
initial  value 28.058888 
iter  10 value 4.452575
final  value 4.452380 
converged
# weights:  111
initial  value 9.943836 
iter  10 value 3.757790
iter  20 value 3.741784
iter  30 value 3.738089
iter  40 value 3.737208
iter  50 value 3.737186
final  value 3.737186 
converged
# weights:  199
initial  value 7.559857 
iter  10 value 0.515016
iter  20 value 0.350069
iter  30 value 0.333248
iter  40 value 0.330435
iter  50 value 0.327273
iter  60 value 0.325647
iter  70 value 0.324647
iter  80 value 0.324148
iter  90 value 0.323864
iter 100 value 0.323423
final  value 0.323423 
stopped after 100 iterations
# weights:  177
initial  value 17.812604 
iter  10 value 4.453563
iter  20 value 4.452100
iter  20 value 4.452100
iter  20 value 4.452100
final  value 4.452100 
converged
# weights:  210
initial  value 9.348532 
iter  10 value 3.157834
iter  20 value 3.132060
iter  30 value 3.127716
iter  40 value 3.126601
iter  50 value 3.126203
iter  60 value 3.126170
iter  60 value 3.126170
iter  60 value 3.126170
final  value 3.126170 
converged
# weights:  78
initial  value 6.517494 
iter  10 value 0.245062
iter  20 value 0.144149
iter  30 value 0.128298
iter  40 value 0.120936
iter  50 value 0.118106
iter  60 value 0.115238
iter  70 value 0.113676
iter  80 value 0.112220
iter  90 value 0.111662
iter 100 value 0.111225
final  value 0.111225 
stopped after 100 iterations
# weights:  177
initial  value 5.317988 
iter  10 value 0.501974
iter  20 value 0.328041
iter  30 value 0.306135
iter  40 value 0.301295
iter  50 value 0.297799
iter  60 value 0.295885
iter  70 value 0.294658
iter  80 value 0.293826
iter  90 value 0.293659
iter 100 value 0.293618
final  value 0.293618 
stopped after 100 iterations
# weights:  199
initial  value 4.873452 
iter  10 value 0.270417
iter  20 value 0.185324
iter  30 value 0.168027
iter  40 value 0.163526
iter  50 value 0.162276
iter  60 value 0.161490
iter  70 value 0.160937
iter  80 value 0.160564
iter  90 value 0.160252
iter 100 value 0.160105
final  value 0.160105 
stopped after 100 iterations
# weights:  144
initial  value 7.356510 
iter  10 value 2.934132
iter  20 value 2.906797
iter  30 value 2.901260
iter  40 value 2.900428
iter  50 value 2.900014
iter  60 value 2.899970
final  value 2.899968 
converged
# weights:  34
initial  value 4.684122 
iter  10 value 0.159454
iter  20 value 0.067057
iter  30 value 0.048733
iter  40 value 0.040906
iter  50 value 0.031712
iter  60 value 0.029630
iter  70 value 0.028480
iter  80 value 0.027851
iter  90 value 0.027562
iter 100 value 0.027231
final  value 0.027231 
stopped after 100 iterations
# weights:  133
initial  value 6.087083 
iter  10 value 1.119758
iter  20 value 0.984293
iter  30 value 0.978524
iter  40 value 0.976662
iter  50 value 0.975893
iter  60 value 0.975773
iter  70 value 0.975710
iter  80 value 0.975706
iter  90 value 0.975703
iter 100 value 0.975702
final  value 0.975702 
stopped after 100 iterations
# weights:  221
initial  value 115.626133 
iter  10 value 4.716930
iter  20 value 4.393984
final  value 4.393977 
converged
# weights:  89
initial  value 7.343613 
iter  10 value 3.618260
iter  20 value 3.608378
iter  30 value 3.603615
iter  40 value 3.602291
iter  50 value 3.602284
iter  50 value 3.602284
iter  50 value 3.602284
final  value 3.602284 
converged
# weights:  111
initial  value 5.607123 
iter  10 value 0.681262
iter  20 value 0.548019
iter  30 value 0.530442
iter  40 value 0.520979
iter  50 value 0.518641
iter  60 value 0.517706
iter  70 value 0.516863
iter  80 value 0.516034
iter  90 value 0.513703
iter 100 value 0.512674
final  value 0.512674 
stopped after 100 iterations
# weights:  111
initial  value 52.774628 
iter  10 value 4.487915
iter  20 value 4.394066
final  value 4.394064 
converged
# weights:  111
initial  value 87.271804 
iter  10 value 4.426472
iter  20 value 4.394146
final  value 4.394145 
converged
# weights:  56
initial  value 16.092546 
iter  10 value 4.393966
final  value 4.393931 
converged
# weights:  111
initial  value 15.146333 
iter  10 value 4.393859
final  value 4.393857 
converged
# weights:  67
initial  value 14.232659 
iter  10 value 4.393954
final  value 4.393926 
converged
# weights:  89
initial  value 8.050771 
iter  10 value 2.164736
iter  20 value 2.077768
iter  30 value 2.060073
iter  40 value 2.054808
iter  50 value 2.051705
iter  60 value 2.051216
iter  70 value 2.051205
final  value 2.051203 
converged
# weights:  122
initial  value 31.951847 
iter  10 value 4.396571
final  value 4.393918 
converged
# weights:  111
initial  value 7.246698 
iter  10 value 3.734561
iter  20 value 3.708356
iter  30 value 3.706189
iter  40 value 3.706000
iter  50 value 3.701250
iter  60 value 3.700966
final  value 3.700964 
converged
# weights:  199
initial  value 8.094453 
iter  10 value 0.468145
iter  20 value 0.348770
iter  30 value 0.335083
iter  40 value 0.332379
iter  50 value 0.329395
iter  60 value 0.327291
iter  70 value 0.326511
iter  80 value 0.326089
iter  90 value 0.325672
iter 100 value 0.325578
final  value 0.325578 
stopped after 100 iterations
# weights:  177
initial  value 15.201912 
iter  10 value 4.393954
final  value 4.393827 
converged
# weights:  210
initial  value 8.261548 
iter  10 value 2.965207
iter  20 value 2.943192
iter  30 value 2.940284
iter  40 value 2.938181
iter  50 value 2.936980
iter  60 value 2.936031
iter  70 value 2.934920
iter  80 value 2.934867
iter  90 value 2.934842
final  value 2.934841 
converged
# weights:  78
initial  value 4.065627 
iter  10 value 0.112274
iter  20 value 0.095119
iter  30 value 0.089539
iter  40 value 0.087746
iter  50 value 0.086963
iter  60 value 0.086604
iter  70 value 0.086071
iter  80 value 0.085532
iter  90 value 0.085274
iter 100 value 0.085158
final  value 0.085158 
stopped after 100 iterations
# weights:  177
initial  value 4.299901 
iter  10 value 0.423848
iter  20 value 0.290790
iter  30 value 0.272006
iter  40 value 0.268782
iter  50 value 0.265926
iter  60 value 0.264600
iter  70 value 0.263075
iter  80 value 0.261605
iter  90 value 0.261203
iter 100 value 0.261002
final  value 0.261002 
stopped after 100 iterations
# weights:  199
initial  value 4.623928 
iter  10 value 0.219979
iter  20 value 0.158178
iter  30 value 0.140824
iter  40 value 0.137373
iter  50 value 0.136871
iter  60 value 0.136234
iter  70 value 0.135284
iter  80 value 0.134636
iter  90 value 0.134106
iter 100 value 0.133778
final  value 0.133778 
stopped after 100 iterations
# weights:  144
initial  value 8.621709 
iter  10 value 2.767078
iter  20 value 2.740340
iter  30 value 2.728220
iter  40 value 2.724084
iter  50 value 2.722073
iter  60 value 2.721638
iter  70 value 2.721622
final  value 2.721620 
converged
# weights:  34
initial  value 3.962182 
iter  10 value 0.248530
iter  20 value 0.041680
iter  30 value 0.031238
iter  40 value 0.024432
iter  50 value 0.022145
iter  60 value 0.021475
iter  70 value 0.021220
iter  80 value 0.020999
iter  90 value 0.020740
iter 100 value 0.020338
final  value 0.020338 
stopped after 100 iterations
# weights:  133
initial  value 4.498122 
iter  10 value 1.031876
iter  20 value 0.920908
iter  30 value 0.915864
iter  40 value 0.912043
iter  50 value 0.911845
iter  60 value 0.911748
iter  70 value 0.911732
iter  80 value 0.911729
final  value 0.911729 
converged
# weights:  221
initial  value 122.300034 
iter  10 value 4.383122
iter  20 value 3.909900
final  value 3.909868 
converged
# weights:  89
initial  value 7.601579 
iter  10 value 3.371026
iter  20 value 3.358207
iter  30 value 3.353030
iter  40 value 3.352108
iter  50 value 3.351688
iter  60 value 3.348503
iter  70 value 3.348395
iter  70 value 3.348395
iter  70 value 3.348395
final  value 3.348395 
converged
# weights:  111
initial  value 4.117929 
iter  10 value 0.630965
iter  20 value 0.484483
iter  30 value 0.476812
iter  40 value 0.472864
iter  50 value 0.470956
iter  60 value 0.469813
iter  70 value 0.468954
iter  80 value 0.468539
iter  90 value 0.468230
iter 100 value 0.468103
final  value 0.468103 
stopped after 100 iterations
# weights:  111
initial  value 64.692296 
iter  10 value 3.940081
iter  20 value 3.910070
final  value 3.910069 
converged
# weights:  111
initial  value 97.263697 
iter  10 value 3.961323
iter  20 value 3.910258
iter  20 value 3.910258
iter  20 value 3.910258
final  value 3.910258 
converged
# weights:  56
initial  value 12.190433 
iter  10 value 3.909776
final  value 3.909765 
converged
# weights:  111
initial  value 13.882020 
iter  10 value 3.909597
final  value 3.909596 
converged
# weights:  67
initial  value 14.257740 
iter  10 value 3.909784
final  value 3.909754 
converged
# weights:  89
initial  value 4.894270 
iter  10 value 1.992818
iter  20 value 1.951019
iter  30 value 1.937415
iter  40 value 1.932337
iter  50 value 1.931394
iter  60 value 1.931336
iter  70 value 1.931320
iter  80 value 1.931320
iter  80 value 1.931320
iter  80 value 1.931320
final  value 1.931320 
converged
# weights:  122
initial  value 28.032803 
iter  10 value 3.910176
final  value 3.909733 
converged
# weights:  111
initial  value 9.141131 
iter  10 value 3.451606
iter  20 value 3.446453
iter  30 value 3.442432
iter  40 value 3.438109
iter  50 value 3.437686
final  value 3.437682 
converged
# weights:  199
initial  value 3.886153 
iter  10 value 0.433893
iter  20 value 0.308937
iter  30 value 0.299759
iter  40 value 0.298134
iter  50 value 0.296856
iter  60 value 0.295768
iter  70 value 0.294050
iter  80 value 0.293423
iter  90 value 0.293079
iter 100 value 0.292593
final  value 0.292593 
stopped after 100 iterations
# weights:  177
initial  value 15.893480 
iter  10 value 3.913489
iter  20 value 3.909529
final  value 3.909529 
converged
# weights:  34
initial  value 6.209818 
iter  10 value 0.133057
iter  20 value 0.088725
iter  30 value 0.070472
iter  40 value 0.064952
iter  50 value 0.056889
iter  60 value 0.036556
iter  70 value 0.031624
iter  80 value 0.030435
iter  90 value 0.028783
iter 100 value 0.028087
final  value 0.028087 
stopped after 100 iterations
Neural Network 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  size  decay        RMSE        Rsquared   MAE         Selected
   3    0.000107316  0.01588771  0.9975934  0.01196483  *       
   5    0.880642313  0.28972995  0.9246101  0.25141682          
   6    0.933051753  0.28973165  0.9246843  0.25141838          
   7    0.001347284  0.03669076  0.9867794  0.03004011          
   8    0.062220260  0.10528941  0.9850209  0.08777115          
   8    0.149971978  0.19050802  0.9637471  0.16494781          
  10    0.011324397  0.04059209  0.9906217  0.02921002          
  10    0.159493552  0.19770141  0.9625677  0.17140703          
  10    0.561950384  0.28975772  0.9240696  0.25144389          
  10    3.154983040  0.28968150  0.9239014  0.25136767          
  10    4.593273730  0.28965190  0.9240594  0.25133683          
  11    1.253977888  0.28973550  0.9247497  0.25142203          
  12    0.024664092  0.05722332  0.9913847  0.04398509          
  13    0.104343923  0.14697105  0.9780006  0.12621930          
  16    0.005924272  0.03505928  0.9901283  0.02662227          
  16    0.393247400  0.28976882  0.9239503  0.25145457          
  18    0.002660376  0.03408463  0.9890544  0.02733277          
  18    0.006789566  0.03536092  0.9904558  0.02640602          
  19    0.119107524  0.15943939  0.9757063  0.13767015          
  20    3.216321273  0.28971358  0.9244444  0.25140034          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 3 and decay = 0.000107316.
[1] "Sat Mar 10 02:54:35 2018"
Non-Negative Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results:

  RMSE         Rsquared   MAE         
  0.001427356  0.9999737  0.0005940687

[1] "Sat Mar 10 02:54:48 2018"
Non-Informative Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results:

  RMSE       Rsquared  MAE      
  0.2897769  NaN       0.2514618

[1] "Sat Mar 10 02:55:01 2018"
Parallel Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE         Selected
  1     0.21117999  0.7050257  0.17330440          
  3     0.12727613  0.9086688  0.09637874          
  4     0.09379866  0.9485508  0.06757677          
  5     0.07496301  0.9657237  0.05380314          
  6     0.05971435  0.9758704  0.04369246          
  7     0.04483030  0.9842977  0.03374611          
  8     0.03360334  0.9904288  0.02545281          
  9     0.02629406  0.9934858  0.02069956  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 02:55:18 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: Warning messages:
1: package 'mxnet' is not available (for R version 3.4.3) 
2: package 'mxnet' is not available (for R version 3.4.3) 
3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
# weights:  210
initial  value 9.254202 
iter  10 value 1.312642
iter  20 value 1.294284
iter  30 value 1.282922
iter  40 value 1.280809
iter  50 value 1.280731
final  value 1.280730 
converged
# weights:  78
initial  value 5.261384 
iter  10 value 0.129350
iter  20 value 0.070086
iter  30 value 0.062237
iter  40 value 0.059163
iter  50 value 0.057602
iter  60 value 0.057051
iter  70 value 0.056677
iter  80 value 0.056385
iter  90 value 0.056061
iter 100 value 0.055808
final  value 0.055808 
stopped after 100 iterations
# weights:  177
initial  value 4.681301 
iter  10 value 0.344378
iter  20 value 0.162799
iter  30 value 0.133549
iter  40 value 0.130838
iter  50 value 0.129542
iter  60 value 0.128431
iter  70 value 0.128047
iter  80 value 0.127981
iter  90 value 0.127962
iter 100 value 0.127948
final  value 0.127948 
stopped after 100 iterations
# weights:  199
initial  value 4.634560 
iter  10 value 0.153707
iter  20 value 0.095192
iter  30 value 0.086534
iter  40 value 0.083561
iter  50 value 0.081929
iter  60 value 0.081270
iter  70 value 0.080966
iter  80 value 0.080891
iter  90 value 0.080766
iter 100 value 0.080276
final  value 0.080276 
stopped after 100 iterations
# weights:  144
initial  value 6.576703 
iter  10 value 1.190082
iter  20 value 1.166357
iter  30 value 1.165262
iter  40 value 1.165235
final  value 1.165235 
converged
# weights:  34
initial  value 7.215093 
iter  10 value 0.117167
iter  20 value 0.053598
iter  30 value 0.045128
iter  40 value 0.036988
iter  50 value 0.031655
iter  60 value 0.030250
iter  70 value 0.029662
iter  80 value 0.029462
iter  90 value 0.029222
iter 100 value 0.028496
final  value 0.028496 
stopped after 100 iterations
# weights:  133
initial  value 7.116090 
iter  10 value 0.449925
iter  20 value 0.367180
iter  30 value 0.359977
iter  40 value 0.357116
iter  50 value 0.355339
iter  60 value 0.355120
iter  70 value 0.355116
final  value 0.355115 
converged
# weights:  221
initial  value 108.284769 
iter  10 value 4.489607
iter  20 value 4.452544
final  value 4.452531 
converged
# weights:  89
initial  value 8.755733 
iter  10 value 1.619975
iter  20 value 1.595587
iter  30 value 1.592683
final  value 1.592681 
converged
# weights:  111
initial  value 5.696265 
iter  10 value 0.251047
iter  20 value 0.210574
iter  30 value 0.204550
iter  40 value 0.202484
iter  50 value 0.202236
iter  60 value 0.202223
iter  70 value 0.202222
final  value 0.202222 
converged
# weights:  111
initial  value 69.035709 
iter  10 value 4.525205
iter  20 value 4.452796
final  value 4.452793 
converged
# weights:  111
initial  value 80.100118 
iter  10 value 4.579677
iter  20 value 4.453070
final  value 4.453064 
converged
# weights:  56
initial  value 13.043009 
iter  10 value 4.428412
iter  20 value 4.422487
final  value 4.421738 
converged
# weights:  111
initial  value 14.574357 
iter  10 value 3.823049
iter  20 value 3.802970
iter  30 value 3.802074
iter  30 value 3.802074
iter  30 value 3.802074
final  value 3.802074 
converged
# weights:  67
initial  value 17.553997 
iter  10 value 4.447412
iter  20 value 4.445868
iter  30 value 4.445514
final  value 4.445489 
converged
# weights:  89
initial  value 6.172096 
iter  10 value 0.822325
iter  20 value 0.792722
iter  30 value 0.790142
iter  40 value 0.789931
iter  50 value 0.789925
iter  50 value 0.789925
iter  50 value 0.789925
final  value 0.789925 
converged
# weights:  122
initial  value 28.694256 
iter  10 value 4.491757
iter  20 value 4.452227
final  value 4.452222 
converged
# weights:  111
initial  value 11.071496 
iter  10 value 1.700093
iter  20 value 1.680366
iter  30 value 1.662914
iter  40 value 1.653089
iter  50 value 1.652700
final  value 1.652698 
converged
# weights:  199
initial  value 7.854644 
iter  10 value 0.270635
iter  20 value 0.147847
iter  30 value 0.142276
iter  40 value 0.139773
iter  50 value 0.138318
iter  60 value 0.137565
iter  70 value 0.137351
iter  80 value 0.137206
iter  90 value 0.136925
iter 100 value 0.136864
final  value 0.136864 
stopped after 100 iterations
# weights:  177
initial  value 18.569398 
iter  10 value 3.134488
iter  20 value 3.107229
iter  30 value 3.101985
iter  40 value 3.100670
final  value 3.100670 
converged
# weights:  210
initial  value 9.268270 
iter  10 value 1.327904
iter  20 value 1.295102
iter  30 value 1.291381
iter  40 value 1.288709
iter  50 value 1.286146
iter  60 value 1.284885
final  value 1.284872 
converged
# weights:  78
initial  value 5.692209 
iter  10 value 0.104291
iter  20 value 0.076892
iter  30 value 0.067557
iter  40 value 0.063671
iter  50 value 0.061243
iter  60 value 0.059964
iter  70 value 0.059514
iter  80 value 0.059280
iter  90 value 0.058789
iter 100 value 0.058214
final  value 0.058214 
stopped after 100 iterations
# weights:  177
initial  value 5.357865 
iter  10 value 0.237197
iter  20 value 0.149345
iter  30 value 0.134917
iter  40 value 0.132205
iter  50 value 0.131104
iter  60 value 0.130111
iter  70 value 0.129296
iter  80 value 0.129133
iter  90 value 0.129075
iter 100 value 0.129063
final  value 0.129063 
stopped after 100 iterations
# weights:  199
initial  value 4.835450 
iter  10 value 0.206120
iter  20 value 0.115173
iter  30 value 0.088447
iter  40 value 0.084770
iter  50 value 0.082987
iter  60 value 0.082286
iter  70 value 0.081588
iter  80 value 0.081037
iter  90 value 0.080669
iter 100 value 0.080410
final  value 0.080410 
stopped after 100 iterations
# weights:  144
initial  value 8.378451 
iter  10 value 1.244860
iter  20 value 1.173997
iter  30 value 1.170317
iter  40 value 1.170133
final  value 1.170132 
converged
# weights:  34
initial  value 4.897937 
iter  10 value 0.190611
iter  20 value 0.048473
iter  30 value 0.021466
iter  40 value 0.018360
iter  50 value 0.017241
iter  60 value 0.016385
iter  70 value 0.015438
iter  80 value 0.015167
iter  90 value 0.014894
iter 100 value 0.014536
final  value 0.014536 
stopped after 100 iterations
# weights:  133
initial  value 5.234108 
iter  10 value 0.394045
iter  20 value 0.364577
iter  30 value 0.361295
iter  40 value 0.359369
iter  50 value 0.358944
iter  60 value 0.358920
iter  70 value 0.358919
final  value 0.358919 
converged
# weights:  221
initial  value 115.445455 
iter  10 value 4.442376
iter  20 value 4.393979
final  value 4.393967 
converged
# weights:  89
initial  value 8.863453 
iter  10 value 1.642003
iter  20 value 1.614774
iter  30 value 1.613493
iter  40 value 1.610170
final  value 1.610155 
converged
# weights:  111
initial  value 6.333492 
iter  10 value 0.273866
iter  20 value 0.211872
iter  30 value 0.206057
iter  40 value 0.204797
iter  50 value 0.204652
iter  60 value 0.204644
iter  70 value 0.204643
final  value 0.204643 
converged
# weights:  111
initial  value 53.467187 
iter  10 value 4.414528
iter  20 value 4.394056
final  value 4.394051 
converged
# weights:  111
initial  value 87.601529 
iter  10 value 4.500054
iter  20 value 4.394147
final  value 4.394138 
converged
# weights:  56
initial  value 15.560827 
iter  10 value 4.372531
iter  20 value 4.366741
iter  30 value 4.366436
final  value 4.366436 
converged
# weights:  111
initial  value 14.985621 
iter  10 value 3.782049
iter  20 value 3.769183
iter  30 value 3.767224
final  value 3.767224 
converged
# weights:  67
initial  value 13.902554 
iter  10 value 4.390271
iter  20 value 4.389037
iter  30 value 4.388699
iter  40 value 4.388440
final  value 4.388311 
converged
# weights:  89
initial  value 9.221119 
iter  10 value 0.849544
iter  20 value 0.789279
iter  30 value 0.788425
iter  40 value 0.788411
final  value 0.788411 
converged
# weights:  122
initial  value 32.141189 
iter  10 value 4.438922
iter  20 value 4.393867
iter  20 value 4.393867
iter  20 value 4.393867
final  value 4.393867 
converged
# weights:  111
initial  value 7.307509 
iter  10 value 1.707344
iter  20 value 1.669917
iter  30 value 1.665599
iter  40 value 1.662753
iter  50 value 1.662606
final  value 1.662605 
converged
# weights:  199
initial  value 8.073797 
iter  10 value 0.322019
iter  20 value 0.149373
iter  30 value 0.140777
iter  40 value 0.139371
iter  50 value 0.138978
iter  60 value 0.138734
iter  70 value 0.138589
iter  80 value 0.138568
iter  90 value 0.138566
final  value 0.138565 
converged
# weights:  177
initial  value 15.266574 
iter  10 value 3.115484
iter  20 value 3.085205
iter  30 value 3.083926
iter  40 value 3.083379
final  value 3.083378 
converged
# weights:  210
initial  value 8.617630 
iter  10 value 1.189076
iter  20 value 1.155942
iter  30 value 1.151506
iter  40 value 1.150563
iter  50 value 1.149556
iter  60 value 1.149245
final  value 1.149241 
converged
# weights:  78
initial  value 4.794577 
iter  10 value 0.102121
iter  20 value 0.058954
iter  30 value 0.049327
iter  40 value 0.045515
iter  50 value 0.043524
iter  60 value 0.042331
iter  70 value 0.040812
iter  80 value 0.039878
iter  90 value 0.039297
iter 100 value 0.039004
final  value 0.039004 
stopped after 100 iterations
# weights:  177
initial  value 4.231014 
iter  10 value 0.155420
iter  20 value 0.102540
iter  30 value 0.099428
iter  40 value 0.098173
iter  50 value 0.097818
iter  60 value 0.097711
iter  70 value 0.097671
iter  80 value 0.097506
iter  90 value 0.097325
iter 100 value 0.097271
final  value 0.097271 
stopped after 100 iterations
# weights:  199
initial  value 2.329930 
iter  10 value 0.116804
iter  20 value 0.065223
iter  30 value 0.060053
iter  40 value 0.058966
iter  50 value 0.058314
iter  60 value 0.057901
iter  70 value 0.057727
iter  80 value 0.057661
iter  90 value 0.057652
iter 100 value 0.057649
final  value 0.057649 
stopped after 100 iterations
# weights:  144
initial  value 8.491512 
iter  10 value 1.075790
iter  20 value 1.045537
iter  30 value 1.042669
iter  40 value 1.042228
final  value 1.042226 
converged
# weights:  34
initial  value 3.647187 
iter  10 value 0.060443
iter  20 value 0.036141
iter  30 value 0.025159
iter  40 value 0.023620
iter  50 value 0.022034
iter  60 value 0.020861
iter  70 value 0.020124
iter  80 value 0.019960
iter  90 value 0.019659
iter 100 value 0.019268
final  value 0.019268 
stopped after 100 iterations
# weights:  133
initial  value 4.100777 
iter  10 value 0.351342
iter  20 value 0.309819
iter  30 value 0.305837
iter  40 value 0.304769
iter  50 value 0.304495
iter  60 value 0.304483
iter  70 value 0.304482
final  value 0.304482 
converged
# weights:  221
initial  value 121.013826 
iter  10 value 4.068730
iter  20 value 3.909862
final  value 3.909846 
converged
# weights:  89
initial  value 8.997915 
iter  10 value 1.492920
iter  20 value 1.441082
iter  30 value 1.429632
iter  40 value 1.429434
final  value 1.429434 
converged
# weights:  111
initial  value 4.581599 
iter  10 value 0.270443
iter  20 value 0.170559
iter  30 value 0.164620
iter  40 value 0.163378
iter  50 value 0.163217
iter  60 value 0.163205
iter  70 value 0.163203
final  value 0.163203 
converged
# weights:  111
initial  value 65.498044 
iter  10 value 4.103665
iter  20 value 3.910285
final  value 3.910042 
converged
# weights:  111
initial  value 95.742775 
iter  10 value 4.076341
iter  20 value 3.910514
final  value 3.910243 
converged
# weights:  56
initial  value 13.067516 
iter  10 value 3.905987
iter  20 value 3.902190
iter  30 value 3.902009
iter  40 value 3.901733
iter  40 value 3.901733
iter  40 value 3.901733
final  value 3.901733 
converged
# weights:  111
initial  value 15.733747 
iter  10 value 3.430421
iter  20 value 3.414685
iter  30 value 3.409549
final  value 3.409546 
converged
# weights:  67
initial  value 13.810463 
iter  10 value 3.915855
iter  20 value 3.909498
final  value 3.909494 
converged
# weights:  89
initial  value 5.473931 
iter  10 value 0.737564
iter  20 value 0.705013
iter  30 value 0.699689
iter  40 value 0.697790
iter  50 value 0.697572
iter  60 value 0.697569
final  value 0.697569 
converged
# weights:  122
initial  value 28.075737 
iter  10 value 3.921203
iter  20 value 3.909628
final  value 3.909628 
converged
# weights:  111
initial  value 10.695496 
iter  10 value 1.516319
iter  20 value 1.496425
iter  30 value 1.488012
iter  40 value 1.485403
final  value 1.485401 
converged
# weights:  199
initial  value 4.098319 
iter  10 value 0.275034
iter  20 value 0.129934
iter  30 value 0.109004
iter  40 value 0.107188
iter  50 value 0.106430
iter  60 value 0.106045
iter  70 value 0.105776
iter  80 value 0.105601
iter  90 value 0.105554
iter 100 value 0.105546
final  value 0.105546 
stopped after 100 iterations
# weights:  177
initial  value 16.132722 
iter  10 value 2.812654
iter  20 value 2.797352
iter  30 value 2.796475
iter  40 value 2.795931
final  value 2.795931 
converged
# weights:  199
initial  value 9.299723 
iter  10 value 0.484611
iter  20 value 0.205742
iter  30 value 0.162149
iter  40 value 0.157751
iter  50 value 0.155439
iter  60 value 0.154708
iter  70 value 0.154588
iter  80 value 0.154551
iter  90 value 0.154547
iter 100 value 0.154547
final  value 0.154547 
stopped after 100 iterations
Neural Networks with Feature Extraction 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  size  decay        RMSE        Rsquared   MAE         Selected
   3    0.000107316  0.05043838  0.9627093  0.04116112          
   5    0.880642313  0.27452206  0.9298493  0.23839226          
   6    0.933051753  0.28430981  0.9232702  0.24683237          
   7    0.001347284  0.05499621  0.9697004  0.04528841          
   8    0.062220260  0.05193699  0.9892557  0.03754430          
   8    0.149971978  0.08408835  0.9900850  0.06849218          
  10    0.011324397  0.03825082  0.9862272  0.03066823          
  10    0.159493552  0.08591134  0.9903165  0.07048718          
  10    0.561950384  0.19981629  0.9634143  0.17349480          
  10    3.154983040  0.28968384  0.9195744  0.25137065          
  10    4.593273730  0.28965347  0.9193248  0.25133883          
  11    1.253977888  0.28974572  0.9195340  0.25143353          
  12    0.024664092  0.03879250  0.9882076  0.02929829          
  13    0.104343923  0.06457846  0.9914631  0.05062384          
  16    0.005924272  0.03642395  0.9869989  0.02956194          
  16    0.393247400  0.15476240  0.9776413  0.13390009          
  18    0.002660376  0.04162292  0.9830312  0.03322875          
  18    0.006789566  0.03600339  0.9873618  0.02912263  *       
  19    0.119107524  0.06862704  0.9918970  0.05493753          
  20    3.216321273  0.28971606  0.9193967  0.25140293          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 18 and decay = 0.006789566.
[1] "Sat Mar 10 02:55:35 2018"
Principal Component Analysis 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  ncomp  RMSE        Rsquared   MAE         Selected
  1      0.28388463  0.0476923  0.25057870          
  2      0.24562119  0.2864593  0.20671503          
  3      0.21509527  0.4481661  0.17801046          
  4      0.16216288  0.6988150  0.13500864          
  5      0.15084872  0.7353706  0.12183891          
  7      0.13478336  0.7931043  0.11142610          
  8      0.06187385  0.9479624  0.04940114  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 8.
[1] "Sat Mar 10 02:55:48 2018"
____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
No more significant predictors (<0.13586463984102) found
Warning only 4 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
No more significant predictors (<0.0924211652483791) found
Warning only 5 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
No more significant predictors (<0.0808314330875874) found
Warning only 5 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
No more significant predictors (<0.133948905346915) found
Warning only 4 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
No more significant predictors (<0.113068837765604) found
Warning only 5 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
No more significant predictors (<0.183578647440299) found
Warning only 4 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
No more significant predictors (<0.183299900963902) found
Warning only 4 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
No more significant predictors (<0.158323265751824) found
Warning only 4 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
No more significant predictors (<0.169942995952442) found
Warning only 4 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
No more significant predictors (<0.140091437706724) found
Warning only 4 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
No more significant predictors (<0.0943947330582887) found
Warning only 5 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
No more significant predictors (<0.153155528660864) found
Warning only 4 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
No more significant predictors (<0.13586463984102) found
Warning only 5 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
No more significant predictors (<0.0924211652483791) found
Warning only 5 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
No more significant predictors (<0.0808314330875874) found
Warning only 5 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
No more significant predictors (<0.133948905346915) found
Warning only 5 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
No more significant predictors (<0.113068837765604) found
Warning only 5 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 8 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 8 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 8 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 8 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 8 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
No more significant predictors (<0.0943947330582887) found
Warning only 5 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 8 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 2 7 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 2 7 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 2 7 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 2 7 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 2 7 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
No more significant predictors (<0.183578647440299) found
Warning only 5 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 2 7 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 2 7 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Component____ 5 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 2 7 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 2 7 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 2 7 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 2 7 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 2 7 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 2 7 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 8 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

Partial Least Squares Generalized Linear Models  

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  nt  alpha.pvals.expli  RMSE         Rsquared   MAE           Selected
  1   0.03435548         0.084553679  0.9195105  0.0718198549          
  3   0.07098197         0.002605612  0.9999034  0.0019332388          
  3   0.16482665         0.001686863  0.9999631  0.0009374563          
  3   0.16566352         0.001686863  0.9999631  0.0009374563          
  4   0.12646439         0.001567812  0.9999677  0.0008891190          
  4   0.13920034         0.001567812  0.9999677  0.0008891190          
  5   0.10180050         0.001567171  0.9999689  0.0007774164          
  5   0.14009144         0.001514992  0.9999701  0.0007627943          
  5   0.15832327         0.001514669  0.9999701  0.0006904340          
  5   0.16994300         0.001514669  0.9999701  0.0006904340  *       
  5   0.18329990         0.001665193  0.9999678  0.0008909629          
  5   0.18873741         0.001705606  0.9999672  0.0009256497          
  6   0.11306884         0.001567171  0.9999689  0.0007774164          
  6   0.13394891         0.001514992  0.9999701  0.0007627943          
  7   0.09242117         0.001554200  0.9999698  0.0007751929          
  7   0.15315553         0.001514669  0.9999701  0.0006904340          
  8   0.08083143         0.001558000  0.9999696  0.0007671127          
  8   0.09439473         0.001554200  0.9999698  0.0007751929          
  9   0.13586464         0.001514992  0.9999701  0.0007627943          
  9   0.18357865         0.001665193  0.9999678  0.0008909629          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nt = 5 and alpha.pvals.expli
 = 0.169943.
[1] "Sat Mar 10 02:56:35 2018"
Projection Pursuit Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  nterms  RMSE         Rsquared   MAE          Selected
   1      0.003827291  0.9998119  0.001816599  *       
   2      0.004190216  0.9998143  0.002275330          
   3      0.004377116  0.9998005  0.002504823          
   4      0.004156853  0.9998217  0.002419426          
   5      0.004456937  0.9998027  0.002548112          
   6      0.004480224  0.9997990  0.002619528          
   7      0.004453064  0.9998037  0.002532325          
   8      0.004452824  0.9998033  0.002522329          
   9      0.004458205  0.9998028  0.002525663          
  10      0.004457796  0.9998027  0.002530282          
  11      0.004456453  0.9998028  0.002531046          
  12      0.004456352  0.9998029  0.002528563          
  13      0.004457489  0.9998028  0.002527820          
  14      0.004457413  0.9998027  0.002528367          
  15      0.004459716  0.9998026  0.002529428          
  16      0.004460150  0.9998025  0.002529845          
  17      0.004459934  0.9998025  0.002529020          
  18      0.004459925  0.9998026  0.002529591          
  19      0.004460349  0.9998025  0.002529842          
  20      0.004460053  0.9998026  0.002529886          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nterms = 1.
[1] "Sat Mar 10 02:56:49 2018"
Quantile Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE         Selected
  1     0.43841912  0.4389553  0.36999572          
  3     0.33450583  0.6627110  0.28447491          
  4     0.26357929  0.7870399  0.22483696          
  5     0.20060432  0.9032774  0.17707711          
  6     0.15761590  0.9529967  0.14254116          
  7     0.13760888  0.9612509  0.12360867          
  8     0.09659708  0.9851000  0.08975031          
  9     0.07835606  0.9893950  0.07250328  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 02:57:27 2018"
Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  min.node.size  mtry  splitrule   RMSE        Rsquared   MAE         Selected
   3             2     maxstat     0.19316621  0.8760064  0.15857245          
   5             8     maxstat     0.03306059  0.9934657  0.02236172          
   6             8     maxstat     0.03455261  0.9933319  0.02378079          
   7             4     maxstat     0.12881220  0.9473300  0.09990495          
   8             6     extratrees  0.06572776  0.9906300  0.05116608          
   8             7     maxstat     0.05360246  0.9871505  0.03773344          
  10             5     variance    0.08282777  0.9568136  0.06027040          
  10             7     extratrees  0.05354786  0.9917446  0.03994259          
  10             8     maxstat     0.03853227  0.9923363  0.02599460          
  10             9     maxstat     0.02325654  0.9960167  0.01616659  *       
  11             8     maxstat     0.03876590  0.9919597  0.02624435          
  12             6     maxstat     0.07871711  0.9751677  0.05652614          
  13             7     maxstat     0.06254955  0.9802164  0.04258131          
  16             5     variance    0.09959489  0.9371459  0.07306041          
  16             7     maxstat     0.06903215  0.9764307  0.04826167          
  18             4     maxstat     0.14515446  0.9305427  0.11334912          
  18             5     extratrees  0.11346515  0.9841129  0.09468540          
  19             7     extratrees  0.07237085  0.9893576  0.05502955          
  20             9     extratrees  0.04807349  0.9900939  0.03097159          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 9, splitrule = maxstat
 and min.node.size = 10.
[1] "Sat Mar 10 02:57:46 2018"
Error in code$varImp(object$finalModel, ...) : 
  No importance values available
In addition: Warning messages:
1: package 'gpls' is not available (for R version 3.4.3) 
2: package 'rPython' is not available (for R version 3.4.3) 
Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  predFixed  minNode  RMSE        Rsquared   MAE         Selected
  1           4       0.20563804  0.7298312  0.16918004          
  3           8       0.12970712  0.9130873  0.09973065          
  3          17       0.16592703  0.8527288  0.12974012          
  4          13       0.12469889  0.9158470  0.09397076          
  4          14       0.13066842  0.9070442  0.09808170          
  5          11       0.09307438  0.9484717  0.06738307          
  5          15       0.11757484  0.9110249  0.08739852          
  5          16       0.12059784  0.9006817  0.08969609          
  5          17       0.12790447  0.8909670  0.09656827          
  5          19       0.13931219  0.8575506  0.10652127          
  6          12       0.08114310  0.9553758  0.05834387          
  6          14       0.09462339  0.9367753  0.06829821          
  7          10       0.05651773  0.9769105  0.04014724          
  7          16       0.09069754  0.9312892  0.06811361          
  8           9       0.03939276  0.9882539  0.02818456  *       
  8          10       0.04485961  0.9846014  0.03296995          
  9          14       0.05172572  0.9747296  0.03996395          
  9          19       0.10303009  0.8853389  0.08676990          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were predFixed = 8 and minNode = 9.
[1] "Sat Mar 10 02:59:25 2018"
Relaxed Lasso 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  phi        lambda       RMSE         Rsquared   MAE           Selected
  0.1717774   0.01684722  0.003392839  0.9998611  0.0023874765          
  0.3549099   0.09384970  0.001390964  0.9999753  0.0006575107  *       
  0.4041572   8.63111892  0.576225902        NaN  0.4979846667          
  0.4621058   3.82789231  0.001414405  0.9999749  0.0007066103          
  0.4719737   9.44364979  0.576225902        NaN  0.4979846667          
  0.5090025   0.37878569  0.001428887  0.9999747  0.0007379207          
  0.5653442   0.72355854  0.001449390  0.9999745  0.0007800921          
  0.6323220   0.13201293  0.001477772  0.9999742  0.0008303317          
  0.6697445   0.99257681  0.001495342  0.9999740  0.0008584021          
  0.6793232  14.75305800  0.576225902        NaN  0.4979846667          
  0.6960017   0.17168573  0.001508338  0.9999738  0.0008791044          
  0.7004572   0.35860781  0.001510596  0.9999738  0.0008826286          
  0.7657776   3.37343578  0.001545312  0.9999734  0.0009342960          
  0.7916163   0.34483058  0.001559825  0.9999732  0.0009556089          
  0.8241333   0.05173582  0.001578660  0.9999730  0.0009824695          
  0.8283176   0.05784610  0.001581128  0.9999729  0.0009859260          
  0.8497150   0.62580068  0.001593899  0.9999728  0.0010036013          
  0.9164995   0.34746247  0.001635272  0.9999722  0.0010587685          
  0.9178932  23.85436348  0.576225902        NaN  0.4979846667          
  0.9436871   0.36054477  0.001652716  0.9999720  0.0010812268          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 0.0938497 and phi = 0.3549099.
[1] "Sat Mar 10 02:59:40 2018"
Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  mtry  RMSE        Rsquared   MAE         Selected
  1     0.20716911  0.7375528  0.17131402          
  3     0.12128275  0.9196885  0.09109411          
  4     0.09518966  0.9436217  0.06882968          
  5     0.07521074  0.9641812  0.05369871          
  6     0.05575441  0.9775775  0.03993568          
  7     0.04439597  0.9849165  0.03252426          
  8     0.03276926  0.9905836  0.02524129          
  9     0.02600101  0.9935049  0.02084004  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 02:59:57 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Ridge Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  lambda        RMSE         Rsquared   MAE          Selected
  4.397204e-05  0.001687754  0.9999678  0.001004022  *       
  2.918513e-04  0.001691729  0.9999677  0.001006403          
  3.523297e-04  0.001692768  0.9999677  0.001007262          
  7.970308e-04  0.001701209  0.9999674  0.001013578          
  1.417269e-03  0.001715244  0.9999670  0.001023282          
  2.207814e-03  0.001736674  0.9999663  0.001049036          
  7.159615e-03  0.001942110  0.9999572  0.001348042          
  7.252039e-03  0.001946897  0.9999570  0.001354111          
  7.648764e-03  0.001967779  0.9999559  0.001381306          
  7.718587e-03  0.001971510  0.9999557  0.001386152          
  8.388733e-03  0.002008144  0.9999538  0.001434716          
  1.956688e-02  0.002798030  0.9999027  0.002327082          
  2.499554e-02  0.003268748  0.9998656  0.002772191          
  4.260531e-02  0.004931784  0.9996952  0.004227575          
  3.355449e-01  0.025769740  0.9919485  0.022232843          
  4.152813e-01  0.029566840  0.9894348  0.025535976          
  1.636871e+00  0.057080370  0.9628248  0.049487517          
  1.905145e+00  0.059973744  0.9593620  0.052008261          
  4.043524e+00  0.072285156  0.9437778  0.062597700          
  9.094966e+00  0.081063480  0.9321019  0.070052437          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was lambda = 4.397204e-05.
[1] "Sat Mar 10 03:00:12 2018"
Robust Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  intercept  psi           RMSE         Rsquared  MAE           Selected
  FALSE      psi.huber     0.001387053  0.999974  0.0004031058  *       
  FALSE      psi.hampel    0.001387054  0.999974  0.0004030769          
  FALSE      psi.bisquare  0.001387054  0.999974  0.0004030769          
   TRUE      psi.huber     0.001387059  0.999974  0.0004031105          
   TRUE      psi.hampel    0.001387054  0.999974  0.0004030769          
   TRUE      psi.bisquare  0.001387054  0.999974  0.0004030769          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were intercept = FALSE and psi = psi.huber.
[1] "Sat Mar 10 03:00:26 2018"
CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  cp          RMSE        Rsquared   MAE         Selected
  0.00000000  0.09121118  0.9083358  0.07934677          
  0.01442910  0.09121118  0.9083358  0.07934677  *       
  0.08611735  0.13421358  0.7845923  0.11272629          
  0.09696393  0.13421358  0.7845923  0.11272629          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.0144291.
[1] "Sat Mar 10 03:00:39 2018"
CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results:

  RMSE        Rsquared   MAE       
  0.09121118  0.9083358  0.07934677

[1] "Sat Mar 10 03:00:54 2018"
CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  maxdepth  RMSE        Rsquared   MAE         Selected
  1         0.14707644  0.7505821  0.12477158          
  2         0.12530983  0.8174809  0.10389684          
  3         0.09121118  0.9083358  0.07934677  *       
  4         0.09121118  0.9083358  0.07934677          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was maxdepth = 3.
[1] "Sat Mar 10 03:01:08 2018"
Quantile Regression with LASSO penalty 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  lambda        RMSE         Rsquared  MAE           Selected
  4.397204e-05  0.001387054  0.999974  0.0004030769          
  2.918513e-04  0.001387054  0.999974  0.0004030769          
  3.523297e-04  0.001387054  0.999974  0.0004030769          
  7.970308e-04  0.001387054  0.999974  0.0004030769          
  1.417269e-03  0.001387054  0.999974  0.0004030769          
  2.207814e-03  0.001387054  0.999974  0.0004030769          
  7.159615e-03  0.001387054  0.999974  0.0004030769          
  7.252039e-03  0.001387054  0.999974  0.0004030769          
  7.648764e-03  0.001387054  0.999974  0.0004030769          
  7.718587e-03  0.001387054  0.999974  0.0004030769          
  8.388733e-03  0.001387054  0.999974  0.0004030769          
  1.956688e-02  0.001387054  0.999974  0.0004030769          
  2.499554e-02  0.001387054  0.999974  0.0004030769          
  4.260531e-02  0.001387054  0.999974  0.0004030769  *       
  3.355449e-01  0.290270522       NaN  0.2523661026          
  4.152813e-01  0.290270522       NaN  0.2523661026          
  1.636871e+00  0.290270522       NaN  0.2523661026          
  1.905145e+00  0.290270522       NaN  0.2523661026          
  4.043524e+00  0.290270522       NaN  0.2523661026          
  9.094966e+00  0.290270522       NaN  0.2523661026          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was lambda = 0.04260531.
[1] "Sat Mar 10 03:01:22 2018"
Non-Convex Penalized Quantile Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  lambda        penalty  RMSE         Rsquared  MAE           Selected
  4.397204e-05  MCP      0.001387054  0.999974  0.0004030769          
  2.918513e-04  SCAD     0.001387054  0.999974  0.0004030769          
  3.523297e-04  SCAD     0.001387054  0.999974  0.0004030769          
  7.970308e-04  MCP      0.001387054  0.999974  0.0004030769          
  1.417269e-03  SCAD     0.001387054  0.999974  0.0004030769          
  2.207814e-03  SCAD     0.001387054  0.999974  0.0004030769          
  7.159615e-03  SCAD     0.001387054  0.999974  0.0004030769          
  7.252039e-03  SCAD     0.001387054  0.999974  0.0004030769          
  7.648764e-03  SCAD     0.001387054  0.999974  0.0004030769          
  7.718587e-03  SCAD     0.001387054  0.999974  0.0004030769          
  8.388733e-03  SCAD     0.001387054  0.999974  0.0004030769          
  1.956688e-02  SCAD     0.001387054  0.999974  0.0004030769          
  2.499554e-02  SCAD     0.001387054  0.999974  0.0004030769          
  4.260531e-02  SCAD     0.001387054  0.999974  0.0004030769  *       
  3.355449e-01  SCAD     0.290270522       NaN  0.2523661026          
  4.152813e-01  MCP      0.290270522       NaN  0.2523661026          
  1.636871e+00  MCP      0.290270522       NaN  0.2523661026          
  1.905145e+00  MCP      0.290270522       NaN  0.2523661026          
  4.043524e+00  SCAD     0.290270522       NaN  0.2523661026          
  9.094966e+00  SCAD     0.290270522       NaN  0.2523661026          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 0.04260531 and penalty = SCAD.
[1] "Sat Mar 10 03:01:37 2018"
Regularized Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  mtry  coefReg    coefImp     RMSE        Rsquared   MAE         Selected
  1     0.1717774  0.77650359  0.24807220  0.7466643  0.20865833          
  3     0.3549099  0.69717311  0.11859770  0.9146734  0.08893261          
  3     0.8241333  0.75843493  0.12115013  0.9211273  0.09162707          
  3     0.8283176  0.94648649  0.12173791  0.9160000  0.09013174          
  4     0.6323220  0.37307923  0.09713358  0.9448701  0.07144760          
  4     0.6960017  0.99241649  0.09847000  0.9446104  0.07376413          
  5     0.5090025  0.07887242  0.07335736  0.9655235  0.05262531          
  5     0.7004572  0.48758068  0.07522983  0.9643048  0.05432458          
  5     0.7916163  0.75909291  0.07840640  0.9621866  0.05665047          
  5     0.8497150  0.97297557  0.06985404  0.9670769  0.05117678          
  5     0.9164995  0.68491304  0.07061140  0.9678160  0.05045774          
  5     0.9436871  0.90051267  0.07624119  0.9618479  0.05425605          
  6     0.5653442  0.77527675  0.05310401  0.9810673  0.03893303          
  6     0.6697445  0.87219586  0.05085479  0.9821839  0.03740089          
  7     0.4621058  0.16919903  0.04328798  0.9854749  0.03250918          
  7     0.7657776  0.73832121  0.03991366  0.9870834  0.03063673          
  8     0.4041572  0.71063127  0.02839243  0.9929758  0.02193300          
  8     0.4719737  0.53037089  0.02981249  0.9918490  0.02261171          
  9     0.6793232  0.54029545  0.01921357  0.9960416  0.01556219          
  9     0.9178932  0.47294181  0.01835169  0.9965582  0.01480543  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 9, coefReg = 0.9178932
 and coefImp = 0.4729418.
[1] "Sat Mar 10 03:02:03 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 20 warnings (use warnings() to see them)
Regularized Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  mtry  coefReg    RMSE        Rsquared   MAE         Selected
  1     0.1717774  0.21219888  0.6924800  0.17522533          
  3     0.3549099  0.12365796  0.9184956  0.09340167          
  3     0.8241333  0.12499543  0.9181911  0.09464013          
  3     0.8283176  0.12401257  0.9167015  0.09381091          
  4     0.6323220  0.09232327  0.9504906  0.06777665          
  4     0.6960017  0.09520083  0.9453723  0.06910429          
  5     0.5090025  0.07494539  0.9657846  0.05495618          
  5     0.7004572  0.07437215  0.9648122  0.05364410          
  5     0.7916163  0.07325871  0.9673467  0.05384694          
  5     0.8497150  0.07764778  0.9637062  0.05567927          
  5     0.9164995  0.07600697  0.9646783  0.05488029          
  5     0.9436871  0.07141894  0.9685360  0.05208067          
  6     0.5653442  0.05772309  0.9770726  0.04222832          
  6     0.6697445  0.05635967  0.9783490  0.04146989          
  7     0.4621058  0.04358726  0.9849950  0.03220006          
  7     0.7657776  0.04385238  0.9852397  0.03253155          
  8     0.4041572  0.03251562  0.9909461  0.02524240          
  8     0.4719737  0.03291282  0.9910028  0.02457105          
  9     0.6793232  0.01846955  0.9964646  0.01502332  *       
  9     0.9178932  0.01928953  0.9962321  0.01558515          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 9 and coefReg = 0.6793232.
[1] "Sat Mar 10 03:02:22 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:02:41 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "rvmLinear"               
Error in .local(object, ...) : test vector does not match model !
In addition: There were 23 warnings (use warnings() to see them)
Error in .local(object, ...) : test vector does not match model !
In addition: Warning messages:
1: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
2: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
3: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
4: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
5: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
6: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
7: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
8: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
9: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
Error in .local(object, ...) : test vector does not match model !
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:03:21 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "rvmPoly"                 
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:03:45 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "rvmRadial"               
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |======                                                                |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  12%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  26%  |                                                                              |====================                                                  |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  40%  |                                                                              |=============================                                         |  42%  |                                                                              |===============================                                       |  44%  |                                                                              |================================                                      |  46%  |                                                                              |==================================                                    |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  52%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  56%  |                                                                              |=========================================                             |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |===========================================                           |  62%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |================================================                      |  68%  |                                                                              |=================================================                     |  70%  |                                                                              |==================================================                    |  72%  |                                                                              |====================================================                  |  74%  |                                                                              |=====================================================                 |  76%  |                                                                              |=======================================================               |  78%  |                                                                              |========================================================              |  80%  |                                                                              |=========================================================             |  82%  |                                                                              |===========================================================           |  84%  |                                                                              |============================================================          |  86%  |                                                                              |==============================================================        |  88%  |                                                                              |===============================================================       |  90%  |                                                                              |================================================================      |  92%  |                                                                              |==================================================================    |  94%  |                                                                              |===================================================================   |  96%  |                                                                              |===================================================================== |  98%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |===========                                                           |  15%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  19%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  23%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  27%  |                                                                              |====================                                                  |  29%  |                                                                              |======================                                                |  31%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |===============================                                       |  44%  |                                                                              |================================                                      |  46%  |                                                                              |==================================                                    |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  52%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  56%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  69%  |                                                                              |==================================================                    |  71%  |                                                                              |===================================================                   |  73%  |                                                                              |====================================================                  |  75%  |                                                                              |======================================================                |  77%  |                                                                              |=======================================================               |  79%  |                                                                              |=========================================================             |  81%  |                                                                              |==========================================================            |  83%  |                                                                              |===========================================================           |  85%  |                                                                              |=============================================================         |  87%  |                                                                              |==============================================================        |  88%  |                                                                              |===============================================================       |  90%  |                                                                              |=================================================================     |  92%  |                                                                              |==================================================================    |  94%  |                                                                              |===================================================================   |  96%  |                                                                              |===================================================================== |  98%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |======                                                                |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  12%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  26%  |                                                                              |====================                                                  |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  40%  |                                                                              |=============================                                         |  42%  |                                                                              |===============================                                       |  44%  |                                                                              |================================                                      |  46%  |                                                                              |==================================                                    |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  52%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  56%  |                                                                              |=========================================                             |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |===========================================                           |  62%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |================================================                      |  68%  |                                                                              |=================================================                     |  70%  |                                                                              |==================================================                    |  72%  |                                                                              |====================================================                  |  74%  |                                                                              |=====================================================                 |  76%  |                                                                              |=======================================================               |  78%  |                                                                              |========================================================              |  80%  |                                                                              |=========================================================             |  82%  |                                                                              |===========================================================           |  84%  |                                                                              |============================================================          |  86%  |                                                                              |==============================================================        |  88%  |                                                                              |===============================================================       |  90%  |                                                                              |================================================================      |  92%  |                                                                              |==================================================================    |  94%  |                                                                              |===================================================================   |  96%  |                                                                              |===================================================================== |  98%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%  |                                                                              |=================================================                     |  70%  |                                                                              |==================================================                    |  71%  |                                                                              |===================================================                   |  72%  |                                                                              |====================================================                  |  74%  |                                                                              |====================================================                  |  75%  |                                                                              |=====================================================                 |  76%  |                                                                              |======================================================                |  78%  |                                                                              |=======================================================               |  79%  |                                                                              |========================================================              |  80%  |                                                                              |=========================================================             |  82%  |                                                                              |==========================================================            |  83%  |                                                                              |===========================================================           |  84%  |                                                                              |============================================================          |  86%  |                                                                              |=============================================================         |  87%  |                                                                              |==============================================================        |  88%  |                                                                              |===============================================================       |  89%  |                                                                              |================================================================      |  91%  |                                                                              |================================================================      |  92%  |                                                                              |=================================================================     |  93%  |                                                                              |==================================================================    |  95%  |                                                                              |===================================================================   |  96%  |                                                                              |====================================================================  |  97%  |                                                                              |===================================================================== |  99%  |                                                                              |======================================================================| 100%
Error in confusionMatrix.default(p[, 1], p[, 2]) : 
  The data must contain some levels that overlap the reference.
In addition: There were 31 warnings (use warnings() to see them)
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%  |                                                                              |=================================================                     |  70%  |                                                                              |==================================================                    |  71%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%  |                                                                              |=================================================                     |  70%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%  |                                                                              |=================================================                     |  70%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%  |                                                                              |=================================================                     |  70%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%  |                                                                              |=================================================                     |  70%  |                                                                              |==================================================                    |  71%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  55%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%  |                                                                              |=================================================                     |  70%  |                                                                              |==================================================                    |  71%  |                                                                              |===================================================                   |  72%  |                                                                              |====================================================                  |  74%  |                                                                              |====================================================                  |  75%  |                                                                              |=====================================================                 |  76%  |                                                                              |======================================================                |  78%  |                                                                              |=======================================================               |  79%  |                                                                              |========================================================              |  80%  |                                                                              |=========================================================             |  82%  |                                                                              |==========================================================            |  83%  |                                                                              |===========================================================           |  84%  |                                                                              |============================================================          |  86%  |                                                                              |=============================================================         |  87%  |                                                                              |==============================================================        |  88%  |                                                                              |===============================================================       |  89%  |                                                                              |================================================================      |  91%  |                                                                              |================================================================      |  92%  |                                                                              |=================================================================     |  93%  |                                                                              |==================================================================    |  95%  |                                                                              |===================================================================   |  96%  |                                                                              |====================================================================  |  97%  |                                                                              |===================================================================== |  99%  |                                                                              |======================================================================| 100%
Subtractive Clustering and Fuzzy c-Means Rules 

76 samples
 9 predictor

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 76, 76, 76, 76, 76, 76, ... 
Resampling results across tuning parameters:

  r.a  RMSE       Rsquared   MAE        Selected
  0.0        NaN        NaN        NaN          
  0.5  0.2287217  0.4237958  0.1830346          
  1.0  0.1992927  0.5291306  0.1619963  *       

Tuning parameter 'eps.high' was held constant at a value of 0.5

Tuning parameter 'eps.low' was held constant at a value of 0
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were r.a = 1, eps.high = 0.5 and eps.low
 = 0.
[1] "Sat Mar 10 03:05:44 2018"
Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  ncomp  RMSE         Rsquared   MAE          Selected
  1      0.082395168  0.9239946  0.070146270          
  2      0.030379072  0.9889749  0.024511193          
  3      0.011799208  0.9983129  0.010151307          
  4      0.003586359  0.9998485  0.002917955          
  5      0.001842350  0.9999614  0.001228023          
  7      0.001686104  0.9999679  0.001001770  *       
  8      0.001687072  0.9999678  0.001003645          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 7.
[1] "Sat Mar 10 03:05:59 2018"
Error in mvrValstats(object = object, estimate = "train") : 
  could not find function "mvrValstats"
In addition: There were 37 warnings (use warnings() to see them)
Spike and Slab Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  vars  RMSE         Rsquared   MAE           Selected
  1     0.001441887  0.9999740  0.0006686751          
  3     0.001417221  0.9999742  0.0006043393  *       
  4     0.001424470  0.9999739  0.0006102815          
  5     0.001429981  0.9999738  0.0006233767          
  6     0.001453854  0.9999734  0.0006702932          
  7     0.001498221  0.9999724  0.0007433924          
  8     0.001521374  0.9999720  0.0007799834          
  9     0.001541423  0.9999714  0.0008014143          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was vars = 3.
[1] "Sat Mar 10 03:06:24 2018"
Sparse Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  kappa       eta        K  RMSE         Rsquared   MAE           Selected
  0.05359805  0.1717774  7  0.001685431  0.9999679  0.0010039405          
  0.12209680  0.8241333  7  0.001687167  0.9999677  0.0010096292          
  0.12891243  0.8283176  9  0.001687096  0.9999678  0.0010036809          
  0.15845626  0.3549099  7  0.001685431  0.9999679  0.0010039405          
  0.17928770  0.6323220  4  0.001637998  0.9999686  0.0010174184          
  0.19533021  0.6960017  9  0.001687096  0.9999678  0.0010036809          
  0.23790747  0.7916163  7  0.001687167  0.9999677  0.0010096292          
  0.23837168  0.9164995  7  0.001673572  0.9999681  0.0009991040          
  0.24029927  0.7004572  5  0.001655542  0.9999689  0.0009789356          
  0.24062815  0.9436871  9  0.001687096  0.9999678  0.0010036809          
  0.24364136  0.5090025  1  0.001406252  0.9999740  0.0004817503  *       
  0.27429347  0.8497150  9  0.001687096  0.9999678  0.0010036809          
  0.28315521  0.5653442  7  0.001685431  0.9999679  0.0010039405          
  0.30245531  0.6697445  8  0.001687061  0.9999678  0.0010036419          
  0.37714589  0.7657776  7  0.001687167  0.9999677  0.0010096292          
  0.38486186  0.4621058  2  0.009416467  0.9987788  0.0081372623          
  0.43450120  0.4041572  7  0.001685431  0.9999679  0.0010039405          
  0.43999400  0.4719737  5  0.001717129  0.9999664  0.0010459626          
  0.46723000  0.6793232  5  0.001667148  0.9999683  0.0009730000          
  0.49656676  0.9178932  5  0.001632179  0.9999688  0.0009674687          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were K = 1, eta = 0.5090025 and kappa
 = 0.2436414.
[1] "Sat Mar 10 03:06:39 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: Warning messages:
1: predictions failed for Fold1: threshold=0.9931, n.components=3 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
2: predictions failed for Fold2: threshold=0.9931, n.components=3 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
3: predictions failed for Fold3: threshold=0.9931, n.components=3 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Error in rowMeans(x) : 'x' must be an array of at least two dimensions
In addition: Warning messages:
1: predictions failed for Fold3: threshold=0.8399, n.components=3 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
2: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Error in rowMeans(x) : 'x' must be an array of at least two dimensions
In addition: Warning messages:
1: predictions failed for Resample03: n.components=3, threshold=0.9 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
2: predictions failed for Resample12: n.components=3, threshold=0.9 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:06:59 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "superpc"                 
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:07:12 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "svmBoundrangeString"     
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:07:25 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "svmExpoString"           
Support Vector Machines with Linear Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  C             RMSE        Rsquared   MAE         Selected
    0.09525518  0.02270481  0.9993256  0.01923655          
    0.39581657  0.02036160  0.9996630  0.01751208  *       
    0.45608450  0.02036160  0.9996630  0.01751208          
    0.84304862  0.02036160  0.9996630  0.01751208          
    1.30010753  0.02036160  0.9996630  0.01751208          
    1.81492039  0.02036160  0.9996630  0.01751208          
    4.39914806  0.02036160  0.9996630  0.01751208          
    4.44181837  0.02036160  0.9996630  0.01751208          
    4.62347677  0.02036160  0.9996630  0.01751208          
    4.65520437  0.02036160  0.9996630  0.01751208          
    4.95622222  0.02036160  0.9996630  0.01751208          
    9.37490448  0.02036160  0.9996630  0.01751208          
   11.27188031  0.02036160  0.9996630  0.01751208          
   16.83812131  0.02036160  0.9996630  0.01751208          
   79.58235231  0.02036160  0.9996630  0.01751208          
   93.43271457  0.02036160  0.9996630  0.01751208          
  262.29311446  0.02036160  0.9996630  0.01751208          
  294.03009015  0.02036160  0.9996630  0.01751208          
  518.03296622  0.02036160  0.9996630  0.01751208          
  953.44290681  0.02036160  0.9996630  0.01751208          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was C = 0.3958166.
[1] "Sat Mar 10 03:07:40 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Linear Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  cost          RMSE        Rsquared   MAE         Selected
    0.09525518  0.02270481  0.9993256  0.01923655          
    0.39581657  0.02036160  0.9996630  0.01751208  *       
    0.45608450  0.02036160  0.9996630  0.01751208          
    0.84304862  0.02036160  0.9996630  0.01751208          
    1.30010753  0.02036160  0.9996630  0.01751208          
    1.81492039  0.02036160  0.9996630  0.01751208          
    4.39914806  0.02036160  0.9996630  0.01751208          
    4.44181837  0.02036160  0.9996630  0.01751208          
    4.62347677  0.02036160  0.9996630  0.01751208          
    4.65520437  0.02036160  0.9996630  0.01751208          
    4.95622222  0.02036160  0.9996630  0.01751208          
    9.37490448  0.02036160  0.9996630  0.01751208          
   11.27188031  0.02036160  0.9996630  0.01751208          
   16.83812131  0.02036160  0.9996630  0.01751208          
   79.58235231  0.02036160  0.9996630  0.01751208          
   93.43271457  0.02036160  0.9996630  0.01751208          
  262.29311446  0.02036160  0.9996630  0.01751208          
  294.03009015  0.02036160  0.9996630  0.01751208          
  518.03296622  0.02036160  0.9996630  0.01751208          
  953.44290681  0.02036160  0.9996630  0.01751208          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cost = 0.3958166.
[1] "Sat Mar 10 03:07:55 2018"
L2 Regularized Support Vector Machine (dual) with Linear Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  cost          Loss  RMSE        Rsquared   MAE         Selected
  4.316034e-03  L1    0.35459844  0.3781135  0.28969808          
  2.883314e-02  L2    0.17999387  0.8972202  0.15610479          
  3.483056e-02  L2    0.16469831  0.9291419  0.14334508          
  7.901390e-02  L1    0.20385343  0.8213882  0.17715470          
  1.407794e-01  L2    0.08806958  0.9853189  0.07550420          
  2.196393e-01  L2    0.07955123  0.9888904  0.06809364          
  7.151406e-01  L2    0.07173317  0.9929635  0.06176931          
  7.244044e-01  L2    0.07300031  0.9916013  0.06278580          
  7.641728e-01  L2    0.07253615  0.9929452  0.06253415          
  7.711728e-01  L2    0.07134855  0.9923475  0.06143731          
  8.383674e-01  L2    0.07133615  0.9929665  0.06104867          
  1.961203e+00  L2    0.07146888  0.9931385  0.06180794          
  2.507428e+00  L2    0.07169147  0.9916952  0.06186438          
  4.281785e+00  L2    0.07238224  0.9920074  0.06217538          
  3.396167e+01  L2    0.07124526  0.9929356  0.06139502          
  4.206284e+01  L1    0.08907061  0.9799955  0.07572485          
  1.665773e+02  L1    0.08865587  0.9801020  0.07535616          
  1.939794e+02  L1    0.08863604  0.9801071  0.07533852          
  4.127717e+02  L2    0.07072580  0.9941891  0.06115280  *       
  9.310213e+02  L2    0.07289421  0.9930177  0.06255235          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were cost = 412.7717 and Loss = L2.
[1] "Sat Mar 10 03:08:09 2018"
Support Vector Machines with Polynomial Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  degree  scale         C             RMSE        Rsquared   MAE       
  1       8.139487e-05  100.25663094  0.18694430  0.9425985  0.16254542
  1       7.610056e-04   43.94407267  0.02933740  0.9986012  0.02489961
  1       2.337519e-01   83.08545384  0.02037186  0.9996570  0.01752454
  1       2.460008e-01  587.03280182  0.02036573  0.9996587  0.01751820
  2       4.991575e-03    0.07095687  0.28076469  0.8991111  0.24379654
  2       9.929023e-03   98.98590804  0.02536790  0.9988468  0.02110763
  2       2.248836e-02    1.51171732  0.03069358  0.9980128  0.02569611
  2       3.550879e-02  271.14822108  0.03154101  0.9971997  0.02579394
  2       4.892443e-02  946.36130678  0.03399148  0.9965235  0.02773948
  2       5.165883e-02    4.97160017  0.03448992  0.9963799  0.02813670
  2       1.571744e-01   83.65579619  0.04815342  0.9898263  0.03912767
  2       3.194218e-01  773.16553325  0.06477858  0.9734880  0.05192482
  2       7.217576e-01   38.68482034  0.09078164  0.9245879  0.07049144
  2       1.005804e+00  363.97359528  0.10625108  0.8852333  0.08178786
  3       1.388191e-03   50.54403677  0.02246037  0.9992460  0.01916007
  3       2.816027e-03    0.18149295  0.27012223  0.9118496  0.23477324
  3       3.176483e-03    7.75734684  0.02635065  0.9989450  0.02226807
  3       3.991284e-02    8.60056719  0.04074563  0.9943209  0.03330364
  3       1.146596e-01   67.40660698  0.06433782  0.9762027  0.05210571
  3       7.341412e-01    4.26967766  0.13364753  0.8033947  0.11191240
  Selected
          
          
          
  *       
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 1, scale = 0.2460008 and C
 = 587.0328.
[1] "Sat Mar 10 03:08:23 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Support Vector Machines with Radial Basis Function Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  sigma       C             RMSE        Rsquared   MAE         Selected
  0.03391969  218.52566887  0.05770998  0.9778232  0.04377246  *       
  0.03814609    0.99969239  0.06816082  0.9729375  0.05250062          
  0.04166508    0.24272810  0.17673305  0.9240145  0.15326919          
  0.04223698    0.05806884  0.26128475  0.8856915  0.22729185          
  0.07400096  100.89940587  0.09762648  0.9243354  0.07503337          
  0.09101842    1.15730114  0.11352668  0.8962724  0.08746209          
  0.09755760    0.33840819  0.16076419  0.8734513  0.13367264          
  0.09948856    8.82539025  0.11990565  0.8826602  0.09251627          
  0.10221405   19.46495307  0.12212589  0.8781621  0.09429347          
  0.11274654    6.35703499  0.13016796  0.8609308  0.10076017          
  0.12297189    2.83891452  0.13727208  0.8442557  0.10638998          
  0.14572124   32.65249246  0.15212033  0.8071008  0.11854121          
  0.14957689  311.08836639  0.15451887  0.8007125  0.12058479          
  0.16808092   41.81887145  0.16550214  0.7708169  0.12994445          
  0.17632513  469.95790567  0.17004571  0.7578835  0.13390778          
  0.18249816   25.78258505  0.17333968  0.7482632  0.13694528          
  0.20765256  830.04356907  0.18606937  0.7085456  0.14879832          
  0.22776118  472.83913338  0.19539967  0.6762303  0.15780149          
  0.24251694  628.44237566  0.20171613  0.6533837  0.16408515          
  0.27178609    3.18162353  0.21305143  0.6100877  0.17561053          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were sigma = 0.03391969 and C = 218.5257.
[1] "Sat Mar 10 03:08:39 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  C             RMSE        Rsquared   MAE         Selected
    0.09525518  0.24250061  0.8637731  0.21064574          
    0.39581657  0.13266293  0.9202207  0.10818754          
    0.45608450  0.11916821  0.9317177  0.09446487          
    0.84304862  0.09355361  0.9419709  0.07171988          
    1.30010753  0.08833008  0.9414581  0.06721991          
    1.81492039  0.09841273  0.9249483  0.07516818          
    4.39914806  0.09794585  0.9246512  0.07463762          
    4.44181837  0.09215035  0.9335100  0.07071675          
    4.62347677  0.09735460  0.9234871  0.07512641          
    4.65520437  0.09241055  0.9306919  0.07147722          
    4.95622222  0.09493227  0.9292815  0.07280481          
    9.37490448  0.09477179  0.9283241  0.07304143          
   11.27188031  0.08628209  0.9407714  0.06646575          
   16.83812131  0.09028925  0.9340729  0.06974469          
   79.58235231  0.08959431  0.9373712  0.06875131          
   93.43271457  0.09139335  0.9329926  0.07053648          
  262.29311446  0.08664769  0.9403218  0.06658716          
  294.03009015  0.08518543  0.9443516  0.06519604  *       
  518.03296622  0.09681055  0.9274953  0.07387933          
  953.44290681  0.08694335  0.9404578  0.06685664          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was C = 294.0301.
[1] "Sat Mar 10 03:08:54 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  sigma       C             RMSE        Rsquared   MAE         Selected
  0.03391969  218.52566887  0.05770998  0.9778232  0.04377246  *       
  0.03814609    0.99969239  0.06816082  0.9729375  0.05250062          
  0.04166508    0.24272810  0.17673305  0.9240145  0.15326919          
  0.04223698    0.05806884  0.26128475  0.8856915  0.22729185          
  0.07400096  100.89940587  0.09762648  0.9243354  0.07503337          
  0.09101842    1.15730114  0.11352668  0.8962724  0.08746209          
  0.09755760    0.33840819  0.16076419  0.8734513  0.13367264          
  0.09948856    8.82539025  0.11990565  0.8826602  0.09251627          
  0.10221405   19.46495307  0.12212589  0.8781621  0.09429347          
  0.11274654    6.35703499  0.13016796  0.8609308  0.10076017          
  0.12297189    2.83891452  0.13727208  0.8442557  0.10638998          
  0.14572124   32.65249246  0.15212033  0.8071008  0.11854121          
  0.14957689  311.08836639  0.15451887  0.8007125  0.12058479          
  0.16808092   41.81887145  0.16550214  0.7708169  0.12994445          
  0.17632513  469.95790567  0.17004571  0.7578835  0.13390778          
  0.18249816   25.78258505  0.17333968  0.7482632  0.13694528          
  0.20765256  830.04356907  0.18606937  0.7085456  0.14879832          
  0.22776118  472.83913338  0.19539967  0.6762303  0.15780149          
  0.24251694  628.44237566  0.20171613  0.6533837  0.16408515          
  0.27178609    3.18162353  0.21305143  0.6100877  0.17561053          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were sigma = 0.03391969 and C = 218.5257.
[1] "Sat Mar 10 03:09:08 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:09:22 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "svmSpectrumString"       
Bagged CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results:

  RMSE        Rsquared   MAE       
  0.05402405  0.9721326  0.04449277

[1] "Sat Mar 10 03:09:37 2018"
Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 51, 50, 51 
Resampling results across tuning parameters:

  ncomp  RMSE         Rsquared   MAE          Selected
  1      0.082395168  0.9239946  0.070146270          
  2      0.030379072  0.9889749  0.024511193          
  3      0.011799208  0.9983129  0.010151307          
  4      0.003586359  0.9998485  0.002917955          
  5      0.001842350  0.9999614  0.001228023          
  7      0.001686104  0.9999679  0.001001770  *       
  8      0.001687072  0.9999678  0.001003645          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 7.
[1] "Sat Mar 10 03:10:07 2018"
Error in mvrValstats(object = object, estimate = "train") : 
  could not find function "mvrValstats"
Error : package xgboost is required
Error : package xgboost is required
Error : package xgboost is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:10:23 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "xgbDART"                 
Error : package xgboost is required
Error : package xgboost is required
Error : package xgboost is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:10:37 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "xgbLinear"               
Error : package xgboost is required
Error : package xgboost is required
Error : package xgboost is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:10:51 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "xgbTree"                 
Error : package kohonen is required
Error : package kohonen is required
Error : package kohonen is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:11:05 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "xyf"                     
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  No variation for for: V11, V12
Fitting Repeat 1 

# weights:  166
initial  value 58.443743 
iter  10 value 45.726292
iter  20 value 44.880591
iter  30 value 30.752075
iter  40 value 29.912113
iter  50 value 29.756588
iter  60 value 29.653959
iter  70 value 29.190131
iter  80 value 28.871394
iter  90 value 28.837802
iter 100 value 28.822862
final  value 28.822862 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 31.716814 
iter  10 value 23.766439
iter  20 value 23.459638
iter  30 value 21.380970
iter  40 value 17.605637
iter  50 value 17.100451
iter  60 value 16.967622
iter  70 value 16.918941
iter  80 value 16.898188
iter  90 value 16.888273
iter 100 value 16.883797
final  value 16.883797 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  166
initial  value 36.823877 
iter  10 value 27.599911
iter  20 value 27.525120
iter  30 value 20.961545
iter  40 value 18.016919
iter  50 value 17.827231
iter  60 value 17.761974
iter  70 value 17.696657
iter  80 value 17.679372
iter  90 value 17.667419
iter 100 value 17.650652
final  value 17.650652 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  166
initial  value 32.936490 
iter  10 value 30.499737
iter  20 value 27.687197
iter  30 value 23.159743
iter  40 value 22.550719
iter  50 value 22.463679
iter  60 value 22.434193
iter  70 value 22.412854
iter  80 value 22.400360
iter  90 value 22.392359
iter 100 value 22.386268
final  value 22.386268 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 34.474726 
iter  10 value 27.516157
iter  20 value 27.504532
iter  30 value 27.497172
iter  40 value 27.463831
iter  50 value 26.161655
iter  60 value 22.429991
iter  70 value 22.091926
iter  80 value 22.072131
iter  90 value 22.065256
iter 100 value 22.052622
final  value 22.052622 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  45
initial  value 50.482356 
iter  10 value 31.635091
iter  20 value 30.502455
iter  30 value 24.865455
iter  40 value 24.399565
iter  50 value 24.392969
iter  60 value 24.391202
iter  70 value 24.391003
iter  80 value 24.390990
final  value 24.390984 
converged
Fitting Repeat 2 

# weights:  45
initial  value 38.945809 
iter  10 value 31.421930
iter  20 value 25.433614
iter  30 value 24.396112
iter  40 value 24.392637
iter  50 value 24.391189
iter  60 value 24.390996
iter  70 value 24.390987
final  value 24.390985 
converged
Fitting Repeat 3 

# weights:  45
initial  value 58.493597 
iter  10 value 31.678004
iter  20 value 30.221125
iter  30 value 24.807745
iter  40 value 24.420532
iter  50 value 24.401242
iter  60 value 24.393327
iter  70 value 24.391492
iter  80 value 24.391012
iter  90 value 24.390990
final  value 24.390989 
converged
Fitting Repeat 4 

# weights:  45
initial  value 38.884734 
iter  10 value 31.417924
iter  20 value 30.349474
iter  30 value 24.705684
iter  40 value 24.277560
iter  50 value 24.056933
iter  60 value 23.958073
iter  70 value 23.951953
iter  80 value 23.950253
iter  90 value 23.950220
final  value 23.950216 
converged
Fitting Repeat 5 

# weights:  45
initial  value 39.265095 
iter  10 value 31.435775
iter  20 value 25.421781
iter  30 value 24.392908
iter  40 value 24.391542
iter  50 value 24.391010
iter  60 value 24.390986
final  value 24.390985 
converged
Fitting Repeat 1 

# weights:  12
initial  value 30.782716 
iter  10 value 16.641698
final  value 16.635712 
converged
Fitting Repeat 2 

# weights:  12
initial  value 50.127184 
iter  10 value 38.409673
final  value 38.409581 
converged
Fitting Repeat 3 

# weights:  12
initial  value 30.981838 
iter  10 value 28.060990
final  value 28.060979 
converged
Fitting Repeat 4 

# weights:  12
initial  value 49.244419 
iter  10 value 42.985899
final  value 42.985788 
converged
Fitting Repeat 5 

# weights:  12
initial  value 39.835838 
iter  10 value 30.867410
final  value 30.867406 
converged
Fitting Repeat 1 

# weights:  199
initial  value 29.238869 
iter  10 value 26.529655
iter  20 value 26.528995
iter  30 value 26.528200
iter  40 value 26.527182
iter  50 value 26.525743
iter  60 value 26.523377
iter  70 value 26.518224
iter  80 value 26.494938
iter  90 value 24.242722
iter 100 value 14.954446
final  value 14.954446 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 77.878003 
iter  10 value 38.489011
iter  20 value 38.424878
iter  30 value 31.069078
iter  40 value 27.069708
iter  50 value 26.241700
iter  60 value 26.161059
iter  70 value 26.133152
iter  80 value 26.117441
iter  90 value 26.102994
iter 100 value 26.091818
final  value 26.091818 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 47.685969 
iter  10 value 33.944915
iter  20 value 33.920737
iter  30 value 33.918015
iter  40 value 33.910940
iter  50 value 33.858044
iter  60 value 23.635160
iter  70 value 22.432772
iter  80 value 21.832560
iter  90 value 21.743771
iter 100 value 21.715088
final  value 21.715088 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 57.720998 
iter  10 value 23.739387
iter  20 value 23.738356
iter  30 value 23.736883
iter  40 value 23.734397
iter  50 value 23.728738
iter  60 value 23.701300
iter  70 value 16.338935
iter  80 value 14.248837
iter  90 value 13.284800
iter 100 value 13.235499
final  value 13.235499 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 43.404544 
iter  10 value 31.440116
iter  20 value 31.423337
iter  30 value 31.422547
iter  40 value 31.421430
iter  50 value 31.419627
iter  60 value 31.415985
iter  70 value 31.404051
iter  80 value 30.977412
iter  90 value 19.707652
iter 100 value 18.625264
final  value 18.625264 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  166
initial  value 45.499070 
iter  10 value 28.879566
iter  20 value 21.091670
iter  30 value 15.085513
iter  40 value 14.812124
iter  50 value 14.669875
iter  60 value 14.558283
iter  70 value 14.154561
iter  80 value 14.055463
iter  90 value 14.039982
iter 100 value 14.035444
final  value 14.035444 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 71.933115 
iter  10 value 33.141277
iter  20 value 32.945891
iter  30 value 32.940419
iter  40 value 32.940121
final  value 32.940120 
converged
Fitting Repeat 3 

# weights:  166
initial  value 37.835356 
iter  10 value 17.877544
iter  20 value 17.773404
iter  30 value 17.742354
iter  40 value 17.732236
iter  50 value 17.566656
iter  60 value 17.403842
iter  70 value 17.368060
iter  80 value 17.353688
iter  90 value 17.351986
iter 100 value 17.351347
final  value 17.351347 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  166
initial  value 50.772528 
iter  10 value 30.793684
iter  20 value 30.675800
iter  30 value 26.450131
iter  40 value 23.088250
iter  50 value 23.008661
iter  60 value 22.978451
iter  70 value 22.963741
iter  80 value 22.954325
iter  90 value 22.942153
iter 100 value 22.938402
final  value 22.938402 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 36.779198 
iter  10 value 28.959015
iter  20 value 28.743558
iter  30 value 20.721250
iter  40 value 18.114383
iter  50 value 17.964392
iter  60 value 17.919351
iter  70 value 17.880354
iter  80 value 17.846527
iter  90 value 17.831755
iter 100 value 17.826549
final  value 17.826549 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  199
initial  value 40.216097 
iter  10 value 21.461724
iter  20 value 20.128888
iter  30 value 17.808848
iter  40 value 17.395983
iter  50 value 17.229150
iter  60 value 17.176341
iter  70 value 17.164844
iter  80 value 17.159036
iter  90 value 17.157879
iter 100 value 17.155963
final  value 17.155963 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 56.032922 
iter  10 value 23.740741
iter  20 value 23.665410
iter  30 value 23.434626
iter  40 value 19.752843
iter  50 value 19.248691
iter  60 value 18.982378
iter  70 value 18.948527
iter  80 value 18.925606
iter  90 value 18.920085
iter 100 value 18.916104
final  value 18.916104 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 61.916365 
iter  10 value 46.139307
iter  20 value 45.163410
iter  30 value 34.371790
iter  40 value 33.946079
iter  50 value 33.896632
iter  60 value 33.870597
iter  70 value 33.860323
iter  80 value 33.856826
iter  90 value 33.854713
iter 100 value 33.853659
final  value 33.853659 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 16.989487 
iter  10 value 10.474555
iter  20 value 8.695940
iter  30 value 8.530618
iter  40 value 8.208768
iter  50 value 8.195206
iter  60 value 8.186061
iter  70 value 8.183486
iter  80 value 8.181952
iter  90 value 8.181874
iter 100 value 8.181848
final  value 8.181848 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 34.348658 
iter  10 value 17.435801
iter  20 value 9.574905
iter  30 value 9.218224
iter  40 value 9.186907
iter  50 value 9.169954
iter  60 value 9.165107
iter  70 value 9.164561
iter  80 value 9.164191
iter  90 value 9.164150
iter 100 value 9.164137
final  value 9.164137 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 42.902923 
iter  10 value 31.718161
final  value 31.703420 
converged
Fitting Repeat 2 

# weights:  177
initial  value 70.814743 
iter  10 value 31.811154
iter  20 value 31.703420
iter  20 value 31.703420
iter  20 value 31.703420
final  value 31.703420 
converged
Fitting Repeat 3 

# weights:  177
initial  value 58.440904 
iter  10 value 31.946262
iter  20 value 31.703421
final  value 31.703420 
converged
Fitting Repeat 4 

# weights:  177
initial  value 48.129613 
iter  10 value 31.806257
iter  20 value 31.703420
final  value 31.703420 
converged
Fitting Repeat 5 

# weights:  177
initial  value 41.695243 
iter  10 value 31.704317
final  value 31.703420 
converged
Fitting Repeat 1 

# weights:  34
initial  value 36.457002 
iter  10 value 10.314781
iter  20 value 10.313427
iter  30 value 10.311693
iter  40 value 10.309431
iter  50 value 10.306392
iter  60 value 10.302098
iter  70 value 10.295522
iter  80 value 10.284039
iter  90 value 10.258614
iter 100 value 10.160291
final  value 10.160291 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  34
initial  value 32.650645 
iter  10 value 16.849435
iter  20 value 16.848376
iter  30 value 16.770450
iter  40 value 14.267052
iter  50 value 14.097655
iter  60 value 14.091723
iter  70 value 14.086003
iter  80 value 14.070934
iter  90 value 14.044792
iter 100 value 14.037618
final  value 14.037618 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  34
initial  value 46.524605 
iter  10 value 39.080521
iter  20 value 38.983448
iter  30 value 26.888730
iter  40 value 25.783430
iter  50 value 25.450955
iter  60 value 24.944922
iter  70 value 24.924370
iter  80 value 24.916586
iter  90 value 24.912287
iter 100 value 24.908965
final  value 24.908965 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  34
initial  value 56.237585 
iter  10 value 49.013006
iter  20 value 49.012820
iter  30 value 49.012606
iter  40 value 49.012351
iter  50 value 49.012039
iter  60 value 49.011643
iter  70 value 49.011117
iter  80 value 49.010377
iter  90 value 49.009249
iter 100 value 49.007314
final  value 49.007314 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  34
initial  value 57.417622 
iter  10 value 35.250764
iter  20 value 33.113533
iter  30 value 33.062686
iter  40 value 32.806830
iter  50 value 32.605166
iter  60 value 32.586020
iter  70 value 32.569905
iter  80 value 32.541135
iter  90 value 32.535768
iter 100 value 32.526868
final  value 32.526868 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  144
initial  value 31.766938 
iter  10 value 26.392996
iter  20 value 26.391118
iter  30 value 26.390464
iter  40 value 26.389452
iter  50 value 26.387700
iter  60 value 26.384012
iter  70 value 26.372284
iter  80 value 26.168763
iter  90 value 20.907671
iter 100 value 20.077719
final  value 20.077719 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  144
initial  value 75.781846 
iter  10 value 51.056102
iter  20 value 51.054551
iter  30 value 36.451869
iter  40 value 36.135275
iter  50 value 35.940889
iter  60 value 35.689702
iter  70 value 35.638700
iter  80 value 35.628489
iter  90 value 35.625098
iter 100 value 35.622370
final  value 35.622370 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  144
initial  value 36.562799 
iter  10 value 32.733275
iter  20 value 32.732764
iter  30 value 32.731987
iter  40 value 32.730671
iter  50 value 32.728015
iter  60 value 32.720383
iter  70 value 32.646711
iter  80 value 17.980114
iter  90 value 17.332682
iter 100 value 16.971072
final  value 16.971072 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  144
initial  value 20.399061 
iter  10 value 17.461047
iter  20 value 17.450920
iter  30 value 17.381581
iter  40 value 12.652821
iter  50 value 11.226488
iter  60 value 11.199889
iter  70 value 11.175009
iter  80 value 11.138881
iter  90 value 11.124719
iter 100 value 11.117794
final  value 11.117794 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  144
initial  value 26.173156 
iter  10 value 23.003463
iter  20 value 23.003459
iter  30 value 23.003454
iter  40 value 23.003450
iter  50 value 23.003445
iter  60 value 23.003440
iter  70 value 23.003435
iter  80 value 23.003430
iter  90 value 23.003425
iter 100 value 23.003420
final  value 23.003420 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  12
initial  value 41.894129 
iter  10 value 35.334283
final  value 35.334282 
converged
Fitting Repeat 2 

# weights:  12
initial  value 45.406253 
iter  10 value 35.334282
iter  10 value 35.334282
iter  10 value 35.334282
final  value 35.334282 
converged
Fitting Repeat 3 

# weights:  12
initial  value 42.913540 
iter  10 value 35.334283
final  value 35.334282 
converged
Fitting Repeat 4 

# weights:  12
initial  value 40.733997 
iter  10 value 35.334284
final  value 35.334282 
converged
Fitting Repeat 5 

# weights:  12
initial  value 47.741164 
iter  10 value 35.334328
final  value 35.334282 
converged
Fitting Repeat 1 

# weights:  111
initial  value 154.261274 
iter  10 value 36.833585
final  value 36.791278 
converged
Fitting Repeat 2 

# weights:  111
initial  value 154.071130 
iter  10 value 36.926879
iter  20 value 36.791297
final  value 36.791252 
converged
Fitting Repeat 3 

# weights:  111
initial  value 165.481353 
iter  10 value 37.221758
iter  20 value 36.791620
final  value 36.791251 
converged
Fitting Repeat 4 

# weights:  111
initial  value 144.396932 
iter  10 value 36.809847
iter  20 value 36.791252
iter  20 value 36.791251
iter  20 value 36.791251
final  value 36.791251 
converged
Fitting Repeat 5 

# weights:  111
initial  value 144.568146 
iter  10 value 36.801774
final  value 36.791251 
converged
Fitting Repeat 1 

# weights:  133
initial  value 43.415270 
iter  10 value 31.218584
iter  20 value 31.172894
iter  30 value 31.141416
iter  40 value 28.707276
iter  50 value 22.319883
iter  60 value 21.699917
iter  70 value 21.614878
iter  80 value 21.569974
iter  90 value 21.533860
iter 100 value 21.513261
final  value 21.513261 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  133
initial  value 42.586748 
iter  10 value 31.209174
iter  20 value 28.450967
iter  30 value 22.093566
iter  40 value 21.737116
iter  50 value 21.696477
iter  60 value 21.673712
iter  70 value 21.657187
iter  80 value 21.607651
iter  90 value 21.553749
iter 100 value 21.512641
final  value 21.512641 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 54.815530 
iter  10 value 31.237232
iter  20 value 27.386134
iter  30 value 22.115447
iter  40 value 21.798677
iter  50 value 21.769219
iter  60 value 21.748794
iter  70 value 21.722579
iter  80 value 21.714868
iter  90 value 21.708137
iter 100 value 21.685576
final  value 21.685576 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  133
initial  value 55.008078 
iter  10 value 31.249276
iter  20 value 31.169813
iter  30 value 31.108199
iter  40 value 23.523616
iter  50 value 21.929646
iter  60 value 21.727079
iter  70 value 21.651967
iter  80 value 21.605287
iter  90 value 21.530746
iter 100 value 21.491612
final  value 21.491612 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  133
initial  value 44.538696 
iter  10 value 31.203914
iter  20 value 31.174766
iter  30 value 31.145364
iter  40 value 29.025073
iter  50 value 22.457801
iter  60 value 21.739245
iter  70 value 21.680339
iter  80 value 21.606627
iter  90 value 21.527321
iter 100 value 21.489718
final  value 21.489718 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 61.598521 
iter  10 value 37.867082
final  value 37.867020 
converged
Fitting Repeat 2 

# weights:  177
initial  value 89.766590 
iter  10 value 38.153760
final  value 37.953642 
converged
Fitting Repeat 3 

# weights:  177
initial  value 89.268952 
iter  10 value 40.370381
final  value 40.159158 
converged
Fitting Repeat 4 

# weights:  177
initial  value 47.450925 
iter  10 value 24.661123
final  value 24.661113 
converged
Fitting Repeat 5 

# weights:  177
initial  value 76.538039 
iter  10 value 42.936900
iter  20 value 42.742069
final  value 42.742068 
converged
Fitting Repeat 1 

# weights:  155
initial  value 65.588400 
iter  10 value 31.359983
iter  20 value 30.888332
iter  30 value 25.121141
iter  40 value 24.211581
iter  50 value 24.108592
iter  60 value 24.090644
iter  70 value 24.085648
iter  80 value 24.084280
iter  90 value 24.083387
iter 100 value 24.082691
final  value 24.082691 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  155
initial  value 45.309116 
iter  10 value 31.300447
iter  20 value 29.902584
iter  30 value 24.741260
iter  40 value 24.267891
iter  50 value 24.195425
iter  60 value 24.190705
iter  70 value 24.190096
iter  80 value 24.189940
iter  90 value 24.189828
iter 100 value 24.189783
final  value 24.189783 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  155
initial  value 32.519372 
iter  10 value 30.305464
iter  20 value 25.188723
iter  30 value 24.444949
iter  40 value 24.322236
iter  50 value 24.169338
iter  60 value 24.133428
iter  70 value 24.125847
iter  80 value 24.124503
iter  90 value 24.123281
iter 100 value 24.123236
final  value 24.123236 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  155
initial  value 41.472461 
iter  10 value 31.287459
iter  20 value 27.576208
iter  30 value 24.391090
iter  40 value 24.189878
iter  50 value 24.130958
iter  60 value 24.106757
iter  70 value 24.094284
iter  80 value 24.090684
iter  90 value 24.089821
iter 100 value 24.089562
final  value 24.089562 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  155
initial  value 41.420368 
iter  10 value 31.279483
iter  20 value 29.542924
iter  30 value 24.607277
iter  40 value 24.186521
iter  50 value 24.107856
iter  60 value 24.097032
iter  70 value 24.094352
iter  80 value 24.093656
iter  90 value 24.093522
iter 100 value 24.093496
final  value 24.093496 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  155
initial  value 60.711778 
iter  10 value 26.146462
final  value 26.036432 
converged
Fitting Repeat 2 

# weights:  155
initial  value 75.153573 
iter  10 value 37.966468
final  value 37.830377 
converged
Fitting Repeat 3 

# weights:  155
initial  value 35.510258 
iter  10 value 22.354134
iter  20 value 22.323410
iter  20 value 22.323410
iter  20 value 22.323410
final  value 22.323410 
converged
Fitting Repeat 4 

# weights:  155
initial  value 31.401864 
iter  10 value 19.332522
iter  20 value 17.713682
iter  30 value 17.684397
iter  40 value 17.681204
iter  50 value 17.680990
final  value 17.680987 
converged
Fitting Repeat 5 

# weights:  155
initial  value 74.573557 
iter  10 value 32.288947
final  value 32.265014 
converged
Fitting Repeat 1 

# weights:  45
initial  value 37.954632 
iter  10 value 31.164282
iter  20 value 23.639704
iter  30 value 22.526637
iter  40 value 22.147554
iter  50 value 21.717954
iter  60 value 21.505715
iter  70 value 21.411490
iter  80 value 21.304515
iter  90 value 21.211272
iter 100 value 21.174986
final  value 21.174986 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 49.458594 
iter  10 value 31.170341
iter  20 value 31.158118
iter  30 value 31.136114
iter  40 value 31.039854
iter  50 value 26.490578
iter  60 value 21.326394
iter  70 value 21.176548
iter  80 value 21.144002
iter  90 value 21.125849
iter 100 value 21.116647
final  value 21.116647 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  45
initial  value 48.336589 
iter  10 value 31.169411
iter  20 value 27.187999
iter  30 value 22.455096
iter  40 value 21.801612
iter  50 value 21.180858
iter  60 value 21.152717
iter  70 value 21.139762
iter  80 value 21.131758
iter  90 value 21.120703
iter 100 value 21.116629
final  value 21.116629 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 44.028096 
iter  10 value 31.165507
iter  20 value 28.040750
iter  30 value 21.994211
iter  40 value 21.188626
iter  50 value 21.150080
iter  60 value 21.132283
iter  70 value 21.125133
iter  80 value 21.116643
iter  90 value 21.114217
iter 100 value 21.112205
final  value 21.112205 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 38.137211 
iter  10 value 31.162786
iter  20 value 31.161270
iter  30 value 31.159226
iter  40 value 31.155353
iter  50 value 31.143344
iter  60 value 30.942375
iter  70 value 22.677991
iter  80 value 21.349019
iter  90 value 21.228640
iter 100 value 21.210465
final  value 21.210465 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  23
initial  value 25.062518 
iter  10 value 16.519829
final  value 16.519829 
converged
Fitting Repeat 2 

# weights:  23
initial  value 72.531716 
iter  10 value 50.020530
iter  10 value 50.020530
iter  10 value 50.020530
final  value 50.020530 
converged
Fitting Repeat 3 

# weights:  23
initial  value 31.323213 
iter  10 value 24.591626
final  value 24.591603 
converged
Fitting Repeat 4 

# weights:  23
initial  value 37.575105 
iter  10 value 23.818540
iter  10 value 23.818540
iter  10 value 23.818540
final  value 23.818540 
converged
Fitting Repeat 5 

# weights:  23
initial  value 62.950531 
iter  10 value 39.342494
final  value 39.342491 
converged
Fitting Repeat 1 

# weights:  89
initial  value 35.999856 
iter  10 value 22.800182
iter  20 value 21.879693
iter  30 value 19.388034
iter  40 value 19.382719
iter  50 value 19.364004
iter  60 value 19.088017
iter  70 value 18.902846
iter  80 value 18.900170
iter  90 value 18.898048
iter 100 value 18.896808
final  value 18.896808 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 38.651133 
iter  10 value 25.006506
iter  20 value 18.353050
iter  30 value 17.874522
iter  40 value 17.751645
iter  50 value 17.703923
iter  60 value 17.702023
iter  70 value 17.701133
iter  80 value 17.701022
final  value 17.701018 
converged
Fitting Repeat 3 

# weights:  89
initial  value 33.276873 
iter  10 value 20.874601
iter  20 value 19.051078
iter  30 value 17.968818
iter  40 value 17.966663
iter  50 value 17.966532
iter  60 value 17.966464
iter  70 value 17.966380
iter  80 value 17.937958
iter  90 value 17.642165
iter 100 value 17.555684
final  value 17.555684 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 70.702033 
iter  10 value 38.216898
iter  20 value 38.183346
iter  30 value 38.058118
iter  40 value 37.799730
iter  50 value 36.170756
iter  60 value 35.715864
iter  70 value 35.611214
iter  80 value 35.581282
iter  90 value 35.576802
iter 100 value 35.521479
final  value 35.521479 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 43.402266 
iter  10 value 28.142391
iter  20 value 21.960742
iter  30 value 21.314927
iter  40 value 21.215474
iter  50 value 21.199102
iter  60 value 21.193089
iter  70 value 21.190966
iter  80 value 21.190735
iter  90 value 21.190721
final  value 21.190719 
converged
Fitting Repeat 1 

# weights:  166
initial  value 55.951826 
iter  10 value 31.248892
iter  20 value 31.119186
iter  30 value 22.664607
iter  40 value 21.792189
iter  50 value 21.645581
iter  60 value 21.578848
iter  70 value 21.539052
iter  80 value 21.504539
iter  90 value 21.474679
iter 100 value 21.448391
final  value 21.448391 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 52.695714 
iter  10 value 31.236496
iter  20 value 30.938146
iter  30 value 22.467469
iter  40 value 21.663024
iter  50 value 21.587405
iter  60 value 21.558725
iter  70 value 21.525189
iter  80 value 21.490682
iter  90 value 21.446971
iter 100 value 21.420461
final  value 21.420461 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  166
initial  value 55.413640 
iter  10 value 31.239924
iter  20 value 30.959740
iter  30 value 22.729780
iter  40 value 21.735486
iter  50 value 21.617110
iter  60 value 21.576446
iter  70 value 21.563880
iter  80 value 21.549163
iter  90 value 21.514290
iter 100 value 21.479659
final  value 21.479659 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  166
initial  value 41.910105 
iter  10 value 31.197703
iter  20 value 29.630586
iter  30 value 22.484598
iter  40 value 21.864894
iter  50 value 21.668825
iter  60 value 21.621510
iter  70 value 21.591463
iter  80 value 21.570593
iter  90 value 21.554150
iter 100 value 21.523194
final  value 21.523194 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 49.712550 
iter  10 value 31.236820
iter  20 value 30.119616
iter  30 value 22.070263
iter  40 value 21.715085
iter  50 value 21.661176
iter  60 value 21.616007
iter  70 value 21.588955
iter  80 value 21.574429
iter  90 value 21.555923
iter 100 value 21.523045
final  value 21.523045 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  100
initial  value 55.083640 
iter  10 value 33.325072
final  value 33.324995 
converged
Fitting Repeat 2 

# weights:  100
initial  value 50.954267 
iter  10 value 33.325021
final  value 33.324995 
converged
Fitting Repeat 3 

# weights:  100
initial  value 66.798890 
iter  10 value 33.326633
final  value 33.324995 
converged
Fitting Repeat 4 

# weights:  100
initial  value 49.508488 
iter  10 value 33.325009
final  value 33.324995 
converged
Fitting Repeat 5 

# weights:  100
initial  value 87.500489 
iter  10 value 33.325284
final  value 33.324995 
converged
Fitting Repeat 1 

# weights:  166
initial  value 41.363802 
iter  10 value 34.224381
iter  20 value 30.823184
iter  30 value 24.285223
iter  40 value 24.151304
iter  50 value 24.047924
iter  60 value 23.981867
iter  70 value 23.973097
iter  80 value 23.967946
iter  90 value 23.965377
iter 100 value 23.961911
final  value 23.961911 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 82.776495 
iter  10 value 70.574307
iter  20 value 70.552781
iter  30 value 70.547634
iter  40 value 70.517486
iter  50 value 46.704108
iter  60 value 41.951427
iter  70 value 41.478560
iter  80 value 41.398920
iter  90 value 41.358974
iter 100 value 41.310703
final  value 41.310703 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  166
initial  value 125.752828 
iter  10 value 82.157207
iter  20 value 82.089106
iter  30 value 82.079239
iter  40 value 67.471065
iter  50 value 65.309237
iter  60 value 65.146631
iter  70 value 64.946655
iter  80 value 64.889686
iter  90 value 64.853752
iter 100 value 64.831089
final  value 64.831089 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  166
initial  value 76.296391 
iter  10 value 56.554817
iter  20 value 56.518561
iter  30 value 56.512537
iter  40 value 56.481915
iter  50 value 47.568168
iter  60 value 40.905295
iter  70 value 40.569646
iter  80 value 40.438102
iter  90 value 40.384156
iter 100 value 40.358604
final  value 40.358604 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 68.807931 
iter  10 value 59.958704
iter  20 value 59.919529
iter  30 value 47.987186
iter  40 value 46.674399
iter  50 value 46.611378
iter  60 value 46.593057
iter  70 value 46.586175
iter  80 value 46.578282
iter  90 value 46.568516
iter 100 value 46.552774
final  value 46.552774 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  45
initial  value 62.089434 
iter  10 value 48.624115
iter  20 value 39.699708
iter  30 value 37.387376
iter  40 value 37.333015
iter  50 value 37.326794
iter  60 value 37.326396
iter  70 value 37.325797
iter  80 value 37.325765
iter  80 value 37.325765
iter  80 value 37.325764
final  value 37.325764 
converged
Fitting Repeat 2 

# weights:  45
initial  value 61.175870 
iter  10 value 48.546174
iter  20 value 39.896154
iter  30 value 37.338676
iter  40 value 37.331381
iter  50 value 37.326909
iter  60 value 37.326362
iter  70 value 37.325794
iter  80 value 37.325773
iter  90 value 37.325759
final  value 37.325758 
converged
Fitting Repeat 3 

# weights:  45
initial  value 66.365858 
iter  10 value 48.763076
iter  20 value 44.086560
iter  30 value 37.329795
iter  40 value 37.326532
iter  50 value 37.325784
final  value 37.325764 
converged
Fitting Repeat 4 

# weights:  45
initial  value 57.153603 
iter  10 value 48.471417
iter  20 value 42.579015
iter  30 value 37.335314
iter  40 value 37.330139
iter  50 value 37.326074
iter  60 value 37.325892
iter  70 value 37.325774
final  value 37.325758 
converged
Fitting Repeat 5 

# weights:  45
initial  value 66.396328 
iter  10 value 48.681482
iter  20 value 47.946905
iter  30 value 37.363040
iter  40 value 37.331349
iter  50 value 37.329398
iter  60 value 37.329297
final  value 37.329296 
converged
Fitting Repeat 1 

# weights:  12
initial  value 91.959289 
iter  10 value 62.197537
final  value 62.175546 
converged
Fitting Repeat 2 

# weights:  12
initial  value 94.185535 
iter  10 value 80.414021
final  value 80.413733 
converged
Fitting Repeat 3 

# weights:  12
initial  value 51.102473 
iter  10 value 37.758254
final  value 37.677875 
converged
Fitting Repeat 4 

# weights:  12
initial  value 74.908464 
iter  10 value 66.253575
final  value 66.253497 
converged
Fitting Repeat 5 

# weights:  12
initial  value 78.707752 
iter  10 value 58.567180
final  value 58.551763 
converged
Fitting Repeat 1 

# weights:  199
initial  value 65.825590 
iter  10 value 51.497967
iter  20 value 51.474426
iter  30 value 51.369027
iter  40 value 40.863150
iter  50 value 40.458534
iter  60 value 40.443571
iter  70 value 40.437947
iter  80 value 40.433897
iter  90 value 40.430206
iter 100 value 40.425255
final  value 40.425255 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 69.850645 
iter  10 value 61.055193
iter  20 value 60.121647
iter  30 value 43.472762
iter  40 value 42.527916
iter  50 value 41.911780
iter  60 value 41.725107
iter  70 value 41.642962
iter  80 value 41.552466
iter  90 value 41.511647
iter 100 value 41.486509
final  value 41.486509 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 56.416317 
iter  10 value 50.169658
iter  20 value 50.091373
iter  30 value 39.375447
iter  40 value 37.407523
iter  50 value 37.307240
iter  60 value 37.260216
iter  70 value 37.237215
iter  80 value 37.231640
iter  90 value 37.229504
iter 100 value 37.228492
final  value 37.228492 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 73.802592 
iter  10 value 47.612469
iter  20 value 47.577981
iter  30 value 46.647672
iter  40 value 32.839258
iter  50 value 32.456653
iter  60 value 32.315627
iter  70 value 32.139649
iter  80 value 31.657396
iter  90 value 31.620028
iter 100 value 31.599549
final  value 31.599549 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 47.955035 
iter  10 value 35.707414
iter  20 value 35.573806
iter  30 value 19.527621
iter  40 value 18.885255
iter  50 value 18.802999
iter  60 value 18.770342
iter  70 value 18.722025
iter  80 value 18.707281
iter  90 value 18.701429
iter 100 value 18.695940
final  value 18.695940 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  166
initial  value 59.445431 
iter  10 value 33.548557
iter  20 value 33.390093
iter  30 value 33.328458
iter  40 value 33.316635
iter  50 value 33.312587
iter  60 value 33.310914
iter  70 value 33.309370
iter  80 value 33.307645
iter  90 value 33.305681
iter 100 value 33.303233
final  value 33.303233 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 66.915235 
iter  10 value 50.650833
iter  20 value 45.937065
iter  30 value 33.486874
iter  40 value 33.346610
iter  50 value 33.322111
iter  60 value 33.284050
iter  70 value 33.194783
iter  80 value 33.113652
iter  90 value 33.078829
iter 100 value 33.059369
final  value 33.059369 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  166
initial  value 76.680594 
iter  10 value 46.045233
iter  20 value 45.784424
iter  30 value 45.776731
iter  40 value 45.776219
final  value 45.776211 
converged
Fitting Repeat 4 

# weights:  166
initial  value 97.050064 
iter  10 value 53.345022
iter  20 value 51.230846
iter  30 value 35.155709
iter  40 value 34.956168
iter  50 value 34.894622
iter  60 value 34.847835
iter  70 value 34.784689
iter  80 value 34.762548
iter  90 value 34.751042
iter 100 value 34.748063
final  value 34.748063 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 42.607402 
iter  10 value 38.351917
iter  20 value 38.324433
iter  30 value 34.045951
iter  40 value 22.358943
iter  50 value 22.194995
iter  60 value 22.075340
iter  70 value 21.627171
iter  80 value 21.561301
iter  90 value 21.530410
iter 100 value 21.509329
final  value 21.509329 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  199
initial  value 56.948261 
iter  10 value 44.879918
iter  20 value 40.539522
iter  30 value 35.538327
iter  40 value 35.099292
iter  50 value 34.963626
iter  60 value 34.944460
iter  70 value 34.920449
iter  80 value 34.915693
iter  90 value 34.912915
iter 100 value 34.911734
final  value 34.911734 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 54.042907 
iter  10 value 51.775780
iter  20 value 40.840958
iter  30 value 37.900104
iter  40 value 37.101642
iter  50 value 36.754608
iter  60 value 36.704893
iter  70 value 36.668743
iter  80 value 36.657345
iter  90 value 36.646992
iter 100 value 36.646044
final  value 36.646044 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 77.003973 
iter  10 value 44.053214
iter  20 value 42.511449
iter  30 value 34.112093
iter  40 value 33.761367
iter  50 value 33.703672
iter  60 value 33.671459
iter  70 value 33.657819
iter  80 value 33.647945
iter  90 value 33.639278
iter 100 value 33.637287
final  value 33.637287 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 49.115356 
iter  10 value 40.955050
iter  20 value 40.949009
final  value 40.948979 
converged
Fitting Repeat 5 

# weights:  199
initial  value 58.641184 
iter  10 value 51.558644
iter  20 value 39.863299
iter  30 value 34.480180
iter  40 value 34.144241
iter  50 value 34.038063
iter  60 value 33.999825
iter  70 value 33.956695
iter  80 value 33.939130
iter  90 value 33.921917
iter 100 value 33.919346
final  value 33.919346 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 73.343196 
iter  10 value 49.104771
iter  20 value 48.825181
final  value 48.825179 
converged
Fitting Repeat 2 

# weights:  177
initial  value 58.596329 
iter  10 value 48.832427
final  value 48.825179 
converged
Fitting Repeat 3 

# weights:  177
initial  value 63.593737 
iter  10 value 48.922922
iter  20 value 48.825179
iter  20 value 48.825179
iter  20 value 48.825179
final  value 48.825179 
converged
Fitting Repeat 4 

# weights:  177
initial  value 75.457876 
iter  10 value 48.961357
iter  20 value 48.825179
iter  20 value 48.825179
iter  20 value 48.825179
final  value 48.825179 
converged
Fitting Repeat 5 

# weights:  177
initial  value 55.432707 
iter  10 value 48.825224
final  value 48.825179 
converged
Fitting Repeat 1 

# weights:  34
initial  value 66.276907 
iter  10 value 40.941749
iter  20 value 40.941722
iter  30 value 40.941696
iter  40 value 40.941669
iter  50 value 40.941642
iter  60 value 40.941616
iter  70 value 40.941589
iter  80 value 40.941563
iter  90 value 40.941536
iter 100 value 40.941509
final  value 40.941509 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  34
initial  value 36.050485 
iter  10 value 30.338953
iter  20 value 30.337343
iter  30 value 30.335028
iter  40 value 30.330642
iter  50 value 30.317926
iter  60 value 30.212793
iter  70 value 22.625905
iter  80 value 21.605306
iter  90 value 21.567482
iter 100 value 21.481266
final  value 21.481266 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  34
initial  value 41.120721 
iter  10 value 32.412027
iter  20 value 23.143742
iter  30 value 18.875810
iter  40 value 18.851366
iter  50 value 18.849165
iter  60 value 18.825403
iter  70 value 18.819412
iter  80 value 18.811830
iter  90 value 18.757906
iter 100 value 18.702279
final  value 18.702279 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  34
initial  value 64.257951 
iter  10 value 51.684977
iter  20 value 51.680226
iter  30 value 51.678472
iter  40 value 51.675419
iter  50 value 51.669557
iter  60 value 51.656961
iter  70 value 51.624570
iter  80 value 51.460171
iter  90 value 38.410847
iter 100 value 37.152605
final  value 37.152605 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  34
initial  value 55.980124 
iter  10 value 46.129696
iter  20 value 27.408789
iter  30 value 27.292282
iter  40 value 27.284411
iter  50 value 27.258167
iter  60 value 27.243293
iter  70 value 27.238833
iter  80 value 27.236620
iter  90 value 27.062475
iter 100 value 26.937395
final  value 26.937395 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  144
initial  value 74.932855 
iter  10 value 40.898771
final  value 40.896751 
converged
Fitting Repeat 2 

# weights:  144
initial  value 79.081782 
iter  10 value 69.283781
iter  20 value 69.283727
iter  30 value 69.283666
iter  40 value 69.283596
iter  50 value 69.283512
iter  60 value 69.283413
iter  70 value 69.283292
iter  80 value 69.283142
iter  90 value 69.282951
iter 100 value 69.282699
final  value 69.282699 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  144
initial  value 56.557853 
iter  10 value 41.311948
iter  20 value 41.311268
iter  30 value 41.302083
iter  40 value 41.285615
iter  50 value 37.903385
iter  60 value 30.969589
iter  70 value 30.724572
iter  80 value 30.716354
iter  90 value 30.715146
iter 100 value 30.714263
final  value 30.714263 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  144
initial  value 53.789167 
final  value 51.604555 
converged
Fitting Repeat 5 

# weights:  144
initial  value 60.277974 
iter  10 value 40.617000
final  value 40.615592 
converged
Fitting Repeat 1 

# weights:  12
initial  value 66.609605 
iter  10 value 53.190597
final  value 53.190588 
converged
Fitting Repeat 2 

# weights:  12
initial  value 65.113740 
iter  10 value 53.190612
final  value 53.190588 
converged
Fitting Repeat 3 

# weights:  12
initial  value 59.092166 
iter  10 value 53.190591
final  value 53.190588 
converged
Fitting Repeat 4 

# weights:  12
initial  value 63.630079 
iter  10 value 53.190590
iter  10 value 53.190590
iter  10 value 53.190590
final  value 53.190590 
converged
Fitting Repeat 5 

# weights:  12
initial  value 71.893051 
iter  10 value 53.190719
final  value 53.190588 
converged
Fitting Repeat 1 

# weights:  111
initial  value 158.910916 
iter  10 value 54.988923
iter  20 value 54.962446
final  value 54.962439 
converged
Fitting Repeat 2 

# weights:  111
initial  value 172.962460 
iter  10 value 55.239461
iter  20 value 54.962441
final  value 54.962439 
converged
Fitting Repeat 3 

# weights:  111
initial  value 173.605484 
iter  10 value 55.285899
iter  20 value 54.962490
final  value 54.962440 
converged
Fitting Repeat 4 

# weights:  111
initial  value 168.269536 
iter  10 value 55.189556
iter  20 value 54.962642
final  value 54.962440 
converged
Fitting Repeat 5 

# weights:  111
initial  value 199.061604 
iter  10 value 55.216176
iter  20 value 54.962439
iter  20 value 54.962439
iter  20 value 54.962439
final  value 54.962439 
converged
Fitting Repeat 1 

# weights:  133
initial  value 64.165547 
iter  10 value 48.201765
iter  20 value 48.080844
iter  30 value 35.714866
iter  40 value 34.527912
iter  50 value 34.465187
iter  60 value 34.403353
iter  70 value 34.340541
iter  80 value 34.302288
iter  90 value 34.284157
iter 100 value 34.243661
final  value 34.243661 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  133
initial  value 78.637231 
iter  10 value 48.238420
iter  20 value 47.668002
iter  30 value 35.006906
iter  40 value 34.664561
iter  50 value 34.429891
iter  60 value 34.324372
iter  70 value 34.222875
iter  80 value 34.158719
iter  90 value 34.120041
iter 100 value 34.096107
final  value 34.096107 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 61.023666 
iter  10 value 48.184510
iter  20 value 43.974820
iter  30 value 34.629835
iter  40 value 34.457055
iter  50 value 34.388098
iter  60 value 34.295952
iter  70 value 34.220376
iter  80 value 34.191870
iter  90 value 34.165758
iter 100 value 34.109242
final  value 34.109242 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  133
initial  value 63.231051 
iter  10 value 48.201004
iter  20 value 47.949164
iter  30 value 34.753928
iter  40 value 34.549862
iter  50 value 34.485881
iter  60 value 34.457240
iter  70 value 34.404837
iter  80 value 34.235130
iter  90 value 34.129537
iter 100 value 34.088818
final  value 34.088818 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  133
initial  value 60.014196 
iter  10 value 48.178323
iter  20 value 42.989724
iter  30 value 34.712590
iter  40 value 34.535087
iter  50 value 34.512477
iter  60 value 34.510221
iter  70 value 34.509677
iter  80 value 34.499167
iter  90 value 34.473438
iter 100 value 34.421578
final  value 34.421578 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 95.550140 
iter  10 value 51.760242
final  value 51.760239 
converged
Fitting Repeat 2 

# weights:  177
initial  value 65.877898 
iter  10 value 40.419583
final  value 40.415602 
converged
Fitting Repeat 3 

# weights:  177
initial  value 61.379974 
iter  10 value 39.415025
final  value 39.415024 
converged
Fitting Repeat 4 

# weights:  177
initial  value 76.119978 
iter  10 value 41.339457
final  value 41.339424 
converged
Fitting Repeat 5 

# weights:  177
initial  value 97.311996 
iter  10 value 65.095800
final  value 65.095599 
converged
Fitting Repeat 1 

# weights:  155
initial  value 61.864602 
iter  10 value 48.320538
iter  20 value 42.636007
iter  30 value 38.198516
iter  40 value 37.345419
iter  50 value 37.068454
iter  60 value 36.999734
iter  70 value 36.979495
iter  80 value 36.956899
iter  90 value 36.954253
iter 100 value 36.952670
final  value 36.952670 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  155
initial  value 55.119994 
iter  10 value 47.625546
iter  20 value 38.951070
iter  30 value 37.472484
iter  40 value 37.102445
iter  50 value 37.004830
iter  60 value 36.962313
iter  70 value 36.948593
iter  80 value 36.940630
iter  90 value 36.935732
iter 100 value 36.934034
final  value 36.934034 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  155
initial  value 55.278273 
iter  10 value 48.215367
iter  20 value 37.942969
iter  30 value 37.138184
iter  40 value 37.033518
iter  50 value 36.990155
iter  60 value 36.967765
iter  70 value 36.956615
iter  80 value 36.953977
iter  90 value 36.952767
iter 100 value 36.952519
final  value 36.952519 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  155
initial  value 80.060077 
iter  10 value 48.363566
iter  20 value 42.690662
iter  30 value 37.521901
iter  40 value 37.173882
iter  50 value 37.013115
iter  60 value 36.990799
iter  70 value 36.989097
iter  80 value 36.987476
iter  90 value 36.987009
iter 100 value 36.986851
final  value 36.986851 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  155
initial  value 73.320718 
iter  10 value 48.339597
iter  20 value 45.883241
iter  30 value 38.078646
iter  40 value 37.124861
iter  50 value 36.961270
iter  60 value 36.942541
iter  70 value 36.936155
iter  80 value 36.934143
iter  90 value 36.933794
iter 100 value 36.933763
final  value 36.933763 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  155
initial  value 50.273524 
iter  10 value 40.671553
final  value 40.671074 
converged
Fitting Repeat 2 

# weights:  155
initial  value 95.089531 
iter  10 value 52.671075
final  value 52.537499 
converged
Fitting Repeat 3 

# weights:  155
initial  value 98.726999 
iter  10 value 68.377658
iter  20 value 65.262211
iter  30 value 63.487273
iter  40 value 60.731845
iter  50 value 60.379952
iter  60 value 60.373172
final  value 60.373132 
converged
Fitting Repeat 4 

# weights:  155
initial  value 99.068566 
iter  10 value 78.953238
iter  20 value 78.548052
iter  30 value 72.555168
iter  40 value 68.604102
iter  50 value 68.031479
iter  60 value 67.935542
iter  70 value 67.912321
iter  80 value 67.911743
iter  90 value 67.911645
final  value 67.911584 
converged
Fitting Repeat 5 

# weights:  155
initial  value 57.463812 
iter  10 value 45.292503
iter  20 value 43.080486
iter  30 value 41.632803
iter  40 value 40.573980
iter  50 value 40.502864
iter  60 value 40.500012
final  value 40.499972 
converged
Fitting Repeat 1 

# weights:  45
initial  value 57.462550 
iter  10 value 48.130141
iter  20 value 41.940212
iter  30 value 34.096463
iter  40 value 33.895439
iter  50 value 33.775836
iter  60 value 33.742635
iter  70 value 33.726792
iter  80 value 33.717496
iter  90 value 33.709489
iter 100 value 33.705485
final  value 33.705485 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 74.899079 
iter  10 value 48.136254
iter  20 value 46.387273
iter  30 value 34.023553
iter  40 value 33.898913
iter  50 value 33.889748
iter  60 value 33.886265
iter  70 value 33.884923
iter  80 value 33.870720
iter  90 value 33.738043
iter 100 value 33.719067
final  value 33.719067 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  45
initial  value 81.137821 
iter  10 value 48.133405
iter  20 value 48.107786
iter  30 value 47.866387
iter  40 value 34.223161
iter  50 value 33.899555
iter  60 value 33.890571
iter  70 value 33.885497
iter  80 value 33.879570
iter  90 value 33.766847
iter 100 value 33.717916
final  value 33.717916 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 60.587320 
iter  10 value 48.129904
iter  20 value 48.128912
iter  30 value 48.127937
iter  40 value 48.126550
iter  50 value 48.124161
iter  60 value 48.118675
iter  70 value 48.095971
iter  80 value 46.881734
iter  90 value 34.178182
iter 100 value 33.895197
final  value 33.895197 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 54.072057 
iter  10 value 48.130780
iter  20 value 48.127072
iter  30 value 48.121779
iter  40 value 48.104363
iter  50 value 47.819329
iter  60 value 35.806813
iter  70 value 34.147074
iter  80 value 33.784835
iter  90 value 33.721286
iter 100 value 33.712779
final  value 33.712779 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  23
initial  value 77.681243 
iter  10 value 70.676636
iter  10 value 70.676636
iter  10 value 70.676636
final  value 70.676636 
converged
Fitting Repeat 2 

# weights:  23
initial  value 38.723978 
iter  10 value 26.009395
final  value 26.009391 
converged
Fitting Repeat 3 

# weights:  23
initial  value 63.111463 
iter  10 value 44.528805
final  value 44.528805 
converged
Fitting Repeat 4 

# weights:  23
initial  value 40.829069 
iter  10 value 35.530734
final  value 35.530711 
converged
Fitting Repeat 5 

# weights:  23
initial  value 59.877055 
iter  10 value 43.982612
final  value 43.982608 
converged
Fitting Repeat 1 

# weights:  89
initial  value 84.328992 
iter  10 value 61.454531
iter  20 value 51.508304
iter  30 value 50.829994
iter  40 value 50.596336
iter  50 value 50.586198
iter  60 value 50.586103
iter  70 value 50.585963
iter  70 value 50.585963
iter  70 value 50.585963
final  value 50.585963 
converged
Fitting Repeat 2 

# weights:  89
initial  value 66.529258 
iter  10 value 50.013094
iter  20 value 39.072951
iter  30 value 38.500265
iter  40 value 38.419061
iter  50 value 38.296370
iter  60 value 38.292008
iter  70 value 38.289207
iter  80 value 38.288968
iter  90 value 38.288841
final  value 38.288841 
converged
Fitting Repeat 3 

# weights:  89
initial  value 57.199699 
iter  10 value 36.858134
iter  20 value 35.755294
iter  30 value 34.881636
iter  40 value 34.806543
iter  50 value 34.800506
iter  60 value 34.799981
iter  70 value 34.799772
final  value 34.799770 
converged
Fitting Repeat 4 

# weights:  89
initial  value 65.719377 
iter  10 value 38.911095
iter  20 value 35.436311
iter  30 value 30.282152
iter  40 value 29.523233
iter  50 value 29.418443
iter  60 value 29.383739
iter  70 value 29.374444
iter  80 value 29.371244
iter  90 value 29.370309
iter 100 value 29.370123
final  value 29.370123 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 71.122596 
iter  10 value 49.428122
iter  20 value 38.020156
iter  30 value 37.482474
iter  40 value 37.302526
iter  50 value 37.215399
iter  60 value 37.211323
iter  70 value 37.211138
iter  80 value 37.211025
final  value 37.211022 
converged
Fitting Repeat 1 

# weights:  166
initial  value 98.901198 
iter  10 value 48.153608
iter  20 value 42.368414
iter  30 value 34.596147
iter  40 value 34.430992
iter  50 value 34.370486
iter  60 value 34.310880
iter  70 value 34.127147
iter  80 value 34.025626
iter  90 value 33.994023
iter 100 value 33.980270
final  value 33.980270 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 55.877175 
iter  10 value 48.153183
iter  20 value 46.565012
iter  30 value 35.070577
iter  40 value 34.446449
iter  50 value 34.309051
iter  60 value 34.234659
iter  70 value 34.149253
iter  80 value 34.037330
iter  90 value 34.006929
iter 100 value 33.996923
final  value 33.996923 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  166
initial  value 65.223040 
iter  10 value 48.201933
iter  20 value 47.808218
iter  30 value 35.152341
iter  40 value 34.448071
iter  50 value 34.419323
iter  60 value 34.403063
iter  70 value 34.384527
iter  80 value 34.314969
iter  90 value 34.250597
iter 100 value 34.217622
final  value 34.217622 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  166
initial  value 63.351746 
iter  10 value 48.201289
iter  20 value 48.126180
iter  30 value 48.039728
iter  40 value 36.355219
iter  50 value 34.657430
iter  60 value 34.299471
iter  70 value 34.168397
iter  80 value 34.109302
iter  90 value 34.076900
iter 100 value 34.032102
final  value 34.032102 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 99.552767 
iter  10 value 48.152770
iter  20 value 46.957546
iter  30 value 34.556315
iter  40 value 34.409516
iter  50 value 34.382362
iter  60 value 34.363740
iter  70 value 34.297229
iter  80 value 34.098497
iter  90 value 34.049598
iter 100 value 34.023093
final  value 34.023093 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  100
initial  value 71.415236 
iter  10 value 50.792791
final  value 50.792579 
converged
Fitting Repeat 2 

# weights:  100
initial  value 104.569583 
iter  10 value 50.793867
final  value 50.792579 
converged
Fitting Repeat 3 

# weights:  100
initial  value 83.062365 
iter  10 value 50.792822
final  value 50.792579 
converged
Fitting Repeat 4 

# weights:  100
initial  value 77.416129 
iter  10 value 50.792685
final  value 50.792579 
converged
Fitting Repeat 5 

# weights:  100
initial  value 86.461177 
iter  10 value 50.792659
final  value 50.792579 
converged
Fitting Repeat 1 

# weights:  166
initial  value 42.701536 
iter  10 value 22.919080
iter  20 value 22.885244
iter  30 value 22.883965
iter  40 value 22.881753
iter  50 value 22.876567
iter  60 value 22.849876
iter  70 value 21.307932
iter  80 value 13.780859
iter  90 value 13.414662
iter 100 value 13.306714
final  value 13.306714 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 33.375529 
iter  10 value 23.827595
iter  20 value 22.951441
iter  30 value 14.332634
iter  40 value 13.720894
iter  50 value 13.500966
iter  60 value 13.435641
iter  70 value 13.411737
iter  80 value 13.394924
iter  90 value 13.382783
iter 100 value 13.367131
final  value 13.367131 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  166
initial  value 29.972610 
iter  10 value 21.215296
iter  20 value 21.188823
iter  30 value 21.075130
iter  40 value 14.634154
iter  50 value 10.950741
iter  60 value 10.488890
iter  70 value 10.417243
iter  80 value 10.368475
iter  90 value 10.352664
iter 100 value 10.342502
final  value 10.342502 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  166
initial  value 48.514057 
iter  10 value 44.958333
iter  20 value 44.954309
iter  30 value 44.938783
iter  40 value 42.919072
iter  50 value 30.256506
iter  60 value 24.377863
iter  70 value 24.084813
iter  80 value 23.961161
iter  90 value 23.916206
iter 100 value 23.872460
final  value 23.872460 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 56.169014 
iter  10 value 44.214539
iter  20 value 44.175318
iter  30 value 32.489074
iter  40 value 22.086334
iter  50 value 21.090449
iter  60 value 21.019327
iter  70 value 20.961865
iter  80 value 20.906096
iter  90 value 20.894939
iter 100 value 20.888825
final  value 20.888825 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  45
initial  value 39.736863 
iter  10 value 32.092823
iter  20 value 25.317993
iter  30 value 22.592894
iter  40 value 22.578491
iter  50 value 22.565067
iter  60 value 22.564631
iter  70 value 22.564592
iter  80 value 22.564583
iter  90 value 22.564001
iter 100 value 22.250155
final  value 22.250155 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 52.523230 
iter  10 value 32.308526
iter  20 value 25.700458
iter  30 value 22.566512
iter  40 value 22.558506
iter  50 value 22.558177
iter  60 value 22.557969
final  value 22.557964 
converged
Fitting Repeat 3 

# weights:  45
initial  value 47.417790 
iter  10 value 32.210705
iter  20 value 24.040114
iter  30 value 22.571238
iter  40 value 22.564680
iter  50 value 22.555708
iter  60 value 22.251765
iter  70 value 22.058523
iter  80 value 22.055022
iter  90 value 22.054944
final  value 22.054926 
converged
Fitting Repeat 4 

# weights:  45
initial  value 49.626702 
iter  10 value 32.237713
iter  20 value 23.971377
iter  30 value 22.574815
iter  40 value 22.566999
iter  50 value 22.564576
iter  60 value 22.564459
iter  70 value 22.557619
iter  80 value 22.252802
iter  90 value 22.078186
iter 100 value 22.056960
final  value 22.056960 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 56.186374 
iter  10 value 32.249759
iter  20 value 29.186017
iter  30 value 22.705921
iter  40 value 22.583467
iter  50 value 22.566695
iter  60 value 22.564375
iter  70 value 22.563441
iter  80 value 22.295195
iter  90 value 22.111482
iter 100 value 22.097067
final  value 22.097067 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  12
initial  value 75.341406 
iter  10 value 59.621591
final  value 59.621590 
converged
Fitting Repeat 2 

# weights:  12
initial  value 40.411570 
iter  10 value 31.151161
final  value 31.151105 
converged
Fitting Repeat 3 

# weights:  12
initial  value 53.591219 
iter  10 value 35.336652
final  value 35.336245 
converged
Fitting Repeat 4 

# weights:  12
initial  value 41.335566 
iter  10 value 32.053888
final  value 32.053834 
converged
Fitting Repeat 5 

# weights:  12
initial  value 62.900646 
iter  10 value 41.420943
final  value 41.420889 
converged
Fitting Repeat 1 

# weights:  199
initial  value 50.877161 
iter  10 value 18.027448
iter  20 value 17.970988
iter  30 value 17.970230
final  value 17.970069 
converged
Fitting Repeat 2 

# weights:  199
initial  value 35.788222 
iter  10 value 33.031016
iter  20 value 33.029906
iter  30 value 33.028546
iter  40 value 33.026321
iter  50 value 33.021256
iter  60 value 32.997113
iter  70 value 29.543279
iter  80 value 21.513059
iter  90 value 21.481578
iter 100 value 21.471486
final  value 21.471486 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  199
initial  value 48.233817 
iter  10 value 25.295043
iter  20 value 25.260142
iter  30 value 25.239091
iter  40 value 25.027619
iter  50 value 19.449634
iter  60 value 18.166108
iter  70 value 17.952216
iter  80 value 17.889997
iter  90 value 17.836978
iter 100 value 17.810718
final  value 17.810718 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 61.649196 
iter  10 value 31.455795
iter  20 value 31.392084
iter  30 value 21.169441
iter  40 value 20.011853
iter  50 value 19.967680
iter  60 value 19.941997
iter  70 value 19.936907
iter  80 value 19.932390
iter  90 value 19.929131
iter 100 value 19.919359
final  value 19.919359 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 18.473432 
iter  10 value 6.894647
iter  20 value 5.856105
iter  30 value 5.445860
iter  40 value 5.400210
iter  50 value 5.388877
iter  60 value 5.379701
iter  70 value 5.375149
iter  80 value 5.372000
iter  90 value 5.370388
iter 100 value 5.369112
final  value 5.369112 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  166
initial  value 49.805237 
iter  10 value 34.644716
iter  20 value 34.570096
iter  30 value 34.495258
iter  40 value 24.766815
iter  50 value 19.672563
iter  60 value 18.767411
iter  70 value 18.068539
iter  80 value 17.768204
iter  90 value 17.662396
iter 100 value 17.605248
final  value 17.605248 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 32.820202 
iter  10 value 20.589229
iter  20 value 20.326246
iter  30 value 10.181958
iter  40 value 10.009396
iter  50 value 9.955191
iter  60 value 9.922599
iter  70 value 9.890730
iter  80 value 9.869135
iter  90 value 9.864377
iter 100 value 9.862181
final  value 9.862181 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  166
initial  value 28.064448 
iter  10 value 14.480705
iter  20 value 13.071070
iter  30 value 12.386192
iter  40 value 12.209059
iter  50 value 12.175945
iter  60 value 12.158758
iter  70 value 12.150957
iter  80 value 12.143831
iter  90 value 12.137267
iter 100 value 12.133502
final  value 12.133502 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  166
initial  value 37.861181 
iter  10 value 30.092129
iter  20 value 30.067827
iter  30 value 30.052013
iter  40 value 29.485727
iter  50 value 18.554416
iter  60 value 18.435199
iter  70 value 18.401110
iter  80 value 18.374421
iter  90 value 18.364028
iter 100 value 18.356095
final  value 18.356095 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 51.081542 
iter  10 value 33.950821
iter  20 value 33.834508
iter  30 value 33.750717
iter  40 value 23.353975
iter  50 value 21.133349
iter  60 value 21.005192
iter  70 value 20.942638
iter  80 value 20.876925
iter  90 value 20.823218
iter 100 value 20.782322
final  value 20.782322 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  199
initial  value 18.146685 
iter  10 value 8.992638
iter  20 value 8.504463
iter  30 value 8.453987
iter  40 value 8.423255
iter  50 value 8.415298
iter  60 value 8.412743
iter  70 value 8.412039
iter  80 value 8.411570
iter  90 value 8.411274
iter 100 value 8.411091
final  value 8.411091 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  199
initial  value 64.976245 
iter  10 value 33.159063
iter  20 value 33.081241
iter  30 value 33.080789
final  value 33.080775 
converged
Fitting Repeat 3 

# weights:  199
initial  value 48.635522 
iter  10 value 33.451680
iter  20 value 32.686482
iter  30 value 26.532307
iter  40 value 26.162738
iter  50 value 26.015736
iter  60 value 25.974268
iter  70 value 25.955050
iter  80 value 25.943070
iter  90 value 25.938594
iter 100 value 25.938073
final  value 25.938073 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  199
initial  value 38.145475 
iter  10 value 20.529776
iter  20 value 15.494586
iter  30 value 13.529550
iter  40 value 13.286722
iter  50 value 13.223846
iter  60 value 13.192089
iter  70 value 13.179268
iter  80 value 13.174690
iter  90 value 13.174211
iter 100 value 13.174056
final  value 13.174056 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  199
initial  value 56.669768 
iter  10 value 31.724359
iter  20 value 31.292381
iter  30 value 24.001587
iter  40 value 23.106867
iter  50 value 22.262297
iter  60 value 22.219970
iter  70 value 22.202631
iter  80 value 22.198044
iter  90 value 22.196604
iter 100 value 22.196040
final  value 22.196040 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 43.321881 
iter  10 value 32.209226
final  value 32.185656 
converged
Fitting Repeat 2 

# weights:  177
initial  value 61.330315 
iter  10 value 32.393807
iter  20 value 32.185656
iter  20 value 32.185656
iter  20 value 32.185656
final  value 32.185656 
converged
Fitting Repeat 3 

# weights:  177
initial  value 86.133364 
iter  10 value 32.408435
iter  20 value 32.185652
iter  20 value 32.185652
iter  20 value 32.185652
final  value 32.185652 
converged
Fitting Repeat 4 

# weights:  177
initial  value 44.130125 
iter  10 value 32.241018
final  value 32.185649 
converged
Fitting Repeat 5 

# weights:  177
initial  value 70.652799 
iter  10 value 32.341427
iter  20 value 32.185649
iter  20 value 32.185649
iter  20 value 32.185649
final  value 32.185649 
converged
Fitting Repeat 1 

# weights:  34
initial  value 73.328130 
iter  10 value 47.957201
iter  20 value 47.955842
final  value 47.955692 
converged
Fitting Repeat 2 

# weights:  34
initial  value 48.468273 
iter  10 value 31.759775
iter  20 value 31.741831
iter  30 value 31.399447
iter  40 value 16.745616
iter  50 value 15.604749
iter  60 value 15.566113
iter  70 value 15.542741
iter  80 value 15.536992
iter  90 value 15.533510
iter 100 value 15.529416
final  value 15.529416 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  34
initial  value 41.413753 
iter  10 value 28.852533
iter  20 value 28.843576
iter  30 value 28.839239
iter  40 value 28.827379
iter  50 value 28.743690
iter  60 value 20.145726
iter  70 value 18.708536
iter  80 value 18.690570
iter  90 value 18.663971
iter 100 value 18.518294
final  value 18.518294 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  34
initial  value 40.556830 
iter  10 value 35.105993
iter  20 value 35.102753
iter  30 value 35.101620
iter  40 value 35.099435
iter  50 value 35.093706
iter  60 value 35.060292
iter  70 value 29.143827
iter  80 value 21.442917
iter  90 value 21.345859
iter 100 value 21.337207
final  value 21.337207 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  34
initial  value 50.507381 
iter  10 value 35.109209
iter  20 value 35.106845
iter  30 value 35.103374
iter  40 value 35.097215
iter  50 value 35.085170
iter  60 value 35.058202
iter  70 value 34.942226
iter  80 value 23.356208
iter  90 value 21.638567
iter 100 value 21.621964
final  value 21.621964 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  144
initial  value 58.638415 
iter  10 value 43.081017
iter  20 value 43.080947
iter  30 value 43.080865
iter  40 value 43.080769
iter  50 value 43.080655
iter  60 value 43.080515
iter  70 value 43.080341
iter  80 value 43.080119
iter  90 value 43.079826
iter 100 value 43.079421
final  value 43.079421 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  144
initial  value 45.492509 
iter  10 value 28.959214
iter  20 value 28.957763
iter  30 value 28.957674
iter  40 value 28.957570
iter  50 value 28.957447
iter  60 value 28.957301
iter  70 value 28.957123
iter  80 value 28.956902
iter  90 value 28.956619
iter 100 value 28.956244
final  value 28.956244 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  144
initial  value 57.199920 
iter  10 value 30.688506
iter  20 value 30.688497
iter  30 value 30.688488
iter  40 value 30.688479
iter  50 value 30.688469
iter  60 value 30.688459
iter  70 value 30.688449
iter  80 value 30.688438
iter  90 value 30.688427
iter 100 value 30.688416
final  value 30.688416 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  144
initial  value 48.796827 
iter  10 value 30.227700
iter  20 value 30.227165
iter  30 value 30.226466
iter  40 value 30.224258
iter  50 value 24.679084
iter  60 value 24.422262
iter  70 value 24.339513
iter  80 value 24.305737
iter  90 value 24.278423
iter 100 value 24.238002
final  value 24.238002 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  144
initial  value 48.844423 
iter  10 value 46.174385
iter  20 value 46.173621
iter  30 value 46.172274
iter  40 value 46.169343
iter  50 value 46.159204
iter  60 value 45.738276
iter  70 value 26.821630
iter  80 value 26.763646
iter  90 value 26.050862
iter 100 value 25.589293
final  value 25.589293 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  12
initial  value 37.766978 
iter  10 value 35.574345
iter  10 value 35.574345
iter  10 value 35.574345
final  value 35.574345 
converged
Fitting Repeat 2 

# weights:  12
initial  value 49.614374 
iter  10 value 35.574407
final  value 35.574345 
converged
Fitting Repeat 3 

# weights:  12
initial  value 44.863839 
iter  10 value 35.574346
iter  10 value 35.574345
iter  10 value 35.574345
final  value 35.574345 
converged
Fitting Repeat 4 

# weights:  12
initial  value 43.540433 
iter  10 value 35.574348
final  value 35.574345 
converged
Fitting Repeat 5 

# weights:  12
initial  value 48.329444 
iter  10 value 35.574394
final  value 35.574345 
converged
Fitting Repeat 1 

# weights:  111
initial  value 163.997889 
iter  10 value 37.212533
iter  20 value 36.945158
final  value 36.945115 
converged
Fitting Repeat 2 

# weights:  111
initial  value 148.997391 
iter  10 value 37.109851
iter  20 value 36.945134
final  value 36.945116 
converged
Fitting Repeat 3 

# weights:  111
initial  value 172.881952 
iter  10 value 37.067728
iter  20 value 36.945116
iter  20 value 36.945115
iter  20 value 36.945115
final  value 36.945115 
converged
Fitting Repeat 4 

# weights:  111
initial  value 151.490362 
iter  10 value 37.205641
iter  20 value 36.945152
final  value 36.945115 
converged
Fitting Repeat 5 

# weights:  111
initial  value 144.496641 
iter  10 value 37.131565
iter  20 value 36.945142
final  value 36.945115 
converged
Fitting Repeat 1 

# weights:  133
initial  value 37.278515 
iter  10 value 31.867288
iter  20 value 30.714670
iter  30 value 20.270293
iter  40 value 19.854131
iter  50 value 19.755985
iter  60 value 19.713693
iter  70 value 19.639796
iter  80 value 19.590884
iter  90 value 19.553329
iter 100 value 19.512601
final  value 19.512601 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  133
initial  value 44.265743 
iter  10 value 31.901285
iter  20 value 31.649892
iter  30 value 20.652041
iter  40 value 19.830824
iter  50 value 19.757741
iter  60 value 19.724397
iter  70 value 19.624009
iter  80 value 19.525604
iter  90 value 19.489140
iter 100 value 19.476887
final  value 19.476887 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 71.532290 
iter  10 value 31.894785
iter  20 value 30.629969
iter  30 value 19.957424
iter  40 value 19.725078
iter  50 value 19.585186
iter  60 value 19.542981
iter  70 value 19.501204
iter  80 value 19.480790
iter  90 value 19.469617
iter 100 value 19.465416
final  value 19.465416 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  133
initial  value 36.333633 
iter  10 value 31.874182
iter  20 value 30.855703
iter  30 value 20.043549
iter  40 value 19.674188
iter  50 value 19.577873
iter  60 value 19.540220
iter  70 value 19.501024
iter  80 value 19.485010
iter  90 value 19.479199
iter 100 value 19.473234
final  value 19.473234 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  133
initial  value 53.390266 
iter  10 value 31.971752
iter  20 value 31.222441
iter  30 value 21.065575
iter  40 value 19.976690
iter  50 value 19.830628
iter  60 value 19.782111
iter  70 value 19.682291
iter  80 value 19.589462
iter  90 value 19.537700
iter 100 value 19.489424
final  value 19.489424 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 46.856679 
iter  10 value 29.244464
final  value 29.244463 
converged
Fitting Repeat 2 

# weights:  177
initial  value 92.971065 
iter  10 value 34.709067
final  value 34.709048 
converged
Fitting Repeat 3 

# weights:  177
initial  value 35.088019 
iter  10 value 16.196633
final  value 16.196604 
converged
Fitting Repeat 4 

# weights:  177
initial  value 78.702147 
iter  10 value 35.005855
iter  20 value 34.954967
iter  20 value 34.954966
iter  20 value 34.954966
final  value 34.954966 
converged
Fitting Repeat 5 

# weights:  177
initial  value 51.197165 
iter  10 value 29.574066
final  value 29.574035 
converged
Fitting Repeat 1 

# weights:  155
initial  value 48.175806 
iter  10 value 31.879703
iter  20 value 23.458135
iter  30 value 22.524771
iter  40 value 22.303989
iter  50 value 22.244296
iter  60 value 22.224514
iter  70 value 22.211884
iter  80 value 22.206139
iter  90 value 22.205013
iter 100 value 22.204934
final  value 22.204934 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  155
initial  value 45.715045 
iter  10 value 31.833581
iter  20 value 23.693303
iter  30 value 22.479225
iter  40 value 22.264140
iter  50 value 22.232451
iter  60 value 22.219057
iter  70 value 22.212847
iter  80 value 22.208976
iter  90 value 22.207891
iter 100 value 22.205448
final  value 22.205448 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  155
initial  value 68.058554 
iter  10 value 32.041399
iter  20 value 25.491362
iter  30 value 23.375939
iter  40 value 22.936401
iter  50 value 22.401275
iter  60 value 22.326282
iter  70 value 22.258307
iter  80 value 22.220591
iter  90 value 22.217851
iter 100 value 22.217107
final  value 22.217107 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  155
initial  value 55.182735 
iter  10 value 31.897877
iter  20 value 24.994052
iter  30 value 22.828183
iter  40 value 22.409373
iter  50 value 22.297412
iter  60 value 22.250550
iter  70 value 22.233472
iter  80 value 22.224014
iter  90 value 22.218470
iter 100 value 22.217322
final  value 22.217322 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  155
initial  value 46.291409 
iter  10 value 31.913230
iter  20 value 24.436234
iter  30 value 22.346945
iter  40 value 22.240535
iter  50 value 22.221702
iter  60 value 22.217736
iter  70 value 22.217077
iter  80 value 22.216949
final  value 22.216941 
converged
Fitting Repeat 1 

# weights:  155
initial  value 63.294895 
iter  10 value 35.415162
iter  20 value 33.370396
iter  30 value 32.111665
iter  40 value 31.542633
iter  50 value 31.515211
final  value 31.514714 
converged
Fitting Repeat 2 

# weights:  155
initial  value 32.348096 
iter  10 value 23.832660
iter  20 value 19.639608
iter  30 value 19.405497
iter  40 value 19.332919
iter  50 value 19.307382
iter  60 value 19.303675
iter  70 value 19.303647
iter  70 value 19.303647
iter  70 value 19.303647
final  value 19.303647 
converged
Fitting Repeat 3 

# weights:  155
initial  value 75.063558 
iter  10 value 43.592934
final  value 43.486947 
converged
Fitting Repeat 4 

# weights:  155
initial  value 74.885300 
iter  10 value 29.789960
iter  20 value 29.748643
final  value 29.748642 
converged
Fitting Repeat 5 

# weights:  155
initial  value 69.778209 
iter  10 value 24.646812
iter  20 value 24.588181
iter  20 value 24.588181
iter  20 value 24.588181
final  value 24.588181 
converged
Fitting Repeat 1 

# weights:  45
initial  value 47.176688 
iter  10 value 31.855389
iter  20 value 30.692856
iter  30 value 20.060065
iter  40 value 19.303660
iter  50 value 19.200920
iter  60 value 19.154205
iter  70 value 19.132119
iter  80 value 19.112504
iter  90 value 19.102466
iter 100 value 19.098973
final  value 19.098973 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 39.665465 
iter  10 value 31.849224
iter  20 value 31.837588
iter  30 value 31.661831
iter  40 value 19.822342
iter  50 value 19.443183
iter  60 value 19.267511
iter  70 value 19.134919
iter  80 value 19.111209
iter  90 value 19.101544
iter 100 value 19.098588
final  value 19.098588 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  45
initial  value 42.956205 
iter  10 value 31.849032
iter  20 value 23.523079
iter  30 value 20.388700
iter  40 value 19.486600
iter  50 value 19.361838
iter  60 value 19.175057
iter  70 value 19.137786
iter  80 value 19.124097
iter  90 value 19.111022
iter 100 value 19.102968
final  value 19.102968 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 43.263802 
iter  10 value 31.857394
iter  20 value 31.775874
iter  30 value 21.331778
iter  40 value 19.857964
iter  50 value 19.217490
iter  60 value 19.164976
iter  70 value 19.146569
iter  80 value 19.135130
iter  90 value 19.119660
iter 100 value 19.112250
final  value 19.112250 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 51.026317 
iter  10 value 31.855547
iter  20 value 31.833878
iter  30 value 31.319408
iter  40 value 19.827762
iter  50 value 19.229448
iter  60 value 19.133587
iter  70 value 19.111390
iter  80 value 19.106420
iter  90 value 19.101836
iter 100 value 19.100137
final  value 19.100137 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  23
initial  value 34.319989 
iter  10 value 31.252333
final  value 31.252322 
converged
Fitting Repeat 2 

# weights:  23
initial  value 58.346427 
iter  10 value 47.958546
final  value 47.957637 
converged
Fitting Repeat 3 

# weights:  23
initial  value 52.425806 
iter  10 value 40.159895
final  value 40.159637 
converged
Fitting Repeat 4 

# weights:  23
initial  value 56.024635 
iter  10 value 44.603562
final  value 44.603561 
converged
Fitting Repeat 5 

# weights:  23
initial  value 53.638840 
iter  10 value 33.945094
final  value 33.945092 
converged
Fitting Repeat 1 

# weights:  89
initial  value 19.122932 
iter  10 value 14.579854
iter  20 value 8.342072
iter  30 value 8.300640
iter  40 value 7.669263
iter  50 value 7.555999
iter  60 value 7.533734
iter  70 value 7.522249
iter  80 value 7.519962
iter  90 value 7.519376
final  value 7.519369 
converged
Fitting Repeat 2 

# weights:  89
initial  value 42.850696 
iter  10 value 35.259087
iter  20 value 26.047857
iter  30 value 23.915299
iter  40 value 23.270889
iter  50 value 23.096911
iter  60 value 23.034813
iter  70 value 23.030143
iter  80 value 23.029394
iter  90 value 23.029343
final  value 23.029336 
converged
Fitting Repeat 3 

# weights:  89
initial  value 65.729612 
iter  10 value 41.315648
iter  20 value 29.330595
iter  30 value 29.177116
iter  40 value 28.521436
iter  50 value 28.441297
iter  60 value 28.437004
iter  70 value 28.432147
iter  80 value 28.430178
iter  90 value 28.429028
iter 100 value 28.428543
final  value 28.428543 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 50.261446 
iter  10 value 44.620781
iter  20 value 30.619908
iter  30 value 28.609205
iter  40 value 28.293822
iter  50 value 27.593013
iter  60 value 27.550581
iter  70 value 27.527016
iter  80 value 27.520844
iter  90 value 27.519987
iter 100 value 27.519837
final  value 27.519837 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 60.212738 
iter  10 value 29.421044
iter  20 value 24.425406
iter  30 value 24.038651
iter  40 value 24.036212
iter  50 value 23.974992
iter  60 value 23.592061
iter  70 value 23.501155
iter  80 value 23.490129
iter  90 value 23.485068
iter 100 value 23.483531
final  value 23.483531 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  166
initial  value 44.168937 
iter  10 value 31.912095
iter  20 value 31.850149
iter  30 value 21.427148
iter  40 value 19.907447
iter  50 value 19.760589
iter  60 value 19.659031
iter  70 value 19.553637
iter  80 value 19.542091
iter  90 value 19.537016
iter 100 value 19.534028
final  value 19.534028 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 36.510547 
iter  10 value 31.870340
iter  20 value 31.821755
iter  30 value 24.800685
iter  40 value 19.771445
iter  50 value 19.536556
iter  60 value 19.426951
iter  70 value 19.401980
iter  80 value 19.384393
iter  90 value 19.378273
iter 100 value 19.372371
final  value 19.372371 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  166
initial  value 48.464128 
iter  10 value 31.922455
iter  20 value 31.845256
iter  30 value 29.514958
iter  40 value 20.469462
iter  50 value 19.862858
iter  60 value 19.633470
iter  70 value 19.537930
iter  80 value 19.464747
iter  90 value 19.436334
iter 100 value 19.418360
final  value 19.418360 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  166
initial  value 35.854394 
iter  10 value 31.866002
iter  20 value 28.511332
iter  30 value 19.906091
iter  40 value 19.706443
iter  50 value 19.650499
iter  60 value 19.596596
iter  70 value 19.480171
iter  80 value 19.431387
iter  90 value 19.410151
iter 100 value 19.398852
final  value 19.398852 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 62.351953 
iter  10 value 31.934739
iter  20 value 31.841024
iter  30 value 21.388497
iter  40 value 19.983257
iter  50 value 19.683009
iter  60 value 19.547544
iter  70 value 19.509616
iter  80 value 19.478152
iter  90 value 19.445447
iter 100 value 19.408353
final  value 19.408353 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  100
initial  value 62.668587 
iter  10 value 33.707437
final  value 33.707133 
converged
Fitting Repeat 2 

# weights:  100
initial  value 65.465734 
iter  10 value 33.707336
final  value 33.707133 
converged
Fitting Repeat 3 

# weights:  100
initial  value 67.782315 
iter  10 value 33.707344
final  value 33.707133 
converged
Fitting Repeat 4 

# weights:  100
initial  value 60.770947 
iter  10 value 33.707306
final  value 33.707133 
converged
Fitting Repeat 5 

# weights:  100
initial  value 75.399566 
iter  10 value 33.713061
final  value 33.707133 
converged
Fitting Repeat 1 

# weights:  45
initial  value 66.766363 
iter  10 value 55.570476
iter  20 value 55.455937
iter  30 value 40.168084
iter  40 value 37.493473
iter  50 value 37.354312
iter  60 value 37.249070
iter  70 value 37.207490
iter  80 value 37.155369
iter  90 value 37.105460
iter 100 value 37.075950
final  value 37.075950 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 87.231378 
iter  10 value 55.575891
iter  20 value 40.432369
iter  30 value 38.096350
iter  40 value 37.176123
iter  50 value 37.085010
iter  60 value 37.034492
iter  70 value 37.013093
iter  80 value 36.996238
iter  90 value 36.981436
iter 100 value 36.949165
final  value 36.949165 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  45
initial  value 77.274929 
iter  10 value 55.575781
iter  20 value 55.552160
iter  30 value 54.894990
iter  40 value 38.134986
iter  50 value 37.223235
iter  60 value 37.161596
iter  70 value 37.113363
iter  80 value 37.050346
iter  90 value 36.988047
iter 100 value 36.963371
final  value 36.963371 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 70.621556 
iter  10 value 55.571210
iter  20 value 55.568646
iter  30 value 55.567361
iter  40 value 55.565255
iter  50 value 55.560400
iter  60 value 55.535334
iter  70 value 49.958189
iter  80 value 37.458088
iter  90 value 37.094406
iter 100 value 37.041392
final  value 37.041392 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 66.225941 
iter  10 value 55.571037
iter  20 value 55.566116
iter  30 value 55.562398
iter  40 value 55.549851
iter  50 value 55.288487
iter  60 value 38.622977
iter  70 value 37.139556
iter  80 value 37.050070
iter  90 value 37.019223
iter 100 value 37.000675
final  value 37.000675 
stopped after 100 iterations
Model Averaged Neural Network 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  size  decay         bag    RMSE       Rsquared    MAE        Selected
   1    1.146607e+00  FALSE  0.8510083  0.13801033  0.5639818          
   1    2.288961e+00   TRUE  0.8675480  0.05682583  0.5935797          
   2    6.409401e-01   TRUE  0.8442303  0.38749561  0.5540986          
   3    5.046684e-05   TRUE  0.7279007  0.52557355  0.4517101          
   4    3.153077e-04  FALSE  0.6864958  0.55113778  0.4087346  *       
   4    2.306609e-02  FALSE  0.7020934  0.58027034  0.4458602          
   8    2.873822e-02   TRUE  0.7100631  0.56918650  0.4540485          
   9    9.557146e-01  FALSE  0.8390771  0.28535120  0.5399469          
  10    5.497725e+00  FALSE  0.8634886  0.05520093  0.5853870          
  12    2.005756e-03  FALSE  0.6901505  0.53485434  0.4187315          
  13    3.886082e-05   TRUE  0.7546977  0.42501223  0.4515011          
  14    2.810136e-02  FALSE  0.6997400  0.58815140  0.4449098          
  14    1.478120e-01   TRUE  0.7827931  0.62595614  0.4907403          
  15    6.484945e-04   TRUE  0.7031413  0.51206616  0.4359111          
  15    1.586781e-03  FALSE  0.6886367  0.54088599  0.4146812          
  15    2.259918e-03   TRUE  0.7024737  0.54689427  0.4274872          
  16    2.040700e-01  FALSE  0.8310769  0.57612597  0.5247503          
  16    6.234085e-01   TRUE  0.8383732  0.44977550  0.5375298          
  18    5.864250e-04   TRUE  0.7085039  0.52096858  0.4442231          
  18    2.130540e-02   TRUE  0.7209388  0.55828878  0.4519877          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 4, decay = 0.0003153077 and
 bag = FALSE.
[1] "Sat Mar 10 03:11:31 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:11:46 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "bag"                     
Bagged MARS 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  degree  nprune  RMSE       Rsquared   MAE        Selected
  1        3      0.2800356  0.9235907  0.1707310          
  1        4      0.2463984  0.9308117  0.1555064  *       
  1        9      0.2791284  0.9030377  0.1940748          
  1       10      0.2820288  0.8958280  0.2017968          
  1       11      0.2818089  0.9071297  0.2022118          
  1       13      0.2905924  0.9006971  0.2084470          
  2        2      0.4594966  0.7732416  0.2619592          
  2        3      0.2954943  0.9214229  0.1684362          
  2        4      0.2875697  0.9220901  0.1654313          
  2        6      0.2683746  0.9329740  0.1564321          
  2        7      0.2836959  0.9224315  0.1704613          
  2        8      0.2810600  0.9243354  0.1685337          
  2       10      0.3068139  0.9161656  0.1822443          
  2       11      0.3014870  0.9160740  0.1838011          
  2       12      0.3012361  0.9117108  0.1870361          
  2       13      0.2812259  0.9334906  0.1715552          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nprune = 4 and degree = 1.
[1] "Sat Mar 10 03:12:50 2018"
Bagged MARS using gCV Pruning 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results:

  RMSE       Rsquared   MAE    
  0.2885739  0.8991546  0.20367

Tuning parameter 'degree' was held constant at a value of 1
[1] "Sat Mar 10 03:13:37 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :6     NA's   :6     NA's   :6    
Error : Stopping
In addition: There were 19 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 13 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:14:23 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "bam"                     
bartMachine initializing with 74 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 73/477.6MB
Iteration 200/1250  mem: 111.5/477.6MB
Iteration 300/1250  mem: 149.9/477.6MB
Iteration 400/1250  mem: 71.6/477.6MB
Iteration 500/1250  mem: 107.4/477.6MB
Iteration 600/1250  mem: 145.6/477.6MB
Iteration 700/1250  mem: 73.9/477.6MB
Iteration 800/1250  mem: 112.2/477.6MB
Iteration 900/1250  mem: 148/477.6MB
Iteration 1000/1250  mem: 76.3/477.6MB
Iteration 1100/1250  mem: 114.6/477.6MB
Iteration 1200/1250  mem: 152.9/477.6MB
done building BART in 0.824 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 27 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 77.4/477.6MB
Iteration 200/1250  mem: 92.4/477.6MB
Iteration 300/1250  mem: 107.5/477.6MB
Iteration 400/1250  mem: 122.5/477.6MB
Iteration 500/1250  mem: 136.4/477.6MB
Iteration 600/1250  mem: 151.4/477.6MB
Iteration 700/1250  mem: 166.5/477.6MB
Iteration 800/1250  mem: 182.8/477.6MB
Iteration 900/1250  mem: 84.7/477.6MB
Iteration 1000/1250  mem: 99.9/477.6MB
Iteration 1100/1250  mem: 115.1/477.6MB
Iteration 1200/1250  mem: 128.6/477.6MB
done building BART in 0.338 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 14 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 148.4/477.6MB
Iteration 200/1250  mem: 155.9/477.6MB
Iteration 300/1250  mem: 160.9/477.6MB
Iteration 400/1250  mem: 168.4/477.6MB
Iteration 500/1250  mem: 175.8/477.6MB
Iteration 600/1250  mem: 183.3/477.6MB
Iteration 700/1250  mem: 190.8/477.6MB
Iteration 800/1250  mem: 195.8/477.6MB
Iteration 900/1250  mem: 90.2/477.6MB
Iteration 1000/1250  mem: 97.9/477.6MB
Iteration 1100/1250  mem: 103.7/477.6MB
Iteration 1200/1250  mem: 111.4/477.6MB
done building BART in 0.169 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 167.3/477.6MB
Iteration 200/1250  mem: 215.2/477.6MB
Iteration 300/1250  mem: 138/477.6MB
Iteration 400/1250  mem: 185.5/477.6MB
Iteration 500/1250  mem: 112/477.6MB
Iteration 600/1250  mem: 160.4/477.6MB
Iteration 700/1250  mem: 206.9/477.6MB
Iteration 800/1250  mem: 133.8/477.6MB
Iteration 900/1250  mem: 180.9/477.6MB
Iteration 1000/1250  mem: 228/477.6MB
Iteration 1100/1250  mem: 152.1/477.6MB
Iteration 1200/1250  mem: 198.3/477.6MB
done building BART in 1.049 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 76 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 145.2/477.6MB
Iteration 200/1250  mem: 188.1/477.6MB
Iteration 300/1250  mem: 229.4/477.6MB
Iteration 400/1250  mem: 271.8/477.6MB
Iteration 500/1250  mem: 188.4/477.6MB
Iteration 600/1250  mem: 230.5/477.6MB
Iteration 700/1250  mem: 274.7/477.6MB
Iteration 800/1250  mem: 195.7/477.6MB
Iteration 900/1250  mem: 237.7/477.6MB
Iteration 1000/1250  mem: 279.7/477.6MB
Iteration 1100/1250  mem: 200.6/477.6MB
Iteration 1200/1250  mem: 241.8/477.6MB
done building BART in 0.975 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 87 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 190.7/477.6MB
Iteration 200/1250  mem: 235.9/477.6MB
Iteration 300/1250  mem: 279.2/477.6MB
Iteration 400/1250  mem: 323.6/477.6MB
Iteration 500/1250  mem: 237.8/477.6MB
Iteration 600/1250  mem: 282.3/477.6MB
Iteration 700/1250  mem: 326.7/477.6MB
Iteration 800/1250  mem: 244/477.6MB
Iteration 900/1250  mem: 288.1/477.6MB
Iteration 1000/1250  mem: 332.2/477.6MB
Iteration 1100/1250  mem: 250.6/477.6MB
Iteration 1200/1250  mem: 297.4/477.6MB
done building BART in 0.964 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 81 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 242.2/477.6MB
Iteration 200/1250  mem: 285.6/477.6MB
Iteration 300/1250  mem: 330.6/477.6MB
Iteration 400/1250  mem: 244.8/477.6MB
Iteration 500/1250  mem: 291.3/477.6MB
Iteration 600/1250  mem: 335.8/477.6MB
Iteration 700/1250  mem: 256.3/477.6MB
Iteration 800/1250  mem: 299.6/477.6MB
Iteration 900/1250  mem: 345.1/477.6MB
Iteration 1000/1250  mem: 76.6/477.6MB
Iteration 1100/1250  mem: 120.7/477.6MB
Iteration 1200/1250  mem: 164.8/477.6MB
done building BART in 1.151 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 23 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 202.1/477.6MB
Iteration 200/1250  mem: 88.6/477.6MB
Iteration 300/1250  mem: 100.9/477.6MB
Iteration 400/1250  mem: 111.7/477.6MB
Iteration 500/1250  mem: 122.5/477.6MB
Iteration 600/1250  mem: 133.3/477.6MB
Iteration 700/1250  mem: 144.1/477.6MB
Iteration 800/1250  mem: 156.4/477.6MB
Iteration 900/1250  mem: 167.2/477.6MB
Iteration 1000/1250  mem: 178/477.6MB
Iteration 1100/1250  mem: 188.8/477.6MB
Iteration 1200/1250  mem: 201.1/477.6MB
done building BART in 0.237 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 67 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 114.1/477.6MB
Iteration 200/1250  mem: 148.3/477.6MB
Iteration 300/1250  mem: 182.5/477.6MB
Iteration 400/1250  mem: 216.7/477.6MB
Iteration 500/1250  mem: 116.8/477.6MB
Iteration 600/1250  mem: 151/477.6MB
Iteration 700/1250  mem: 185.2/477.6MB
Iteration 800/1250  mem: 219.3/477.6MB
Iteration 900/1250  mem: 124.8/477.6MB
Iteration 1000/1250  mem: 160/477.6MB
Iteration 1100/1250  mem: 193.3/477.6MB
Iteration 1200/1250  mem: 226.6/477.6MB
done building BART in 0.748 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 13 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 125.9/477.6MB
Iteration 200/1250  mem: 134.3/477.6MB
Iteration 300/1250  mem: 139.9/477.6MB
Iteration 400/1250  mem: 145.5/477.6MB
Iteration 500/1250  mem: 153.9/477.6MB
Iteration 600/1250  mem: 159.5/477.6MB
Iteration 700/1250  mem: 165.1/477.6MB
Iteration 800/1250  mem: 173.5/477.6MB
Iteration 900/1250  mem: 179.1/477.6MB
Iteration 1000/1250  mem: 184.7/477.6MB
Iteration 1100/1250  mem: 193.1/477.6MB
Iteration 1200/1250  mem: 198.7/477.6MB
done building BART in 0.125 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 53 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 233.4/477.6MB
Iteration 200/1250  mem: 128/477.6MB
Iteration 300/1250  mem: 155.1/477.6MB
Iteration 400/1250  mem: 184.1/477.6MB
Iteration 500/1250  mem: 211.2/477.6MB
Iteration 600/1250  mem: 238.3/477.6MB
Iteration 700/1250  mem: 133.3/477.6MB
Iteration 800/1250  mem: 160.3/477.6MB
Iteration 900/1250  mem: 187.3/477.6MB
Iteration 1000/1250  mem: 216.5/477.6MB
Iteration 1100/1250  mem: 243.5/477.6MB
Iteration 1200/1250  mem: 270.5/477.6MB
done building BART in 0.614 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 62 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 192.5/477.6MB
Iteration 200/1250  mem: 224.1/477.6MB
Iteration 300/1250  mem: 255.7/477.6MB
Iteration 400/1250  mem: 151.5/477.6MB
Iteration 500/1250  mem: 184.4/477.6MB
Iteration 600/1250  mem: 217.2/477.6MB
Iteration 700/1250  mem: 250/477.6MB
Iteration 800/1250  mem: 282.9/477.6MB
Iteration 900/1250  mem: 183.5/477.6MB
Iteration 1000/1250  mem: 217.3/477.6MB
Iteration 1100/1250  mem: 251/477.6MB
Iteration 1200/1250  mem: 282/477.6MB
done building BART in 0.665 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 79 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 215.6/477.6MB
Iteration 200/1250  mem: 261/477.6MB
Iteration 300/1250  mem: 306.4/477.6MB
Iteration 400/1250  mem: 211.4/477.6MB
Iteration 500/1250  mem: 256.7/477.6MB
Iteration 600/1250  mem: 302.1/477.6MB
Iteration 700/1250  mem: 218.7/477.6MB
Iteration 800/1250  mem: 262.1/477.6MB
Iteration 900/1250  mem: 307.8/477.6MB
Iteration 1000/1250  mem: 225.7/477.6MB
Iteration 1100/1250  mem: 271.2/477.6MB
Iteration 1200/1250  mem: 316.7/477.6MB
done building BART in 0.983 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 72 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 265.1/477.6MB
Iteration 200/1250  mem: 312.3/477.6MB
Iteration 300/1250  mem: 359.5/477.6MB
Iteration 400/1250  mem: 277.8/477.6MB
Iteration 500/1250  mem: 328.6/477.6MB
Iteration 600/1250  mem: 270.3/477.6MB
Iteration 700/1250  mem: 320.1/477.6MB
Iteration 800/1250  mem: 372/477.6MB
Iteration 900/1250  mem: 310.3/477.6MB
Iteration 1000/1250  mem: 359.2/477.6MB
Iteration 1100/1250  mem: 327.6/477.6MB
Iteration 1200/1250  mem: 379/477.6MB
done building BART in 1.555 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 70 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 187.5/493.9MB
Iteration 200/1250  mem: 224.2/493.9MB
Iteration 300/1250  mem: 161.3/496.5MB
Iteration 400/1250  mem: 197.3/496.5MB
Iteration 500/1250  mem: 231.4/496.5MB
Iteration 600/1250  mem: 173.1/498.6MB
Iteration 700/1250  mem: 208.3/498.6MB
Iteration 800/1250  mem: 243.6/498.6MB
Iteration 900/1250  mem: 180.5/499.1MB
Iteration 1000/1250  mem: 215.9/499.1MB
Iteration 1100/1250  mem: 251.3/499.1MB
Iteration 1200/1250  mem: 188.1/498.6MB
done building BART in 0.775 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 26 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 223.5/498.6MB
Iteration 200/1250  mem: 236.4/498.6MB
Iteration 300/1250  mem: 251.3/498.6MB
Iteration 400/1250  mem: 264.2/498.6MB
Iteration 500/1250  mem: 277/498.6MB
Iteration 600/1250  mem: 189/499.6MB
Iteration 700/1250  mem: 201.4/499.6MB
Iteration 800/1250  mem: 213.7/499.6MB
Iteration 900/1250  mem: 227.8/499.6MB
Iteration 1000/1250  mem: 240.2/499.6MB
Iteration 1100/1250  mem: 254.3/499.6MB
Iteration 1200/1250  mem: 266.6/499.6MB
done building BART in 0.287 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 17 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 184.8/500.2MB
Iteration 200/1250  mem: 194.3/500.2MB
Iteration 300/1250  mem: 202.7/500.2MB
Iteration 400/1250  mem: 211.1/500.2MB
Iteration 500/1250  mem: 220.7/500.2MB
Iteration 600/1250  mem: 229.1/500.2MB
Iteration 700/1250  mem: 238.7/500.2MB
Iteration 800/1250  mem: 247/500.2MB
Iteration 900/1250  mem: 255.4/500.2MB
Iteration 1000/1250  mem: 263.8/500.2MB
Iteration 1100/1250  mem: 272.2/500.2MB
Iteration 1200/1250  mem: 281.8/500.2MB
done building BART in 0.191 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 42 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 210.5/501.2MB
Iteration 200/1250  mem: 235.8/501.2MB
Iteration 300/1250  mem: 261.6/501.2MB
Iteration 400/1250  mem: 287.5/501.2MB
Iteration 500/1250  mem: 211.9/502.8MB
Iteration 600/1250  mem: 237.8/502.8MB
Iteration 700/1250  mem: 263.8/502.8MB
Iteration 800/1250  mem: 289.7/502.8MB
Iteration 900/1250  mem: 315.6/502.8MB
Iteration 1000/1250  mem: 246.2/503.3MB
Iteration 1100/1250  mem: 273.5/503.3MB
Iteration 1200/1250  mem: 297.7/503.3MB
done building BART in 0.545 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 74 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 259/507MB
Iteration 200/1250  mem: 301.2/507MB
Iteration 300/1250  mem: 347.5/507MB
Iteration 400/1250  mem: 278.6/482.9MB
Iteration 500/1250  mem: 322.3/482.9MB
Iteration 600/1250  mem: 263.8/506.5MB
Iteration 700/1250  mem: 307.1/506.5MB
Iteration 800/1250  mem: 352.3/506.5MB
Iteration 900/1250  mem: 295.1/506.5MB
Iteration 1000/1250  mem: 341.6/506.5MB
Iteration 1100/1250  mem: 388/506.5MB
Iteration 1200/1250  mem: 328.8/507.5MB
done building BART in 1.053 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 46 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 381.1/507.5MB
Iteration 200/1250  mem: 407.5/507.5MB
Iteration 300/1250  mem: 322.3/489.7MB
Iteration 400/1250  mem: 346.6/489.7MB
Iteration 500/1250  mem: 371/489.7MB
Iteration 600/1250  mem: 397.2/489.7MB
Iteration 700/1250  mem: 311.9/507.5MB
Iteration 800/1250  mem: 337.8/507.5MB
Iteration 900/1250  mem: 361.7/507.5MB
Iteration 1000/1250  mem: 385.5/507.5MB
Iteration 1100/1250  mem: 411.4/507.5MB
Iteration 1200/1250  mem: 325.5/506.5MB
done building BART in 0.556 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 74 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 383.2/506.5MB
Iteration 200/1250  mem: 422/506.5MB
Iteration 300/1250  mem: 344.3/507.5MB
Iteration 400/1250  mem: 383.1/507.5MB
Iteration 500/1250  mem: 421.8/507.5MB
Iteration 600/1250  mem: 346.8/507.5MB
Iteration 700/1250  mem: 384.3/507.5MB
Iteration 800/1250  mem: 424.1/507.5MB
Iteration 900/1250  mem: 346.8/509.6MB
Iteration 1000/1250  mem: 386.3/509.6MB
Iteration 1100/1250  mem: 425.7/509.6MB
Iteration 1200/1250  mem: 462.7/509.6MB
done building BART in 1.028 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 27 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 85.3/477.6MB
Iteration 200/1250  mem: 100.6/477.6MB
Iteration 300/1250  mem: 115.9/477.6MB
Iteration 400/1250  mem: 131.2/477.6MB
Iteration 500/1250  mem: 146.5/477.6MB
Iteration 600/1250  mem: 161.8/477.6MB
Iteration 700/1250  mem: 177/477.6MB
Iteration 800/1250  mem: 78.2/477.6MB
Iteration 900/1250  mem: 93.2/477.6MB
Iteration 1000/1250  mem: 105.8/477.6MB
Iteration 1100/1250  mem: 120.9/477.6MB
Iteration 1200/1250  mem: 136/477.6MB
done building BART in 0.285 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 14 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 153.9/477.6MB
Iteration 200/1250  mem: 160.5/477.6MB
Iteration 300/1250  mem: 168.4/477.6MB
Iteration 400/1250  mem: 175/477.6MB
Iteration 500/1250  mem: 182.9/477.6MB
Iteration 600/1250  mem: 189.5/477.6MB
Iteration 700/1250  mem: 196/477.6MB
Iteration 800/1250  mem: 83.6/477.6MB
Iteration 900/1250  mem: 90.8/477.6MB
Iteration 1000/1250  mem: 98.1/477.6MB
Iteration 1100/1250  mem: 105.3/477.6MB
Iteration 1200/1250  mem: 112.6/477.6MB
done building BART in 0.17 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 168.6/477.6MB
Iteration 200/1250  mem: 93/477.6MB
Iteration 300/1250  mem: 142.2/477.6MB
Iteration 400/1250  mem: 189.7/477.6MB
Iteration 500/1250  mem: 113/477.6MB
Iteration 600/1250  mem: 161.3/477.6MB
Iteration 700/1250  mem: 209.5/477.6MB
Iteration 800/1250  mem: 133.8/477.6MB
Iteration 900/1250  mem: 183.7/477.6MB
Iteration 1000/1250  mem: 231.3/477.6MB
Iteration 1100/1250  mem: 154.5/477.6MB
Iteration 1200/1250  mem: 202.9/477.6MB
done building BART in 1.037 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 76 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 149.6/477.6MB
Iteration 200/1250  mem: 193.9/477.6MB
Iteration 300/1250  mem: 236.6/477.6MB
Iteration 400/1250  mem: 150.9/477.6MB
Iteration 500/1250  mem: 195.7/477.6MB
Iteration 600/1250  mem: 238.5/477.6MB
Iteration 700/1250  mem: 159/477.6MB
Iteration 800/1250  mem: 202.4/477.6MB
Iteration 900/1250  mem: 245.8/477.6MB
Iteration 1000/1250  mem: 289.2/477.6MB
Iteration 1100/1250  mem: 212.2/477.6MB
Iteration 1200/1250  mem: 253.8/477.6MB
done building BART in 0.966 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 87 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 202.9/477.6MB
Iteration 200/1250  mem: 248.5/477.6MB
Iteration 300/1250  mem: 294.1/477.6MB
Iteration 400/1250  mem: 208.5/477.6MB
Iteration 500/1250  mem: 254.6/477.6MB
Iteration 600/1250  mem: 300.6/477.6MB
Iteration 700/1250  mem: 219.7/477.6MB
Iteration 800/1250  mem: 264.4/477.6MB
Iteration 900/1250  mem: 311.4/477.6MB
Iteration 1000/1250  mem: 230/477.6MB
Iteration 1100/1250  mem: 275.5/477.6MB
Iteration 1200/1250  mem: 320.9/477.6MB
done building BART in 1.021 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 81 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 269.8/477.6MB
Iteration 200/1250  mem: 315.6/477.6MB
Iteration 300/1250  mem: 230.2/477.6MB
Iteration 400/1250  mem: 277.1/477.6MB
Iteration 500/1250  mem: 321.4/477.6MB
Iteration 600/1250  mem: 245.3/477.6MB
Iteration 700/1250  mem: 290.7/477.6MB
Iteration 800/1250  mem: 333.3/477.6MB
Iteration 900/1250  mem: 255.6/477.6MB
Iteration 1000/1250  mem: 301.3/477.6MB
Iteration 1100/1250  mem: 347/477.6MB
Iteration 1200/1250  mem: 271.5/477.6MB
done building BART in 1.012 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 23 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 308.3/477.6MB
Iteration 200/1250  mem: 322/477.6MB
Iteration 300/1250  mem: 332.9/477.6MB
Iteration 400/1250  mem: 343.9/477.6MB
Iteration 500/1250  mem: 357.6/477.6MB
Iteration 600/1250  mem: 368.5/477.6MB
Iteration 700/1250  mem: 379.5/477.6MB
Iteration 800/1250  mem: 390.4/477.6MB
Iteration 900/1250  mem: 273.2/477.6MB
Iteration 1000/1250  mem: 284.9/477.6MB
Iteration 1100/1250  mem: 296.7/477.6MB
Iteration 1200/1250  mem: 308.4/477.6MB
done building BART in 0.246 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 67 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 354.9/477.6MB
Iteration 200/1250  mem: 390.4/477.6MB
Iteration 300/1250  mem: 293.8/477.6MB
Iteration 400/1250  mem: 329.2/477.6MB
Iteration 500/1250  mem: 362.3/477.6MB
Iteration 600/1250  mem: 397.7/477.6MB
Iteration 700/1250  mem: 302.5/477.6MB
Iteration 800/1250  mem: 338.5/477.6MB
Iteration 900/1250  mem: 374.4/477.6MB
Iteration 1000/1250  mem: 407.9/477.6MB
Iteration 1100/1250  mem: 316.3/477.6MB
Iteration 1200/1250  mem: 352.1/477.6MB
done building BART in 0.733 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 13 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 379.6/477.6MB
Iteration 200/1250  mem: 387.9/477.6MB
Iteration 300/1250  mem: 393.5/477.6MB
Iteration 400/1250  mem: 399/477.6MB
Iteration 500/1250  mem: 407.4/477.6MB
Iteration 600/1250  mem: 412.9/477.6MB
Iteration 700/1250  mem: 421.3/477.6MB
Iteration 800/1250  mem: 426.8/477.6MB
Iteration 900/1250  mem: 304.1/477.6MB
Iteration 1000/1250  mem: 309.9/477.6MB
Iteration 1100/1250  mem: 317.7/477.6MB
Iteration 1200/1250  mem: 323.6/477.6MB
done building BART in 0.153 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 53 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 359.1/477.6MB
Iteration 200/1250  mem: 386.9/477.6MB
Iteration 300/1250  mem: 414.7/477.6MB
Iteration 400/1250  mem: 17.7/477.6MB
Iteration 500/1250  mem: 45.1/477.6MB
Iteration 600/1250  mem: 74.3/477.6MB
Iteration 700/1250  mem: 101.7/477.6MB
Iteration 800/1250  mem: 129.2/477.6MB
Iteration 900/1250  mem: 25.1/477.6MB
Iteration 1000/1250  mem: 53.5/477.6MB
Iteration 1100/1250  mem: 79.7/477.6MB
Iteration 1200/1250  mem: 108.1/477.6MB
done building BART in 0.631 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 62 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 158.2/477.6MB
Iteration 200/1250  mem: 59.1/477.6MB
Iteration 300/1250  mem: 92.6/477.6MB
Iteration 400/1250  mem: 126.1/477.6MB
Iteration 500/1250  mem: 158.4/477.6MB
Iteration 600/1250  mem: 56.5/477.6MB
Iteration 700/1250  mem: 89.9/477.6MB
Iteration 800/1250  mem: 123.4/477.6MB
Iteration 900/1250  mem: 156.8/477.6MB
Iteration 1000/1250  mem: 56.7/477.6MB
Iteration 1100/1250  mem: 91.7/477.6MB
Iteration 1200/1250  mem: 124.4/477.6MB
done building BART in 0.69 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 79 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 192.9/477.6MB
Iteration 200/1250  mem: 103/477.6MB
Iteration 300/1250  mem: 147.4/477.6MB
Iteration 400/1250  mem: 193.8/477.6MB
Iteration 500/1250  mem: 102/477.6MB
Iteration 600/1250  mem: 146.5/477.6MB
Iteration 700/1250  mem: 193.4/477.6MB
Iteration 800/1250  mem: 107.3/477.6MB
Iteration 900/1250  mem: 153.2/477.6MB
Iteration 1000/1250  mem: 199.1/477.6MB
Iteration 1100/1250  mem: 115.2/477.6MB
Iteration 1200/1250  mem: 159.9/477.6MB
done building BART in 0.998 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 72 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 232.3/477.6MB
Iteration 200/1250  mem: 155.8/477.6MB
Iteration 300/1250  mem: 204.7/477.6MB
Iteration 400/1250  mem: 133.8/477.6MB
Iteration 500/1250  mem: 184.2/477.6MB
Iteration 600/1250  mem: 234.6/477.6MB
Iteration 700/1250  mem: 172.9/477.6MB
Iteration 800/1250  mem: 222.9/477.6MB
Iteration 900/1250  mem: 272.9/477.6MB
Iteration 1000/1250  mem: 206.5/477.6MB
Iteration 1100/1250  mem: 255.1/477.6MB
Iteration 1200/1250  mem: 218.7/477.6MB
done building BART in 1.095 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 70 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 286/477.6MB
Iteration 200/1250  mem: 230.8/477.6MB
Iteration 300/1250  mem: 266.2/477.6MB
Iteration 400/1250  mem: 303.2/477.6MB
Iteration 500/1250  mem: 239.8/477.6MB
Iteration 600/1250  mem: 274.5/477.6MB
Iteration 700/1250  mem: 310.8/477.6MB
Iteration 800/1250  mem: 250.2/477.6MB
Iteration 900/1250  mem: 286.5/477.6MB
Iteration 1000/1250  mem: 321.3/477.6MB
Iteration 1100/1250  mem: 259/477.6MB
Iteration 1200/1250  mem: 292.9/477.6MB
done building BART in 0.788 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 26 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 229.5/477.6MB
Iteration 200/1250  mem: 243.1/477.6MB
Iteration 300/1250  mem: 255.5/477.6MB
Iteration 400/1250  mem: 269.1/477.6MB
Iteration 500/1250  mem: 282.7/477.6MB
Iteration 600/1250  mem: 296.3/477.6MB
Iteration 700/1250  mem: 309.9/477.6MB
Iteration 800/1250  mem: 323.5/477.6MB
Iteration 900/1250  mem: 337.1/477.6MB
Iteration 1000/1250  mem: 246.4/477.6MB
Iteration 1100/1250  mem: 260.5/477.6MB
Iteration 1200/1250  mem: 273/477.6MB
done building BART in 0.29 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 17 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 292.9/477.6MB
Iteration 200/1250  mem: 301.6/477.6MB
Iteration 300/1250  mem: 310.4/477.6MB
Iteration 400/1250  mem: 321.3/477.6MB
Iteration 500/1250  mem: 330/477.6MB
Iteration 600/1250  mem: 338.7/477.6MB
Iteration 700/1250  mem: 244.8/477.6MB
Iteration 800/1250  mem: 253.6/477.6MB
Iteration 900/1250  mem: 262.3/477.6MB
Iteration 1000/1250  mem: 271.1/477.6MB
Iteration 1100/1250  mem: 281.6/477.6MB
Iteration 1200/1250  mem: 290.3/477.6MB
done building BART in 0.188 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 42 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 321.9/477.6MB
Iteration 200/1250  mem: 347.7/477.6MB
Iteration 300/1250  mem: 268.6/477.6MB
Iteration 400/1250  mem: 293.8/477.6MB
Iteration 500/1250  mem: 319.1/477.6MB
Iteration 600/1250  mem: 346.6/477.6MB
Iteration 700/1250  mem: 38.4/477.6MB
Iteration 800/1250  mem: 62.1/477.6MB
Iteration 900/1250  mem: 88.9/477.6MB
Iteration 1000/1250  mem: 114.2/477.6MB
Iteration 1100/1250  mem: 141.1/477.6MB
Iteration 1200/1250  mem: 67.1/477.6MB
done building BART in 0.628 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 74 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 129.4/477.6MB
Iteration 200/1250  mem: 66.8/477.6MB
Iteration 300/1250  mem: 111.6/477.6MB
Iteration 400/1250  mem: 156.4/477.6MB
Iteration 500/1250  mem: 95/477.6MB
Iteration 600/1250  mem: 139.6/477.6MB
Iteration 700/1250  mem: 82.4/477.6MB
Iteration 800/1250  mem: 126.7/477.6MB
Iteration 900/1250  mem: 173.1/477.6MB
Iteration 1000/1250  mem: 109.9/477.6MB
Iteration 1100/1250  mem: 155.5/477.6MB
Iteration 1200/1250  mem: 201.2/477.6MB
done building BART in 1.037 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 46 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 143.3/477.6MB
Iteration 200/1250  mem: 168.1/477.6MB
Iteration 300/1250  mem: 195.5/477.6MB
Iteration 400/1250  mem: 220.3/477.6MB
Iteration 500/1250  mem: 126.9/477.6MB
Iteration 600/1250  mem: 153.3/477.6MB
Iteration 700/1250  mem: 179.7/477.6MB
Iteration 800/1250  mem: 203.8/477.6MB
Iteration 900/1250  mem: 230.2/477.6MB
Iteration 1000/1250  mem: 143/477.6MB
Iteration 1100/1250  mem: 167.5/477.6MB
Iteration 1200/1250  mem: 192.1/477.6MB
done building BART in 0.531 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 74 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 250.6/477.6MB
Iteration 200/1250  mem: 174.8/477.6MB
Iteration 300/1250  mem: 215.3/477.6MB
Iteration 400/1250  mem: 254/477.6MB
Iteration 500/1250  mem: 172.7/477.6MB
Iteration 600/1250  mem: 212.5/477.6MB
Iteration 700/1250  mem: 252.3/477.6MB
Iteration 800/1250  mem: 174.1/477.6MB
Iteration 900/1250  mem: 213.3/477.6MB
Iteration 1000/1250  mem: 252.5/477.6MB
Iteration 1100/1250  mem: 176/477.6MB
Iteration 1200/1250  mem: 213.8/477.6MB
done building BART in 0.843 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 27 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 254.7/477.6MB
Iteration 200/1250  mem: 267.7/477.6MB
Iteration 300/1250  mem: 283.3/477.6MB
Iteration 400/1250  mem: 175.5/477.6MB
Iteration 500/1250  mem: 190.1/477.6MB
Iteration 600/1250  mem: 204.7/477.6MB
Iteration 700/1250  mem: 219.4/477.6MB
Iteration 800/1250  mem: 234/477.6MB
Iteration 900/1250  mem: 248.7/477.6MB
Iteration 1000/1250  mem: 263.3/477.6MB
Iteration 1100/1250  mem: 277.9/477.6MB
Iteration 1200/1250  mem: 292.6/477.6MB
done building BART in 0.298 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 14 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 195.6/477.6MB
Iteration 200/1250  mem: 200.9/477.6MB
Iteration 300/1250  mem: 208.8/477.6MB
Iteration 400/1250  mem: 216.8/477.6MB
Iteration 500/1250  mem: 224.7/477.6MB
Iteration 600/1250  mem: 230/477.6MB
Iteration 700/1250  mem: 237.9/477.6MB
Iteration 800/1250  mem: 245.8/477.6MB
Iteration 900/1250  mem: 251.1/477.6MB
Iteration 1000/1250  mem: 259.1/477.6MB
Iteration 1100/1250  mem: 267/477.6MB
Iteration 1200/1250  mem: 274.9/477.6MB
done building BART in 0.143 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 208.1/477.6MB
Iteration 200/1250  mem: 256.7/477.6MB
Iteration 300/1250  mem: 306.9/477.6MB
Iteration 400/1250  mem: 228/477.6MB
Iteration 500/1250  mem: 275.2/477.6MB
Iteration 600/1250  mem: 324.5/477.6MB
Iteration 700/1250  mem: 249.8/477.6MB
Iteration 800/1250  mem: 299.9/477.6MB
Iteration 900/1250  mem: 225.2/477.6MB
Iteration 1000/1250  mem: 274.2/477.6MB
Iteration 1100/1250  mem: 323.3/477.6MB
Iteration 1200/1250  mem: 244.3/477.6MB
done building BART in 1.033 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 76 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 317.6/477.6MB
Iteration 200/1250  mem: 361.2/477.6MB
Iteration 300/1250  mem: 275.9/477.6MB
Iteration 400/1250  mem: 320.3/477.6MB
Iteration 500/1250  mem: 364.8/477.6MB
Iteration 600/1250  mem: 281.5/477.6MB
Iteration 700/1250  mem: 323.9/477.6MB
Iteration 800/1250  mem: 368.7/477.6MB
Iteration 900/1250  mem: 289.9/477.6MB
Iteration 1000/1250  mem: 333.4/477.6MB
Iteration 1100/1250  mem: 379.4/477.6MB
Iteration 1200/1250  mem: 300.4/477.6MB
done building BART in 0.928 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 87 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 372/477.6MB
Iteration 200/1250  mem: 56.9/481.3MB
Iteration 300/1250  mem: 102.5/481.3MB
Iteration 400/1250  mem: 150.3/481.3MB
Iteration 500/1250  mem: 66.8/480.8MB
Iteration 600/1250  mem: 112.9/480.8MB
Iteration 700/1250  mem: 159.1/480.8MB
Iteration 800/1250  mem: 80.7/479.7MB
Iteration 900/1250  mem: 126.3/479.7MB
Iteration 1000/1250  mem: 171.9/479.7MB
Iteration 1100/1250  mem: 92.3/480.2MB
Iteration 1200/1250  mem: 136.4/480.2MB
done building BART in 1.088 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 81 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 210.1/480.2MB
Iteration 200/1250  mem: 128.3/481.3MB
Iteration 300/1250  mem: 174.7/481.3MB
Iteration 400/1250  mem: 221.1/481.3MB
Iteration 500/1250  mem: 134.6/481.8MB
Iteration 600/1250  mem: 179.1/481.8MB
Iteration 700/1250  mem: 225.8/481.8MB
Iteration 800/1250  mem: 145.4/482.3MB
Iteration 900/1250  mem: 191.8/482.3MB
Iteration 1000/1250  mem: 238.2/482.3MB
Iteration 1100/1250  mem: 155.6/482.9MB
Iteration 1200/1250  mem: 201.8/482.9MB
done building BART in 0.963 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 23 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 240.3/482.9MB
Iteration 200/1250  mem: 254.3/482.9MB
Iteration 300/1250  mem: 135.3/483.4MB
Iteration 400/1250  mem: 147.5/483.4MB
Iteration 500/1250  mem: 159.7/483.4MB
Iteration 600/1250  mem: 171.9/483.4MB
Iteration 700/1250  mem: 182.3/483.4MB
Iteration 800/1250  mem: 194.5/483.4MB
Iteration 900/1250  mem: 206.7/483.4MB
Iteration 1000/1250  mem: 217.1/483.4MB
Iteration 1100/1250  mem: 229.3/483.4MB
Iteration 1200/1250  mem: 241.5/483.4MB
done building BART in 0.253 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 67 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 153.9/483.4MB
Iteration 200/1250  mem: 189.8/483.4MB
Iteration 300/1250  mem: 225.7/483.4MB
Iteration 400/1250  mem: 260/483.4MB
Iteration 500/1250  mem: 160.9/483.4MB
Iteration 600/1250  mem: 195.8/483.4MB
Iteration 700/1250  mem: 232.8/483.4MB
Iteration 800/1250  mem: 267.7/483.4MB
Iteration 900/1250  mem: 173.3/483.4MB
Iteration 1000/1250  mem: 208.1/483.4MB
Iteration 1100/1250  mem: 242.9/483.4MB
Iteration 1200/1250  mem: 277.7/483.4MB
done building BART in 0.712 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 13 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 174.6/482.9MB
Iteration 200/1250  mem: 183.1/482.9MB
Iteration 300/1250  mem: 188.8/482.9MB
Iteration 400/1250  mem: 194.5/482.9MB
Iteration 500/1250  mem: 203.1/482.9MB
Iteration 600/1250  mem: 208.8/482.9MB
Iteration 700/1250  mem: 214.5/482.9MB
Iteration 800/1250  mem: 223.1/482.9MB
Iteration 900/1250  mem: 228.8/482.9MB
Iteration 1000/1250  mem: 234.5/482.9MB
Iteration 1100/1250  mem: 243/482.9MB
Iteration 1200/1250  mem: 248.7/482.9MB
done building BART in 0.121 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 53 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 284/482.9MB
Iteration 200/1250  mem: 177.3/483.4MB
Iteration 300/1250  mem: 206.9/483.4MB
Iteration 400/1250  mem: 234.6/483.4MB
Iteration 500/1250  mem: 262.2/483.4MB
Iteration 600/1250  mem: 289.8/483.4MB
Iteration 700/1250  mem: 184.3/485MB
Iteration 800/1250  mem: 212.2/485MB
Iteration 900/1250  mem: 240.1/485MB
Iteration 1000/1250  mem: 267.9/485MB
Iteration 1100/1250  mem: 295.8/485MB
Iteration 1200/1250  mem: 323.7/485MB
done building BART in 0.605 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 62 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 243.9/485.5MB
Iteration 200/1250  mem: 278.8/485.5MB
Iteration 300/1250  mem: 310.7/485.5MB
Iteration 400/1250  mem: 207.4/486.5MB
Iteration 500/1250  mem: 241/486.5MB
Iteration 600/1250  mem: 274.5/486.5MB
Iteration 700/1250  mem: 308.1/486.5MB
Iteration 800/1250  mem: 341.6/486.5MB
Iteration 900/1250  mem: 239/486.5MB
Iteration 1000/1250  mem: 273.1/486.5MB
Iteration 1100/1250  mem: 307.2/486.5MB
Iteration 1200/1250  mem: 341.4/486.5MB
done building BART in 0.698 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 79 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 273.3/486.5MB
Iteration 200/1250  mem: 320.1/486.5MB
Iteration 300/1250  mem: 366.9/486.5MB
Iteration 400/1250  mem: 275.1/486.5MB
Iteration 500/1250  mem: 321.7/486.5MB
Iteration 600/1250  mem: 368.3/486.5MB
Iteration 700/1250  mem: 282.7/479.2MB
Iteration 800/1250  mem: 332.6/479.2MB
Iteration 900/1250  mem: 377/479.2MB
Iteration 1000/1250  mem: 300.6/478.7MB
Iteration 1100/1250  mem: 347.8/478.7MB
Iteration 1200/1250  mem: 394.9/478.7MB
done building BART in 1.011 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 72 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 340.6/480.8MB
Iteration 200/1250  mem: 389.5/480.8MB
Iteration 300/1250  mem: 307.1/481.3MB
Iteration 400/1250  mem: 357.4/481.3MB
Iteration 500/1250  mem: 407.6/481.3MB
Iteration 600/1250  mem: 344.7/477.6MB
Iteration 700/1250  mem: 394.8/477.6MB
Iteration 800/1250  mem: 345.3/477.6MB
Iteration 900/1250  mem: 398.7/477.6MB
Iteration 1000/1250  mem: 447.2/477.6MB
Iteration 1100/1250  mem: 171.6/504.4MB
Iteration 1200/1250  mem: 224.3/504.4MB
done building BART in 1.424 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 70 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 197.7/497MB
Iteration 200/1250  mem: 235.1/497MB
Iteration 300/1250  mem: 161/504.4MB
Iteration 400/1250  mem: 197.5/504.4MB
Iteration 500/1250  mem: 234/504.4MB
Iteration 600/1250  mem: 164.7/481.3MB
Iteration 700/1250  mem: 202.1/481.3MB
Iteration 800/1250  mem: 237.3/481.3MB
Iteration 900/1250  mem: 168.1/504.9MB
Iteration 1000/1250  mem: 204.1/504.9MB
Iteration 1100/1250  mem: 240.2/504.9MB
Iteration 1200/1250  mem: 278.5/504.9MB
done building BART in 0.79 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 26 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 204.7/504.4MB
Iteration 200/1250  mem: 218.6/504.4MB
Iteration 300/1250  mem: 232.5/504.4MB
Iteration 400/1250  mem: 244.1/504.4MB
Iteration 500/1250  mem: 258/504.4MB
Iteration 600/1250  mem: 271.9/504.4MB
Iteration 700/1250  mem: 285.8/504.4MB
Iteration 800/1250  mem: 190.7/505.4MB
Iteration 900/1250  mem: 203.8/505.4MB
Iteration 1000/1250  mem: 217/505.4MB
Iteration 1100/1250  mem: 232.3/505.4MB
Iteration 1200/1250  mem: 245.5/505.4MB
done building BART in 0.266 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 17 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 265.1/505.4MB
Iteration 200/1250  mem: 274.5/505.4MB
Iteration 300/1250  mem: 284/505.4MB
Iteration 400/1250  mem: 291.1/505.4MB
Iteration 500/1250  mem: 189.5/505.4MB
Iteration 600/1250  mem: 199.2/505.4MB
Iteration 700/1250  mem: 207.3/505.4MB
Iteration 800/1250  mem: 216.9/505.4MB
Iteration 900/1250  mem: 226.6/505.4MB
Iteration 1000/1250  mem: 234.7/505.4MB
Iteration 1100/1250  mem: 244.4/505.4MB
Iteration 1200/1250  mem: 252.4/505.4MB
done building BART in 0.178 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 42 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 285.6/505.4MB
Iteration 200/1250  mem: 201.4/506.5MB
Iteration 300/1250  mem: 229.2/506.5MB
Iteration 400/1250  mem: 254.9/506.5MB
Iteration 500/1250  mem: 281.6/506.5MB
Iteration 600/1250  mem: 307.2/506.5MB
Iteration 700/1250  mem: 224.7/507MB
Iteration 800/1250  mem: 252.5/507MB
Iteration 900/1250  mem: 278.8/507MB
Iteration 1000/1250  mem: 306.6/507MB
Iteration 1100/1250  mem: 228.9/509.1MB
Iteration 1200/1250  mem: 255.5/509.1MB
done building BART in 0.557 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 74 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 317.2/509.1MB
Iteration 200/1250  mem: 249.4/492.8MB
Iteration 300/1250  mem: 293.8/492.8MB
Iteration 400/1250  mem: 340.2/492.8MB
Iteration 500/1250  mem: 272.8/510.7MB
Iteration 600/1250  mem: 319.4/510.7MB
Iteration 700/1250  mem: 366/510.7MB
Iteration 800/1250  mem: 302.7/509.1MB
Iteration 900/1250  mem: 349.3/509.1MB
Iteration 1000/1250  mem: 286.9/510.1MB
Iteration 1100/1250  mem: 330.1/510.1MB
Iteration 1200/1250  mem: 378/510.1MB
done building BART in 0.995 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 46 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 319.6/510.7MB
Iteration 200/1250  mem: 345.5/510.7MB
Iteration 300/1250  mem: 370/510.7MB
Iteration 400/1250  mem: 395.9/510.7MB
Iteration 500/1250  mem: 301.8/510.7MB
Iteration 600/1250  mem: 327.6/510.7MB
Iteration 700/1250  mem: 353.5/510.7MB
Iteration 800/1250  mem: 379.4/510.7MB
Iteration 900/1250  mem: 405.2/510.7MB
Iteration 1000/1250  mem: 315.1/496MB
Iteration 1100/1250  mem: 339.1/496MB
Iteration 1200/1250  mem: 365.2/496MB
done building BART in 0.57 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 70 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 433.6/496MB
Iteration 200/1250  mem: 366.2/510.1MB
Iteration 300/1250  mem: 415.9/510.1MB
Iteration 400/1250  mem: 339.5/509.6MB
Iteration 500/1250  mem: 387.5/509.6MB
Iteration 600/1250  mem: 437.5/509.6MB
Iteration 700/1250  mem: 365.9/511.7MB
Iteration 800/1250  mem: 414/511.7MB
Iteration 900/1250  mem: 338/511.7MB
Iteration 1000/1250  mem: 388.3/511.7MB
Iteration 1100/1250  mem: 436.2/511.7MB
Iteration 1200/1250  mem: 361.9/512.2MB
done building BART in 0.965 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
Bayesian Additive Regression Trees 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  num_trees  k          alpha      beta       nu         RMSE       Rsquared 
  13         4.2161787  0.9800921  1.8677506  3.3257891  0.2332049  0.9479102
  14         4.4663653  0.9333352  2.3991429  4.8504620  0.2116022  0.9530697
  17         4.0056812  0.9257908  1.5125866  2.6267655  0.2350930  0.9608929
  23         0.5858385  0.9204165  0.5893979  3.9607923  0.2673740  0.9257469
  26         1.2489455  0.9536479  3.9846083  4.7719082  0.2221610  0.9459795
  27         2.8024783  0.9950826  1.0825287  0.2229974  0.3687083  0.8480092
  42         2.8820499  0.9061100  0.6828556  3.3522290  0.3812453  0.8553564
  46         4.1502735  0.9839629  1.6247698  4.8043217  0.3058038  0.9129386
  53         4.7834859  0.9833856  2.8784044  1.5993711  0.3020201  0.9040264
  62         1.9185651  0.9823315  1.9148177  3.3385621  0.2926301  0.9138532
  67         0.4912600  0.9414942  2.4528652  3.6419385  0.5270376  0.6764941
  70         3.4747581  0.9088153  3.4560448  1.0877748  0.2017629  0.9608137
  72         2.8739395  0.9751643  0.6075525  1.6649636  0.4664222  0.7873723
  74         1.5099219  0.9200719  2.2285266  1.6030771  0.3138442  0.8803398
  74         1.8337641  0.9614880  0.8658444  4.0057029  0.4576505  0.7762741
  76         1.9617439  0.9196219  1.0341828  1.4447854  0.4276581  0.8011293
  79         3.9956439  0.9027687  1.1034380  4.4158173  0.4105916  0.8397052
  81         3.5914827  0.9913263  0.1529084  2.3983467  0.3492945  0.8814309
  87         2.7737414  0.9261390  3.9478597  4.6574652  0.2785819  0.9206480
  90         1.4735104  0.9420207  1.7549800  1.7043492  0.3561699  0.8460758
  MAE        Selected
  0.1314566          
  0.1201548          
  0.1385814          
  0.1486203          
  0.1311251          
  0.2771441          
  0.2483045          
  0.1895251          
  0.1886454          
  0.1988427          
  0.4149023          
  0.1285231  *       
  0.3080868          
  0.2237536          
  0.3110271          
  0.3080601          
  0.2627052          
  0.2222389          
  0.1889618          
  0.2601405          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were num_trees = 70, k = 3.474758, alpha
 = 0.9088153, beta = 3.456045 and nu = 1.087775.
[1] "Sat Mar 10 03:16:49 2018"
Error in investigate_var_importance(object, plot = FALSE) : 
  could not find function "investigate_var_importance"
Error : package arm is required
Error : package arm is required
Error : package arm is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:17:03 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "bayesglm"                
t=100, m=5
t=200, m=4
t=300, m=6
t=400, m=3
t=500, m=7
t=600, m=6
t=700, m=5
t=800, m=5
t=900, m=3
t=100, m=5
t=200, m=5
t=300, m=5
t=400, m=3
t=500, m=6
t=600, m=6
t=700, m=5
t=800, m=4
t=900, m=4
t=100, m=3
t=200, m=2
t=300, m=7
t=400, m=6
t=500, m=3
t=600, m=2
t=700, m=3
t=800, m=5
t=900, m=4
t=100, m=3
t=200, m=5
t=300, m=4
t=400, m=6
t=500, m=2
t=600, m=7
t=700, m=7
t=800, m=5
t=900, m=2
The Bayesian lasso 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  sparsity    RMSE       Rsquared   MAE        Selected
  0.04353090  0.5036042  0.7151524  0.3893680          
  0.04607404  0.5036042  0.7151524  0.3893680          
  0.08097647  0.5036042  0.7151524  0.3893680          
  0.14289509  0.5036042  0.7151524  0.3893680          
  0.18011931  0.5036042  0.7151524  0.3893680          
  0.19184555  0.5036042  0.7151524  0.3893680          
  0.36095149  0.4989344  0.7197342  0.3867196  *       
  0.40285652  0.5072114  0.7083020  0.3923069          
  0.48235978  0.5083947  0.7062431  0.3926628          
  0.57551994  0.5030918  0.7167247  0.3858476          
  0.63314495  0.5083804  0.7140191  0.3863153          
  0.66271528  0.5083804  0.7140191  0.3863153          
  0.68545433  0.5083804  0.7140191  0.3863153          
  0.70332297  0.5083804  0.7140191  0.3863153          
  0.70619491  0.5083804  0.7140191  0.3863153          
  0.73549568  0.5083804  0.7140191  0.3863153          
  0.76438883  0.5083804  0.7140191  0.3863153          
  0.79102354  0.5083804  0.7140191  0.3863153          
  0.85138569  0.5083804  0.7140191  0.3863153          
  0.88143842  0.5083804  0.7140191  0.3863153          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was sparsity = 0.3609515.
[1] "Sat Mar 10 03:17:19 2018"
t=100, m=4
t=200, m=4
t=300, m=4
t=400, m=4
t=500, m=5
t=600, m=5
t=700, m=4
t=800, m=4
t=900, m=4
t=100, m=2
t=200, m=8
t=300, m=3
t=400, m=4
t=500, m=6
t=600, m=3
t=700, m=5
t=800, m=5
t=900, m=4
t=100, m=3
t=200, m=6
t=300, m=4
t=400, m=5
t=500, m=2
t=600, m=3
t=700, m=3
t=800, m=1
t=900, m=4
t=100, m=5
t=200, m=5
t=300, m=5
t=400, m=6
t=500, m=3
t=600, m=4
t=700, m=5
t=800, m=6
t=900, m=5
Bayesian Ridge Regression (Model Averaged) 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results:

  RMSE       Rsquared  MAE      
  0.5028523  0.71498   0.3901105

[1] "Sat Mar 10 03:17:35 2018"
t=100, m=3
t=200, m=4
t=300, m=4
t=400, m=3
t=500, m=3
t=600, m=3
t=700, m=3
t=800, m=7
t=900, m=5
t=100, m=4
t=200, m=1
t=300, m=3
t=400, m=1
t=500, m=4
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=3
t=100, m=1
t=200, m=2
t=300, m=4
t=400, m=1
t=500, m=1
t=600, m=4
t=700, m=3
t=800, m=2
t=900, m=2
t=100, m=3
t=200, m=2
t=300, m=1
t=400, m=2
t=500, m=1
t=600, m=5
t=700, m=5
t=800, m=4
t=900, m=3
Bayesian Ridge Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results:

  RMSE       Rsquared   MAE      
  0.5089891  0.7099824  0.4023888

[1] "Sat Mar 10 03:17:49 2018"
Boosted Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  nu          mstop  RMSE       Rsquared   MAE        Selected
  0.05985295  316    0.5825033  0.6123235  0.4100424  *       
  0.07118345   72    0.6448762  0.6230972  0.3951062          
  0.15062367   90    0.5863932  0.6106123  0.3989375          
  0.17752655  440    0.6165023  0.5869298  0.4590635          
  0.18188865  353    0.6109091  0.5925100  0.4528913          
  0.22068494  351    0.6166582  0.5865599  0.4592929          
  0.23084410  288    0.6130609  0.5904126  0.4552003          
  0.23601692  368    0.6192662  0.5841929  0.4620118          
  0.33329422  425    0.6252442  0.5775883  0.4691353          
  0.33673690   96    0.5898566  0.6107536  0.4325888          
  0.34529795  343    0.6238060  0.5791615  0.4674277          
  0.34626957  181    0.6137722  0.5895308  0.4561653          
  0.41727602  331    0.6249973  0.5779724  0.4687956          
  0.43125962  395    0.6258756  0.5769419  0.4698732          
  0.47967814  382    0.6261797  0.5765371  0.4702711          
  0.48088061   41    0.5842934  0.6145503  0.4219883          
  0.49820277  202    0.6226150  0.5802798  0.4661537          
  0.50609821   22    0.5864233  0.6094071  0.4000058          
  0.53607057   23    0.5858505  0.6129462  0.4066251          
  0.57406161  241    0.6256671  0.5771911  0.4695965          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 316 and nu = 0.05985295.
[1] "Sat Mar 10 03:19:14 2018"
Conditional Inference Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE        Selected
  1     0.8214214  0.3581655  0.5162125          
  2     0.7962095  0.5307150  0.4965662          
  4     0.6914677  0.7317115  0.4116989          
  5     0.6230950  0.7685046  0.3571171          
  6     0.5430696  0.8099894  0.2932681          
  7     0.4667157  0.8382366  0.2381568          
  8     0.4209976  0.8434418  0.2151350  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 8.
[1] "Sat Mar 10 03:19:32 2018"
Error in varimp(object, ...) : could not find function "varimp"
Conditional Inference Tree 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  mincriterion  RMSE       Rsquared   MAE        Selected
  0.04353090    0.4363769  0.7570957  0.2881059          
  0.04607404    0.4363769  0.7570957  0.2881059          
  0.08097647    0.4363769  0.7570957  0.2881059          
  0.14289509    0.4363769  0.7570957  0.2881059          
  0.18011931    0.4363769  0.7570957  0.2881059          
  0.19184555    0.4363769  0.7570957  0.2881059          
  0.36095149    0.4363769  0.7570957  0.2881059          
  0.40285652    0.4363769  0.7570957  0.2881059          
  0.48235978    0.4363769  0.7570957  0.2881059          
  0.57551994    0.4363769  0.7570957  0.2881059          
  0.63314495    0.4363769  0.7570957  0.2881059          
  0.66271528    0.4363769  0.7570957  0.2881059          
  0.68545433    0.4363769  0.7570957  0.2881059          
  0.70332297    0.4363769  0.7570957  0.2881059          
  0.70619491    0.4363769  0.7570957  0.2881059          
  0.73549568    0.4363769  0.7570957  0.2881059          
  0.76438883    0.4363769  0.7570957  0.2881059          
  0.79102354    0.4363769  0.7570957  0.2881059          
  0.85138569    0.4363769  0.7570957  0.2881059          
  0.88143842    0.4363769  0.7570957  0.2881059  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mincriterion = 0.8814384.
[1] "Sat Mar 10 03:19:48 2018"
Conditional Inference Tree 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  maxdepth  mincriterion  RMSE       Rsquared   MAE        Selected
   1        0.8432357     0.6789408  0.3306216  0.4867791          
   1        0.8932731     0.6789408  0.3306216  0.4867791          
   2        0.8011362     0.4676200  0.6922072  0.3388849          
   3        0.1171677     0.4424466  0.7421640  0.3071047          
   3        0.2497891     0.4424466  0.7421640  0.3071047          
   3        0.5604957     0.4424466  0.7421640  0.3071047          
   6        0.5764100     0.4363769  0.7570957  0.2881059  *       
   7        0.8300547     0.4363769  0.7570957  0.2881059          
   8        0.9566972     0.4363769  0.7570957  0.2881059          
   9        0.3837130     0.4363769  0.7570957  0.2881059          
  10        0.0982520     0.4363769  0.7570957  0.2881059          
  10        0.6949516     0.4363769  0.7570957  0.2881059          
  11        0.3019844     0.4363769  0.7570957  0.2881059          
  11        0.3667528     0.4363769  0.7570957  0.2881059          
  11        0.5747879     0.4363769  0.7570957  0.2881059          
  12        0.3923488     0.4363769  0.7570957  0.2881059          
  12        0.7182965     0.4363769  0.7570957  0.2881059          
  12        0.7991288     0.4363769  0.7570957  0.2881059          
  13        0.5547483     0.4363769  0.7570957  0.2881059          
  14        0.2947021     0.4363769  0.7570957  0.2881059          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were maxdepth = 6 and mincriterion
 = 0.57641.
[1] "Sat Mar 10 03:20:05 2018"
Cubist 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  committees  neighbors  RMSE       Rsquared   MAE         Selected
  10          6          0.1262611  0.9780317  0.07214004  *       
  12          1          0.1342660  0.9761204  0.07898024          
  25          1          0.1455805  0.9782247  0.08140304          
  30          8          0.1470882  0.9784392  0.08102418          
  31          7          0.1492425  0.9783970  0.08246267          
  37          7          0.1518385  0.9779871  0.08293213          
  39          5          0.1523295  0.9775835  0.08284267          
  40          7          0.1518568  0.9779105  0.08245218          
  56          8          0.1564936  0.9768315  0.08464974          
  57          1          0.1548098  0.9763883  0.08269976          
  58          3          0.1536865  0.9766257  0.08175646          
  58          6          0.1547015  0.9770498  0.08416044          
  70          6          0.1419707  0.9796028  0.07943914          
  72          7          0.1408311  0.9798503  0.07881647          
  80          7          0.1361149  0.9803618  0.07749860          
  81          0          0.1348407  0.9804617  0.07621649          
  84          4          0.1346055  0.9796373  0.07609664          
  85          0          0.1338389  0.9803538  0.07626248          
  90          0          0.1312959  0.9808977  0.07541390          
  96          4          0.1292507  0.9804147  0.07439144          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were committees = 10 and neighbors = 6.
[1] "Sat Mar 10 03:20:52 2018"
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
Stacked AutoEncoder Deep Neural Network 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  layer1  layer2  layer3  hidden_dropout  visible_dropout  RMSE     
   2      18       8      0.41985001      0.67906468       0.8363571
   2      18      17      0.32685636      0.46561048       0.8495138
   3      17       6      0.26470266      0.36774717       0.8306641
   4       4       5      0.10314463      0.55451092       0.8299263
   5       6      12      0.69730645      0.66806715       0.8277691
   5      12      20      0.18944253      0.03121964       0.8662795
   8      12       3      0.11949972      0.46931206       0.8390300
   9      17      17      0.28433472      0.67260504       0.8461310
  11      20      17      0.50372077      0.22391195       0.8395730
  12       9      17      0.33509310      0.46739869       0.9058318
  14       3       9      0.42925141      0.50987138       0.8570249
  14      15       3      0.60480784      0.15228848       0.8412752
  15       7       5      0.38999216      0.22443080       0.8353924
  15       8      13      0.15152277      0.56079841       0.8303972
  15       9       5      0.18098199      0.20226996       0.8317267
  15      12      16      0.10632169      0.23309491       0.8318891
  16      17       2      0.19310164      0.61821442       0.8397902
  17      15      19      0.02675897      0.33576854       0.8304008
  18       7       9      0.30712149      0.23860889       0.8336442
  18      12       6      0.69087544      0.65204513       0.8425879
  Rsquared     MAE        Selected
  0.035677057  0.5289507          
  0.187976825  0.5734246          
  0.077886222  0.5249141          
  0.209018258  0.5234697          
  0.059235386  0.5322979  *       
  0.133828996  0.5989827          
  0.002572031  0.5325462          
  0.148168810  0.5482839          
  0.115600836  0.5325723          
  0.042747488  0.6254600          
  0.045484774  0.5562944          
  0.186783541  0.5342967          
  0.046651097  0.5270661          
  0.046294081  0.5239600          
  0.074236830  0.5239992          
  0.190582242  0.5283004          
  0.059461434  0.5351040          
  0.147647667  0.5235900          
  0.100953749  0.5354966          
  0.207268792  0.5360227          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were layer1 = 5, layer2 = 6, layer3 =
 12, hidden_dropout = 0.6973064 and visible_dropout = 0.6680672.
[1] "Sat Mar 10 03:21:08 2018"
Multivariate Adaptive Regression Spline 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  degree  nprune  RMSE       Rsquared   MAE        Selected
  1        3      0.2437302  0.9092638  0.1866253  *       
  1        4      0.2579064  0.9125975  0.1895978          
  1        9      0.2855705  0.8926330  0.2179846          
  1       10      0.2855705  0.8926330  0.2179846          
  1       11      0.2855705  0.8926330  0.2179846          
  1       13      0.2855705  0.8926330  0.2179846          
  2        2      0.5464195  0.6636520  0.4147201          
  2        3      0.3728785  0.7906069  0.2433992          
  2        4      0.4317512  0.7723249  0.2585959          
  2        6      0.4187891  0.7892015  0.2277376          
  2        7      0.4474680  0.7785372  0.2578754          
  2        8      0.4431140  0.7861821  0.2622379          
  2       10      0.4351885  0.7951213  0.2536638          
  2       11      0.4351885  0.7951213  0.2536638          
  2       12      0.4351885  0.7951213  0.2536638          
  2       13      0.4351885  0.7951213  0.2536638          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nprune = 3 and degree = 1.
[1] "Sat Mar 10 03:21:24 2018"
Extreme Learning Machine 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  nhid  actfun   RMSE       Rsquared    MAE        Selected
   1    tansig   0.8181210  0.07935202  0.5293642          
   2    tansig   0.7772076  0.20551284  0.5206233          
   3    sin      0.7924976  0.13256600  0.5049720          
   4    purelin  0.6854035  0.38138242  0.4945309          
   4    sin      0.6634474  0.39095200  0.4947989          
   7    purelin  0.6554846  0.47814612  0.4469184          
   8    tansig   0.8275956  0.20350443  0.5885643          
  10    tansig   0.7108137  0.41668494  0.5460381          
  11    radbas   0.7752516  0.33274945  0.5507629          
  13    purelin  0.5359023  0.68609267  0.4230051          
  13    sin      0.5996175  0.61803702  0.4348867          
  14    purelin  0.5359023  0.68609267  0.4230051          
  14    radbas   0.8076367  0.30855996  0.5995648          
  15    tansig   0.6319329  0.59667260  0.4955411          
  16    purelin  0.5359023  0.68609267  0.4230051          
  17    purelin  0.5359023  0.68609267  0.4230051  *       
  17    radbas   0.6995438  0.37726251  0.4996420          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nhid = 17 and actfun = purelin.
[1] "Sat Mar 10 03:21:39 2018"
Elasticnet 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  lambda        fraction   RMSE       Rsquared   MAE        Selected
  1.824675e-05  0.8432357  0.5119041  0.7062205  0.4016130          
  1.889923e-05  0.8932731  0.5184366  0.7011419  0.4077925          
  3.060968e-05  0.8011362  0.5076358  0.7092315  0.3966884          
  7.200631e-05  0.1171677  0.7155278  0.7447680  0.4130437          
  1.204248e-04  0.2497891  0.5958083  0.7447680  0.3126051          
  1.416033e-04  0.5604957  0.4896829  0.7185684  0.3561768  *       
  1.464566e-03  0.5764100  0.4914655  0.7169218  0.3605114          
  2.612998e-03  0.8300547  0.5104628  0.7072020  0.3999222          
  7.837159e-03  0.9566972  0.5282215  0.6927621  0.4157973          
  2.838701e-02  0.3837130  0.5109397  0.7421902  0.2918276          
  6.293173e-02  0.0982520  0.7342786  0.7447680  0.4307813          
  9.468729e-02  0.6949516  0.5039711  0.7077048  0.3843844          
  1.296361e-01  0.5747879  0.4947543  0.7130013  0.3604240          
  1.659349e-01  0.3667528  0.5163111  0.7447680  0.2906123          
  1.726511e-01  0.3019844  0.5565318  0.7447680  0.2968604          
  2.588058e-01  0.3923488  0.5053567  0.7437744  0.2983622          
  3.857740e-01  0.7991288  0.5180046  0.6933824  0.3919086          
  5.573670e-01  0.7182965  0.5110121  0.6986877  0.3803075          
  1.283258e+00  0.5547483  0.5022044  0.7018353  0.3548538          
  1.943706e+00  0.2947021  0.5327318  0.7447680  0.2926935          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were fraction = 0.5604957 and lambda
 = 0.0001416033.
[1] "Sat Mar 10 03:21:54 2018"
Error : package evtree is required
In addition: There were 47 warnings (use warnings() to see them)
Error : package evtree is required
Error : package evtree is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:22:09 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "evtree"                  
Random Forest by Randomization 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  mtry  numRandomCuts  RMSE       Rsquared   MAE         Selected
  1     21             0.7338340  0.4158145  0.46945089          
  1     22             0.7385075  0.3939885  0.47249008          
  1     23             0.7408309  0.4026653  0.47182543          
  2      3             0.6080265  0.7088560  0.35589543          
  2      7             0.6039603  0.7046412  0.36054087          
  2     15             0.6054816  0.7017112  0.37595749          
  4     15             0.4205003  0.8788897  0.23888548          
  4     21             0.4006671  0.8935262  0.22821590          
  5     24             0.3337937  0.9181774  0.18622226          
  6      3             0.3009281  0.9334379  0.16615311          
  6     10             0.2772950  0.9414608  0.14813101          
  6     18             0.2632420  0.9535230  0.13962049          
  7      8             0.2287849  0.9602902  0.12248213          
  7     10             0.2319820  0.9589669  0.12154536          
  7     15             0.2370767  0.9570198  0.12061391          
  7     20             0.2237135  0.9621005  0.11488066          
  8      8             0.1995191  0.9668174  0.10602988          
  8     14             0.1957917  0.9706962  0.09862889          
  8     18             0.1820280  0.9719875  0.08708785  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 8 and numRandomCuts = 18.
[1] "Sat Mar 10 03:22:42 2018"
Ridge Regression with Variable Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  lambda        k  RMSE       Rsquared   MAE        Selected
  1.824675e-05  8  0.5317468  0.6921582  0.4193510          
  1.889923e-05  9  0.5358992  0.6860928  0.4229975          
  3.060968e-05  8  0.5317450  0.6921580  0.4193463          
  7.200631e-05  2  0.5318656  0.6917431  0.4225928          
  1.204248e-04  3  0.5438253  0.6772316  0.4317674          
  1.416033e-04  6  0.5381650  0.6799218  0.4296177          
  1.464566e-03  6  0.5379956  0.6798414  0.4292039          
  2.612998e-03  8  0.5313780  0.6921042  0.4183746          
  7.837159e-03  9  0.5346265  0.6862923  0.4195988          
  2.838701e-02  4  0.5405329  0.6704044  0.4290516          
  6.293173e-02  1  0.4779521  0.7447680  0.3671431  *       
  9.468729e-02  7  0.5254418  0.6855693  0.3969446          
  1.296361e-01  6  0.5272917  0.6832429  0.3839072          
  1.659349e-01  4  0.4871951  0.6530964  0.3659064          
  1.726511e-01  3  0.4869895  0.6526445  0.3641829          
  2.588058e-01  4  0.5866707  0.6731270  0.3891295          
  3.857740e-01  8  0.6101689  0.6734800  0.3825571          
  5.573670e-01  7        NaN        NaN        NaN          
  1.283258e+00  5        NaN        NaN        NaN          
  1.943706e+00  3        NaN        NaN        NaN          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were k = 1 and lambda = 0.06293173.
[1] "Sat Mar 10 03:22:56 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 30 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: select=FALSE, method=GCV.Cp Error in as.matrix(x) : object 'V2' not found
 
2: model fit failed for Fold1: select= TRUE, method=ML Error in as.matrix(x) : object 'V2' not found
 
3: model fit failed for Fold1: select= TRUE, method=GCV.Cp Error in as.matrix(x) : object 'V2' not found
 
4: model fit failed for Fold2: select=FALSE, method=GCV.Cp Error in as.matrix(x) : object 'V3' not found
 
5: model fit failed for Fold2: select= TRUE, method=ML Error in as.matrix(x) : object 'V3' not found
 
6: model fit failed for Fold2: select= TRUE, method=GCV.Cp Error in as.matrix(x) : object 'V3' not found
 
7: model fit failed for Fold3: select=FALSE, method=GCV.Cp Error in as.matrix(x) : object 'V9' not found
 
8: model fit failed for Fold3: select= TRUE, method=ML Error in as.matrix(x) : object 'V9' not found
 
9: model fit failed for Fold3: select= TRUE, method=GCV.Cp Error in as.matrix(x) : object 'V9' not found
 
10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:23:13 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "gam"                     
Boosted Generalized Additive Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  mstop  prune  RMSE       Rsquared   MAE        Selected
   44    no     0.2673965  0.9188293  0.1715553          
   47    no     0.2661545  0.9190989  0.1722507          
   81    no     0.2503305  0.9259083  0.1734788          
  143    yes    0.2421638  0.9301858  0.1723360          
  181    yes    0.2394087  0.9313662  0.1698672          
  192    no     0.2383004  0.9318891  0.1690507          
  361    no     0.2294063  0.9344827  0.1647323          
  403    no     0.2289565  0.9339982  0.1655135  *       
  483    no     0.2290314  0.9328705  0.1674336          
  576    yes    0.2336267  0.9331465  0.1664283          
  634    yes    0.2336267  0.9331465  0.1664283          
  663    no     0.2310129  0.9298046  0.1722244          
  686    no     0.2311192  0.9294981  0.1726618          
  704    yes    0.2336267  0.9331465  0.1664283          
  707    yes    0.2336267  0.9331465  0.1664283          
  736    yes    0.2336267  0.9331465  0.1664283          
  765    no     0.2318523  0.9285385  0.1740386          
  792    no     0.2319574  0.9282657  0.1743596          
  852    no     0.2322908  0.9277316  0.1750240          
  882    yes    0.2336267  0.9331465  0.1664283          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 403 and prune = no.
[1] "Sat Mar 10 03:23:44 2018"
Generalized Additive Model using Splines 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  df         RMSE       Rsquared   MAE        Selected
  0.2176545  0.5359020  0.6860928  0.4230050          
  0.2303702  0.5359020  0.6860928  0.4230050          
  0.4048823  0.5359020  0.6860928  0.4230050          
  0.7144755  0.5359020  0.6860928  0.4230050          
  0.9005966  0.5359020  0.6860928  0.4230050          
  0.9592278  0.5359020  0.6860928  0.4230050          
  1.8047575  0.5053632  0.7109458  0.3946342          
  2.0142826  0.4929563  0.7227976  0.3839046          
  2.4117989  0.4627600  0.7528792  0.3570672          
  2.8775997  0.4240326  0.7906783  0.3215379          
  3.1657247  0.4026002  0.8103629  0.3031275          
  3.3135764  0.3929492  0.8188055  0.2948746          
  3.4272717  0.3861265  0.8245836  0.2890447          
  3.5166149  0.3810965  0.8287388  0.2846942          
  3.5309745  0.3803377  0.8293577  0.2840466          
  3.6774784  0.3729824  0.8352152  0.2779367          
  3.8219442  0.3665138  0.8401587  0.2728611          
  3.9551177  0.3611656  0.8440881  0.2690601          
  4.2569284  0.3511745  0.8509573  0.2631153          
  4.4071921  0.3471052  0.8535527  0.2604224  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was df = 4.407192.
[1] "Sat Mar 10 03:24:03 2018"
Error in .local(object, ...) : test vector does not match model !
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:24:21 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "gaussprLinear"           
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:24:45 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "gaussprPoly"             
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:25:04 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "gaussprRadial"           
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.6265             nan     0.0176    0.0078
     2        0.6187             nan     0.0176    0.0076
     3        0.6098             nan     0.0176    0.0075
     4        0.6006             nan     0.0176    0.0072
     5        0.5932             nan     0.0176    0.0069
     6        0.5884             nan     0.0176    0.0062
     7        0.5831             nan     0.0176    0.0062
     8        0.5756             nan     0.0176    0.0047
     9        0.5686             nan     0.0176    0.0050
    10        0.5626             nan     0.0176    0.0054
    20        0.5023             nan     0.0176    0.0029
    40        0.4348             nan     0.0176    0.0013
    60        0.3906             nan     0.0176   -0.0010
    80        0.3607             nan     0.0176    0.0002
   100        0.3399             nan     0.0176    0.0001
   120        0.3173             nan     0.0176    0.0003
   140        0.3019             nan     0.0176   -0.0000
   160        0.2910             nan     0.0176   -0.0006
   180        0.2816             nan     0.0176   -0.0001
   200        0.2748             nan     0.0176   -0.0011
   220        0.2655             nan     0.0176   -0.0004
   240        0.2595             nan     0.0176    0.0004
   260        0.2499             nan     0.0176   -0.0012
   280        0.2401             nan     0.0176   -0.0005
   300        0.2345             nan     0.0176    0.0002
   320        0.2305             nan     0.0176   -0.0006
   340        0.2267             nan     0.0176   -0.0014
   360        0.2209             nan     0.0176   -0.0006
   380        0.2177             nan     0.0176   -0.0004
   400        0.2106             nan     0.0176   -0.0004
   420        0.2064             nan     0.0176   -0.0012
   440        0.2016             nan     0.0176   -0.0003
   460        0.1990             nan     0.0176   -0.0013
   480        0.1961             nan     0.0176   -0.0012
   500        0.1941             nan     0.0176   -0.0009
   520        0.1912             nan     0.0176   -0.0002
   540        0.1898             nan     0.0176   -0.0005
   560        0.1874             nan     0.0176   -0.0008
   580        0.1841             nan     0.0176   -0.0002
   600        0.1815             nan     0.0176   -0.0003
   620        0.1781             nan     0.0176   -0.0005
   640        0.1747             nan     0.0176   -0.0008
   660        0.1726             nan     0.0176   -0.0010
   680        0.1688             nan     0.0176   -0.0005
   700        0.1663             nan     0.0176   -0.0006
   720        0.1644             nan     0.0176   -0.0010
   740        0.1627             nan     0.0176   -0.0008
   760        0.1600             nan     0.0176   -0.0004
   780        0.1571             nan     0.0176   -0.0000
   800        0.1551             nan     0.0176   -0.0004
   820        0.1532             nan     0.0176   -0.0003
   840        0.1511             nan     0.0176   -0.0007
   860        0.1492             nan     0.0176   -0.0011
   880        0.1475             nan     0.0176   -0.0002
   900        0.1460             nan     0.0176   -0.0006
   920        0.1447             nan     0.0176   -0.0004
   940        0.1424             nan     0.0176    0.0001
   960        0.1409             nan     0.0176   -0.0003
   980        0.1383             nan     0.0176   -0.0003
  1000        0.1369             nan     0.0176   -0.0006
  1020        0.1354             nan     0.0176   -0.0005
  1040        0.1339             nan     0.0176   -0.0010
  1060        0.1327             nan     0.0176   -0.0006
  1080        0.1305             nan     0.0176   -0.0002
  1100        0.1285             nan     0.0176    0.0002
  1120        0.1269             nan     0.0176   -0.0005
  1140        0.1255             nan     0.0176   -0.0004
  1160        0.1246             nan     0.0176   -0.0004
  1180        0.1236             nan     0.0176   -0.0003
  1200        0.1223             nan     0.0176   -0.0003
  1220        0.1211             nan     0.0176   -0.0004
  1240        0.1203             nan     0.0176   -0.0004
  1260        0.1193             nan     0.0176   -0.0003
  1280        0.1179             nan     0.0176   -0.0005
  1300        0.1166             nan     0.0176   -0.0003
  1320        0.1152             nan     0.0176   -0.0004
  1340        0.1140             nan     0.0176   -0.0004
  1360        0.1128             nan     0.0176   -0.0003
  1380        0.1115             nan     0.0176   -0.0009
  1400        0.1101             nan     0.0176   -0.0002
  1420        0.1087             nan     0.0176   -0.0002
  1440        0.1080             nan     0.0176   -0.0005
  1460        0.1074             nan     0.0176   -0.0001
  1480        0.1065             nan     0.0176   -0.0002
  1500        0.1054             nan     0.0176   -0.0003
  1520        0.1044             nan     0.0176   -0.0004
  1540        0.1032             nan     0.0176   -0.0007
  1560        0.1023             nan     0.0176   -0.0004
  1580        0.1010             nan     0.0176   -0.0001
  1600        0.1002             nan     0.0176   -0.0003
  1620        0.0992             nan     0.0176   -0.0004
  1640        0.0983             nan     0.0176   -0.0002
  1660        0.0974             nan     0.0176   -0.0001
  1680        0.0964             nan     0.0176   -0.0003
  1700        0.0955             nan     0.0176   -0.0004
  1720        0.0942             nan     0.0176    0.0002
  1740        0.0931             nan     0.0176   -0.0004
  1760        0.0922             nan     0.0176   -0.0002
  1780        0.0914             nan     0.0176   -0.0005
  1800        0.0906             nan     0.0176   -0.0005
  1820        0.0897             nan     0.0176   -0.0005
  1840        0.0887             nan     0.0176   -0.0003
  1860        0.0880             nan     0.0176   -0.0004
  1880        0.0873             nan     0.0176   -0.0007
  1900        0.0864             nan     0.0176   -0.0003
  1920        0.0854             nan     0.0176   -0.0003
  1940        0.0844             nan     0.0176   -0.0003
  1960        0.0832             nan     0.0176   -0.0005
  1980        0.0825             nan     0.0176   -0.0004
  2000        0.0816             nan     0.0176   -0.0002
  2020        0.0808             nan     0.0176   -0.0001
  2040        0.0798             nan     0.0176   -0.0003
  2060        0.0792             nan     0.0176   -0.0003
  2080        0.0787             nan     0.0176   -0.0002
  2100        0.0780             nan     0.0176   -0.0004
  2120        0.0773             nan     0.0176   -0.0003
  2140        0.0764             nan     0.0176   -0.0003
  2160        0.0757             nan     0.0176   -0.0004
  2180        0.0750             nan     0.0176   -0.0003
  2200        0.0743             nan     0.0176   -0.0006
  2220        0.0738             nan     0.0176   -0.0005
  2240        0.0729             nan     0.0176   -0.0002
  2260        0.0723             nan     0.0176   -0.0001
  2280        0.0716             nan     0.0176   -0.0004
  2300        0.0708             nan     0.0176   -0.0002
  2320        0.0702             nan     0.0176   -0.0003
  2340        0.0699             nan     0.0176   -0.0003
  2360        0.0692             nan     0.0176   -0.0001
  2380        0.0685             nan     0.0176   -0.0001
  2400        0.0676             nan     0.0176   -0.0002
  2420        0.0670             nan     0.0176   -0.0001
  2440        0.0665             nan     0.0176   -0.0002
  2460        0.0659             nan     0.0176   -0.0001
  2480        0.0653             nan     0.0176   -0.0002
  2500        0.0647             nan     0.0176   -0.0002
  2520        0.0640             nan     0.0176   -0.0003
  2540        0.0635             nan     0.0176   -0.0000
  2560        0.0629             nan     0.0176   -0.0004
  2580        0.0622             nan     0.0176   -0.0002
  2600        0.0618             nan     0.0176   -0.0003
  2620        0.0612             nan     0.0176   -0.0001
  2640        0.0605             nan     0.0176   -0.0001
  2660        0.0600             nan     0.0176   -0.0002
  2680        0.0596             nan     0.0176   -0.0001
  2700        0.0592             nan     0.0176   -0.0002
  2720        0.0587             nan     0.0176   -0.0001
  2740        0.0584             nan     0.0176   -0.0001
  2760        0.0581             nan     0.0176   -0.0003
  2780        0.0577             nan     0.0176   -0.0002
  2800        0.0571             nan     0.0176   -0.0001
  2820        0.0567             nan     0.0176   -0.0002
  2840        0.0561             nan     0.0176   -0.0001
  2860        0.0554             nan     0.0176   -0.0003
  2880        0.0550             nan     0.0176   -0.0002
  2900        0.0547             nan     0.0176   -0.0004
  2920        0.0543             nan     0.0176   -0.0001
  2940        0.0539             nan     0.0176   -0.0001
  2960        0.0534             nan     0.0176   -0.0001
  2980        0.0530             nan     0.0176   -0.0001
  3000        0.0524             nan     0.0176   -0.0002
  3020        0.0519             nan     0.0176   -0.0002
  3040        0.0516             nan     0.0176   -0.0004
  3060        0.0511             nan     0.0176   -0.0001
  3080        0.0506             nan     0.0176   -0.0002
  3100        0.0501             nan     0.0176   -0.0002
  3120        0.0498             nan     0.0176   -0.0001
  3140        0.0494             nan     0.0176   -0.0003
  3160        0.0491             nan     0.0176   -0.0003
  3180        0.0487             nan     0.0176   -0.0003
  3200        0.0483             nan     0.0176   -0.0005
  3220        0.0479             nan     0.0176   -0.0003
  3240        0.0474             nan     0.0176   -0.0002
  3260        0.0468             nan     0.0176   -0.0001
  3280        0.0464             nan     0.0176   -0.0003
  3300        0.0460             nan     0.0176   -0.0003
  3320        0.0456             nan     0.0176   -0.0002
  3340        0.0451             nan     0.0176   -0.0001
  3360        0.0448             nan     0.0176   -0.0001
  3380        0.0444             nan     0.0176   -0.0001
  3400        0.0441             nan     0.0176   -0.0001
  3420        0.0438             nan     0.0176   -0.0002
  3440        0.0434             nan     0.0176   -0.0002
  3460        0.0431             nan     0.0176   -0.0003
  3480        0.0427             nan     0.0176   -0.0001
  3500        0.0424             nan     0.0176   -0.0002
  3520        0.0422             nan     0.0176   -0.0001
  3540        0.0416             nan     0.0176   -0.0002
  3560        0.0413             nan     0.0176   -0.0002
  3580        0.0410             nan     0.0176   -0.0004
  3600        0.0407             nan     0.0176   -0.0001
  3620        0.0404             nan     0.0176   -0.0001
  3640        0.0401             nan     0.0176   -0.0002
  3660        0.0399             nan     0.0176   -0.0001
  3680        0.0396             nan     0.0176   -0.0001
  3700        0.0392             nan     0.0176   -0.0001
  3720        0.0388             nan     0.0176   -0.0001
  3740        0.0385             nan     0.0176   -0.0002
  3760        0.0382             nan     0.0176   -0.0001
  3780        0.0380             nan     0.0176   -0.0001
  3800        0.0376             nan     0.0176   -0.0001
  3820        0.0373             nan     0.0176   -0.0002
  3822        0.0372             nan     0.0176   -0.0001

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.6109             nan     0.0376    0.0190
     2        0.5891             nan     0.0376    0.0194
     3        0.5646             nan     0.0376    0.0177
     4        0.5413             nan     0.0376    0.0140
     5        0.5217             nan     0.0376    0.0162
     6        0.5063             nan     0.0376    0.0125
     7        0.4936             nan     0.0376    0.0132
     8        0.4844             nan     0.0376    0.0102
     9        0.4778             nan     0.0376    0.0097
    10        0.4671             nan     0.0376    0.0117
    20        0.3810             nan     0.0376    0.0045
    40        0.2984             nan     0.0376    0.0008
    60        0.2604             nan     0.0376    0.0012
    80        0.2244             nan     0.0376    0.0002
   100        0.2034             nan     0.0376   -0.0025
   120        0.1829             nan     0.0376   -0.0006
   140        0.1723             nan     0.0376   -0.0007
   160        0.1546             nan     0.0376   -0.0010
   180        0.1427             nan     0.0376    0.0002
   200        0.1331             nan     0.0376   -0.0008
   220        0.1211             nan     0.0376   -0.0010
   240        0.1142             nan     0.0376   -0.0006
   260        0.1070             nan     0.0376   -0.0004
   280        0.1008             nan     0.0376   -0.0012
   300        0.0954             nan     0.0376   -0.0005
   320        0.0917             nan     0.0376   -0.0009
   340        0.0882             nan     0.0376   -0.0008
   360        0.0854             nan     0.0376   -0.0006
   380        0.0798             nan     0.0376   -0.0008
   400        0.0770             nan     0.0376   -0.0003
   420        0.0743             nan     0.0376   -0.0003
   440        0.0720             nan     0.0376   -0.0009
   460        0.0694             nan     0.0376   -0.0004
   480        0.0669             nan     0.0376   -0.0005
   500        0.0638             nan     0.0376   -0.0007
   520        0.0600             nan     0.0376   -0.0003
   540        0.0569             nan     0.0376   -0.0001
   560        0.0534             nan     0.0376   -0.0003
   580        0.0505             nan     0.0376   -0.0012
   600        0.0487             nan     0.0376   -0.0004
   620        0.0461             nan     0.0376   -0.0003
   640        0.0448             nan     0.0376   -0.0004
   660        0.0432             nan     0.0376   -0.0003
   680        0.0416             nan     0.0376   -0.0001
   700        0.0398             nan     0.0376   -0.0003
   720        0.0383             nan     0.0376   -0.0003
   740        0.0361             nan     0.0376   -0.0004
   760        0.0348             nan     0.0376   -0.0001
   780        0.0334             nan     0.0376   -0.0004
   800        0.0322             nan     0.0376   -0.0004
   820        0.0304             nan     0.0376   -0.0002
   840        0.0292             nan     0.0376   -0.0003
   860        0.0282             nan     0.0376   -0.0001
   880        0.0270             nan     0.0376   -0.0004
   900        0.0262             nan     0.0376   -0.0006
   920        0.0252             nan     0.0376   -0.0001
   940        0.0244             nan     0.0376   -0.0001
   960        0.0231             nan     0.0376   -0.0001
   980        0.0224             nan     0.0376   -0.0001
  1000        0.0215             nan     0.0376   -0.0005
  1020        0.0210             nan     0.0376   -0.0003
  1040        0.0199             nan     0.0376   -0.0003
  1060        0.0191             nan     0.0376   -0.0002
  1080        0.0181             nan     0.0376   -0.0001
  1100        0.0174             nan     0.0376   -0.0001
  1120        0.0167             nan     0.0376   -0.0001
  1140        0.0163             nan     0.0376   -0.0001
  1160        0.0159             nan     0.0376   -0.0002
  1180        0.0151             nan     0.0376   -0.0002
  1200        0.0145             nan     0.0376   -0.0001
  1220        0.0141             nan     0.0376   -0.0001
  1240        0.0134             nan     0.0376   -0.0002
  1260        0.0127             nan     0.0376   -0.0002
  1280        0.0121             nan     0.0376   -0.0001
  1300        0.0117             nan     0.0376   -0.0001
  1320        0.0111             nan     0.0376   -0.0002
  1340        0.0107             nan     0.0376   -0.0000
  1360        0.0104             nan     0.0376   -0.0000
  1380        0.0100             nan     0.0376   -0.0001
  1400        0.0096             nan     0.0376   -0.0000
  1420        0.0093             nan     0.0376   -0.0001
  1440        0.0090             nan     0.0376   -0.0001
  1460        0.0088             nan     0.0376   -0.0001
  1480        0.0085             nan     0.0376   -0.0001
  1500        0.0082             nan     0.0376   -0.0001
  1520        0.0080             nan     0.0376   -0.0001
  1540        0.0077             nan     0.0376   -0.0000
  1560        0.0074             nan     0.0376   -0.0001
  1580        0.0071             nan     0.0376   -0.0001
  1600        0.0069             nan     0.0376   -0.0001
  1620        0.0066             nan     0.0376   -0.0001
  1640        0.0065             nan     0.0376   -0.0000
  1660        0.0062             nan     0.0376   -0.0000
  1680        0.0060             nan     0.0376   -0.0001
  1700        0.0057             nan     0.0376   -0.0001
  1720        0.0056             nan     0.0376   -0.0001
  1740        0.0054             nan     0.0376   -0.0000
  1760        0.0052             nan     0.0376   -0.0000
  1780        0.0051             nan     0.0376   -0.0001
  1800        0.0049             nan     0.0376   -0.0000
  1805        0.0049             nan     0.0376   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5941             nan     0.1185    0.0502
     2        0.5522             nan     0.1185    0.0437
     3        0.5094             nan     0.1185    0.0280
     4        0.4884             nan     0.1185    0.0274
     5        0.4618             nan     0.1185    0.0198
     6        0.4386             nan     0.1185    0.0113
     7        0.4198             nan     0.1185    0.0114
     8        0.3924             nan     0.1185    0.0076
     9        0.3907             nan     0.1185   -0.0070
    10        0.3738             nan     0.1185    0.0051
    20        0.3090             nan     0.1185   -0.0085
    40        0.2593             nan     0.1185   -0.0057
    60        0.2297             nan     0.1185   -0.0051
    80        0.2059             nan     0.1185   -0.0010
   100        0.1922             nan     0.1185   -0.0020
   120        0.1807             nan     0.1185   -0.0099
   140        0.1647             nan     0.1185   -0.0052
   160        0.1466             nan     0.1185   -0.0014
   180        0.1322             nan     0.1185   -0.0022
   200        0.1236             nan     0.1185   -0.0014
   220        0.1177             nan     0.1185   -0.0020
   240        0.1076             nan     0.1185   -0.0051
   260        0.1033             nan     0.1185   -0.0017
   280        0.0943             nan     0.1185   -0.0020
   300        0.0880             nan     0.1185   -0.0034
   320        0.0826             nan     0.1185   -0.0026
   340        0.0791             nan     0.1185   -0.0045
   360        0.0739             nan     0.1185   -0.0011
   380        0.0722             nan     0.1185   -0.0039
   400        0.0688             nan     0.1185   -0.0024
   420        0.0657             nan     0.1185   -0.0017
   440        0.0615             nan     0.1185   -0.0007
   460        0.0578             nan     0.1185   -0.0010
   480        0.0536             nan     0.1185   -0.0015
   500        0.0525             nan     0.1185   -0.0015
   520        0.0495             nan     0.1185   -0.0008
   540        0.0469             nan     0.1185   -0.0010
   560        0.0446             nan     0.1185   -0.0014
   580        0.0421             nan     0.1185   -0.0024
   600        0.0410             nan     0.1185   -0.0009
   620        0.0380             nan     0.1185   -0.0012
   640        0.0359             nan     0.1185   -0.0012
   660        0.0344             nan     0.1185   -0.0005
   680        0.0326             nan     0.1185   -0.0005
   700        0.0317             nan     0.1185   -0.0009
   720        0.0299             nan     0.1185   -0.0009
   740        0.0285             nan     0.1185   -0.0003
   760        0.0275             nan     0.1185   -0.0006
   780        0.0256             nan     0.1185   -0.0004
   800        0.0243             nan     0.1185   -0.0008
   820        0.0227             nan     0.1185   -0.0010
   840        0.0222             nan     0.1185   -0.0013
   860        0.0208             nan     0.1185   -0.0004
   880        0.0197             nan     0.1185   -0.0006
   900        0.0188             nan     0.1185   -0.0002
   920        0.0177             nan     0.1185   -0.0006
   940        0.0169             nan     0.1185   -0.0003
   960        0.0163             nan     0.1185   -0.0005
   980        0.0159             nan     0.1185   -0.0006
  1000        0.0151             nan     0.1185   -0.0002
  1020        0.0144             nan     0.1185   -0.0002
  1040        0.0133             nan     0.1185   -0.0005
  1060        0.0126             nan     0.1185   -0.0003
  1080        0.0119             nan     0.1185   -0.0004
  1100        0.0115             nan     0.1185   -0.0005
  1120        0.0113             nan     0.1185   -0.0002
  1140        0.0106             nan     0.1185   -0.0004
  1160        0.0101             nan     0.1185   -0.0001
  1180        0.0096             nan     0.1185   -0.0001
  1200        0.0090             nan     0.1185   -0.0002
  1220        0.0086             nan     0.1185   -0.0002
  1240        0.0084             nan     0.1185   -0.0004
  1260        0.0080             nan     0.1185   -0.0001
  1280        0.0076             nan     0.1185   -0.0001
  1300        0.0073             nan     0.1185   -0.0001
  1320        0.0069             nan     0.1185   -0.0001
  1340        0.0066             nan     0.1185   -0.0002
  1360        0.0063             nan     0.1185   -0.0001
  1380        0.0062             nan     0.1185   -0.0001
  1400        0.0059             nan     0.1185   -0.0001
  1420        0.0058             nan     0.1185   -0.0002
  1440        0.0056             nan     0.1185   -0.0002
  1460        0.0053             nan     0.1185   -0.0001
  1480        0.0051             nan     0.1185   -0.0001
  1500        0.0049             nan     0.1185   -0.0003
  1520        0.0049             nan     0.1185   -0.0000
  1540        0.0046             nan     0.1185   -0.0001
  1560        0.0043             nan     0.1185   -0.0000
  1580        0.0042             nan     0.1185   -0.0001
  1600        0.0040             nan     0.1185   -0.0001
  1620        0.0040             nan     0.1185   -0.0002
  1640        0.0038             nan     0.1185   -0.0001
  1660        0.0036             nan     0.1185   -0.0000
  1680        0.0034             nan     0.1185   -0.0000
  1700        0.0033             nan     0.1185   -0.0000
  1720        0.0033             nan     0.1185   -0.0000
  1740        0.0031             nan     0.1185   -0.0001
  1760        0.0030             nan     0.1185   -0.0001
  1780        0.0029             nan     0.1185   -0.0001
  1800        0.0028             nan     0.1185   -0.0001
  1820        0.0027             nan     0.1185   -0.0001
  1840        0.0026             nan     0.1185   -0.0000
  1860        0.0024             nan     0.1185   -0.0001
  1880        0.0023             nan     0.1185   -0.0001
  1900        0.0022             nan     0.1185   -0.0001
  1920        0.0022             nan     0.1185   -0.0001
  1940        0.0021             nan     0.1185   -0.0001
  1960        0.0020             nan     0.1185   -0.0001
  1980        0.0019             nan     0.1185   -0.0000
  2000        0.0019             nan     0.1185   -0.0001
  2020        0.0018             nan     0.1185   -0.0001
  2040        0.0017             nan     0.1185   -0.0000
  2060        0.0016             nan     0.1185   -0.0001
  2080        0.0016             nan     0.1185   -0.0000
  2100        0.0016             nan     0.1185   -0.0001
  2120        0.0014             nan     0.1185   -0.0000
  2140        0.0014             nan     0.1185   -0.0000
  2160        0.0013             nan     0.1185   -0.0000
  2180        0.0013             nan     0.1185   -0.0000
  2200        0.0013             nan     0.1185   -0.0000
  2220        0.0012             nan     0.1185   -0.0001
  2240        0.0011             nan     0.1185   -0.0000
  2260        0.0011             nan     0.1185   -0.0000
  2280        0.0011             nan     0.1185   -0.0000
  2300        0.0011             nan     0.1185   -0.0000
  2320        0.0010             nan     0.1185   -0.0000
  2340        0.0010             nan     0.1185   -0.0000
  2360        0.0010             nan     0.1185   -0.0000
  2380        0.0009             nan     0.1185   -0.0000
  2400        0.0009             nan     0.1185   -0.0000
  2420        0.0009             nan     0.1185   -0.0000
  2440        0.0009             nan     0.1185   -0.0000
  2460        0.0008             nan     0.1185   -0.0000
  2480        0.0008             nan     0.1185   -0.0000
  2500        0.0008             nan     0.1185   -0.0000
  2520        0.0008             nan     0.1185   -0.0000
  2540        0.0007             nan     0.1185   -0.0000
  2560        0.0007             nan     0.1185   -0.0000
  2580        0.0007             nan     0.1185   -0.0000
  2600        0.0007             nan     0.1185   -0.0000
  2620        0.0006             nan     0.1185   -0.0000
  2640        0.0006             nan     0.1185   -0.0000
  2660        0.0006             nan     0.1185   -0.0000
  2680        0.0006             nan     0.1185   -0.0000
  2700        0.0006             nan     0.1185   -0.0000
  2720        0.0006             nan     0.1185   -0.0000
  2740        0.0006             nan     0.1185   -0.0000
  2760        0.0005             nan     0.1185   -0.0000
  2780        0.0005             nan     0.1185   -0.0000
  2800        0.0005             nan     0.1185   -0.0000
  2820        0.0005             nan     0.1185   -0.0000
  2840        0.0005             nan     0.1185   -0.0000
  2860        0.0005             nan     0.1185   -0.0000
  2880        0.0005             nan     0.1185   -0.0000
  2900        0.0004             nan     0.1185   -0.0000
  2920        0.0004             nan     0.1185   -0.0000
  2940        0.0004             nan     0.1185   -0.0000
  2960        0.0004             nan     0.1185   -0.0000
  2980        0.0004             nan     0.1185   -0.0000
  3000        0.0004             nan     0.1185   -0.0000
  3020        0.0004             nan     0.1185   -0.0000
  3040        0.0004             nan     0.1185   -0.0000
  3060        0.0003             nan     0.1185   -0.0000
  3080        0.0003             nan     0.1185   -0.0000
  3100        0.0003             nan     0.1185   -0.0000
  3120        0.0003             nan     0.1185   -0.0000
  3140        0.0003             nan     0.1185   -0.0000
  3160        0.0003             nan     0.1185   -0.0000
  3180        0.0003             nan     0.1185   -0.0000
  3200        0.0003             nan     0.1185   -0.0000
  3220        0.0003             nan     0.1185   -0.0000
  3240        0.0002             nan     0.1185   -0.0000
  3260        0.0002             nan     0.1185   -0.0000
  3280        0.0002             nan     0.1185   -0.0000
  3300        0.0002             nan     0.1185   -0.0000
  3320        0.0002             nan     0.1185   -0.0000
  3340        0.0002             nan     0.1185   -0.0000
  3360        0.0002             nan     0.1185   -0.0000
  3380        0.0002             nan     0.1185   -0.0000
  3400        0.0002             nan     0.1185   -0.0000
  3420        0.0002             nan     0.1185   -0.0000
  3440        0.0002             nan     0.1185   -0.0000
  3460        0.0002             nan     0.1185   -0.0000
  3480        0.0002             nan     0.1185   -0.0000
  3500        0.0002             nan     0.1185   -0.0000
  3520        0.0001             nan     0.1185   -0.0000
  3540        0.0001             nan     0.1185   -0.0000
  3560        0.0001             nan     0.1185   -0.0000
  3580        0.0001             nan     0.1185   -0.0000
  3600        0.0001             nan     0.1185   -0.0000
  3620        0.0001             nan     0.1185   -0.0000
  3640        0.0001             nan     0.1185   -0.0000
  3660        0.0001             nan     0.1185   -0.0000
  3677        0.0001             nan     0.1185   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5678             nan     0.1233    0.0675
     2        0.4913             nan     0.1233    0.0475
     3        0.4640             nan     0.1233    0.0362
     4        0.4330             nan     0.1233    0.0152
     5        0.4023             nan     0.1233    0.0179
     6        0.3823             nan     0.1233    0.0227
     7        0.3616             nan     0.1233    0.0223
     8        0.3510             nan     0.1233    0.0119
     9        0.3538             nan     0.1233   -0.0094
    10        0.3386             nan     0.1233    0.0119
    20        0.2627             nan     0.1233    0.0031
    40        0.1767             nan     0.1233   -0.0050
    60        0.1326             nan     0.1233   -0.0014
    80        0.1112             nan     0.1233   -0.0020
   100        0.0841             nan     0.1233   -0.0010
   120        0.0723             nan     0.1233   -0.0024
   140        0.0605             nan     0.1233   -0.0003
   160        0.0512             nan     0.1233   -0.0012
   180        0.0447             nan     0.1233   -0.0007
   200        0.0380             nan     0.1233   -0.0005
   220        0.0311             nan     0.1233   -0.0010
   240        0.0289             nan     0.1233   -0.0006
   260        0.0246             nan     0.1233   -0.0012
   280        0.0206             nan     0.1233   -0.0004
   300        0.0178             nan     0.1233   -0.0003
   320        0.0158             nan     0.1233   -0.0004
   340        0.0146             nan     0.1233   -0.0004
   360        0.0138             nan     0.1233   -0.0006
   380        0.0124             nan     0.1233   -0.0006
   400        0.0112             nan     0.1233   -0.0001
   420        0.0099             nan     0.1233   -0.0004
   440        0.0090             nan     0.1233   -0.0004
   460        0.0081             nan     0.1233   -0.0003
   480        0.0075             nan     0.1233   -0.0001
   500        0.0069             nan     0.1233   -0.0001
   520        0.0062             nan     0.1233   -0.0002
   540        0.0056             nan     0.1233   -0.0002
   560        0.0052             nan     0.1233   -0.0001
   580        0.0046             nan     0.1233   -0.0001
   600        0.0042             nan     0.1233   -0.0001
   620        0.0038             nan     0.1233   -0.0002
   640        0.0035             nan     0.1233   -0.0001
   660        0.0032             nan     0.1233   -0.0001
   680        0.0030             nan     0.1233   -0.0002
   700        0.0027             nan     0.1233   -0.0001
   715        0.0025             nan     0.1233   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.4882             nan     0.3693    0.0950
     2        0.4285             nan     0.3693    0.0845
     3        0.3558             nan     0.3693    0.0568
     4        0.3191             nan     0.3693    0.0013
     5        0.2868             nan     0.3693    0.0226
     6        0.2871             nan     0.3693   -0.0182
     7        0.2815             nan     0.3693   -0.0035
     8        0.2855             nan     0.3693   -0.0173
     9        0.2747             nan     0.3693   -0.0525
    10        0.2685             nan     0.3693   -0.0186
    20        0.2289             nan     0.3693   -0.0191
    40        0.1677             nan     0.3693   -0.0195
    60        0.1242             nan     0.3693   -0.0204
    80        0.1018             nan     0.3693   -0.0077
   100        0.0852             nan     0.3693    0.0002
   120        0.0688             nan     0.3693   -0.0129
   140        0.0522             nan     0.3693   -0.0029
   160        0.0448             nan     0.3693   -0.0006
   180        0.0290             nan     0.3693   -0.0007
   200        0.0242             nan     0.3693   -0.0024
   220        0.0208             nan     0.3693   -0.0016
   240        0.0171             nan     0.3693   -0.0018
   260        0.0150             nan     0.3693   -0.0015
   280        0.0129             nan     0.3693   -0.0034
   300        0.0117             nan     0.3693   -0.0019
   320        0.0097             nan     0.3693   -0.0004
   340        0.0095             nan     0.3693   -0.0007
   360        0.0083             nan     0.3693   -0.0009
   380        0.0070             nan     0.3693   -0.0010
   400        0.0058             nan     0.3693   -0.0005
   420        0.0057             nan     0.3693   -0.0005
   440        0.0050             nan     0.3693   -0.0005
   460        0.0043             nan     0.3693   -0.0004
   480        0.0036             nan     0.3693   -0.0000
   500        0.0031             nan     0.3693   -0.0003
   520        0.0027             nan     0.3693   -0.0002
   540        0.0024             nan     0.3693   -0.0003
   560        0.0024             nan     0.3693   -0.0001
   580        0.0018             nan     0.3693   -0.0001
   600        0.0016             nan     0.3693   -0.0001
   620        0.0014             nan     0.3693   -0.0001
   640        0.0012             nan     0.3693   -0.0001
   660        0.0011             nan     0.3693   -0.0001
   680        0.0010             nan     0.3693   -0.0002
   700        0.0008             nan     0.3693   -0.0001
   720        0.0007             nan     0.3693   -0.0000
   740        0.0006             nan     0.3693   -0.0001
   760        0.0005             nan     0.3693   -0.0000
   780        0.0005             nan     0.3693   -0.0000
   800        0.0004             nan     0.3693   -0.0001
   820        0.0003             nan     0.3693   -0.0000
   840        0.0003             nan     0.3693   -0.0000
   860        0.0002             nan     0.3693   -0.0000
   880        0.0002             nan     0.3693    0.0000
   900        0.0002             nan     0.3693   -0.0000
   920        0.0002             nan     0.3693   -0.0000
   940        0.0001             nan     0.3693   -0.0000
   960        0.0001             nan     0.3693   -0.0000
   980        0.0001             nan     0.3693   -0.0000
  1000        0.0001             nan     0.3693   -0.0000
  1020        0.0001             nan     0.3693   -0.0000
  1040        0.0001             nan     0.3693   -0.0000
  1060        0.0001             nan     0.3693   -0.0000
  1080        0.0001             nan     0.3693   -0.0000
  1100        0.0001             nan     0.3693   -0.0000
  1120        0.0000             nan     0.3693   -0.0000
  1140        0.0000             nan     0.3693   -0.0000
  1160        0.0000             nan     0.3693   -0.0000
  1180        0.0000             nan     0.3693   -0.0000
  1200        0.0000             nan     0.3693   -0.0000
  1220        0.0000             nan     0.3693   -0.0000
  1240        0.0000             nan     0.3693   -0.0000
  1260        0.0000             nan     0.3693   -0.0000
  1280        0.0000             nan     0.3693   -0.0000
  1300        0.0000             nan     0.3693   -0.0000
  1320        0.0000             nan     0.3693   -0.0000
  1340        0.0000             nan     0.3693   -0.0000
  1360        0.0000             nan     0.3693   -0.0000
  1380        0.0000             nan     0.3693   -0.0000
  1400        0.0000             nan     0.3693   -0.0000
  1420        0.0000             nan     0.3693   -0.0000
  1440        0.0000             nan     0.3693   -0.0000
  1460        0.0000             nan     0.3693   -0.0000
  1480        0.0000             nan     0.3693    0.0000
  1500        0.0000             nan     0.3693   -0.0000
  1520        0.0000             nan     0.3693   -0.0000
  1540        0.0000             nan     0.3693   -0.0000
  1560        0.0000             nan     0.3693   -0.0000
  1580        0.0000             nan     0.3693   -0.0000
  1600        0.0000             nan     0.3693   -0.0000
  1620        0.0000             nan     0.3693   -0.0000
  1640        0.0000             nan     0.3693   -0.0000
  1660        0.0000             nan     0.3693   -0.0000
  1680        0.0000             nan     0.3693   -0.0000
  1700        0.0000             nan     0.3693   -0.0000
  1720        0.0000             nan     0.3693   -0.0000
  1740        0.0000             nan     0.3693   -0.0000
  1760        0.0000             nan     0.3693   -0.0000
  1780        0.0000             nan     0.3693   -0.0000
  1800        0.0000             nan     0.3693   -0.0000
  1820        0.0000             nan     0.3693   -0.0000
  1840        0.0000             nan     0.3693   -0.0000
  1860        0.0000             nan     0.3693   -0.0000
  1880        0.0000             nan     0.3693   -0.0000
  1900        0.0000             nan     0.3693    0.0000
  1920        0.0000             nan     0.3693   -0.0000
  1940        0.0000             nan     0.3693   -0.0000
  1960        0.0000             nan     0.3693   -0.0000
  1980        0.0000             nan     0.3693   -0.0000
  2000        0.0000             nan     0.3693    0.0000
  2020        0.0000             nan     0.3693   -0.0000
  2040        0.0000             nan     0.3693   -0.0000
  2060        0.0000             nan     0.3693   -0.0000
  2080        0.0000             nan     0.3693   -0.0000
  2100        0.0000             nan     0.3693   -0.0000
  2120        0.0000             nan     0.3693   -0.0000
  2140        0.0000             nan     0.3693   -0.0000
  2160        0.0000             nan     0.3693   -0.0000
  2180        0.0000             nan     0.3693   -0.0000
  2200        0.0000             nan     0.3693   -0.0000
  2220        0.0000             nan     0.3693   -0.0000
  2240        0.0000             nan     0.3693   -0.0000
  2260        0.0000             nan     0.3693   -0.0000
  2280        0.0000             nan     0.3693   -0.0000
  2300        0.0000             nan     0.3693   -0.0000
  2320        0.0000             nan     0.3693   -0.0000
  2340        0.0000             nan     0.3693   -0.0000
  2360        0.0000             nan     0.3693   -0.0000
  2380        0.0000             nan     0.3693   -0.0000
  2400        0.0000             nan     0.3693   -0.0000
  2420        0.0000             nan     0.3693   -0.0000
  2440        0.0000             nan     0.3693   -0.0000
  2460        0.0000             nan     0.3693   -0.0000
  2480        0.0000             nan     0.3693   -0.0000
  2500        0.0000             nan     0.3693   -0.0000
  2520        0.0000             nan     0.3693   -0.0000
  2540        0.0000             nan     0.3693   -0.0000
  2560        0.0000             nan     0.3693   -0.0000
  2580        0.0000             nan     0.3693   -0.0000
  2600        0.0000             nan     0.3693   -0.0000
  2620        0.0000             nan     0.3693   -0.0000
  2640        0.0000             nan     0.3693   -0.0000
  2660        0.0000             nan     0.3693   -0.0000
  2680        0.0000             nan     0.3693   -0.0000
  2700        0.0000             nan     0.3693   -0.0000
  2720        0.0000             nan     0.3693   -0.0000
  2740        0.0000             nan     0.3693   -0.0000
  2760        0.0000             nan     0.3693   -0.0000
  2780        0.0000             nan     0.3693   -0.0000
  2800        0.0000             nan     0.3693   -0.0000
  2820        0.0000             nan     0.3693   -0.0000
  2840        0.0000             nan     0.3693   -0.0000
  2860        0.0000             nan     0.3693   -0.0000
  2880        0.0000             nan     0.3693   -0.0000
  2900        0.0000             nan     0.3693   -0.0000
  2920        0.0000             nan     0.3693   -0.0000
  2940        0.0000             nan     0.3693   -0.0000
  2960        0.0000             nan     0.3693   -0.0000
  2980        0.0000             nan     0.3693   -0.0000
  3000        0.0000             nan     0.3693   -0.0000
  3020        0.0000             nan     0.3693   -0.0000
  3040        0.0000             nan     0.3693   -0.0000
  3060        0.0000             nan     0.3693   -0.0000
  3080        0.0000             nan     0.3693   -0.0000
  3100        0.0000             nan     0.3693   -0.0000
  3120        0.0000             nan     0.3693   -0.0000
  3140        0.0000             nan     0.3693   -0.0000
  3160        0.0000             nan     0.3693   -0.0000
  3180        0.0000             nan     0.3693   -0.0000
  3200        0.0000             nan     0.3693   -0.0000
  3220        0.0000             nan     0.3693   -0.0000
  3240        0.0000             nan     0.3693   -0.0000
  3260        0.0000             nan     0.3693   -0.0000
  3280        0.0000             nan     0.3693   -0.0000
  3300        0.0000             nan     0.3693   -0.0000
  3320        0.0000             nan     0.3693   -0.0000
  3340        0.0000             nan     0.3693   -0.0000
  3360        0.0000             nan     0.3693   -0.0000
  3380        0.0000             nan     0.3693   -0.0000
  3400        0.0000             nan     0.3693   -0.0000
  3420        0.0000             nan     0.3693   -0.0000
  3440        0.0000             nan     0.3693   -0.0000
  3460        0.0000             nan     0.3693   -0.0000
  3480        0.0000             nan     0.3693   -0.0000
  3500        0.0000             nan     0.3693   -0.0000
  3516        0.0000             nan     0.3693   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.4415             nan     0.4512    0.0731
     2        0.3199             nan     0.4512    0.0748
     3        0.2793             nan     0.4512    0.0101
     4        0.2584             nan     0.4512    0.0064
     5        0.2271             nan     0.4512    0.0135
     6        0.2022             nan     0.4512   -0.0383
     7        0.1798             nan     0.4512    0.0095
     8        0.1561             nan     0.4512   -0.0385
     9        0.1512             nan     0.4512   -0.0259
    10        0.1419             nan     0.4512    0.0013
    20        0.1125             nan     0.4512   -0.0149
    40        0.0650             nan     0.4512   -0.0095
    60        0.0437             nan     0.4512    0.0008
    80        0.0327             nan     0.4512    0.0013
   100        0.0256             nan     0.4512   -0.0061
   120        0.0187             nan     0.4512   -0.0008
   140        0.0146             nan     0.4512   -0.0029
   160        0.0078             nan     0.4512   -0.0013
   180        0.0053             nan     0.4512   -0.0011
   200        0.0039             nan     0.4512   -0.0005
   220        0.0021             nan     0.4512   -0.0005
   240        0.0011             nan     0.4512   -0.0001
   260        0.0008             nan     0.4512   -0.0002
   280        0.0005             nan     0.4512   -0.0000
   300        0.0004             nan     0.4512   -0.0001
   320        0.0003             nan     0.4512   -0.0000
   340        0.0002             nan     0.4512   -0.0000
   360        0.0001             nan     0.4512   -0.0000
   380        0.0001             nan     0.4512   -0.0000
   400        0.0001             nan     0.4512   -0.0000
   420        0.0000             nan     0.4512   -0.0000
   440        0.0000             nan     0.4512   -0.0000
   460        0.0000             nan     0.4512   -0.0000
   480        0.0000             nan     0.4512   -0.0000
   500        0.0000             nan     0.4512   -0.0000
   520        0.0000             nan     0.4512   -0.0000
   540        0.0000             nan     0.4512   -0.0000
   560        0.0000             nan     0.4512   -0.0000
   580        0.0000             nan     0.4512   -0.0000
   600        0.0000             nan     0.4512   -0.0000
   620        0.0000             nan     0.4512   -0.0000
   640        0.0000             nan     0.4512   -0.0000
   660        0.0000             nan     0.4512   -0.0000
   680        0.0000             nan     0.4512   -0.0000
   700        0.0000             nan     0.4512   -0.0000
   720        0.0000             nan     0.4512   -0.0000
   740        0.0000             nan     0.4512   -0.0000
   760        0.0000             nan     0.4512   -0.0000
   780        0.0000             nan     0.4512   -0.0000
   800        0.0000             nan     0.4512   -0.0000
   820        0.0000             nan     0.4512   -0.0000
   840        0.0000             nan     0.4512   -0.0000
   860        0.0000             nan     0.4512   -0.0000
   880        0.0000             nan     0.4512   -0.0000
   900        0.0000             nan     0.4512   -0.0000
   920        0.0000             nan     0.4512   -0.0000
   940        0.0000             nan     0.4512   -0.0000
   960        0.0000             nan     0.4512   -0.0000
   980        0.0000             nan     0.4512   -0.0000
  1000        0.0000             nan     0.4512   -0.0000
  1020        0.0000             nan     0.4512   -0.0000
  1040        0.0000             nan     0.4512   -0.0000
  1060        0.0000             nan     0.4512   -0.0000
  1080        0.0000             nan     0.4512   -0.0000
  1100        0.0000             nan     0.4512   -0.0000
  1120        0.0000             nan     0.4512   -0.0000
  1140        0.0000             nan     0.4512   -0.0000
  1160        0.0000             nan     0.4512   -0.0000
  1180        0.0000             nan     0.4512   -0.0000
  1200        0.0000             nan     0.4512    0.0000
  1220        0.0000             nan     0.4512   -0.0000
  1240        0.0000             nan     0.4512   -0.0000
  1260        0.0000             nan     0.4512   -0.0000
  1280        0.0000             nan     0.4512   -0.0000
  1300        0.0000             nan     0.4512   -0.0000
  1320        0.0000             nan     0.4512   -0.0000
  1340        0.0000             nan     0.4512    0.0000
  1360        0.0000             nan     0.4512   -0.0000
  1380        0.0000             nan     0.4512   -0.0000
  1400        0.0000             nan     0.4512   -0.0000
  1420        0.0000             nan     0.4512   -0.0000
  1440        0.0000             nan     0.4512   -0.0000
  1460        0.0000             nan     0.4512   -0.0000
  1480        0.0000             nan     0.4512   -0.0000
  1500        0.0000             nan     0.4512   -0.0000
  1520        0.0000             nan     0.4512   -0.0000
  1540        0.0000             nan     0.4512   -0.0000
  1560        0.0000             nan     0.4512   -0.0000
  1580        0.0000             nan     0.4512   -0.0000
  1600        0.0000             nan     0.4512   -0.0000
  1620        0.0000             nan     0.4512   -0.0000
  1640        0.0000             nan     0.4512   -0.0000
  1660        0.0000             nan     0.4512   -0.0000
  1680        0.0000             nan     0.4512   -0.0000
  1700        0.0000             nan     0.4512   -0.0000
  1720        0.0000             nan     0.4512   -0.0000
  1740        0.0000             nan     0.4512   -0.0000
  1760        0.0000             nan     0.4512   -0.0000
  1780        0.0000             nan     0.4512   -0.0000
  1800        0.0000             nan     0.4512   -0.0000
  1820        0.0000             nan     0.4512   -0.0000
  1840        0.0000             nan     0.4512   -0.0000
  1860        0.0000             nan     0.4512   -0.0000
  1880        0.0000             nan     0.4512   -0.0000
  1900        0.0000             nan     0.4512   -0.0000
  1920        0.0000             nan     0.4512   -0.0000
  1940        0.0000             nan     0.4512    0.0000
  1960        0.0000             nan     0.4512   -0.0000
  1980        0.0000             nan     0.4512   -0.0000
  2000        0.0000             nan     0.4512   -0.0000
  2020        0.0000             nan     0.4512   -0.0000
  2040        0.0000             nan     0.4512   -0.0000
  2060        0.0000             nan     0.4512   -0.0000
  2080        0.0000             nan     0.4512   -0.0000
  2100        0.0000             nan     0.4512   -0.0000
  2120        0.0000             nan     0.4512   -0.0000
  2140        0.0000             nan     0.4512   -0.0000
  2160        0.0000             nan     0.4512   -0.0000
  2180        0.0000             nan     0.4512   -0.0000
  2200        0.0000             nan     0.4512   -0.0000
  2220        0.0000             nan     0.4512   -0.0000
  2240        0.0000             nan     0.4512   -0.0000
  2260        0.0000             nan     0.4512   -0.0000
  2280        0.0000             nan     0.4512   -0.0000
  2300        0.0000             nan     0.4512   -0.0000
  2320        0.0000             nan     0.4512   -0.0000
  2340        0.0000             nan     0.4512   -0.0000
  2360        0.0000             nan     0.4512   -0.0000
  2380        0.0000             nan     0.4512   -0.0000
  2400        0.0000             nan     0.4512   -0.0000
  2420        0.0000             nan     0.4512   -0.0000
  2440        0.0000             nan     0.4512   -0.0000
  2460        0.0000             nan     0.4512   -0.0000
  2480        0.0000             nan     0.4512   -0.0000
  2500        0.0000             nan     0.4512   -0.0000
  2520        0.0000             nan     0.4512   -0.0000
  2540        0.0000             nan     0.4512   -0.0000
  2560        0.0000             nan     0.4512   -0.0000
  2580        0.0000             nan     0.4512   -0.0000
  2600        0.0000             nan     0.4512   -0.0000
  2620        0.0000             nan     0.4512   -0.0000
  2640        0.0000             nan     0.4512   -0.0000
  2660        0.0000             nan     0.4512   -0.0000
  2680        0.0000             nan     0.4512   -0.0000
  2700        0.0000             nan     0.4512   -0.0000
  2720        0.0000             nan     0.4512   -0.0000
  2740        0.0000             nan     0.4512   -0.0000
  2760        0.0000             nan     0.4512   -0.0000
  2780        0.0000             nan     0.4512   -0.0000
  2800        0.0000             nan     0.4512   -0.0000
  2820        0.0000             nan     0.4512   -0.0000
  2840        0.0000             nan     0.4512   -0.0000
  2860        0.0000             nan     0.4512   -0.0000
  2880        0.0000             nan     0.4512   -0.0000
  2900        0.0000             nan     0.4512   -0.0000
  2920        0.0000             nan     0.4512   -0.0000
  2940        0.0000             nan     0.4512   -0.0000
  2960        0.0000             nan     0.4512   -0.0000
  2980        0.0000             nan     0.4512   -0.0000
  3000        0.0000             nan     0.4512   -0.0000
  3020        0.0000             nan     0.4512   -0.0000
  3040        0.0000             nan     0.4512   -0.0000
  3060        0.0000             nan     0.4512   -0.0000
  3080        0.0000             nan     0.4512   -0.0000
  3100        0.0000             nan     0.4512   -0.0000
  3120        0.0000             nan     0.4512   -0.0000
  3140        0.0000             nan     0.4512   -0.0000
  3160        0.0000             nan     0.4512   -0.0000
  3180        0.0000             nan     0.4512   -0.0000
  3200        0.0000             nan     0.4512   -0.0000
  3220        0.0000             nan     0.4512   -0.0000
  3240        0.0000             nan     0.4512    0.0000
  3260        0.0000             nan     0.4512   -0.0000
  3280        0.0000             nan     0.4512   -0.0000
  3300        0.0000             nan     0.4512   -0.0000
  3320        0.0000             nan     0.4512   -0.0000
  3340        0.0000             nan     0.4512   -0.0000
  3360        0.0000             nan     0.4512   -0.0000
  3380        0.0000             nan     0.4512   -0.0000
  3400        0.0000             nan     0.4512   -0.0000
  3420        0.0000             nan     0.4512   -0.0000
  3427        0.0000             nan     0.4512   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2904             nan     0.5480    0.2777
     2        0.2154             nan     0.5480    0.0205
     3        0.1610             nan     0.5480   -0.0142
     4        0.1335             nan     0.5480    0.0193
     5        0.1302             nan     0.5480   -0.0356
     6        0.1087             nan     0.5480   -0.0222
     7        0.1044             nan     0.5480   -0.0101
     8        0.1070             nan     0.5480   -0.0302
     9        0.0884             nan     0.5480    0.0109
    10        0.0910             nan     0.5480   -0.0207
    20        0.0325             nan     0.5480   -0.0105
    40        0.0079             nan     0.5480   -0.0008
    60        0.0018             nan     0.5480   -0.0002
    80        0.0005             nan     0.5480   -0.0002
   100        0.0001             nan     0.5480   -0.0000
   120        0.0000             nan     0.5480    0.0000
   140        0.0000             nan     0.5480   -0.0000
   160        0.0000             nan     0.5480   -0.0000
   180        0.0000             nan     0.5480   -0.0000
   200        0.0000             nan     0.5480   -0.0000
   220        0.0000             nan     0.5480   -0.0000
   240        0.0000             nan     0.5480   -0.0000
   260        0.0000             nan     0.5480    0.0000
   280        0.0000             nan     0.5480   -0.0000
   300        0.0000             nan     0.5480    0.0000
   320        0.0000             nan     0.5480   -0.0000
   340        0.0000             nan     0.5480   -0.0000
   360        0.0000             nan     0.5480   -0.0000
   380        0.0000             nan     0.5480   -0.0000
   400        0.0000             nan     0.5480   -0.0000
   420        0.0000             nan     0.5480   -0.0000
   440        0.0000             nan     0.5480   -0.0000
   460        0.0000             nan     0.5480   -0.0000
   480        0.0000             nan     0.5480   -0.0000
   500        0.0000             nan     0.5480   -0.0000
   520        0.0000             nan     0.5480   -0.0000
   540        0.0000             nan     0.5480   -0.0000
   560        0.0000             nan     0.5480   -0.0000
   580        0.0000             nan     0.5480    0.0000
   600        0.0000             nan     0.5480   -0.0000
   620        0.0000             nan     0.5480    0.0000
   640        0.0000             nan     0.5480   -0.0000
   660        0.0000             nan     0.5480   -0.0000
   680        0.0000             nan     0.5480   -0.0000
   700        0.0000             nan     0.5480   -0.0000
   720        0.0000             nan     0.5480    0.0000
   740        0.0000             nan     0.5480   -0.0000
   760        0.0000             nan     0.5480   -0.0000
   780        0.0000             nan     0.5480   -0.0000
   800        0.0000             nan     0.5480   -0.0000
   820        0.0000             nan     0.5480   -0.0000
   840        0.0000             nan     0.5480   -0.0000
   860        0.0000             nan     0.5480   -0.0000
   880        0.0000             nan     0.5480   -0.0000
   900        0.0000             nan     0.5480   -0.0000
   920        0.0000             nan     0.5480   -0.0000
   940        0.0000             nan     0.5480   -0.0000
   960        0.0000             nan     0.5480   -0.0000
   980        0.0000             nan     0.5480   -0.0000
  1000        0.0000             nan     0.5480   -0.0000
  1020        0.0000             nan     0.5480   -0.0000
  1040        0.0000             nan     0.5480   -0.0000
  1060        0.0000             nan     0.5480   -0.0000
  1080        0.0000             nan     0.5480   -0.0000
  1100        0.0000             nan     0.5480   -0.0000
  1120        0.0000             nan     0.5480    0.0000
  1140        0.0000             nan     0.5480   -0.0000
  1160        0.0000             nan     0.5480   -0.0000
  1180        0.0000             nan     0.5480   -0.0000
  1200        0.0000             nan     0.5480   -0.0000
  1220        0.0000             nan     0.5480   -0.0000
  1240        0.0000             nan     0.5480    0.0000
  1260        0.0000             nan     0.5480   -0.0000
  1280        0.0000             nan     0.5480   -0.0000
  1300        0.0000             nan     0.5480   -0.0000
  1320        0.0000             nan     0.5480   -0.0000
  1340        0.0000             nan     0.5480   -0.0000
  1360        0.0000             nan     0.5480   -0.0000
  1380        0.0000             nan     0.5480   -0.0000
  1400        0.0000             nan     0.5480   -0.0000
  1420        0.0000             nan     0.5480   -0.0000
  1440        0.0000             nan     0.5480   -0.0000
  1460        0.0000             nan     0.5480   -0.0000
  1480        0.0000             nan     0.5480   -0.0000
  1500        0.0000             nan     0.5480   -0.0000
  1520        0.0000             nan     0.5480   -0.0000
  1540        0.0000             nan     0.5480   -0.0000
  1560        0.0000             nan     0.5480   -0.0000
  1580        0.0000             nan     0.5480    0.0000
  1600        0.0000             nan     0.5480   -0.0000
  1620        0.0000             nan     0.5480   -0.0000
  1640        0.0000             nan     0.5480   -0.0000
  1660        0.0000             nan     0.5480   -0.0000
  1680        0.0000             nan     0.5480   -0.0000
  1700        0.0000             nan     0.5480   -0.0000
  1720        0.0000             nan     0.5480   -0.0000
  1740        0.0000             nan     0.5480   -0.0000
  1760        0.0000             nan     0.5480   -0.0000
  1780        0.0000             nan     0.5480   -0.0000
  1800        0.0000             nan     0.5480   -0.0000
  1820        0.0000             nan     0.5480   -0.0000
  1840        0.0000             nan     0.5480   -0.0000
  1860        0.0000             nan     0.5480   -0.0000
  1880        0.0000             nan     0.5480   -0.0000
  1900        0.0000             nan     0.5480   -0.0000
  1920        0.0000             nan     0.5480   -0.0000
  1940        0.0000             nan     0.5480   -0.0000
  1960        0.0000             nan     0.5480   -0.0000
  1980        0.0000             nan     0.5480    0.0000
  2000        0.0000             nan     0.5480   -0.0000
  2020        0.0000             nan     0.5480   -0.0000
  2040        0.0000             nan     0.5480   -0.0000
  2060        0.0000             nan     0.5480    0.0000
  2080        0.0000             nan     0.5480   -0.0000
  2100        0.0000             nan     0.5480   -0.0000
  2120        0.0000             nan     0.5480   -0.0000
  2140        0.0000             nan     0.5480   -0.0000
  2160        0.0000             nan     0.5480   -0.0000
  2180        0.0000             nan     0.5480   -0.0000
  2200        0.0000             nan     0.5480   -0.0000
  2220        0.0000             nan     0.5480   -0.0000
  2240        0.0000             nan     0.5480   -0.0000
  2260        0.0000             nan     0.5480   -0.0000
  2280        0.0000             nan     0.5480   -0.0000
  2300        0.0000             nan     0.5480   -0.0000
  2320        0.0000             nan     0.5480   -0.0000
  2340        0.0000             nan     0.5480   -0.0000
  2360        0.0000             nan     0.5480   -0.0000
  2380        0.0000             nan     0.5480   -0.0000
  2400        0.0000             nan     0.5480   -0.0000
  2420        0.0000             nan     0.5480   -0.0000
  2440        0.0000             nan     0.5480   -0.0000
  2460        0.0000             nan     0.5480   -0.0000
  2480        0.0000             nan     0.5480   -0.0000
  2500        0.0000             nan     0.5480   -0.0000
  2520        0.0000             nan     0.5480   -0.0000
  2540        0.0000             nan     0.5480   -0.0000
  2560        0.0000             nan     0.5480   -0.0000
  2580        0.0000             nan     0.5480   -0.0000
  2600        0.0000             nan     0.5480   -0.0000
  2620        0.0000             nan     0.5480   -0.0000
  2640        0.0000             nan     0.5480   -0.0000
  2660        0.0000             nan     0.5480   -0.0000
  2680        0.0000             nan     0.5480   -0.0000
  2700        0.0000             nan     0.5480   -0.0000
  2720        0.0000             nan     0.5480   -0.0000
  2740        0.0000             nan     0.5480   -0.0000
  2760        0.0000             nan     0.5480   -0.0000
  2780        0.0000             nan     0.5480   -0.0000
  2800        0.0000             nan     0.5480   -0.0000
  2820        0.0000             nan     0.5480    0.0000
  2840        0.0000             nan     0.5480   -0.0000
  2860        0.0000             nan     0.5480   -0.0000
  2880        0.0000             nan     0.5480   -0.0000
  2900        0.0000             nan     0.5480   -0.0000
  2920        0.0000             nan     0.5480   -0.0000
  2940        0.0000             nan     0.5480   -0.0000
  2960        0.0000             nan     0.5480   -0.0000
  2980        0.0000             nan     0.5480    0.0000
  3000        0.0000             nan     0.5480   -0.0000
  3020        0.0000             nan     0.5480   -0.0000
  3040        0.0000             nan     0.5480   -0.0000
  3060        0.0000             nan     0.5480   -0.0000
  3080        0.0000             nan     0.5480   -0.0000
  3100        0.0000             nan     0.5480   -0.0000
  3120        0.0000             nan     0.5480   -0.0000
  3140        0.0000             nan     0.5480   -0.0000
  3160        0.0000             nan     0.5480    0.0000
  3180        0.0000             nan     0.5480   -0.0000
  3200        0.0000             nan     0.5480    0.0000
  3220        0.0000             nan     0.5480   -0.0000
  3240        0.0000             nan     0.5480   -0.0000
  3260        0.0000             nan     0.5480   -0.0000
  3280        0.0000             nan     0.5480   -0.0000
  3300        0.0000             nan     0.5480   -0.0000
  3320        0.0000             nan     0.5480   -0.0000
  3340        0.0000             nan     0.5480   -0.0000
  3360        0.0000             nan     0.5480   -0.0000
  3380        0.0000             nan     0.5480   -0.0000
  3400        0.0000             nan     0.5480   -0.0000
  3420        0.0000             nan     0.5480   -0.0000
  3440        0.0000             nan     0.5480   -0.0000
  3460        0.0000             nan     0.5480   -0.0000
  3480        0.0000             nan     0.5480   -0.0000
  3500        0.0000             nan     0.5480   -0.0000
  3520        0.0000             nan     0.5480   -0.0000
  3540        0.0000             nan     0.5480   -0.0000
  3560        0.0000             nan     0.5480   -0.0000
  3580        0.0000             nan     0.5480   -0.0000
  3600        0.0000             nan     0.5480   -0.0000
  3620        0.0000             nan     0.5480   -0.0000
  3640        0.0000             nan     0.5480   -0.0000
  3660        0.0000             nan     0.5480   -0.0000
  3680        0.0000             nan     0.5480   -0.0000
  3700        0.0000             nan     0.5480   -0.0000
  3720        0.0000             nan     0.5480   -0.0000
  3740        0.0000             nan     0.5480   -0.0000
  3760        0.0000             nan     0.5480   -0.0000
  3780        0.0000             nan     0.5480   -0.0000
  3800        0.0000             nan     0.5480   -0.0000
  3820        0.0000             nan     0.5480   -0.0000
  3840        0.0000             nan     0.5480   -0.0000
  3860        0.0000             nan     0.5480   -0.0000
  3880        0.0000             nan     0.5480   -0.0000
  3900        0.0000             nan     0.5480   -0.0000
  3920        0.0000             nan     0.5480   -0.0000
  3940        0.0000             nan     0.5480   -0.0000
  3955        0.0000             nan     0.5480   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.4313             nan     0.5705    0.1734
     2        0.3846             nan     0.5705    0.0246
     3        0.3867             nan     0.5705   -0.0296
     4        0.3721             nan     0.5705    0.0028
     5        0.3470             nan     0.5705    0.0347
     6        0.3240             nan     0.5705   -0.0283
     7        0.3047             nan     0.5705   -0.0089
     8        0.2796             nan     0.5705   -0.0112
     9        0.2712             nan     0.5705   -0.0157
    10        0.2697             nan     0.5705   -0.0157
    20        0.2487             nan     0.5705   -0.0435
    40        0.1922             nan     0.5705   -0.0299
    60        0.1058             nan     0.5705   -0.0154
    80        0.0748             nan     0.5705   -0.0142
   100        0.0556             nan     0.5705   -0.0101
   120        0.0477             nan     0.5705   -0.0135
   140        0.0407             nan     0.5705   -0.0046
   160        0.0321             nan     0.5705    0.0004
   180        0.0290             nan     0.5705   -0.0027
   200        0.0236             nan     0.5705   -0.0036
   220        0.0202             nan     0.5705   -0.0065
   240        0.0151             nan     0.5705   -0.0015
   260        0.0134             nan     0.5705   -0.0016
   280        0.0123             nan     0.5705   -0.0025
   300        0.0096             nan     0.5705   -0.0007
   320        0.0104             nan     0.5705   -0.0028
   340        0.0079             nan     0.5705   -0.0017
   360        0.0068             nan     0.5705   -0.0017
   380        0.0057             nan     0.5705   -0.0002
   400        0.0052             nan     0.5705   -0.0010
   420        0.0041             nan     0.5705   -0.0002
   440        0.0030             nan     0.5705   -0.0001
   460        0.0030             nan     0.5705   -0.0001
   480        0.0025             nan     0.5705    0.0000
   500        0.0023             nan     0.5705   -0.0002
   520        0.0019             nan     0.5705   -0.0002
   540        0.0018             nan     0.5705   -0.0001
   560        0.0017             nan     0.5705   -0.0003
   580        0.0011             nan     0.5705   -0.0000
   600        0.0010             nan     0.5705   -0.0001
   620        0.0008             nan     0.5705   -0.0001
   640        0.0007             nan     0.5705   -0.0001
   660        0.0006             nan     0.5705   -0.0001
   680        0.0006             nan     0.5705   -0.0001
   700        0.0005             nan     0.5705   -0.0001
   720        0.0005             nan     0.5705   -0.0001
   740        0.0004             nan     0.5705   -0.0000
   760        0.0004             nan     0.5705   -0.0001
   780        0.0003             nan     0.5705   -0.0001
   800        0.0003             nan     0.5705   -0.0000
   820        0.0003             nan     0.5705   -0.0000
   840        0.0002             nan     0.5705   -0.0000
   860        0.0002             nan     0.5705   -0.0000
   880        0.0002             nan     0.5705   -0.0000
   900        0.0001             nan     0.5705   -0.0000
   920        0.0001             nan     0.5705   -0.0000
   940        0.0001             nan     0.5705   -0.0000
   960        0.0001             nan     0.5705   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.9232             nan     0.0176    0.0133
     2        0.9128             nan     0.0176    0.0114
     3        0.8968             nan     0.0176    0.0092
     4        0.8841             nan     0.0176    0.0098
     5        0.8725             nan     0.0176    0.0085
     6        0.8583             nan     0.0176    0.0104
     7        0.8444             nan     0.0176    0.0101
     8        0.8318             nan     0.0176    0.0103
     9        0.8235             nan     0.0176    0.0096
    10        0.8136             nan     0.0176    0.0099
    20        0.7200             nan     0.0176    0.0056
    40        0.6155             nan     0.0176    0.0042
    60        0.5349             nan     0.0176    0.0020
    80        0.4844             nan     0.0176    0.0017
   100        0.4437             nan     0.0176    0.0009
   120        0.4134             nan     0.0176    0.0005
   140        0.3827             nan     0.0176   -0.0002
   160        0.3592             nan     0.0176   -0.0007
   180        0.3380             nan     0.0176   -0.0010
   200        0.3191             nan     0.0176   -0.0007
   220        0.3048             nan     0.0176   -0.0006
   240        0.2937             nan     0.0176    0.0000
   260        0.2836             nan     0.0176   -0.0007
   280        0.2753             nan     0.0176   -0.0008
   300        0.2676             nan     0.0176   -0.0013
   320        0.2625             nan     0.0176   -0.0011
   340        0.2561             nan     0.0176   -0.0015
   360        0.2505             nan     0.0176   -0.0013
   380        0.2448             nan     0.0176   -0.0002
   400        0.2394             nan     0.0176   -0.0002
   420        0.2352             nan     0.0176   -0.0009
   440        0.2321             nan     0.0176   -0.0012
   460        0.2273             nan     0.0176   -0.0005
   480        0.2239             nan     0.0176   -0.0005
   500        0.2186             nan     0.0176   -0.0003
   520        0.2141             nan     0.0176   -0.0010
   540        0.2110             nan     0.0176   -0.0010
   560        0.2072             nan     0.0176   -0.0003
   580        0.2035             nan     0.0176   -0.0018
   600        0.2007             nan     0.0176   -0.0012
   620        0.1979             nan     0.0176   -0.0000
   640        0.1949             nan     0.0176   -0.0009
   660        0.1920             nan     0.0176   -0.0006
   680        0.1870             nan     0.0176   -0.0012
   700        0.1836             nan     0.0176   -0.0002
   720        0.1812             nan     0.0176   -0.0002
   740        0.1784             nan     0.0176   -0.0004
   760        0.1759             nan     0.0176   -0.0004
   780        0.1736             nan     0.0176   -0.0002
   800        0.1701             nan     0.0176   -0.0005
   820        0.1679             nan     0.0176   -0.0007
   840        0.1658             nan     0.0176   -0.0004
   860        0.1629             nan     0.0176   -0.0006
   880        0.1609             nan     0.0176   -0.0005
   900        0.1592             nan     0.0176   -0.0012
   920        0.1570             nan     0.0176   -0.0001
   940        0.1545             nan     0.0176   -0.0004
   960        0.1528             nan     0.0176   -0.0004
   980        0.1513             nan     0.0176   -0.0006
  1000        0.1492             nan     0.0176   -0.0004
  1020        0.1465             nan     0.0176   -0.0005
  1040        0.1449             nan     0.0176   -0.0004
  1060        0.1432             nan     0.0176   -0.0002
  1080        0.1410             nan     0.0176   -0.0003
  1100        0.1390             nan     0.0176   -0.0004
  1120        0.1369             nan     0.0176   -0.0001
  1140        0.1353             nan     0.0176   -0.0004
  1160        0.1338             nan     0.0176   -0.0011
  1180        0.1317             nan     0.0176   -0.0008
  1200        0.1300             nan     0.0176   -0.0002
  1220        0.1288             nan     0.0176   -0.0002
  1240        0.1271             nan     0.0176   -0.0005
  1260        0.1252             nan     0.0176   -0.0004
  1280        0.1246             nan     0.0176   -0.0006
  1300        0.1232             nan     0.0176   -0.0005
  1320        0.1212             nan     0.0176   -0.0001
  1340        0.1199             nan     0.0176   -0.0003
  1360        0.1186             nan     0.0176   -0.0007
  1380        0.1172             nan     0.0176   -0.0004
  1400        0.1162             nan     0.0176   -0.0002
  1420        0.1149             nan     0.0176   -0.0006
  1440        0.1141             nan     0.0176   -0.0004
  1460        0.1127             nan     0.0176   -0.0005
  1480        0.1110             nan     0.0176   -0.0003
  1500        0.1098             nan     0.0176   -0.0005
  1520        0.1087             nan     0.0176   -0.0002
  1540        0.1076             nan     0.0176   -0.0002
  1560        0.1067             nan     0.0176   -0.0005
  1580        0.1054             nan     0.0176   -0.0009
  1600        0.1042             nan     0.0176   -0.0005
  1620        0.1028             nan     0.0176   -0.0005
  1640        0.1016             nan     0.0176   -0.0003
  1660        0.1004             nan     0.0176   -0.0002
  1680        0.0996             nan     0.0176   -0.0005
  1700        0.0984             nan     0.0176   -0.0004
  1720        0.0972             nan     0.0176   -0.0001
  1740        0.0960             nan     0.0176   -0.0006
  1760        0.0951             nan     0.0176   -0.0006
  1780        0.0939             nan     0.0176   -0.0004
  1800        0.0932             nan     0.0176   -0.0001
  1820        0.0923             nan     0.0176   -0.0004
  1840        0.0914             nan     0.0176   -0.0003
  1860        0.0903             nan     0.0176   -0.0002
  1880        0.0893             nan     0.0176   -0.0003
  1900        0.0884             nan     0.0176   -0.0002
  1920        0.0873             nan     0.0176   -0.0002
  1940        0.0864             nan     0.0176   -0.0003
  1960        0.0857             nan     0.0176   -0.0004
  1980        0.0848             nan     0.0176   -0.0001
  2000        0.0841             nan     0.0176   -0.0003
  2020        0.0835             nan     0.0176   -0.0002
  2040        0.0828             nan     0.0176   -0.0003
  2060        0.0817             nan     0.0176   -0.0001
  2080        0.0807             nan     0.0176   -0.0005
  2100        0.0801             nan     0.0176   -0.0002
  2120        0.0793             nan     0.0176   -0.0002
  2140        0.0783             nan     0.0176   -0.0003
  2160        0.0777             nan     0.0176   -0.0002
  2180        0.0770             nan     0.0176   -0.0004
  2200        0.0763             nan     0.0176   -0.0005
  2220        0.0757             nan     0.0176   -0.0005
  2240        0.0752             nan     0.0176   -0.0002
  2260        0.0744             nan     0.0176   -0.0004
  2280        0.0736             nan     0.0176   -0.0003
  2300        0.0728             nan     0.0176   -0.0002
  2320        0.0722             nan     0.0176   -0.0004
  2340        0.0716             nan     0.0176   -0.0002
  2360        0.0709             nan     0.0176   -0.0003
  2380        0.0704             nan     0.0176   -0.0006
  2400        0.0697             nan     0.0176   -0.0001
  2420        0.0691             nan     0.0176   -0.0003
  2440        0.0682             nan     0.0176   -0.0004
  2460        0.0676             nan     0.0176   -0.0001
  2480        0.0668             nan     0.0176   -0.0002
  2500        0.0662             nan     0.0176   -0.0002
  2520        0.0654             nan     0.0176   -0.0001
  2540        0.0648             nan     0.0176   -0.0001
  2560        0.0644             nan     0.0176   -0.0003
  2580        0.0636             nan     0.0176   -0.0003
  2600        0.0632             nan     0.0176   -0.0005
  2620        0.0626             nan     0.0176   -0.0002
  2640        0.0620             nan     0.0176   -0.0003
  2660        0.0613             nan     0.0176   -0.0002
  2680        0.0607             nan     0.0176   -0.0001
  2700        0.0601             nan     0.0176   -0.0002
  2720        0.0594             nan     0.0176   -0.0003
  2740        0.0588             nan     0.0176   -0.0001
  2760        0.0583             nan     0.0176   -0.0003
  2780        0.0579             nan     0.0176   -0.0002
  2800        0.0574             nan     0.0176   -0.0001
  2820        0.0570             nan     0.0176   -0.0001
  2840        0.0564             nan     0.0176   -0.0002
  2860        0.0559             nan     0.0176   -0.0002
  2880        0.0553             nan     0.0176   -0.0003
  2900        0.0549             nan     0.0176   -0.0002
  2920        0.0542             nan     0.0176   -0.0001
  2940        0.0538             nan     0.0176   -0.0001
  2960        0.0532             nan     0.0176   -0.0002
  2980        0.0529             nan     0.0176   -0.0004
  3000        0.0524             nan     0.0176   -0.0001
  3020        0.0519             nan     0.0176   -0.0001
  3040        0.0515             nan     0.0176   -0.0000
  3060        0.0510             nan     0.0176   -0.0004
  3080        0.0504             nan     0.0176   -0.0005
  3100        0.0499             nan     0.0176   -0.0004
  3120        0.0496             nan     0.0176   -0.0003
  3140        0.0493             nan     0.0176   -0.0001
  3160        0.0489             nan     0.0176   -0.0001
  3180        0.0485             nan     0.0176   -0.0004
  3200        0.0482             nan     0.0176   -0.0001
  3220        0.0478             nan     0.0176   -0.0001
  3240        0.0474             nan     0.0176   -0.0002
  3260        0.0471             nan     0.0176   -0.0003
  3280        0.0467             nan     0.0176   -0.0001
  3300        0.0462             nan     0.0176   -0.0001
  3320        0.0457             nan     0.0176   -0.0001
  3340        0.0455             nan     0.0176   -0.0002
  3360        0.0451             nan     0.0176   -0.0001
  3380        0.0448             nan     0.0176   -0.0004
  3400        0.0446             nan     0.0176   -0.0003
  3420        0.0441             nan     0.0176   -0.0001
  3440        0.0438             nan     0.0176   -0.0001
  3460        0.0434             nan     0.0176    0.0000
  3480        0.0432             nan     0.0176   -0.0001
  3500        0.0428             nan     0.0176    0.0000
  3520        0.0423             nan     0.0176   -0.0001
  3540        0.0419             nan     0.0176   -0.0001
  3560        0.0415             nan     0.0176   -0.0000
  3580        0.0410             nan     0.0176   -0.0001
  3600        0.0409             nan     0.0176   -0.0002
  3620        0.0405             nan     0.0176   -0.0002
  3640        0.0402             nan     0.0176   -0.0001
  3660        0.0399             nan     0.0176   -0.0001
  3680        0.0397             nan     0.0176   -0.0001
  3700        0.0391             nan     0.0176   -0.0003
  3720        0.0388             nan     0.0176   -0.0001
  3740        0.0385             nan     0.0176   -0.0001
  3760        0.0383             nan     0.0176   -0.0001
  3780        0.0379             nan     0.0176   -0.0001
  3800        0.0376             nan     0.0176   -0.0001
  3820        0.0374             nan     0.0176   -0.0001
  3822        0.0374             nan     0.0176   -0.0001

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.8879             nan     0.0376    0.0320
     2        0.8561             nan     0.0376    0.0352
     3        0.8265             nan     0.0376    0.0312
     4        0.7913             nan     0.0376    0.0321
     5        0.7673             nan     0.0376    0.0236
     6        0.7377             nan     0.0376    0.0274
     7        0.7080             nan     0.0376    0.0243
     8        0.6877             nan     0.0376    0.0210
     9        0.6705             nan     0.0376    0.0188
    10        0.6515             nan     0.0376    0.0190
    20        0.5147             nan     0.0376    0.0064
    40        0.3730             nan     0.0376    0.0005
    60        0.2960             nan     0.0376   -0.0035
    80        0.2514             nan     0.0376   -0.0041
   100        0.2078             nan     0.0376   -0.0003
   120        0.1876             nan     0.0376   -0.0014
   140        0.1660             nan     0.0376   -0.0024
   160        0.1560             nan     0.0376   -0.0038
   180        0.1433             nan     0.0376   -0.0005
   200        0.1309             nan     0.0376   -0.0012
   220        0.1220             nan     0.0376   -0.0011
   240        0.1124             nan     0.0376   -0.0009
   260        0.1055             nan     0.0376    0.0001
   280        0.0984             nan     0.0376   -0.0009
   300        0.0907             nan     0.0376   -0.0007
   320        0.0852             nan     0.0376   -0.0006
   340        0.0780             nan     0.0376   -0.0005
   360        0.0719             nan     0.0376   -0.0009
   380        0.0675             nan     0.0376   -0.0008
   400        0.0641             nan     0.0376   -0.0002
   420        0.0596             nan     0.0376   -0.0013
   440        0.0556             nan     0.0376   -0.0007
   460        0.0526             nan     0.0376   -0.0005
   480        0.0491             nan     0.0376   -0.0003
   500        0.0468             nan     0.0376   -0.0002
   520        0.0450             nan     0.0376   -0.0004
   540        0.0416             nan     0.0376   -0.0003
   560        0.0400             nan     0.0376   -0.0004
   580        0.0379             nan     0.0376   -0.0003
   600        0.0364             nan     0.0376   -0.0007
   620        0.0349             nan     0.0376   -0.0002
   640        0.0330             nan     0.0376   -0.0002
   660        0.0312             nan     0.0376   -0.0004
   680        0.0296             nan     0.0376   -0.0001
   700        0.0276             nan     0.0376   -0.0001
   720        0.0261             nan     0.0376   -0.0002
   740        0.0250             nan     0.0376   -0.0001
   760        0.0241             nan     0.0376   -0.0002
   780        0.0228             nan     0.0376   -0.0002
   800        0.0219             nan     0.0376   -0.0002
   820        0.0206             nan     0.0376   -0.0004
   840        0.0198             nan     0.0376   -0.0001
   860        0.0189             nan     0.0376   -0.0003
   880        0.0176             nan     0.0376   -0.0002
   900        0.0167             nan     0.0376   -0.0001
   920        0.0157             nan     0.0376   -0.0002
   940        0.0149             nan     0.0376   -0.0002
   960        0.0143             nan     0.0376   -0.0001
   980        0.0138             nan     0.0376    0.0000
  1000        0.0132             nan     0.0376   -0.0002
  1020        0.0128             nan     0.0376   -0.0002
  1040        0.0121             nan     0.0376   -0.0000
  1060        0.0114             nan     0.0376   -0.0002
  1080        0.0110             nan     0.0376   -0.0001
  1100        0.0106             nan     0.0376   -0.0001
  1120        0.0100             nan     0.0376   -0.0001
  1140        0.0095             nan     0.0376   -0.0000
  1160        0.0091             nan     0.0376   -0.0001
  1180        0.0087             nan     0.0376   -0.0000
  1200        0.0083             nan     0.0376   -0.0001
  1220        0.0080             nan     0.0376   -0.0001
  1240        0.0076             nan     0.0376   -0.0000
  1260        0.0072             nan     0.0376   -0.0000
  1280        0.0070             nan     0.0376   -0.0000
  1300        0.0067             nan     0.0376   -0.0001
  1320        0.0065             nan     0.0376   -0.0001
  1340        0.0062             nan     0.0376   -0.0001
  1360        0.0059             nan     0.0376   -0.0001
  1380        0.0057             nan     0.0376   -0.0000
  1400        0.0054             nan     0.0376   -0.0000
  1420        0.0052             nan     0.0376   -0.0001
  1440        0.0049             nan     0.0376   -0.0000
  1460        0.0047             nan     0.0376   -0.0000
  1480        0.0045             nan     0.0376   -0.0000
  1500        0.0043             nan     0.0376   -0.0000
  1520        0.0041             nan     0.0376   -0.0001
  1540        0.0039             nan     0.0376   -0.0000
  1560        0.0038             nan     0.0376   -0.0000
  1580        0.0037             nan     0.0376   -0.0000
  1600        0.0035             nan     0.0376   -0.0000
  1620        0.0034             nan     0.0376   -0.0000
  1640        0.0032             nan     0.0376   -0.0000
  1660        0.0031             nan     0.0376   -0.0000
  1680        0.0030             nan     0.0376   -0.0000
  1700        0.0028             nan     0.0376   -0.0000
  1720        0.0027             nan     0.0376   -0.0000
  1740        0.0026             nan     0.0376   -0.0000
  1760        0.0025             nan     0.0376   -0.0000
  1780        0.0024             nan     0.0376   -0.0000
  1800        0.0023             nan     0.0376   -0.0000
  1805        0.0023             nan     0.0376   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.8538             nan     0.1185    0.0765
     2        0.7844             nan     0.1185    0.0707
     3        0.7259             nan     0.1185    0.0527
     4        0.6819             nan     0.1185    0.0508
     5        0.6621             nan     0.1185    0.0228
     6        0.6310             nan     0.1185    0.0289
     7        0.5985             nan     0.1185    0.0331
     8        0.5728             nan     0.1185    0.0252
     9        0.5609             nan     0.1185   -0.0081
    10        0.5314             nan     0.1185    0.0244
    20        0.3907             nan     0.1185   -0.0099
    40        0.3318             nan     0.1185   -0.0070
    60        0.2686             nan     0.1185   -0.0043
    80        0.2471             nan     0.1185   -0.0042
   100        0.2234             nan     0.1185   -0.0015
   120        0.2087             nan     0.1185   -0.0073
   140        0.1840             nan     0.1185   -0.0033
   160        0.1664             nan     0.1185   -0.0025
   180        0.1495             nan     0.1185   -0.0012
   200        0.1355             nan     0.1185   -0.0031
   220        0.1239             nan     0.1185   -0.0046
   240        0.1170             nan     0.1185   -0.0024
   260        0.1109             nan     0.1185   -0.0058
   280        0.1026             nan     0.1185   -0.0013
   300        0.0962             nan     0.1185   -0.0065
   320        0.0888             nan     0.1185   -0.0052
   340        0.0825             nan     0.1185   -0.0024
   360        0.0783             nan     0.1185   -0.0013
   380        0.0734             nan     0.1185   -0.0022
   400        0.0687             nan     0.1185   -0.0011
   420        0.0642             nan     0.1185   -0.0028
   440        0.0600             nan     0.1185   -0.0012
   460        0.0569             nan     0.1185   -0.0022
   480        0.0547             nan     0.1185   -0.0016
   500        0.0531             nan     0.1185   -0.0009
   520        0.0503             nan     0.1185   -0.0007
   540        0.0476             nan     0.1185   -0.0009
   560        0.0448             nan     0.1185   -0.0017
   580        0.0416             nan     0.1185   -0.0009
   600        0.0402             nan     0.1185   -0.0004
   620        0.0382             nan     0.1185   -0.0011
   640        0.0368             nan     0.1185   -0.0005
   660        0.0344             nan     0.1185   -0.0004
   680        0.0333             nan     0.1185   -0.0009
   700        0.0316             nan     0.1185   -0.0005
   720        0.0309             nan     0.1185   -0.0003
   740        0.0294             nan     0.1185   -0.0002
   760        0.0281             nan     0.1185   -0.0003
   780        0.0267             nan     0.1185   -0.0013
   800        0.0252             nan     0.1185   -0.0004
   820        0.0240             nan     0.1185   -0.0008
   840        0.0232             nan     0.1185   -0.0012
   860        0.0228             nan     0.1185   -0.0003
   880        0.0221             nan     0.1185   -0.0004
   900        0.0212             nan     0.1185   -0.0005
   920        0.0203             nan     0.1185   -0.0001
   940        0.0197             nan     0.1185   -0.0007
   960        0.0194             nan     0.1185   -0.0006
   980        0.0192             nan     0.1185   -0.0005
  1000        0.0185             nan     0.1185   -0.0005
  1020        0.0172             nan     0.1185   -0.0004
  1040        0.0170             nan     0.1185   -0.0002
  1060        0.0164             nan     0.1185   -0.0003
  1080        0.0163             nan     0.1185   -0.0007
  1100        0.0154             nan     0.1185   -0.0002
  1120        0.0152             nan     0.1185   -0.0005
  1140        0.0145             nan     0.1185   -0.0002
  1160        0.0138             nan     0.1185    0.0000
  1180        0.0131             nan     0.1185   -0.0003
  1200        0.0127             nan     0.1185   -0.0005
  1220        0.0124             nan     0.1185   -0.0005
  1240        0.0117             nan     0.1185   -0.0003
  1260        0.0114             nan     0.1185   -0.0000
  1280        0.0112             nan     0.1185   -0.0001
  1300        0.0111             nan     0.1185   -0.0002
  1320        0.0108             nan     0.1185   -0.0001
  1340        0.0106             nan     0.1185   -0.0004
  1360        0.0106             nan     0.1185   -0.0002
  1380        0.0099             nan     0.1185   -0.0002
  1400        0.0094             nan     0.1185   -0.0002
  1420        0.0090             nan     0.1185   -0.0002
  1440        0.0086             nan     0.1185   -0.0002
  1460        0.0083             nan     0.1185   -0.0001
  1480        0.0079             nan     0.1185   -0.0002
  1500        0.0079             nan     0.1185   -0.0002
  1520        0.0075             nan     0.1185   -0.0002
  1540        0.0074             nan     0.1185   -0.0002
  1560        0.0071             nan     0.1185   -0.0001
  1580        0.0070             nan     0.1185   -0.0001
  1600        0.0067             nan     0.1185   -0.0003
  1620        0.0065             nan     0.1185   -0.0001
  1640        0.0063             nan     0.1185   -0.0002
  1660        0.0060             nan     0.1185   -0.0001
  1680        0.0058             nan     0.1185   -0.0002
  1700        0.0055             nan     0.1185   -0.0002
  1720        0.0053             nan     0.1185   -0.0002
  1740        0.0051             nan     0.1185   -0.0000
  1760        0.0050             nan     0.1185   -0.0001
  1780        0.0049             nan     0.1185   -0.0001
  1800        0.0047             nan     0.1185   -0.0001
  1820        0.0045             nan     0.1185   -0.0002
  1840        0.0044             nan     0.1185   -0.0000
  1860        0.0043             nan     0.1185   -0.0001
  1880        0.0042             nan     0.1185   -0.0001
  1900        0.0039             nan     0.1185   -0.0001
  1920        0.0038             nan     0.1185   -0.0002
  1940        0.0037             nan     0.1185   -0.0000
  1960        0.0036             nan     0.1185   -0.0001
  1980        0.0034             nan     0.1185   -0.0001
  2000        0.0034             nan     0.1185   -0.0001
  2020        0.0034             nan     0.1185   -0.0000
  2040        0.0032             nan     0.1185   -0.0001
  2060        0.0031             nan     0.1185   -0.0000
  2080        0.0031             nan     0.1185   -0.0001
  2100        0.0030             nan     0.1185   -0.0000
  2120        0.0030             nan     0.1185   -0.0001
  2140        0.0029             nan     0.1185   -0.0001
  2160        0.0029             nan     0.1185   -0.0000
  2180        0.0027             nan     0.1185   -0.0000
  2200        0.0026             nan     0.1185   -0.0001
  2220        0.0025             nan     0.1185   -0.0001
  2240        0.0024             nan     0.1185   -0.0001
  2260        0.0024             nan     0.1185   -0.0000
  2280        0.0023             nan     0.1185   -0.0000
  2300        0.0022             nan     0.1185   -0.0001
  2320        0.0021             nan     0.1185   -0.0000
  2340        0.0021             nan     0.1185   -0.0001
  2360        0.0020             nan     0.1185   -0.0000
  2380        0.0020             nan     0.1185   -0.0000
  2400        0.0020             nan     0.1185   -0.0001
  2420        0.0019             nan     0.1185   -0.0000
  2440        0.0019             nan     0.1185   -0.0000
  2460        0.0018             nan     0.1185   -0.0001
  2480        0.0018             nan     0.1185   -0.0000
  2500        0.0017             nan     0.1185   -0.0001
  2520        0.0016             nan     0.1185   -0.0000
  2540        0.0016             nan     0.1185   -0.0000
  2560        0.0016             nan     0.1185   -0.0001
  2580        0.0016             nan     0.1185   -0.0001
  2600        0.0016             nan     0.1185   -0.0000
  2620        0.0015             nan     0.1185   -0.0000
  2640        0.0015             nan     0.1185   -0.0000
  2660        0.0015             nan     0.1185   -0.0001
  2680        0.0015             nan     0.1185   -0.0000
  2700        0.0014             nan     0.1185   -0.0000
  2720        0.0013             nan     0.1185   -0.0000
  2740        0.0013             nan     0.1185   -0.0000
  2760        0.0013             nan     0.1185   -0.0000
  2780        0.0012             nan     0.1185   -0.0000
  2800        0.0012             nan     0.1185   -0.0001
  2820        0.0012             nan     0.1185   -0.0000
  2840        0.0011             nan     0.1185   -0.0000
  2860        0.0011             nan     0.1185   -0.0000
  2880        0.0011             nan     0.1185   -0.0000
  2900        0.0010             nan     0.1185   -0.0000
  2920        0.0010             nan     0.1185   -0.0000
  2940        0.0010             nan     0.1185   -0.0000
  2960        0.0010             nan     0.1185   -0.0000
  2980        0.0010             nan     0.1185   -0.0000
  3000        0.0009             nan     0.1185   -0.0000
  3020        0.0009             nan     0.1185   -0.0000
  3040        0.0009             nan     0.1185   -0.0000
  3060        0.0009             nan     0.1185   -0.0000
  3080        0.0009             nan     0.1185   -0.0000
  3100        0.0008             nan     0.1185   -0.0000
  3120        0.0008             nan     0.1185   -0.0000
  3140        0.0008             nan     0.1185   -0.0000
  3160        0.0008             nan     0.1185   -0.0000
  3180        0.0008             nan     0.1185   -0.0000
  3200        0.0007             nan     0.1185   -0.0000
  3220        0.0007             nan     0.1185   -0.0000
  3240        0.0007             nan     0.1185   -0.0000
  3260        0.0007             nan     0.1185   -0.0000
  3280        0.0007             nan     0.1185   -0.0000
  3300        0.0007             nan     0.1185   -0.0000
  3320        0.0006             nan     0.1185   -0.0000
  3340        0.0006             nan     0.1185   -0.0000
  3360        0.0006             nan     0.1185   -0.0000
  3380        0.0006             nan     0.1185   -0.0000
  3400        0.0006             nan     0.1185   -0.0000
  3420        0.0006             nan     0.1185   -0.0000
  3440        0.0006             nan     0.1185   -0.0000
  3460        0.0006             nan     0.1185   -0.0000
  3480        0.0005             nan     0.1185   -0.0000
  3500        0.0005             nan     0.1185   -0.0000
  3520        0.0005             nan     0.1185   -0.0000
  3540        0.0005             nan     0.1185   -0.0000
  3560        0.0005             nan     0.1185   -0.0000
  3580        0.0005             nan     0.1185   -0.0000
  3600        0.0005             nan     0.1185   -0.0000
  3620        0.0005             nan     0.1185   -0.0000
  3640        0.0005             nan     0.1185   -0.0000
  3660        0.0004             nan     0.1185   -0.0000
  3677        0.0004             nan     0.1185   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.7589             nan     0.1233    0.0770
     2        0.6790             nan     0.1233    0.0842
     3        0.5976             nan     0.1233    0.0709
     4        0.5717             nan     0.1233    0.0381
     5        0.5296             nan     0.1233    0.0477
     6        0.5048             nan     0.1233    0.0345
     7        0.4571             nan     0.1233    0.0314
     8        0.4479             nan     0.1233   -0.0019
     9        0.4335             nan     0.1233    0.0094
    10        0.4092             nan     0.1233    0.0060
    20        0.2999             nan     0.1233   -0.0019
    40        0.1715             nan     0.1233   -0.0066
    60        0.1350             nan     0.1233   -0.0053
    80        0.0981             nan     0.1233   -0.0010
   100        0.0844             nan     0.1233   -0.0037
   120        0.0702             nan     0.1233   -0.0016
   140        0.0592             nan     0.1233   -0.0018
   160        0.0455             nan     0.1233   -0.0022
   180        0.0370             nan     0.1233   -0.0011
   200        0.0292             nan     0.1233   -0.0019
   220        0.0263             nan     0.1233   -0.0007
   240        0.0221             nan     0.1233   -0.0007
   260        0.0191             nan     0.1233   -0.0005
   280        0.0162             nan     0.1233   -0.0004
   300        0.0143             nan     0.1233   -0.0005
   320        0.0123             nan     0.1233   -0.0002
   340        0.0108             nan     0.1233   -0.0003
   360        0.0095             nan     0.1233   -0.0005
   380        0.0086             nan     0.1233   -0.0004
   400        0.0076             nan     0.1233   -0.0002
   420        0.0067             nan     0.1233    0.0000
   440        0.0057             nan     0.1233   -0.0003
   460        0.0046             nan     0.1233   -0.0000
   480        0.0041             nan     0.1233   -0.0001
   500        0.0033             nan     0.1233   -0.0001
   520        0.0028             nan     0.1233   -0.0001
   540        0.0024             nan     0.1233   -0.0000
   560        0.0022             nan     0.1233   -0.0001
   580        0.0019             nan     0.1233   -0.0001
   600        0.0017             nan     0.1233   -0.0001
   620        0.0015             nan     0.1233   -0.0000
   640        0.0013             nan     0.1233   -0.0000
   660        0.0011             nan     0.1233   -0.0001
   680        0.0010             nan     0.1233   -0.0001
   700        0.0009             nan     0.1233   -0.0000
   715        0.0008             nan     0.1233   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.8418             nan     0.1555    0.0986
     2        0.7920             nan     0.1555    0.0692
     3        0.7029             nan     0.1555    0.0509
     4        0.6708             nan     0.1555    0.0400
     5        0.6466             nan     0.1555    0.0346
     6        0.6174             nan     0.1555    0.0038
     7        0.5917             nan     0.1555    0.0277
     8        0.5753             nan     0.1555   -0.0252
     9        0.5552             nan     0.1555    0.0175
    10        0.5328             nan     0.1555    0.0087
    20        0.4458             nan     0.1555    0.0161
    40        0.3605             nan     0.1555   -0.0188
    60        0.3198             nan     0.1555   -0.0009
    80        0.2844             nan     0.1555   -0.0052
   100        0.2620             nan     0.1555   -0.0070
   120        0.2487             nan     0.1555   -0.0176
   140        0.2283             nan     0.1555   -0.0086
   160        0.2101             nan     0.1555   -0.0091
   180        0.2027             nan     0.1555   -0.0042
   200        0.1904             nan     0.1555   -0.0063
   220        0.1828             nan     0.1555   -0.0110
   240        0.1703             nan     0.1555   -0.0058
   260        0.1585             nan     0.1555   -0.0023
   280        0.1475             nan     0.1555    0.0018
   300        0.1412             nan     0.1555   -0.0016
   320        0.1403             nan     0.1555   -0.0006
   340        0.1320             nan     0.1555   -0.0010
   360        0.1314             nan     0.1555   -0.0031
   380        0.1238             nan     0.1555   -0.0019
   400        0.1179             nan     0.1555   -0.0014
   405        0.1163             nan     0.1555   -0.0009

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.6821             nan     0.3693    0.2473
     2        0.5511             nan     0.3693    0.1457
     3        0.4358             nan     0.3693    0.0322
     4        0.3756             nan     0.3693    0.0412
     5        0.3684             nan     0.3693   -0.0510
     6        0.3310             nan     0.3693    0.0181
     7        0.3080             nan     0.3693   -0.0112
     8        0.3001             nan     0.3693   -0.0196
     9        0.2786             nan     0.3693    0.0163
    10        0.2691             nan     0.3693   -0.0238
    20        0.1658             nan     0.3693   -0.0060
    40        0.1264             nan     0.3693    0.0015
    60        0.0908             nan     0.3693   -0.0034
    80        0.0707             nan     0.3693   -0.0055
   100        0.0653             nan     0.3693   -0.0080
   120        0.0497             nan     0.3693   -0.0031
   140        0.0452             nan     0.3693   -0.0018
   160        0.0421             nan     0.3693   -0.0022
   180        0.0341             nan     0.3693   -0.0040
   200        0.0310             nan     0.3693   -0.0052
   220        0.0255             nan     0.3693   -0.0023
   240        0.0220             nan     0.3693   -0.0023
   260        0.0185             nan     0.3693   -0.0033
   280        0.0172             nan     0.3693   -0.0013
   300        0.0143             nan     0.3693   -0.0012
   320        0.0126             nan     0.3693   -0.0017
   340        0.0109             nan     0.3693   -0.0012
   360        0.0088             nan     0.3693   -0.0011
   380        0.0077             nan     0.3693   -0.0007
   400        0.0071             nan     0.3693   -0.0011
   420        0.0059             nan     0.3693   -0.0010
   440        0.0049             nan     0.3693   -0.0007
   460        0.0041             nan     0.3693   -0.0005
   480        0.0038             nan     0.3693   -0.0004
   500        0.0035             nan     0.3693   -0.0002
   520        0.0030             nan     0.3693    0.0000
   540        0.0026             nan     0.3693   -0.0001
   560        0.0023             nan     0.3693   -0.0001
   580        0.0020             nan     0.3693   -0.0004
   600        0.0019             nan     0.3693   -0.0002
   620        0.0018             nan     0.3693   -0.0002
   640        0.0016             nan     0.3693   -0.0002
   660        0.0014             nan     0.3693    0.0000
   680        0.0013             nan     0.3693   -0.0002
   700        0.0010             nan     0.3693   -0.0001
   720        0.0009             nan     0.3693   -0.0001
   740        0.0008             nan     0.3693   -0.0000
   760        0.0007             nan     0.3693   -0.0002
   780        0.0006             nan     0.3693   -0.0000
   800        0.0005             nan     0.3693   -0.0000
   820        0.0005             nan     0.3693   -0.0000
   840        0.0005             nan     0.3693   -0.0001
   860        0.0004             nan     0.3693   -0.0000
   880        0.0004             nan     0.3693   -0.0000
   900        0.0003             nan     0.3693   -0.0000
   920        0.0003             nan     0.3693   -0.0000
   940        0.0003             nan     0.3693   -0.0000
   960        0.0003             nan     0.3693   -0.0000
   980        0.0002             nan     0.3693   -0.0000
  1000        0.0002             nan     0.3693   -0.0000
  1020        0.0002             nan     0.3693   -0.0000
  1040        0.0002             nan     0.3693   -0.0000
  1060        0.0002             nan     0.3693   -0.0000
  1080        0.0001             nan     0.3693   -0.0000
  1100        0.0001             nan     0.3693   -0.0000
  1120        0.0001             nan     0.3693   -0.0000
  1140        0.0001             nan     0.3693   -0.0000
  1160        0.0001             nan     0.3693   -0.0000
  1180        0.0001             nan     0.3693   -0.0000
  1200        0.0001             nan     0.3693   -0.0000
  1220        0.0001             nan     0.3693    0.0000
  1240        0.0001             nan     0.3693   -0.0000
  1260        0.0000             nan     0.3693   -0.0000
  1280        0.0000             nan     0.3693   -0.0000
  1300        0.0000             nan     0.3693   -0.0000
  1320        0.0000             nan     0.3693   -0.0000
  1340        0.0000             nan     0.3693   -0.0000
  1360        0.0000             nan     0.3693   -0.0000
  1380        0.0000             nan     0.3693   -0.0000
  1400        0.0000             nan     0.3693   -0.0000
  1420        0.0000             nan     0.3693   -0.0000
  1440        0.0000             nan     0.3693   -0.0000
  1460        0.0000             nan     0.3693   -0.0000
  1480        0.0000             nan     0.3693   -0.0000
  1500        0.0000             nan     0.3693   -0.0000
  1520        0.0000             nan     0.3693   -0.0000
  1540        0.0000             nan     0.3693   -0.0000
  1560        0.0000             nan     0.3693   -0.0000
  1580        0.0000             nan     0.3693   -0.0000
  1600        0.0000             nan     0.3693   -0.0000
  1620        0.0000             nan     0.3693   -0.0000
  1640        0.0000             nan     0.3693   -0.0000
  1660        0.0000             nan     0.3693   -0.0000
  1680        0.0000             nan     0.3693   -0.0000
  1700        0.0000             nan     0.3693   -0.0000
  1720        0.0000             nan     0.3693   -0.0000
  1740        0.0000             nan     0.3693   -0.0000
  1760        0.0000             nan     0.3693   -0.0000
  1780        0.0000             nan     0.3693    0.0000
  1800        0.0000             nan     0.3693   -0.0000
  1820        0.0000             nan     0.3693   -0.0000
  1840        0.0000             nan     0.3693   -0.0000
  1860        0.0000             nan     0.3693   -0.0000
  1880        0.0000             nan     0.3693   -0.0000
  1900        0.0000             nan     0.3693   -0.0000
  1920        0.0000             nan     0.3693   -0.0000
  1940        0.0000             nan     0.3693   -0.0000
  1960        0.0000             nan     0.3693   -0.0000
  1980        0.0000             nan     0.3693   -0.0000
  2000        0.0000             nan     0.3693    0.0000
  2020        0.0000             nan     0.3693   -0.0000
  2040        0.0000             nan     0.3693   -0.0000
  2060        0.0000             nan     0.3693   -0.0000
  2080        0.0000             nan     0.3693   -0.0000
  2100        0.0000             nan     0.3693   -0.0000
  2120        0.0000             nan     0.3693   -0.0000
  2140        0.0000             nan     0.3693   -0.0000
  2160        0.0000             nan     0.3693   -0.0000
  2180        0.0000             nan     0.3693   -0.0000
  2200        0.0000             nan     0.3693   -0.0000
  2220        0.0000             nan     0.3693   -0.0000
  2240        0.0000             nan     0.3693   -0.0000
  2260        0.0000             nan     0.3693   -0.0000
  2280        0.0000             nan     0.3693   -0.0000
  2300        0.0000             nan     0.3693   -0.0000
  2320        0.0000             nan     0.3693   -0.0000
  2340        0.0000             nan     0.3693   -0.0000
  2360        0.0000             nan     0.3693   -0.0000
  2380        0.0000             nan     0.3693   -0.0000
  2400        0.0000             nan     0.3693   -0.0000
  2420        0.0000             nan     0.3693   -0.0000
  2440        0.0000             nan     0.3693   -0.0000
  2460        0.0000             nan     0.3693   -0.0000
  2480        0.0000             nan     0.3693   -0.0000
  2500        0.0000             nan     0.3693   -0.0000
  2520        0.0000             nan     0.3693   -0.0000
  2540        0.0000             nan     0.3693   -0.0000
  2560        0.0000             nan     0.3693   -0.0000
  2580        0.0000             nan     0.3693   -0.0000
  2600        0.0000             nan     0.3693    0.0000
  2620        0.0000             nan     0.3693   -0.0000
  2640        0.0000             nan     0.3693   -0.0000
  2660        0.0000             nan     0.3693   -0.0000
  2680        0.0000             nan     0.3693   -0.0000
  2700        0.0000             nan     0.3693   -0.0000
  2720        0.0000             nan     0.3693   -0.0000
  2740        0.0000             nan     0.3693   -0.0000
  2760        0.0000             nan     0.3693   -0.0000
  2780        0.0000             nan     0.3693   -0.0000
  2800        0.0000             nan     0.3693   -0.0000
  2820        0.0000             nan     0.3693   -0.0000
  2840        0.0000             nan     0.3693   -0.0000
  2860        0.0000             nan     0.3693   -0.0000
  2880        0.0000             nan     0.3693   -0.0000
  2900        0.0000             nan     0.3693   -0.0000
  2920        0.0000             nan     0.3693   -0.0000
  2940        0.0000             nan     0.3693   -0.0000
  2960        0.0000             nan     0.3693   -0.0000
  2980        0.0000             nan     0.3693   -0.0000
  3000        0.0000             nan     0.3693   -0.0000
  3020        0.0000             nan     0.3693   -0.0000
  3040        0.0000             nan     0.3693   -0.0000
  3060        0.0000             nan     0.3693   -0.0000
  3080        0.0000             nan     0.3693   -0.0000
  3100        0.0000             nan     0.3693   -0.0000
  3120        0.0000             nan     0.3693   -0.0000
  3140        0.0000             nan     0.3693   -0.0000
  3160        0.0000             nan     0.3693   -0.0000
  3180        0.0000             nan     0.3693   -0.0000
  3200        0.0000             nan     0.3693   -0.0000
  3220        0.0000             nan     0.3693   -0.0000
  3240        0.0000             nan     0.3693   -0.0000
  3260        0.0000             nan     0.3693   -0.0000
  3280        0.0000             nan     0.3693   -0.0000
  3300        0.0000             nan     0.3693   -0.0000
  3320        0.0000             nan     0.3693   -0.0000
  3340        0.0000             nan     0.3693   -0.0000
  3360        0.0000             nan     0.3693   -0.0000
  3380        0.0000             nan     0.3693    0.0000
  3400        0.0000             nan     0.3693   -0.0000
  3420        0.0000             nan     0.3693   -0.0000
  3440        0.0000             nan     0.3693   -0.0000
  3460        0.0000             nan     0.3693   -0.0000
  3480        0.0000             nan     0.3693   -0.0000
  3500        0.0000             nan     0.3693   -0.0000
  3516        0.0000             nan     0.3693   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5722             nan     0.4512    0.3772
     2        0.4778             nan     0.4512    0.1066
     3        0.4521             nan     0.4512    0.0115
     4        0.3704             nan     0.4512    0.0054
     5        0.3171             nan     0.4512   -0.0229
     6        0.3072             nan     0.4512   -0.0126
     7        0.2980             nan     0.4512   -0.0448
     8        0.2707             nan     0.4512   -0.0316
     9        0.2375             nan     0.4512   -0.0044
    10        0.2291             nan     0.4512   -0.0039
    20        0.1490             nan     0.4512   -0.0049
    40        0.0743             nan     0.4512   -0.0093
    60        0.0407             nan     0.4512   -0.0042
    80        0.0272             nan     0.4512   -0.0007
   100        0.0165             nan     0.4512   -0.0012
   120        0.0088             nan     0.4512   -0.0005
   140        0.0069             nan     0.4512   -0.0007
   160        0.0041             nan     0.4512   -0.0008
   180        0.0026             nan     0.4512   -0.0002
   200        0.0016             nan     0.4512   -0.0001
   220        0.0011             nan     0.4512   -0.0001
   240        0.0008             nan     0.4512   -0.0001
   260        0.0004             nan     0.4512   -0.0001
   280        0.0003             nan     0.4512   -0.0000
   300        0.0002             nan     0.4512   -0.0000
   320        0.0001             nan     0.4512   -0.0000
   340        0.0001             nan     0.4512   -0.0000
   360        0.0000             nan     0.4512   -0.0000
   380        0.0000             nan     0.4512   -0.0000
   400        0.0000             nan     0.4512   -0.0000
   420        0.0000             nan     0.4512   -0.0000
   440        0.0000             nan     0.4512   -0.0000
   460        0.0000             nan     0.4512   -0.0000
   480        0.0000             nan     0.4512   -0.0000
   500        0.0000             nan     0.4512   -0.0000
   520        0.0000             nan     0.4512   -0.0000
   540        0.0000             nan     0.4512   -0.0000
   560        0.0000             nan     0.4512   -0.0000
   580        0.0000             nan     0.4512   -0.0000
   600        0.0000             nan     0.4512   -0.0000
   620        0.0000             nan     0.4512   -0.0000
   640        0.0000             nan     0.4512   -0.0000
   660        0.0000             nan     0.4512   -0.0000
   680        0.0000             nan     0.4512    0.0000
   700        0.0000             nan     0.4512   -0.0000
   720        0.0000             nan     0.4512   -0.0000
   740        0.0000             nan     0.4512   -0.0000
   760        0.0000             nan     0.4512   -0.0000
   780        0.0000             nan     0.4512   -0.0000
   800        0.0000             nan     0.4512   -0.0000
   820        0.0000             nan     0.4512   -0.0000
   840        0.0000             nan     0.4512   -0.0000
   860        0.0000             nan     0.4512   -0.0000
   880        0.0000             nan     0.4512   -0.0000
   900        0.0000             nan     0.4512   -0.0000
   920        0.0000             nan     0.4512   -0.0000
   940        0.0000             nan     0.4512   -0.0000
   960        0.0000             nan     0.4512   -0.0000
   980        0.0000             nan     0.4512   -0.0000
  1000        0.0000             nan     0.4512   -0.0000
  1020        0.0000             nan     0.4512   -0.0000
  1040        0.0000             nan     0.4512   -0.0000
  1060        0.0000             nan     0.4512   -0.0000
  1080        0.0000             nan     0.4512   -0.0000
  1100        0.0000             nan     0.4512   -0.0000
  1120        0.0000             nan     0.4512   -0.0000
  1140        0.0000             nan     0.4512   -0.0000
  1160        0.0000             nan     0.4512   -0.0000
  1180        0.0000             nan     0.4512   -0.0000
  1200        0.0000             nan     0.4512   -0.0000
  1220        0.0000             nan     0.4512   -0.0000
  1240        0.0000             nan     0.4512   -0.0000
  1260        0.0000             nan     0.4512    0.0000
  1280        0.0000             nan     0.4512   -0.0000
  1300        0.0000             nan     0.4512   -0.0000
  1320        0.0000             nan     0.4512    0.0000
  1340        0.0000             nan     0.4512   -0.0000
  1360        0.0000             nan     0.4512   -0.0000
  1380        0.0000             nan     0.4512   -0.0000
  1400        0.0000             nan     0.4512   -0.0000
  1420        0.0000             nan     0.4512   -0.0000
  1440        0.0000             nan     0.4512   -0.0000
  1460        0.0000             nan     0.4512   -0.0000
  1480        0.0000             nan     0.4512   -0.0000
  1500        0.0000             nan     0.4512   -0.0000
  1520        0.0000             nan     0.4512   -0.0000
  1540        0.0000             nan     0.4512   -0.0000
  1560        0.0000             nan     0.4512   -0.0000
  1580        0.0000             nan     0.4512   -0.0000
  1600        0.0000             nan     0.4512   -0.0000
  1620        0.0000             nan     0.4512   -0.0000
  1640        0.0000             nan     0.4512   -0.0000
  1660        0.0000             nan     0.4512    0.0000
  1680        0.0000             nan     0.4512   -0.0000
  1700        0.0000             nan     0.4512   -0.0000
  1720        0.0000             nan     0.4512   -0.0000
  1740        0.0000             nan     0.4512   -0.0000
  1760        0.0000             nan     0.4512   -0.0000
  1780        0.0000             nan     0.4512   -0.0000
  1800        0.0000             nan     0.4512   -0.0000
  1820        0.0000             nan     0.4512   -0.0000
  1840        0.0000             nan     0.4512   -0.0000
  1860        0.0000             nan     0.4512   -0.0000
  1880        0.0000             nan     0.4512   -0.0000
  1900        0.0000             nan     0.4512   -0.0000
  1920        0.0000             nan     0.4512    0.0000
  1940        0.0000             nan     0.4512   -0.0000
  1960        0.0000             nan     0.4512   -0.0000
  1980        0.0000             nan     0.4512   -0.0000
  2000        0.0000             nan     0.4512    0.0000
  2020        0.0000             nan     0.4512   -0.0000
  2040        0.0000             nan     0.4512   -0.0000
  2060        0.0000             nan     0.4512   -0.0000
  2080        0.0000             nan     0.4512   -0.0000
  2100        0.0000             nan     0.4512   -0.0000
  2120        0.0000             nan     0.4512   -0.0000
  2140        0.0000             nan     0.4512   -0.0000
  2160        0.0000             nan     0.4512   -0.0000
  2180        0.0000             nan     0.4512   -0.0000
  2200        0.0000             nan     0.4512   -0.0000
  2220        0.0000             nan     0.4512   -0.0000
  2240        0.0000             nan     0.4512   -0.0000
  2260        0.0000             nan     0.4512   -0.0000
  2280        0.0000             nan     0.4512   -0.0000
  2300        0.0000             nan     0.4512   -0.0000
  2320        0.0000             nan     0.4512   -0.0000
  2340        0.0000             nan     0.4512   -0.0000
  2360        0.0000             nan     0.4512   -0.0000
  2380        0.0000             nan     0.4512   -0.0000
  2400        0.0000             nan     0.4512   -0.0000
  2420        0.0000             nan     0.4512   -0.0000
  2440        0.0000             nan     0.4512   -0.0000
  2460        0.0000             nan     0.4512   -0.0000
  2480        0.0000             nan     0.4512   -0.0000
  2500        0.0000             nan     0.4512   -0.0000
  2520        0.0000             nan     0.4512   -0.0000
  2540        0.0000             nan     0.4512   -0.0000
  2560        0.0000             nan     0.4512   -0.0000
  2580        0.0000             nan     0.4512   -0.0000
  2600        0.0000             nan     0.4512   -0.0000
  2620        0.0000             nan     0.4512   -0.0000
  2640        0.0000             nan     0.4512   -0.0000
  2660        0.0000             nan     0.4512   -0.0000
  2680        0.0000             nan     0.4512   -0.0000
  2700        0.0000             nan     0.4512   -0.0000
  2720        0.0000             nan     0.4512   -0.0000
  2740        0.0000             nan     0.4512   -0.0000
  2760        0.0000             nan     0.4512   -0.0000
  2780        0.0000             nan     0.4512    0.0000
  2800        0.0000             nan     0.4512   -0.0000
  2820        0.0000             nan     0.4512   -0.0000
  2840        0.0000             nan     0.4512   -0.0000
  2860        0.0000             nan     0.4512   -0.0000
  2880        0.0000             nan     0.4512   -0.0000
  2900        0.0000             nan     0.4512   -0.0000
  2920        0.0000             nan     0.4512   -0.0000
  2940        0.0000             nan     0.4512   -0.0000
  2960        0.0000             nan     0.4512   -0.0000
  2980        0.0000             nan     0.4512    0.0000
  3000        0.0000             nan     0.4512   -0.0000
  3020        0.0000             nan     0.4512   -0.0000
  3040        0.0000             nan     0.4512   -0.0000
  3060        0.0000             nan     0.4512   -0.0000
  3080        0.0000             nan     0.4512   -0.0000
  3100        0.0000             nan     0.4512   -0.0000
  3120        0.0000             nan     0.4512   -0.0000
  3140        0.0000             nan     0.4512   -0.0000
  3160        0.0000             nan     0.4512   -0.0000
  3180        0.0000             nan     0.4512   -0.0000
  3200        0.0000             nan     0.4512   -0.0000
  3220        0.0000             nan     0.4512   -0.0000
  3240        0.0000             nan     0.4512   -0.0000
  3260        0.0000             nan     0.4512   -0.0000
  3280        0.0000             nan     0.4512   -0.0000
  3300        0.0000             nan     0.4512   -0.0000
  3320        0.0000             nan     0.4512   -0.0000
  3340        0.0000             nan     0.4512   -0.0000
  3360        0.0000             nan     0.4512    0.0000
  3380        0.0000             nan     0.4512   -0.0000
  3400        0.0000             nan     0.4512   -0.0000
  3420        0.0000             nan     0.4512   -0.0000
  3427        0.0000             nan     0.4512   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.4234             nan     0.5480    0.5368
     2        0.1955             nan     0.5480    0.1165
     3        0.1528             nan     0.5480    0.0176
     4        0.1560             nan     0.5480   -0.0407
     5        0.1285             nan     0.5480   -0.0384
     6        0.1083             nan     0.5480   -0.0097
     7        0.0909             nan     0.5480   -0.0154
     8        0.0878             nan     0.5480   -0.0181
     9        0.0764             nan     0.5480   -0.0151
    10        0.0716             nan     0.5480   -0.0120
    20        0.0312             nan     0.5480   -0.0031
    40        0.0051             nan     0.5480   -0.0015
    60        0.0009             nan     0.5480   -0.0003
    80        0.0005             nan     0.5480   -0.0001
   100        0.0001             nan     0.5480   -0.0001
   120        0.0000             nan     0.5480   -0.0000
   140        0.0000             nan     0.5480   -0.0000
   160        0.0000             nan     0.5480   -0.0000
   180        0.0000             nan     0.5480   -0.0000
   200        0.0000             nan     0.5480   -0.0000
   220        0.0000             nan     0.5480    0.0000
   240        0.0000             nan     0.5480    0.0000
   260        0.0000             nan     0.5480   -0.0000
   280        0.0000             nan     0.5480   -0.0000
   300        0.0000             nan     0.5480   -0.0000
   320        0.0000             nan     0.5480   -0.0000
   340        0.0000             nan     0.5480   -0.0000
   360        0.0000             nan     0.5480   -0.0000
   380        0.0000             nan     0.5480   -0.0000
   400        0.0000             nan     0.5480   -0.0000
   420        0.0000             nan     0.5480   -0.0000
   440        0.0000             nan     0.5480   -0.0000
   460        0.0000             nan     0.5480   -0.0000
   480        0.0000             nan     0.5480    0.0000
   500        0.0000             nan     0.5480   -0.0000
   520        0.0000             nan     0.5480   -0.0000
   540        0.0000             nan     0.5480   -0.0000
   560        0.0000             nan     0.5480   -0.0000
   580        0.0000             nan     0.5480   -0.0000
   600        0.0000             nan     0.5480    0.0000
   620        0.0000             nan     0.5480   -0.0000
   640        0.0000             nan     0.5480   -0.0000
   660        0.0000             nan     0.5480   -0.0000
   680        0.0000             nan     0.5480   -0.0000
   700        0.0000             nan     0.5480    0.0000
   720        0.0000             nan     0.5480   -0.0000
   740        0.0000             nan     0.5480    0.0000
   760        0.0000             nan     0.5480   -0.0000
   780        0.0000             nan     0.5480   -0.0000
   800        0.0000             nan     0.5480   -0.0000
   820        0.0000             nan     0.5480   -0.0000
   840        0.0000             nan     0.5480   -0.0000
   860        0.0000             nan     0.5480   -0.0000
   880        0.0000             nan     0.5480   -0.0000
   900        0.0000             nan     0.5480   -0.0000
   920        0.0000             nan     0.5480   -0.0000
   940        0.0000             nan     0.5480   -0.0000
   960        0.0000             nan     0.5480   -0.0000
   980        0.0000             nan     0.5480   -0.0000
  1000        0.0000             nan     0.5480   -0.0000
  1020        0.0000             nan     0.5480   -0.0000
  1040        0.0000             nan     0.5480   -0.0000
  1060        0.0000             nan     0.5480   -0.0000
  1080        0.0000             nan     0.5480   -0.0000
  1100        0.0000             nan     0.5480   -0.0000
  1120        0.0000             nan     0.5480   -0.0000
  1140        0.0000             nan     0.5480   -0.0000
  1160        0.0000             nan     0.5480   -0.0000
  1180        0.0000             nan     0.5480   -0.0000
  1200        0.0000             nan     0.5480   -0.0000
  1220        0.0000             nan     0.5480   -0.0000
  1240        0.0000             nan     0.5480   -0.0000
  1260        0.0000             nan     0.5480   -0.0000
  1280        0.0000             nan     0.5480   -0.0000
  1300        0.0000             nan     0.5480   -0.0000
  1320        0.0000             nan     0.5480   -0.0000
  1340        0.0000             nan     0.5480   -0.0000
  1360        0.0000             nan     0.5480   -0.0000
  1380        0.0000             nan     0.5480   -0.0000
  1400        0.0000             nan     0.5480   -0.0000
  1420        0.0000             nan     0.5480   -0.0000
  1440        0.0000             nan     0.5480   -0.0000
  1460        0.0000             nan     0.5480   -0.0000
  1480        0.0000             nan     0.5480    0.0000
  1500        0.0000             nan     0.5480   -0.0000
  1520        0.0000             nan     0.5480   -0.0000
  1540        0.0000             nan     0.5480   -0.0000
  1560        0.0000             nan     0.5480   -0.0000
  1580        0.0000             nan     0.5480   -0.0000
  1600        0.0000             nan     0.5480   -0.0000
  1620        0.0000             nan     0.5480   -0.0000
  1640        0.0000             nan     0.5480   -0.0000
  1660        0.0000             nan     0.5480   -0.0000
  1680        0.0000             nan     0.5480   -0.0000
  1700        0.0000             nan     0.5480   -0.0000
  1720        0.0000             nan     0.5480   -0.0000
  1740        0.0000             nan     0.5480   -0.0000
  1760        0.0000             nan     0.5480   -0.0000
  1780        0.0000             nan     0.5480   -0.0000
  1800        0.0000             nan     0.5480   -0.0000
  1820        0.0000             nan     0.5480   -0.0000
  1840        0.0000             nan     0.5480   -0.0000
  1860        0.0000             nan     0.5480   -0.0000
  1880        0.0000             nan     0.5480   -0.0000
  1900        0.0000             nan     0.5480   -0.0000
  1920        0.0000             nan     0.5480   -0.0000
  1940        0.0000             nan     0.5480   -0.0000
  1960        0.0000             nan     0.5480   -0.0000
  1980        0.0000             nan     0.5480   -0.0000
  2000        0.0000             nan     0.5480   -0.0000
  2020        0.0000             nan     0.5480   -0.0000
  2040        0.0000             nan     0.5480   -0.0000
  2060        0.0000             nan     0.5480   -0.0000
  2080        0.0000             nan     0.5480   -0.0000
  2100        0.0000             nan     0.5480   -0.0000
  2120        0.0000             nan     0.5480   -0.0000
  2140        0.0000             nan     0.5480   -0.0000
  2160        0.0000             nan     0.5480   -0.0000
  2180        0.0000             nan     0.5480   -0.0000
  2200        0.0000             nan     0.5480   -0.0000
  2220        0.0000             nan     0.5480   -0.0000
  2240        0.0000             nan     0.5480   -0.0000
  2260        0.0000             nan     0.5480   -0.0000
  2280        0.0000             nan     0.5480   -0.0000
  2300        0.0000             nan     0.5480   -0.0000
  2320        0.0000             nan     0.5480   -0.0000
  2340        0.0000             nan     0.5480   -0.0000
  2360        0.0000             nan     0.5480   -0.0000
  2380        0.0000             nan     0.5480   -0.0000
  2400        0.0000             nan     0.5480   -0.0000
  2420        0.0000             nan     0.5480   -0.0000
  2440        0.0000             nan     0.5480   -0.0000
  2460        0.0000             nan     0.5480    0.0000
  2480        0.0000             nan     0.5480   -0.0000
  2500        0.0000             nan     0.5480   -0.0000
  2520        0.0000             nan     0.5480   -0.0000
  2540        0.0000             nan     0.5480    0.0000
  2560        0.0000             nan     0.5480   -0.0000
  2580        0.0000             nan     0.5480   -0.0000
  2600        0.0000             nan     0.5480   -0.0000
  2620        0.0000             nan     0.5480   -0.0000
  2640        0.0000             nan     0.5480   -0.0000
  2660        0.0000             nan     0.5480   -0.0000
  2680        0.0000             nan     0.5480   -0.0000
  2700        0.0000             nan     0.5480   -0.0000
  2720        0.0000             nan     0.5480   -0.0000
  2740        0.0000             nan     0.5480   -0.0000
  2760        0.0000             nan     0.5480   -0.0000
  2780        0.0000             nan     0.5480    0.0000
  2800        0.0000             nan     0.5480   -0.0000
  2820        0.0000             nan     0.5480   -0.0000
  2840        0.0000             nan     0.5480   -0.0000
  2860        0.0000             nan     0.5480   -0.0000
  2880        0.0000             nan     0.5480   -0.0000
  2900        0.0000             nan     0.5480   -0.0000
  2920        0.0000             nan     0.5480   -0.0000
  2940        0.0000             nan     0.5480   -0.0000
  2960        0.0000             nan     0.5480   -0.0000
  2980        0.0000             nan     0.5480   -0.0000
  3000        0.0000             nan     0.5480   -0.0000
  3020        0.0000             nan     0.5480   -0.0000
  3040        0.0000             nan     0.5480   -0.0000
  3060        0.0000             nan     0.5480   -0.0000
  3080        0.0000             nan     0.5480   -0.0000
  3100        0.0000             nan     0.5480   -0.0000
  3120        0.0000             nan     0.5480   -0.0000
  3140        0.0000             nan     0.5480   -0.0000
  3160        0.0000             nan     0.5480   -0.0000
  3180        0.0000             nan     0.5480   -0.0000
  3200        0.0000             nan     0.5480   -0.0000
  3220        0.0000             nan     0.5480   -0.0000
  3240        0.0000             nan     0.5480   -0.0000
  3260        0.0000             nan     0.5480   -0.0000
  3280        0.0000             nan     0.5480   -0.0000
  3300        0.0000             nan     0.5480   -0.0000
  3320        0.0000             nan     0.5480   -0.0000
  3340        0.0000             nan     0.5480   -0.0000
  3360        0.0000             nan     0.5480   -0.0000
  3380        0.0000             nan     0.5480   -0.0000
  3400        0.0000             nan     0.5480   -0.0000
  3420        0.0000             nan     0.5480   -0.0000
  3440        0.0000             nan     0.5480   -0.0000
  3460        0.0000             nan     0.5480   -0.0000
  3480        0.0000             nan     0.5480   -0.0000
  3500        0.0000             nan     0.5480   -0.0000
  3520        0.0000             nan     0.5480   -0.0000
  3540        0.0000             nan     0.5480   -0.0000
  3560        0.0000             nan     0.5480   -0.0000
  3580        0.0000             nan     0.5480   -0.0000
  3600        0.0000             nan     0.5480   -0.0000
  3620        0.0000             nan     0.5480   -0.0000
  3640        0.0000             nan     0.5480   -0.0000
  3660        0.0000             nan     0.5480   -0.0000
  3680        0.0000             nan     0.5480   -0.0000
  3700        0.0000             nan     0.5480   -0.0000
  3720        0.0000             nan     0.5480   -0.0000
  3740        0.0000             nan     0.5480   -0.0000
  3760        0.0000             nan     0.5480   -0.0000
  3780        0.0000             nan     0.5480   -0.0000
  3800        0.0000             nan     0.5480   -0.0000
  3820        0.0000             nan     0.5480   -0.0000
  3840        0.0000             nan     0.5480   -0.0000
  3860        0.0000             nan     0.5480   -0.0000
  3880        0.0000             nan     0.5480   -0.0000
  3900        0.0000             nan     0.5480    0.0000
  3920        0.0000             nan     0.5480   -0.0000
  3940        0.0000             nan     0.5480   -0.0000
  3955        0.0000             nan     0.5480   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.6750             nan     0.5705    0.2800
     2        0.5465             nan     0.5705    0.1098
     3        0.5499             nan     0.5705   -0.1003
     4        0.5652             nan     0.5705   -0.1869
     5        0.4709             nan     0.5705   -0.0200
     6        0.4347             nan     0.5705   -0.0332
     7        0.4073             nan     0.5705   -0.0084
     8        0.4054             nan     0.5705   -0.0942
     9        0.4095             nan     0.5705   -0.0738
    10        0.4290             nan     0.5705   -0.0862
    20        0.2643             nan     0.5705   -0.0143
    40        0.2373             nan     0.5705   -0.0865
    60        0.1398             nan     0.5705   -0.0240
    80        0.0968             nan     0.5705   -0.0069
   100        0.0692             nan     0.5705   -0.0083
   120        0.0596             nan     0.5705   -0.0178
   140        0.0362             nan     0.5705   -0.0080
   160        0.0304             nan     0.5705   -0.0163
   180        0.0244             nan     0.5705   -0.0042
   200        0.0206             nan     0.5705   -0.0021
   220        0.0150             nan     0.5705   -0.0018
   240        0.0142             nan     0.5705   -0.0025
   260        0.0119             nan     0.5705   -0.0016
   280        0.0089             nan     0.5705   -0.0007
   300        0.0076             nan     0.5705   -0.0010
   320        0.0078             nan     0.5705   -0.0016
   340        0.0068             nan     0.5705   -0.0001
   360        0.0070             nan     0.5705   -0.0023
   380        0.0058             nan     0.5705   -0.0000
   400        0.0053             nan     0.5705   -0.0004
   420        0.0042             nan     0.5705   -0.0003
   440        0.0034             nan     0.5705   -0.0003
   460        0.0032             nan     0.5705   -0.0005
   480        0.0030             nan     0.5705   -0.0004
   500        0.0027             nan     0.5705   -0.0003
   520        0.0025             nan     0.5705   -0.0002
   540        0.0022             nan     0.5705   -0.0005
   560        0.0019             nan     0.5705   -0.0004
   580        0.0017             nan     0.5705   -0.0001
   600        0.0017             nan     0.5705   -0.0004
   620        0.0013             nan     0.5705   -0.0001
   640        0.0012             nan     0.5705   -0.0002
   660        0.0011             nan     0.5705   -0.0000
   680        0.0009             nan     0.5705   -0.0001
   700        0.0009             nan     0.5705   -0.0001
   720        0.0008             nan     0.5705   -0.0000
   740        0.0007             nan     0.5705   -0.0000
   760        0.0006             nan     0.5705   -0.0001
   780        0.0006             nan     0.5705   -0.0001
   800        0.0005             nan     0.5705   -0.0001
   820        0.0005             nan     0.5705   -0.0000
   840        0.0005             nan     0.5705   -0.0002
   860        0.0004             nan     0.5705   -0.0001
   880        0.0003             nan     0.5705   -0.0001
   900        0.0003             nan     0.5705   -0.0001
   920        0.0003             nan     0.5705   -0.0000
   940        0.0003             nan     0.5705   -0.0001
   960        0.0003             nan     0.5705   -0.0001

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.6023             nan     0.0176    0.0070
     2        0.5942             nan     0.0176    0.0040
     3        0.5868             nan     0.0176    0.0067
     4        0.5775             nan     0.0176    0.0065
     5        0.5678             nan     0.0176    0.0054
     6        0.5580             nan     0.0176    0.0035
     7        0.5496             nan     0.0176    0.0045
     8        0.5411             nan     0.0176    0.0038
     9        0.5333             nan     0.0176    0.0060
    10        0.5269             nan     0.0176    0.0068
    20        0.4679             nan     0.0176    0.0052
    40        0.3845             nan     0.0176    0.0029
    60        0.3413             nan     0.0176   -0.0018
    80        0.3102             nan     0.0176    0.0007
   100        0.2898             nan     0.0176   -0.0011
   120        0.2724             nan     0.0176   -0.0000
   140        0.2569             nan     0.0176   -0.0014
   160        0.2458             nan     0.0176   -0.0007
   180        0.2370             nan     0.0176    0.0005
   200        0.2270             nan     0.0176   -0.0012
   220        0.2174             nan     0.0176   -0.0006
   240        0.2104             nan     0.0176   -0.0014
   260        0.2059             nan     0.0176   -0.0007
   280        0.1993             nan     0.0176   -0.0007
   300        0.1965             nan     0.0176    0.0003
   320        0.1934             nan     0.0176   -0.0008
   340        0.1886             nan     0.0176   -0.0006
   360        0.1842             nan     0.0176   -0.0005
   380        0.1809             nan     0.0176   -0.0006
   400        0.1795             nan     0.0176   -0.0010
   420        0.1782             nan     0.0176   -0.0008
   440        0.1742             nan     0.0176   -0.0003
   460        0.1715             nan     0.0176   -0.0003
   480        0.1684             nan     0.0176   -0.0005
   500        0.1659             nan     0.0176   -0.0008
   520        0.1628             nan     0.0176   -0.0010
   540        0.1600             nan     0.0176   -0.0005
   560        0.1579             nan     0.0176   -0.0008
   580        0.1550             nan     0.0176   -0.0010
   600        0.1535             nan     0.0176   -0.0004
   620        0.1509             nan     0.0176   -0.0009
   640        0.1491             nan     0.0176   -0.0004
   660        0.1466             nan     0.0176    0.0001
   680        0.1447             nan     0.0176   -0.0006
   700        0.1428             nan     0.0176   -0.0005
   720        0.1410             nan     0.0176   -0.0003
   740        0.1389             nan     0.0176   -0.0006
   760        0.1363             nan     0.0176   -0.0011
   780        0.1346             nan     0.0176   -0.0007
   800        0.1337             nan     0.0176   -0.0005
   820        0.1321             nan     0.0176   -0.0003
   840        0.1307             nan     0.0176   -0.0007
   860        0.1290             nan     0.0176   -0.0001
   880        0.1273             nan     0.0176   -0.0004
   900        0.1256             nan     0.0176   -0.0006
   920        0.1244             nan     0.0176   -0.0002
   940        0.1228             nan     0.0176   -0.0002
   960        0.1210             nan     0.0176   -0.0004
   980        0.1199             nan     0.0176   -0.0003
  1000        0.1185             nan     0.0176   -0.0004
  1020        0.1171             nan     0.0176   -0.0004
  1040        0.1155             nan     0.0176   -0.0006
  1060        0.1143             nan     0.0176   -0.0003
  1080        0.1129             nan     0.0176   -0.0004
  1100        0.1118             nan     0.0176   -0.0004
  1120        0.1104             nan     0.0176   -0.0001
  1140        0.1093             nan     0.0176   -0.0003
  1160        0.1078             nan     0.0176   -0.0002
  1180        0.1064             nan     0.0176   -0.0002
  1200        0.1052             nan     0.0176   -0.0003
  1220        0.1044             nan     0.0176   -0.0007
  1240        0.1032             nan     0.0176   -0.0008
  1260        0.1021             nan     0.0176   -0.0008
  1280        0.1008             nan     0.0176   -0.0005
  1300        0.0998             nan     0.0176   -0.0004
  1320        0.0986             nan     0.0176   -0.0002
  1340        0.0978             nan     0.0176   -0.0002
  1360        0.0967             nan     0.0176   -0.0006
  1380        0.0956             nan     0.0176   -0.0000
  1400        0.0946             nan     0.0176   -0.0003
  1420        0.0934             nan     0.0176   -0.0003
  1440        0.0924             nan     0.0176   -0.0005
  1460        0.0915             nan     0.0176   -0.0004
  1480        0.0906             nan     0.0176   -0.0003
  1500        0.0898             nan     0.0176   -0.0000
  1520        0.0887             nan     0.0176   -0.0004
  1540        0.0875             nan     0.0176   -0.0003
  1560        0.0866             nan     0.0176   -0.0003
  1580        0.0858             nan     0.0176   -0.0006
  1600        0.0852             nan     0.0176   -0.0008
  1620        0.0845             nan     0.0176   -0.0003
  1640        0.0838             nan     0.0176   -0.0004
  1660        0.0829             nan     0.0176   -0.0003
  1680        0.0819             nan     0.0176   -0.0002
  1700        0.0810             nan     0.0176   -0.0003
  1720        0.0803             nan     0.0176   -0.0002
  1740        0.0794             nan     0.0176   -0.0003
  1760        0.0787             nan     0.0176   -0.0003
  1780        0.0779             nan     0.0176   -0.0002
  1800        0.0766             nan     0.0176   -0.0001
  1820        0.0759             nan     0.0176   -0.0003
  1840        0.0750             nan     0.0176   -0.0002
  1860        0.0742             nan     0.0176   -0.0004
  1880        0.0735             nan     0.0176   -0.0002
  1900        0.0726             nan     0.0176   -0.0001
  1920        0.0719             nan     0.0176   -0.0002
  1940        0.0714             nan     0.0176   -0.0002
  1960        0.0704             nan     0.0176   -0.0002
  1980        0.0698             nan     0.0176   -0.0003
  2000        0.0689             nan     0.0176   -0.0005
  2020        0.0682             nan     0.0176   -0.0002
  2040        0.0675             nan     0.0176   -0.0001
  2060        0.0667             nan     0.0176   -0.0001
  2080        0.0660             nan     0.0176    0.0000
  2100        0.0654             nan     0.0176   -0.0002
  2120        0.0648             nan     0.0176   -0.0004
  2140        0.0642             nan     0.0176   -0.0003
  2160        0.0638             nan     0.0176   -0.0001
  2180        0.0632             nan     0.0176   -0.0002
  2200        0.0625             nan     0.0176   -0.0004
  2220        0.0619             nan     0.0176   -0.0001
  2240        0.0615             nan     0.0176   -0.0003
  2260        0.0610             nan     0.0176   -0.0001
  2280        0.0604             nan     0.0176   -0.0002
  2300        0.0597             nan     0.0176   -0.0002
  2320        0.0589             nan     0.0176   -0.0001
  2340        0.0584             nan     0.0176   -0.0003
  2360        0.0579             nan     0.0176   -0.0002
  2380        0.0576             nan     0.0176   -0.0002
  2400        0.0570             nan     0.0176   -0.0001
  2420        0.0566             nan     0.0176   -0.0003
  2440        0.0560             nan     0.0176   -0.0003
  2460        0.0555             nan     0.0176   -0.0002
  2480        0.0552             nan     0.0176   -0.0002
  2500        0.0546             nan     0.0176   -0.0005
  2520        0.0540             nan     0.0176   -0.0002
  2540        0.0535             nan     0.0176   -0.0004
  2560        0.0531             nan     0.0176   -0.0002
  2580        0.0527             nan     0.0176   -0.0003
  2600        0.0522             nan     0.0176   -0.0001
  2620        0.0515             nan     0.0176   -0.0005
  2640        0.0512             nan     0.0176   -0.0002
  2660        0.0505             nan     0.0176   -0.0001
  2680        0.0500             nan     0.0176   -0.0000
  2700        0.0497             nan     0.0176   -0.0001
  2720        0.0491             nan     0.0176   -0.0001
  2740        0.0485             nan     0.0176   -0.0002
  2760        0.0482             nan     0.0176   -0.0002
  2780        0.0477             nan     0.0176   -0.0001
  2800        0.0473             nan     0.0176   -0.0002
  2820        0.0469             nan     0.0176   -0.0003
  2840        0.0463             nan     0.0176   -0.0001
  2860        0.0460             nan     0.0176   -0.0001
  2880        0.0455             nan     0.0176   -0.0001
  2900        0.0451             nan     0.0176   -0.0001
  2920        0.0447             nan     0.0176   -0.0002
  2940        0.0444             nan     0.0176   -0.0003
  2960        0.0437             nan     0.0176   -0.0001
  2980        0.0433             nan     0.0176   -0.0003
  3000        0.0427             nan     0.0176    0.0000
  3020        0.0423             nan     0.0176   -0.0002
  3040        0.0420             nan     0.0176    0.0000
  3060        0.0416             nan     0.0176   -0.0001
  3080        0.0412             nan     0.0176   -0.0003
  3100        0.0409             nan     0.0176   -0.0002
  3120        0.0405             nan     0.0176   -0.0003
  3140        0.0402             nan     0.0176   -0.0001
  3160        0.0399             nan     0.0176   -0.0003
  3180        0.0396             nan     0.0176   -0.0002
  3200        0.0391             nan     0.0176   -0.0003
  3220        0.0389             nan     0.0176   -0.0001
  3240        0.0385             nan     0.0176   -0.0001
  3260        0.0382             nan     0.0176   -0.0002
  3280        0.0377             nan     0.0176   -0.0002
  3300        0.0374             nan     0.0176   -0.0001
  3320        0.0371             nan     0.0176   -0.0001
  3340        0.0367             nan     0.0176   -0.0001
  3360        0.0364             nan     0.0176   -0.0001
  3380        0.0361             nan     0.0176   -0.0002
  3400        0.0358             nan     0.0176   -0.0001
  3420        0.0355             nan     0.0176   -0.0003
  3440        0.0352             nan     0.0176   -0.0001
  3460        0.0349             nan     0.0176   -0.0001
  3480        0.0346             nan     0.0176   -0.0002
  3500        0.0343             nan     0.0176   -0.0001
  3520        0.0340             nan     0.0176   -0.0001
  3540        0.0337             nan     0.0176   -0.0002
  3560        0.0334             nan     0.0176   -0.0000
  3580        0.0331             nan     0.0176   -0.0001
  3600        0.0329             nan     0.0176   -0.0002
  3620        0.0327             nan     0.0176   -0.0002
  3640        0.0325             nan     0.0176   -0.0001
  3660        0.0322             nan     0.0176   -0.0001
  3680        0.0318             nan     0.0176   -0.0001
  3700        0.0315             nan     0.0176   -0.0001
  3720        0.0312             nan     0.0176   -0.0001
  3740        0.0309             nan     0.0176   -0.0001
  3760        0.0306             nan     0.0176   -0.0001
  3780        0.0303             nan     0.0176   -0.0001
  3800        0.0299             nan     0.0176   -0.0001
  3820        0.0296             nan     0.0176   -0.0001
  3822        0.0295             nan     0.0176   -0.0001

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5877             nan     0.0376    0.0226
     2        0.5592             nan     0.0376    0.0228
     3        0.5304             nan     0.0376    0.0184
     4        0.5155             nan     0.0376    0.0190
     5        0.5000             nan     0.0376    0.0162
     6        0.4822             nan     0.0376    0.0111
     7        0.4611             nan     0.0376    0.0159
     8        0.4519             nan     0.0376    0.0131
     9        0.4360             nan     0.0376    0.0140
    10        0.4195             nan     0.0376    0.0133
    20        0.3287             nan     0.0376   -0.0016
    40        0.2582             nan     0.0376    0.0012
    60        0.2103             nan     0.0376   -0.0026
    80        0.1882             nan     0.0376   -0.0010
   100        0.1622             nan     0.0376   -0.0006
   120        0.1420             nan     0.0376   -0.0000
   140        0.1287             nan     0.0376   -0.0007
   160        0.1121             nan     0.0376   -0.0004
   180        0.1029             nan     0.0376   -0.0010
   200        0.0902             nan     0.0376   -0.0010
   220        0.0826             nan     0.0376   -0.0004
   240        0.0764             nan     0.0376   -0.0008
   260        0.0687             nan     0.0376   -0.0012
   280        0.0618             nan     0.0376   -0.0002
   300        0.0562             nan     0.0376   -0.0001
   320        0.0537             nan     0.0376    0.0000
   340        0.0499             nan     0.0376   -0.0005
   360        0.0464             nan     0.0376   -0.0004
   380        0.0440             nan     0.0376   -0.0003
   400        0.0409             nan     0.0376   -0.0004
   420        0.0371             nan     0.0376   -0.0001
   440        0.0346             nan     0.0376   -0.0005
   460        0.0333             nan     0.0376   -0.0003
   480        0.0317             nan     0.0376   -0.0004
   500        0.0304             nan     0.0376   -0.0003
   520        0.0282             nan     0.0376   -0.0003
   540        0.0265             nan     0.0376   -0.0002
   560        0.0242             nan     0.0376   -0.0002
   580        0.0229             nan     0.0376   -0.0001
   600        0.0217             nan     0.0376   -0.0003
   620        0.0203             nan     0.0376   -0.0002
   640        0.0192             nan     0.0376   -0.0004
   660        0.0179             nan     0.0376   -0.0004
   680        0.0165             nan     0.0376   -0.0001
   700        0.0153             nan     0.0376   -0.0001
   720        0.0142             nan     0.0376   -0.0001
   740        0.0136             nan     0.0376   -0.0001
   760        0.0130             nan     0.0376   -0.0000
   780        0.0123             nan     0.0376   -0.0002
   800        0.0117             nan     0.0376   -0.0001
   820        0.0109             nan     0.0376   -0.0001
   840        0.0105             nan     0.0376   -0.0000
   860        0.0099             nan     0.0376   -0.0001
   880        0.0095             nan     0.0376   -0.0001
   900        0.0091             nan     0.0376   -0.0000
   920        0.0087             nan     0.0376    0.0000
   940        0.0082             nan     0.0376   -0.0001
   960        0.0079             nan     0.0376   -0.0001
   980        0.0075             nan     0.0376   -0.0001
  1000        0.0072             nan     0.0376   -0.0001
  1020        0.0067             nan     0.0376   -0.0000
  1040        0.0064             nan     0.0376   -0.0001
  1060        0.0062             nan     0.0376   -0.0000
  1080        0.0059             nan     0.0376   -0.0001
  1100        0.0057             nan     0.0376   -0.0001
  1120        0.0054             nan     0.0376   -0.0001
  1140        0.0051             nan     0.0376   -0.0000
  1160        0.0049             nan     0.0376   -0.0001
  1180        0.0047             nan     0.0376   -0.0001
  1200        0.0044             nan     0.0376   -0.0001
  1220        0.0043             nan     0.0376   -0.0000
  1240        0.0040             nan     0.0376   -0.0000
  1260        0.0038             nan     0.0376   -0.0000
  1280        0.0037             nan     0.0376   -0.0000
  1300        0.0036             nan     0.0376   -0.0000
  1320        0.0035             nan     0.0376   -0.0000
  1340        0.0033             nan     0.0376   -0.0000
  1360        0.0032             nan     0.0376   -0.0000
  1380        0.0031             nan     0.0376   -0.0000
  1400        0.0029             nan     0.0376    0.0000
  1420        0.0028             nan     0.0376   -0.0000
  1440        0.0026             nan     0.0376   -0.0000
  1460        0.0025             nan     0.0376   -0.0000
  1480        0.0024             nan     0.0376   -0.0000
  1500        0.0024             nan     0.0376   -0.0000
  1520        0.0022             nan     0.0376   -0.0000
  1540        0.0021             nan     0.0376   -0.0000
  1560        0.0020             nan     0.0376   -0.0000
  1580        0.0019             nan     0.0376   -0.0000
  1600        0.0018             nan     0.0376   -0.0000
  1620        0.0017             nan     0.0376   -0.0000
  1640        0.0017             nan     0.0376   -0.0000
  1660        0.0016             nan     0.0376   -0.0000
  1680        0.0015             nan     0.0376   -0.0000
  1700        0.0014             nan     0.0376   -0.0000
  1720        0.0014             nan     0.0376   -0.0000
  1740        0.0013             nan     0.0376   -0.0000
  1760        0.0013             nan     0.0376   -0.0000
  1780        0.0012             nan     0.0376   -0.0000
  1800        0.0012             nan     0.0376   -0.0000
  1805        0.0012             nan     0.0376   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5450             nan     0.1185    0.0486
     2        0.4959             nan     0.1185    0.0307
     3        0.4633             nan     0.1185    0.0341
     4        0.4296             nan     0.1185    0.0251
     5        0.4103             nan     0.1185    0.0204
     6        0.3937             nan     0.1185    0.0086
     7        0.3693             nan     0.1185    0.0118
     8        0.3491             nan     0.1185    0.0061
     9        0.3447             nan     0.1185   -0.0123
    10        0.3244             nan     0.1185    0.0045
    20        0.2555             nan     0.1185    0.0007
    40        0.2146             nan     0.1185   -0.0103
    60        0.1870             nan     0.1185   -0.0080
    80        0.1571             nan     0.1185    0.0010
   100        0.1480             nan     0.1185   -0.0046
   120        0.1362             nan     0.1185   -0.0035
   140        0.1207             nan     0.1185   -0.0017
   160        0.1144             nan     0.1185   -0.0061
   180        0.1077             nan     0.1185   -0.0057
   200        0.1021             nan     0.1185   -0.0040
   220        0.0966             nan     0.1185   -0.0034
   240        0.0903             nan     0.1185   -0.0017
   260        0.0844             nan     0.1185   -0.0022
   280        0.0832             nan     0.1185   -0.0016
   300        0.0746             nan     0.1185   -0.0024
   320        0.0699             nan     0.1185   -0.0015
   340        0.0670             nan     0.1185   -0.0016
   360        0.0630             nan     0.1185   -0.0017
   380        0.0590             nan     0.1185   -0.0008
   400        0.0566             nan     0.1185   -0.0007
   420        0.0540             nan     0.1185   -0.0009
   440        0.0511             nan     0.1185   -0.0013
   460        0.0482             nan     0.1185   -0.0005
   480        0.0454             nan     0.1185   -0.0010
   500        0.0435             nan     0.1185   -0.0004
   520        0.0405             nan     0.1185   -0.0007
   540        0.0388             nan     0.1185   -0.0007
   560        0.0371             nan     0.1185   -0.0012
   580        0.0347             nan     0.1185   -0.0003
   600        0.0323             nan     0.1185   -0.0004
   620        0.0297             nan     0.1185   -0.0006
   640        0.0281             nan     0.1185   -0.0009
   660        0.0262             nan     0.1185   -0.0004
   680        0.0255             nan     0.1185   -0.0009
   700        0.0243             nan     0.1185   -0.0004
   720        0.0233             nan     0.1185   -0.0007
   740        0.0223             nan     0.1185   -0.0008
   760        0.0207             nan     0.1185   -0.0004
   780        0.0197             nan     0.1185   -0.0007
   800        0.0188             nan     0.1185   -0.0004
   820        0.0175             nan     0.1185   -0.0006
   840        0.0164             nan     0.1185   -0.0006
   860        0.0155             nan     0.1185   -0.0004
   880        0.0149             nan     0.1185   -0.0002
   900        0.0144             nan     0.1185   -0.0004
   920        0.0138             nan     0.1185   -0.0010
   940        0.0127             nan     0.1185   -0.0003
   960        0.0122             nan     0.1185   -0.0002
   980        0.0115             nan     0.1185   -0.0001
  1000        0.0109             nan     0.1185   -0.0002
  1020        0.0104             nan     0.1185   -0.0005
  1040        0.0099             nan     0.1185   -0.0002
  1060        0.0095             nan     0.1185   -0.0003
  1080        0.0091             nan     0.1185   -0.0003
  1100        0.0085             nan     0.1185   -0.0000
  1120        0.0081             nan     0.1185   -0.0000
  1140        0.0078             nan     0.1185   -0.0002
  1160        0.0074             nan     0.1185   -0.0003
  1180        0.0071             nan     0.1185   -0.0002
  1200        0.0067             nan     0.1185   -0.0002
  1220        0.0064             nan     0.1185   -0.0003
  1240        0.0061             nan     0.1185   -0.0001
  1260        0.0058             nan     0.1185   -0.0001
  1280        0.0056             nan     0.1185   -0.0001
  1300        0.0053             nan     0.1185   -0.0002
  1320        0.0050             nan     0.1185   -0.0001
  1340        0.0047             nan     0.1185   -0.0001
  1360        0.0046             nan     0.1185   -0.0001
  1380        0.0043             nan     0.1185   -0.0001
  1400        0.0042             nan     0.1185   -0.0001
  1420        0.0040             nan     0.1185   -0.0001
  1440        0.0038             nan     0.1185   -0.0001
  1460        0.0036             nan     0.1185   -0.0001
  1480        0.0034             nan     0.1185   -0.0001
  1500        0.0033             nan     0.1185   -0.0001
  1520        0.0031             nan     0.1185   -0.0000
  1540        0.0030             nan     0.1185   -0.0001
  1560        0.0028             nan     0.1185   -0.0001
  1580        0.0027             nan     0.1185   -0.0000
  1600        0.0026             nan     0.1185   -0.0001
  1620        0.0025             nan     0.1185   -0.0001
  1640        0.0024             nan     0.1185   -0.0001
  1660        0.0022             nan     0.1185   -0.0000
  1680        0.0021             nan     0.1185   -0.0000
  1700        0.0020             nan     0.1185   -0.0000
  1720        0.0019             nan     0.1185   -0.0001
  1740        0.0018             nan     0.1185   -0.0000
  1760        0.0018             nan     0.1185   -0.0000
  1780        0.0017             nan     0.1185   -0.0000
  1800        0.0017             nan     0.1185   -0.0000
  1820        0.0016             nan     0.1185   -0.0001
  1840        0.0015             nan     0.1185   -0.0000
  1860        0.0014             nan     0.1185   -0.0000
  1880        0.0014             nan     0.1185   -0.0001
  1900        0.0013             nan     0.1185   -0.0000
  1920        0.0013             nan     0.1185   -0.0000
  1940        0.0012             nan     0.1185   -0.0000
  1960        0.0012             nan     0.1185   -0.0000
  1980        0.0012             nan     0.1185   -0.0001
  2000        0.0011             nan     0.1185   -0.0000
  2020        0.0010             nan     0.1185   -0.0000
  2040        0.0010             nan     0.1185   -0.0000
  2060        0.0009             nan     0.1185   -0.0000
  2080        0.0009             nan     0.1185   -0.0000
  2100        0.0009             nan     0.1185   -0.0000
  2120        0.0009             nan     0.1185   -0.0000
  2140        0.0008             nan     0.1185   -0.0000
  2160        0.0008             nan     0.1185   -0.0000
  2180        0.0007             nan     0.1185   -0.0000
  2200        0.0007             nan     0.1185   -0.0000
  2220        0.0007             nan     0.1185   -0.0000
  2240        0.0007             nan     0.1185   -0.0000
  2260        0.0006             nan     0.1185   -0.0000
  2280        0.0006             nan     0.1185   -0.0000
  2300        0.0006             nan     0.1185   -0.0000
  2320        0.0005             nan     0.1185   -0.0000
  2340        0.0005             nan     0.1185   -0.0000
  2360        0.0005             nan     0.1185   -0.0000
  2380        0.0005             nan     0.1185   -0.0000
  2400        0.0004             nan     0.1185   -0.0000
  2420        0.0004             nan     0.1185   -0.0000
  2440        0.0004             nan     0.1185   -0.0000
  2460        0.0004             nan     0.1185   -0.0000
  2480        0.0004             nan     0.1185   -0.0000
  2500        0.0003             nan     0.1185   -0.0000
  2520        0.0003             nan     0.1185   -0.0000
  2540        0.0003             nan     0.1185   -0.0000
  2560        0.0003             nan     0.1185   -0.0000
  2580        0.0003             nan     0.1185   -0.0000
  2600        0.0003             nan     0.1185   -0.0000
  2620        0.0003             nan     0.1185   -0.0000
  2640        0.0003             nan     0.1185   -0.0000
  2660        0.0002             nan     0.1185   -0.0000
  2680        0.0002             nan     0.1185   -0.0000
  2700        0.0002             nan     0.1185   -0.0000
  2720        0.0002             nan     0.1185   -0.0000
  2740        0.0002             nan     0.1185   -0.0000
  2760        0.0002             nan     0.1185   -0.0000
  2780        0.0002             nan     0.1185   -0.0000
  2800        0.0002             nan     0.1185   -0.0000
  2820        0.0002             nan     0.1185   -0.0000
  2840        0.0002             nan     0.1185   -0.0000
  2860        0.0002             nan     0.1185   -0.0000
  2880        0.0002             nan     0.1185   -0.0000
  2900        0.0001             nan     0.1185   -0.0000
  2920        0.0001             nan     0.1185   -0.0000
  2940        0.0001             nan     0.1185   -0.0000
  2960        0.0001             nan     0.1185   -0.0000
  2980        0.0001             nan     0.1185   -0.0000
  3000        0.0001             nan     0.1185   -0.0000
  3020        0.0001             nan     0.1185   -0.0000
  3040        0.0001             nan     0.1185   -0.0000
  3060        0.0001             nan     0.1185   -0.0000
  3080        0.0001             nan     0.1185   -0.0000
  3100        0.0001             nan     0.1185   -0.0000
  3120        0.0001             nan     0.1185   -0.0000
  3140        0.0001             nan     0.1185   -0.0000
  3160        0.0001             nan     0.1185   -0.0000
  3180        0.0001             nan     0.1185   -0.0000
  3200        0.0001             nan     0.1185   -0.0000
  3220        0.0001             nan     0.1185   -0.0000
  3240        0.0001             nan     0.1185   -0.0000
  3260        0.0001             nan     0.1185   -0.0000
  3280        0.0001             nan     0.1185   -0.0000
  3300        0.0001             nan     0.1185   -0.0000
  3320        0.0001             nan     0.1185   -0.0000
  3340        0.0001             nan     0.1185   -0.0000
  3360        0.0001             nan     0.1185   -0.0000
  3380        0.0000             nan     0.1185   -0.0000
  3400        0.0000             nan     0.1185   -0.0000
  3420        0.0000             nan     0.1185   -0.0000
  3440        0.0000             nan     0.1185   -0.0000
  3460        0.0000             nan     0.1185   -0.0000
  3480        0.0000             nan     0.1185   -0.0000
  3500        0.0000             nan     0.1185   -0.0000
  3520        0.0000             nan     0.1185   -0.0000
  3540        0.0000             nan     0.1185   -0.0000
  3560        0.0000             nan     0.1185    0.0000
  3580        0.0000             nan     0.1185   -0.0000
  3600        0.0000             nan     0.1185   -0.0000
  3620        0.0000             nan     0.1185   -0.0000
  3640        0.0000             nan     0.1185   -0.0000
  3660        0.0000             nan     0.1185   -0.0000
  3677        0.0000             nan     0.1185   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5352             nan     0.1233    0.0775
     2        0.4745             nan     0.1233    0.0642
     3        0.4463             nan     0.1233    0.0315
     4        0.4074             nan     0.1233    0.0442
     5        0.3654             nan     0.1233    0.0217
     6        0.3518             nan     0.1233    0.0184
     7        0.3298             nan     0.1233    0.0155
     8        0.3216             nan     0.1233    0.0079
     9        0.3045             nan     0.1233    0.0193
    10        0.2867             nan     0.1233    0.0157
    20        0.1865             nan     0.1233    0.0023
    40        0.1273             nan     0.1233   -0.0017
    60        0.0988             nan     0.1233   -0.0023
    80        0.0679             nan     0.1233   -0.0028
   100        0.0530             nan     0.1233   -0.0024
   120        0.0421             nan     0.1233   -0.0022
   140        0.0326             nan     0.1233   -0.0008
   160        0.0265             nan     0.1233   -0.0011
   180        0.0216             nan     0.1233   -0.0011
   200        0.0185             nan     0.1233   -0.0006
   220        0.0152             nan     0.1233    0.0001
   240        0.0122             nan     0.1233   -0.0003
   260        0.0097             nan     0.1233   -0.0001
   280        0.0081             nan     0.1233   -0.0002
   300        0.0067             nan     0.1233   -0.0004
   320        0.0061             nan     0.1233   -0.0001
   340        0.0051             nan     0.1233   -0.0001
   360        0.0045             nan     0.1233   -0.0001
   380        0.0039             nan     0.1233   -0.0001
   400        0.0034             nan     0.1233   -0.0001
   420        0.0028             nan     0.1233   -0.0001
   440        0.0023             nan     0.1233   -0.0001
   460        0.0020             nan     0.1233   -0.0000
   480        0.0016             nan     0.1233   -0.0000
   500        0.0013             nan     0.1233   -0.0000
   520        0.0011             nan     0.1233   -0.0000
   540        0.0010             nan     0.1233   -0.0000
   560        0.0008             nan     0.1233   -0.0000
   580        0.0007             nan     0.1233   -0.0000
   600        0.0005             nan     0.1233   -0.0000
   620        0.0005             nan     0.1233   -0.0000
   640        0.0004             nan     0.1233   -0.0000
   660        0.0004             nan     0.1233   -0.0000
   680        0.0004             nan     0.1233   -0.0000
   700        0.0003             nan     0.1233   -0.0000
   715        0.0003             nan     0.1233   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.5347             nan     0.1555    0.0577
     2        0.4845             nan     0.1555    0.0495
     3        0.4417             nan     0.1555    0.0313
     4        0.4129             nan     0.1555    0.0253
     5        0.4019             nan     0.1555    0.0158
     6        0.3792             nan     0.1555    0.0098
     7        0.3667             nan     0.1555    0.0001
     8        0.3552             nan     0.1555    0.0037
     9        0.3550             nan     0.1555   -0.0170
    10        0.3420             nan     0.1555   -0.0042
    20        0.3022             nan     0.1555   -0.0057
    40        0.2577             nan     0.1555   -0.0044
    60        0.2425             nan     0.1555   -0.0079
    80        0.2257             nan     0.1555   -0.0091
   100        0.2092             nan     0.1555   -0.0073
   120        0.1982             nan     0.1555   -0.0079
   140        0.1848             nan     0.1555   -0.0044
   160        0.1688             nan     0.1555   -0.0058
   180        0.1592             nan     0.1555   -0.0066
   200        0.1477             nan     0.1555   -0.0041
   220        0.1459             nan     0.1555   -0.0080
   240        0.1380             nan     0.1555   -0.0040
   260        0.1327             nan     0.1555   -0.0026
   280        0.1206             nan     0.1555   -0.0029
   300        0.1156             nan     0.1555   -0.0052
   320        0.1097             nan     0.1555   -0.0053
   340        0.1067             nan     0.1555   -0.0019
   360        0.1014             nan     0.1555   -0.0031
   380        0.0978             nan     0.1555   -0.0013
   400        0.0916             nan     0.1555   -0.0026
   405        0.0902             nan     0.1555   -0.0021

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.4787             nan     0.3693    0.1539
     2        0.4547             nan     0.3693   -0.0063
     3        0.3753             nan     0.3693    0.0471
     4        0.3266             nan     0.3693    0.0402
     5        0.3198             nan     0.3693   -0.0051
     6        0.2921             nan     0.3693    0.0281
     7        0.2926             nan     0.3693   -0.0121
     8        0.3065             nan     0.3693   -0.0434
     9        0.2786             nan     0.3693   -0.0107
    10        0.2778             nan     0.3693   -0.0214
    20        0.1830             nan     0.3693   -0.0086
    40        0.1262             nan     0.3693   -0.0091
    60        0.1027             nan     0.3693   -0.0206
    80        0.0768             nan     0.3693   -0.0080
   100        0.0626             nan     0.3693   -0.0024
   120        0.0518             nan     0.3693   -0.0099
   140        0.0374             nan     0.3693   -0.0041
   160        0.0277             nan     0.3693   -0.0014
   180        0.0252             nan     0.3693   -0.0029
   200        0.0184             nan     0.3693   -0.0007
   220        0.0156             nan     0.3693   -0.0005
   240        0.0140             nan     0.3693   -0.0014
   260        0.0105             nan     0.3693   -0.0005
   280        0.0083             nan     0.3693   -0.0012
   300        0.0067             nan     0.3693   -0.0002
   320        0.0053             nan     0.3693   -0.0006
   340        0.0040             nan     0.3693   -0.0000
   360        0.0033             nan     0.3693   -0.0005
   380        0.0027             nan     0.3693   -0.0001
   400        0.0022             nan     0.3693   -0.0002
   420        0.0018             nan     0.3693   -0.0001
   440        0.0013             nan     0.3693    0.0000
   460        0.0013             nan     0.3693   -0.0001
   480        0.0011             nan     0.3693   -0.0000
   500        0.0011             nan     0.3693   -0.0000
   520        0.0008             nan     0.3693   -0.0001
   540        0.0006             nan     0.3693   -0.0000
   560        0.0005             nan     0.3693   -0.0000
   580        0.0004             nan     0.3693   -0.0000
   600        0.0004             nan     0.3693   -0.0000
   620        0.0003             nan     0.3693   -0.0000
   640        0.0003             nan     0.3693   -0.0000
   660        0.0002             nan     0.3693   -0.0000
   680        0.0002             nan     0.3693   -0.0000
   700        0.0002             nan     0.3693   -0.0000
   720        0.0001             nan     0.3693   -0.0000
   740        0.0001             nan     0.3693   -0.0000
   760        0.0001             nan     0.3693   -0.0000
   780        0.0001             nan     0.3693   -0.0000
   800        0.0001             nan     0.3693   -0.0000
   820        0.0001             nan     0.3693   -0.0000
   840        0.0000             nan     0.3693   -0.0000
   860        0.0000             nan     0.3693   -0.0000
   880        0.0000             nan     0.3693   -0.0000
   900        0.0000             nan     0.3693   -0.0000
   920        0.0000             nan     0.3693   -0.0000
   940        0.0000             nan     0.3693   -0.0000
   960        0.0000             nan     0.3693   -0.0000
   980        0.0000             nan     0.3693   -0.0000
  1000        0.0000             nan     0.3693   -0.0000
  1020        0.0000             nan     0.3693   -0.0000
  1040        0.0000             nan     0.3693   -0.0000
  1060        0.0000             nan     0.3693   -0.0000
  1080        0.0000             nan     0.3693   -0.0000
  1100        0.0000             nan     0.3693   -0.0000
  1120        0.0000             nan     0.3693   -0.0000
  1140        0.0000             nan     0.3693   -0.0000
  1160        0.0000             nan     0.3693   -0.0000
  1180        0.0000             nan     0.3693   -0.0000
  1200        0.0000             nan     0.3693   -0.0000
  1220        0.0000             nan     0.3693   -0.0000
  1240        0.0000             nan     0.3693   -0.0000
  1260        0.0000             nan     0.3693   -0.0000
  1280        0.0000             nan     0.3693   -0.0000
  1300        0.0000             nan     0.3693   -0.0000
  1320        0.0000             nan     0.3693   -0.0000
  1340        0.0000             nan     0.3693   -0.0000
  1360        0.0000             nan     0.3693   -0.0000
  1380        0.0000             nan     0.3693   -0.0000
  1400        0.0000             nan     0.3693   -0.0000
  1420        0.0000             nan     0.3693   -0.0000
  1440        0.0000             nan     0.3693   -0.0000
  1460        0.0000             nan     0.3693   -0.0000
  1480        0.0000             nan     0.3693   -0.0000
  1500        0.0000             nan     0.3693   -0.0000
  1520        0.0000             nan     0.3693   -0.0000
  1540        0.0000             nan     0.3693   -0.0000
  1560        0.0000             nan     0.3693   -0.0000
  1580        0.0000             nan     0.3693   -0.0000
  1600        0.0000             nan     0.3693   -0.0000
  1620        0.0000             nan     0.3693   -0.0000
  1640        0.0000             nan     0.3693   -0.0000
  1660        0.0000             nan     0.3693   -0.0000
  1680        0.0000             nan     0.3693   -0.0000
  1700        0.0000             nan     0.3693   -0.0000
  1720        0.0000             nan     0.3693   -0.0000
  1740        0.0000             nan     0.3693   -0.0000
  1760        0.0000             nan     0.3693   -0.0000
  1780        0.0000             nan     0.3693   -0.0000
  1800        0.0000             nan     0.3693   -0.0000
  1820        0.0000             nan     0.3693   -0.0000
  1840        0.0000             nan     0.3693   -0.0000
  1860        0.0000             nan     0.3693   -0.0000
  1880        0.0000             nan     0.3693   -0.0000
  1900        0.0000             nan     0.3693   -0.0000
  1920        0.0000             nan     0.3693    0.0000
  1940        0.0000             nan     0.3693   -0.0000
  1960        0.0000             nan     0.3693   -0.0000
  1980        0.0000             nan     0.3693   -0.0000
  2000        0.0000             nan     0.3693   -0.0000
  2020        0.0000             nan     0.3693   -0.0000
  2040        0.0000             nan     0.3693   -0.0000
  2060        0.0000             nan     0.3693   -0.0000
  2080        0.0000             nan     0.3693   -0.0000
  2100        0.0000             nan     0.3693   -0.0000
  2120        0.0000             nan     0.3693   -0.0000
  2140        0.0000             nan     0.3693   -0.0000
  2160        0.0000             nan     0.3693   -0.0000
  2180        0.0000             nan     0.3693   -0.0000
  2200        0.0000             nan     0.3693   -0.0000
  2220        0.0000             nan     0.3693   -0.0000
  2240        0.0000             nan     0.3693   -0.0000
  2260        0.0000             nan     0.3693   -0.0000
  2280        0.0000             nan     0.3693   -0.0000
  2300        0.0000             nan     0.3693   -0.0000
  2320        0.0000             nan     0.3693   -0.0000
  2340        0.0000             nan     0.3693   -0.0000
  2360        0.0000             nan     0.3693   -0.0000
  2380        0.0000             nan     0.3693   -0.0000
  2400        0.0000             nan     0.3693   -0.0000
  2420        0.0000             nan     0.3693   -0.0000
  2440        0.0000             nan     0.3693   -0.0000
  2460        0.0000             nan     0.3693   -0.0000
  2480        0.0000             nan     0.3693   -0.0000
  2500        0.0000             nan     0.3693   -0.0000
  2520        0.0000             nan     0.3693   -0.0000
  2540        0.0000             nan     0.3693   -0.0000
  2560        0.0000             nan     0.3693   -0.0000
  2580        0.0000             nan     0.3693   -0.0000
  2600        0.0000             nan     0.3693   -0.0000
  2620        0.0000             nan     0.3693   -0.0000
  2640        0.0000             nan     0.3693   -0.0000
  2660        0.0000             nan     0.3693   -0.0000
  2680        0.0000             nan     0.3693   -0.0000
  2700        0.0000             nan     0.3693   -0.0000
  2720        0.0000             nan     0.3693   -0.0000
  2740        0.0000             nan     0.3693   -0.0000
  2760        0.0000             nan     0.3693   -0.0000
  2780        0.0000             nan     0.3693   -0.0000
  2800        0.0000             nan     0.3693   -0.0000
  2820        0.0000             nan     0.3693   -0.0000
  2840        0.0000             nan     0.3693   -0.0000
  2860        0.0000             nan     0.3693   -0.0000
  2880        0.0000             nan     0.3693   -0.0000
  2900        0.0000             nan     0.3693   -0.0000
  2920        0.0000             nan     0.3693   -0.0000
  2940        0.0000             nan     0.3693   -0.0000
  2960        0.0000             nan     0.3693   -0.0000
  2980        0.0000             nan     0.3693   -0.0000
  3000        0.0000             nan     0.3693   -0.0000
  3020        0.0000             nan     0.3693   -0.0000
  3040        0.0000             nan     0.3693   -0.0000
  3060        0.0000             nan     0.3693   -0.0000
  3080        0.0000             nan     0.3693   -0.0000
  3100        0.0000             nan     0.3693   -0.0000
  3120        0.0000             nan     0.3693   -0.0000
  3140        0.0000             nan     0.3693   -0.0000
  3160        0.0000             nan     0.3693   -0.0000
  3180        0.0000             nan     0.3693   -0.0000
  3200        0.0000             nan     0.3693   -0.0000
  3220        0.0000             nan     0.3693   -0.0000
  3240        0.0000             nan     0.3693   -0.0000
  3260        0.0000             nan     0.3693   -0.0000
  3280        0.0000             nan     0.3693   -0.0000
  3300        0.0000             nan     0.3693   -0.0000
  3320        0.0000             nan     0.3693   -0.0000
  3340        0.0000             nan     0.3693   -0.0000
  3360        0.0000             nan     0.3693   -0.0000
  3380        0.0000             nan     0.3693   -0.0000
  3400        0.0000             nan     0.3693   -0.0000
  3420        0.0000             nan     0.3693   -0.0000
  3440        0.0000             nan     0.3693   -0.0000
  3460        0.0000             nan     0.3693   -0.0000
  3480        0.0000             nan     0.3693   -0.0000
  3500        0.0000             nan     0.3693   -0.0000
  3516        0.0000             nan     0.3693   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.3841             nan     0.4512    0.2532
     2        0.2743             nan     0.4512    0.0384
     3        0.2530             nan     0.4512   -0.0205
     4        0.2279             nan     0.4512    0.0006
     5        0.2362             nan     0.4512   -0.0526
     6        0.2098             nan     0.4512   -0.0111
     7        0.2124             nan     0.4512   -0.0233
     8        0.2145             nan     0.4512   -0.0123
     9        0.1888             nan     0.4512    0.0203
    10        0.1877             nan     0.4512   -0.0476
    20        0.1107             nan     0.4512   -0.0145
    40        0.0400             nan     0.4512   -0.0071
    60        0.0164             nan     0.4512   -0.0018
    80        0.0092             nan     0.4512    0.0002
   100        0.0040             nan     0.4512   -0.0002
   120        0.0023             nan     0.4512   -0.0001
   140        0.0014             nan     0.4512   -0.0004
   160        0.0009             nan     0.4512   -0.0000
   180        0.0005             nan     0.4512   -0.0000
   200        0.0004             nan     0.4512   -0.0001
   220        0.0002             nan     0.4512   -0.0000
   240        0.0001             nan     0.4512   -0.0000
   260        0.0001             nan     0.4512   -0.0000
   280        0.0000             nan     0.4512   -0.0000
   300        0.0000             nan     0.4512   -0.0000
   320        0.0000             nan     0.4512    0.0000
   340        0.0000             nan     0.4512   -0.0000
   360        0.0000             nan     0.4512   -0.0000
   380        0.0000             nan     0.4512   -0.0000
   400        0.0000             nan     0.4512   -0.0000
   420        0.0000             nan     0.4512   -0.0000
   440        0.0000             nan     0.4512   -0.0000
   460        0.0000             nan     0.4512   -0.0000
   480        0.0000             nan     0.4512   -0.0000
   500        0.0000             nan     0.4512   -0.0000
   520        0.0000             nan     0.4512   -0.0000
   540        0.0000             nan     0.4512   -0.0000
   560        0.0000             nan     0.4512   -0.0000
   580        0.0000             nan     0.4512   -0.0000
   600        0.0000             nan     0.4512   -0.0000
   620        0.0000             nan     0.4512   -0.0000
   640        0.0000             nan     0.4512   -0.0000
   660        0.0000             nan     0.4512   -0.0000
   680        0.0000             nan     0.4512   -0.0000
   700        0.0000             nan     0.4512   -0.0000
   720        0.0000             nan     0.4512   -0.0000
   740        0.0000             nan     0.4512   -0.0000
   760        0.0000             nan     0.4512   -0.0000
   780        0.0000             nan     0.4512   -0.0000
   800        0.0000             nan     0.4512   -0.0000
   820        0.0000             nan     0.4512   -0.0000
   840        0.0000             nan     0.4512   -0.0000
   860        0.0000             nan     0.4512   -0.0000
   880        0.0000             nan     0.4512   -0.0000
   900        0.0000             nan     0.4512   -0.0000
   920        0.0000             nan     0.4512   -0.0000
   940        0.0000             nan     0.4512   -0.0000
   960        0.0000             nan     0.4512   -0.0000
   980        0.0000             nan     0.4512   -0.0000
  1000        0.0000             nan     0.4512   -0.0000
  1020        0.0000             nan     0.4512   -0.0000
  1040        0.0000             nan     0.4512   -0.0000
  1060        0.0000             nan     0.4512   -0.0000
  1080        0.0000             nan     0.4512   -0.0000
  1100        0.0000             nan     0.4512   -0.0000
  1120        0.0000             nan     0.4512   -0.0000
  1140        0.0000             nan     0.4512   -0.0000
  1160        0.0000             nan     0.4512   -0.0000
  1180        0.0000             nan     0.4512   -0.0000
  1200        0.0000             nan     0.4512   -0.0000
  1220        0.0000             nan     0.4512   -0.0000
  1240        0.0000             nan     0.4512   -0.0000
  1260        0.0000             nan     0.4512   -0.0000
  1280        0.0000             nan     0.4512   -0.0000
  1300        0.0000             nan     0.4512   -0.0000
  1320        0.0000             nan     0.4512   -0.0000
  1340        0.0000             nan     0.4512   -0.0000
  1360        0.0000             nan     0.4512   -0.0000
  1380        0.0000             nan     0.4512   -0.0000
  1400        0.0000             nan     0.4512   -0.0000
  1420        0.0000             nan     0.4512   -0.0000
  1440        0.0000             nan     0.4512   -0.0000
  1460        0.0000             nan     0.4512   -0.0000
  1480        0.0000             nan     0.4512    0.0000
  1500        0.0000             nan     0.4512   -0.0000
  1520        0.0000             nan     0.4512   -0.0000
  1540        0.0000             nan     0.4512   -0.0000
  1560        0.0000             nan     0.4512   -0.0000
  1580        0.0000             nan     0.4512   -0.0000
  1600        0.0000             nan     0.4512   -0.0000
  1620        0.0000             nan     0.4512   -0.0000
  1640        0.0000             nan     0.4512   -0.0000
  1660        0.0000             nan     0.4512   -0.0000
  1680        0.0000             nan     0.4512   -0.0000
  1700        0.0000             nan     0.4512   -0.0000
  1720        0.0000             nan     0.4512   -0.0000
  1740        0.0000             nan     0.4512   -0.0000
  1760        0.0000             nan     0.4512   -0.0000
  1780        0.0000             nan     0.4512   -0.0000
  1800        0.0000             nan     0.4512   -0.0000
  1820        0.0000             nan     0.4512   -0.0000
  1840        0.0000             nan     0.4512   -0.0000
  1860        0.0000             nan     0.4512   -0.0000
  1880        0.0000             nan     0.4512   -0.0000
  1900        0.0000             nan     0.4512   -0.0000
  1920        0.0000             nan     0.4512   -0.0000
  1940        0.0000             nan     0.4512   -0.0000
  1960        0.0000             nan     0.4512   -0.0000
  1980        0.0000             nan     0.4512   -0.0000
  2000        0.0000             nan     0.4512   -0.0000
  2020        0.0000             nan     0.4512   -0.0000
  2040        0.0000             nan     0.4512    0.0000
  2060        0.0000             nan     0.4512   -0.0000
  2080        0.0000             nan     0.4512   -0.0000
  2100        0.0000             nan     0.4512   -0.0000
  2120        0.0000             nan     0.4512   -0.0000
  2140        0.0000             nan     0.4512   -0.0000
  2160        0.0000             nan     0.4512   -0.0000
  2180        0.0000             nan     0.4512   -0.0000
  2200        0.0000             nan     0.4512   -0.0000
  2220        0.0000             nan     0.4512   -0.0000
  2240        0.0000             nan     0.4512   -0.0000
  2260        0.0000             nan     0.4512   -0.0000
  2280        0.0000             nan     0.4512   -0.0000
  2300        0.0000             nan     0.4512   -0.0000
  2320        0.0000             nan     0.4512   -0.0000
  2340        0.0000             nan     0.4512   -0.0000
  2360        0.0000             nan     0.4512   -0.0000
  2380        0.0000             nan     0.4512   -0.0000
  2400        0.0000             nan     0.4512   -0.0000
  2420        0.0000             nan     0.4512   -0.0000
  2440        0.0000             nan     0.4512   -0.0000
  2460        0.0000             nan     0.4512   -0.0000
  2480        0.0000             nan     0.4512   -0.0000
  2500        0.0000             nan     0.4512    0.0000
  2520        0.0000             nan     0.4512   -0.0000
  2540        0.0000             nan     0.4512   -0.0000
  2560        0.0000             nan     0.4512   -0.0000
  2580        0.0000             nan     0.4512   -0.0000
  2600        0.0000             nan     0.4512   -0.0000
  2620        0.0000             nan     0.4512   -0.0000
  2640        0.0000             nan     0.4512   -0.0000
  2660        0.0000             nan     0.4512   -0.0000
  2680        0.0000             nan     0.4512   -0.0000
  2700        0.0000             nan     0.4512   -0.0000
  2720        0.0000             nan     0.4512   -0.0000
  2740        0.0000             nan     0.4512    0.0000
  2760        0.0000             nan     0.4512   -0.0000
  2780        0.0000             nan     0.4512   -0.0000
  2800        0.0000             nan     0.4512   -0.0000
  2820        0.0000             nan     0.4512   -0.0000
  2840        0.0000             nan     0.4512    0.0000
  2860        0.0000             nan     0.4512   -0.0000
  2880        0.0000             nan     0.4512   -0.0000
  2900        0.0000             nan     0.4512   -0.0000
  2920        0.0000             nan     0.4512   -0.0000
  2940        0.0000             nan     0.4512   -0.0000
  2960        0.0000             nan     0.4512   -0.0000
  2980        0.0000             nan     0.4512   -0.0000
  3000        0.0000             nan     0.4512   -0.0000
  3020        0.0000             nan     0.4512   -0.0000
  3040        0.0000             nan     0.4512   -0.0000
  3060        0.0000             nan     0.4512   -0.0000
  3080        0.0000             nan     0.4512   -0.0000
  3100        0.0000             nan     0.4512   -0.0000
  3120        0.0000             nan     0.4512   -0.0000
  3140        0.0000             nan     0.4512   -0.0000
  3160        0.0000             nan     0.4512   -0.0000
  3180        0.0000             nan     0.4512   -0.0000
  3200        0.0000             nan     0.4512    0.0000
  3220        0.0000             nan     0.4512   -0.0000
  3240        0.0000             nan     0.4512   -0.0000
  3260        0.0000             nan     0.4512   -0.0000
  3280        0.0000             nan     0.4512   -0.0000
  3300        0.0000             nan     0.4512   -0.0000
  3320        0.0000             nan     0.4512   -0.0000
  3340        0.0000             nan     0.4512   -0.0000
  3360        0.0000             nan     0.4512   -0.0000
  3380        0.0000             nan     0.4512   -0.0000
  3400        0.0000             nan     0.4512   -0.0000
  3420        0.0000             nan     0.4512   -0.0000
  3427        0.0000             nan     0.4512   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2953             nan     0.5480    0.3162
     2        0.1379             nan     0.5480    0.0262
     3        0.1177             nan     0.5480   -0.0250
     4        0.0760             nan     0.5480   -0.0032
     5        0.0757             nan     0.5480   -0.0107
     6        0.0725             nan     0.5480   -0.0107
     7        0.0664             nan     0.5480   -0.0065
     8        0.0554             nan     0.5480   -0.0011
     9        0.0441             nan     0.5480    0.0012
    10        0.0403             nan     0.5480   -0.0121
    20        0.0116             nan     0.5480   -0.0017
    40        0.0017             nan     0.5480   -0.0001
    60        0.0002             nan     0.5480   -0.0001
    80        0.0001             nan     0.5480   -0.0000
   100        0.0000             nan     0.5480   -0.0000
   120        0.0000             nan     0.5480   -0.0000
   140        0.0000             nan     0.5480   -0.0000
   160        0.0000             nan     0.5480   -0.0000
   180        0.0000             nan     0.5480   -0.0000
   200        0.0000             nan     0.5480   -0.0000
   220        0.0000             nan     0.5480   -0.0000
   240        0.0000             nan     0.5480   -0.0000
   260        0.0000             nan     0.5480   -0.0000
   280        0.0000             nan     0.5480   -0.0000
   300        0.0000             nan     0.5480   -0.0000
   320        0.0000             nan     0.5480   -0.0000
   340        0.0000             nan     0.5480   -0.0000
   360        0.0000             nan     0.5480   -0.0000
   380        0.0000             nan     0.5480   -0.0000
   400        0.0000             nan     0.5480   -0.0000
   420        0.0000             nan     0.5480   -0.0000
   440        0.0000             nan     0.5480   -0.0000
   460        0.0000             nan     0.5480   -0.0000
   480        0.0000             nan     0.5480   -0.0000
   500        0.0000             nan     0.5480   -0.0000
   520        0.0000             nan     0.5480   -0.0000
   540        0.0000             nan     0.5480   -0.0000
   560        0.0000             nan     0.5480   -0.0000
   580        0.0000             nan     0.5480   -0.0000
   600        0.0000             nan     0.5480   -0.0000
   620        0.0000             nan     0.5480   -0.0000
   640        0.0000             nan     0.5480   -0.0000
   660        0.0000             nan     0.5480   -0.0000
   680        0.0000             nan     0.5480   -0.0000
   700        0.0000             nan     0.5480   -0.0000
   720        0.0000             nan     0.5480   -0.0000
   740        0.0000             nan     0.5480   -0.0000
   760        0.0000             nan     0.5480   -0.0000
   780        0.0000             nan     0.5480    0.0000
   800        0.0000             nan     0.5480   -0.0000
   820        0.0000             nan     0.5480   -0.0000
   840        0.0000             nan     0.5480   -0.0000
   860        0.0000             nan     0.5480   -0.0000
   880        0.0000             nan     0.5480   -0.0000
   900        0.0000             nan     0.5480   -0.0000
   920        0.0000             nan     0.5480   -0.0000
   940        0.0000             nan     0.5480   -0.0000
   960        0.0000             nan     0.5480   -0.0000
   980        0.0000             nan     0.5480   -0.0000
  1000        0.0000             nan     0.5480   -0.0000
  1020        0.0000             nan     0.5480   -0.0000
  1040        0.0000             nan     0.5480   -0.0000
  1060        0.0000             nan     0.5480   -0.0000
  1080        0.0000             nan     0.5480   -0.0000
  1100        0.0000             nan     0.5480   -0.0000
  1120        0.0000             nan     0.5480   -0.0000
  1140        0.0000             nan     0.5480   -0.0000
  1160        0.0000             nan     0.5480   -0.0000
  1180        0.0000             nan     0.5480    0.0000
  1200        0.0000             nan     0.5480    0.0000
  1220        0.0000             nan     0.5480   -0.0000
  1240        0.0000             nan     0.5480   -0.0000
  1260        0.0000             nan     0.5480    0.0000
  1280        0.0000             nan     0.5480   -0.0000
  1300        0.0000             nan     0.5480   -0.0000
  1320        0.0000             nan     0.5480   -0.0000
  1340        0.0000             nan     0.5480   -0.0000
  1360        0.0000             nan     0.5480   -0.0000
  1380        0.0000             nan     0.5480   -0.0000
  1400        0.0000             nan     0.5480   -0.0000
  1420        0.0000             nan     0.5480   -0.0000
  1440        0.0000             nan     0.5480   -0.0000
  1460        0.0000             nan     0.5480   -0.0000
  1480        0.0000             nan     0.5480   -0.0000
  1500        0.0000             nan     0.5480   -0.0000
  1520        0.0000             nan     0.5480   -0.0000
  1540        0.0000             nan     0.5480    0.0000
  1560        0.0000             nan     0.5480   -0.0000
  1580        0.0000             nan     0.5480   -0.0000
  1600        0.0000             nan     0.5480   -0.0000
  1620        0.0000             nan     0.5480   -0.0000
  1640        0.0000             nan     0.5480   -0.0000
  1660        0.0000             nan     0.5480   -0.0000
  1680        0.0000             nan     0.5480   -0.0000
  1700        0.0000             nan     0.5480   -0.0000
  1720        0.0000             nan     0.5480   -0.0000
  1740        0.0000             nan     0.5480   -0.0000
  1760        0.0000             nan     0.5480   -0.0000
  1780        0.0000             nan     0.5480   -0.0000
  1800        0.0000             nan     0.5480   -0.0000
  1820        0.0000             nan     0.5480   -0.0000
  1840        0.0000             nan     0.5480   -0.0000
  1860        0.0000             nan     0.5480   -0.0000
  1880        0.0000             nan     0.5480   -0.0000
  1900        0.0000             nan     0.5480   -0.0000
  1920        0.0000             nan     0.5480   -0.0000
  1940        0.0000             nan     0.5480   -0.0000
  1960        0.0000             nan     0.5480    0.0000
  1980        0.0000             nan     0.5480   -0.0000
  2000        0.0000             nan     0.5480   -0.0000
  2020        0.0000             nan     0.5480   -0.0000
  2040        0.0000             nan     0.5480   -0.0000
  2060        0.0000             nan     0.5480   -0.0000
  2080        0.0000             nan     0.5480   -0.0000
  2100        0.0000             nan     0.5480   -0.0000
  2120        0.0000             nan     0.5480   -0.0000
  2140        0.0000             nan     0.5480   -0.0000
  2160        0.0000             nan     0.5480    0.0000
  2180        0.0000             nan     0.5480   -0.0000
  2200        0.0000             nan     0.5480   -0.0000
  2220        0.0000             nan     0.5480    0.0000
  2240        0.0000             nan     0.5480   -0.0000
  2260        0.0000             nan     0.5480   -0.0000
  2280        0.0000             nan     0.5480   -0.0000
  2300        0.0000             nan     0.5480   -0.0000
  2320        0.0000             nan     0.5480   -0.0000
  2340        0.0000             nan     0.5480   -0.0000
  2360        0.0000             nan     0.5480   -0.0000
  2380        0.0000             nan     0.5480   -0.0000
  2400        0.0000             nan     0.5480   -0.0000
  2420        0.0000             nan     0.5480   -0.0000
  2440        0.0000             nan     0.5480   -0.0000
  2460        0.0000             nan     0.5480   -0.0000
  2480        0.0000             nan     0.5480   -0.0000
  2500        0.0000             nan     0.5480   -0.0000
  2520        0.0000             nan     0.5480   -0.0000
  2540        0.0000             nan     0.5480   -0.0000
  2560        0.0000             nan     0.5480   -0.0000
  2580        0.0000             nan     0.5480   -0.0000
  2600        0.0000             nan     0.5480   -0.0000
  2620        0.0000             nan     0.5480   -0.0000
  2640        0.0000             nan     0.5480   -0.0000
  2660        0.0000             nan     0.5480   -0.0000
  2680        0.0000             nan     0.5480   -0.0000
  2700        0.0000             nan     0.5480   -0.0000
  2720        0.0000             nan     0.5480   -0.0000
  2740        0.0000             nan     0.5480   -0.0000
  2760        0.0000             nan     0.5480   -0.0000
  2780        0.0000             nan     0.5480   -0.0000
  2800        0.0000             nan     0.5480   -0.0000
  2820        0.0000             nan     0.5480   -0.0000
  2840        0.0000             nan     0.5480   -0.0000
  2860        0.0000             nan     0.5480   -0.0000
  2880        0.0000             nan     0.5480   -0.0000
  2900        0.0000             nan     0.5480   -0.0000
  2920        0.0000             nan     0.5480    0.0000
  2940        0.0000             nan     0.5480   -0.0000
  2960        0.0000             nan     0.5480   -0.0000
  2980        0.0000             nan     0.5480   -0.0000
  3000        0.0000             nan     0.5480   -0.0000
  3020        0.0000             nan     0.5480    0.0000
  3040        0.0000             nan     0.5480   -0.0000
  3060        0.0000             nan     0.5480   -0.0000
  3080        0.0000             nan     0.5480   -0.0000
  3100        0.0000             nan     0.5480   -0.0000
  3120        0.0000             nan     0.5480   -0.0000
  3140        0.0000             nan     0.5480   -0.0000
  3160        0.0000             nan     0.5480   -0.0000
  3180        0.0000             nan     0.5480    0.0000
  3200        0.0000             nan     0.5480   -0.0000
  3220        0.0000             nan     0.5480   -0.0000
  3240        0.0000             nan     0.5480   -0.0000
  3260        0.0000             nan     0.5480   -0.0000
  3280        0.0000             nan     0.5480   -0.0000
  3300        0.0000             nan     0.5480   -0.0000
  3320        0.0000             nan     0.5480   -0.0000
  3340        0.0000             nan     0.5480   -0.0000
  3360        0.0000             nan     0.5480   -0.0000
  3380        0.0000             nan     0.5480   -0.0000
  3400        0.0000             nan     0.5480   -0.0000
  3420        0.0000             nan     0.5480   -0.0000
  3440        0.0000             nan     0.5480   -0.0000
  3460        0.0000             nan     0.5480   -0.0000
  3480        0.0000             nan     0.5480   -0.0000
  3500        0.0000             nan     0.5480   -0.0000
  3520        0.0000             nan     0.5480   -0.0000
  3540        0.0000             nan     0.5480   -0.0000
  3560        0.0000             nan     0.5480   -0.0000
  3580        0.0000             nan     0.5480   -0.0000
  3600        0.0000             nan     0.5480   -0.0000
  3620        0.0000             nan     0.5480   -0.0000
  3640        0.0000             nan     0.5480   -0.0000
  3660        0.0000             nan     0.5480    0.0000
  3680        0.0000             nan     0.5480   -0.0000
  3700        0.0000             nan     0.5480   -0.0000
  3720        0.0000             nan     0.5480   -0.0000
  3740        0.0000             nan     0.5480   -0.0000
  3760        0.0000             nan     0.5480   -0.0000
  3780        0.0000             nan     0.5480   -0.0000
  3800        0.0000             nan     0.5480   -0.0000
  3820        0.0000             nan     0.5480   -0.0000
  3840        0.0000             nan     0.5480   -0.0000
  3860        0.0000             nan     0.5480   -0.0000
  3880        0.0000             nan     0.5480   -0.0000
  3900        0.0000             nan     0.5480   -0.0000
  3920        0.0000             nan     0.5480   -0.0000
  3940        0.0000             nan     0.5480   -0.0000
  3955        0.0000             nan     0.5480   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.4206             nan     0.5705    0.1276
     2        0.3103             nan     0.5705    0.1160
     3        0.2753             nan     0.5705    0.0228
     4        0.2622             nan     0.5705   -0.0119
     5        0.2103             nan     0.5705    0.0313
     6        0.2050             nan     0.5705   -0.0050
     7        0.2033             nan     0.5705   -0.0427
     8        0.1782             nan     0.5705    0.0074
     9        0.1718             nan     0.5705   -0.0114
    10        0.1693             nan     0.5705   -0.0206
    20        0.1551             nan     0.5705   -0.0299
    40        0.1214             nan     0.5705   -0.0236
    60        0.0886             nan     0.5705   -0.0065
    80        0.0668             nan     0.5705   -0.0163
   100        0.0458             nan     0.5705   -0.0103
   120        0.0316             nan     0.5705   -0.0044
   140        0.0272             nan     0.5705   -0.0041
   160        0.0186             nan     0.5705   -0.0032
   180        0.0153             nan     0.5705   -0.0032
   200        0.0137             nan     0.5705    0.0006
   220        0.0101             nan     0.5705   -0.0013
   240        0.0075             nan     0.5705   -0.0020
   260        0.0060             nan     0.5705   -0.0006
   280        0.0045             nan     0.5705   -0.0007
   300        0.0037             nan     0.5705   -0.0011
   320        0.0034             nan     0.5705   -0.0003
   340        0.0027             nan     0.5705    0.0001
   360        0.0020             nan     0.5705   -0.0001
   380        0.0016             nan     0.5705   -0.0000
   400        0.0013             nan     0.5705   -0.0000
   420        0.0013             nan     0.5705   -0.0003
   440        0.0011             nan     0.5705   -0.0001
   460        0.0010             nan     0.5705   -0.0003
   480        0.0008             nan     0.5705   -0.0000
   500        0.0005             nan     0.5705   -0.0001
   520        0.0005             nan     0.5705   -0.0001
   540        0.0003             nan     0.5705   -0.0001
   560        0.0003             nan     0.5705   -0.0001
   580        0.0002             nan     0.5705   -0.0000
   600        0.0002             nan     0.5705   -0.0000
   620        0.0001             nan     0.5705   -0.0000
   640        0.0001             nan     0.5705    0.0000
   660        0.0001             nan     0.5705   -0.0000
   680        0.0001             nan     0.5705   -0.0000
   700        0.0001             nan     0.5705    0.0000
   720        0.0001             nan     0.5705   -0.0000
   740        0.0001             nan     0.5705   -0.0000
   760        0.0000             nan     0.5705   -0.0000
   780        0.0000             nan     0.5705   -0.0000
   800        0.0000             nan     0.5705   -0.0000
   820        0.0000             nan     0.5705   -0.0000
   840        0.0000             nan     0.5705   -0.0000
   860        0.0000             nan     0.5705   -0.0000
   880        0.0000             nan     0.5705   -0.0000
   900        0.0000             nan     0.5705   -0.0000
   920        0.0000             nan     0.5705   -0.0000
   940        0.0000             nan     0.5705   -0.0000
   960        0.0000             nan     0.5705   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.2717             nan     0.5480    0.4724
     2        0.1681             nan     0.5480    0.0834
     3        0.1007             nan     0.5480    0.0532
     4        0.0793             nan     0.5480    0.0065
     5        0.0672             nan     0.5480    0.0025
     6        0.0606             nan     0.5480   -0.0089
     7        0.0523             nan     0.5480    0.0035
     8        0.0501             nan     0.5480   -0.0132
     9        0.0446             nan     0.5480    0.0008
    10        0.0402             nan     0.5480   -0.0124
    20        0.0160             nan     0.5480   -0.0086
    40        0.0019             nan     0.5480   -0.0003
    60        0.0002             nan     0.5480   -0.0001
    80        0.0000             nan     0.5480   -0.0000
   100        0.0000             nan     0.5480   -0.0000
   120        0.0000             nan     0.5480    0.0000
   140        0.0000             nan     0.5480   -0.0000
   160        0.0000             nan     0.5480   -0.0000
   180        0.0000             nan     0.5480   -0.0000
   200        0.0000             nan     0.5480    0.0000
   220        0.0000             nan     0.5480   -0.0000
   240        0.0000             nan     0.5480   -0.0000
   260        0.0000             nan     0.5480   -0.0000
   280        0.0000             nan     0.5480   -0.0000
   300        0.0000             nan     0.5480   -0.0000
   320        0.0000             nan     0.5480   -0.0000
   340        0.0000             nan     0.5480   -0.0000
   360        0.0000             nan     0.5480   -0.0000
   380        0.0000             nan     0.5480   -0.0000
   400        0.0000             nan     0.5480   -0.0000
   420        0.0000             nan     0.5480   -0.0000
   440        0.0000             nan     0.5480   -0.0000
   460        0.0000             nan     0.5480   -0.0000
   480        0.0000             nan     0.5480   -0.0000
   500        0.0000             nan     0.5480   -0.0000
   520        0.0000             nan     0.5480   -0.0000
   540        0.0000             nan     0.5480   -0.0000
   560        0.0000             nan     0.5480   -0.0000
   580        0.0000             nan     0.5480   -0.0000
   600        0.0000             nan     0.5480   -0.0000
   620        0.0000             nan     0.5480   -0.0000
   640        0.0000             nan     0.5480   -0.0000
   660        0.0000             nan     0.5480   -0.0000
   680        0.0000             nan     0.5480   -0.0000
   700        0.0000             nan     0.5480   -0.0000
   720        0.0000             nan     0.5480   -0.0000
   740        0.0000             nan     0.5480   -0.0000
   760        0.0000             nan     0.5480   -0.0000
   780        0.0000             nan     0.5480   -0.0000
   800        0.0000             nan     0.5480   -0.0000
   820        0.0000             nan     0.5480   -0.0000
   840        0.0000             nan     0.5480   -0.0000
   860        0.0000             nan     0.5480   -0.0000
   880        0.0000             nan     0.5480   -0.0000
   900        0.0000             nan     0.5480   -0.0000
   920        0.0000             nan     0.5480   -0.0000
   940        0.0000             nan     0.5480   -0.0000
   960        0.0000             nan     0.5480   -0.0000
   980        0.0000             nan     0.5480   -0.0000
  1000        0.0000             nan     0.5480   -0.0000
  1020        0.0000             nan     0.5480   -0.0000
  1040        0.0000             nan     0.5480   -0.0000
  1060        0.0000             nan     0.5480   -0.0000
  1080        0.0000             nan     0.5480   -0.0000
  1100        0.0000             nan     0.5480   -0.0000
  1120        0.0000             nan     0.5480    0.0000
  1140        0.0000             nan     0.5480   -0.0000
  1160        0.0000             nan     0.5480   -0.0000
  1180        0.0000             nan     0.5480   -0.0000
  1200        0.0000             nan     0.5480   -0.0000
  1220        0.0000             nan     0.5480   -0.0000
  1240        0.0000             nan     0.5480   -0.0000
  1260        0.0000             nan     0.5480   -0.0000
  1280        0.0000             nan     0.5480   -0.0000
  1300        0.0000             nan     0.5480   -0.0000
  1320        0.0000             nan     0.5480   -0.0000
  1340        0.0000             nan     0.5480    0.0000
  1360        0.0000             nan     0.5480   -0.0000
  1380        0.0000             nan     0.5480   -0.0000
  1400        0.0000             nan     0.5480   -0.0000
  1420        0.0000             nan     0.5480   -0.0000
  1440        0.0000             nan     0.5480   -0.0000
  1460        0.0000             nan     0.5480   -0.0000
  1480        0.0000             nan     0.5480   -0.0000
  1500        0.0000             nan     0.5480   -0.0000
  1520        0.0000             nan     0.5480   -0.0000
  1540        0.0000             nan     0.5480   -0.0000
  1560        0.0000             nan     0.5480   -0.0000
  1580        0.0000             nan     0.5480   -0.0000
  1600        0.0000             nan     0.5480   -0.0000
  1620        0.0000             nan     0.5480   -0.0000
  1640        0.0000             nan     0.5480   -0.0000
  1660        0.0000             nan     0.5480   -0.0000
  1680        0.0000             nan     0.5480   -0.0000
  1700        0.0000             nan     0.5480   -0.0000
  1720        0.0000             nan     0.5480   -0.0000
  1740        0.0000             nan     0.5480   -0.0000
  1760        0.0000             nan     0.5480   -0.0000
  1780        0.0000             nan     0.5480   -0.0000
  1800        0.0000             nan     0.5480    0.0000
  1820        0.0000             nan     0.5480   -0.0000
  1840        0.0000             nan     0.5480   -0.0000
  1860        0.0000             nan     0.5480   -0.0000
  1880        0.0000             nan     0.5480   -0.0000
  1900        0.0000             nan     0.5480   -0.0000
  1920        0.0000             nan     0.5480   -0.0000
  1940        0.0000             nan     0.5480   -0.0000
  1960        0.0000             nan     0.5480   -0.0000
  1980        0.0000             nan     0.5480   -0.0000
  2000        0.0000             nan     0.5480   -0.0000
  2020        0.0000             nan     0.5480   -0.0000
  2040        0.0000             nan     0.5480   -0.0000
  2060        0.0000             nan     0.5480   -0.0000
  2080        0.0000             nan     0.5480   -0.0000
  2100        0.0000             nan     0.5480   -0.0000
  2120        0.0000             nan     0.5480   -0.0000
  2140        0.0000             nan     0.5480   -0.0000
  2160        0.0000             nan     0.5480   -0.0000
  2180        0.0000             nan     0.5480   -0.0000
  2200        0.0000             nan     0.5480   -0.0000
  2220        0.0000             nan     0.5480   -0.0000
  2240        0.0000             nan     0.5480   -0.0000
  2260        0.0000             nan     0.5480   -0.0000
  2280        0.0000             nan     0.5480   -0.0000
  2300        0.0000             nan     0.5480   -0.0000
  2320        0.0000             nan     0.5480   -0.0000
  2340        0.0000             nan     0.5480   -0.0000
  2360        0.0000             nan     0.5480   -0.0000
  2380        0.0000             nan     0.5480   -0.0000
  2400        0.0000             nan     0.5480   -0.0000
  2420        0.0000             nan     0.5480   -0.0000
  2440        0.0000             nan     0.5480   -0.0000
  2460        0.0000             nan     0.5480   -0.0000
  2480        0.0000             nan     0.5480   -0.0000
  2500        0.0000             nan     0.5480   -0.0000
  2520        0.0000             nan     0.5480   -0.0000
  2540        0.0000             nan     0.5480   -0.0000
  2560        0.0000             nan     0.5480   -0.0000
  2580        0.0000             nan     0.5480   -0.0000
  2600        0.0000             nan     0.5480   -0.0000
  2620        0.0000             nan     0.5480   -0.0000
  2640        0.0000             nan     0.5480   -0.0000
  2660        0.0000             nan     0.5480   -0.0000
  2680        0.0000             nan     0.5480   -0.0000
  2700        0.0000             nan     0.5480   -0.0000
  2720        0.0000             nan     0.5480   -0.0000
  2740        0.0000             nan     0.5480   -0.0000
  2760        0.0000             nan     0.5480   -0.0000
  2780        0.0000             nan     0.5480   -0.0000
  2800        0.0000             nan     0.5480   -0.0000
  2820        0.0000             nan     0.5480   -0.0000
  2840        0.0000             nan     0.5480   -0.0000
  2860        0.0000             nan     0.5480   -0.0000
  2880        0.0000             nan     0.5480   -0.0000
  2900        0.0000             nan     0.5480   -0.0000
  2920        0.0000             nan     0.5480   -0.0000
  2940        0.0000             nan     0.5480   -0.0000
  2960        0.0000             nan     0.5480   -0.0000
  2980        0.0000             nan     0.5480   -0.0000
  3000        0.0000             nan     0.5480   -0.0000
  3020        0.0000             nan     0.5480   -0.0000
  3040        0.0000             nan     0.5480   -0.0000
  3060        0.0000             nan     0.5480   -0.0000
  3080        0.0000             nan     0.5480   -0.0000
  3100        0.0000             nan     0.5480   -0.0000
  3120        0.0000             nan     0.5480   -0.0000
  3140        0.0000             nan     0.5480   -0.0000
  3160        0.0000             nan     0.5480   -0.0000
  3180        0.0000             nan     0.5480   -0.0000
  3200        0.0000             nan     0.5480   -0.0000
  3220        0.0000             nan     0.5480   -0.0000
  3240        0.0000             nan     0.5480   -0.0000
  3260        0.0000             nan     0.5480   -0.0000
  3280        0.0000             nan     0.5480   -0.0000
  3300        0.0000             nan     0.5480   -0.0000
  3320        0.0000             nan     0.5480   -0.0000
  3340        0.0000             nan     0.5480   -0.0000
  3360        0.0000             nan     0.5480   -0.0000
  3380        0.0000             nan     0.5480   -0.0000
  3400        0.0000             nan     0.5480   -0.0000
  3420        0.0000             nan     0.5480   -0.0000
  3440        0.0000             nan     0.5480   -0.0000
  3460        0.0000             nan     0.5480   -0.0000
  3480        0.0000             nan     0.5480   -0.0000
  3500        0.0000             nan     0.5480   -0.0000
  3520        0.0000             nan     0.5480   -0.0000
  3540        0.0000             nan     0.5480   -0.0000
  3560        0.0000             nan     0.5480   -0.0000
  3580        0.0000             nan     0.5480   -0.0000
  3600        0.0000             nan     0.5480    0.0000
  3620        0.0000             nan     0.5480    0.0000
  3640        0.0000             nan     0.5480   -0.0000
  3660        0.0000             nan     0.5480   -0.0000
  3680        0.0000             nan     0.5480   -0.0000
  3700        0.0000             nan     0.5480   -0.0000
  3720        0.0000             nan     0.5480   -0.0000
  3740        0.0000             nan     0.5480   -0.0000
  3760        0.0000             nan     0.5480   -0.0000
  3780        0.0000             nan     0.5480   -0.0000
  3800        0.0000             nan     0.5480   -0.0000
  3820        0.0000             nan     0.5480   -0.0000
  3840        0.0000             nan     0.5480   -0.0000
  3860        0.0000             nan     0.5480   -0.0000
  3880        0.0000             nan     0.5480   -0.0000
  3900        0.0000             nan     0.5480   -0.0000
  3920        0.0000             nan     0.5480   -0.0000
  3940        0.0000             nan     0.5480   -0.0000
  3955        0.0000             nan     0.5480   -0.0000

Stochastic Gradient Boosting 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  shrinkage   interaction.depth  n.minobsinnode  n.trees  RMSE       Rsquared 
  0.01758466   8                 10              3822     0.7567810  0.3624352
  0.03759880   6                  8              1805     0.6349515  0.5521905
  0.05380386   7                 23              3313           NaN        NaN
  0.11853535   4                 10              3677     0.8079889  0.3450934
  0.12123046   4                 16              3531           NaN        NaN
  0.12329470   2                  8               715     0.6344705  0.5267978
  0.15548697   9                 12               405     0.8881443  0.2371540
  0.15757272   6                 25              4257           NaN        NaN
  0.20067783   9                 17               231           NaN        NaN
  0.24955016   1                 17              3166           NaN        NaN
  0.25270422   3                 14              4407           NaN        NaN
  0.32235103   3                 25               901           NaN        NaN
  0.36931289   4                  9              3516     0.7554430  0.3810756
  0.45123425   6                  8              3427     0.6809644  0.5104018
  0.48075156   9                 14               218           NaN        NaN
  0.49416575   4                 15              2878           NaN        NaN
  0.50047964  10                 20              2412           NaN        NaN
  0.50393785   9                 13              2014           NaN        NaN
  0.54804439   8                  5              3955     0.5530583  0.6966311
  0.57054498   6                 10               960     0.9340671  0.2459703
  MAE        Selected
  0.5832814          
  0.4692130          
        NaN          
  0.6415056          
        NaN          
  0.4858283          
  0.7140155          
        NaN          
        NaN          
        NaN          
        NaN          
        NaN          
  0.6030104          
  0.5433622          
        NaN          
        NaN          
        NaN          
        NaN          
  0.4270830  *       
  0.7392568          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were n.trees = 3955, interaction.depth =
 8, shrinkage = 0.5480444 and n.minobsinnode = 5.
[1] "Sat Mar 10 03:25:23 2018"
Error in relative.influence(object, n.trees = numTrees) : 
  could not find function "relative.influence"
In addition: There were 36 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:28:49 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "gbm_h2o"                 
Multivariate Adaptive Regression Splines 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results:

  RMSE       Rsquared  MAE      
  0.2855705  0.892633  0.2179846

Tuning parameter 'degree' was held constant at a value of 1
[1] "Sat Mar 10 03:29:03 2018"
Generalized Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results:

  RMSE       Rsquared   MAE      
  0.5359023  0.6860927  0.4230051

[1] "Sat Mar 10 03:29:17 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
2: model fit failed for Fold1: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
3: model fit failed for Fold1: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
4: model fit failed for Fold2: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
5: model fit failed for Fold2: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
6: model fit failed for Fold2: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
7: model fit failed for Fold3: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
8: model fit failed for Fold3: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
9: model fit failed for Fold3: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
2: model fit failed for Fold1: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
3: model fit failed for Fold1: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
4: model fit failed for Fold2: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
5: model fit failed for Fold2: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
6: model fit failed for Fold2: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
7: model fit failed for Fold3: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
8: model fit failed for Fold3: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
9: model fit failed for Fold3: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family
 
10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3     NA's   :3    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:29:33 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "glm.nb"                  
Boosted Generalized Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  mstop  prune  RMSE       Rsquared   MAE        Selected
   44    no     0.5017136  0.7161864  0.3816505  *       
   47    no     0.5027382  0.7157227  0.3841749          
   81    no     0.5094961  0.7119509  0.3970860          
  143    yes    0.5045893  0.7134266  0.3878645          
  181    yes    0.5045893  0.7134266  0.3878645          
  192    no     0.5275666  0.6956940  0.4149598          
  361    no     0.5344055  0.6880085  0.4214610          
  403    no     0.5349121  0.6873774  0.4219953          
  483    no     0.5354632  0.6866759  0.4225481          
  576    yes    0.5045893  0.7134266  0.3878645          
  634    yes    0.5045893  0.7134266  0.3878645          
  663    no     0.5358177  0.6862066  0.4229181          
  686    no     0.5358366  0.6861810  0.4229385          
  704    yes    0.5045893  0.7134266  0.3878645          
  707    yes    0.5045893  0.7134266  0.3878645          
  736    yes    0.5045893  0.7134266  0.3878645          
  765    no     0.5358711  0.6861347  0.4229729          
  792    no     0.5358779  0.6861256  0.4229797          
  852    no     0.5358883  0.6861116  0.4229904          
  882    yes    0.5045893  0.7134266  0.3878645          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 44 and prune = no.
[1] "Sat Mar 10 03:29:50 2018"
glmnet 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  alpha       lambda       RMSE       Rsquared   MAE        Selected
  0.04353090  1.948095960  0.6805170  0.6867709  0.3856387          
  0.04607404  3.057918235  0.7287824  0.6979987  0.4217827          
  0.08097647  1.333087230  0.6464855  0.6967591  0.3558280          
  0.14289509  0.002806874  0.5343198  0.6868965  0.4203706          
  0.18011931  0.009273041  0.5327010  0.6884430  0.4175965          
  0.19184555  0.152453742  0.5159834  0.7056803  0.3606886          
  0.36095149  0.175961246  0.5117293  0.7118944  0.3485888          
  0.40285652  1.729926992  0.8166565  0.7179365  0.5096483          
  0.48235978  5.415382501  0.8347529        NaN  0.5276532          
  0.57551994  0.030996931  0.5160738  0.7053086  0.4003458          
  0.63314495  0.002367001  0.5336264  0.6881158  0.4205965          
  0.66271528  0.512055114  0.5982848  0.7447680  0.3207240          
  0.68545433  0.173408044  0.5055025  0.7239338  0.3371984          
  0.70332297  0.026603994  0.5157314  0.7060223  0.4016453          
  0.70619491  0.014841667  0.5231606  0.6988880  0.4100124          
  0.73549568  0.033505321  0.5119155  0.7095807  0.3970412          
  0.76438883  1.309189779  0.8347529        NaN  0.5276532          
  0.79102354  0.631937986  0.6838737  0.7447680  0.3804749          
  0.85138569  0.144759273  0.5011638  0.7266821  0.3427098  *       
  0.88143842  0.013899021  0.5215738  0.7008427  0.4094270          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alpha = 0.8513857 and lambda
 = 0.1447593.
[1] "Sat Mar 10 03:30:06 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:33:29 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "glmnet_h2o"              
Start:  AIC=63.91
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V10   1   6.7771  62.120
- V8    1   6.8222  62.446
- V5    1   6.8828  62.879
- V4    1   6.9070  63.051
- V7    1   6.9359  63.256
- V3    1   6.9627  63.444
- V6    1   6.9918  63.649
<none>      6.7474  63.905
- V9    1   8.6889  74.297
- V2    1  25.5720 127.190

Step:  AIC=62.12
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9

       Df Deviance     AIC
- V8    1   6.8426  60.592
- V5    1   6.9013  61.010
- V4    1   6.9224  61.160
- V7    1   6.9436  61.310
- V3    1   7.0158  61.817
- V6    1   7.0282  61.903
<none>      6.7771  62.120
- V9    1   8.7051  72.388
- V2    1  25.6650 125.368

Step:  AIC=60.59
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9

       Df Deviance     AIC
- V5    1   6.9687  59.487
- V4    1   6.9783  59.554
- V7    1   6.9877  59.621
- V6    1   7.0847  60.296
- V3    1   7.0892  60.327
<none>      6.8426  60.592
- V9    1   8.7302  70.529
- V2    1  26.2026 124.384

Step:  AIC=59.49
.outcome ~ V2 + V3 + V4 + V6 + V7 + V9

       Df Deviance     AIC
- V3    1   7.1585  58.803
- V7    1   7.1792  58.945
- V6    1   7.1838  58.977
- V4    1   7.1923  59.034
<none>      6.9687  59.487
- V9    1   9.0829  70.470
- V2    1  27.0067 123.865

Step:  AIC=58.8
.outcome ~ V2 + V4 + V6 + V7 + V9

       Df Deviance     AIC
- V6    1   7.3245  57.927
- V4    1   7.4135  58.519
<none>      7.1585  58.803
- V7    1   7.4582  58.813
- V9    1   9.4898  70.618
- V2    1  27.6309 122.985

Step:  AIC=57.93
.outcome ~ V2 + V4 + V7 + V9

       Df Deviance     AIC
- V4    1   7.5827  57.625
<none>      7.3245  57.927
- V7    1   7.7593  58.753
- V9    1   9.7235  69.810
- V2    1  28.2789 122.120

Step:  AIC=57.62
.outcome ~ V2 + V7 + V9

       Df Deviance     AIC
<none>      7.5827  57.625
- V7    1   7.9641  58.029
- V9    1   9.8233  68.310
- V2    1  28.4934 120.491
Start:  AIC=79.23
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V7    1    9.174  77.242
- V8    1    9.210  77.445
- V5    1    9.285  77.860
- V3    1    9.317  78.035
- V4    1    9.346  78.191
- V10   1    9.366  78.301
- V9    1    9.414  78.561
<none>       9.171  79.226
- V6    1    9.897  81.110
- V2    1   36.215 147.271

Step:  AIC=77.24
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V9 + V10

       Df Deviance     AIC
- V8    1    9.217  75.480
- V5    1    9.288  75.871
- V3    1    9.318  76.036
- V10   1    9.372  76.334
- V4    1    9.375  76.349
- V9    1    9.423  76.611
<none>       9.174  77.242
- V6    1    9.898  79.116
- V2    1   36.220 145.279

Step:  AIC=75.48
.outcome ~ V2 + V3 + V4 + V5 + V6 + V9 + V10

       Df Deviance     AIC
- V5    1    9.354  74.232
- V3    1    9.387  74.416
- V4    1    9.425  74.622
- V9    1    9.477  74.903
- V10   1    9.492  74.982
<none>       9.217  75.480
- V6    1    9.913  77.192
- V2    1   36.683 143.926

Step:  AIC=74.23
.outcome ~ V2 + V3 + V4 + V6 + V9 + V10

       Df Deviance     AIC
- V4    1    9.490  72.970
- V3    1    9.538  73.228
- V10   1    9.601  73.564
- V9    1    9.652  73.834
<none>       9.354  74.232
- V6    1   10.052  75.906
- V2    1   39.634 145.873

Step:  AIC=72.97
.outcome ~ V2 + V3 + V6 + V9 + V10

       Df Deviance     AIC
- V9    1    9.758  72.392
- V10   1    9.795  72.582
- V3    1    9.815  72.687
<none>       9.490  72.970
- V6    1   10.189  74.597
- V2    1   39.647 143.890

Step:  AIC=72.39
.outcome ~ V2 + V3 + V6 + V10

       Df Deviance     AIC
- V10   1   10.008  71.683
- V3    1   10.135  72.326
<none>       9.758  72.392
- V6    1   10.641  74.809
- V2    1   40.427 142.884

Step:  AIC=71.68
.outcome ~ V2 + V3 + V6

       Df Deviance     AIC
<none>      10.008  71.683
- V3    1   10.461  71.940
- V6    1   11.169  75.277
- V2    1   47.252 148.839
Start:  AIC=67.04
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V8    1   7.2398  65.044
- V10   1   7.2577  65.172
- V6    1   7.2916  65.415
- V9    1   7.3379  65.744
- V5    1   7.3625  65.918
- V3    1   7.4641  66.630
<none>      7.2389  67.038
- V4    1   7.6222  67.720
- V7    1   7.6344  67.803
- V2    1  25.5821 130.683

Step:  AIC=65.04
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9 + V10

       Df Deviance     AIC
- V10   1   7.2584  63.177
- V6    1   7.2929  63.424
- V9    1   7.3380  63.745
- V5    1   7.3626  63.918
- V3    1   7.4659  64.643
<none>      7.2398  65.044
- V4    1   7.6224  65.721
- V7    1   7.6490  65.903
- V2    1  26.0455 129.617

Step:  AIC=63.18
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9

       Df Deviance     AIC
- V6    1   7.3084  61.534
- V9    1   7.3612  61.908
- V5    1   7.3708  61.976
- V3    1   7.5028  62.899
<none>      7.2584  63.177
- V4    1   7.6894  64.177
- V7    1   7.6948  64.213
- V2    1  28.7224 132.704

Step:  AIC=61.53
.outcome ~ V2 + V3 + V4 + V5 + V7 + V9

       Df Deviance     AIC
- V5    1   7.4370  60.441
- V9    1   7.4449  60.496
<none>      7.3084  61.534
- V3    1   7.5992  61.563
- V4    1   7.7560  62.625
- V7    1   7.8070  62.966
- V2    1  28.9843 131.176

Step:  AIC=60.44
.outcome ~ V2 + V3 + V4 + V7 + V9

       Df Deviance     AIC
- V9    1   7.5480  59.211
- V3    1   7.6813  60.122
<none>      7.4370  60.441
- V4    1   7.8489  61.244
- V7    1   7.9424  61.860
- V2    1  30.0287 131.017

Step:  AIC=59.21
.outcome ~ V2 + V3 + V4 + V7

       Df Deviance     AIC
- V3    1   7.8051  58.953
<none>      7.5480  59.211
- V4    1   7.9324  59.795
- V7    1   8.0029  60.255
- V2    1  30.3281 129.533

Step:  AIC=58.95
.outcome ~ V2 + V4 + V7

       Df Deviance     AIC
<none>      7.8051  58.953
- V4    1   8.1939  59.481
- V7    1   8.1978  59.506
- V2    1  30.3337 127.542
Start:  AIC=104.16
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance    AIC
- V8    1   13.117 102.16
- V7    1   13.125 102.21
- V10   1   13.224 102.78
- V5    1   13.306 103.25
- V4    1   13.390 103.72
<none>      13.116 104.16
- V3    1   13.535 104.55
- V9    1   13.623 105.03
- V6    1   13.724 105.60
- V2    1   47.733 200.33

Step:  AIC=102.16
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9 + V10

       Df Deviance    AIC
- V7    1   13.126 100.21
- V10   1   13.224 100.78
- V5    1   13.306 101.25
- V4    1   13.390 101.72
<none>      13.117 102.16
- V3    1   13.538 102.56
- V9    1   13.623 103.03
- V6    1   13.724 103.60
- V2    1   47.735 198.33

Step:  AIC=100.21
.outcome ~ V2 + V3 + V4 + V5 + V6 + V9 + V10

       Df Deviance     AIC
- V10   1   13.231  98.816
- V5    1   13.314  99.295
- V4    1   13.394  99.747
<none>      13.126 100.213
- V3    1   13.541 100.575
- V9    1   13.652 101.201
- V6    1   13.731 101.635
- V2    1   47.740 196.342

Step:  AIC=98.82
.outcome ~ V2 + V3 + V4 + V5 + V6 + V9

       Df Deviance     AIC
- V5    1   13.412  97.852
- V4    1   13.561  98.691
<none>      13.231  98.816
- V3    1   13.657  99.227
- V9    1   13.744  99.707
- V6    1   13.876 100.435
- V2    1   51.814 200.565

Step:  AIC=97.85
.outcome ~ V2 + V3 + V4 + V6 + V9

       Df Deviance     AIC
- V4    1   13.722  97.587
<none>      13.412  97.852
- V3    1   13.793  97.980
- V9    1   13.997  99.093
- V6    1   14.061  99.439
- V2    1   53.712 201.300

Step:  AIC=97.59
.outcome ~ V2 + V3 + V6 + V9

       Df Deviance     AIC
<none>      13.722  97.587
- V3    1   14.205  98.217
- V9    1   14.268  98.551
- V6    1   14.374  99.116
- V2    1   53.787 199.405
Generalized Linear Model with Stepwise Feature Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results:

  RMSE       Rsquared  MAE      
  0.5438442  0.67723   0.4318044

[1] "Sat Mar 10 03:33:43 2018"
Independent Component Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  n.comp  RMSE       Rsquared   MAE        Selected
  1       0.7469214  0.2632870  0.5035682          
  2       0.7239179  0.3419745  0.5166996          
  4       0.7319330  0.3305105  0.5257016          
  5       0.7405714  0.3116806  0.5332197          
  6       0.7291411  0.3598832  0.5320434          
  7       0.7335187  0.3468455  0.5486907          
  8       0.5866078  0.5781294  0.4533415  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was n.comp = 8.
[1] "Sat Mar 10 03:34:00 2018"
Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      0.5822732  0.6101292  0.4136012          
  2      0.5336362  0.6775216  0.4120290          
  4      0.5326788  0.6904780  0.4201245  *       
  5      0.5356064  0.6866097  0.4226744          
  6      0.5358860  0.6861297  0.4229933          
  7      0.5359015  0.6860909  0.4230087          
  8      0.5359024  0.6860927  0.4230052          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 4.
[1] "Sat Mar 10 03:34:13 2018"
Error in MSEP(object) : could not find function "MSEP"
k-Nearest Neighbors 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  kmax  distance   kernel        RMSE       Rsquared   MAE        Selected
   2    2.5297072  inv           0.7935069  0.2160322  0.5282312          
   2    2.6798192  epanechnikov  0.7878365  0.2216447  0.5215353          
   3    2.4034087  epanechnikov  0.7472200  0.2737700  0.4964129          
   4    0.3515031  triangular    0.8180572  0.1993761  0.4958597          
   5    0.7493673  triweight     0.7312847  0.2783006  0.4695361          
   5    1.6814870  gaussian      0.6988933  0.3873517  0.4605538          
  10    1.7292299  rectangular   0.7319697  0.3243129  0.4749076          
  11    2.4901641  inv           0.7596100  0.3061420  0.5089562          
  13    2.8700915  inv           0.7561178  0.2487634  0.4996292          
  15    1.1511390  inv           0.7047539  0.3988815  0.4380083          
  16    0.2947560  biweight      0.7641573  0.2909274  0.4612558          
  17    2.0848548  rectangular   0.7027997  0.3727344  0.4634708          
  18    0.9059532  triangular    0.7118724  0.3757145  0.4429157          
  18    1.1002585  triweight     0.6955829  0.3926929  0.4391405          
  18    1.7243637  inv           0.7282755  0.3409523  0.4700985          
  19    1.1770463  triangular    0.6952637  0.3938412  0.4320858  *       
  20    2.1548896  gaussian      0.7162479  0.3967563  0.4677433          
  20    2.3973864  rectangular   0.7536362  0.3052749  0.5003251          
  22    1.6642448  epanechnikov  0.7017863  0.4112552  0.4558933          
  23    0.8841062  biweight      0.7030670  0.3766474  0.4410810          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were kmax = 19, distance = 1.177046
 and kernel = triangular.
[1] "Sat Mar 10 03:34:30 2018"
k-Nearest Neighbors 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  k   RMSE       Rsquared   MAE        Selected
   2  0.7233388  0.3050577  0.4878177          
   3  0.6914041  0.4055832  0.4589085          
   4  0.7136182  0.3839979  0.4597578          
   5  0.7025962  0.3925649  0.4454786          
  10  0.6700002  0.5605853  0.4277409          
  11  0.6577238  0.5937366  0.4247667  *       
  13  0.6657351  0.5911359  0.4246193          
  15  0.6705756  0.5566510  0.4091570          
  16  0.6639381  0.5720359  0.3988808          
  17  0.6671094  0.5714158  0.4038564          
  18  0.6655726  0.5932453  0.4009532          
  19  0.6756007  0.5706852  0.4034059          
  20  0.6793547  0.5584697  0.4078933          
  22  0.6950244  0.5218062  0.4127413          
  23  0.6946711  0.5232848  0.4127997          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 11.
[1] "Sat Mar 10 03:34:43 2018"
Polynomial Kernel Regularized Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  lambda        degree  RMSE          Rsquared    MAE           Selected
  1.650648e-05  3          0.8341968  0.44738244     0.5270553          
  1.699692e-05  3          0.8341968  0.44738244     0.5270553          
  2.540284e-05  3          0.8341968  0.44738245     0.5270553          
  5.181738e-05  1       3050.8136098  0.23855163  2407.2130613          
  7.954201e-05  1       1987.5574314  0.23854996  1568.2727926          
  9.103906e-05  2          0.8620917  0.03719027     0.5928514          
  6.379071e-04  2          0.8615448  0.03723342     0.5920196          
  1.033434e-03  3          0.8341968  0.44738421     0.5270553          
  2.581065e-03  3          0.8341968  0.44738690     0.5270553          
  7.543966e-03  2          0.8556986  0.03778945     0.5832603          
  1.464619e-02  1         11.1476830  0.23766879     8.8154355          
  2.058621e-02  3          0.8341969  0.44741816     0.5270554          
  2.674679e-02  2          0.8458720  0.03940414     0.5676934          
  3.285601e-02  2          0.8438818  0.03992841     0.5643828          
  3.396054e-02  1          5.0409423  0.23650114     4.0013359          
  4.758591e-02  2          0.8403151  0.04119729     0.5580090          
  6.636577e-02  3          0.8341969  0.44749748     0.5270554          
  9.018155e-02  3          0.8341970  0.44753864     0.5270555          
  1.806876e-01  2          0.8310605  0.05224351     0.5382587  *       
  2.553831e-01  1          1.2058085  0.22318396     0.9055846          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 0.1806876 and degree = 2.
[1] "Sat Mar 10 03:34:58 2018"

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.80010617  0.12213173 -0.07552261 -0.22300601  0.23710428  0.17913091 
         V8          V9         V10 
 0.14267095 -0.60338878 -0.01285442 

 Quartiles of Marginal Effects:
 
          V2        V3          V4         V5        V6        V7        V8
25% 1.791139 0.1168716 -0.08248878 -0.2284163 0.2315417 0.1746751 0.1365343
50% 1.798897 0.1231355 -0.07512963 -0.2243922 0.2371116 0.1811517 0.1416886
75% 1.811642 0.1271921 -0.06862470 -0.2175512 0.2439111 0.1845609 0.1485568
            V9          V10
25% -0.6104411 -0.017230343
50% -0.6023628 -0.013349114
75% -0.5952885 -0.007554334

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.57023567  0.29717539 -0.07887619 -0.08007647  0.30640339  0.16928047 
         V8          V9         V10 
 0.27525901 -0.57375057  0.02660181 

 Quartiles of Marginal Effects:
 
           V2         V3         V4          V5         V6         V7
25% 0.6951749 -0.3616809 -0.5117125 -0.46593510 -0.2002134 -0.5364441
50% 1.6297223  0.3616892 -0.1924611 -0.08219863  0.3870361  0.1890510
75% 2.3524677  0.6884332  0.2742205  0.46539168  0.7755988  0.8947878
             V8          V9         V10
25% -0.06880714 -1.10478094 -0.41921971
50%  0.28690875 -0.45422237 -0.04322827
75%  0.65180679 -0.01950163  0.55421764

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.309129042  0.013039613  0.090284940 -0.000245568  0.041930139  0.062109582 
          V8           V9          V10 
 0.018475383 -0.093714276  0.048812777 

 Quartiles of Marginal Effects:
 
            V2          V3          V4          V5          V6          V7
25% 0.06340959 -0.05394867 -0.06132077 -0.08377893 -0.05982317 -0.02618628
50% 0.20428290  0.00403907  0.02467237 -0.01645467  0.01735070  0.03032466
75% 0.40178087  0.10616287  0.10741997  0.04215230  0.10753368  0.13255893
              V8          V9         V10
25% -0.072034513 -0.17212694 -0.03598573
50%  0.002460734 -0.02837547  0.00120665
75%  0.132088561  0.01460847  0.11151465

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.76659643 -0.03528550  0.05884817 -0.15047414  0.13637930  0.07555730 
         V8          V9         V10 
 0.08742302 -0.25649994  0.05657515 

 Quartiles of Marginal Effects:
 
           V2          V3         V4         V5        V6         V7         V8
25% 0.7637498 -0.03695022 0.05773672 -0.1521903 0.1351831 0.07428365 0.08611901
50% 0.7671423 -0.03541139 0.05919930 -0.1503929 0.1366757 0.07572717 0.08730588
75% 0.7701550 -0.03365367 0.06102876 -0.1492884 0.1382914 0.07738003 0.08890103
            V9        V10
25% -0.2589561 0.05508568
50% -0.2563894 0.05654475
75% -0.2546223 0.05816207

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.92248947  0.15574232 -0.10969904 -0.22729084  0.24062846  0.19797075 
         V8          V9         V10 
 0.15112908 -0.64013268 -0.02983057 

 Quartiles of Marginal Effects:
 
          V2        V3          V4         V5        V6        V7        V8
25% 1.887900 0.1350874 -0.13240168 -0.2444972 0.2215816 0.1828891 0.1298254
50% 1.914049 0.1574786 -0.11082164 -0.2327580 0.2444157 0.2022477 0.1494834
75% 1.960008 0.1757307 -0.08744938 -0.2117707 0.2675634 0.2155188 0.1680322
            V9         V10
25% -0.6594840 -0.04513400
50% -0.6369797 -0.03013494
75% -0.6123872 -0.01234678

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.78194505  0.14723960 -0.08629746 -0.24238030  0.22617269  0.19127389 
         V8          V9         V10 
 0.15946672 -0.58372754 -0.01294537 

 Quartiles of Marginal Effects:
 
          V2         V3          V4         V5        V6        V7         V8
25% 1.665503 0.08013866 -0.15522752 -0.3012154 0.1670205 0.1436064 0.09445183
50% 1.754479 0.15615823 -0.08198028 -0.2572401 0.2352820 0.1981559 0.14717332
75% 1.918834 0.21467826 -0.01240706 -0.1835396 0.3092321 0.2521841 0.23056864
            V9          V10
25% -0.6555735 -0.072120718
50% -0.5610691 -0.008981996
75% -0.4905052  0.045587062

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.43468533  0.16036350 -0.04708169 -0.22612921  0.23590115  0.14735110 
         V8          V9         V10 
 0.14782128 -0.44044927  0.04386911 

 Quartiles of Marginal Effects:
 
          V2          V3          V4          V5           V6         V7
25% 1.017583 -0.04078985 -0.27148970 -0.49096964 -0.009357827 -0.0984053
50% 1.368780  0.24775194 -0.09586004 -0.22947738  0.267813548  0.1447286
75% 1.833882  0.39870166  0.21766991 -0.02254927  0.575341769  0.3787602
             V8         V9         V10
25% -0.07605094 -0.6129147 -0.20394883
50%  0.06656678 -0.3694948 -0.03474301
75%  0.34392314 -0.1702916  0.23829114

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.21355398  0.24010527 -0.19396825 -0.25914661  0.24662222  0.25674728 
         V8          V9         V10 
 0.21213852 -0.71128285 -0.07387572 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6        V7        V8
25% 2.067199 0.1497872 -0.2821515 -0.3139088 0.1516883 0.2028504 0.1218966
50% 2.209685 0.2378287 -0.1989619 -0.2599649 0.2609540 0.2537208 0.2061210
75% 2.319874 0.3118561 -0.1136997 -0.2047841 0.3701868 0.3450020 0.3013457
            V9          V10
25% -0.8572715 -0.171692817
50% -0.7110808 -0.076467362
75% -0.5829221  0.008311452

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.34339291  0.02888923  0.01483768 -0.20928969  0.20494200  0.12975798 
         V8          V9         V10 
 0.12557535 -0.45296759  0.03558669 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5        V6        V7        V8
25% 1.342580 0.02840173 0.01433809 -0.2098367 0.2045649 0.1295281 0.1251275
50% 1.343364 0.02892528 0.01494938 -0.2093400 0.2050625 0.1298625 0.1255279
75% 1.344560 0.02939925 0.01544357 -0.2087889 0.2055020 0.1302043 0.1260616
            V9        V10
25% -0.4535007 0.03512095
50% -0.4528903 0.03551315
75% -0.4523369 0.03607163

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.68312998  0.05078863  0.08494008 -0.07645272  0.10884736  0.11120846 
         V8          V9         V10 
 0.03558652 -0.21544409  0.08096597 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6          V7
25% 0.2551567 -0.13599572 -0.17716235 -0.26992999 -0.11399161 -0.03731689
50% 0.5938753  0.02128809  0.01328652 -0.09616798  0.06844438  0.06586137
75% 0.8395133  0.20515190  0.30048903  0.09232825  0.28836724  0.22475800
            V8         V9         V10
25% -0.1068364 -0.3679603 -0.12824465
50%  0.0412722 -0.1034440  0.01196215
75%  0.2419144 -0.0309795  0.17074717

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.069414613 -0.001195857  0.038934323  0.020063477  0.007057849  0.013044416 
          V8           V9          V10 
 0.009499837 -0.018645745  0.013690744 

 Quartiles of Marginal Effects:
 
             V2            V3           V4            V5           V6
25% 0.002696946 -0.0111984641 -0.007315947 -0.0147829918 -0.010279147
50% 0.014437398 -0.0007996568  0.002738166  0.0009209244  0.001148069
75% 0.071144813  0.0128502592  0.016275701  0.0135306892  0.022935778
              V7           V8           V9           V10
25% -0.011087885 -0.011334074 -0.023091910 -0.0120673018
50%  0.004572495  0.001401356 -0.003637248  0.0008380473
75%  0.022903827  0.023152304  0.008610546  0.0220934239

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.13570017  0.22183902 -0.17345014 -0.25502871  0.24451566  0.24310591 
         V8          V9         V10 
 0.19886465 -0.69035989 -0.06173798 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6        V7        V8
25% 2.013998 0.1382880 -0.2550575 -0.3135725 0.1664187 0.1817510 0.1099580
50% 2.135893 0.2268267 -0.1741176 -0.2555289 0.2488347 0.2360214 0.1964309
75% 2.243409 0.2933796 -0.1019192 -0.1963791 0.3664554 0.3221892 0.2737151
            V9          V10
25% -0.7863542 -0.143679164
50% -0.6946178 -0.062729408
75% -0.5887584  0.008441168

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.99016548  0.08919185  0.03589657 -0.14637646  0.16759253  0.12988671 
         V8          V9         V10 
 0.06590524 -0.31197062  0.08503217 

 Quartiles of Marginal Effects:
 
           V2         V3           V4          V5          V6         V7
25% 0.5474698 -0.1442706 -0.260529873 -0.43995260 -0.01470521 -0.1080318
50% 0.8935222  0.1403916 -0.004056642 -0.16094648  0.19598922  0.1343835
75% 1.2163370  0.3116012  0.315749729  0.09137899  0.40192518  0.2835759
             V8          V9          V10
25% -0.18040236 -0.47561489 -0.165527258
50%  0.08694534 -0.22755288  0.009002421
75%  0.29983708 -0.06158459  0.200259285

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.88980573  0.23438711 -0.12111931 -0.29713769  0.25681475  0.19309850 
         V8          V9         V10 
 0.25640683 -0.55869412 -0.04409394 

 Quartiles of Marginal Effects:
 
          V2           V3          V4         V5         V6         V7
25% 1.444205 -0.008250568 -0.28585154 -0.4784929 0.05291309 -0.0333732
50% 1.944526  0.268990958 -0.15795342 -0.2659204 0.28632565  0.2137425
75% 2.178326  0.466645504  0.06626145 -0.1626919 0.55097691  0.4265238
            V8         V9         V10
25% 0.08335231 -0.8363756 -0.28013141
50% 0.20415077 -0.5199702 -0.01635616
75% 0.39938143 -0.2624053  0.19904197

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.50919717  0.18913128 -0.06381692 -0.20952434  0.27180249  0.15843777 
         V8          V9         V10 
 0.18277542 -0.47224170  0.04241753 

 Quartiles of Marginal Effects:
 
           V2           V3         V4          V5         V6         V7
25% 0.8958851 -0.001833432 -0.3630359 -0.53446721 0.00192635 -0.2037737
50% 1.5636303  0.248712775 -0.1399441 -0.21124642 0.30163700  0.1250002
75% 1.9132872  0.489097214  0.2359096  0.05091478 0.68353235  0.4247264
             V8         V9          V10
25% -0.04318309 -0.7489826 -0.289270586
50%  0.08697337 -0.4146309  0.003719922
75%  0.41584904 -0.1448313  0.316153795

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.07919164  0.28418899 -0.14377156 -0.33347416  0.26264213  0.21339616 
         V8          V9         V10 
 0.32691109 -0.58929442 -0.09460359 

 Quartiles of Marginal Effects:
 
          V2          V3          V4         V5          V6          V7
25% 1.642102 -0.01337611 -0.33436338 -0.4918872 -0.03324554 -0.02950035
50% 2.133328  0.35538856 -0.17488706 -0.2866522  0.31372851  0.23057324
75% 2.398879  0.51685503  0.04514336 -0.1815851  0.57026260  0.53199081
           V8         V9         V10
25% 0.1662061 -0.9508167 -0.37506934
50% 0.2752222 -0.4948753 -0.06092175
75% 0.5044989 -0.2054319  0.19690710

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.01145427  0.09164679  0.03400210 -0.13859777  0.17614664  0.14116998 
         V8          V9         V10 
 0.06998871 -0.32332842  0.08841503 

 Quartiles of Marginal Effects:
 
           V2         V3          V4         V5         V6         V7
25% 0.5055453 -0.1367136 -0.30383979 -0.4182631 -0.0437769 -0.1079991
50% 0.9423364  0.1493274 -0.02988263 -0.1535801  0.1999594  0.1438088
75% 1.2558537  0.3437491  0.29787693  0.1075916  0.3786393  0.3038172
            V8          V9         V10
25% -0.1966631 -0.50954677 -0.19152335
50%  0.1036905 -0.26623884  0.01706909
75%  0.3082316 -0.06498779  0.21543284

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 1.619431278  0.292703874 -0.075243717 -0.136160808  0.309957508  0.154220303 
          V8           V9          V10 
 0.274110643 -0.547297272  0.007852435 

 Quartiles of Marginal Effects:
 
           V2         V3         V4         V5         V6          V7
25% 0.8499664 -0.3004128 -0.4559839 -0.4726748 -0.1942193 -0.50130301
50% 1.6852063  0.3576444 -0.2192482 -0.1412730  0.4069674  0.07707744
75% 2.3490157  0.7024347  0.2578188  0.3029917  0.7473830  0.73428678
             V8          V9         V10
25% 0.001136831 -1.16492245 -0.41952586
50% 0.324374705 -0.39843518 -0.03475179
75% 0.575141769 -0.07987521  0.49644963

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.96792902  0.16534814 -0.12157051 -0.22592433  0.24305631  0.20313312 
         V8          V9         V10 
 0.15212914 -0.65520329 -0.03629076 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6        V7        V8
25% 1.936993 0.1467005 -0.1425031 -0.2415795 0.2275342 0.1885916 0.1322716
50% 1.961634 0.1667930 -0.1209397 -0.2308918 0.2458885 0.2065442 0.1509101
75% 2.001505 0.1825097 -0.1010051 -0.2114937 0.2659344 0.2195980 0.1684463
            V9         V10
25% -0.6723555 -0.05116686
50% -0.6523508 -0.03677011
75% -0.6310300 -0.02087172

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.79071339  0.06335670  0.07182198 -0.09892642  0.13013605  0.12135435 
         V8          V9         V10 
 0.04418180 -0.25049206  0.08544983 

 Quartiles of Marginal Effects:
 
           V2          V3          V4         V5          V6          V7
25% 0.3422276 -0.13784423 -0.21817945 -0.3284322 -0.07908873 -0.06847742
50% 0.7465637  0.04510168  0.02636809 -0.1126023  0.11527710  0.08786086
75% 0.9224537  0.25034368  0.32925503  0.1033694  0.35168148  0.25811254
             V8          V9          V10
25% -0.14244124 -0.43242082 -0.155798445
50%  0.07405836 -0.15760373  0.005840819
75%  0.26329636 -0.04006771  0.217020589

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.13892394  0.15554051 -0.17623949 -0.26777563  0.30320119  0.03631008 
         V8          V9         V10 
-0.03334044 -0.22294259  0.40768743 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6         V7          V8
25% 2.129170 0.1464930 -0.1804414 -0.2746556 0.2933956 0.03190050 -0.04187878
50% 2.140454 0.1558414 -0.1764467 -0.2688012 0.3026093 0.03789033 -0.03231582
75% 2.151538 0.1663422 -0.1709613 -0.2610271 0.3127991 0.04119393 -0.02608223
            V9       V10
25% -0.2326207 0.3985412
50% -0.2223017 0.4098330
75% -0.2156989 0.4173228

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.34880871  0.28118114 -0.19352294 -0.23256195  0.05616070  0.04554894 
         V8          V9         V10 
-0.13097960 -0.25364451  0.35313099 

 Quartiles of Marginal Effects:
 
          V2         V3         V4          V5          V6          V7
25% 1.575048 -0.1451861 -1.1190362 -0.77001045 -0.59562476 -0.51419082
50% 2.425901  0.4115230 -0.2675108 -0.09823342  0.08002062 -0.08189846
75% 2.877267  0.6631154  0.7079642  0.26311804  0.64819731  0.50199521
            V8         V9         V10
25% -0.6713436 -0.8396423 -0.05929567
50% -0.1335087 -0.2954148  0.34733638
75%  0.5657823  0.3979118  0.77606552

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.432417199  0.014758523  0.007311835 -0.030322011  0.008818144  0.058316080 
          V8           V9          V10 
-0.034584687 -0.014939431  0.150133952 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6          V7
25% 0.1268853 -0.11584621 -0.13866265 -0.10831678 -0.10749585 -0.12162081
50% 0.2938116  0.05032967 -0.01029011 -0.01176376  0.02492765  0.04522568
75% 0.5022228  0.12383549  0.11291165  0.08777304  0.10023401  0.21363480
             V8          V9         V10
25% -0.16960028 -0.16703626 -0.01265747
50% -0.01896016 -0.02177900  0.10212152
75%  0.06383526  0.07232658  0.24391946

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.950895561  0.048418203 -0.055831501 -0.220855951  0.111841958  0.033631088 
          V8           V9          V10 
-0.008040096 -0.115955599  0.347179618 

 Quartiles of Marginal Effects:
 
           V2         V3          V4         V5        V6         V7
25% 0.9468645 0.04618449 -0.05813338 -0.2238396 0.1101642 0.03155008
50% 0.9523497 0.04805257 -0.05587606 -0.2214486 0.1128365 0.03395665
75% 0.9550085 0.05060892 -0.05378966 -0.2193122 0.1143669 0.03542900
              V8         V9       V10
25% -0.010379734 -0.1185579 0.3450529
50% -0.008554935 -0.1164541 0.3484273
75% -0.006076628 -0.1138469 0.3517178

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.28341426  0.17851732 -0.18692315 -0.25304379  0.33340289  0.03445735 
         V8          V9         V10 
-0.04555448 -0.23335563  0.38895547 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6         V7          V8
25% 2.259248 0.1527274 -0.2017375 -0.2742866 0.3032369 0.01917302 -0.07211720
50% 2.283974 0.1852768 -0.1847409 -0.2571275 0.3334363 0.03686527 -0.04073919
75% 2.318245 0.2077620 -0.1735571 -0.2315368 0.3632640 0.05008542 -0.02214283
            V9       V10
25% -0.2657269 0.3644287
50% -0.2302796 0.3957832
75% -0.2086194 0.4140872

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.14776174  0.19263743 -0.15356443 -0.26364546  0.29724868  0.04232108 
         V8          V9         V10 
-0.03047544 -0.23458210  0.44808326 

 Quartiles of Marginal Effects:
 
          V2        V3          V4         V5        V6          V7          V8
25% 2.042738 0.1041506 -0.22021433 -0.3386262 0.2054078 -0.02172834 -0.12942579
50% 2.169261 0.2026181 -0.15403018 -0.2714482 0.2923062  0.05717942 -0.02264092
75% 2.273275 0.3108885 -0.08947913 -0.1942964 0.4065404  0.08654908  0.05376487
            V9       V10
25% -0.3526023 0.3581693
50% -0.2372636 0.4619921
75% -0.1569604 0.5558847

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.90093217  0.27367112 -0.04061425 -0.20885056  0.16259544  0.08056563 
         V8          V9         V10 
-0.02833564 -0.19015784  0.50454801 

 Quartiles of Marginal Effects:
 
          V2          V3           V4          V5         V6         V7
25% 1.486047 -0.01926734 -0.417508093 -0.43936877 -0.1089923 -0.2056588
50% 1.903806  0.22927666 -0.008257978 -0.21311401  0.1350553  0.1323671
75% 2.270981  0.61129040  0.270008938  0.04109692  0.4906268  0.3604107
            V8          V9       V10
25% -0.3551680 -0.52699675 0.2091664
50%  0.0682825 -0.24929825 0.4417921
75%  0.2677921  0.08064495 0.8392529

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.65515279  0.27256525 -0.17414972 -0.19837684  0.40199576  0.02893160 
         V8          V9         V10 
-0.09069853 -0.26356181  0.32040732 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6          V7          V8
25% 2.576013 0.2016383 -0.2433450 -0.2660222 0.3000769 -0.05402007 -0.20487734
50% 2.649035 0.2895228 -0.1843709 -0.2027878 0.3925104  0.02140363 -0.09599957
75% 2.761246 0.3681543 -0.1063001 -0.1267587 0.5114728  0.11059279  0.01871660
            V9       V10
25% -0.4047872 0.2518055
50% -0.2706805 0.3173725
75% -0.1158472 0.4157998

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 1.621470012  0.102805549 -0.121360882 -0.282832166  0.207250672  0.039970939 
          V8           V9          V10 
-0.009875196 -0.183798254  0.440499902 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6         V7           V8
25% 1.620197 0.1020763 -0.1219415 -0.2836537 0.2065241 0.03945211 -0.010709911
50% 1.621767 0.1026819 -0.1214287 -0.2829856 0.2073645 0.04013946 -0.009847069
75% 1.622531 0.1036030 -0.1207966 -0.2822144 0.2080628 0.04038293 -0.009282339
            V9       V10
25% -0.1845019 0.4398541
50% -0.1839386 0.4407079
75% -0.1831070 0.4415260

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.96500812  0.07295968 -0.01667530 -0.10156785  0.03220551  0.10257432 
         V8          V9         V10 
-0.03599653 -0.07631349  0.32281246 

 Quartiles of Marginal Effects:
 
           V2         V3          V4          V5         V6         V7
25% 0.4265881 -0.2064145 -0.29869164 -0.33526503 -0.2402639 -0.2173797
50% 0.8106678  0.1111251 -0.01399782 -0.03238468  0.0850739  0.1240657
75% 1.2690607  0.2853715  0.25146321  0.14180062  0.2475364  0.4188402
             V8          V9         V10
25% -0.26129435 -0.32560163 -0.03380947
50% -0.05870879 -0.09079122  0.30135081
75%  0.17971151  0.15341452  0.63100379

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.0870579460  0.0009385025  0.0071357222  0.0073278797 -0.0056207180 
           V7            V8            V9           V10 
 0.0165724094 -0.0109496018  0.0144590828  0.0296861018 

 Quartiles of Marginal Effects:
 
             V2           V3           V4            V5           V6
25% 0.007635878 -0.030291255 -0.017204511 -0.0304600386 -0.021092380
50% 0.030413267 -0.002748387  0.000303058 -0.0006965473  0.001556965
75% 0.081451328  0.027225730  0.022671547  0.0355961851  0.030584600
              V7           V8           V9          V10
25% -0.018541093 -0.035524620 -0.029375554 -0.010803175
50%  0.003828072 -0.002331447 -0.001024686  0.009381531
75%  0.042337207  0.014094378  0.025547819  0.059884534

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.55851036  0.25376340 -0.17505737 -0.21488262  0.38156531  0.03139306 
         V8          V9         V10 
-0.07603542 -0.25789646  0.35167327 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6          V7          V8
25% 2.494765 0.1831636 -0.2406037 -0.2768504 0.2981535 -0.03499841 -0.16432037
50% 2.571076 0.2675122 -0.1777440 -0.2287689 0.3794935  0.03323940 -0.07072481
75% 2.638521 0.3461491 -0.1183350 -0.1503766 0.4871068  0.09660005  0.02916719
            V9       V10
25% -0.3791524 0.2813072
50% -0.2573041 0.3612114
75% -0.1327622 0.4297780

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.37510496  0.14796086 -0.03635162 -0.15404495  0.07079317  0.10539723 
         V8          V9         V10 
-0.02656128 -0.12760425  0.43287343 

 Quartiles of Marginal Effects:
 
           V2         V3          V4          V5         V6         V7
25% 0.8404183 -0.1711191 -0.40314185 -0.45674967 -0.2177802 -0.2872184
50% 1.3159173  0.1505356 -0.08758334 -0.07909905  0.0989703  0.1772280
75% 1.7794670  0.4392772  0.27829554  0.12957913  0.3280918  0.4486025
             V8         V9        V10
25% -0.33587615 -0.4470187 0.02554771
50% -0.08204484 -0.1120724 0.35892086
75%  0.24623161  0.1207286 0.83767335

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.36246682  0.35156409 -0.04912637 -0.23102138  0.26737420  0.06631128 
         V8          V9         V10 
-0.05030312 -0.25484519  0.47395259 

 Quartiles of Marginal Effects:
 
          V2         V3          V4          V5           V6          V7
25% 2.137972 0.08641671 -0.29861353 -0.41469247 -0.005496964 -0.19961286
50% 2.391696 0.33925324  0.01373673 -0.26438069  0.226890638  0.03287428
75% 2.604333 0.64147147  0.21434753 -0.02777855  0.536239546  0.33671549
             V8          V9       V10
25% -0.31645660 -0.56626124 0.2803608
50% -0.01922927 -0.34595563 0.5304974
75%  0.24405113  0.04529034 0.6894345

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.05428352  0.32573436 -0.04226894 -0.18781354  0.15058923  0.07414168 
         V8          V9         V10 
-0.04798068 -0.17504053  0.46426689 

 Quartiles of Marginal Effects:
 
          V2          V3          V4         V5          V6          V7
25% 1.638686 -0.02543973 -0.55973264 -0.4549912 -0.22563380 -0.27416077
50% 2.087266  0.30319163  0.06625984 -0.2660231  0.05422612  0.06576563
75% 2.427916  0.65801816  0.35232672  0.1217015  0.52506885  0.39804679
             V8         V9        V10
25% -0.46849632 -0.5728326 0.05419179
50%  0.03503253 -0.2127223 0.40834559
75%  0.31701392  0.1995909 0.81279186

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.57241421  0.40011275 -0.04107384 -0.22679333  0.29013164  0.07846783 
         V8          V9         V10 
-0.06272446 -0.28737407  0.43041559 

 Quartiles of Marginal Effects:
 
          V2        V3          V4          V5         V6          V7
25% 2.287331 0.1140120 -0.32574173 -0.46014858 0.02019947 -0.20685720
50% 2.525236 0.4011977 -0.05310846 -0.26003887 0.23236674  0.06531542
75% 2.915997 0.6711858  0.25226305 -0.02881549 0.56592489  0.41267497
             V8         V9       V10
25% -0.43048891 -0.6461085 0.2589250
50%  0.02110009 -0.4010853 0.4034219
75%  0.27569224  0.1455135 0.6412254

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.40830678  0.15309827 -0.04156092 -0.14915572  0.05441347  0.11009941 
         V8          V9         V10 
-0.02465343 -0.11617746  0.42614031 

 Quartiles of Marginal Effects:
 
           V2         V3          V4         V5          V6         V7
25% 0.8511148 -0.1836474 -0.40501300 -0.4781780 -0.23449996 -0.2934686
50% 1.2775508  0.1778057 -0.05833038 -0.1059619  0.08089089  0.1866066
75% 1.8070376  0.4534546  0.29250828  0.1491482  0.35870816  0.4798425
             V8         V9          V10
25% -0.32919544 -0.4369854 -0.004439486
50% -0.09278571 -0.1123056  0.354014817
75%  0.25484956  0.1360648  0.847722707

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.33601298  0.32583357 -0.13973144 -0.21561811  0.09803226  0.05724493 
         V8          V9         V10 
-0.11148692 -0.23314795  0.37569738 

 Quartiles of Marginal Effects:
 
          V2          V3         V4          V5         V6          V7
25% 1.635505 -0.08617799 -1.0178875 -0.67936366 -0.4741061 -0.45955438
50% 2.391497  0.45088719 -0.1219008 -0.08326595  0.1074214 -0.02735326
75% 2.835351  0.70345501  0.6446810  0.26965951  0.6770095  0.43976274
             V8         V9        V10
25% -0.65322812 -0.7907387 0.02005053
50% -0.01287106 -0.2395826 0.30159557
75%  0.53766474  0.3989964 0.73331180

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.33421018  0.18312224 -0.19237240 -0.24713050  0.34478961  0.03338563 
         V8          V9         V10 
-0.05088114 -0.23539949  0.37600409 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6         V7          V8
25% 2.318429 0.1605455 -0.2059873 -0.2655036 0.3201545 0.01986712 -0.07633243
50% 2.336970 0.1879644 -0.1922521 -0.2502985 0.3456014 0.03420146 -0.04922853
75% 2.364672 0.2085671 -0.1801894 -0.2273205 0.3711364 0.04710882 -0.02930301
            V9       V10
25% -0.2644188 0.3533473
50% -0.2333719 0.3799211
75% -0.2116537 0.3971708

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.11215886  0.09613349 -0.02520842 -0.11925537  0.03891056  0.10843998 
         V8          V9         V10 
-0.03223978 -0.09133485  0.36317576 

 Quartiles of Marginal Effects:
 
           V2         V3          V4          V5         V6         V7
25% 0.5346893 -0.1906250 -0.34635839 -0.37897232 -0.2435091 -0.2647094
50% 0.9742277  0.1056735 -0.06281182 -0.03802743  0.0811343  0.1505121
75% 1.4604582  0.3292672  0.25661719  0.13184809  0.3030748  0.4362729
             V8         V9         V10
25% -0.27824973 -0.3849746 -0.01992126
50% -0.08603093 -0.1247958  0.31380619
75%  0.21359294  0.1194509  0.73086563

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.76168401  0.14971145 -0.24852125 -0.18088535  0.15479617 -0.24366415 
         V8          V9         V10 
-0.04723714  0.13899091  0.18116806 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6         V7          V8
25% 1.751026 0.1455287 -0.2530130 -0.1890867 0.1470292 -0.2476451 -0.05480178
50% 1.765218 0.1492265 -0.2481295 -0.1832148 0.1575081 -0.2444738 -0.04783238
75% 1.773278 0.1548356 -0.2435266 -0.1756237 0.1652332 -0.2384579 -0.04156280
           V9       V10
25% 0.1271421 0.1748292
50% 0.1409437 0.1836259
75% 0.1537905 0.1873904

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.86270945  0.02600989 -0.23644715  0.00718298  0.30505221 -0.21005637 
         V8          V9         V10 
-0.12833975  0.43652465  0.01555304 

 Quartiles of Marginal Effects:
 
          V2          V3         V4         V5         V6         V7         V8
25% 1.457364 -0.19964092 -0.5464965 -0.4354182 -0.1530419 -0.5406246 -0.6981435
50% 1.862434  0.04527431 -0.2665909 -0.1175579  0.3320744 -0.2747586 -0.1701567
75% 2.444811  0.27072753  0.1596388  0.6688226  0.7206082  0.1389922  0.4507844
            V9         V10
25% -0.2684953 -0.51322152
50%  0.4609563 -0.02542441
75%  1.1682050  0.31791938

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.279047197  0.005982315 -0.048713044 -0.051498170  0.036329351 -0.035357078 
          V8           V9          V10 
-0.049137166  0.036370606  0.061513552 

 Quartiles of Marginal Effects:
 
            V2           V3          V4          V5          V6          V7
25% 0.07416331 -0.093117928 -0.13762499 -0.14075098 -0.08441270 -0.10335413
50% 0.19164786  0.009355424 -0.01360971 -0.02229284  0.01290822 -0.01079339
75% 0.36098993  0.128152217  0.06027097  0.06186233  0.16451734  0.07376144
             V8          V9         V10
25% -0.13195034 -0.06777673 -0.04196222
50% -0.04293903  0.02353540  0.04281515
75%  0.04678516  0.13265851  0.17020053

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.79139090  0.02215528 -0.13037097 -0.12357940  0.11810179 -0.11683825 
         V8          V9         V10 
-0.08237031  0.06466900  0.20530461 

 Quartiles of Marginal Effects:
 
           V2         V3         V4         V5        V6         V7          V8
25% 0.7878552 0.02031665 -0.1321088 -0.1257572 0.1169018 -0.1186032 -0.08461768
50% 0.7909308 0.02218435 -0.1303341 -0.1245592 0.1191285 -0.1171206 -0.08255080
75% 0.7950678 0.02422372 -0.1291467 -0.1218462 0.1200432 -0.1153551 -0.08030821
            V9       V10
25% 0.06313097 0.2031863
50% 0.06505857 0.2060652
75% 0.06631690 0.2082438

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.87499042  0.17855772 -0.26020609 -0.17709943  0.15226082 -0.25838400 
         V8          V9         V10 
-0.03141384  0.15545940  0.14863117 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6         V7          V8
25% 1.846853 0.1679117 -0.2726771 -0.2022028 0.1257986 -0.2708381 -0.05560471
50% 1.882587 0.1779004 -0.2582150 -0.1850128 0.1601502 -0.2596491 -0.03154371
75% 1.908664 0.1939142 -0.2458568 -0.1580455 0.1861981 -0.2428104 -0.01313320
           V9       V10
25% 0.1147020 0.1273275
50% 0.1627662 0.1529693
75% 0.2014440 0.1661674

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.76066767  0.17043398 -0.23930168 -0.16608486  0.16508169 -0.24688953 
         V8          V9         V10 
-0.03987805  0.16863228  0.14089104 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6         V7          V8
25% 1.642178 0.1284182 -0.2885325 -0.2461726 0.0890293 -0.2943445 -0.12022963
50% 1.785052 0.1619472 -0.2242076 -0.1811291 0.1959981 -0.2433724 -0.04917896
75% 1.880244 0.2328643 -0.1849848 -0.1084549 0.2582930 -0.1900838  0.03491112
            V9        V10
25% 0.03240711 0.06968279
50% 0.18702027 0.15387479
75% 0.33173091 0.20240764

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.49038840  0.11854009 -0.19122480 -0.12868386  0.20307726 -0.19967153 
         V8          V9         V10 
-0.06036055  0.23989031  0.13987710 

 Quartiles of Marginal Effects:
 
          V2          V3          V4         V5          V6          V7
25% 1.207195 -0.05017967 -0.43345909 -0.3787689 -0.03478768 -0.37832772
50% 1.447844  0.09017928 -0.16678521 -0.1260496  0.20812889 -0.13315763
75% 1.798218  0.33521466  0.07564279  0.1215009  0.45055719  0.02614054
             V8         V9        V10
25% -0.38340098 -0.1496074 -0.1104698
50% -0.02888559  0.2415691  0.1006466
75%  0.19815030  0.6345925  0.3595597

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.15910755  0.26490655 -0.28551815 -0.16647851  0.15147637 -0.28343833 
         V8          V9         V10 
 0.01977947  0.22292285  0.05054815 

 Quartiles of Marginal Effects:
 
          V2        V3         V4          V5          V6         V7
25% 2.040065 0.2174494 -0.3255972 -0.25727660 0.007430581 -0.3596622
50% 2.178158 0.2688312 -0.2901510 -0.17171093 0.168061102 -0.2750871
75% 2.274804 0.3196403 -0.2511241 -0.06072626 0.271185805 -0.2291296
             V8         V9         V10
25% -0.08723081 0.04278531 -0.01657469
50%  0.03203435 0.24716961  0.01859042
75%  0.11893960 0.40067005  0.13473362

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.34394938  0.07750154 -0.20067429 -0.17159732  0.15530071 -0.19002781 
         V8          V9         V10 
-0.08134721  0.10540768  0.23404492 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5        V6         V7          V8
25% 1.342943 0.07704382 -0.2011684 -0.1722749 0.1547632 -0.1904714 -0.08207835
50% 1.343932 0.07756985 -0.2006042 -0.1717862 0.1555118 -0.1901486 -0.08139910
75% 1.345040 0.07805699 -0.2002343 -0.1712408 0.1559677 -0.1895805 -0.08084646
           V9       V10
25% 0.1048956 0.2333636
50% 0.1055826 0.2343043
75% 0.1060141 0.2347415

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.66691667  0.02107453 -0.11073471 -0.10972645  0.09522587 -0.07972967 
         V8          V9         V10 
-0.07585056  0.10397780  0.11688383 

 Quartiles of Marginal Effects:
 
           V2           V3         V4          V5          V6           V7
25% 0.3109820 -0.201314146 -0.2378521 -0.27170154 -0.09540391 -0.233647491
50% 0.5523898  0.009285988 -0.0792772 -0.08602254  0.07432329 -0.004272623
75% 0.8756119  0.188596795  0.1329314  0.12615179  0.30815551  0.108626108
             V8          V9         V10
25% -0.27395305 -0.08960787 -0.04635259
50% -0.07213627  0.09450414  0.08708447
75%  0.08545108  0.29293594  0.29984118

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.046392450  0.001337848 -0.010657836 -0.010224490  0.005119633 -0.007074863 
          V8           V9          V10 
-0.010952110  0.004673133  0.011372604 

 Quartiles of Marginal Effects:
 
             V2          V3           V4           V5            V6
25% 0.003688549 -0.01845207 -0.021300825 -0.020463272 -0.0146145024
50% 0.018164511  0.00118924 -0.001097289 -0.006308516 -0.0004540552
75% 0.073949095  0.02087705  0.013223331  0.009955974  0.0349864144
              V7           V8           V9          V10
25% -0.020857374 -0.026571394 -0.024509635 -0.011413820
50%  0.002920269 -0.004248625  0.002117844  0.003404741
75%  0.015322221  0.003426301  0.012589784  0.040201955

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.086370099  0.245958806 -0.277907824 -0.166983285  0.153808268 -0.278276672 
          V8           V9          V10 
 0.007343269  0.210632680  0.070307522 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5         V6         V7          V8
25% 1.977602 0.2040100 -0.3132276 -0.2467929 0.02061715 -0.3459197 -0.08518783
50% 2.100191 0.2581327 -0.2718405 -0.1841382 0.16760194 -0.2626791  0.01062743
75% 2.211791 0.2940596 -0.2367006 -0.0726476 0.26332332 -0.2325670  0.09610083
            V9         V10
25% 0.05109738 0.004067117
50% 0.23370449 0.051353494
75% 0.37448783 0.148822551

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.00504416  0.04599566 -0.15050723 -0.13172719  0.14574801 -0.12754117 
         V8          V9         V10 
-0.08047790  0.16416784  0.14957046 

 Quartiles of Marginal Effects:
 
           V2          V3         V4         V5          V6          V7
25% 0.6341401 -0.20081049 -0.3481869 -0.3238471 -0.08738931 -0.34355710
50% 0.9597520  0.01928152 -0.1013037 -0.1166259  0.09862591 -0.09013713
75% 1.2629232  0.24992549  0.1454469  0.1473295  0.41316444  0.10534672
             V8          V9         V10
25% -0.31865796 -0.04226251 -0.06928569
50% -0.04573087  0.18112198  0.12868183
75%  0.19188403  0.43339665  0.32348436

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.92138885  0.22524922 -0.23093807 -0.13668370  0.21696395 -0.24505110 
         V8          V9         V10 
-0.01543051  0.27220682  0.05977928 

 Quartiles of Marginal Effects:
 
          V2        V3          V4          V5         V6          V7
25% 1.590333 0.1062117 -0.44520074 -0.36370059 0.02033564 -0.47966588
50% 1.972922 0.2543516 -0.20509373 -0.17283303 0.23686094 -0.21828218
75% 2.228854 0.3533576 -0.07186836  0.09819397 0.45921323 -0.05973905
             V8         V9          V10
25% -0.32774755 -0.1477871 -0.188422366
50% -0.01897147  0.3431389  0.006784257
75%  0.23530241  0.7252595  0.228148283

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.60519208  0.11179379 -0.21564898 -0.10828097  0.21988474 -0.19470706 
         V8          V9         V10 
-0.05983629  0.30044631  0.12164284 

 Quartiles of Marginal Effects:
 
          V2          V3          V4         V5          V6          V7
25% 1.269542 -0.05347035 -0.43127773 -0.4642877 -0.04912711 -0.47284695
50% 1.604231  0.08297492 -0.24310537 -0.1327929  0.28153775 -0.20707846
75% 2.004382  0.31350224  0.08265108  0.2891471  0.49826078  0.08328213
              V8         V9         V10
25% -0.452395922 -0.2383864 -0.18180242
50%  0.006955089  0.3495371  0.04706121
75%  0.334589051  0.8443234  0.34029463

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.099215608  0.276069749 -0.241660994 -0.143845388  0.226070161 -0.245644071 
          V8           V9          V10 
 0.005140726  0.295834295  0.005951819 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6          V7         V8
25% 1.768582 0.1370657 -0.3647639 -0.4031109 0.0289837 -0.53470969 -0.3366866
50% 2.157053 0.3055209 -0.2634047 -0.1747310 0.2545916 -0.25224575 -0.0418115
75% 2.372665 0.4113206 -0.0952833  0.1344613 0.4423961 -0.04108708  0.3057156
            V9        V10
25% -0.2144230 -0.2283630
50%  0.3968177 -0.0988584
75%  0.7497348  0.2706625

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.01967581  0.04677461 -0.16531725 -0.13667076  0.14044857 -0.12048036 
         V8          V9         V10 
-0.07650130  0.17729489  0.14089014 

 Quartiles of Marginal Effects:
 
           V2          V3         V4         V5         V6          V7
25% 0.6481903 -0.20348156 -0.3740079 -0.3550738 -0.1157245 -0.36193247
50% 0.9767775  0.01434899 -0.1188571 -0.1128988  0.1051877 -0.08782144
75% 1.2893304  0.25084620  0.1485503  0.1653757  0.4130783  0.12397824
             V8          V9         V10
25% -0.30946964 -0.04030545 -0.06474201
50% -0.03811539  0.18665184  0.11300202
75%  0.20952480  0.46936096  0.31905814

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.85306220  0.05951129 -0.23390104 -0.01953373  0.28941685 -0.20757112 
         V8          V9         V10 
-0.10543358  0.41261536  0.03297634 

 Quartiles of Marginal Effects:
 
          V2          V3         V4         V5         V6         V7         V8
25% 1.474573 -0.19282239 -0.5049984 -0.4117023 -0.1036012 -0.5671971 -0.6529845
50% 1.850655  0.05932486 -0.2781539 -0.1261476  0.2894025 -0.2793148 -0.1057068
75% 2.420113  0.26364808  0.1232761  0.5896140  0.6567028  0.1390648  0.4431849
            V9         V10
25% -0.2735961 -0.47863801
50%  0.4460140 -0.02602805
75%  1.0873880  0.27608678

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.91526893  0.18681514 -0.26535955 -0.17711123  0.14988274 -0.26322286 
         V8          V9         V10 
-0.02632065  0.15811020  0.14074068 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6         V7          V8
25% 1.890016 0.1775270 -0.2766175 -0.1996451 0.1245759 -0.2753660 -0.04930359
50% 1.920306 0.1859736 -0.2632862 -0.1839697 0.1574623 -0.2635062 -0.02503377
75% 1.950186 0.2002216 -0.2527070 -0.1594486 0.1812748 -0.2505204 -0.01035802
           V9       V10
25% 0.1205289 0.1227997
50% 0.1648802 0.1436365
75% 0.1982473 0.1556320

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.78063557  0.02815789 -0.12848826 -0.12173382  0.11095892 -0.09308340 
         V8          V9         V10 
-0.07796008  0.12644725  0.12757276 

 Quartiles of Marginal Effects:
 
           V2           V3         V4          V5          V6          V7
25% 0.4277792 -0.207753752 -0.2708946 -0.29192723 -0.10827311 -0.26083165
50% 0.7000614  0.001370052 -0.1089840 -0.09870414  0.07977278 -0.01582642
75% 1.0359406  0.198270372  0.1509887  0.14501943  0.34238066  0.12324550
             V8          V9         V10
25% -0.27752851 -0.09669194 -0.06175742
50% -0.05928522  0.12786466  0.10321970
75%  0.11235113  0.34546923  0.29282925

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.068398129  0.194471962 -0.166030139 -0.199269623  0.279631464 -0.027161572 
          V8           V9          V10 
 0.003425461 -0.253695637  0.205331391 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5        V6          V7           V8
25% 2.060103 0.1864577 -0.1711272 -0.2065518 0.2696176 -0.03279598 -0.006653478
50% 2.068182 0.1952515 -0.1668985 -0.2021300 0.2816474 -0.02705489  0.001167856
75% 2.080916 0.2042704 -0.1585544 -0.1922519 0.2886426 -0.02034428  0.013263205
            V9       V10
25% -0.2647177 0.1988795
50% -0.2539026 0.2036122
75% -0.2427829 0.2111545
Radial Basis Function Kernel Regularized Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  lambda        sigma        RMSE       Rsquared   MAE        Selected
  1.650648e-05     4.236986  0.7475998  0.3286230  0.4597064          
  1.699692e-05     2.672439  0.7857062  0.2219195  0.4642886          
  2.540284e-05     6.243887  0.7232279  0.3960692  0.4707210          
  5.181738e-05  3398.828337  0.5542125  0.6597022  0.4317935          
  7.954201e-05  1001.944349  0.5975336  0.5828805  0.4583672          
  9.103906e-05    57.281897  0.7378960  0.3742458  0.5407952          
  6.379071e-04    49.472074  0.7105620  0.4134089  0.5153926          
  1.033434e-03     4.783890  0.7391711  0.3530949  0.4634291          
  2.581065e-03     1.490086  0.8229445  0.1114310  0.5088322          
  7.543966e-03   291.842098  0.5534306  0.6578163  0.4228740          
  1.464619e-02  4045.684451  0.5652580  0.6630459  0.3500828          
  2.058621e-02    16.603266  0.6748670  0.4781624  0.4812585          
  2.674679e-02    50.216726  0.6079067  0.5719398  0.4517859          
  3.285601e-02   341.184047  0.5357071  0.6755111  0.3877991          
  3.396054e-02   619.530172  0.5348820  0.6765775  0.3703748  *       
  4.758591e-02   269.528631  0.5373789  0.6733398  0.3846462          
  6.636577e-02     6.360406  0.7166041  0.4084472  0.4635451          
  9.018155e-02    13.391027  0.6639381  0.4993504  0.4633644          
  1.806876e-01    60.395818  0.5649738  0.6402147  0.3955343          
  2.553831e-01   662.508842  0.6504202  0.6328177  0.3841890          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 0.03396054 and sigma
 = 619.5302.
[1] "Sat Mar 10 03:35:14 2018"
Least Angle Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  fraction    RMSE       Rsquared   MAE        Selected
  0.04353090  0.7894393  0.7447680  0.4833673          
  0.04607404  0.7868251  0.7447680  0.4809220          
  0.08097647  0.7513658  0.7447680  0.4473631          
  0.14289509  0.6907266  0.7447680  0.3902429          
  0.18011931  0.6560221  0.7447680  0.3591234          
  0.19184555  0.6454181  0.7447680  0.3499463          
  0.36095149  0.5189457  0.7443023  0.2888249          
  0.40285652  0.5049360  0.7394082  0.2969867          
  0.48235978  0.4906201  0.7289160  0.3243605  *       
  0.57551994  0.4913173  0.7170696  0.3602676          
  0.63314495  0.4989635  0.7113115  0.3760916          
  0.66271528  0.5003379  0.7112893  0.3806667          
  0.68545433  0.5009581  0.7116095  0.3833657          
  0.70332297  0.5013868  0.7119156  0.3853897          
  0.70619491  0.5014411  0.7119775  0.3857105          
  0.73549568  0.5021976  0.7125865  0.3888249          
  0.76438883  0.5046186  0.7112417  0.3925921          
  0.79102354  0.5067162  0.7098573  0.3955074          
  0.85138569  0.5128111  0.7055647  0.4025673          
  0.88143842  0.5167156  0.7025353  0.4062776          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.4823598.
[1] "Sat Mar 10 03:35:28 2018"
Least Angle Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  step  RMSE       Rsquared   MAE        Selected
  1     0.8347529        NaN  0.5276532          
  2     0.4952516  0.7447680  0.3376477  *       
  4     0.4991646  0.7160523  0.3699150          
  5     0.4996383  0.7155447  0.3739551          
  6     0.5014093  0.7147850  0.3807253          
  7     0.5043036  0.7145081  0.3877253          
  8     0.5041778  0.7134324  0.3911709          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was step = 2.
[1] "Sat Mar 10 03:35:42 2018"
The lasso 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  fraction    RMSE       Rsquared   MAE        Selected
  0.04353090  0.7894393  0.7447680  0.4833673          
  0.04607404  0.7868251  0.7447680  0.4809220          
  0.08097647  0.7513658  0.7447680  0.4473631          
  0.14289509  0.6907266  0.7447680  0.3902429          
  0.18011931  0.6560221  0.7447680  0.3591234          
  0.19184555  0.6454181  0.7447680  0.3499463          
  0.36095149  0.5189457  0.7443023  0.2888249          
  0.40285652  0.5049360  0.7394082  0.2969867          
  0.48235978  0.4906201  0.7289160  0.3243605  *       
  0.57551994  0.4913173  0.7170696  0.3602676          
  0.63314495  0.4989635  0.7113115  0.3760916          
  0.66271528  0.5003379  0.7112893  0.3806667          
  0.68545433  0.5009581  0.7116095  0.3833657          
  0.70332297  0.5013868  0.7119156  0.3853897          
  0.70619491  0.5014411  0.7119775  0.3857105          
  0.73549568  0.5021976  0.7125865  0.3888249          
  0.76438883  0.5046186  0.7112417  0.3925921          
  0.79102354  0.5067162  0.7098573  0.3955074          
  0.85138569  0.5128111  0.7055647  0.4025673          
  0.88143842  0.5167156  0.7025353  0.4062776          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.4823598.
[1] "Sat Mar 10 03:35:56 2018"
Linear Regression with Backwards Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  nvmax  RMSE       Rsquared   MAE        Selected
  2      0.5318764  0.6917433  0.4226150          
  3      0.5438442  0.6772300  0.4318044          
  4      0.5440809  0.6713743  0.4374923          
  5      0.5450647  0.6675758  0.4342179          
  6      0.5381833  0.6799304  0.4296620          
  7      0.5287365  0.6941704  0.4176382  *       
  8      0.5317494  0.6921586  0.4193579          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nvmax = 7.
[1] "Sat Mar 10 03:36:10 2018"
Linear Regression with Forward Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  nvmax  RMSE       Rsquared   MAE        Selected
  2      0.5318764  0.6917433  0.4226150          
  3      0.5438442  0.6772300  0.4318044          
  4      0.5440809  0.6713743  0.4374923          
  5      0.5450647  0.6675758  0.4342179          
  6      0.5381833  0.6799304  0.4296620          
  7      0.5287365  0.6941704  0.4176382  *       
  8      0.5317494  0.6921586  0.4193579          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nvmax = 7.
[1] "Sat Mar 10 03:36:23 2018"
Linear Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results:

  RMSE       Rsquared   MAE      
  0.5359023  0.6860927  0.4230051

Tuning parameter 'intercept' was held constant at a value of TRUE
[1] "Sat Mar 10 03:36:37 2018"
Start:  AIC=-77.15
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq     RSS     AIC
- V10   1    0.0297  6.7771 -78.936
- V8    1    0.0748  6.8222 -78.610
- V5    1    0.1355  6.8828 -78.177
- V4    1    0.1596  6.9070 -78.005
- V7    1    0.1886  6.9359 -77.800
- V3    1    0.2153  6.9627 -77.612
- V6    1    0.2444  6.9918 -77.407
<none>               6.7474 -77.151
- V9    1    1.9415  8.6889 -66.759
- V2    1   18.8246 25.5720 -13.866

Step:  AIC=-78.94
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9

       Df Sum of Sq     RSS     AIC
- V8    1    0.0656  6.8426 -80.464
- V5    1    0.1242  6.9013 -80.046
- V4    1    0.1453  6.9224 -79.896
- V7    1    0.1666  6.9436 -79.746
- V3    1    0.2388  7.0158 -79.239
- V6    1    0.2511  7.0282 -79.153
<none>               6.7771 -78.936
- V9    1    1.9280  8.7051 -68.668
- V2    1   18.8879 25.6650 -15.688

Step:  AIC=-80.46
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9

       Df Sum of Sq     RSS     AIC
- V5    1    0.1261  6.9687 -81.569
- V4    1    0.1357  6.9783 -81.502
- V7    1    0.1451  6.9877 -81.435
- V6    1    0.2421  7.0847 -80.760
- V3    1    0.2466  7.0892 -80.729
<none>               6.8426 -80.464
- V9    1    1.8875  8.7302 -70.527
- V2    1   19.3600 26.2026 -16.672

Step:  AIC=-81.57
.outcome ~ V2 + V3 + V4 + V6 + V7 + V9

       Df Sum of Sq     RSS     AIC
- V3    1    0.1898  7.1585 -82.253
- V7    1    0.2105  7.1792 -82.111
- V6    1    0.2151  7.1838 -82.079
- V4    1    0.2236  7.1923 -82.022
<none>               6.9687 -81.569
- V9    1    2.1142  9.0829 -70.586
- V2    1   20.0380 27.0067 -17.191

Step:  AIC=-82.25
.outcome ~ V2 + V4 + V6 + V7 + V9

       Df Sum of Sq     RSS     AIC
- V6    1    0.1660  7.3245 -83.129
- V4    1    0.2551  7.4135 -82.537
<none>               7.1585 -82.253
- V7    1    0.2997  7.4582 -82.243
- V9    1    2.3314  9.4898 -70.438
- V2    1   20.4724 27.6309 -18.071

Step:  AIC=-83.13
.outcome ~ V2 + V4 + V7 + V9

       Df Sum of Sq     RSS     AIC
- V4    1    0.2583  7.5827 -83.431
<none>               7.3245 -83.129
- V7    1    0.4349  7.7593 -82.303
- V9    1    2.3991  9.7235 -71.246
- V2    1   20.9544 28.2789 -18.936

Step:  AIC=-83.43
.outcome ~ V2 + V7 + V9

       Df Sum of Sq     RSS     AIC
<none>               7.5827 -83.431
- V7    1    0.3814  7.9641 -83.027
- V9    1    2.2406  9.8233 -72.746
- V2    1   20.9107 28.4934 -20.565
Start:  AIC=-67.51
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V7    1    0.0029  9.174 -69.490
- V8    1    0.0396  9.210 -69.286
- V5    1    0.1147  9.285 -68.872
- V3    1    0.1467  9.317 -68.697
- V4    1    0.1753  9.346 -68.540
- V10   1    0.1953  9.366 -68.431
- V9    1    0.2432  9.414 -68.171
<none>               9.171 -67.506
- V6    1    0.7258  9.897 -65.621
- V2    1   27.0439 36.215   0.540

Step:  AIC=-69.49
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V8    1    0.0429  9.217 -71.252
- V5    1    0.1139  9.288 -70.861
- V3    1    0.1440  9.318 -70.696
- V10   1    0.1986  9.372 -70.398
- V4    1    0.2013  9.375 -70.383
- V9    1    0.2496  9.423 -70.121
<none>               9.174 -69.490
- V6    1    0.7241  9.898 -67.615
- V2    1   27.0462 36.220  -1.453

Step:  AIC=-71.25
.outcome ~ V2 + V3 + V4 + V5 + V6 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V5    1    0.1370  9.354 -72.500
- V3    1    0.1707  9.387 -72.316
- V4    1    0.2087  9.425 -72.110
- V9    1    0.2609  9.477 -71.828
- V10   1    0.2756  9.492 -71.749
<none>               9.217 -71.252
- V6    1    0.6960  9.913 -69.539
- V2    1   27.4662 36.683  -2.805

Step:  AIC=-72.5
.outcome ~ V2 + V3 + V4 + V6 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V4    1    0.1363  9.490 -73.762
- V3    1    0.1845  9.538 -73.504
- V10   1    0.2474  9.601 -73.168
- V9    1    0.2986  9.652 -72.897
<none>               9.354 -72.500
- V6    1    0.6987 10.052 -70.826
- V2    1   30.2803 39.634  -0.859

Step:  AIC=-73.76
.outcome ~ V2 + V3 + V6 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V9    1    0.2683  9.758 -74.340
- V10   1    0.3049  9.795 -74.149
- V3    1    0.3249  9.815 -74.045
<none>               9.490 -73.762
- V6    1    0.6996 10.189 -72.135
- V2    1   30.1573 39.647  -2.842

Step:  AIC=-74.34
.outcome ~ V2 + V3 + V6 + V10

       Df Sum of Sq    RSS     AIC
- V10   1    0.2503 10.008 -75.048
- V3    1    0.3772 10.135 -74.406
<none>               9.758 -74.340
- V6    1    0.8828 10.641 -71.923
- V2    1   30.6693 40.427  -3.848

Step:  AIC=-75.05
.outcome ~ V2 + V3 + V6

       Df Sum of Sq    RSS     AIC
<none>              10.008 -75.048
- V3    1     0.453 10.461 -74.792
- V6    1     1.160 11.169 -71.454
- V2    1    37.244 47.252   2.107
Start:  AIC=-82.53
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq     RSS     AIC
- V8    1    0.0008  7.2398 -84.526
- V10   1    0.0187  7.2577 -84.398
- V6    1    0.0527  7.2916 -84.155
- V9    1    0.0990  7.3379 -83.826
- V5    1    0.1236  7.3625 -83.652
- V3    1    0.2251  7.4641 -82.939
<none>               7.2389 -82.532
- V4    1    0.3833  7.6222 -81.849
- V7    1    0.3954  7.6344 -81.766
- V2    1   18.3431 25.5821 -18.886

Step:  AIC=-84.53
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9 + V10

       Df Sum of Sq     RSS     AIC
- V10   1    0.0187  7.2584 -86.392
- V6    1    0.0532  7.2929 -86.146
- V9    1    0.0983  7.3380 -85.825
- V5    1    0.1228  7.3626 -85.651
- V3    1    0.2262  7.4659 -84.927
<none>               7.2398 -84.526
- V4    1    0.3826  7.6224 -83.848
- V7    1    0.4093  7.6490 -83.667
- V2    1   18.8058 26.0455 -19.953

Step:  AIC=-86.39
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9

       Df Sum of Sq     RSS     AIC
- V6    1    0.0499  7.3084 -88.036
- V9    1    0.1027  7.3612 -87.661
- V5    1    0.1123  7.3708 -87.594
- V3    1    0.2443  7.5028 -86.671
<none>               7.2584 -86.392
- V4    1    0.4310  7.6894 -85.393
- V7    1    0.4364  7.6948 -85.356
- V2    1   21.4639 28.7224 -16.866

Step:  AIC=-88.04
.outcome ~ V2 + V3 + V4 + V5 + V7 + V9

       Df Sum of Sq     RSS     AIC
- V5    1    0.1287  7.4370 -89.128
- V9    1    0.1365  7.4449 -89.073
<none>               7.3084 -88.036
- V3    1    0.2908  7.5992 -88.006
- V4    1    0.4476  7.7560 -86.944
- V7    1    0.4986  7.8070 -86.604
- V2    1   21.6759 28.9843 -18.393

Step:  AIC=-89.13
.outcome ~ V2 + V3 + V4 + V7 + V9

       Df Sum of Sq     RSS     AIC
- V9    1    0.1109  7.5480 -90.358
- V3    1    0.2442  7.6813 -89.448
<none>               7.4370 -89.128
- V4    1    0.4118  7.8489 -88.325
- V7    1    0.5054  7.9424 -87.710
- V2    1   22.5916 30.0287 -18.553

Step:  AIC=-90.36
.outcome ~ V2 + V3 + V4 + V7

       Df Sum of Sq     RSS     AIC
- V3    1    0.2571  7.8051 -90.616
<none>               7.5480 -90.358
- V4    1    0.3844  7.9324 -89.775
- V7    1    0.4550  8.0029 -89.315
- V2    1   22.7801 30.3281 -20.037

Step:  AIC=-90.62
.outcome ~ V2 + V4 + V7

       Df Sum of Sq     RSS     AIC
<none>               7.8051 -90.616
- V4    1    0.3888  8.1939 -90.088
- V7    1    0.3927  8.1978 -90.064
- V2    1   22.5286 30.3337 -22.027
Start:  AIC=-113.52
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS      AIC
- V8    1     0.000 13.117 -115.520
- V7    1     0.009 13.125 -115.470
- V10   1     0.107 13.224 -114.904
- V5    1     0.189 13.306 -114.433
- V4    1     0.273 13.390 -113.955
<none>              13.116 -113.522
- V3    1     0.419 13.535 -113.132
- V9    1     0.506 13.623 -112.644
- V6    1     0.608 13.724 -112.080
- V2    1    34.616 47.733  -17.349

Step:  AIC=-115.52
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V9 + V10

       Df Sum of Sq    RSS      AIC
- V7    1     0.010 13.126 -117.465
- V10   1     0.107 13.224 -116.904
- V5    1     0.189 13.306 -116.432
- V4    1     0.273 13.390 -115.955
<none>              13.117 -115.520
- V3    1     0.421 13.538 -115.117
- V9    1     0.506 13.623 -114.644
- V6    1     0.608 13.724 -114.079
- V2    1    34.619 47.735  -19.345

Step:  AIC=-117.47
.outcome ~ V2 + V3 + V4 + V5 + V6 + V9 + V10

       Df Sum of Sq    RSS      AIC
- V10   1     0.104 13.231 -118.863
- V5    1     0.188 13.314 -118.383
- V4    1     0.268 13.394 -117.932
<none>              13.126 -117.465
- V3    1     0.414 13.541 -117.103
- V9    1     0.526 13.652 -116.478
- V6    1     0.605 13.731 -116.043
- V2    1    34.614 47.740  -21.337

Step:  AIC=-118.86
.outcome ~ V2 + V3 + V4 + V5 + V6 + V9

       Df Sum of Sq    RSS      AIC
- V5    1     0.182 13.412 -119.827
- V4    1     0.330 13.561 -118.988
<none>              13.231 -118.863
- V3    1     0.426 13.657 -118.452
- V9    1     0.513 13.744 -117.972
- V6    1     0.645 13.876 -117.244
- V2    1    38.583 51.814  -17.113

Step:  AIC=-119.83
.outcome ~ V2 + V3 + V4 + V6 + V9

       Df Sum of Sq    RSS      AIC
- V4    1     0.310 13.722 -120.091
<none>              13.412 -119.827
- V3    1     0.381 13.793 -119.699
- V9    1     0.584 13.997 -118.586
- V6    1     0.648 14.061 -118.239
- V2    1    40.300 53.712  -16.379

Step:  AIC=-120.09
.outcome ~ V2 + V3 + V6 + V9

       Df Sum of Sq    RSS      AIC
<none>              13.722 -120.091
- V3    1     0.483 14.205 -119.462
- V9    1     0.546 14.268 -119.128
- V6    1     0.652 14.374 -118.563
- V2    1    40.065 53.787  -18.273
Linear Regression with Stepwise Selection 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results:

  RMSE       Rsquared  MAE      
  0.5438442  0.67723   0.4318044

[1] "Sat Mar 10 03:36:51 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:37:25 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "logreg"                  
Error : package RWeka is required
Error : package RWeka is required
Error : package RWeka is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:37:38 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "M5"                      
Error : package RWeka is required
Error : package RWeka is required
Error : package RWeka is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:37:53 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "M5Rules"                 
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:38:07 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "mlpKerasDecay"           
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:38:21 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "mlpKerasDropout"         
Multi-Step Adaptive MCP-Net 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  alphas      nsteps  scale      RMSE       Rsquared   MAE        Selected
  0.08917781   9      3.2534530  0.4787337  0.7447680  0.3809729  *       
  0.09146663  10      1.5000699  0.5055684  0.7067780  0.4014860          
  0.12287882   9      1.2171555  0.5064540  0.7059561  0.4025391          
  0.17860558   3      1.0156179  0.5069941  0.7055649  0.4033526          
  0.21210738   4      2.2617969  0.4791353  0.7447680  0.3823392          
  0.22266100   7      3.8155988  0.4791495  0.7447680  0.3823865          
  0.37485634   7      0.4791244  0.5075091  0.7052632  0.4042124          
  0.41257086   9      3.3986092  0.4792823  0.7447680  0.3828243          
  0.48412380  10      3.3769594  0.4793056  0.7447680  0.3829003          
  0.56796795   5      3.3374317  0.4793255  0.7447680  0.3829650          
  0.61983045   2      1.8060319  0.4793351  0.7447680  0.3829963          
  0.64644375   8      0.5805751  0.4793394  0.7447680  0.3830104          
  0.66690890   7      3.0686618  0.4793425  0.7447680  0.3830205          
  0.68299067   5      2.5557985  0.4793448  0.7447680  0.3830280          
  0.68557542   4      1.0026949  0.4793452  0.7447680  0.3830292          
  0.71194611   5      0.9858223  0.4793487  0.7447680  0.3830406          
  0.73794995   9      0.3538271  0.4793519  0.7447680  0.3830512          
  0.76192119   8      3.6747353  0.4793547  0.7447680  0.3830602          
  0.81624712   6      1.2302132  0.4793605  0.7447680  0.3830788          
  0.84329458   4      1.8257777  0.4793630  0.7447680  0.3830871          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alphas = 0.08917781, nsteps = 9
 and scale = 3.253453.
[1] "Sat Mar 10 03:38:41 2018"
# weights:  166
initial  value 44.338621 
iter  10 value 31.179891
iter  20 value 31.109880
iter  30 value 26.216567
iter  40 value 21.688203
iter  50 value 21.422836
iter  60 value 21.331351
iter  70 value 21.281467
iter  80 value 21.254161
iter  90 value 21.245739
iter 100 value 21.242225
final  value 21.242225 
stopped after 100 iterations
# weights:  45
initial  value 53.094050 
iter  10 value 31.673295
iter  20 value 30.765275
iter  30 value 24.624811
iter  40 value 24.407559
iter  50 value 24.403735
iter  60 value 24.402074
iter  70 value 24.397209
iter  80 value 24.391371
iter  90 value 24.390993
final  value 24.390989 
converged
# weights:  12
initial  value 43.246781 
iter  10 value 37.146831
final  value 37.146798 
converged
# weights:  199
initial  value 61.160681 
iter  10 value 31.183424
iter  20 value 26.115476
iter  30 value 22.046026
iter  40 value 21.463595
iter  50 value 21.419861
iter  60 value 21.365946
iter  70 value 21.314349
iter  80 value 21.270965
iter  90 value 21.258400
iter 100 value 21.253024
final  value 21.253024 
stopped after 100 iterations
# weights:  166
initial  value 54.552231 
iter  10 value 31.315458
iter  20 value 31.175974
iter  30 value 31.154275
iter  40 value 30.239977
iter  50 value 22.682777
iter  60 value 21.860373
iter  70 value 21.777695
iter  80 value 21.706012
iter  90 value 21.632199
iter 100 value 21.582803
final  value 21.582803 
stopped after 100 iterations
# weights:  199
initial  value 48.532277 
iter  10 value 31.279816
iter  20 value 29.700135
iter  30 value 23.937048
iter  40 value 23.635085
iter  50 value 23.592763
iter  60 value 23.582595
iter  70 value 23.579870
iter  80 value 23.579635
iter  90 value 23.579546
final  value 23.579530 
converged
# weights:  177
initial  value 46.925005 
iter  10 value 31.768899
final  value 31.703420 
converged
# weights:  34
initial  value 40.756947 
iter  10 value 31.156911
iter  20 value 23.594806
iter  30 value 21.980456
iter  40 value 21.275264
iter  50 value 21.068625
iter  60 value 21.049676
iter  70 value 21.035675
iter  80 value 21.030021
iter  90 value 21.013619
iter 100 value 21.008987
final  value 21.008987 
stopped after 100 iterations
# weights:  144
initial  value 57.050629 
iter  10 value 31.156054
iter  20 value 30.980768
iter  30 value 22.354350
iter  40 value 21.185364
iter  50 value 21.082421
iter  60 value 21.055974
iter  70 value 21.040497
iter  80 value 21.021748
iter  90 value 21.011774
iter 100 value 21.007107
final  value 21.007107 
stopped after 100 iterations
# weights:  12
initial  value 47.399519 
iter  10 value 35.334283
final  value 35.334282 
converged
# weights:  111
initial  value 130.548346 
iter  10 value 36.807072
iter  20 value 36.791253
final  value 36.791251 
converged
# weights:  133
initial  value 54.509882 
iter  10 value 31.256504
iter  20 value 31.175668
iter  30 value 31.149470
iter  40 value 29.805934
iter  50 value 23.807656
iter  60 value 21.939253
iter  70 value 21.660942
iter  80 value 21.549565
iter  90 value 21.512982
iter 100 value 21.494951
final  value 21.494951 
stopped after 100 iterations
# weights:  177
initial  value 55.970990 
iter  10 value 32.449914
final  value 32.422010 
converged
# weights:  155
initial  value 58.511470 
iter  10 value 31.324571
iter  20 value 28.571944
iter  30 value 24.956573
iter  40 value 24.318589
iter  50 value 24.185846
iter  60 value 24.132085
iter  70 value 24.125487
iter  80 value 24.124036
iter  90 value 24.123386
iter 100 value 24.123209
final  value 24.123209 
stopped after 100 iterations
# weights:  155
initial  value 48.333305 
iter  10 value 31.678361
iter  20 value 31.532720
iter  30 value 31.118627
iter  40 value 29.557904
iter  50 value 29.438357
iter  60 value 29.435201
final  value 29.434967 
converged
# weights:  45
initial  value 46.773077 
iter  10 value 31.165126
iter  20 value 22.834843
iter  30 value 21.452377
iter  40 value 21.173450
iter  50 value 21.141633
iter  60 value 21.129590
iter  70 value 21.122040
iter  80 value 21.116292
iter  90 value 21.113374
iter 100 value 21.112040
final  value 21.112040 
stopped after 100 iterations
# weights:  23
initial  value 47.493358 
iter  10 value 33.771589
final  value 33.771586 
converged
# weights:  89
initial  value 56.017901 
iter  10 value 31.216233
iter  20 value 25.328691
iter  30 value 24.561466
iter  40 value 24.255809
iter  50 value 24.244116
iter  60 value 24.242515
iter  70 value 24.242301
final  value 24.242274 
converged
# weights:  166
initial  value 35.651490 
iter  10 value 31.176845
iter  20 value 31.160297
iter  30 value 31.104444
iter  40 value 23.321865
iter  50 value 21.623592
iter  60 value 21.529220
iter  70 value 21.472059
iter  80 value 21.434820
iter  90 value 21.419234
iter 100 value 21.409335
final  value 21.409335 
stopped after 100 iterations
# weights:  100
initial  value 67.251469 
iter  10 value 33.333106
final  value 33.324995 
converged
# weights:  166
initial  value 71.181418 
iter  10 value 48.174183
iter  20 value 48.128162
iter  30 value 47.365267
iter  40 value 34.291212
iter  50 value 34.158223
iter  60 value 34.134055
iter  70 value 34.125983
iter  80 value 34.122253
iter  90 value 34.116920
iter 100 value 34.112236
final  value 34.112236 
stopped after 100 iterations
# weights:  45
initial  value 68.621299 
iter  10 value 48.693651
iter  20 value 47.229859
iter  30 value 37.677828
iter  40 value 37.345012
iter  50 value 36.919863
iter  60 value 36.792150
iter  70 value 36.732591
iter  80 value 36.704994
iter  90 value 36.702336
iter 100 value 36.702151
final  value 36.702151 
stopped after 100 iterations
# weights:  12
initial  value 65.863477 
iter  10 value 55.376002
final  value 55.375095 
converged
# weights:  199
initial  value 79.804136 
iter  10 value 48.168980
iter  20 value 48.122894
iter  30 value 34.376945
iter  40 value 34.175166
iter  50 value 34.131689
iter  60 value 34.099367
iter  70 value 34.068607
iter  80 value 34.041309
iter  90 value 33.860514
iter 100 value 33.788202
final  value 33.788202 
stopped after 100 iterations
# weights:  166
initial  value 71.320365 
iter  10 value 48.269303
iter  20 value 44.928259
iter  30 value 34.698596
iter  40 value 34.610065
iter  50 value 34.591704
iter  60 value 34.581336
iter  70 value 34.459899
iter  80 value 34.310578
iter  90 value 34.259379
iter 100 value 34.234597
final  value 34.234597 
stopped after 100 iterations
# weights:  199
initial  value 90.732561 
iter  10 value 48.376016
iter  20 value 42.194943
iter  30 value 36.868540
iter  40 value 36.504526
iter  50 value 36.400686
iter  60 value 36.380707
iter  70 value 36.373084
iter  80 value 36.362995
iter  90 value 36.355124
iter 100 value 36.350544
final  value 36.350544 
stopped after 100 iterations
# weights:  177
initial  value 86.780062 
iter  10 value 48.969106
iter  20 value 48.825180
final  value 48.825179 
converged
# weights:  34
initial  value 65.900309 
iter  10 value 48.122984
iter  20 value 48.120747
iter  30 value 48.120364
iter  40 value 48.119870
iter  50 value 48.119211
iter  60 value 48.118293
iter  70 value 48.116935
iter  80 value 48.114768
iter  90 value 48.110919
iter 100 value 48.102960
final  value 48.102960 
stopped after 100 iterations
# weights:  144
initial  value 71.993793 
iter  10 value 48.121026
iter  20 value 48.119560
iter  30 value 48.118952
iter  40 value 48.118177
iter  50 value 48.117032
iter  60 value 48.114996
iter  70 value 48.110182
iter  80 value 48.088658
iter  90 value 46.512009
iter 100 value 34.170089
final  value 34.170089 
stopped after 100 iterations
# weights:  12
initial  value 67.176932 
iter  10 value 53.190741
final  value 53.190588 
converged
# weights:  111
initial  value 141.711805 
iter  10 value 55.065512
iter  20 value 54.962459
final  value 54.962439 
converged
# weights:  133
initial  value 91.254613 
iter  10 value 48.191771
iter  20 value 47.899503
iter  30 value 34.671855
iter  40 value 34.515229
iter  50 value 34.476484
iter  60 value 34.445084
iter  70 value 34.264117
iter  80 value 34.192768
iter  90 value 34.159581
iter 100 value 34.112308
final  value 34.112308 
stopped after 100 iterations
# weights:  177
initial  value 78.698909 
iter  10 value 49.707049
final  value 49.706943 
converged
# weights:  155
initial  value 56.430828 
iter  10 value 48.279493
iter  20 value 37.974155
iter  30 value 37.099232
iter  40 value 36.979435
iter  50 value 36.967790
iter  60 value 36.959684
iter  70 value 36.956257
iter  80 value 36.952977
iter  90 value 36.952577
iter 100 value 36.952432
final  value 36.952432 
stopped after 100 iterations
# weights:  155
initial  value 68.605661 
iter  10 value 48.831697
iter  20 value 47.797620
iter  30 value 45.912217
iter  40 value 43.737975
iter  50 value 43.518842
iter  60 value 43.495570
iter  70 value 43.494839
final  value 43.494832 
converged
# weights:  45
initial  value 84.781282 
iter  10 value 48.141804
iter  20 value 37.615289
iter  30 value 34.167220
iter  40 value 33.893664
iter  50 value 33.809199
iter  60 value 33.786641
iter  70 value 33.757030
iter  80 value 33.736472
iter  90 value 33.723987
iter 100 value 33.714558
final  value 33.714558 
stopped after 100 iterations
# weights:  23
initial  value 76.331501 
iter  10 value 51.302900
final  value 51.302898 
converged
# weights:  89
initial  value 69.559732 
iter  10 value 48.220603
iter  20 value 38.252299
iter  30 value 38.057989
iter  40 value 37.807722
iter  50 value 37.231037
iter  60 value 37.128451
iter  70 value 37.100781
iter  80 value 37.074090
iter  90 value 37.068993
iter 100 value 37.066994
final  value 37.066994 
stopped after 100 iterations
# weights:  166
initial  value 50.896944 
iter  10 value 48.138307
iter  20 value 48.099604
iter  30 value 38.562206
iter  40 value 34.493855
iter  50 value 34.241208
iter  60 value 34.178148
iter  70 value 34.074009
iter  80 value 34.014388
iter  90 value 33.992046
iter 100 value 33.983935
final  value 33.983935 
stopped after 100 iterations
# weights:  100
initial  value 71.184124 
iter  10 value 50.792588
final  value 50.792579 
converged
# weights:  166
initial  value 42.104142 
iter  10 value 31.867735
iter  20 value 31.804953
iter  30 value 20.660812
iter  40 value 19.675949
iter  50 value 19.495166
iter  60 value 19.375686
iter  70 value 19.265714
iter  80 value 19.232031
iter  90 value 19.209145
iter 100 value 19.196311
final  value 19.196311 
stopped after 100 iterations
# weights:  45
initial  value 49.360908 
iter  10 value 32.247978
iter  20 value 29.829294
iter  30 value 22.689034
iter  40 value 22.180449
iter  50 value 22.121581
iter  60 value 22.088557
iter  70 value 22.031877
iter  80 value 22.021895
iter  90 value 22.020096
iter 100 value 22.019981
final  value 22.019981 
stopped after 100 iterations
# weights:  12
initial  value 51.567000 
iter  10 value 37.284347
final  value 37.282675 
converged
# weights:  199
initial  value 42.376084 
iter  10 value 31.873506
iter  20 value 31.822346
iter  30 value 29.906604
iter  40 value 20.381645
iter  50 value 19.592407
iter  60 value 19.352484
iter  70 value 19.288501
iter  80 value 19.255334
iter  90 value 19.225003
iter 100 value 19.211648
final  value 19.211648 
stopped after 100 iterations
# weights:  166
initial  value 35.975202 
iter  10 value 31.872010
iter  20 value 25.239604
iter  30 value 20.093118
iter  40 value 19.824625
iter  50 value 19.704721
iter  60 value 19.650021
iter  70 value 19.612265
iter  80 value 19.582879
iter  90 value 19.560353
iter 100 value 19.549644
final  value 19.549644 
stopped after 100 iterations
# weights:  199
initial  value 70.005231 
iter  10 value 32.019029
iter  20 value 31.286851
iter  30 value 23.271383
iter  40 value 21.947479
iter  50 value 21.690368
iter  60 value 21.648543
iter  70 value 21.637531
iter  80 value 21.632849
iter  90 value 21.631387
iter 100 value 21.630448
final  value 21.630448 
stopped after 100 iterations
# weights:  177
initial  value 48.244586 
iter  10 value 32.381693
iter  20 value 32.185654
iter  20 value 32.185653
iter  20 value 32.185653
final  value 32.185653 
converged
# weights:  34
initial  value 47.972851 
iter  10 value 31.848954
iter  20 value 31.840230
iter  30 value 31.821191
iter  40 value 31.712565
iter  50 value 22.236840
iter  60 value 19.512385
iter  70 value 19.064992
iter  80 value 19.035963
iter  90 value 19.021469
iter 100 value 19.017665
final  value 19.017665 
stopped after 100 iterations
# weights:  144
initial  value 71.339046 
iter  10 value 31.847405
iter  20 value 31.847349
iter  30 value 31.847286
iter  40 value 31.847215
iter  50 value 31.847134
iter  60 value 31.847041
iter  70 value 31.846930
iter  80 value 31.846798
iter  90 value 31.846634
iter 100 value 31.846429
final  value 31.846429 
stopped after 100 iterations
# weights:  12
initial  value 43.877889 
iter  10 value 35.574346
iter  10 value 35.574345
iter  10 value 35.574345
final  value 35.574345 
converged
# weights:  111
initial  value 163.976868 
iter  10 value 37.021603
iter  20 value 36.945115
iter  20 value 36.945115
iter  20 value 36.945115
final  value 36.945115 
converged
# weights:  133
initial  value 49.379611 
iter  10 value 31.945080
iter  20 value 31.817395
iter  30 value 22.606366
iter  40 value 19.930203
iter  50 value 19.782925
iter  60 value 19.652953
iter  70 value 19.605992
iter  80 value 19.547552
iter  90 value 19.504297
iter 100 value 19.483089
final  value 19.483089 
stopped after 100 iterations
# weights:  177
initial  value 67.593732 
iter  10 value 32.887907
final  value 32.887897 
converged
# weights:  155
initial  value 52.564961 
iter  10 value 31.867453
iter  20 value 24.783152
iter  30 value 22.377596
iter  40 value 22.283910
iter  50 value 22.242013
iter  60 value 22.225809
iter  70 value 22.219271
iter  80 value 22.218326
iter  90 value 22.217979
iter 100 value 22.217761
final  value 22.217761 
stopped after 100 iterations
# weights:  155
initial  value 75.505302 
iter  10 value 31.989230
iter  20 value 29.800098
iter  30 value 28.256502
iter  40 value 28.180676
iter  50 value 28.179472
final  value 28.179460 
converged
# weights:  45
initial  value 51.272440 
iter  10 value 31.857676
iter  20 value 31.854180
iter  30 value 31.850386
iter  40 value 31.841181
iter  50 value 31.775833
iter  60 value 22.539421
iter  70 value 19.459870
iter  80 value 19.164185
iter  90 value 19.122887
iter 100 value 19.110910
final  value 19.110910 
stopped after 100 iterations
# weights:  23
initial  value 47.236629 
iter  10 value 34.102609
final  value 34.102593 
converged
# weights:  89
initial  value 41.751403 
iter  10 value 31.524382
iter  20 value 23.662123
iter  30 value 23.054676
iter  40 value 22.622743
iter  50 value 22.515880
iter  60 value 22.476530
iter  70 value 22.384550
iter  80 value 22.380996
iter  90 value 22.379646
iter 100 value 22.379604
final  value 22.379604 
stopped after 100 iterations
# weights:  166
initial  value 39.562465 
iter  10 value 31.875013
iter  20 value 30.476918
iter  30 value 20.196669
iter  40 value 19.789493
iter  50 value 19.613752
iter  60 value 19.544567
iter  70 value 19.499598
iter  80 value 19.463081
iter  90 value 19.430076
iter 100 value 19.409338
final  value 19.409338 
stopped after 100 iterations
# weights:  100
initial  value 66.232337 
iter  10 value 33.707419
final  value 33.707133 
converged
# weights:  133
initial  value 74.656213 
iter  10 value 55.684545
iter  20 value 55.585867
iter  30 value 55.562747
iter  40 value 53.862467
iter  50 value 38.706451
iter  60 value 37.853698
iter  70 value 37.656706
iter  80 value 37.567285
iter  90 value 37.505579
iter 100 value 37.459902
final  value 37.459902 
stopped after 100 iterations
Neural Network 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  size  decay         RMSE       Rsquared    MAE        Selected
   1    1.146607e+00  0.8510082  0.13790162  0.5639809          
   1    2.288961e+00  0.8659758  0.05261027  0.5899622          
   2    6.409401e-01  0.8404525  0.30592479  0.5445576          
   3    5.046684e-05  0.7389043  0.54354038  0.4383881          
   4    3.153077e-04  0.6989141  0.49459974  0.4276794          
   4    2.306609e-02  0.7035204  0.57037056  0.4489920          
   8    2.873822e-02  0.7017521  0.58272106  0.4475709          
   9    9.557146e-01  0.8390775  0.28531724  0.5399477          
  10    5.497725e+00  0.8634883  0.05553497  0.5853867          
  12    2.005756e-03  0.6869448  0.55071549  0.4139702  *       
  13    3.886082e-05  0.7543970  0.46440223  0.4864142          
  14    2.810136e-02  0.6997691  0.58826864  0.4448813          
  14    1.478120e-01  0.7402520  0.63479137  0.4894716          
  15    6.484945e-04  0.7024044  0.47577148  0.4310010          
  15    1.586781e-03  0.6924275  0.52152347  0.4183632          
  15    2.259918e-03  0.6960827  0.52154443  0.4229991          
  16    2.040700e-01  0.8310765  0.57615577  0.5247496          
  16    6.234085e-01  0.8349956  0.40008940  0.5305128          
  18    5.864250e-04  0.6894167  0.52740141  0.4139642          
  18    2.130540e-02  0.6965527  0.58003127  0.4396743          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 12 and decay = 0.002005756.
[1] "Sat Mar 10 03:39:32 2018"
Non-Negative Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results:

  RMSE       Rsquared  MAE      
  0.7669439  0.744768  0.5453359

[1] "Sat Mar 10 03:39:45 2018"
Non-Informative Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results:

  RMSE       Rsquared  MAE      
  0.8347529  NaN       0.5276532

[1] "Sat Mar 10 03:39:59 2018"
Parallel Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE        Selected
  1     0.7375558  0.4124003  0.4716421          
  2     0.6239145  0.6664531  0.3970095          
  4     0.4396429  0.8664152  0.2579768          
  5     0.3638728  0.9104435  0.2059164          
  6     0.3046925  0.9366891  0.1646329          
  7     0.2493508  0.9519488  0.1359072          
  8     0.2105121  0.9628614  0.1100544  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 8.
[1] "Sat Mar 10 03:40:15 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: Warning messages:
1: package 'mxnet' is not available (for R version 3.4.3) 
2: package 'mxnet' is not available (for R version 3.4.3) 
3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
# weights:  166
initial  value 47.397057 
iter  10 value 31.174664
iter  20 value 28.346143
iter  30 value 21.285526
iter  40 value 21.125575
iter  50 value 21.066328
iter  60 value 21.056391
iter  70 value 21.051885
iter  80 value 21.047521
iter  90 value 21.046136
iter 100 value 21.044789
final  value 21.044789 
stopped after 100 iterations
# weights:  45
initial  value 53.766558 
iter  10 value 31.654580
iter  20 value 23.368008
iter  30 value 22.939252
iter  40 value 22.833557
iter  50 value 22.831063
iter  60 value 22.830535
iter  70 value 22.830465
iter  80 value 22.830429
iter  90 value 22.830421
iter  90 value 22.830420
iter  90 value 22.830420
final  value 22.830420 
converged
# weights:  12
initial  value 43.036913 
iter  10 value 36.880211
final  value 36.879109 
converged
# weights:  199
initial  value 59.149790 
iter  10 value 31.183009
iter  20 value 31.140018
iter  30 value 26.787215
iter  40 value 22.508072
iter  50 value 21.565913
iter  60 value 21.239063
iter  70 value 21.186517
iter  80 value 21.150373
iter  90 value 21.114491
iter 100 value 21.082491
final  value 21.082491 
stopped after 100 iterations
# weights:  166
initial  value 51.950017 
iter  10 value 31.290145
iter  20 value 28.878320
iter  30 value 22.985725
iter  40 value 21.466458
iter  50 value 21.307525
iter  60 value 21.258483
iter  70 value 21.239591
iter  80 value 21.231695
iter  90 value 21.226776
iter 100 value 21.224075
final  value 21.224075 
stopped after 100 iterations
# weights:  199
initial  value 44.666928 
iter  10 value 31.238343
iter  20 value 22.891023
iter  30 value 22.784535
iter  40 value 22.416452
iter  50 value 22.320593
iter  60 value 22.270060
iter  70 value 22.243137
iter  80 value 22.225797
iter  90 value 22.219617
iter 100 value 22.218081
final  value 22.218081 
stopped after 100 iterations
# weights:  177
initial  value 45.869532 
iter  10 value 30.251593
iter  20 value 27.566661
iter  30 value 26.360709
iter  40 value 26.214906
iter  50 value 26.211730
final  value 26.211730 
converged
# weights:  34
initial  value 41.418246 
iter  10 value 31.157949
iter  20 value 31.157816
iter  30 value 31.157674
iter  40 value 31.157518
iter  50 value 31.157342
iter  60 value 31.157138
iter  70 value 31.156892
iter  80 value 31.156583
iter  90 value 31.156174
iter 100 value 31.155598
final  value 31.155598 
stopped after 100 iterations
# weights:  144
initial  value 56.973517 
iter  10 value 31.155763
iter  20 value 29.910190
iter  30 value 21.442245
iter  40 value 21.130612
iter  50 value 21.051410
iter  60 value 21.037205
iter  70 value 21.032578
iter  80 value 21.020332
iter  90 value 20.987972
iter 100 value 20.979005
final  value 20.979005 
stopped after 100 iterations
# weights:  12
initial  value 45.662882 
iter  10 value 34.637153
final  value 34.626949 
converged
# weights:  111
initial  value 133.665398 
iter  10 value 36.891310
iter  20 value 36.683014
final  value 36.683007 
converged
# weights:  133
initial  value 55.708561 
iter  10 value 31.247049
iter  20 value 28.750668
iter  30 value 21.894213
iter  40 value 21.475615
iter  50 value 21.318501
iter  60 value 21.287464
iter  70 value 21.273624
iter  80 value 21.247935
iter  90 value 21.211046
iter 100 value 21.190597
final  value 21.190597 
stopped after 100 iterations
# weights:  177
initial  value 54.729695 
iter  10 value 31.583393
iter  20 value 30.700268
iter  30 value 30.689235
final  value 30.689232 
converged
# weights:  155
initial  value 58.480769 
iter  10 value 29.520434
iter  20 value 23.682810
iter  30 value 22.826629
iter  40 value 22.623318
iter  50 value 22.572136
iter  60 value 22.540160
iter  70 value 22.534975
iter  80 value 22.534156
iter  90 value 22.533948
iter 100 value 22.533932
final  value 22.533932 
stopped after 100 iterations
# weights:  155
initial  value 49.229576 
iter  10 value 30.364175
iter  20 value 26.493277
iter  30 value 25.439037
iter  40 value 25.419727
final  value 25.419698 
converged
# weights:  45
initial  value 46.609301 
iter  10 value 31.159361
iter  20 value 31.114245
iter  30 value 25.317145
iter  40 value 21.288504
iter  50 value 21.200052
iter  60 value 21.105543
iter  70 value 21.056611
iter  80 value 21.051042
iter  90 value 21.045832
iter 100 value 21.041587
final  value 21.041587 
stopped after 100 iterations
# weights:  23
initial  value 50.200740 
iter  10 value 32.045878
final  value 32.039033 
converged
# weights:  89
initial  value 55.948402 
iter  10 value 25.302843
iter  20 value 22.714655
iter  30 value 22.629899
iter  40 value 22.622750
iter  50 value 22.618103
iter  60 value 22.616463
iter  70 value 22.616273
iter  80 value 22.616205
iter  90 value 22.616153
final  value 22.616143 
converged
# weights:  166
initial  value 34.846950 
iter  10 value 31.159290
iter  20 value 25.894435
iter  30 value 21.322441
iter  40 value 21.212605
iter  50 value 21.149551
iter  60 value 21.138071
iter  70 value 21.133663
iter  80 value 21.131644
iter  90 value 21.129976
iter 100 value 21.128982
final  value 21.128982 
stopped after 100 iterations
# weights:  100
initial  value 64.286111 
iter  10 value 32.550748
iter  20 value 32.533825
final  value 32.533823 
converged
# weights:  166
initial  value 69.040967 
iter  10 value 48.151037
iter  20 value 47.856818
iter  30 value 34.036268
iter  40 value 33.855560
iter  50 value 33.701884
iter  60 value 33.666334
iter  70 value 33.659293
iter  80 value 33.653319
iter  90 value 33.647029
iter 100 value 33.643551
final  value 33.643551 
stopped after 100 iterations
# weights:  45
initial  value 67.365021 
iter  10 value 48.750187
iter  20 value 36.053446
iter  30 value 35.646299
iter  40 value 35.623555
iter  50 value 35.621574
iter  60 value 35.621261
final  value 35.621250 
converged
# weights:  12
initial  value 66.891056 
iter  10 value 54.771450
final  value 54.766962 
converged
# weights:  199
initial  value 85.907214 
iter  10 value 48.151396
iter  20 value 48.118248
iter  30 value 48.063774
iter  40 value 35.223844
iter  50 value 34.083018
iter  60 value 34.025851
iter  70 value 34.003068
iter  80 value 33.984247
iter  90 value 33.964689
iter 100 value 33.765188
final  value 33.765188 
stopped after 100 iterations
# weights:  166
initial  value 67.840495 
iter  10 value 48.280415
iter  20 value 37.519353
iter  30 value 34.016044
iter  40 value 33.867954
iter  50 value 33.818424
iter  60 value 33.804394
iter  70 value 33.796404
iter  80 value 33.791741
iter  90 value 33.788799
iter 100 value 33.786568
final  value 33.786568 
stopped after 100 iterations
# weights:  199
initial  value 91.894906 
iter  10 value 42.734161
iter  20 value 35.078923
iter  30 value 34.932878
iter  40 value 34.902307
iter  50 value 34.898553
iter  60 value 34.896383
iter  70 value 34.895144
iter  80 value 34.894902
iter  90 value 34.894801
iter 100 value 34.894615
final  value 34.894615 
stopped after 100 iterations
# weights:  177
initial  value 88.443193 
iter  10 value 48.112478
iter  20 value 41.567664
iter  30 value 40.803337
iter  40 value 39.629228
iter  50 value 39.621987
final  value 39.621984 
converged
# weights:  34
initial  value 64.840408 
iter  10 value 48.123957
iter  20 value 48.085323
iter  30 value 48.037323
iter  40 value 47.445898
iter  50 value 33.916238
iter  60 value 33.677424
iter  70 value 33.588162
iter  80 value 33.579579
iter  90 value 33.577354
iter 100 value 33.576622
final  value 33.576622 
stopped after 100 iterations
# weights:  144
initial  value 70.520824 
iter  10 value 48.120922
iter  20 value 48.118060
iter  30 value 48.116455
iter  40 value 48.112828
iter  50 value 48.096913
iter  60 value 40.914138
iter  70 value 33.947035
iter  80 value 33.885194
iter  90 value 33.868787
iter 100 value 33.863566
final  value 33.863566 
stopped after 100 iterations
# weights:  12
initial  value 66.434259 
iter  10 value 51.763855
iter  20 value 51.744222
iter  20 value 51.744222
iter  20 value 51.744222
final  value 51.744222 
converged
# weights:  111
initial  value 144.329734 
iter  10 value 54.773488
iter  20 value 54.741193
iter  20 value 54.741193
iter  20 value 54.741193
final  value 54.741193 
converged
# weights:  133
initial  value 88.672235 
iter  10 value 48.199894
iter  20 value 45.020749
iter  30 value 34.515618
iter  40 value 34.321291
iter  50 value 34.164749
iter  60 value 33.938219
iter  70 value 33.856340
iter  80 value 33.809182
iter  90 value 33.788478
iter 100 value 33.776962
final  value 33.776962 
stopped after 100 iterations
# weights:  177
initial  value 75.010415 
iter  10 value 47.418088
iter  20 value 45.277122
iter  30 value 45.197191
iter  40 value 45.171926
final  value 45.171925 
converged
# weights:  155
initial  value 58.062223 
iter  10 value 40.485047
iter  20 value 35.641653
iter  30 value 35.304388
iter  40 value 35.259388
iter  50 value 35.233662
iter  60 value 35.225289
iter  70 value 35.222457
iter  80 value 35.221641
iter  90 value 35.221238
iter 100 value 35.221006
final  value 35.221006 
stopped after 100 iterations
# weights:  155
initial  value 65.011874 
iter  10 value 44.029749
iter  20 value 39.901721
iter  30 value 38.983655
iter  40 value 38.697140
iter  50 value 38.685476
final  value 38.685468 
converged
# weights:  45
initial  value 86.211169 
iter  10 value 48.145470
iter  20 value 39.922368
iter  30 value 33.840833
iter  40 value 33.653317
iter  50 value 33.632311
iter  60 value 33.629251
iter  70 value 33.627762
iter  80 value 33.627009
iter  90 value 33.626373
iter 100 value 33.625867
final  value 33.625867 
stopped after 100 iterations
# weights:  23
initial  value 76.749689 
iter  10 value 47.846248
iter  20 value 47.823274
iter  20 value 47.823273
iter  20 value 47.823273
final  value 47.823273 
converged
# weights:  89
initial  value 70.968085 
iter  10 value 38.406466
iter  20 value 35.982829
iter  30 value 35.918366
iter  40 value 35.912323
iter  50 value 35.911805
iter  60 value 35.911467
iter  70 value 35.911203
iter  80 value 35.911034
iter  90 value 35.910966
iter 100 value 35.910951
final  value 35.910951 
stopped after 100 iterations
# weights:  166
initial  value 52.008702 
iter  10 value 48.098419
iter  20 value 34.425327
iter  30 value 33.794267
iter  40 value 33.749739
iter  50 value 33.738783
iter  60 value 33.729542
iter  70 value 33.725003
iter  80 value 33.722203
iter  90 value 33.721073
iter 100 value 33.720327
final  value 33.720327 
stopped after 100 iterations
# weights:  100
initial  value 75.599179 
iter  10 value 49.175552
iter  20 value 49.096794
final  value 49.096422 
converged
# weights:  166
initial  value 40.898584 
iter  10 value 31.866324
iter  20 value 31.758178
iter  30 value 20.242902
iter  40 value 19.469957
iter  50 value 19.244084
iter  60 value 19.164252
iter  70 value 19.132937
iter  80 value 19.103315
iter  90 value 19.091727
iter 100 value 19.085234
final  value 19.085234 
stopped after 100 iterations
# weights:  45
initial  value 47.539094 
iter  10 value 32.203675
iter  20 value 21.304939
iter  30 value 20.926818
iter  40 value 20.902995
iter  50 value 20.898361
iter  60 value 20.897435
iter  70 value 20.897365
iter  70 value 20.897364
iter  70 value 20.897364
final  value 20.897364 
converged
# weights:  12
initial  value 52.105525 
iter  10 value 36.924049
final  value 36.919713 
converged
# weights:  199
initial  value 46.078717 
iter  10 value 31.875316
iter  20 value 31.842718
iter  30 value 31.185966
iter  40 value 19.307894
iter  50 value 19.123295
iter  60 value 19.079520
iter  70 value 19.047760
iter  80 value 19.040154
iter  90 value 19.035949
iter 100 value 19.033164
final  value 19.033164 
stopped after 100 iterations
# weights:  166
initial  value 38.686959 
iter  10 value 31.878698
iter  20 value 25.378296
iter  30 value 19.360182
iter  40 value 19.252328
iter  50 value 19.227623
iter  60 value 19.222465
iter  70 value 19.216545
iter  80 value 19.205512
iter  90 value 19.197458
iter 100 value 19.191163
final  value 19.191163 
stopped after 100 iterations
# weights:  199
initial  value 69.832455 
iter  10 value 31.778693
iter  20 value 20.382305
iter  30 value 20.246801
iter  40 value 20.222733
iter  50 value 20.217922
iter  60 value 20.216799
iter  70 value 20.215965
iter  80 value 20.215493
iter  90 value 20.215207
iter 100 value 20.215071
final  value 20.215071 
stopped after 100 iterations
# weights:  177
initial  value 51.678645 
iter  10 value 30.003358
iter  20 value 25.411612
iter  30 value 24.508468
final  value 24.508239 
converged
# weights:  34
initial  value 47.457611 
iter  10 value 31.851278
iter  20 value 31.842832
iter  30 value 31.839968
iter  40 value 31.834669
iter  50 value 31.823584
iter  60 value 31.795894
iter  70 value 31.667827
iter  80 value 20.885812
iter  90 value 19.203281
iter 100 value 19.076950
final  value 19.076950 
stopped after 100 iterations
# weights:  144
initial  value 66.687917 
iter  10 value 31.848245
iter  20 value 31.843307
iter  30 value 31.840787
iter  40 value 31.833420
iter  50 value 31.762932
iter  60 value 19.801586
iter  70 value 19.211500
iter  80 value 19.107514
iter  90 value 19.035147
iter 100 value 19.030734
final  value 19.030734 
stopped after 100 iterations
# weights:  12
initial  value 43.556727 
iter  10 value 34.564688
iter  20 value 34.557089
iter  20 value 34.557089
iter  20 value 34.557089
final  value 34.557089 
converged
# weights:  111
initial  value 160.519575 
iter  10 value 36.901187
iter  20 value 36.809550
final  value 36.809546 
converged
# weights:  133
initial  value 50.120406 
iter  10 value 31.925551
iter  20 value 31.691325
iter  30 value 20.165067
iter  40 value 19.504246
iter  50 value 19.301079
iter  60 value 19.237793
iter  70 value 19.209176
iter  80 value 19.194116
iter  90 value 19.183896
iter 100 value 19.177462
final  value 19.177462 
stopped after 100 iterations
# weights:  177
initial  value 70.157878 
iter  10 value 29.694593
iter  20 value 29.491653
final  value 29.491571 
converged
# weights:  155
initial  value 51.007017 
iter  10 value 30.343713
iter  20 value 21.227559
iter  30 value 21.092350
iter  40 value 20.752207
iter  50 value 20.709904
iter  60 value 20.700559
iter  70 value 20.699401
iter  80 value 20.699095
iter  90 value 20.699060
iter 100 value 20.699042
final  value 20.699042 
stopped after 100 iterations
# weights:  155
initial  value 75.576050 
iter  10 value 25.698756
iter  20 value 23.978435
iter  30 value 23.954862
iter  40 value 23.952943
final  value 23.952937 
converged
# weights:  45
initial  value 51.165529 
iter  10 value 31.860072
iter  20 value 31.858806
iter  30 value 31.856538
iter  40 value 31.850882
iter  50 value 31.819746
iter  60 value 28.278224
iter  70 value 19.325038
iter  80 value 19.100003
iter  90 value 19.052689
iter 100 value 19.040702
final  value 19.040702 
stopped after 100 iterations
# weights:  23
initial  value 48.166411 
iter  10 value 31.486714
iter  20 value 31.481476
iter  20 value 31.481476
iter  20 value 31.481476
final  value 31.481476 
converged
# weights:  89
initial  value 45.649754 
iter  10 value 23.823695
iter  20 value 21.213155
iter  30 value 21.160440
iter  40 value 21.152588
iter  50 value 21.148680
iter  60 value 21.148307
iter  70 value 21.148250
iter  80 value 21.148236
final  value 21.148233 
converged
# weights:  166
initial  value 35.693024 
iter  10 value 31.880901
iter  20 value 31.869838
iter  30 value 31.722239
iter  40 value 19.590568
iter  50 value 19.264905
iter  60 value 19.196920
iter  70 value 19.173945
iter  80 value 19.144106
iter  90 value 19.137827
iter 100 value 19.133676
final  value 19.133676 
stopped after 100 iterations
# weights:  100
initial  value 61.570321 
iter  10 value 32.558331
iter  20 value 32.476204
final  value 32.476188 
converged
# weights:  133
initial  value 76.400480 
iter  10 value 55.703576
iter  20 value 55.493202
iter  30 value 44.742397
iter  40 value 38.050736
iter  50 value 37.438512
iter  60 value 37.205513
iter  70 value 37.121468
iter  80 value 37.083284
iter  90 value 37.048660
iter 100 value 37.032352
final  value 37.032352 
stopped after 100 iterations
Neural Networks with Feature Extraction 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  size  decay         RMSE       Rsquared   MAE        Selected
   1    1.146607e+00  0.8280267  0.6341785  0.5501997          
   1    2.288961e+00  0.8563741  0.6184943  0.5832817          
   2    6.409401e-01  0.7849520  0.6363580  0.5139492          
   3    5.046684e-05  0.7488562  0.3376011  0.4834396          
   4    3.153077e-04  0.6897411  0.4978266  0.4191608          
   4    2.306609e-02  0.6959201  0.5785082  0.4259049          
   8    2.873822e-02  0.6922379  0.5764617  0.4236036          
   9    9.557146e-01  0.8099078  0.6308518  0.5242170          
  10    5.497725e+00  0.8603444  0.5956975  0.5830484          
  12    2.005756e-03  0.6839535  0.5560328  0.4093598  *       
  13    3.886082e-05  0.7031903  0.4430934  0.4379596          
  14    2.810136e-02  0.6875912  0.5899085  0.4192177          
  14    1.478120e-01  0.7049691  0.6176662  0.4451991          
  15    6.484945e-04  0.6878867  0.5266742  0.4183970          
  15    1.586781e-03  0.6861811  0.5502477  0.4184123          
  15    2.259918e-03  0.6875405  0.5431963  0.4147233          
  16    2.040700e-01  0.7096593  0.6266302  0.4517130          
  16    6.234085e-01  0.7527382  0.6443993  0.4956262          
  18    5.864250e-04  0.6926303  0.5321033  0.4225098          
  18    2.130540e-02  0.6854536  0.5852613  0.4159325          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 12 and decay = 0.002005756.
[1] "Sat Mar 10 03:40:32 2018"
Principal Component Analysis 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      0.7281134  0.2918155  0.4898945          
  2      0.7130326  0.3831701  0.5094874          
  3      0.7138402  0.3723928  0.5170357          
  4      0.7211954  0.3646689  0.5180528          
  5      0.7192266  0.3643843  0.5224772          
  6      0.7087308  0.3914375  0.5150141          
  7      0.7179928  0.3747761  0.5380612          
  8      0.5508825  0.6389881  0.4266065  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 8.
[1] "Sat Mar 10 03:40:46 2018"
____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0603968768846244) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.112099130311981) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0589404160622507) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0784697547089309) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.110949656553566) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.143659306084737) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.019650399684906) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.191339434497058) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.076742603071034) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.159825757611543) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.114957580482587) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.138990323152393) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.115281994314864) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0733505642041564) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.166010940819979) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.110949656553566) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.143659306084737) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.191339434497058) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.159825757611543) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.114957580482587) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.138990323152393) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.115281994314864) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.166010940819979) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.019650399684906) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 9 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

Partial Least Squares Generalized Linear Models  

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  nt  alpha.pvals.expli  RMSE       Rsquared   MAE        Selected
  1   0.16022725         0.5529666  0.6234140  0.4126905          
  1   0.16864715         0.5529666  0.6234140  0.4126905          
  1   0.17865461         0.5623276  0.6122162  0.4191886          
  2   0.02343354         0.5014681  0.7058592  0.3980510          
  2   0.04995782         0.5014681  0.7058592  0.3980510          
  2   0.11209913         0.5116572  0.6926393  0.3941000          
  4   0.11528199         0.5150266  0.6981936  0.4068994          
  4   0.16601094         0.5237943  0.6984931  0.4127271          
  5   0.19133943         0.5208812  0.7064856  0.4038216          
  6   0.01965040         0.5157533  0.6901508  0.4122288          
  6   0.07674260         0.5014681  0.7058592  0.3980510          
  6   0.13899032         0.5220369  0.7017533  0.4033859          
  7   0.06039688         0.5014681  0.7058592  0.3980510          
  7   0.07335056         0.5014681  0.7058592  0.3980510          
  7   0.07846975         0.5014681  0.7058592  0.3980510  *       
  7   0.11495758         0.5150266  0.6981936  0.4068994          
  7   0.15982576         0.5237943  0.6984931  0.4127271          
  8   0.05894042         0.5014681  0.7058592  0.3980510          
  8   0.11094966         0.5150266  0.6981936  0.4068994          
  8   0.14365931         0.5220369  0.7017533  0.4033859          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nt = 7 and alpha.pvals.expli
 = 0.07846975.
[1] "Sat Mar 10 03:41:27 2018"
Projection Pursuit Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  nterms  RMSE       Rsquared   MAE        Selected
   1      0.3339095  0.8630213  0.1941304          
   2      0.3319106  0.8628286  0.1939234          
   3      0.3308105  0.8670151  0.1997836  *       
   4      0.3449594  0.8517472  0.2254327          
   5      0.3565131  0.8417589  0.2341043          
   6      0.3542858  0.8444913  0.2302027          
   7      0.3508261  0.8509705  0.2254825          
   8      0.3518347  0.8489835  0.2256980          
   9      0.3492647  0.8512983  0.2238397          
  10      0.3495096  0.8510018  0.2235361          
  11      0.3494959  0.8512938  0.2238788          
  12      0.3487029  0.8516977  0.2234278          
  13      0.3477640  0.8526816  0.2221611          
  14      0.3483257  0.8522629  0.2225238          
  15      0.3485224  0.8520117  0.2224940          
  16      0.3486111  0.8519296  0.2225259          
  17      0.3486637  0.8519095  0.2225907          
  18      0.3485487  0.8520691  0.2225085          
  19      0.3485999  0.8519917  0.2225239          
  20      0.3485935  0.8519941  0.2225153          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nterms = 3.
[1] "Sat Mar 10 03:41:42 2018"
Quantile Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE        Selected
  1     1.2775761  0.2251054  1.0061836          
  2     1.0737396  0.3874740  0.8522272          
  4     0.8361781  0.4727880  0.6044168          
  5     0.7170103  0.5870084  0.5064835          
  6     0.6462462  0.6489976  0.4508701          
  7     0.5816980  0.7253794  0.4012195          
  8     0.5109178  0.7685559  0.3219162  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 8.
[1] "Sat Mar 10 03:42:19 2018"
Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  min.node.size  mtry  splitrule   RMSE       Rsquared   MAE        Selected
   1             8     maxstat     0.3594591  0.8811647  0.2055033          
   1             9     extratrees  0.3085126  0.9170558  0.1928300          
   2             8     variance    0.1994560  0.9673512  0.1043300  *       
   3             2     variance    0.6170004  0.7035092  0.3879270          
   4             3     extratrees  0.5544788  0.7726413  0.3177584          
   4             6     maxstat     0.4606068  0.8107508  0.2492197          
   8             6     variance    0.3179902  0.9266967  0.1711343          
   9             8     maxstat     0.3849566  0.8512236  0.2224636          
  10             9     maxstat     0.3523436  0.8651182  0.2035555          
  12             4     maxstat     0.6033137  0.6444461  0.3340112          
  13             1     extratrees  0.7304766  0.5796750  0.4279969          
  14             6     maxstat     0.5524075  0.6761147  0.3202477          
  14             7     variance    0.2940135  0.9342394  0.1546174          
  15             3     variance    0.5714506  0.7605877  0.3590401          
  15             4     extratrees  0.5532068  0.7565370  0.3058466          
  15             4     variance    0.4849231  0.8502752  0.2928943          
  16             7     maxstat     0.5304210  0.6963614  0.3189030          
  16             8     variance    0.2550915  0.9505152  0.1361598          
  18             3     extratrees  0.6184734  0.7135679  0.3458251          
  18             5     variance    0.4132889  0.9036384  0.2410164          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 8, splitrule = variance
 and min.node.size = 2.
[1] "Sat Mar 10 03:42:37 2018"
Error in code$varImp(object$finalModel, ...) : 
  No importance values available
In addition: Warning messages:
1: package 'gpls' is not available (for R version 3.4.3) 
2: package 'rPython' is not available (for R version 3.4.3) 
Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  predFixed  minNode  RMSE       Rsquared   MAE         Selected
  1          17       0.7790202  0.2961608  0.49895281          
  1          18       0.7847894  0.2863596  0.49781544          
  2           3       0.6159795  0.6751982  0.39128883          
  2           5       0.6368318  0.6526309  0.40865991          
  2          12       0.6649954  0.6385161  0.42868159          
  4          12       0.4807429  0.8417590  0.29778469          
  4          17       0.5117680  0.8221524  0.32457892          
  5          20       0.4437001  0.9026056  0.28026070          
  6           2       0.2424170  0.9589172  0.12275360          
  6           8       0.3196752  0.9326920  0.17287367          
  6          14       0.3481412  0.9292472  0.19609671          
  7           7       0.2522877  0.9485208  0.12906302          
  7           8       0.2680764  0.9503634  0.13670666          
  7          12       0.2956083  0.9477101  0.15755274          
  7          16       0.2840102  0.9537449  0.15484424          
  8           6       0.1847919  0.9707887  0.09020371  *       
  8          12       0.2453847  0.9557463  0.12357776          
  8          15       0.2406736  0.9605048  0.11980438          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were predFixed = 8 and minNode = 6.
[1] "Sat Mar 10 03:45:20 2018"
Relaxed Lasso 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  phi        lambda    RMSE       Rsquared   MAE        Selected
  0.0982520  30.84917  0.9768256  0.3737573  0.7553729          
  0.1171677  17.40482  0.9594895  0.5896647  0.7495611          
  0.2497891  18.17789  0.8612934  0.6074668  0.6522624          
  0.2947021  41.22276  5.7751811  0.2787422  4.8110029          
  0.3019844  33.59560  0.9176916  0.4480538  0.7493010          
  0.3667528  33.48315  0.9142482  0.4803499  0.7596116          
  0.3837130  28.84202  0.9028535  0.5102969  0.7518138          
  0.3923488  34.76474  0.9465943  0.4302281  0.7926445          
  0.5547483  39.80148  3.9380573  0.3800031  3.3093055          
  0.5604957  18.42847  0.7415730  0.6626470  0.5328686          
  0.5747879  32.79187  0.9531199  0.5800684  0.8271969          
  0.5764100  22.45080  0.7394858  0.5578267  0.5897295  *       
  0.6949516  31.93278  1.0137825  0.6403711  0.8983830          
  0.7182965  37.09313  1.0856957  0.6219947  0.9720536          
  0.7991288  35.95744  1.1227049  0.6844173  1.0173836          
  0.8011362  16.19104  0.8042952  0.7912314  0.5518758          
  0.8300547  23.57649  0.9573130  0.6945331  0.8586018          
  0.8432357  15.49846  0.8092857  0.7912314  0.5620553          
  0.8932731  15.54454  0.8175204  0.7912314  0.5751575          
  0.9566972  25.86962  1.1449548  0.7111467  1.0537014          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 22.4508 and phi = 0.57641.
[1] "Sat Mar 10 03:45:36 2018"
Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE        Selected
  1     0.7384211  0.4321811  0.4623667          
  2     0.6340071  0.6603344  0.4017033          
  4     0.4424419  0.8669090  0.2658646          
  5     0.3644015  0.9099428  0.2074182          
  6     0.2995280  0.9339144  0.1623836          
  7     0.2454081  0.9542262  0.1343482          
  8     0.2090556  0.9647797  0.1117047  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 8.
[1] "Sat Mar 10 03:45:52 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Ridge Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  lambda        RMSE       Rsquared   MAE        Selected
  1.824675e-05  0.5359016  0.6860928  0.4230031          
  1.889923e-05  0.5359016  0.6860928  0.4230030          
  3.060968e-05  0.5359012  0.6860929  0.4230017          
  7.200631e-05  0.5358997  0.6860932  0.4229972          
  1.204248e-04  0.5358980  0.6860936  0.4229919          
  1.416033e-04  0.5358973  0.6860938  0.4229895          
  1.464566e-03  0.5358511  0.6861035  0.4228447          
  2.612998e-03  0.5358122  0.6861110  0.4227199          
  7.837159e-03  0.5356473  0.6861357  0.4221613          
  2.838701e-02  0.5351791  0.6860918  0.4205345          
  6.293173e-02  0.5349213  0.6856023  0.4184034  *       
  9.468729e-02  0.5351221  0.6848099  0.4170953          
  1.296361e-01  0.5356881  0.6836715  0.4160542          
  1.659349e-01  0.5365472  0.6822856  0.4152031          
  1.726511e-01  0.5367293  0.6820124  0.4150676          
  2.588058e-01  0.5394778  0.6782361  0.4147070          
  3.857740e-01  0.5441612  0.6723983  0.4168924          
  5.573670e-01  0.5505035  0.6649697  0.4206776          
  1.283258e+00  0.5711048  0.6428991  0.4371399          
  1.943706e+00  0.5829126  0.6314526  0.4462148          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was lambda = 0.06293173.
[1] "Sat Mar 10 03:46:07 2018"
Robust Linear Model 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  intercept  psi           RMSE       Rsquared   MAE        Selected
  FALSE      psi.huber     0.6151257  0.5819445  0.4425158          
  FALSE      psi.hampel    0.6204516  0.5812078  0.4622888          
  FALSE      psi.bisquare  0.6414921  0.5516483  0.4167051          
   TRUE      psi.huber     0.5514496  0.7014635  0.3837958  *       
   TRUE      psi.hampel    0.5681237  0.7134579  0.4047430          
   TRUE      psi.bisquare  0.5911928  0.6992983  0.3772238          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were intercept = TRUE and psi = psi.huber.
[1] "Sat Mar 10 03:46:22 2018"
CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  cp           RMSE       Rsquared   MAE        Selected
  0.000000000  0.4158207  0.7722862  0.2754392          
  0.003506003  0.4158207  0.7722862  0.2754392  *       
  0.025746847  0.4252383  0.7564815  0.2954407          
  0.067271573  0.4555975  0.6958288  0.3306647          
  0.325129801  0.6777677  0.3301384  0.4854682          
  0.493049739  0.8329110  0.1275955  0.5335659          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.003506003.
[1] "Sat Mar 10 03:46:37 2018"
CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results:

  RMSE       Rsquared   MAE      
  0.4158207  0.7722862  0.2754392

[1] "Sat Mar 10 03:46:50 2018"
CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  maxdepth  RMSE       Rsquared   MAE        Selected
  1         0.6777677  0.3301384  0.4854682          
  2         0.4555975  0.6958288  0.3306647          
  3         0.4252383  0.7564815  0.2954407          
  4         0.4158207  0.7722862  0.2754392  *       
  5         0.4158207  0.7722862  0.2754392          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was maxdepth = 4.
[1] "Sat Mar 10 03:47:05 2018"
Quantile Regression with LASSO penalty 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  lambda        RMSE       Rsquared   MAE        Selected
  1.824675e-05  0.5686997  0.6863580  0.3821830          
  1.889923e-05  0.5686997  0.6863580  0.3821830          
  3.060968e-05  0.5686997  0.6863580  0.3821830          
  7.200631e-05  0.5686997  0.6863580  0.3821830          
  1.204248e-04  0.5686997  0.6863580  0.3821830          
  1.416033e-04  0.5686997  0.6863580  0.3821830          
  1.464566e-03  0.5597720  0.6723733  0.3713170          
  2.612998e-03  0.5351202  0.6962790  0.3429940          
  7.837159e-03  0.5293572  0.7345526  0.3134159  *       
  2.838701e-02  0.5704122  0.7398326  0.3043711          
  6.293173e-02  0.6065222  0.7447680  0.3169233          
  9.468729e-02  0.6575458  0.7447680  0.3515192          
  1.296361e-01  0.7669236  0.7545839  0.4575641          
  1.659349e-01  0.8309564        NaN  0.5229399          
  1.726511e-01  0.8309564        NaN  0.5229399          
  2.588058e-01  0.8309564        NaN  0.5229399          
  3.857740e-01  0.8309564        NaN  0.5229399          
  5.573670e-01  0.8309564        NaN  0.5229399          
  1.283258e+00  0.8309564        NaN  0.5229399          
  1.943706e+00  0.8309564        NaN  0.5229399          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was lambda = 0.007837159.
[1] "Sat Mar 10 03:47:20 2018"
Non-Convex Penalized Quantile Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  lambda        penalty  RMSE       Rsquared   MAE        Selected
  1.824675e-05  SCAD     0.5686997  0.6863580  0.3821830          
  1.889923e-05  SCAD     0.5686997  0.6863580  0.3821830          
  3.060968e-05  SCAD     0.5686997  0.6863580  0.3821830          
  7.200631e-05  MCP      0.5686997  0.6863580  0.3821830          
  1.204248e-04  MCP      0.5686997  0.6863580  0.3821830          
  1.416033e-04  SCAD     0.5686997  0.6863580  0.3821830          
  1.464566e-03  SCAD     0.5657613  0.6837020  0.3790360          
  2.612998e-03  SCAD     0.5657613  0.6837020  0.3790360          
  7.837159e-03  SCAD     0.5571451  0.6911107  0.3687290          
  2.838701e-02  MCP      0.5310505  0.7220674  0.3349528          
  6.293173e-02  MCP      0.5248455  0.7447680  0.2992345          
  9.468729e-02  SCAD     0.5248455  0.7447680  0.2992345  *       
  1.296361e-01  SCAD     0.6361294  0.7545839  0.3825126          
  1.659349e-01  MCP      0.8309564        NaN  0.5229399          
  1.726511e-01  MCP      0.8309564        NaN  0.5229399          
  2.588058e-01  MCP      0.8309564        NaN  0.5229399          
  3.857740e-01  SCAD     0.8309564        NaN  0.5229399          
  5.573670e-01  SCAD     0.8309564        NaN  0.5229399          
  1.283258e+00  SCAD     0.8309564        NaN  0.5229399          
  1.943706e+00  MCP      0.8309564        NaN  0.5229399          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 0.09468729 and penalty = SCAD.
[1] "Sat Mar 10 03:47:35 2018"
Regularized Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  mtry  coefReg    coefImp     RMSE       Rsquared   MAE         Selected
  1     0.8011362  0.25790814  0.7403317  0.4120601  0.46994276          
  1     0.8432357  0.80092081  0.7379138  0.4158016  0.46580532          
  1     0.8932731  0.33335198  0.7343083  0.4525032  0.46600915          
  2     0.1171677  0.20416477  0.6166023  0.6866456  0.39249805          
  2     0.2497891  0.53647918  0.6309977  0.6701537  0.39855664          
  2     0.5604957  0.95082635  0.6832571  0.6825586  0.43324288          
  4     0.5764100  0.06109983  0.4370405  0.8712199  0.25428068          
  4     0.8300547  0.83962913  0.4538051  0.8532470  0.27101607          
  5     0.9566972  0.83385583  0.3712785  0.9088299  0.21119190          
  6     0.0982520  0.41494183  0.2724897  0.9507748  0.14473073          
  6     0.3837130  0.82331511  0.2688340  0.9417101  0.14616565          
  6     0.6949516  0.08815335  0.3187665  0.9270673  0.17318223          
  7     0.3019844  0.20071863  0.2429524  0.9525147  0.12466267          
  7     0.3667528  0.61487961  0.2354309  0.9525995  0.12230111          
  7     0.3923488  0.19621928  0.2511320  0.9527785  0.13361336          
  7     0.5747879  0.75164316  0.2424123  0.9509213  0.12361892          
  7     0.7991288  0.02768724  0.2563033  0.9542210  0.13545720          
  8     0.2947021  0.42020738  0.1850888  0.9666789  0.09548750          
  8     0.5547483  0.26139018  0.2124475  0.9618190  0.11411654          
  8     0.7182965  0.91326275  0.1731265  0.9726538  0.09128521  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 8, coefReg = 0.7182965
 and coefImp = 0.9132627.
[1] "Sat Mar 10 03:48:02 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 33 warnings (use warnings() to see them)
Regularized Random Forest 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  mtry  coefReg    RMSE       Rsquared   MAE        Selected
  1     0.8011362  0.7586487  0.3378896  0.4801751          
  1     0.8432357  0.7423049  0.3890375  0.4742548          
  1     0.8932731  0.7384377  0.4203316  0.4709080          
  2     0.1171677  0.6280052  0.6622000  0.3932673          
  2     0.2497891  0.6233202  0.6873511  0.3907808          
  2     0.5604957  0.6279895  0.6610990  0.4025935          
  4     0.5764100  0.4409997  0.8710745  0.2646223          
  4     0.8300547  0.4526342  0.8492313  0.2690882          
  5     0.9566972  0.3639333  0.9099782  0.2058297          
  6     0.0982520  0.2927505  0.9373049  0.1559641          
  6     0.3837130  0.3063746  0.9363697  0.1660217          
  6     0.6949516  0.3186966  0.9318386  0.1729310          
  7     0.3019844  0.2458078  0.9586049  0.1352278          
  7     0.3667528  0.2562211  0.9483449  0.1395992          
  7     0.3923488  0.2582769  0.9506671  0.1374145          
  7     0.5747879  0.2551288  0.9516214  0.1344929          
  7     0.7991288  0.2631098  0.9505042  0.1431997          
  8     0.2947021  0.2051451  0.9656120  0.1117522  *       
  8     0.5547483  0.2179929  0.9602080  0.1185551          
  8     0.7182965  0.2169999  0.9602132  0.1180377          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 8 and coefReg = 0.2947021.
[1] "Sat Mar 10 03:48:22 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
In addition: Warning messages:
1: model fit failed for Resample03: parameter=none Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
2: model fit failed for Resample04: parameter=none Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
3: model fit failed for Resample07: parameter=none Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
4: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
5: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:48:40 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "rvmLinear"               
Error in .local(object, ...) : test vector does not match model !
In addition: There were 45 warnings (use warnings() to see them)
Error in .local(object, ...) : test vector does not match model !
In addition: There were 22 warnings (use warnings() to see them)
Error in .local(object, ...) : test vector does not match model !
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:49:16 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "rvmPoly"                 
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
In addition: Warning messages:
1: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
2: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
3: model fit failed for Resample19: sigma=0.0684 Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:49:38 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "rvmRadial"               
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |======                                                                |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |===================                                                   |  27%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  31%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  39%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  12%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  22%  |                                                                              |================                                                      |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  27%  |                                                                              |=====================                                                 |  29%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |===========                                                           |  15%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  19%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  23%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  27%  |                                                                              |====================                                                  |  29%  |                                                                              |======================                                                |  31%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%
Subtractive Clustering and Fuzzy c-Means Rules 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  r.a  eps.high   eps.low      RMSE       Rsquared   MAE        Selected
   2   0.2764713  0.147669467  0.7125919  0.3501273  0.4731666  *       
   5   0.5070820  0.375994304        NaN        NaN        NaN          
   7   0.7148963  0.180655428        NaN        NaN        NaN          
   8   0.6404033  0.260244126        NaN        NaN        NaN          
   8   0.6435194  0.457955131        NaN        NaN        NaN          
  10   0.4746745  0.387029053        NaN        NaN        NaN          
  12   0.4242409  0.008104378        NaN        NaN        NaN          
  12   0.7709122  0.016529469        NaN        NaN        NaN          
  12   0.8822604  0.779740957        NaN        NaN        NaN          
  12   0.9169644  0.509730691        NaN        NaN        NaN          
  13   0.9718137  0.603668987        NaN        NaN        NaN          
  14   0.6422539  0.444638971        NaN        NaN        NaN          
  14   0.6809500  0.438221688        NaN        NaN        NaN          
  14   0.6894682  0.602712618        NaN        NaN        NaN          
  16   0.3675833  0.310510904        NaN        NaN        NaN          
  16   0.8320616  0.651436732        NaN        NaN        NaN          
  17   0.3292564  0.296333134        NaN        NaN        NaN          
  17   0.3447441  0.073198978        NaN        NaN        NaN          
  18   0.3506973  0.147572334        NaN        NaN        NaN          
  18   0.6060371  0.205293495        NaN        NaN        NaN          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were r.a = 2, eps.high = 0.2764713
 and eps.low = 0.1476695.
[1] "Sat Mar 10 03:50:10 2018"
Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      0.5822732  0.6101292  0.4136012          
  2      0.5336362  0.6775216  0.4120290          
  3      0.5294099  0.6942393  0.4171558  *       
  4      0.5326788  0.6904780  0.4201245          
  5      0.5356064  0.6866097  0.4226744          
  6      0.5358860  0.6861297  0.4229933          
  7      0.5359015  0.6860909  0.4230087          
  8      0.5359024  0.6860927  0.4230052          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 3.
[1] "Sat Mar 10 03:50:25 2018"
Error in mvrValstats(object = object, estimate = "train") : 
  could not find function "mvrValstats"
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Spike and Slab Regression 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  vars  RMSE       Rsquared   MAE        Selected
  1     0.4977755  0.7447680  0.3436689          
  2     0.4882423  0.7201303  0.3555753  *       
  4     0.5002026  0.7148865  0.3769494          
  5     0.5010180  0.7140772  0.3814830          
  6     0.5045220  0.7140116  0.3893832          
  7     0.5044882  0.7122819  0.3923275          
  8     0.5180605  0.7038668  0.4107716          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was vars = 2.
[1] "Sat Mar 10 03:50:49 2018"
Sparse Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  kappa       eta        K  RMSE       Rsquared   MAE        Selected
  0.02176545  0.8432357  8  0.5359023  0.6860926  0.4230051          
  0.02303702  0.8932731  4  0.5331088  0.6854503  0.4249813          
  0.04048823  0.8011362  3  0.5351399  0.6940267  0.4236262          
  0.07144755  0.1171677  2  0.5364051  0.6716142  0.4132412          
  0.09005966  0.2497891  5  0.5356114  0.6865823  0.4227106          
  0.09592278  0.5604957  9  0.5359023  0.6860927  0.4230051          
  0.18047575  0.5764100  1  0.4793752  0.7447680  0.3831266          
  0.20142826  0.8300547  8  0.5359023  0.6860926  0.4230051          
  0.24117989  0.9566972  8  0.5320920  0.6912738  0.4200054          
  0.28775997  0.3837130  8  0.5359023  0.6860926  0.4230051          
  0.31657247  0.0982520  4  0.5335487  0.6891407  0.4208328          
  0.33135764  0.6949516  1  0.4793752  0.7447680  0.3831266          
  0.34272717  0.5747879  7  0.5359014  0.6860956  0.4230026          
  0.35166149  0.3667528  6  0.5358972  0.6861096  0.4229965          
  0.35309745  0.3019844  2  0.5126290  0.7074750  0.4008990          
  0.36774784  0.3923488  2  0.5160240  0.7021298  0.4079016          
  0.38219442  0.7991288  1  0.4793752  0.7447680  0.3831266  *       
  0.39551177  0.7182965  9  0.5359023  0.6860927  0.4230051          
  0.42569284  0.5547483  3  0.5267029  0.6982019  0.4168338          
  0.44071921  0.2947021  4  0.5335487  0.6891407  0.4208328          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were K = 1, eta = 0.7991288 and kappa
 = 0.3821944.
[1] "Sat Mar 10 03:51:05 2018"
Supervised Principal Component Analysis 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  threshold   n.components  RMSE       Rsquared   MAE        Selected
  0.04353090  3             0.7079983  0.3753078  0.5049483          
  0.04607404  3             0.7079983  0.3753078  0.5049483          
  0.08097647  3             0.7079983  0.3753078  0.5049483          
  0.14289509  1             0.7242690  0.4057498  0.5110773          
  0.18011931  1             0.7189750  0.4198412  0.5081603          
  0.19184555  2             0.7209770  0.3789447  0.5132609          
  0.36095149  2             0.7249507  0.3700462  0.5188253          
  0.40285652  3             0.6797738  0.4567784  0.4997899          
  0.48235978  3             0.6094130  0.5782373  0.4476713          
  0.57551994  2             0.6341783  0.5103620  0.4574581          
  0.63314495  1             0.6614903  0.4602590  0.4864379          
  0.66271528  3             0.5179488  0.6868645  0.4037314          
  0.68545433  2             0.5701645  0.5801990  0.4369726          
  0.70332297  2             0.5551572  0.5996293  0.4216130          
  0.70619491  1             0.5823000  0.5643978  0.4306021          
  0.73549568  2             0.5551572  0.5996293  0.4216130          
  0.76438883  3             0.5162523  0.6886109  0.4021766  *       
  0.79102354  3             0.5162523  0.6886109  0.4021766          
  0.85138569  2             0.5551572  0.5996293  0.4216130          
  0.88143842  1             0.5887181  0.5441397  0.4445701          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were threshold = 0.7643888 and
 n.components = 3.
[1] "Sat Mar 10 03:51:19 2018"
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:51:33 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "svmBoundrangeString"     
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:51:47 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "svmExpoString"           
Support Vector Machines with Linear Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  C             RMSE       Rsquared   MAE        Selected
    0.04913734  0.5590343  0.6998492  0.3282981          
    0.05045393  0.5586887  0.6999381  0.3284223          
    0.07252625  0.5460212  0.7029656  0.3317707          
    0.13806565  0.5414400  0.7035902  0.3387741          
    0.20331515  0.5426647  0.7033708  0.3448104          
    0.22967779  0.5432688  0.7015973  0.3463777          
    1.33262639  0.5434842  0.7003636  0.3542746          
    2.06029046  0.5433493  0.7001264  0.3543197          
    4.70892144  0.5413301  0.7012681  0.3538357          
   12.40456370  0.5415420  0.7010635  0.3540576          
   22.58314057  0.5415481  0.7010970  0.3540902          
   30.71197096  0.5411686  0.7011693  0.3538630  *       
   38.90314683  0.5430425  0.6996071  0.3559551          
   46.84569476  0.5430151  0.6996981  0.3558608          
   48.26560633  0.5412223  0.7011361  0.3539166          
   65.45516662  0.5431412  0.6995578  0.3560233          
   88.39130068  0.5433200  0.6996326  0.3559828          
  116.59427348  0.5429690  0.6997441  0.3558285          
  218.39338217  0.5432235  0.6997043  0.3558402          
  298.49776802  0.5430801  0.6998041  0.3557507          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was C = 30.71197.
[1] "Sat Mar 10 03:52:18 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Linear Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  cost          RMSE       Rsquared   MAE        Selected
    0.04913734  0.5590343  0.6998492  0.3282981          
    0.05045393  0.5586887  0.6999381  0.3284223          
    0.07252625  0.5460212  0.7029656  0.3317707          
    0.13806565  0.5414400  0.7035902  0.3387741          
    0.20331515  0.5426647  0.7033708  0.3448104          
    0.22967779  0.5432688  0.7015973  0.3463777          
    1.33262639  0.5434842  0.7003636  0.3542746          
    2.06029046  0.5433493  0.7001264  0.3543197          
    4.70892144  0.5413301  0.7012681  0.3538357          
   12.40456370  0.5415420  0.7010635  0.3540576          
   22.58314057  0.5415481  0.7010970  0.3540902          
   30.71197096  0.5411686  0.7011693  0.3538630  *       
   38.90314683  0.5430425  0.6996071  0.3559551          
   46.84569476  0.5430151  0.6996981  0.3558608          
   48.26560633  0.5412223  0.7011361  0.3539166          
   65.45516662  0.5431412  0.6995578  0.3560233          
   88.39130068  0.5433200  0.6996326  0.3559828          
  116.59427348  0.5429690  0.6997441  0.3558285          
  218.39338217  0.5432235  0.6997043  0.3558402          
  298.49776802  0.5430801  0.6998041  0.3557507          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cost = 30.71197.
[1] "Sat Mar 10 03:52:48 2018"
L2 Regularized Support Vector Machine (dual) with Linear Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  cost          Loss  RMSE       Rsquared   MAE        Selected
  1.785592e-03  L2    0.8251650  0.5946227  0.5179381          
  1.849666e-03  L2    0.8249900  0.5946227  0.5177757          
  3.000730e-03  L2    0.8221056  0.6018912  0.5147481          
  7.079690e-03  L1    0.8022728  0.5448774  0.4920264          
  1.186114e-02  L1    0.7851749  0.5514302  0.4747386          
  1.395486e-02  L2    0.7949192  0.5954852  0.4868842          
  1.454938e-01  L2    0.6290334  0.6685785  0.3558167          
  2.600986e-01  L2    0.5850555  0.6665401  0.3383609          
  7.830604e-01  L2    0.5483844  0.7022926  0.3322326          
  2.848888e+00  L1    0.5372554  0.6800423  0.4172893          
  6.333043e+00  L1    0.5392490  0.6817339  0.4276107          
  9.542092e+00  L2    0.5363262  0.7051831  0.3518989          
  1.307815e+01  L2    0.5383519  0.7033071  0.3549086          
  1.675430e+01  L1    0.5397473  0.6844529  0.4324890          
  1.743480e+01  L1    0.5397539  0.6845039  0.4326042          
  2.617130e+01  L1    0.5403655  0.6842233  0.4335904          
  3.906424e+01  L2    0.5276564  0.7087727  0.3514440  *       
  5.651143e+01  L2    0.5358294  0.7043552  0.3522221          
  1.304826e+02  L2    0.5371316  0.7055692  0.3689554          
  1.979193e+02  L1    0.5405985  0.6848380  0.4353366          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were cost = 39.06424 and Loss = L2.
[1] "Sat Mar 10 03:53:04 2018"
Support Vector Machines with Polynomial Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  degree  scale         C             RMSE       Rsquared   MAE        Selected
  1       4.179357e-05    0.26106330  0.8315835  0.6190132  0.5245156          
  1       2.109306e-04    8.26599085  0.7839984  0.6107229  0.4762071          
  1       9.358457e-03  614.12784030  0.5413205  0.7011899  0.3538699          
  1       1.765417e-01    0.45647953  0.5445669  0.7036191  0.3322739          
  1       2.951332e-01  129.23133663  0.5431349  0.6995595  0.3559999          
  1       5.435848e-01    1.00019384  0.5436675  0.7005576  0.3539608          
  2       3.317698e-05    2.33614125  0.8270230  0.6190158  0.5200178          
  2       1.081619e-03  163.11284863  0.5411690  0.7009704  0.3462629  *       
  2       1.136492e-02    0.05898523  0.7943287  0.6111686  0.4868331          
  2       4.830136e-02    0.07814515  0.6804266  0.6414265  0.3815565          
  2       2.512725e-01  193.26505170  1.0952148  0.2729526  0.9005362          
  2       1.178909e+00  182.00540575  1.1112836  0.2588582  0.8977474          
  3       3.649426e-04    2.46760373  0.7633960  0.6168864  0.4554578          
  3       3.988672e-04    0.25187493  0.8225481  0.6223799  0.5156548          
  3       8.793620e-04   18.67704256  0.5594484  0.7001040  0.3290931          
  3       1.201857e-03    0.24036342  0.8065958  0.6120653  0.4995001          
  3       8.724435e-03    0.47330847  0.6365561  0.6724891  0.3522870          
  3       1.114212e-02   77.42069516  0.6879208  0.4583927  0.5196065          
  3       6.422609e-02  415.56813323  0.6338429  0.5358821  0.4566960          
  3       1.722684e-01    0.04167450  0.6070942  0.6749092  0.3662945          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 2, scale = 0.001081619 and
 C = 163.1128.
[1] "Sat Mar 10 03:53:20 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Support Vector Machines with Radial Basis Function Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  sigma       C             RMSE       Rsquared   MAE        Selected
  0.01908047    0.41347145  0.6567738  0.6438861  0.3687353          
  0.02548806   33.28238594  0.6584483  0.4978017  0.4846971          
  0.02831072  129.52697843  0.6764912  0.4724557  0.4921703          
  0.02838191    1.07872971  0.5916490  0.6529081  0.3462500  *       
  0.03343768    0.63846856  0.6135158  0.6412167  0.3506277          
  0.03589743    0.06526901  0.7731302  0.5861217  0.4634518          
  0.03595857  195.84309277  0.6740812  0.4787567  0.4904934          
  0.03699505  766.76936740  0.6740355  0.4791908  0.4904115          
  0.03767020  235.04925425  0.6740163  0.4794510  0.4903249          
  0.04103612    7.36301433  0.6584676  0.5133937  0.4643874          
  0.04376537  681.68674314  0.6742820  0.4812608  0.4895152          
  0.05180764    2.74288892  0.6433101  0.5560286  0.4251344          
  0.07931273    0.20477735  0.7061841  0.5587293  0.4031517          
  0.07977928    8.45899836  0.6854863  0.4737043  0.4855399          
  0.09170414   44.68338955  0.6911703  0.4654730  0.4840319          
  0.10616327    1.38237029  0.6744261  0.5063683  0.4292959          
  0.13083220    0.26668541  0.7263683  0.4878599  0.4172719          
  0.14618530   11.91635591  0.7194478  0.4068297  0.4757043          
  0.15407272   45.39018341  0.7235427  0.3963149  0.4767032          
  0.15973160    0.06260202  0.7979120  0.4555682  0.4852821          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were sigma = 0.02838191 and C = 1.07873.
[1] "Sat Mar 10 03:53:36 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  C             RMSE       Rsquared   MAE        Selected
    0.04913734  0.7839526  0.5516170  0.4730892          
    0.05045393  0.7840675  0.5473885  0.4735132          
    0.07252625  0.7645304  0.5724554  0.4545938          
    0.13806565  0.7253597  0.5581230  0.4148057          
    0.20331515  0.7118737  0.5443840  0.4098288          
    0.22967779  0.6924158  0.5747564  0.3945013          
    1.33262639  0.6322702  0.5740889  0.4001834  *       
    2.06029046  0.6550037  0.5420716  0.4300599          
    4.70892144  0.6733749  0.5042002  0.4718886          
   12.40456370  0.6757096  0.4870377  0.4853358          
   22.58314057  0.6825737  0.4721927  0.4876588          
   30.71197096  0.6771884  0.4865540  0.4855752          
   38.90314683  0.6812630  0.4772900  0.4852088          
   46.84569476  0.6808419  0.4760160  0.4865929          
   48.26560633  0.6769256  0.4886950  0.4844603          
   65.45516662  0.6822539  0.4787194  0.4834299          
   88.39130068  0.6831683  0.4682484  0.4873242          
  116.59427348  0.6790822  0.4835407  0.4883405          
  218.39338217  0.6786211  0.4830054  0.4851922          
  298.49776802  0.6840006  0.4614470  0.4881082          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was C = 1.332626.
[1] "Sat Mar 10 03:53:51 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  sigma       C             RMSE       Rsquared   MAE        Selected
  0.01908047    0.41347145  0.6567738  0.6438861  0.3687353          
  0.02548806   33.28238594  0.6584483  0.4978017  0.4846971          
  0.02831072  129.52697843  0.6764912  0.4724557  0.4921703          
  0.02838191    1.07872971  0.5916490  0.6529081  0.3462500  *       
  0.03343768    0.63846856  0.6135158  0.6412167  0.3506277          
  0.03589743    0.06526901  0.7731302  0.5861217  0.4634518          
  0.03595857  195.84309277  0.6740812  0.4787567  0.4904934          
  0.03699505  766.76936740  0.6740355  0.4791908  0.4904115          
  0.03767020  235.04925425  0.6740163  0.4794510  0.4903249          
  0.04103612    7.36301433  0.6584676  0.5133937  0.4643874          
  0.04376537  681.68674314  0.6742820  0.4812608  0.4895152          
  0.05180764    2.74288892  0.6433101  0.5560286  0.4251344          
  0.07931273    0.20477735  0.7061841  0.5587293  0.4031517          
  0.07977928    8.45899836  0.6854863  0.4737043  0.4855399          
  0.09170414   44.68338955  0.6911703  0.4654730  0.4840319          
  0.10616327    1.38237029  0.6744261  0.5063683  0.4292959          
  0.13083220    0.26668541  0.7263683  0.4878599  0.4172719          
  0.14618530   11.91635591  0.7194478  0.4068297  0.4757043          
  0.15407272   45.39018341  0.7235427  0.3963149  0.4767032          
  0.15973160    0.06260202  0.7979120  0.4555682  0.4852821          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were sigma = 0.02838191 and C = 1.07873.
[1] "Sat Mar 10 03:54:07 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:54:20 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "svmSpectrumString"       
Bagged CART 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results:

  RMSE       Rsquared   MAE      
  0.3717541  0.8398861  0.2140371

[1] "Sat Mar 10 03:54:35 2018"
Partial Least Squares 

76 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 49, 51, 52 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      0.5822732  0.6101292  0.4136012          
  2      0.5336362  0.6775216  0.4120290          
  3      0.5294099  0.6942393  0.4171558  *       
  4      0.5326788  0.6904780  0.4201245          
  5      0.5356064  0.6866097  0.4226744          
  6      0.5358860  0.6861297  0.4229933          
  7      0.5359015  0.6860909  0.4230087          
  8      0.5359024  0.6860927  0.4230052          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 3.
[1] "Sat Mar 10 03:55:05 2018"
Error in mvrValstats(object = object, estimate = "train") : 
  could not find function "mvrValstats"
Error : package xgboost is required
Error : package xgboost is required
Error : package xgboost is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:55:20 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "xgbDART"                 
Error : package xgboost is required
Error : package xgboost is required
Error : package xgboost is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:55:34 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "xgbLinear"               
Error : package xgboost is required
Error : package xgboost is required
Error : package xgboost is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:55:47 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "xgbTree"                 
Error : package kohonen is required
Error : package kohonen is required
Error : package kohonen is required
 [1] "failed"                   "failed"                  
 [3] "Sat Mar 10 03:56:01 2018" "poly aC1^3 + bC1^2 + dC1"
 [5] "ignore"                   "none"                    
 [7] "quantile"                 "HOPPER"                  
 [9] "14th20hp3cv"              "xyf"                     
Fitting Repeat 1 

# weights:  23
initial  value 171.778758 
iter  10 value 136.886922
iter  20 value 136.317987
iter  30 value 135.926217
iter  40 value 135.922814
iter  50 value 135.611012
iter  60 value 135.269765
iter  70 value 135.267175
final  value 135.267134 
converged
Fitting Repeat 2 

# weights:  23
initial  value 172.778652 
iter  10 value 136.811634
iter  20 value 136.209549
iter  30 value 135.924753
iter  40 value 135.315034
iter  50 value 135.267378
iter  60 value 135.267145
final  value 135.267134 
converged
Fitting Repeat 3 

# weights:  23
initial  value 166.611716 
iter  10 value 136.728118
iter  20 value 136.156568
iter  30 value 135.926530
iter  40 value 135.768587
iter  50 value 135.313579
iter  60 value 135.269580
iter  70 value 135.267153
final  value 135.267136 
converged
Fitting Repeat 4 

# weights:  23
initial  value 156.254995 
iter  10 value 136.570552
iter  20 value 136.402240
iter  30 value 135.934214
iter  40 value 135.918871
iter  50 value 135.516570
iter  60 value 135.267923
final  value 135.267136 
converged
Fitting Repeat 5 

# weights:  23
initial  value 159.409448 
iter  10 value 136.647658
iter  20 value 136.384540
iter  30 value 135.926139
iter  40 value 135.896469
iter  50 value 135.315121
iter  60 value 135.267260
final  value 135.267135 
converged
Fitting Repeat 1 

# weights:  56
initial  value 218.977049 
iter  10 value 192.102100
iter  20 value 192.095571
iter  30 value 189.884206
iter  40 value 189.348163
iter  50 value 189.318674
iter  60 value 189.302678
iter  70 value 189.295972
iter  80 value 189.294113
iter  90 value 189.292910
iter 100 value 189.292289
final  value 189.292289 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 219.052361 
iter  10 value 165.584975
iter  20 value 165.570838
iter  30 value 165.502993
iter  40 value 165.234059
iter  50 value 165.081471
iter  60 value 165.077896
iter  70 value 165.077210
final  value 165.077073 
converged
Fitting Repeat 3 

# weights:  56
initial  value 161.746222 
iter  10 value 110.989967
iter  20 value 110.980306
iter  30 value 110.978151
iter  40 value 110.976511
iter  50 value 110.972922
iter  60 value 110.961445
iter  70 value 110.891005
iter  80 value 109.975187
iter  90 value 109.245532
iter 100 value 109.242956
final  value 109.242956 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 232.162182 
iter  10 value 192.760821
iter  20 value 192.755032
iter  30 value 192.751462
iter  40 value 192.749216
iter  50 value 192.744625
iter  60 value 192.735946
iter  70 value 192.721642
iter  80 value 192.698295
iter  90 value 192.653997
iter 100 value 192.539851
final  value 192.539851 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 37.719713 
iter  10 value 23.863566
iter  20 value 23.862973
iter  30 value 23.862133
iter  40 value 23.860654
iter  50 value 23.857100
iter  60 value 23.841393
iter  70 value 23.604748
iter  80 value 20.698741
iter  90 value 20.663953
iter 100 value 20.662446
final  value 20.662446 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 152.314698 
iter  10 value 137.564358
final  value 137.563129 
converged
Fitting Repeat 2 

# weights:  89
initial  value 186.134262 
iter  10 value 137.629011
final  value 137.563129 
converged
Fitting Repeat 3 

# weights:  89
initial  value 187.629879 
iter  10 value 137.615215
final  value 137.563129 
converged
Fitting Repeat 4 

# weights:  89
initial  value 179.910379 
iter  10 value 137.594002
final  value 137.563129 
converged
Fitting Repeat 5 

# weights:  89
initial  value 160.597122 
iter  10 value 137.659182
final  value 137.563129 
converged
Fitting Repeat 1 

# weights:  89
initial  value 260.708479 
iter  10 value 213.026629
iter  20 value 212.917464
iter  30 value 212.909980
iter  40 value 212.881087
iter  50 value 211.888761
iter  60 value 211.157260
iter  70 value 211.149104
iter  80 value 211.147281
iter  90 value 211.146906
iter 100 value 211.146677
final  value 211.146677 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 327.015966 
iter  10 value 252.618838
iter  20 value 252.494178
iter  30 value 252.487819
iter  40 value 252.415928
iter  50 value 252.243353
iter  60 value 252.185308
iter  70 value 252.182596
final  value 252.182197 
converged
Fitting Repeat 3 

# weights:  89
initial  value 144.139053 
iter  10 value 103.724389
iter  20 value 103.670553
iter  30 value 103.659772
iter  40 value 103.625216
iter  50 value 103.399642
iter  60 value 103.035990
iter  70 value 103.030326
iter  80 value 103.029295
iter  90 value 103.028555
iter 100 value 103.028073
final  value 103.028073 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 99.337709 
iter  10 value 71.776886
iter  20 value 71.737901
iter  30 value 71.496563
iter  40 value 71.400825
iter  50 value 71.396150
iter  60 value 71.395359
iter  70 value 71.395177
iter  70 value 71.395176
iter  70 value 71.395176
final  value 71.395176 
converged
Fitting Repeat 5 

# weights:  89
initial  value 220.887358 
iter  10 value 168.345219
iter  20 value 168.243283
iter  30 value 167.888469
iter  40 value 167.545073
iter  50 value 167.537145
iter  60 value 167.535668
final  value 167.535573 
converged
Fitting Repeat 1 

# weights:  67
initial  value 252.429767 
iter  10 value 179.963703
iter  20 value 179.949219
iter  20 value 179.949219
iter  20 value 179.949219
final  value 179.949219 
converged
Fitting Repeat 2 

# weights:  67
initial  value 161.241444 
iter  10 value 96.493676
iter  20 value 96.444369
final  value 96.444299 
converged
Fitting Repeat 3 

# weights:  67
initial  value 181.578324 
iter  10 value 137.924038
iter  20 value 137.908346
final  value 137.908313 
converged
Fitting Repeat 4 

# weights:  67
initial  value 216.567187 
iter  10 value 155.091621
iter  20 value 155.067483
final  value 155.067434 
converged
Fitting Repeat 5 

# weights:  67
initial  value 212.165648 
iter  10 value 144.328021
iter  20 value 144.294431
final  value 144.294355 
converged
Fitting Repeat 1 

# weights:  166
initial  value 240.883012 
iter  10 value 189.138086
iter  20 value 189.120324
iter  30 value 189.110286
iter  40 value 187.351096
iter  50 value 187.309571
iter  60 value 187.306754
iter  70 value 187.306025
iter  80 value 187.305471
iter  90 value 187.304113
iter 100 value 187.302864
final  value 187.302864 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 91.293736 
iter  10 value 60.338849
iter  20 value 60.330506
iter  30 value 60.197430
iter  40 value 57.780978
iter  50 value 57.770652
iter  60 value 57.769488
iter  70 value 57.768484
iter  80 value 57.768160
iter  90 value 57.767470
iter 100 value 57.767357
final  value 57.767357 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  166
initial  value 110.479479 
iter  10 value 103.364205
iter  20 value 103.295637
iter  30 value 101.868626
iter  40 value 101.549227
iter  50 value 101.547679
iter  60 value 101.546654
iter  70 value 101.545874
iter  80 value 101.545756
iter  90 value 101.545324
iter 100 value 101.544899
final  value 101.544899 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  166
initial  value 239.875070 
iter  10 value 187.794559
iter  20 value 187.779949
iter  30 value 187.769177
iter  40 value 187.753080
iter  50 value 187.065964
iter  60 value 185.699305
iter  70 value 185.697797
iter  80 value 185.696266
iter  90 value 185.695780
iter 100 value 185.695374
final  value 185.695374 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 217.751217 
iter  10 value 191.889588
iter  20 value 191.880740
iter  30 value 191.880587
iter  40 value 191.880403
iter  50 value 191.880177
iter  60 value 191.879888
iter  70 value 191.879505
iter  80 value 191.878966
iter  90 value 191.878149
iter 100 value 191.876768
final  value 191.876768 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  67
initial  value 161.579908 
iter  10 value 136.538004
iter  20 value 136.435502
iter  30 value 136.179961
iter  40 value 136.179097
final  value 136.179063 
converged
Fitting Repeat 2 

# weights:  67
initial  value 158.228071 
iter  10 value 136.541443
iter  20 value 136.469367
iter  30 value 136.263789
iter  40 value 136.179329
iter  50 value 136.179072
final  value 136.179062 
converged
Fitting Repeat 3 

# weights:  67
initial  value 174.233784 
iter  10 value 137.302263
iter  20 value 136.514019
iter  30 value 136.284807
iter  40 value 136.179739
iter  50 value 136.179084
final  value 136.179062 
converged
Fitting Repeat 4 

# weights:  67
initial  value 186.636805 
iter  10 value 136.753868
iter  20 value 136.499317
iter  30 value 136.214259
iter  40 value 136.179263
iter  50 value 136.179067
final  value 136.179062 
converged
Fitting Repeat 5 

# weights:  67
initial  value 165.642342 
iter  10 value 136.546784
iter  20 value 136.476274
iter  30 value 136.325526
iter  40 value 136.179359
iter  50 value 136.179081
final  value 136.179062 
converged
Fitting Repeat 1 

# weights:  155
initial  value 208.908022 
iter  10 value 110.969676
iter  20 value 110.965491
iter  20 value 110.965491
iter  20 value 110.965491
final  value 110.965491 
converged
Fitting Repeat 2 

# weights:  155
initial  value 245.523097 
iter  10 value 167.583994
final  value 167.574618 
converged
Fitting Repeat 3 

# weights:  155
initial  value 291.345932 
iter  10 value 181.398116
final  value 181.393112 
converged
Fitting Repeat 4 

# weights:  155
initial  value 262.183568 
iter  10 value 183.455378
iter  20 value 183.437613
final  value 183.437500 
converged
Fitting Repeat 5 

# weights:  155
initial  value 362.169620 
iter  10 value 256.113524
iter  20 value 256.077263
final  value 256.076840 
converged
Fitting Repeat 1 

# weights:  210
initial  value 150.939083 
iter  10 value 136.050861
final  value 136.050367 
converged
Fitting Repeat 2 

# weights:  210
initial  value 210.279094 
iter  10 value 136.051047
iter  20 value 136.050027
iter  30 value 136.049728
iter  40 value 136.049692
iter  50 value 136.049654
iter  60 value 136.049613
iter  70 value 136.049567
iter  80 value 136.049517
iter  90 value 136.049460
iter 100 value 136.049396
final  value 136.049396 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  210
initial  value 171.777107 
iter  10 value 136.055513
final  value 136.050499 
converged
Fitting Repeat 4 

# weights:  210
initial  value 169.020528 
iter  10 value 136.055322
final  value 136.050408 
converged
Fitting Repeat 5 

# weights:  210
initial  value 146.020920 
iter  10 value 136.050372
final  value 136.050366 
converged
Fitting Repeat 1 

# weights:  89
initial  value 92.104876 
iter  10 value 79.912402
iter  10 value 79.912402
iter  10 value 79.912402
final  value 79.912402 
converged
Fitting Repeat 2 

# weights:  89
initial  value 176.931433 
iter  10 value 142.979065
final  value 142.978940 
converged
Fitting Repeat 3 

# weights:  89
initial  value 168.978116 
iter  10 value 127.635674
final  value 127.635522 
converged
Fitting Repeat 4 

# weights:  89
initial  value 244.856912 
iter  10 value 224.181696
final  value 224.181669 
converged
Fitting Repeat 5 

# weights:  89
initial  value 78.234924 
iter  10 value 61.239504
final  value 61.239491 
converged
Fitting Repeat 1 

# weights:  23
initial  value 93.331352 
iter  10 value 73.346060
iter  20 value 73.272532
iter  30 value 73.171640
iter  40 value 72.818649
iter  50 value 72.631562
iter  60 value 72.629347
iter  70 value 72.629172
final  value 72.629165 
converged
Fitting Repeat 2 

# weights:  23
initial  value 90.811816 
iter  10 value 54.016202
iter  20 value 53.886523
iter  30 value 52.312867
iter  40 value 52.300687
iter  50 value 52.194094
iter  60 value 52.193327
iter  70 value 52.193227
final  value 52.193225 
converged
Fitting Repeat 3 

# weights:  23
initial  value 143.537918 
iter  10 value 97.265680
iter  20 value 97.165512
iter  30 value 96.897993
iter  40 value 96.836312
iter  50 value 96.832258
iter  60 value 96.335922
iter  70 value 96.241820
iter  80 value 96.239401
iter  90 value 96.235541
iter 100 value 96.234342
final  value 96.234342 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  23
initial  value 283.633826 
iter  10 value 238.035020
iter  20 value 237.869842
iter  30 value 237.695312
iter  40 value 235.099292
iter  50 value 235.073147
iter  60 value 235.071152
iter  70 value 235.070986
final  value 235.070958 
converged
Fitting Repeat 5 

# weights:  23
initial  value 154.662205 
iter  10 value 119.507490
iter  20 value 118.857811
iter  30 value 117.983972
iter  40 value 117.905125
iter  50 value 117.901511
iter  60 value 117.901187
final  value 117.901165 
converged
Fitting Repeat 1 

# weights:  45
initial  value 123.028953 
iter  10 value 93.141185
iter  20 value 93.137332
iter  30 value 93.136030
iter  40 value 93.135663
iter  50 value 93.135176
iter  60 value 93.134507
iter  70 value 93.133547
iter  80 value 93.132089
iter  90 value 93.129711
iter 100 value 93.125421
final  value 93.125421 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 144.845899 
iter  10 value 103.631260
iter  20 value 103.623829
iter  30 value 103.615022
iter  40 value 103.583319
iter  50 value 103.256315
iter  60 value 101.787595
iter  70 value 101.732830
iter  80 value 101.731694
iter  90 value 101.731293
iter 100 value 101.731212
final  value 101.731212 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  45
initial  value 122.266098 
iter  10 value 89.980682
iter  20 value 89.976229
iter  30 value 89.974420
iter  40 value 89.971145
iter  50 value 89.964724
iter  60 value 89.951257
iter  70 value 89.919037
iter  80 value 89.781659
iter  90 value 87.835919
iter 100 value 87.752614
final  value 87.752614 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 117.721779 
iter  10 value 77.055248
iter  20 value 77.049713
iter  30 value 77.031401
iter  40 value 76.985277
iter  50 value 76.802098
iter  60 value 75.318373
iter  70 value 74.491961
iter  80 value 74.481778
iter  90 value 74.473287
iter 100 value 74.471961
final  value 74.471961 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 111.909161 
iter  10 value 84.636181
iter  20 value 84.633188
iter  30 value 84.630935
iter  40 value 84.629044
iter  50 value 84.626039
iter  60 value 84.621726
iter  70 value 84.616175
iter  80 value 84.609203
iter  90 value 84.599917
iter 100 value 84.586638
final  value 84.586638 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  122
initial  value 191.918876 
iter  10 value 121.614420
final  value 121.557848 
converged
Fitting Repeat 2 

# weights:  122
initial  value 167.628545 
iter  10 value 122.291231
iter  20 value 122.247847
final  value 122.247841 
converged
Fitting Repeat 3 

# weights:  122
initial  value 134.003857 
iter  10 value 89.429550
iter  20 value 89.278921
final  value 89.278918 
converged
Fitting Repeat 4 

# weights:  122
initial  value 96.493857 
iter  10 value 58.118560
iter  20 value 57.892146
iter  30 value 57.885912
final  value 57.885900 
converged
Fitting Repeat 5 

# weights:  122
initial  value 214.959308 
iter  10 value 138.171253
iter  20 value 138.133476
iter  20 value 138.133475
iter  20 value 138.133475
final  value 138.133475 
converged
Fitting Repeat 1 

# weights:  56
initial  value 174.510729 
iter  10 value 136.416683
iter  20 value 136.205610
iter  30 value 135.424390
iter  40 value 135.407986
iter  50 value 135.407513
iter  60 value 135.407423
final  value 135.407418 
converged
Fitting Repeat 2 

# weights:  56
initial  value 175.831500 
iter  10 value 136.423123
iter  20 value 136.218746
iter  30 value 135.427499
iter  40 value 135.408650
iter  50 value 135.407737
iter  60 value 135.407444
final  value 135.407412 
converged
Fitting Repeat 3 

# weights:  56
initial  value 176.295678 
iter  10 value 136.551029
iter  20 value 136.218584
iter  30 value 135.975638
iter  40 value 135.421173
iter  50 value 135.408457
iter  60 value 135.407524
iter  70 value 135.407416
final  value 135.407411 
converged
Fitting Repeat 4 

# weights:  56
initial  value 190.067085 
iter  10 value 136.488830
iter  20 value 136.216047
iter  30 value 135.912311
iter  40 value 135.412662
iter  50 value 135.408295
iter  60 value 135.407519
iter  70 value 135.407414
iter  70 value 135.407413
iter  70 value 135.407413
final  value 135.407413 
converged
Fitting Repeat 5 

# weights:  56
initial  value 170.148833 
iter  10 value 136.353379
iter  20 value 136.202782
iter  30 value 135.529305
iter  40 value 135.409554
iter  50 value 135.407838
iter  60 value 135.407452
final  value 135.407422 
converged
Fitting Repeat 1 

# weights:  23
initial  value 189.982805 
iter  10 value 136.078896
iter  20 value 136.066996
iter  30 value 136.053579
iter  40 value 136.049719
iter  50 value 136.044331
iter  60 value 136.036038
iter  70 value 136.021224
iter  80 value 135.986942
iter  90 value 135.850818
iter 100 value 135.072719
final  value 135.072719 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  23
initial  value 165.079991 
iter  10 value 136.079717
iter  20 value 136.069296
iter  30 value 136.060379
iter  40 value 136.056294
iter  50 value 136.049380
iter  60 value 136.037665
iter  70 value 136.016166
iter  80 value 135.961618
iter  90 value 135.676513
iter 100 value 135.011943
final  value 135.011943 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  23
initial  value 175.756544 
iter  10 value 136.072623
iter  20 value 136.064037
iter  30 value 136.063306
iter  40 value 136.062662
iter  50 value 136.061745
iter  60 value 136.060371
iter  70 value 136.058228
iter  80 value 136.054806
iter  90 value 136.049400
iter 100 value 136.041072
final  value 136.041072 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  23
initial  value 172.313058 
iter  10 value 136.073748
iter  20 value 136.063531
iter  30 value 135.882962
iter  40 value 135.116097
iter  50 value 135.010969
iter  60 value 135.007472
iter  70 value 135.004898
final  value 135.004720 
converged
Fitting Repeat 5 

# weights:  23
initial  value 154.100541 
iter  10 value 136.065876
iter  20 value 136.036594
iter  30 value 135.978518
iter  40 value 135.720446
iter  50 value 135.030629
iter  60 value 135.007790
iter  70 value 135.006419
iter  80 value 135.005019
iter  90 value 135.004665
final  value 135.004662 
converged
Fitting Repeat 1 

# weights:  177
initial  value 335.886273 
iter  10 value 148.137567
final  value 148.065741 
converged
Fitting Repeat 2 

# weights:  177
initial  value 339.755130 
iter  10 value 148.507188
iter  20 value 148.065872
final  value 148.065741 
converged
Fitting Repeat 3 

# weights:  177
initial  value 350.954466 
iter  10 value 148.130486
iter  20 value 148.065752
final  value 148.065741 
converged
Fitting Repeat 4 

# weights:  177
initial  value 370.862346 
iter  10 value 148.162192
iter  20 value 148.066010
final  value 148.065741 
converged
Fitting Repeat 5 

# weights:  177
initial  value 325.419204 
iter  10 value 148.127938
iter  20 value 148.065741
iter  20 value 148.065741
iter  20 value 148.065741
final  value 148.065741 
converged
Fitting Repeat 1 

# weights:  45
initial  value 198.450622 
iter  10 value 168.829010
iter  20 value 168.710659
iter  30 value 167.592456
iter  40 value 167.491595
iter  50 value 167.481705
iter  60 value 167.479099
iter  70 value 167.478307
final  value 167.478223 
converged
Fitting Repeat 2 

# weights:  45
initial  value 167.941114 
iter  10 value 126.118206
iter  20 value 126.062390
iter  30 value 125.777876
iter  40 value 125.081606
iter  50 value 124.976989
iter  60 value 124.958505
iter  70 value 124.945171
iter  80 value 124.914810
iter  90 value 124.904454
iter 100 value 124.903285
final  value 124.903285 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  45
initial  value 242.392028 
iter  10 value 183.878899
iter  20 value 183.771740
iter  30 value 183.732871
iter  40 value 182.825501
iter  50 value 182.596356
iter  60 value 182.589944
iter  70 value 182.588909
iter  80 value 182.579751
iter  90 value 182.251949
iter 100 value 182.249937
final  value 182.249937 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 147.561929 
iter  10 value 107.625819
iter  20 value 107.445745
iter  30 value 106.271766
iter  40 value 106.233758
iter  50 value 106.229863
iter  60 value 106.227053
iter  70 value 106.225256
iter  80 value 106.225057
final  value 106.225029 
converged
Fitting Repeat 5 

# weights:  45
initial  value 227.966010 
iter  10 value 164.509824
iter  20 value 164.359566
iter  30 value 164.355837
iter  40 value 164.136796
iter  50 value 164.096164
iter  60 value 164.095532
final  value 164.095489 
converged
Fitting Repeat 1 

# weights:  166
initial  value 431.089322 
iter  10 value 320.187537
iter  20 value 320.175756
final  value 320.175182 
converged
Fitting Repeat 2 

# weights:  166
initial  value 305.874986 
iter  10 value 231.096232
iter  20 value 231.090264
iter  30 value 231.080233
iter  40 value 230.905725
iter  50 value 229.043413
iter  60 value 228.956288
iter  70 value 228.953980
iter  80 value 228.951705
iter  90 value 228.950660
iter 100 value 228.950314
final  value 228.950314 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  166
initial  value 62.356714 
iter  10 value 39.176466
iter  20 value 39.174538
final  value 39.174478 
converged
Fitting Repeat 4 

# weights:  166
initial  value 292.462722 
iter  10 value 243.307065
final  value 243.290633 
converged
Fitting Repeat 5 

# weights:  166
initial  value 133.881092 
iter  10 value 92.289649
iter  20 value 92.286838
iter  30 value 92.252676
iter  40 value 91.480661
iter  50 value 89.806035
iter  60 value 89.755838
iter  70 value 89.743105
iter  80 value 89.736113
iter  90 value 89.202571
iter 100 value 89.162377
final  value 89.162377 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  45
initial  value 166.707665 
iter  10 value 136.051390
iter  20 value 136.048879
iter  30 value 136.048457
iter  40 value 136.047886
iter  50 value 136.047063
iter  60 value 136.045788
iter  70 value 136.043639
iter  80 value 136.039641
iter  90 value 136.031315
iter 100 value 136.011475
final  value 136.011475 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 185.193444 
iter  10 value 136.051337
iter  20 value 136.049948
iter  30 value 135.989243
iter  40 value 135.155137
iter  50 value 134.979366
iter  60 value 134.978645
iter  70 value 134.976211
iter  80 value 134.975238
iter  90 value 134.974821
iter 100 value 134.974685
final  value 134.974685 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  45
initial  value 180.630029 
iter  10 value 136.051312
iter  20 value 136.049751
iter  30 value 136.049727
iter  40 value 136.049701
iter  50 value 136.049672
iter  60 value 136.049639
iter  70 value 136.049602
iter  80 value 136.049560
iter  90 value 136.049513
iter 100 value 136.049458
final  value 136.049458 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 194.083823 
iter  10 value 136.050954
iter  20 value 136.049581
iter  30 value 136.049556
iter  40 value 136.049528
iter  50 value 136.049496
iter  60 value 136.049460
iter  70 value 136.049419
iter  80 value 136.049371
iter  90 value 136.049316
iter 100 value 136.049251
final  value 136.049251 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 192.978052 
iter  10 value 136.051762
final  value 136.050605 
converged
Fitting Repeat 1 

# weights:  89
initial  value 183.062857 
iter  10 value 136.100087
iter  20 value 136.065804
iter  30 value 136.053728
iter  40 value 136.005321
iter  50 value 135.538300
iter  60 value 135.029362
iter  70 value 135.021770
iter  80 value 135.019403
iter  90 value 135.018939
iter 100 value 135.018637
final  value 135.018637 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 174.098863 
iter  10 value 136.096240
iter  20 value 136.065110
iter  30 value 135.589084
iter  40 value 135.027312
iter  50 value 135.020428
iter  60 value 135.019399
iter  70 value 135.018867
iter  80 value 135.018596
iter  90 value 135.018407
iter  90 value 135.018406
iter  90 value 135.018405
final  value 135.018405 
converged
Fitting Repeat 3 

# weights:  89
initial  value 181.305844 
iter  10 value 136.097697
iter  20 value 136.066799
iter  30 value 136.049195
iter  40 value 135.906318
iter  50 value 135.066552
iter  60 value 135.026828
iter  70 value 135.022260
iter  80 value 135.020434
iter  90 value 135.019760
iter 100 value 135.019249
final  value 135.019249 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 167.526677 
iter  10 value 136.085829
iter  20 value 136.064116
iter  30 value 135.091968
iter  40 value 135.058937
iter  50 value 135.033940
iter  60 value 135.019234
iter  70 value 135.015832
iter  80 value 135.014109
iter  90 value 135.012679
iter 100 value 135.012212
final  value 135.012212 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 188.491736 
iter  10 value 136.098673
iter  20 value 136.065084
iter  30 value 136.061635
iter  40 value 136.054065
iter  50 value 136.019915
iter  60 value 135.640989
iter  70 value 135.029185
iter  80 value 135.021398
iter  90 value 135.019549
iter 100 value 135.019206
final  value 135.019206 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  23
initial  value 198.892328 
iter  10 value 161.024680
iter  20 value 160.621308
iter  30 value 159.753367
iter  40 value 159.712858
iter  50 value 159.711310
final  value 159.711303 
converged
Fitting Repeat 2 

# weights:  23
initial  value 190.370747 
iter  10 value 161.277010
iter  20 value 160.719857
iter  30 value 160.355692
iter  40 value 159.713381
iter  50 value 159.711308
iter  50 value 159.711307
iter  50 value 159.711306
final  value 159.711306 
converged
Fitting Repeat 3 

# weights:  23
initial  value 190.315151 
iter  10 value 160.894643
iter  20 value 160.069730
iter  30 value 159.712847
iter  40 value 159.711312
final  value 159.711302 
converged
Fitting Repeat 4 

# weights:  23
initial  value 179.555805 
iter  10 value 160.883301
iter  20 value 160.712130
iter  30 value 160.362803
iter  40 value 159.714197
iter  50 value 159.711371
final  value 159.711304 
converged
Fitting Repeat 5 

# weights:  23
initial  value 184.137210 
iter  10 value 160.984202
iter  20 value 160.603292
iter  30 value 159.765936
iter  40 value 159.713372
iter  50 value 159.711387
final  value 159.711303 
converged
Fitting Repeat 1 

# weights:  56
initial  value 318.073201 
iter  10 value 253.158493
iter  20 value 253.142910
iter  30 value 252.792663
iter  40 value 251.558329
iter  50 value 251.541754
iter  60 value 251.527366
iter  70 value 251.518255
iter  80 value 251.512636
iter  90 value 251.511278
iter 100 value 251.510057
final  value 251.510057 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 243.906965 
iter  10 value 197.967935
iter  20 value 197.959177
iter  30 value 197.815899
iter  40 value 196.742769
iter  50 value 196.727143
iter  60 value 196.405394
iter  70 value 196.120902
iter  80 value 196.093887
iter  90 value 196.080607
iter 100 value 196.076751
final  value 196.076751 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 112.190020 
iter  10 value 81.202783
iter  20 value 81.193941
iter  30 value 81.189752
final  value 81.188474 
converged
Fitting Repeat 4 

# weights:  56
initial  value 169.987294 
iter  10 value 125.683172
iter  20 value 125.646332
iter  30 value 123.640386
iter  40 value 123.451069
iter  50 value 123.439431
iter  60 value 123.298799
iter  70 value 123.293013
iter  80 value 123.288800
iter  90 value 123.287330
iter 100 value 123.286963
final  value 123.286963 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 148.875121 
iter  10 value 121.836132
iter  20 value 121.828789
iter  30 value 121.817679
iter  40 value 120.238385
iter  50 value 120.156806
iter  60 value 120.149342
iter  70 value 120.144886
iter  80 value 120.142613
iter  90 value 120.141366
iter 100 value 120.140782
final  value 120.140782 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 201.646579 
iter  10 value 161.823793
final  value 161.788102 
converged
Fitting Repeat 2 

# weights:  89
initial  value 204.222635 
iter  10 value 161.933876
final  value 161.788101 
converged
Fitting Repeat 3 

# weights:  89
initial  value 204.946997 
iter  10 value 161.824851
final  value 161.788101 
converged
Fitting Repeat 4 

# weights:  89
initial  value 196.721854 
iter  10 value 161.810905
final  value 161.788101 
converged
Fitting Repeat 5 

# weights:  89
initial  value 177.628860 
iter  10 value 161.789279
final  value 161.788101 
converged
Fitting Repeat 1 

# weights:  89
initial  value 376.380707 
iter  10 value 318.200887
iter  20 value 318.092469
iter  30 value 316.097408
iter  40 value 315.656160
iter  50 value 315.622074
iter  60 value 315.601742
iter  70 value 315.595980
iter  80 value 315.591542
iter  90 value 315.588414
iter 100 value 315.586681
final  value 315.586681 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 45.710570 
iter  10 value 36.088421
iter  20 value 36.061337
iter  30 value 35.977018
iter  40 value 34.833035
iter  50 value 34.272111
iter  60 value 34.183968
iter  70 value 34.169314
iter  80 value 34.165334
iter  90 value 34.163877
iter 100 value 34.162749
final  value 34.162749 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  89
initial  value 162.614423 
iter  10 value 120.799122
iter  20 value 118.881712
iter  30 value 116.913820
iter  40 value 116.215094
iter  50 value 116.167859
iter  60 value 116.101379
iter  70 value 116.092598
iter  80 value 116.089887
iter  90 value 116.088233
iter 100 value 116.087470
final  value 116.087470 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 48.032021 
iter  10 value 23.353711
iter  20 value 21.541632
iter  30 value 20.572287
iter  40 value 20.542088
iter  50 value 20.532892
iter  60 value 20.521161
iter  70 value 20.472996
iter  80 value 20.465538
iter  90 value 20.462469
iter 100 value 20.458171
final  value 20.458171 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 168.851106 
iter  10 value 134.868359
iter  20 value 134.798515
iter  30 value 134.332539
iter  40 value 133.113036
iter  50 value 133.016573
iter  60 value 132.985934
iter  70 value 132.979954
iter  80 value 132.975765
iter  90 value 132.974863
iter 100 value 132.974131
final  value 132.974131 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  67
initial  value 239.164795 
iter  10 value 176.947108
iter  20 value 176.934777
final  value 176.934675 
converged
Fitting Repeat 2 

# weights:  67
initial  value 227.101913 
iter  10 value 188.610259
iter  20 value 188.589346
final  value 188.589317 
converged
Fitting Repeat 3 

# weights:  67
initial  value 218.376950 
iter  10 value 168.151870
iter  20 value 168.147057
final  value 168.147049 
converged
Fitting Repeat 4 

# weights:  67
initial  value 415.586417 
iter  10 value 331.998213
iter  20 value 331.980782
iter  20 value 331.980780
iter  20 value 331.980779
final  value 331.980779 
converged
Fitting Repeat 5 

# weights:  67
initial  value 68.019201 
iter  10 value 31.926973
iter  20 value 31.909167
final  value 31.909117 
converged
Fitting Repeat 1 

# weights:  166
initial  value 198.983373 
iter  10 value 157.170371
iter  20 value 157.161554
iter  30 value 157.161325
iter  40 value 157.161031
iter  50 value 157.160634
iter  60 value 157.160071
iter  70 value 157.159205
iter  80 value 157.157714
iter  90 value 157.154615
iter 100 value 157.145283
final  value 157.145283 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 116.054453 
iter  10 value 65.886000
iter  20 value 65.883985
iter  30 value 65.883411
iter  40 value 65.883368
iter  50 value 65.883319
iter  60 value 65.883265
iter  70 value 65.883203
iter  80 value 65.883133
iter  90 value 65.883052
iter 100 value 65.882957
final  value 65.882957 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  166
initial  value 417.567094 
iter  10 value 339.056765
iter  20 value 339.033508
iter  30 value 339.030970
iter  40 value 339.030651
iter  50 value 339.030212
iter  60 value 339.029564
iter  70 value 339.028504
iter  80 value 339.026462
iter  90 value 339.021124
iter 100 value 338.989545
final  value 338.989545 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  166
initial  value 158.145817 
iter  10 value 123.668495
iter  20 value 123.661927
iter  30 value 123.660269
iter  40 value 123.660024
iter  50 value 123.659712
iter  60 value 123.659295
iter  70 value 123.658706
iter  80 value 123.657807
iter  90 value 123.656273
iter 100 value 123.653132
final  value 123.653132 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 41.819349 
iter  10 value 29.999499
iter  20 value 29.998570
final  value 29.998430 
converged
Fitting Repeat 1 

# weights:  67
initial  value 199.356867 
iter  10 value 161.728934
iter  20 value 160.790018
iter  30 value 160.211838
iter  40 value 160.150103
iter  50 value 160.150059
iter  50 value 160.150058
iter  50 value 160.150057
final  value 160.150057 
converged
Fitting Repeat 2 

# weights:  67
initial  value 189.713530 
iter  10 value 160.858496
iter  20 value 160.434860
iter  30 value 160.150491
iter  40 value 160.150092
final  value 160.150057 
converged
Fitting Repeat 3 

# weights:  67
initial  value 192.863507 
iter  10 value 160.861379
iter  20 value 160.721969
iter  30 value 160.157483
iter  40 value 160.150958
iter  50 value 160.150065
final  value 160.150061 
converged
Fitting Repeat 4 

# weights:  67
initial  value 192.912161 
iter  10 value 160.868224
iter  20 value 160.422130
iter  30 value 160.150522
iter  40 value 160.150114
final  value 160.150058 
converged
Fitting Repeat 5 

# weights:  67
initial  value 193.880975 
iter  10 value 160.825369
iter  20 value 160.223823
iter  30 value 160.152110
iter  40 value 160.150084
final  value 160.150062 
converged
Fitting Repeat 1 

# weights:  155
initial  value 94.850087 
iter  10 value 38.368206
iter  20 value 38.364505
final  value 38.364504 
converged
Fitting Repeat 2 

# weights:  155
initial  value 230.307199 
iter  10 value 167.680113
iter  20 value 167.666297
final  value 167.666293 
converged
Fitting Repeat 3 

# weights:  155
initial  value 257.968565 
iter  10 value 173.529581
iter  20 value 173.518076
final  value 173.517977 
converged
Fitting Repeat 4 

# weights:  155
initial  value 495.202479 
iter  10 value 350.804993
iter  20 value 350.745421
final  value 350.744963 
converged
Fitting Repeat 5 

# weights:  155
initial  value 251.657638 
iter  10 value 165.012365
iter  20 value 164.981828
final  value 164.981677 
converged
Fitting Repeat 1 

# weights:  210
initial  value 205.652699 
iter  10 value 160.386650
final  value 160.382103 
converged
Fitting Repeat 2 

# weights:  210
initial  value 179.296667 
iter  10 value 160.383844
final  value 160.382013 
converged
Fitting Repeat 3 

# weights:  210
initial  value 193.948237 
iter  10 value 160.386822
final  value 160.382168 
converged
Fitting Repeat 4 

# weights:  210
initial  value 231.324323 
iter  10 value 160.382855
iter  20 value 160.380516
iter  30 value 160.380411
iter  40 value 160.380285
iter  50 value 160.380131
iter  60 value 160.379940
iter  70 value 160.379695
iter  80 value 160.379371
iter  90 value 160.378926
iter 100 value 160.378281
final  value 160.378281 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  210
initial  value 229.024228 
iter  10 value 160.382401
final  value 160.382253 
converged
Fitting Repeat 1 

# weights:  89
initial  value 182.020680 
iter  10 value 154.688257
final  value 154.688188 
converged
Fitting Repeat 2 

# weights:  89
initial  value 200.188491 
iter  10 value 179.747087
final  value 179.746753 
converged
Fitting Repeat 3 

# weights:  89
initial  value 106.476812 
iter  10 value 70.155948
final  value 70.155890 
converged
Fitting Repeat 4 

# weights:  89
initial  value 264.180908 
iter  10 value 250.850261
final  value 250.850194 
converged
Fitting Repeat 5 

# weights:  89
initial  value 232.403731 
iter  10 value 172.823345
final  value 172.823111 
converged
Fitting Repeat 1 

# weights:  23
initial  value 60.781753 
iter  10 value 36.592657
iter  20 value 34.957110
iter  30 value 34.444889
iter  40 value 34.439236
iter  50 value 34.438837
iter  60 value 34.438778
final  value 34.438767 
converged
Fitting Repeat 2 

# weights:  23
initial  value 195.419146 
iter  10 value 165.086981
iter  20 value 164.364500
iter  30 value 162.631704
iter  40 value 162.319752
iter  50 value 162.235592
iter  60 value 162.215630
iter  70 value 162.211578
iter  80 value 162.210586
iter  90 value 162.210357
final  value 162.210352 
converged
Fitting Repeat 3 

# weights:  23
initial  value 238.627617 
iter  10 value 198.383626
iter  20 value 193.657613
iter  30 value 192.669256
iter  40 value 192.339607
iter  50 value 192.316270
iter  60 value 192.315004
iter  70 value 192.314828
final  value 192.314817 
converged
Fitting Repeat 4 

# weights:  23
initial  value 336.556835 
iter  10 value 284.828816
iter  20 value 284.359037
iter  30 value 283.498821
iter  40 value 283.455101
iter  50 value 283.451719
final  value 283.451556 
converged
Fitting Repeat 5 

# weights:  23
initial  value 177.526962 
iter  10 value 145.893153
iter  20 value 145.853155
iter  30 value 145.841349
iter  40 value 145.785854
iter  50 value 145.612112
iter  60 value 145.394281
iter  70 value 145.352206
iter  80 value 145.351008
iter  90 value 145.350850
iter  90 value 145.350849
iter  90 value 145.350849
final  value 145.350849 
converged
Fitting Repeat 1 

# weights:  45
initial  value 181.683549 
iter  10 value 149.442227
iter  20 value 149.438148
iter  30 value 149.435897
iter  40 value 149.435404
iter  50 value 149.434752
iter  60 value 149.433880
iter  70 value 149.432707
iter  80 value 149.431133
iter  90 value 149.429038
iter 100 value 149.426268
final  value 149.426268 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 245.377262 
iter  10 value 193.471694
iter  20 value 193.465490
iter  30 value 193.463728
iter  40 value 193.460080
iter  50 value 193.450303
iter  60 value 193.411946
iter  70 value 193.085491
iter  80 value 191.562917
iter  90 value 191.522759
iter 100 value 191.520390
final  value 191.520390 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  45
initial  value 103.784753 
iter  10 value 79.467750
iter  20 value 79.457555
iter  30 value 79.454042
iter  40 value 79.453593
iter  50 value 79.453064
iter  60 value 79.452438
iter  70 value 79.451692
iter  80 value 79.450797
iter  90 value 79.449710
iter 100 value 79.448368
final  value 79.448368 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 93.739334 
iter  10 value 58.155089
iter  20 value 58.151392
iter  30 value 58.150074
iter  40 value 58.147959
iter  50 value 58.144312
iter  60 value 58.137510
iter  70 value 58.123544
iter  80 value 58.088082
iter  90 value 57.895905
iter 100 value 55.978375
final  value 55.978375 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 118.193261 
iter  10 value 80.190521
iter  20 value 80.186465
iter  30 value 80.186007
iter  40 value 80.185683
iter  50 value 80.185296
iter  60 value 80.184816
iter  70 value 80.184195
iter  80 value 80.183354
iter  90 value 80.182160
iter 100 value 80.180368
final  value 80.180368 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  122
initial  value 189.605902 
iter  10 value 157.349357
iter  20 value 157.159484
iter  30 value 157.120942
iter  40 value 157.120468
final  value 157.120465 
converged
Fitting Repeat 2 

# weights:  122
initial  value 262.776811 
iter  10 value 208.154649
iter  20 value 208.027720
final  value 208.027709 
converged
Fitting Repeat 3 

# weights:  122
initial  value 151.738290 
iter  10 value 120.793138
iter  20 value 120.715203
final  value 120.715097 
converged
Fitting Repeat 4 

# weights:  122
initial  value 185.832684 
iter  10 value 124.086324
iter  20 value 124.024516
iter  30 value 123.797771
iter  40 value 123.797102
iter  40 value 123.797101
iter  40 value 123.797101
final  value 123.797101 
converged
Fitting Repeat 5 

# weights:  122
initial  value 211.299286 
iter  10 value 158.925039
iter  20 value 158.798016
iter  20 value 158.798015
iter  20 value 158.798015
final  value 158.798015 
converged
Fitting Repeat 1 

# weights:  56
initial  value 206.765293 
iter  10 value 160.802975
iter  20 value 160.515567
iter  30 value 160.275229
iter  40 value 159.206960
iter  50 value 159.148699
iter  60 value 159.144078
iter  70 value 159.142041
iter  80 value 159.141168
iter  90 value 159.141129
iter 100 value 159.141103
final  value 159.141103 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 175.899837 
iter  10 value 160.577750
iter  20 value 160.421562
iter  30 value 159.216564
iter  40 value 159.147599
iter  50 value 159.143116
iter  60 value 159.142216
iter  70 value 159.141185
final  value 159.141115 
converged
Fitting Repeat 3 

# weights:  56
initial  value 204.430334 
iter  10 value 160.804164
iter  20 value 160.519640
iter  30 value 160.176375
iter  40 value 159.169230
iter  50 value 159.147660
iter  60 value 159.145240
iter  70 value 159.141952
iter  80 value 159.141418
iter  90 value 159.141195
iter 100 value 159.141122
final  value 159.141122 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 210.045270 
iter  10 value 160.712979
iter  20 value 160.478596
iter  30 value 159.241243
iter  40 value 159.148467
iter  50 value 159.144986
iter  60 value 159.143351
iter  70 value 159.141524
iter  80 value 159.141221
iter  90 value 159.141127
final  value 159.141108 
converged
Fitting Repeat 5 

# weights:  56
initial  value 207.957952 
iter  10 value 160.813468
iter  20 value 160.516397
iter  30 value 160.351767
iter  40 value 159.201661
iter  50 value 159.146936
iter  60 value 159.142645
iter  70 value 159.141876
iter  80 value 159.141183
iter  90 value 159.141114
final  value 159.141107 
converged
Fitting Repeat 1 

# weights:  23
initial  value 212.933189 
iter  10 value 160.417322
iter  20 value 160.406757
iter  30 value 160.400566
iter  40 value 160.386826
iter  50 value 160.381361
iter  60 value 160.372073
iter  70 value 160.353141
iter  80 value 160.298055
iter  90 value 159.858073
iter 100 value 158.535888
final  value 158.535888 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  23
initial  value 183.214398 
iter  10 value 160.398912
iter  20 value 160.367068
iter  30 value 160.326082
iter  40 value 160.124412
iter  50 value 158.882891
iter  60 value 158.536428
iter  70 value 158.526416
iter  80 value 158.520436
iter  90 value 158.516659
iter 100 value 158.515422
final  value 158.515422 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  23
initial  value 205.046292 
iter  10 value 160.423491
iter  20 value 160.407507
iter  30 value 160.401228
iter  40 value 160.384664
iter  50 value 160.376994
iter  60 value 160.364042
iter  70 value 160.335234
iter  80 value 160.217815
iter  90 value 158.821756
iter 100 value 158.521962
final  value 158.521962 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  23
initial  value 185.552232 
iter  10 value 160.404998
iter  20 value 160.397016
iter  30 value 160.297013
iter  40 value 158.738224
iter  50 value 158.694641
iter  60 value 158.556685
iter  70 value 158.535223
iter  80 value 158.525159
iter  90 value 158.522185
iter 100 value 158.518531
final  value 158.518531 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  23
initial  value 200.806278 
iter  10 value 160.397962
iter  20 value 160.389657
iter  30 value 160.385693
iter  40 value 160.377961
iter  50 value 160.361469
iter  60 value 160.318664
iter  70 value 160.090187
iter  80 value 158.715030
iter  90 value 158.518495
iter 100 value 158.516366
final  value 158.516366 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 353.165406 
iter  10 value 171.801645
iter  20 value 171.734702
final  value 171.734690 
converged
Fitting Repeat 2 

# weights:  177
initial  value 367.538960 
iter  10 value 171.887093
iter  20 value 171.736290
final  value 171.734688 
converged
Fitting Repeat 3 

# weights:  177
initial  value 376.418831 
iter  10 value 171.832777
iter  20 value 171.734813
final  value 171.734731 
converged
Fitting Repeat 4 

# weights:  177
initial  value 364.070851 
iter  10 value 172.106751
iter  20 value 171.734743
final  value 171.734688 
converged
Fitting Repeat 5 

# weights:  177
initial  value 375.157452 
iter  10 value 171.847926
iter  20 value 171.734716
final  value 171.734687 
converged
Fitting Repeat 1 

# weights:  45
initial  value 72.016386 
iter  10 value 55.492024
iter  20 value 54.450410
iter  30 value 53.541960
iter  40 value 53.385991
iter  50 value 52.974281
iter  60 value 52.910237
iter  70 value 52.896209
iter  80 value 52.893693
iter  90 value 52.892834
iter 100 value 52.891886
final  value 52.891886 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 139.112922 
iter  10 value 111.079753
iter  20 value 111.037885
iter  30 value 109.618136
iter  40 value 109.564857
iter  50 value 109.555173
iter  60 value 109.554167
iter  70 value 109.553883
iter  80 value 109.553774
final  value 109.553756 
converged
Fitting Repeat 3 

# weights:  45
initial  value 195.224296 
iter  10 value 164.484522
iter  20 value 164.274035
iter  30 value 162.368882
iter  40 value 161.665593
iter  50 value 161.620687
iter  60 value 161.608388
iter  70 value 161.598510
iter  80 value 161.594613
iter  90 value 161.591446
iter 100 value 161.590964
final  value 161.590964 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 328.188913 
iter  10 value 290.441514
iter  20 value 290.209105
iter  30 value 288.668090
iter  40 value 288.545657
iter  50 value 288.486772
iter  60 value 288.464551
iter  70 value 288.459830
iter  80 value 288.457171
iter  90 value 288.455818
iter 100 value 288.455585
final  value 288.455585 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 401.994280 
iter  10 value 327.801586
iter  20 value 327.668886
iter  30 value 327.608258
iter  40 value 327.169401
iter  50 value 325.662758
iter  60 value 325.627622
iter  70 value 325.609133
iter  80 value 325.605801
iter  90 value 325.605109
final  value 325.605053 
converged
Fitting Repeat 1 

# weights:  166
initial  value 168.449585 
iter  10 value 134.913603
iter  20 value 134.909633
iter  30 value 134.902439
iter  40 value 134.896600
iter  50 value 134.867977
iter  60 value 133.252334
iter  70 value 130.934484
iter  80 value 130.931688
iter  90 value 130.930546
iter 100 value 130.930339
final  value 130.930339 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 274.869848 
iter  10 value 247.474137
final  value 247.469774 
converged
Fitting Repeat 3 

# weights:  166
initial  value 126.753869 
iter  10 value 112.004757
final  value 112.004496 
converged
Fitting Repeat 4 

# weights:  166
initial  value 137.694916 
iter  10 value 117.562272
iter  20 value 117.559405
iter  30 value 117.559235
iter  40 value 117.559028
iter  50 value 117.558764
iter  60 value 117.558409
iter  70 value 117.557901
iter  80 value 117.557112
iter  90 value 117.555734
iter 100 value 117.552848
final  value 117.552848 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 178.343781 
iter  10 value 127.364303
iter  20 value 127.359462
iter  30 value 127.358836
iter  40 value 127.357927
iter  50 value 127.357311
iter  60 value 127.356498
iter  70 value 127.355256
iter  80 value 127.352990
iter  90 value 127.347604
iter 100 value 127.326040
final  value 127.326040 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  45
initial  value 185.137990 
final  value 160.395513 
converged
Fitting Repeat 2 

# weights:  45
initial  value 187.643152 
final  value 160.382418 
converged
Fitting Repeat 3 

# weights:  45
initial  value 197.594446 
iter  10 value 160.386318
iter  20 value 160.383790
final  value 160.383766 
converged
Fitting Repeat 4 

# weights:  45
initial  value 192.015538 
final  value 160.382367 
converged
Fitting Repeat 5 

# weights:  45
initial  value 209.957195 
iter  10 value 160.382872
iter  10 value 160.382871
iter  10 value 160.382871
final  value 160.382871 
converged
Fitting Repeat 1 

# weights:  89
initial  value 211.250400 
iter  10 value 160.424613
iter  20 value 160.390754
iter  30 value 159.858766
iter  40 value 158.650574
iter  50 value 158.562424
iter  60 value 158.554192
iter  70 value 158.543514
iter  80 value 158.540514
iter  90 value 158.537785
iter 100 value 158.535958
final  value 158.535958 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 198.441818 
iter  10 value 160.422316
iter  20 value 160.395321
iter  30 value 160.381511
iter  40 value 160.322234
iter  50 value 159.527562
iter  60 value 158.574118
iter  70 value 158.551191
iter  80 value 158.542356
iter  90 value 158.538564
iter 100 value 158.537071
final  value 158.537071 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  89
initial  value 165.799539 
iter  10 value 160.397644
iter  20 value 159.254305
iter  30 value 158.613901
iter  40 value 158.589961
iter  50 value 158.566824
iter  60 value 158.559963
iter  70 value 158.555745
iter  80 value 158.552310
iter  90 value 158.550623
iter 100 value 158.547785
final  value 158.547785 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 187.817069 
iter  10 value 160.412625
iter  20 value 160.388189
iter  30 value 160.377355
iter  40 value 160.306452
iter  50 value 159.484699
iter  60 value 158.585385
iter  70 value 158.565538
iter  80 value 158.557145
iter  90 value 158.553048
iter 100 value 158.552616
final  value 158.552616 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 205.117929 
iter  10 value 160.422063
iter  20 value 160.396793
iter  30 value 160.265428
iter  40 value 158.768142
iter  50 value 158.601109
iter  60 value 158.559761
iter  70 value 158.547987
iter  80 value 158.541823
iter  90 value 158.538573
iter 100 value 158.537274
final  value 158.537274 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  23
initial  value 210.973950 
iter  10 value 181.521785
iter  20 value 181.321556
iter  30 value 180.693868
iter  40 value 180.577766
iter  50 value 180.574193
iter  60 value 180.574108
iter  60 value 180.574107
iter  60 value 180.574106
final  value 180.574106 
converged
Fitting Repeat 2 

# weights:  23
initial  value 200.679069 
iter  10 value 181.452239
iter  20 value 181.310628
iter  30 value 180.696826
iter  40 value 180.580871
iter  50 value 180.574252
iter  60 value 180.574117
final  value 180.574106 
converged
Fitting Repeat 3 

# weights:  23
initial  value 223.258994 
iter  10 value 181.608753
iter  20 value 181.342543
iter  30 value 181.208630
iter  40 value 180.720280
iter  50 value 180.576861
iter  60 value 180.574117
final  value 180.574103 
converged
Fitting Repeat 4 

# weights:  23
initial  value 225.696225 
iter  10 value 181.630618
iter  20 value 181.320014
iter  30 value 180.666492
iter  40 value 180.576593
iter  50 value 180.574239
final  value 180.574109 
converged
Fitting Repeat 5 

# weights:  23
initial  value 222.590176 
iter  10 value 181.727724
iter  20 value 181.346546
iter  30 value 181.248384
iter  40 value 180.661280
iter  50 value 180.574385
final  value 180.574105 
converged
Fitting Repeat 1 

# weights:  56
initial  value 214.263480 
iter  10 value 172.689277
iter  20 value 172.680563
iter  30 value 172.678249
iter  40 value 172.677679
iter  50 value 172.676792
iter  60 value 172.675235
iter  70 value 172.672021
iter  80 value 172.663844
iter  90 value 172.638905
iter 100 value 172.572074
final  value 172.572074 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  56
initial  value 244.253172 
iter  10 value 189.394557
iter  20 value 189.383478
iter  30 value 189.371574
iter  40 value 189.318434
iter  50 value 188.496118
iter  60 value 187.942981
iter  70 value 187.820492
iter  80 value 187.817742
iter  90 value 187.815961
iter 100 value 187.815422
final  value 187.815422 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 471.471725 
iter  10 value 418.619599
iter  20 value 418.594772
iter  30 value 418.578825
iter  40 value 418.528684
iter  50 value 418.303331
iter  60 value 417.852916
iter  70 value 417.829279
iter  80 value 417.825932
iter  90 value 417.824821
iter  90 value 417.824817
iter  90 value 417.824817
final  value 417.824817 
converged
Fitting Repeat 4 

# weights:  56
initial  value 358.348982 
iter  10 value 296.452360
iter  20 value 296.436200
iter  30 value 296.172625
iter  40 value 295.219373
iter  50 value 295.165193
iter  60 value 295.162279
iter  70 value 295.160150
iter  80 value 295.156988
iter  90 value 295.156183
iter 100 value 295.155609
final  value 295.155609 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  56
initial  value 373.137125 
iter  10 value 279.680045
iter  20 value 279.661349
iter  30 value 279.611324
iter  40 value 279.215212
iter  50 value 277.559283
iter  60 value 277.553812
iter  70 value 277.552217
iter  80 value 277.551366
iter  80 value 277.551364
iter  80 value 277.551364
final  value 277.551364 
converged
Fitting Repeat 1 

# weights:  89
initial  value 194.508901 
iter  10 value 182.429016
final  value 182.427789 
converged
Fitting Repeat 2 

# weights:  89
initial  value 223.680681 
iter  10 value 182.482587
final  value 182.427788 
converged
Fitting Repeat 3 

# weights:  89
initial  value 231.248636 
iter  10 value 182.530865
final  value 182.427788 
converged
Fitting Repeat 4 

# weights:  89
initial  value 207.330301 
iter  10 value 182.463660
final  value 182.427788 
converged
Fitting Repeat 5 

# weights:  89
initial  value 201.187225 
iter  10 value 182.432091
final  value 182.427788 
converged
Fitting Repeat 1 

# weights:  89
initial  value 175.671879 
iter  10 value 152.366079
iter  20 value 152.340049
iter  30 value 152.089951
iter  40 value 151.073131
iter  50 value 150.966547
iter  60 value 150.931004
iter  70 value 150.924457
iter  80 value 150.921396
iter  90 value 150.919431
iter 100 value 150.918449
final  value 150.918449 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 279.178285 
iter  10 value 230.538548
iter  20 value 230.433943
iter  30 value 230.312103
iter  40 value 229.735681
iter  50 value 229.539046
iter  60 value 229.535883
iter  70 value 229.535177
final  value 229.535083 
converged
Fitting Repeat 3 

# weights:  89
initial  value 361.667194 
iter  10 value 317.155786
iter  20 value 317.048636
iter  30 value 316.938380
iter  40 value 314.722486
iter  50 value 314.651672
iter  60 value 314.635096
iter  70 value 314.630464
iter  80 value 314.627911
iter  90 value 314.622709
iter 100 value 314.618573
final  value 314.618573 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 165.666601 
iter  10 value 133.727010
iter  20 value 133.668635
iter  30 value 133.665989
iter  40 value 133.664356
iter  50 value 133.662966
iter  60 value 133.660652
iter  70 value 133.655868
iter  80 value 133.642560
iter  90 value 133.593643
iter 100 value 133.440324
final  value 133.440324 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 136.764697 
iter  10 value 80.333084
iter  20 value 80.300601
iter  30 value 80.281295
iter  40 value 80.030409
iter  50 value 78.780085
iter  60 value 78.651593
iter  70 value 78.588408
iter  80 value 78.581843
iter  90 value 78.579533
iter 100 value 78.575057
final  value 78.575057 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  67
initial  value 119.218647 
iter  10 value 84.484366
iter  20 value 84.465255
final  value 84.465241 
converged
Fitting Repeat 2 

# weights:  67
initial  value 238.368241 
iter  10 value 202.522871
final  value 202.520583 
converged
Fitting Repeat 3 

# weights:  67
initial  value 241.562570 
iter  10 value 170.504278
iter  20 value 170.394515
final  value 170.394195 
converged
Fitting Repeat 4 

# weights:  67
initial  value 183.712070 
iter  10 value 149.179223
iter  20 value 149.163971
final  value 149.163963 
converged
Fitting Repeat 5 

# weights:  67
initial  value 156.951190 
iter  10 value 112.478606
iter  20 value 112.476300
final  value 112.476279 
converged
Fitting Repeat 1 

# weights:  166
initial  value 377.735889 
iter  10 value 333.352581
iter  20 value 333.322088
iter  30 value 333.314143
iter  40 value 333.302925
iter  50 value 333.167097
iter  60 value 331.363514
iter  70 value 330.706104
iter  80 value 330.414568
iter  90 value 330.402386
iter 100 value 330.400868
final  value 330.400868 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  166
initial  value 251.064301 
iter  10 value 219.778574
iter  20 value 219.764460
iter  30 value 219.762818
iter  40 value 219.757492
iter  50 value 219.753851
iter  60 value 219.744884
iter  70 value 219.711288
iter  80 value 219.508569
iter  90 value 219.095542
iter 100 value 219.090381
final  value 219.090381 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  166
initial  value 421.959481 
iter  10 value 332.794176
iter  20 value 332.773502
iter  30 value 332.769597
iter  40 value 332.764945
iter  50 value 332.745577
iter  60 value 331.609218
iter  70 value 330.770737
iter  80 value 330.769056
iter  90 value 330.451446
iter 100 value 330.435696
final  value 330.435696 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  166
initial  value 467.977362 
iter  10 value 440.519640
iter  20 value 440.498701
iter  30 value 439.457860
iter  40 value 438.726015
iter  50 value 438.708510
iter  60 value 438.703654
iter  70 value 438.701474
iter  80 value 438.700086
iter  90 value 438.698312
iter 100 value 438.697878
final  value 438.697878 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  166
initial  value 357.373274 
iter  10 value 275.018521
iter  20 value 274.991439
iter  30 value 274.989538
iter  40 value 274.987254
iter  50 value 274.985112
iter  60 value 274.980960
iter  70 value 274.970780
iter  80 value 274.935210
iter  90 value 274.797911
iter 100 value 274.383888
final  value 274.383888 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  67
initial  value 221.271241 
iter  10 value 181.404222
iter  20 value 181.373684
iter  30 value 181.373446
final  value 181.373435 
converged
Fitting Repeat 2 

# weights:  67
initial  value 223.025792 
iter  10 value 181.477241
iter  20 value 181.374180
iter  30 value 181.373461
final  value 181.373437 
converged
Fitting Repeat 3 

# weights:  67
initial  value 247.376033 
iter  10 value 182.333978
iter  20 value 181.381577
iter  30 value 181.373519
iter  40 value 181.373437
final  value 181.373434 
converged
Fitting Repeat 4 

# weights:  67
initial  value 228.225541 
iter  10 value 181.434419
iter  20 value 181.373825
iter  30 value 181.373455
final  value 181.373436 
converged
Fitting Repeat 5 

# weights:  67
initial  value 233.389329 
iter  10 value 182.309570
iter  20 value 181.378289
iter  30 value 181.373514
final  value 181.373437 
converged
Fitting Repeat 1 

# weights:  155
initial  value 304.349820 
iter  10 value 218.524491
iter  20 value 218.438447
final  value 218.438411 
converged
Fitting Repeat 2 

# weights:  155
initial  value 212.777987 
iter  10 value 139.256009
iter  20 value 139.242912
iter  20 value 139.242911
iter  20 value 139.242911
final  value 139.242911 
converged
Fitting Repeat 3 

# weights:  155
initial  value 420.037670 
iter  10 value 306.562951
iter  20 value 306.531183
final  value 306.531138 
converged
Fitting Repeat 4 

# weights:  155
initial  value 313.487112 
iter  10 value 228.043648
final  value 228.040528 
converged
Fitting Repeat 5 

# weights:  155
initial  value 303.763109 
iter  10 value 160.498671
iter  20 value 160.441072
final  value 160.441046 
converged
Fitting Repeat 1 

# weights:  210
initial  value 211.448562 
iter  10 value 180.925331
final  value 180.918840 
converged
Fitting Repeat 2 

# weights:  210
initial  value 234.532042 
iter  10 value 180.925692
final  value 180.918811 
converged
Fitting Repeat 3 

# weights:  210
initial  value 216.617978 
iter  10 value 180.924666
final  value 180.918952 
converged
Fitting Repeat 4 

# weights:  210
initial  value 233.915192 
iter  10 value 180.925334
final  value 180.918926 
converged
Fitting Repeat 5 

# weights:  210
initial  value 215.162944 
iter  10 value 180.924952
final  value 180.918868 
converged
Fitting Repeat 1 

# weights:  89
initial  value 344.083031 
iter  10 value 251.982334
final  value 251.982302 
converged
Fitting Repeat 2 

# weights:  89
initial  value 212.988908 
iter  10 value 173.734474
final  value 173.733971 
converged
Fitting Repeat 3 

# weights:  89
initial  value 89.846284 
iter  10 value 48.928089
final  value 48.927941 
converged
Fitting Repeat 4 

# weights:  89
initial  value 254.083310 
iter  10 value 211.346499
final  value 211.346340 
converged
Fitting Repeat 5 

# weights:  89
initial  value 239.555362 
iter  10 value 213.521457
final  value 213.521257 
converged
Fitting Repeat 1 

# weights:  23
initial  value 251.581617 
iter  10 value 193.495785
iter  20 value 193.324557
iter  30 value 193.263080
iter  40 value 192.666207
iter  50 value 192.076829
iter  60 value 192.073576
final  value 192.073451 
converged
Fitting Repeat 2 

# weights:  23
initial  value 271.511302 
iter  10 value 213.066875
iter  20 value 212.829891
iter  30 value 212.741799
iter  40 value 211.234813
iter  50 value 210.270297
iter  60 value 210.257594
iter  70 value 210.257238
final  value 210.257212 
converged
Fitting Repeat 3 

# weights:  23
initial  value 222.906264 
iter  10 value 204.684864
iter  20 value 204.392008
iter  30 value 201.449934
iter  40 value 201.417577
iter  50 value 201.314367
iter  60 value 201.311903
iter  70 value 201.311683
iter  70 value 201.311681
iter  70 value 201.311681
final  value 201.311681 
converged
Fitting Repeat 4 

# weights:  23
initial  value 243.322878 
iter  10 value 188.073728
iter  20 value 187.982532
iter  30 value 187.477199
iter  40 value 187.019542
iter  50 value 187.013124
iter  60 value 187.013000
iter  60 value 187.013000
iter  60 value 187.012999
final  value 187.012999 
converged
Fitting Repeat 5 

# weights:  23
initial  value 559.098958 
iter  10 value 480.542493
iter  20 value 480.209412
iter  30 value 480.166178
iter  40 value 480.114038
iter  50 value 479.945091
iter  60 value 479.238535
iter  70 value 479.232037
iter  80 value 479.231530
iter  80 value 479.231528
iter  80 value 479.231528
final  value 479.231528 
converged
Fitting Repeat 1 

# weights:  45
initial  value 122.870073 
iter  10 value 99.641491
iter  20 value 99.639623
iter  30 value 99.638792
iter  40 value 99.637546
iter  50 value 99.635280
iter  60 value 99.630816
iter  70 value 99.622959
iter  80 value 99.611296
iter  90 value 99.592878
iter 100 value 99.553503
final  value 99.553503 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 51.700860 
iter  10 value 34.239173
iter  20 value 34.235413
iter  30 value 34.235183
iter  40 value 34.234885
iter  50 value 34.234485
iter  60 value 34.233927
iter  70 value 34.233111
iter  80 value 34.231864
iter  90 value 34.229911
iter 100 value 34.226942
final  value 34.226942 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  45
initial  value 221.533433 
iter  10 value 185.206556
iter  20 value 185.200913
iter  30 value 185.200693
iter  40 value 185.200428
iter  50 value 185.200096
iter  60 value 185.199666
iter  70 value 185.199084
iter  80 value 185.198249
iter  90 value 185.196949
iter 100 value 185.194672
final  value 185.194672 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 383.899102 
iter  10 value 317.031048
iter  20 value 317.023762
iter  30 value 317.022928
iter  40 value 317.022530
iter  50 value 317.022036
iter  60 value 317.021403
iter  70 value 317.020560
iter  80 value 317.019388
iter  90 value 317.017681
iter 100 value 317.015048
final  value 317.015048 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 82.976922 
iter  10 value 44.768830
iter  20 value 44.761489
iter  30 value 44.759284
iter  40 value 44.755265
iter  50 value 44.745506
iter  60 value 44.708570
iter  70 value 44.328300
iter  80 value 42.243150
iter  90 value 42.083715
iter 100 value 41.984853
final  value 41.984853 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  122
initial  value 404.082519 
iter  10 value 319.663833
iter  20 value 319.440838
final  value 319.440810 
converged
Fitting Repeat 2 

# weights:  122
initial  value 345.029649 
iter  10 value 320.585361
iter  20 value 320.461177
iter  30 value 319.944473
iter  40 value 319.932986
final  value 319.932940 
converged
Fitting Repeat 3 

# weights:  122
initial  value 408.738539 
iter  10 value 348.392137
iter  20 value 347.960211
final  value 347.960178 
converged
Fitting Repeat 4 

# weights:  122
initial  value 372.518377 
iter  10 value 296.387450
iter  20 value 296.150187
final  value 296.150164 
converged
Fitting Repeat 5 

# weights:  122
initial  value 82.437970 
iter  10 value 47.963634
iter  20 value 47.745888
iter  30 value 47.744157
final  value 47.744154 
converged
Fitting Repeat 1 

# weights:  56
initial  value 206.213804 
iter  10 value 181.250431
iter  20 value 181.076950
iter  30 value 180.609210
iter  40 value 180.084358
iter  50 value 179.991409
iter  60 value 179.989771
final  value 179.989693 
converged
Fitting Repeat 2 

# weights:  56
initial  value 226.338892 
iter  10 value 181.446157
iter  20 value 181.076677
iter  30 value 180.143127
iter  40 value 179.997411
iter  50 value 179.992600
iter  60 value 179.991618
iter  70 value 179.990486
iter  80 value 179.990065
iter  90 value 179.989757
iter 100 value 179.989637
final  value 179.989637 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  56
initial  value 229.932351 
iter  10 value 181.413385
iter  20 value 181.082195
iter  30 value 180.167418
iter  40 value 180.001561
iter  50 value 179.992784
iter  60 value 179.991852
iter  70 value 179.990721
iter  80 value 179.990176
iter  90 value 179.989820
iter 100 value 179.989642
final  value 179.989642 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  56
initial  value 198.136908 
iter  10 value 181.143975
iter  20 value 181.052358
iter  30 value 180.754134
iter  40 value 180.122978
iter  50 value 179.998839
iter  60 value 179.990195
iter  70 value 179.989907
iter  80 value 179.989736
final  value 179.989638 
converged
Fitting Repeat 5 

# weights:  56
initial  value 213.416330 
iter  10 value 181.424169
iter  20 value 181.075433
iter  30 value 180.633960
iter  40 value 180.045961
iter  50 value 179.991709
iter  60 value 179.989831
iter  70 value 179.989754
iter  80 value 179.989648
final  value 179.989634 
converged
Fitting Repeat 1 

# weights:  23
initial  value 217.684052 
iter  10 value 180.946498
iter  20 value 180.936810
iter  30 value 180.933965
iter  40 value 180.933556
iter  50 value 180.933107
iter  60 value 180.932621
iter  70 value 180.932100
iter  80 value 180.931544
iter  90 value 180.930950
iter 100 value 180.930312
final  value 180.930312 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  23
initial  value 239.541672 
iter  10 value 180.944194
iter  20 value 180.939995
iter  30 value 180.934819
iter  40 value 180.933924
iter  50 value 180.932791
iter  60 value 180.931342
iter  70 value 180.929462
iter  80 value 180.926982
iter  90 value 180.923630
iter 100 value 180.918867
final  value 180.918867 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  23
initial  value 237.222477 
iter  10 value 180.962453
iter  20 value 180.938941
iter  30 value 180.935369
iter  40 value 180.797295
iter  50 value 179.668300
iter  60 value 179.351083
iter  70 value 179.343431
iter  80 value 179.340656
iter  90 value 179.338350
iter 100 value 179.336990
final  value 179.336990 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  23
initial  value 215.295194 
iter  10 value 180.944299
iter  20 value 180.933831
iter  30 value 180.927704
iter  40 value 180.924311
iter  50 value 180.918996
iter  60 value 180.909557
iter  70 value 180.887940
iter  80 value 180.808530
iter  90 value 180.222541
iter 100 value 179.466875
final  value 179.466875 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  23
initial  value 221.070638 
iter  10 value 180.953691
iter  20 value 180.939753
iter  30 value 180.935512
iter  40 value 180.916696
iter  50 value 180.870838
iter  60 value 180.708537
iter  70 value 179.628298
iter  80 value 179.466710
iter  90 value 179.342131
iter 100 value 179.338788
final  value 179.338788 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 424.777728 
iter  10 value 193.561147
iter  20 value 193.289596
final  value 193.289338 
converged
Fitting Repeat 2 

# weights:  177
initial  value 367.810902 
iter  10 value 193.680469
iter  20 value 193.290493
final  value 193.289341 
converged
Fitting Repeat 3 

# weights:  177
initial  value 404.144611 
iter  10 value 193.296911
iter  20 value 193.289344
final  value 193.289340 
converged
Fitting Repeat 4 

# weights:  177
initial  value 410.948632 
iter  10 value 193.297710
final  value 193.289338 
converged
Fitting Repeat 5 

# weights:  177
initial  value 404.904688 
iter  10 value 193.385252
iter  20 value 193.289384
final  value 193.289338 
converged
Fitting Repeat 1 

# weights:  45
initial  value 235.544155 
iter  10 value 184.515365
iter  20 value 184.386877
iter  30 value 184.262458
iter  40 value 184.048174
iter  50 value 183.489028
iter  60 value 183.144625
iter  70 value 183.079413
iter  80 value 183.077624
final  value 183.077558 
converged
Fitting Repeat 2 

# weights:  45
initial  value 216.298720 
iter  10 value 168.365630
iter  20 value 168.299423
iter  30 value 167.416851
iter  40 value 166.922425
iter  50 value 166.817877
iter  60 value 166.810975
iter  70 value 166.809920
iter  80 value 166.809533
iter  90 value 166.809401
final  value 166.809339 
converged
Fitting Repeat 3 

# weights:  45
initial  value 94.957388 
iter  10 value 54.060800
iter  20 value 53.974970
iter  30 value 53.245250
iter  40 value 52.207692
iter  50 value 52.165529
iter  60 value 52.161611
iter  70 value 51.089901
iter  80 value 51.053440
iter  90 value 51.039351
iter 100 value 51.038187
final  value 51.038187 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 192.828781 
iter  10 value 155.153052
iter  20 value 155.057511
iter  30 value 155.015473
iter  40 value 154.949925
iter  50 value 154.852574
iter  60 value 154.344662
iter  70 value 153.543586
iter  80 value 153.527611
iter  90 value 153.525986
iter 100 value 153.524696
final  value 153.524696 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 463.333652 
iter  10 value 374.036653
iter  20 value 373.821211
iter  30 value 373.788903
iter  40 value 373.215948
iter  50 value 373.147056
iter  60 value 373.127869
iter  70 value 373.126588
iter  80 value 373.126280
final  value 373.126228 
converged
Fitting Repeat 1 

# weights:  166
initial  value 149.177699 
iter  10 value 131.266289
final  value 131.265022 
converged
Fitting Repeat 2 

# weights:  166
initial  value 283.305093 
iter  10 value 228.018422
iter  20 value 228.014164
final  value 228.013491 
converged
Fitting Repeat 3 

# weights:  166
initial  value 248.252282 
iter  10 value 209.701999
final  value 209.695258 
converged
Fitting Repeat 4 

# weights:  166
initial  value 227.414206 
iter  10 value 200.737894
final  value 200.731181 
converged
Fitting Repeat 5 

# weights:  166
initial  value 226.914294 
iter  10 value 189.638596
final  value 189.631714 
converged
Fitting Repeat 1 

# weights:  45
initial  value 228.089142 
iter  10 value 180.919432
final  value 180.919427 
converged
Fitting Repeat 2 

# weights:  45
initial  value 240.812303 
iter  10 value 180.920256
iter  20 value 180.918762
iter  30 value 180.918579
iter  40 value 180.917529
iter  50 value 180.915001
iter  60 value 180.911750
iter  70 value 180.906281
iter  80 value 180.895449
iter  90 value 180.868874
iter 100 value 180.764909
final  value 180.764909 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  45
initial  value 239.654193 
iter  10 value 180.919184
final  value 180.919180 
converged
Fitting Repeat 4 

# weights:  45
initial  value 207.588134 
iter  10 value 180.918556
final  value 180.918494 
converged
Fitting Repeat 5 

# weights:  45
initial  value 201.832551 
final  value 180.933418 
converged
Fitting Repeat 1 

# weights:  89
initial  value 240.485516 
iter  10 value 180.970796
iter  20 value 180.934022
iter  30 value 180.920703
iter  40 value 180.882695
iter  50 value 180.568139
iter  60 value 179.632435
iter  70 value 179.513684
iter  80 value 179.389242
iter  90 value 179.381183
iter 100 value 179.376687
final  value 179.376687 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  89
initial  value 242.400404 
iter  10 value 180.975991
iter  20 value 180.937281
iter  30 value 180.931665
iter  40 value 180.928155
iter  50 value 180.921435
iter  60 value 180.904394
iter  70 value 180.835839
iter  80 value 180.343165
iter  90 value 179.523035
iter 100 value 179.496356
final  value 179.496356 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  89
initial  value 210.515592 
iter  10 value 180.964757
iter  20 value 180.932159
iter  30 value 180.802358
iter  40 value 179.570523
iter  50 value 179.512734
iter  60 value 179.388005
iter  70 value 179.379784
iter  80 value 179.377153
iter  90 value 179.376416
iter 100 value 179.376185
final  value 179.376185 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  89
initial  value 216.722715 
iter  10 value 180.974959
iter  20 value 180.933056
iter  30 value 180.524777
iter  40 value 179.649811
iter  50 value 179.515226
iter  60 value 179.387079
iter  70 value 179.380997
iter  80 value 179.375303
iter  90 value 179.374077
iter 100 value 179.371367
final  value 179.371367 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  89
initial  value 229.966733 
iter  10 value 180.982090
iter  20 value 180.934145
iter  30 value 180.929619
iter  40 value 180.924723
iter  50 value 180.913612
iter  60 value 180.874626
iter  70 value 180.695713
iter  80 value 179.784872
iter  90 value 179.526262
iter 100 value 179.423435
final  value 179.423435 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  23
initial  value 292.501578 
iter  10 value 238.713467
iter  20 value 238.698309
iter  30 value 238.680674
iter  40 value 238.671669
iter  50 value 238.656980
iter  60 value 238.624345
iter  70 value 238.476669
iter  80 value 236.737672
iter  90 value 236.360432
iter 100 value 236.354382
final  value 236.354382 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  23
initial  value 300.054317 
iter  10 value 238.696247
iter  20 value 238.690749
iter  30 value 238.690039
iter  40 value 238.689659
iter  50 value 238.689191
iter  60 value 238.688603
iter  70 value 238.687849
iter  80 value 238.686866
iter  90 value 238.685573
iter 100 value 238.683864
final  value 238.683864 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  23
initial  value 296.431439 
iter  10 value 238.703037
iter  20 value 238.695657
iter  30 value 238.687285
iter  40 value 238.685540
iter  50 value 238.683419
iter  60 value 238.680770
iter  70 value 238.677311
iter  80 value 238.672481
iter  90 value 238.665034
iter 100 value 238.651591
final  value 238.651591 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  23
initial  value 320.541770 
iter  10 value 238.715897
iter  20 value 238.703032
iter  30 value 238.692782
iter  40 value 238.685977
iter  50 value 238.682625
iter  60 value 238.677546
iter  70 value 238.669582
iter  80 value 238.655586
iter  90 value 238.623102
iter 100 value 238.467602
final  value 238.467602 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  23
initial  value 297.692848 
iter  10 value 238.699231
iter  20 value 238.692333
iter  30 value 238.689749
iter  40 value 238.687925
iter  50 value 238.685553
iter  60 value 238.682376
iter  70 value 238.677883
iter  80 value 238.671025
iter  90 value 238.659319
iter 100 value 238.634938
final  value 238.634938 
stopped after 100 iterations
Model Averaged Neural Network 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  size  decay         bag    RMSE      Rsquared    MAE       Selected
   2    3.158948e-04  FALSE  2.419056  0.08232800  1.419566  *       
   2    5.244112e-03   TRUE  2.419945  0.19966751  1.425147          
   2    1.772487e-02  FALSE  2.422060  0.10734610  1.423264          
   4    2.434648e-05  FALSE  2.421858  0.06138521  1.430135          
   4    1.283447e-04   TRUE  2.421431  0.07279572  1.428167          
   4    3.258141e-03   TRUE  2.425586  0.16621025  1.434783          
   5    2.777689e-04   TRUE  2.420405  0.11256373  1.418243          
   5    8.202677e-03  FALSE  2.421955  0.08973092  1.423936          
   6    3.330232e-02  FALSE  2.422764  0.15326322  1.423673          
   6    2.104776e+00   TRUE  2.455586  0.72047028  1.466859          
   8    6.531424e-04  FALSE  2.420803  0.08306175  1.424767          
   8    1.079118e-03   TRUE  2.423127  0.13307019  1.431253          
   8    1.736662e-01  FALSE  2.425094  0.57323988  1.434740          
   8    3.659612e-01   TRUE  2.428960  0.57825882  1.439109          
  11    9.240773e-02   TRUE  2.424148  0.24219607  1.433898          
  14    2.170325e+00   TRUE  2.441346  0.66594207  1.451660          
  15    7.081982e-05   TRUE  2.421355  0.17607295  1.424676          
  15    1.431076e-04   TRUE  2.419247  0.15466969  1.423026          
  16    6.022333e+00  FALSE  2.464580  0.71677909  1.474519          
  19    5.219718e-05  FALSE  2.422042  0.35822111  1.430746          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 2, decay = 0.0003158948 and
 bag = FALSE.
[1] "Sat Mar 10 04:00:41 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:00:52 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "bag"                            
Bagged MARS 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  degree  nprune  RMSE       Rsquared   MAE        Selected
  1       2       0.3620308  0.9747184  0.2844539          
  1       3       0.2305309  0.9930587  0.1480140          
  1       4       0.2292590  0.9941116  0.1379946          
  2       2       0.3939457  0.9720448  0.3136890          
  2       3       0.2260067  0.9935158  0.1385767  *       
  2       4       0.2617465  0.9910046  0.1284987          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nprune = 3 and degree = 2.
[1] "Sat Mar 10 04:01:18 2018"
Bagged MARS using gCV Pruning 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results:

  RMSE       Rsquared   MAE     
  0.2313172  0.9936693  0.147635

Tuning parameter 'degree' was held constant at a value of 1
[1] "Sat Mar 10 04:02:01 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :6     NA's   :6     NA's   :6    
Error : Stopping
In addition: There were 19 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 13 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:02:44 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "bam"                            
bartMachine initializing with 14 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 59.8/477.6MB
Iteration 200/1250  mem: 64.3/477.6MB
Iteration 300/1250  mem: 71.1/477.6MB
Iteration 400/1250  mem: 75.6/477.6MB
Iteration 500/1250  mem: 80.1/477.6MB
Iteration 600/1250  mem: 84.6/477.6MB
Iteration 700/1250  mem: 89.1/477.6MB
Iteration 800/1250  mem: 95.9/477.6MB
Iteration 900/1250  mem: 100.4/477.6MB
Iteration 1000/1250  mem: 104.9/477.6MB
Iteration 1100/1250  mem: 109.4/477.6MB
Iteration 1200/1250  mem: 113.9/477.6MB
done building BART in 0.112 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 30 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 131.3/477.6MB
Iteration 200/1250  mem: 140.3/477.6MB
Iteration 300/1250  mem: 44.3/477.6MB
Iteration 400/1250  mem: 54.1/477.6MB
Iteration 500/1250  mem: 65.4/477.6MB
Iteration 600/1250  mem: 75.3/477.6MB
Iteration 700/1250  mem: 85.1/477.6MB
Iteration 800/1250  mem: 96.4/477.6MB
Iteration 900/1250  mem: 106.3/477.6MB
Iteration 1000/1250  mem: 117.5/477.6MB
Iteration 1100/1250  mem: 127.4/477.6MB
Iteration 1200/1250  mem: 137.3/477.6MB
done building BART in 0.25 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 43 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 54.9/477.6MB
Iteration 200/1250  mem: 70.1/477.6MB
Iteration 300/1250  mem: 85.4/477.6MB
Iteration 400/1250  mem: 100.1/477.6MB
Iteration 500/1250  mem: 115.3/477.6MB
Iteration 600/1250  mem: 130.6/477.6MB
Iteration 700/1250  mem: 145.3/477.6MB
Iteration 800/1250  mem: 56/477.6MB
Iteration 900/1250  mem: 70.8/477.6MB
Iteration 1000/1250  mem: 86.9/477.6MB
Iteration 1100/1250  mem: 101.7/477.6MB
Iteration 1200/1250  mem: 116.6/477.6MB
done building BART in 0.395 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 44 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 141.1/477.6MB
Iteration 200/1250  mem: 157.3/477.6MB
Iteration 300/1250  mem: 65.4/477.6MB
Iteration 400/1250  mem: 80.9/477.6MB
Iteration 500/1250  mem: 96.5/477.6MB
Iteration 600/1250  mem: 111/477.6MB
Iteration 700/1250  mem: 126.6/477.6MB
Iteration 800/1250  mem: 141.1/477.6MB
Iteration 900/1250  mem: 156.7/477.6MB
Iteration 1000/1250  mem: 171.2/477.6MB
Iteration 1100/1250  mem: 84/477.6MB
Iteration 1200/1250  mem: 99.5/477.6MB
done building BART in 0.396 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 34 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 121.6/477.6MB
Iteration 200/1250  mem: 134.9/477.6MB
Iteration 300/1250  mem: 147/477.6MB
Iteration 400/1250  mem: 159.2/477.6MB
Iteration 500/1250  mem: 171.3/477.6MB
Iteration 600/1250  mem: 184.6/477.6MB
Iteration 700/1250  mem: 196.7/477.6MB
Iteration 800/1250  mem: 99.9/477.6MB
Iteration 900/1250  mem: 112/477.6MB
Iteration 1000/1250  mem: 124.1/477.6MB
Iteration 1100/1250  mem: 136.2/477.6MB
Iteration 1200/1250  mem: 148.2/477.6MB
done building BART in 0.314 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 76 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 186/477.6MB
Iteration 200/1250  mem: 104.4/477.6MB
Iteration 300/1250  mem: 136.4/477.6MB
Iteration 400/1250  mem: 169.6/477.6MB
Iteration 500/1250  mem: 207.5/477.6MB
Iteration 600/1250  mem: 141.6/477.6MB
Iteration 700/1250  mem: 177.9/477.6MB
Iteration 800/1250  mem: 215.9/477.6MB
Iteration 900/1250  mem: 153.8/477.6MB
Iteration 1000/1250  mem: 189.2/477.6MB
Iteration 1100/1250  mem: 224.6/477.6MB
Iteration 1200/1250  mem: 180.5/477.6MB
done building BART in 0.927 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 33 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 216.1/477.6MB
Iteration 200/1250  mem: 227.9/477.6MB
Iteration 300/1250  mem: 241.8/477.6MB
Iteration 400/1250  mem: 255.7/477.6MB
Iteration 500/1250  mem: 269.4/477.6MB
Iteration 600/1250  mem: 195.8/477.6MB
Iteration 700/1250  mem: 209.4/477.6MB
Iteration 800/1250  mem: 222.9/477.6MB
Iteration 900/1250  mem: 236.5/477.6MB
Iteration 1000/1250  mem: 250/477.6MB
Iteration 1100/1250  mem: 265.2/477.6MB
Iteration 1200/1250  mem: 278.8/477.6MB
done building BART in 0.369 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 73 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 236.2/477.6MB
Iteration 200/1250  mem: 264.1/477.6MB
Iteration 300/1250  mem: 291.9/477.6MB
Iteration 400/1250  mem: 226.3/477.6MB
Iteration 500/1250  mem: 255/477.6MB
Iteration 600/1250  mem: 283.7/477.6MB
Iteration 700/1250  mem: 310.4/477.6MB
Iteration 800/1250  mem: 248.8/477.6MB
Iteration 900/1250  mem: 275.7/477.6MB
Iteration 1000/1250  mem: 304.6/477.6MB
Iteration 1100/1250  mem: 242.7/477.6MB
Iteration 1200/1250  mem: 270.6/477.6MB
done building BART in 0.725 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 93 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 321.1/477.6MB
Iteration 200/1250  mem: 254/477.6MB
Iteration 300/1250  mem: 285.9/477.6MB
Iteration 400/1250  mem: 319.5/477.6MB
Iteration 500/1250  mem: 351.4/477.6MB
Iteration 600/1250  mem: 287.9/477.6MB
Iteration 700/1250  mem: 321.2/477.6MB
Iteration 800/1250  mem: 354.4/477.6MB
Iteration 900/1250  mem: 288.3/477.6MB
Iteration 1000/1250  mem: 319.4/477.6MB
Iteration 1100/1250  mem: 352.5/477.6MB
Iteration 1200/1250  mem: 385.6/477.6MB
done building BART in 0.864 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 44 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 322.5/477.6MB
Iteration 200/1250  mem: 339.1/477.6MB
Iteration 300/1250  mem: 353.3/477.6MB
Iteration 400/1250  mem: 367.6/477.6MB
Iteration 500/1250  mem: 384.1/477.6MB
Iteration 600/1250  mem: 398.4/477.6MB
Iteration 700/1250  mem: 60.6/488.6MB
Iteration 800/1250  mem: 75.9/488.6MB
Iteration 900/1250  mem: 91.2/488.6MB
Iteration 1000/1250  mem: 106.4/488.6MB
Iteration 1100/1250  mem: 121.7/488.6MB
Iteration 1200/1250  mem: 134.8/488.6MB
done building BART in 0.488 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 17 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 153.9/488.6MB
Iteration 200/1250  mem: 158.6/488.6MB
Iteration 300/1250  mem: 165.7/488.6MB
Iteration 400/1250  mem: 172.6/488.6MB
Iteration 500/1250  mem: 71.9/489.7MB
Iteration 600/1250  mem: 78/489.7MB
Iteration 700/1250  mem: 84.1/489.7MB
Iteration 800/1250  mem: 90.2/489.7MB
Iteration 900/1250  mem: 96.2/489.7MB
Iteration 1000/1250  mem: 102.3/489.7MB
Iteration 1100/1250  mem: 109.9/489.7MB
Iteration 1200/1250  mem: 116/489.7MB
done building BART in 0.159 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 25 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 132/489.7MB
Iteration 200/1250  mem: 143.1/489.7MB
Iteration 300/1250  mem: 154.1/489.7MB
Iteration 400/1250  mem: 165.1/489.7MB
Iteration 500/1250  mem: 177.4/489.7MB
Iteration 600/1250  mem: 188.5/489.7MB
Iteration 700/1250  mem: 85.5/491.3MB
Iteration 800/1250  mem: 97.2/491.3MB
Iteration 900/1250  mem: 109/491.3MB
Iteration 1000/1250  mem: 119.4/491.3MB
Iteration 1100/1250  mem: 131.1/491.3MB
Iteration 1200/1250  mem: 141.5/491.3MB
done building BART in 0.3 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 56 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 170.8/491.3MB
Iteration 200/1250  mem: 190.5/491.3MB
Iteration 300/1250  mem: 90.8/493.4MB
Iteration 400/1250  mem: 110/493.4MB
Iteration 500/1250  mem: 130.2/493.4MB
Iteration 600/1250  mem: 149.4/493.4MB
Iteration 700/1250  mem: 169.7/493.4MB
Iteration 800/1250  mem: 188.9/493.4MB
Iteration 900/1250  mem: 96.6/481.3MB
Iteration 1000/1250  mem: 117.6/481.3MB
Iteration 1100/1250  mem: 136.9/481.3MB
Iteration 1200/1250  mem: 156.2/481.3MB
done building BART in 0.495 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 29 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 179.3/481.3MB
Iteration 200/1250  mem: 190.6/481.3MB
Iteration 300/1250  mem: 200.6/481.3MB
Iteration 400/1250  mem: 211.9/481.3MB
Iteration 500/1250  mem: 110/494.4MB
Iteration 600/1250  mem: 120.3/494.4MB
Iteration 700/1250  mem: 130.6/494.4MB
Iteration 800/1250  mem: 142/494.4MB
Iteration 900/1250  mem: 153.5/494.4MB
Iteration 1000/1250  mem: 164.9/494.4MB
Iteration 1100/1250  mem: 176.4/494.4MB
Iteration 1200/1250  mem: 186.7/494.4MB
done building BART in 0.281 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 18 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 200.8/494.4MB
Iteration 200/1250  mem: 205.9/494.4MB
Iteration 300/1250  mem: 212.2/494.4MB
Iteration 400/1250  mem: 218.5/494.4MB
Iteration 500/1250  mem: 223.6/494.4MB
Iteration 600/1250  mem: 230/494.4MB
Iteration 700/1250  mem: 125.5/493.4MB
Iteration 800/1250  mem: 131.6/493.4MB
Iteration 900/1250  mem: 137.7/493.4MB
Iteration 1000/1250  mem: 142.8/493.4MB
Iteration 1100/1250  mem: 148.9/493.4MB
Iteration 1200/1250  mem: 155/493.4MB
done building BART in 0.169 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 80 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 189.2/493.4MB
Iteration 200/1250  mem: 215.8/493.4MB
Iteration 300/1250  mem: 243.7/493.4MB
Iteration 400/1250  mem: 151.5/495.5MB
Iteration 500/1250  mem: 179.3/495.5MB
Iteration 600/1250  mem: 207.1/495.5MB
Iteration 700/1250  mem: 233.4/495.5MB
Iteration 800/1250  mem: 145.1/484.4MB
Iteration 900/1250  mem: 173.7/484.4MB
Iteration 1000/1250  mem: 200.4/484.4MB
Iteration 1100/1250  mem: 229/484.4MB
Iteration 1200/1250  mem: 255.7/484.4MB
done building BART in 0.663 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 24 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 163.7/495.5MB
Iteration 200/1250  mem: 172.2/495.5MB
Iteration 300/1250  mem: 180/495.5MB
Iteration 400/1250  mem: 188.5/495.5MB
Iteration 500/1250  mem: 197/495.5MB
Iteration 600/1250  mem: 204.8/495.5MB
Iteration 700/1250  mem: 212.7/495.5MB
Iteration 800/1250  mem: 221.1/495.5MB
Iteration 900/1250  mem: 229/495.5MB
Iteration 1000/1250  mem: 237.4/495.5MB
Iteration 1100/1250  mem: 245.3/495.5MB
Iteration 1200/1250  mem: 253.1/495.5MB
done building BART in 0.216 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 77 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 171.7/494.9MB
Iteration 200/1250  mem: 202/494.9MB
Iteration 300/1250  mem: 234.2/494.9MB
Iteration 400/1250  mem: 266.4/494.9MB
Iteration 500/1250  mem: 185.7/495.5MB
Iteration 600/1250  mem: 217.7/495.5MB
Iteration 700/1250  mem: 251.3/495.5MB
Iteration 800/1250  mem: 283.3/495.5MB
Iteration 900/1250  mem: 212.3/496MB
Iteration 1000/1250  mem: 246.8/496MB
Iteration 1100/1250  mem: 283.1/496MB
Iteration 1200/1250  mem: 319.5/496MB
done building BART in 0.789 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 25 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 246.1/477.6MB
Iteration 200/1250  mem: 254.4/477.6MB
Iteration 300/1250  mem: 262.7/477.6MB
Iteration 400/1250  mem: 271/477.6MB
Iteration 500/1250  mem: 279.3/477.6MB
Iteration 600/1250  mem: 287.6/477.6MB
Iteration 700/1250  mem: 295.9/477.6MB
Iteration 800/1250  mem: 304.1/477.6MB
Iteration 900/1250  mem: 312.4/477.6MB
Iteration 1000/1250  mem: 320.7/477.6MB
Iteration 1100/1250  mem: 328.6/477.6MB
Iteration 1200/1250  mem: 254.4/477.6MB
done building BART in 0.275 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 44 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 275.4/477.6MB
Iteration 200/1250  mem: 290.1/477.6MB
Iteration 300/1250  mem: 304.8/477.6MB
Iteration 400/1250  mem: 319.4/477.6MB
Iteration 500/1250  mem: 334.1/477.6MB
Iteration 600/1250  mem: 264.6/477.6MB
Iteration 700/1250  mem: 280.1/477.6MB
Iteration 800/1250  mem: 293.8/477.6MB
Iteration 900/1250  mem: 309.2/477.6MB
Iteration 1000/1250  mem: 322.9/477.6MB
Iteration 1100/1250  mem: 338.3/477.6MB
Iteration 1200/1250  mem: 270.8/477.6MB
done building BART in 0.366 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 14 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 284.2/477.6MB
Iteration 200/1250  mem: 289.8/477.6MB
Iteration 300/1250  mem: 295.3/477.6MB
Iteration 400/1250  mem: 299/477.6MB
Iteration 500/1250  mem: 304.5/477.6MB
Iteration 600/1250  mem: 308.2/477.6MB
Iteration 700/1250  mem: 313.8/477.6MB
Iteration 800/1250  mem: 317.5/477.6MB
Iteration 900/1250  mem: 323/477.6MB
Iteration 1000/1250  mem: 328.5/477.6MB
Iteration 1100/1250  mem: 332.2/477.6MB
Iteration 1200/1250  mem: 337.8/477.6MB
done building BART in 0.108 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 30 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 351.4/477.6MB
Iteration 200/1250  mem: 278.4/477.6MB
Iteration 300/1250  mem: 289.4/477.6MB
Iteration 400/1250  mem: 300.3/477.6MB
Iteration 500/1250  mem: 310.2/477.6MB
Iteration 600/1250  mem: 321.1/477.6MB
Iteration 700/1250  mem: 332/477.6MB
Iteration 800/1250  mem: 343/477.6MB
Iteration 900/1250  mem: 353.9/477.6MB
Iteration 1000/1250  mem: 363.7/477.6MB
Iteration 1100/1250  mem: 290.8/477.6MB
Iteration 1200/1250  mem: 301.8/477.6MB
done building BART in 0.263 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 43 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 324.3/477.6MB
Iteration 200/1250  mem: 339.4/477.6MB
Iteration 300/1250  mem: 353.7/477.6MB
Iteration 400/1250  mem: 368.9/477.6MB
Iteration 500/1250  mem: 297.6/478.7MB
Iteration 600/1250  mem: 312.3/478.7MB
Iteration 700/1250  mem: 327/478.7MB
Iteration 800/1250  mem: 340.5/478.7MB
Iteration 900/1250  mem: 356.4/478.7MB
Iteration 1000/1250  mem: 369.9/478.7MB
Iteration 1100/1250  mem: 384.6/478.7MB
Iteration 1200/1250  mem: 309.8/479.7MB
done building BART in 0.373 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 44 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 335.3/479.7MB
Iteration 200/1250  mem: 350.4/479.7MB
Iteration 300/1250  mem: 366.5/479.7MB
Iteration 400/1250  mem: 381.6/479.7MB
Iteration 500/1250  mem: 396.7/479.7MB
Iteration 600/1250  mem: 320.2/481.8MB
Iteration 700/1250  mem: 334.1/481.8MB
Iteration 800/1250  mem: 349.4/481.8MB
Iteration 900/1250  mem: 364.7/481.8MB
Iteration 1000/1250  mem: 380/481.8MB
Iteration 1100/1250  mem: 395.3/481.8MB
Iteration 1200/1250  mem: 410.6/481.8MB
done building BART in 0.39 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 34 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 340.5/482.3MB
Iteration 200/1250  mem: 353.2/482.3MB
Iteration 300/1250  mem: 363.8/482.3MB
Iteration 400/1250  mem: 376.5/482.3MB
Iteration 500/1250  mem: 389.2/482.3MB
Iteration 600/1250  mem: 401.9/482.3MB
Iteration 700/1250  mem: 414.6/482.3MB
Iteration 800/1250  mem: 425.2/482.3MB
Iteration 900/1250  mem: 41.9/477.6MB
Iteration 1000/1250  mem: 54.8/477.6MB
Iteration 1100/1250  mem: 65.7/477.6MB
Iteration 1200/1250  mem: 78.6/477.6MB
done building BART in 0.399 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 76 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 117/477.6MB
Iteration 200/1250  mem: 46.1/477.6MB
Iteration 300/1250  mem: 80.1/477.6MB
Iteration 400/1250  mem: 115.2/477.6MB
Iteration 500/1250  mem: 149.6/477.6MB
Iteration 600/1250  mem: 93.4/477.6MB
Iteration 700/1250  mem: 130.9/477.6MB
Iteration 800/1250  mem: 169.9/477.6MB
Iteration 900/1250  mem: 118.5/477.6MB
Iteration 1000/1250  mem: 157/477.6MB
Iteration 1100/1250  mem: 195.4/477.6MB
Iteration 1200/1250  mem: 142.6/477.6MB
done building BART in 0.874 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 33 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 178.2/477.6MB
Iteration 200/1250  mem: 193/477.6MB
Iteration 300/1250  mem: 207.7/477.6MB
Iteration 400/1250  mem: 132.4/477.6MB
Iteration 500/1250  mem: 146.4/477.6MB
Iteration 600/1250  mem: 158.9/477.6MB
Iteration 700/1250  mem: 172.8/477.6MB
Iteration 800/1250  mem: 186.8/477.6MB
Iteration 900/1250  mem: 200.8/477.6MB
Iteration 1000/1250  mem: 214.8/477.6MB
Iteration 1100/1250  mem: 228.8/477.6MB
Iteration 1200/1250  mem: 155.7/477.6MB
done building BART in 0.369 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 73 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 193.7/477.6MB
Iteration 200/1250  mem: 221.6/477.6MB
Iteration 300/1250  mem: 249.6/477.6MB
Iteration 400/1250  mem: 178.5/477.6MB
Iteration 500/1250  mem: 207.7/477.6MB
Iteration 600/1250  mem: 234.9/477.6MB
Iteration 700/1250  mem: 170/477.6MB
Iteration 800/1250  mem: 197.1/477.6MB
Iteration 900/1250  mem: 226.3/477.6MB
Iteration 1000/1250  mem: 253.4/477.6MB
Iteration 1100/1250  mem: 185/477.6MB
Iteration 1200/1250  mem: 212.8/477.6MB
done building BART in 0.705 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 93 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 266.4/477.6MB
Iteration 200/1250  mem: 197.4/477.6MB
Iteration 300/1250  mem: 232.1/477.6MB
Iteration 400/1250  mem: 266.8/477.6MB
Iteration 500/1250  mem: 301.5/477.6MB
Iteration 600/1250  mem: 231.4/477.6MB
Iteration 700/1250  mem: 267.4/477.6MB
Iteration 800/1250  mem: 301.5/477.6MB
Iteration 900/1250  mem: 235.8/477.6MB
Iteration 1000/1250  mem: 270.7/477.6MB
Iteration 1100/1250  mem: 305.6/477.6MB
Iteration 1200/1250  mem: 340.5/477.6MB
done building BART in 0.859 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 44 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 274/477.6MB
Iteration 200/1250  mem: 288.4/477.6MB
Iteration 300/1250  mem: 305.2/477.6MB
Iteration 400/1250  mem: 319.6/477.6MB
Iteration 500/1250  mem: 334/477.6MB
Iteration 600/1250  mem: 350.8/477.6MB
Iteration 700/1250  mem: 254.8/477.6MB
Iteration 800/1250  mem: 270.6/477.6MB
Iteration 900/1250  mem: 284.2/477.6MB
Iteration 1000/1250  mem: 300/477.6MB
Iteration 1100/1250  mem: 315.8/477.6MB
Iteration 1200/1250  mem: 329.4/477.6MB
done building BART in 0.374 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 17 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 346.6/477.6MB
Iteration 200/1250  mem: 353.9/477.6MB
Iteration 300/1250  mem: 361.3/477.6MB
Iteration 400/1250  mem: 366.2/477.6MB
Iteration 500/1250  mem: 263/477.6MB
Iteration 600/1250  mem: 269.2/477.6MB
Iteration 700/1250  mem: 275.4/477.6MB
Iteration 800/1250  mem: 281.5/477.6MB
Iteration 900/1250  mem: 287.7/477.6MB
Iteration 1000/1250  mem: 293.9/477.6MB
Iteration 1100/1250  mem: 300.1/477.6MB
Iteration 1200/1250  mem: 307.8/477.6MB
done building BART in 0.166 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 25 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 323.8/477.6MB
Iteration 200/1250  mem: 334.9/477.6MB
Iteration 300/1250  mem: 345.9/477.6MB
Iteration 400/1250  mem: 357/477.6MB
Iteration 500/1250  mem: 369.2/477.6MB
Iteration 600/1250  mem: 380.3/477.6MB
Iteration 700/1250  mem: 277/477.6MB
Iteration 800/1250  mem: 289.1/477.6MB
Iteration 900/1250  mem: 299.9/477.6MB
Iteration 1000/1250  mem: 310.6/477.6MB
Iteration 1100/1250  mem: 322.8/477.6MB
Iteration 1200/1250  mem: 333.5/477.6MB
done building BART in 0.289 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 56 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 360.9/477.6MB
Iteration 200/1250  mem: 380.9/477.6MB
Iteration 300/1250  mem: 279.4/477.6MB
Iteration 400/1250  mem: 299.2/477.6MB
Iteration 500/1250  mem: 318.9/477.6MB
Iteration 600/1250  mem: 338.7/477.6MB
Iteration 700/1250  mem: 358.5/477.6MB
Iteration 800/1250  mem: 378.3/477.6MB
Iteration 900/1250  mem: 287.6/477.6MB
Iteration 1000/1250  mem: 307.3/477.6MB
Iteration 1100/1250  mem: 327.1/477.6MB
Iteration 1200/1250  mem: 346.8/477.6MB
done building BART in 0.482 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 29 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 370.1/477.6MB
Iteration 200/1250  mem: 381.4/477.6MB
Iteration 300/1250  mem: 391.5/477.6MB
Iteration 400/1250  mem: 402.8/477.6MB
Iteration 500/1250  mem: 301.3/477.6MB
Iteration 600/1250  mem: 311.8/477.6MB
Iteration 700/1250  mem: 322.4/477.6MB
Iteration 800/1250  mem: 334.1/477.6MB
Iteration 900/1250  mem: 344.6/477.6MB
Iteration 1000/1250  mem: 355.2/477.6MB
Iteration 1100/1250  mem: 366.9/477.6MB
Iteration 1200/1250  mem: 377.4/477.6MB
done building BART in 0.263 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 18 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 391.6/477.6MB
Iteration 200/1250  mem: 397.9/477.6MB
Iteration 300/1250  mem: 402.9/477.6MB
Iteration 400/1250  mem: 409.2/477.6MB
Iteration 500/1250  mem: 415.5/477.6MB
Iteration 600/1250  mem: 421.8/477.6MB
Iteration 700/1250  mem: 24.4/477.6MB
Iteration 800/1250  mem: 29.7/477.6MB
Iteration 900/1250  mem: 36/477.6MB
Iteration 1000/1250  mem: 41.3/477.6MB
Iteration 1100/1250  mem: 47.6/477.6MB
Iteration 1200/1250  mem: 53.9/477.6MB
done building BART in 0.24 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 80 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 87.2/477.6MB
Iteration 200/1250  mem: 115.3/477.6MB
Iteration 300/1250  mem: 142.2/477.6MB
Iteration 400/1250  mem: 50.9/477.6MB
Iteration 500/1250  mem: 77.3/477.6MB
Iteration 600/1250  mem: 105.3/477.6MB
Iteration 700/1250  mem: 133.3/477.6MB
Iteration 800/1250  mem: 45.9/477.6MB
Iteration 900/1250  mem: 73.1/477.6MB
Iteration 1000/1250  mem: 100.2/477.6MB
Iteration 1100/1250  mem: 127.3/477.6MB
Iteration 1200/1250  mem: 154.4/477.6MB
done building BART in 0.665 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 24 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 61.9/477.6MB
Iteration 200/1250  mem: 70.1/477.6MB
Iteration 300/1250  mem: 78.3/477.6MB
Iteration 400/1250  mem: 85.8/477.6MB
Iteration 500/1250  mem: 93.9/477.6MB
Iteration 600/1250  mem: 102.1/477.6MB
Iteration 700/1250  mem: 109.6/477.6MB
Iteration 800/1250  mem: 117.8/477.6MB
Iteration 900/1250  mem: 126/477.6MB
Iteration 1000/1250  mem: 134.2/477.6MB
Iteration 1100/1250  mem: 142.4/477.6MB
Iteration 1200/1250  mem: 151.2/477.6MB
done building BART in 0.212 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 77 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 68.9/477.6MB
Iteration 200/1250  mem: 99/477.6MB
Iteration 300/1250  mem: 131/477.6MB
Iteration 400/1250  mem: 164/477.6MB
Iteration 500/1250  mem: 196/477.6MB
Iteration 600/1250  mem: 112.6/477.6MB
Iteration 700/1250  mem: 143.9/477.6MB
Iteration 800/1250  mem: 176.7/477.6MB
Iteration 900/1250  mem: 211.1/477.6MB
Iteration 1000/1250  mem: 136.7/477.6MB
Iteration 1100/1250  mem: 171.6/477.6MB
Iteration 1200/1250  mem: 208.3/477.6MB
done building BART in 0.828 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 25 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 147.9/477.6MB
Iteration 200/1250  mem: 154.8/477.6MB
Iteration 300/1250  mem: 163.9/477.6MB
Iteration 400/1250  mem: 173/477.6MB
Iteration 500/1250  mem: 182.1/477.6MB
Iteration 600/1250  mem: 188.9/477.6MB
Iteration 700/1250  mem: 198/477.6MB
Iteration 800/1250  mem: 207.1/477.6MB
Iteration 900/1250  mem: 213.9/477.6MB
Iteration 1000/1250  mem: 223/477.6MB
Iteration 1100/1250  mem: 232.1/477.6MB
Iteration 1200/1250  mem: 239/477.6MB
done building BART in 0.188 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 44 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 159.1/477.6MB
Iteration 200/1250  mem: 174/477.6MB
Iteration 300/1250  mem: 188.9/477.6MB
Iteration 400/1250  mem: 203.9/477.6MB
Iteration 500/1250  mem: 217.7/477.6MB
Iteration 600/1250  mem: 232.6/477.6MB
Iteration 700/1250  mem: 247.5/477.6MB
Iteration 800/1250  mem: 159.3/477.6MB
Iteration 900/1250  mem: 174.6/477.6MB
Iteration 1000/1250  mem: 188.3/477.6MB
Iteration 1100/1250  mem: 203.6/477.6MB
Iteration 1200/1250  mem: 218.9/477.6MB
done building BART in 0.387 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 14 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 234.1/477.6MB
Iteration 200/1250  mem: 238.5/477.6MB
Iteration 300/1250  mem: 244.1/477.6MB
Iteration 400/1250  mem: 248.6/477.6MB
Iteration 500/1250  mem: 254.2/477.6MB
Iteration 600/1250  mem: 258.6/477.6MB
Iteration 700/1250  mem: 264.2/477.6MB
Iteration 800/1250  mem: 168.8/477.6MB
Iteration 900/1250  mem: 173.6/477.6MB
Iteration 1000/1250  mem: 178.3/477.6MB
Iteration 1100/1250  mem: 183.9/477.6MB
Iteration 1200/1250  mem: 188.6/477.6MB
done building BART in 0.13 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 30 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 204.6/477.6MB
Iteration 200/1250  mem: 215.8/477.6MB
Iteration 300/1250  mem: 227.1/477.6MB
Iteration 400/1250  mem: 238.4/477.6MB
Iteration 500/1250  mem: 248.5/477.6MB
Iteration 600/1250  mem: 259.8/477.6MB
Iteration 700/1250  mem: 269.9/477.6MB
Iteration 800/1250  mem: 178.9/477.6MB
Iteration 900/1250  mem: 188.7/477.6MB
Iteration 1000/1250  mem: 200/477.6MB
Iteration 1100/1250  mem: 209.9/477.6MB
Iteration 1200/1250  mem: 221.2/477.6MB
done building BART in 0.245 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 43 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 244.5/477.6MB
Iteration 200/1250  mem: 260.3/477.6MB
Iteration 300/1250  mem: 276.1/477.6MB
Iteration 400/1250  mem: 188.3/477.6MB
Iteration 500/1250  mem: 203.7/477.6MB
Iteration 600/1250  mem: 219.2/477.6MB
Iteration 700/1250  mem: 234.6/477.6MB
Iteration 800/1250  mem: 250.1/477.6MB
Iteration 900/1250  mem: 265.5/477.6MB
Iteration 1000/1250  mem: 281/477.6MB
Iteration 1100/1250  mem: 296.4/477.6MB
Iteration 1200/1250  mem: 207.9/477.6MB
done building BART in 0.404 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 44 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 233.9/477.6MB
Iteration 200/1250  mem: 250.1/477.6MB
Iteration 300/1250  mem: 266.2/477.6MB
Iteration 400/1250  mem: 281.2/477.6MB
Iteration 500/1250  mem: 297.4/477.6MB
Iteration 600/1250  mem: 207.7/477.6MB
Iteration 700/1250  mem: 223.3/477.6MB
Iteration 800/1250  mem: 238.9/477.6MB
Iteration 900/1250  mem: 256/477.6MB
Iteration 1000/1250  mem: 271.6/477.6MB
Iteration 1100/1250  mem: 287.2/477.6MB
Iteration 1200/1250  mem: 302.8/477.6MB
done building BART in 0.372 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 34 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 219.5/477.6MB
Iteration 200/1250  mem: 233/477.6MB
Iteration 300/1250  mem: 245.8/477.6MB
Iteration 400/1250  mem: 258.6/477.6MB
Iteration 500/1250  mem: 272.1/477.6MB
Iteration 600/1250  mem: 284.8/477.6MB
Iteration 700/1250  mem: 297.6/477.6MB
Iteration 800/1250  mem: 311.1/477.6MB
Iteration 900/1250  mem: 323.9/477.6MB
Iteration 1000/1250  mem: 230/477.6MB
Iteration 1100/1250  mem: 242/477.6MB
Iteration 1200/1250  mem: 255.4/477.6MB
done building BART in 0.34 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 76 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 294.7/477.6MB
Iteration 200/1250  mem: 325.4/477.6MB
Iteration 300/1250  mem: 243.1/477.6MB
Iteration 400/1250  mem: 276/477.6MB
Iteration 500/1250  mem: 309/477.6MB
Iteration 600/1250  mem: 343.5/477.6MB
Iteration 700/1250  mem: 279.7/477.6MB
Iteration 800/1250  mem: 318.1/477.6MB
Iteration 900/1250  mem: 358.2/477.6MB
Iteration 1000/1250  mem: 303/477.6MB
Iteration 1100/1250  mem: 341.8/477.6MB
Iteration 1200/1250  mem: 380.6/477.6MB
done building BART in 0.853 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 33 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 321.2/477.6MB
Iteration 200/1250  mem: 335.1/477.6MB
Iteration 300/1250  mem: 350.1/477.6MB
Iteration 400/1250  mem: 364/477.6MB
Iteration 500/1250  mem: 379.1/477.6MB
Iteration 600/1250  mem: 393/477.6MB
Iteration 700/1250  mem: 408/477.6MB
Iteration 800/1250  mem: 422.6/477.6MB
Iteration 900/1250  mem: 109.6/502.3MB
Iteration 1000/1250  mem: 123.3/502.3MB
Iteration 1100/1250  mem: 138.5/502.3MB
Iteration 1200/1250  mem: 153.7/502.3MB
done building BART in 0.651 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 73 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 192.7/502.3MB
Iteration 200/1250  mem: 127.2/498.6MB
Iteration 300/1250  mem: 156.4/498.6MB
Iteration 400/1250  mem: 185.6/498.6MB
Iteration 500/1250  mem: 214.8/498.6MB
Iteration 600/1250  mem: 142.2/501.2MB
Iteration 700/1250  mem: 172.5/501.2MB
Iteration 800/1250  mem: 200.9/501.2MB
Iteration 900/1250  mem: 229.3/501.2MB
Iteration 1000/1250  mem: 162.1/501.7MB
Iteration 1100/1250  mem: 192.3/501.7MB
Iteration 1200/1250  mem: 220.5/501.7MB
done building BART in 0.733 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 93 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 174.1/503.8MB
Iteration 200/1250  mem: 214.5/503.8MB
Iteration 300/1250  mem: 253.5/503.8MB
Iteration 400/1250  mem: 184/506.5MB
Iteration 500/1250  mem: 227.1/506.5MB
Iteration 600/1250  mem: 271.9/506.5MB
Iteration 700/1250  mem: 221.6/503.3MB
Iteration 800/1250  mem: 266.8/503.3MB
Iteration 900/1250  mem: 220.5/500.7MB
Iteration 1000/1250  mem: 267.6/500.7MB
Iteration 1100/1250  mem: 312.8/500.7MB
Iteration 1200/1250  mem: 270.2/494.4MB
done building BART in 1.112 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 44 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 313.8/494.4MB
Iteration 200/1250  mem: 329.4/494.4MB
Iteration 300/1250  mem: 248.2/503.3MB
Iteration 400/1250  mem: 263.9/503.3MB
Iteration 500/1250  mem: 279.6/503.3MB
Iteration 600/1250  mem: 295.3/503.3MB
Iteration 700/1250  mem: 311/503.3MB
Iteration 800/1250  mem: 326.7/503.3MB
Iteration 900/1250  mem: 342.4/503.3MB
Iteration 1000/1250  mem: 357.3/503.3MB
Iteration 1100/1250  mem: 273.6/480.2MB
Iteration 1200/1250  mem: 289.8/480.2MB
done building BART in 0.399 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 17 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 306.6/480.2MB
Iteration 200/1250  mem: 313.3/480.2MB
Iteration 300/1250  mem: 319.9/480.2MB
Iteration 400/1250  mem: 326.5/480.2MB
Iteration 500/1250  mem: 333.2/480.2MB
Iteration 600/1250  mem: 339.8/480.2MB
Iteration 700/1250  mem: 346.5/480.2MB
Iteration 800/1250  mem: 353.1/480.2MB
Iteration 900/1250  mem: 359.7/480.2MB
Iteration 1000/1250  mem: 366.4/480.2MB
Iteration 1100/1250  mem: 274.2/502.8MB
Iteration 1200/1250  mem: 281.5/502.8MB
done building BART in 0.157 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 25 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 298.1/502.8MB
Iteration 200/1250  mem: 310.4/502.8MB
Iteration 300/1250  mem: 321.6/502.8MB
Iteration 400/1250  mem: 333.9/502.8MB
Iteration 500/1250  mem: 345.1/502.8MB
Iteration 600/1250  mem: 357.3/502.8MB
Iteration 700/1250  mem: 368.5/502.8MB
Iteration 800/1250  mem: 380.8/502.8MB
Iteration 900/1250  mem: 285.3/501.7MB
Iteration 1000/1250  mem: 295.9/501.7MB
Iteration 1100/1250  mem: 308.1/501.7MB
Iteration 1200/1250  mem: 320.3/501.7MB
done building BART in 0.306 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 56 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 349.4/501.7MB
Iteration 200/1250  mem: 370.6/501.7MB
Iteration 300/1250  mem: 282.8/504.4MB
Iteration 400/1250  mem: 302.7/504.4MB
Iteration 500/1250  mem: 322.7/504.4MB
Iteration 600/1250  mem: 343.8/504.4MB
Iteration 700/1250  mem: 363.7/504.4MB
Iteration 800/1250  mem: 384.8/504.4MB
Iteration 900/1250  mem: 302.7/484.4MB
Iteration 1000/1250  mem: 322.9/484.4MB
Iteration 1100/1250  mem: 343.2/484.4MB
Iteration 1200/1250  mem: 363.5/484.4MB
done building BART in 0.459 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 29 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 386.2/484.4MB
Iteration 200/1250  mem: 397.6/484.4MB
Iteration 300/1250  mem: 305.9/503.8MB
Iteration 400/1250  mem: 317.4/503.8MB
Iteration 500/1250  mem: 328.8/503.8MB
Iteration 600/1250  mem: 340.3/503.8MB
Iteration 700/1250  mem: 350.8/503.8MB
Iteration 800/1250  mem: 362.3/503.8MB
Iteration 900/1250  mem: 373.7/503.8MB
Iteration 1000/1250  mem: 385.2/503.8MB
Iteration 1100/1250  mem: 395.7/503.8MB
Iteration 1200/1250  mem: 407.2/503.8MB
done building BART in 0.264 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 18 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 321.5/502.8MB
Iteration 200/1250  mem: 328.1/502.8MB
Iteration 300/1250  mem: 334.7/502.8MB
Iteration 400/1250  mem: 340.6/502.8MB
Iteration 500/1250  mem: 346.6/502.8MB
Iteration 600/1250  mem: 353.2/502.8MB
Iteration 700/1250  mem: 359.1/502.8MB
Iteration 800/1250  mem: 365.7/502.8MB
Iteration 900/1250  mem: 371.7/502.8MB
Iteration 1000/1250  mem: 377.6/502.8MB
Iteration 1100/1250  mem: 384.2/502.8MB
Iteration 1200/1250  mem: 390.2/502.8MB
done building BART in 0.173 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 80 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 424.9/502.8MB
Iteration 200/1250  mem: 346.6/505.9MB
Iteration 300/1250  mem: 375/505.9MB
Iteration 400/1250  mem: 404.5/505.9MB
Iteration 500/1250  mem: 433/505.9MB
Iteration 600/1250  mem: 352/505.4MB
Iteration 700/1250  mem: 379.9/505.4MB
Iteration 800/1250  mem: 409.2/505.4MB
Iteration 900/1250  mem: 437.1/505.4MB
Iteration 1000/1250  mem: 43.7/487.6MB
Iteration 1100/1250  mem: 73/487.6MB
Iteration 1200/1250  mem: 100.5/487.6MB
done building BART in 0.788 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 24 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 125.8/487.6MB
Iteration 200/1250  mem: 135.5/487.6MB
Iteration 300/1250  mem: 145.3/487.6MB
Iteration 400/1250  mem: 41.6/488.1MB
Iteration 500/1250  mem: 49.4/488.1MB
Iteration 600/1250  mem: 58.8/488.1MB
Iteration 700/1250  mem: 68.2/488.1MB
Iteration 800/1250  mem: 76/488.1MB
Iteration 900/1250  mem: 85.4/488.1MB
Iteration 1000/1250  mem: 93.2/488.1MB
Iteration 1100/1250  mem: 102.5/488.1MB
Iteration 1200/1250  mem: 110.4/488.1MB
done building BART in 0.189 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 77 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 148.6/488.1MB
Iteration 200/1250  mem: 69.6/490.2MB
Iteration 300/1250  mem: 101.7/490.2MB
Iteration 400/1250  mem: 135/490.2MB
Iteration 500/1250  mem: 168.2/490.2MB
Iteration 600/1250  mem: 92.4/490.7MB
Iteration 700/1250  mem: 127.7/490.7MB
Iteration 800/1250  mem: 166.3/490.7MB
Iteration 900/1250  mem: 102.2/477.6MB
Iteration 1000/1250  mem: 138.6/477.6MB
Iteration 1100/1250  mem: 175.1/477.6MB
Iteration 1200/1250  mem: 125.6/477.6MB
done building BART in 0.849 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 25 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 157.6/477.6MB
Iteration 200/1250  mem: 166.3/477.6MB
Iteration 300/1250  mem: 175/477.6MB
Iteration 400/1250  mem: 183.7/477.6MB
Iteration 500/1250  mem: 192.4/477.6MB
Iteration 600/1250  mem: 201.1/477.6MB
Iteration 700/1250  mem: 209.7/477.6MB
Iteration 800/1250  mem: 220.6/477.6MB
Iteration 900/1250  mem: 134.2/484.4MB
Iteration 1000/1250  mem: 141.7/484.4MB
Iteration 1100/1250  mem: 151.1/484.4MB
Iteration 1200/1250  mem: 160.5/484.4MB
done building BART in 0.238 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 44 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 182.5/484.4MB
Iteration 200/1250  mem: 197.7/484.4MB
Iteration 300/1250  mem: 213/484.4MB
Iteration 400/1250  mem: 228.3/484.4MB
Iteration 500/1250  mem: 143.1/477.6MB
Iteration 600/1250  mem: 158.6/477.6MB
Iteration 700/1250  mem: 174.1/477.6MB
Iteration 800/1250  mem: 189.6/477.6MB
Iteration 900/1250  mem: 205.1/477.6MB
Iteration 1000/1250  mem: 220.6/477.6MB
Iteration 1100/1250  mem: 236.1/477.6MB
Iteration 1200/1250  mem: 153.2/483.4MB
done building BART in 0.393 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 14 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 171/483.4MB
Iteration 200/1250  mem: 177.6/483.4MB
Iteration 300/1250  mem: 184.2/483.4MB
Iteration 400/1250  mem: 188.6/483.4MB
Iteration 500/1250  mem: 195.2/483.4MB
Iteration 600/1250  mem: 201.8/483.4MB
Iteration 700/1250  mem: 208.4/483.4MB
Iteration 800/1250  mem: 215/483.4MB
Iteration 900/1250  mem: 221.6/483.4MB
Iteration 1000/1250  mem: 228.2/483.4MB
Iteration 1100/1250  mem: 232.6/483.4MB
Iteration 1200/1250  mem: 239.2/483.4MB
done building BART in 0.133 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
Bayesian Additive Regression Trees 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  num_trees  k          alpha      beta        nu         RMSE       Rsquared 
  14         2.7071526  0.9776543  1.80573436  4.1519940  0.7968000  0.9431819
  17         2.2663933  0.9076242  1.16470070  1.9448419  0.8561955  0.9042244
  18         1.2496188  0.9825955  3.27909274  4.9028514  1.1401358  0.7757547
  24         2.0941416  0.9198677  2.21609184  0.6684435  0.9085580  0.8671878
  25         0.3220302  0.9589718  1.65393405  3.3550018  2.4038037  0.4172336
  25         0.9236483  0.9162561  0.05777341  0.1274060        NaN        NaN
  29         2.4282963  0.9783274  1.34454330  3.8222293  0.9328981  0.8378769
  30         1.2030697  0.9361548  1.72982259  2.3148481  0.9372481  0.8525895
  33         2.9353955  0.9549669  0.90469138  1.8286354  1.0164299  0.8147478
  34         4.4360049  0.9446159  1.97361676  3.8543236  1.5962209  0.7562664
  43         3.5330961  0.9635324  0.11727967  1.5477982  1.6846812  0.7102426
  44         1.5125066  0.9772684  3.69154300  4.7844182  0.9585726  0.8444975
  44         1.6942240  0.9013746  2.13159602  4.7868690  0.9563312  0.8573028
  44         3.8028626  0.9020566  3.36454125  4.4216699  0.9576988  0.8612717
  56         3.3047569  0.9311032  2.50850429  1.6768813  1.2185503  0.7979704
  73         4.4471040  0.9101791  1.40043004  1.7016846  1.9111237  0.6141211
  76         0.9630522  0.9449703  0.42791985  2.9834446  1.5481900  0.5136700
  77         0.7084623  0.9460409  0.36227519  2.0154364  2.1476464  0.2177741
  80         4.8164706  0.9804779  3.73181598  4.1614986  1.9124242  0.6481045
  93         0.5980392  0.9609612  0.37144302  0.2571267  2.9437073  0.3091581
  MAE        Selected
  0.4418700  *       
  0.5555635          
  0.8033744          
  0.6412179          
  1.6969860          
        NaN          
  0.6490688          
  0.6626818          
  0.7192765          
  1.1714216          
  1.1993721          
  0.6504748          
  0.7175702          
  0.6525296          
  0.8416719          
  1.4783709          
  1.1286299          
  1.7226968          
  1.4646177          
  2.5682336          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were num_trees = 14, k = 2.707153, alpha
 = 0.9776543, beta = 1.805734 and nu = 4.151994.
[1] "Sat Mar 10 04:04:22 2018"
Error in investigate_var_importance(object, plot = FALSE) : 
  could not find function "investigate_var_importance"
In addition: Warning messages:
1: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
2: In train.default(x = data.frame(training[, 2:length(training[1,  :
  missing values found in aggregated results
Error : package arm is required
Error : package arm is required
Error : package arm is required
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:04:32 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "bayesglm"                       
t=100, m=5
t=200, m=2
t=300, m=3
t=400, m=4
t=500, m=4
t=600, m=4
t=700, m=4
t=800, m=3
t=900, m=3
t=100, m=2
t=200, m=5
t=300, m=2
t=400, m=5
t=500, m=1
t=600, m=1
t=700, m=1
t=800, m=3
t=900, m=1
t=100, m=6
t=200, m=4
t=300, m=3
t=400, m=2
t=500, m=5
t=600, m=3
t=700, m=2
t=800, m=3
t=900, m=3
t=100, m=1
t=200, m=2
t=300, m=3
t=400, m=2
t=500, m=3
t=600, m=5
t=700, m=1
t=800, m=3
t=900, m=2
The Bayesian lasso 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  sparsity    RMSE       Rsquared   MAE        Selected
  0.05274719  0.3639158  0.9789441  0.2567109          
  0.07865330  0.3639158  0.9789441  0.2567109          
  0.09732015  0.3639158  0.9789441  0.2567109          
  0.16444786  0.3639158  0.9789441  0.2567109          
  0.16937555  0.3639158  0.9789441  0.2567109          
  0.17129523  0.3639158  0.9789441  0.2567109          
  0.21013188  0.3617693  0.9793997  0.2542426          
  0.22396974  0.3641980  0.9791611  0.2561730          
  0.25540271  0.3640027  0.9793020  0.2556298          
  0.27464788  0.3666660  0.9790271  0.2578413          
  0.37048346  0.3677760  0.9789736  0.2580499          
  0.37728372  0.3677760  0.9789736  0.2580499          
  0.37774359  0.3677760  0.9789736  0.2580499          
  0.38219311  0.3677760  0.9789736  0.2580499          
  0.51033865  0.3582367  0.9802501  0.2536733          
  0.69756051  0.3582367  0.9802501  0.2536733          
  0.73390926  0.3582367  0.9802501  0.2536733          
  0.73797904  0.3582367  0.9802501  0.2536733          
  0.77884069  0.3582367  0.9802501  0.2536733          
  0.91644537  0.3582367  0.9802501  0.2536733  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was sparsity = 0.9164454.
[1] "Sat Mar 10 04:04:43 2018"
t=100, m=4
t=200, m=2
t=300, m=2
t=400, m=1
t=500, m=2
t=600, m=4
t=700, m=4
t=800, m=7
t=900, m=4
t=100, m=2
t=200, m=3
t=300, m=3
t=400, m=3
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=3
t=900, m=2
t=100, m=5
t=200, m=2
t=300, m=5
t=400, m=4
t=500, m=4
t=600, m=3
t=700, m=6
t=800, m=5
t=900, m=4
t=100, m=2
t=200, m=2
t=300, m=6
t=400, m=2
t=500, m=3
t=600, m=4
t=700, m=4
t=800, m=3
t=900, m=3
Bayesian Ridge Regression (Model Averaged) 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results:

  RMSE       Rsquared   MAE      
  0.3647538  0.9789754  0.2576116

[1] "Sat Mar 10 04:04:53 2018"
t=100, m=1
t=200, m=1
t=300, m=2
t=400, m=1
t=500, m=1
t=600, m=2
t=700, m=1
t=800, m=1
t=900, m=1
t=100, m=1
t=200, m=2
t=300, m=1
t=400, m=1
t=500, m=1
t=600, m=2
t=700, m=1
t=800, m=1
t=900, m=1
t=100, m=3
t=200, m=1
t=300, m=1
t=400, m=3
t=500, m=2
t=600, m=2
t=700, m=1
t=800, m=3
t=900, m=3
t=100, m=2
t=200, m=1
t=300, m=2
t=400, m=1
t=500, m=1
t=600, m=1
t=700, m=2
t=800, m=1
t=900, m=2
Bayesian Ridge Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results:

  RMSE       Rsquared   MAE      
  0.3642475  0.9797824  0.2601729

[1] "Sat Mar 10 04:05:03 2018"
Boosted Linear Model 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  nu          mstop  RMSE      Rsquared   MAE       Selected
  0.03957921   86    1.600107  0.7422201  1.376042  *       
  0.07264510  458    1.989038  0.4284086  1.683090          
  0.08587379  369    1.980615  0.4308656  1.677358          
  0.11165307   85    1.698995  0.5719753  1.451778          
  0.11637365  367    2.042498  0.4157337  1.719633          
  0.14512775  112    1.835711  0.4881345  1.572358          
  0.15070433   49    1.667779  0.6062299  1.420319          
  0.18219829  189    2.009909  0.4234440  1.698163          
  0.20396804  191    2.035586  0.4175279  1.714524          
  0.25187816   83    1.906557  0.4595925  1.621550          
  0.27251392   40    1.752016  0.5375827  1.501378          
  0.29190990  105    1.998757  0.4269169  1.689487          
  0.32531688   27    1.724992  0.5594731  1.477407          
  0.35266038  128    2.073807  0.4094186  1.740596          
  0.39690988  255    2.138343  0.3947020  1.781593          
  0.42426491  185    2.131548  0.3961702  1.777882          
  0.45658294  189    2.135497  0.3954501  1.779888          
  0.53243339  138    2.131450  0.3961757  1.777418          
  0.53376306  349    2.142258  0.3939202  1.783670          
  0.57801318  389    2.142287  0.3939143  1.783685          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 86 and nu = 0.03957921.
[1] "Sat Mar 10 04:06:05 2018"
Conditional Inference Random Forest 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  mtry  RMSE      Rsquared   MAE        Selected
  1     2.048273  0.5674379  1.3703700          
  2     1.954195  0.6420892  1.2664343          
  3     1.842052  0.6860644  1.1329315          
  4     1.762384  0.6785447  1.0473442          
  5     1.602275  0.7226147  0.9148531          
  7     1.383651  0.7325570  0.7465638          
  8     1.334122  0.7131133  0.7464920          
  9     1.255664  0.7261672  0.7282600  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 04:06:17 2018"
Error in varimp(object, ...) : could not find function "varimp"
Conditional Inference Tree 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  mincriterion  RMSE      Rsquared   MAE        Selected
  0.05274719    1.426879  0.5833117  0.9757718          
  0.07865330    1.426879  0.5833117  0.9757718          
  0.09732015    1.426879  0.5833117  0.9757718          
  0.16444786    1.426879  0.5833117  0.9757718          
  0.16937555    1.426879  0.5833117  0.9757718          
  0.17129523    1.426879  0.5833117  0.9757718          
  0.21013188    1.426879  0.5833117  0.9757718          
  0.22396974    1.426879  0.5833117  0.9757718          
  0.25540271    1.426879  0.5833117  0.9757718          
  0.27464788    1.426879  0.5833117  0.9757718          
  0.37048346    1.426879  0.5833117  0.9757718          
  0.37728372    1.426879  0.5833117  0.9757718          
  0.37774359    1.426879  0.5833117  0.9757718          
  0.38219311    1.426879  0.5833117  0.9757718          
  0.51033865    1.426879  0.5833117  0.9757718          
  0.69756051    1.426879  0.5833117  0.9757718          
  0.73390926    1.426879  0.5833117  0.9757718          
  0.73797904    1.426879  0.5833117  0.9757718          
  0.77884069    1.426879  0.5833117  0.9757718          
  0.91644537    1.426879  0.5833117  0.9757718  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mincriterion = 0.9164454.
[1] "Sat Mar 10 04:06:28 2018"
Conditional Inference Tree 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  maxdepth  mincriterion  RMSE      Rsquared   MAE        Selected
   1        0.54143052    1.465465  0.5580150  1.0289120          
   2        0.24992375    1.426879  0.5833117  0.9757718  *       
   2        0.45327866    1.426879  0.5833117  0.9757718          
   3        0.06440603    1.426879  0.5833117  0.9757718          
   3        0.18472966    1.426879  0.5833117  0.9757718          
   3        0.41882831    1.426879  0.5833117  0.9757718          
   4        0.24061394    1.426879  0.5833117  0.9757718          
   4        0.48565927    1.426879  0.5833117  0.9757718          
   4        0.58707909    1.426879  0.5833117  0.9757718          
   5        0.88720099    1.426879  0.5833117  0.9757718          
   6        0.30250132    1.426879  0.5833117  0.9757718          
   6        0.33884480    1.426879  0.5833117  0.9757718          
   6        0.70661922    1.426879  0.5833117  0.9757718          
   6        0.76057251    1.426879  0.5833117  0.9757718          
   8        0.66095139    1.426879  0.5833117  0.9757718          
  11        0.88942080    1.426879  0.5833117  0.9757718          
  12        0.14169246    1.426879  0.5833117  0.9757718          
  12        0.19261044    1.426879  0.5833117  0.9757718          
  12        0.96329413    1.426879  0.5833117  0.9757718          
  14        0.11960784    1.426879  0.5833117  0.9757718          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were maxdepth = 2 and mincriterion
 = 0.2499238.
[1] "Sat Mar 10 04:06:39 2018"
Cubist 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  committees  neighbors  RMSE       Rsquared   MAE         Selected
   7          1          0.2111500  0.9954948  0.10131330          
  12          9          0.1739949  0.9967287  0.08494897          
  15          7          0.1755769  0.9966223  0.08560055          
  19          1          0.2110978  0.9954989  0.10129010          
  20          7          0.1752191  0.9966269  0.08521094          
  25          0          0.1732582  0.9967288  0.08535102          
  25          2          0.1935790  0.9961395  0.09253504          
  31          3          0.1783424  0.9966096  0.08655077          
  34          3          0.1773460  0.9966446  0.08586068          
  42          1          0.2102356  0.9955098  0.10087448          
  46          0          0.1728468  0.9967359  0.08513288  *       
  49          2          0.1942115  0.9961052  0.09305055          
  55          0          0.1732464  0.9967283  0.08535534          
  59          2          0.1942097  0.9961053  0.09305092          
  67          5          0.1746033  0.9967424  0.08599106          
  71          3          0.1783340  0.9966102  0.08654683          
  77          3          0.1778151  0.9966361  0.08614984          
  89          2          0.1942069  0.9961056  0.09305160          
  89          6          0.1732973  0.9967731  0.08398798          
  97          7          0.1750159  0.9965981  0.08490088          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were committees = 46 and neighbors = 0.
[1] "Sat Mar 10 04:07:00 2018"
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
Stacked AutoEncoder Deep Neural Network 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  layer1  layer2  layer3  hidden_dropout  visible_dropout  RMSE    
   3       6      17      0.57384123      0.68639920       2.675612
   3      10       3      0.20382262      0.27227787       2.149854
   3      12      16      0.31600351      0.58127916       2.895699
   5       3      13      0.28943846      0.46970025       2.123628
   5       5       5      0.01011035      0.01783683       2.201603
   5       9       5      0.38781607      0.09358209       2.269180
   5      11      16      0.23529508      0.53511210       2.395856
   6       6       8      0.30271895      0.32407874       2.257900
   6      13      12      0.15832099      0.25600896       2.343032
   7      18      10      0.34538293      0.53960531       2.222134
   9       7      16      0.64602002      0.66981855       2.267823
   9       8       2      0.37302930      0.67016166       2.135233
   9      15      14      0.02052394      0.21669175       2.152356
   9      16       2      0.58879472      0.61903379       2.254698
  11      14       7      0.43898825      0.23476338       2.246850
  15       5      10      0.07488597      0.41768224       2.291715
  15      18       3      0.24507526      0.23823584       2.192658
  16       4      10      0.06339816      0.28216110       2.315483
  16      20      17      0.65306780      0.58260980       2.202608
  19       4      13      0.06500253      0.03599773       2.361360
  Rsquared     MAE       Selected
  0.159280437  1.921886          
  0.128226766  1.413847          
  0.036442327  2.428051          
  0.068851626  1.472701  *       
  0.235628002  1.359302          
  0.154471975  1.502182          
  0.019362320  1.561316          
  0.086167309  1.342998          
  0.200533797  1.483523          
  0.256676601  1.456808          
  0.180744459  1.458763          
  0.225557025  1.489791          
  0.193285530  1.536766          
  0.004047648  1.814963          
  0.097429306  1.435073          
  0.470335652  1.354174          
  0.292718104  1.567755          
  0.360396050  1.416469          
  0.115577072  1.494149          
  0.229298910  1.418677          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were layer1 = 5, layer2 = 3, layer3 =
 13, hidden_dropout = 0.2894385 and visible_dropout = 0.4697002.
[1] "Sat Mar 10 04:07:12 2018"
Multivariate Adaptive Regression Spline 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  degree  nprune  RMSE       Rsquared   MAE        Selected
  1       2       0.3958769  0.9689124  0.3305184          
  1       3       0.2420377  0.9931089  0.1698671  *       
  1       4       0.2442225  0.9931293  0.1694902          
  2       2       0.3958769  0.9689124  0.3305184          
  2       3       0.2420377  0.9931089  0.1698671          
  2       4       0.2675500  0.9918754  0.1687338          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nprune = 3 and degree = 1.
[1] "Sat Mar 10 04:07:22 2018"
Extreme Learning Machine 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  nhid  actfun   RMSE       Rsquared    MAE        Selected
   2    purelin  2.8831307  0.03392229  2.0868991          
   2    radbas   2.6905554  0.07783695  1.8378365          
   2    sin      3.0580283  0.06400558  2.3398424          
   4    radbas   2.3692910  0.04352934  1.6642737          
   4    sin      2.7335476  0.06962892  2.0279145          
   5    purelin  1.8034634  0.35517175  1.3960872          
   5    sin      3.3632404  0.14477765  2.4921445          
   6    tansig   1.9194597  0.26663471  1.4263290          
   8    purelin  0.8059361  0.92146145  0.5759773          
   8    radbas   2.9247746  0.05892427  1.9895975          
   8    tansig   1.9393459  0.36325391  1.5241913          
  10    purelin  0.4777437  0.95665449  0.3376107  *       
  14    sin      2.9726914  0.14657580  2.2266821          
  14    tansig   2.1984710  0.50830872  1.6903108          
  15    sin      3.0268381  0.29099256  2.4277995          
  15    tansig   1.8881822  0.61886874  1.4194618          
  18    sin      2.7738131  0.21413496  2.2057083          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nhid = 10 and actfun = purelin.
[1] "Sat Mar 10 04:07:32 2018"
Elasticnet 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  lambda        fraction    RMSE       Rsquared   MAE        Selected
  2.072445e-05  0.54143052  0.7368580  0.9802501  0.4403577          
  2.964284e-05  0.45327866  0.9519656  0.9802501  0.5871339          
  3.836374e-05  0.24992375  1.4718397  0.9802501  0.9628710          
  9.698111e-05  0.41882831  1.0385081  0.9802501  0.6469363          
  1.038134e-04  0.18472966  1.6412789  0.9802501  1.0935615          
  1.066035e-04  0.06440603  1.9556354  0.9802501  1.3352747          
  1.823019e-04  0.48565927  0.8716214  0.9802501  0.5315597          
  2.207082e-04  0.24061394  1.4959734  0.9802501  0.9813701          
  3.407346e-04  0.58707909  0.6321939  0.9802501  0.3706200          
  4.445159e-04  0.88720099  0.4144679  0.9704091  0.2855107          
  1.670709e-03  0.70661922  0.4290906  0.9799423  0.2489727          
  1.835280e-03  0.76057251  0.3906457  0.9791255  0.2427288  *       
  1.846977e-03  0.30250132  1.3357026  0.9802501  0.8605993          
  1.964078e-03  0.33884480  1.2421519  0.9802501  0.7927021          
  1.153538e-02  0.66095139  0.4864963  0.9802501  0.2816808          
  1.532368e-01  0.88942080  0.4432975  0.9639994  0.3106241          
  2.531953e-01  0.19261044  1.5985272  0.9802501  1.0607008          
  2.678393e-01  0.14169246  1.7355394  0.9802501  1.1662921          
  4.710252e-01  0.96329413  0.5938610  0.9294695  0.4471102          
  3.152625e+00  0.11960784  1.6639730  0.9802501  1.1118281          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were fraction = 0.7605725 and lambda
 = 0.00183528.
[1] "Sat Mar 10 04:07:42 2018"
Error : package evtree is required
In addition: There were 45 warnings (use warnings() to see them)
Error : package evtree is required
Error : package evtree is required
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:07:52 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "evtree"                         
Random Forest by Randomization 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  mtry  numRandomCuts  RMSE       Rsquared   MAE        Selected
  1      7             1.6558493  0.6685397  1.1346326          
  1     12             1.6203451  0.6928523  1.1153575          
  1     14             1.6564673  0.6705368  1.1424307          
  2      2             1.2879388  0.8867498  0.8467503          
  2      5             1.2844144  0.8726314  0.8797844          
  2     11             1.2813742  0.8712909  0.8900361          
  2     13             1.3087982  0.8573798  0.9055520          
  3      7             0.9978065  0.9377791  0.6678952          
  3     15             1.0735744  0.9165143  0.7428967          
  3     23             1.0790931  0.9076254  0.7439504          
  4      8             0.8330274  0.9546412  0.5448545          
  4      9             0.8799648  0.9464527  0.5785919          
  4     18             0.8834587  0.9376402  0.5916199          
  4     20             0.8609021  0.9413596  0.5726843          
  5     17             0.7385106  0.9554270  0.4786202          
  7      4             0.4372265  0.9756207  0.2041532  *       
  7      5             0.4446633  0.9741625  0.2102465          
  7     23             0.5005847  0.9712087  0.2487725          
  8     25             0.4433550  0.9742570  0.2007211          
  9      3             0.4464505  0.9743430  0.1958395          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7 and numRandomCuts = 4.
[1] "Sat Mar 10 04:08:09 2018"
Ridge Regression with Variable Selection 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  lambda        k  RMSE       Rsquared   MAE        Selected
  2.072445e-05  5  0.4540400  0.9633582  0.3233383          
  2.964284e-05  5  0.4540388  0.9633581  0.3233316          
  3.836374e-05  3  0.3887566  0.9757544  0.2674866          
  9.698111e-05  4  0.4195762  0.9695575  0.2912115          
  1.038134e-04  2  0.3794081  0.9769180  0.2727019          
  1.066035e-04  1  0.3604386  0.9802501  0.2610749  *       
  1.823019e-04  5  0.4540177  0.9633561  0.3232170          
  2.207082e-04  3  0.3887166  0.9757531  0.2673554          
  3.407346e-04  6  0.4748902  0.9586768  0.3425401          
  4.445159e-04  8  0.4769312  0.9570743  0.3360780          
  1.670709e-03  7  0.4787706  0.9568789  0.3468181          
  1.835280e-03  7  0.4787855  0.9568668  0.3467751          
  1.846977e-03  3  0.3883754  0.9757416  0.2662240          
  1.964078e-03  4  0.4192363  0.9695672  0.2898681          
  1.153538e-02  6  0.4759182  0.9579577  0.3401396          
  1.532368e-01  9        NaN        NaN        NaN          
  2.531953e-01  2        NaN        NaN        NaN          
  2.678393e-01  2        NaN        NaN        NaN          
  4.710252e-01  9        NaN        NaN        NaN          
  3.152625e+00  2        NaN        NaN        NaN          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were k = 1 and lambda = 0.0001066035.
[1] "Sat Mar 10 04:08:20 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 30 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 13 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:08:32 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "gam"                            
Boosted Generalized Additive Model 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  mstop  prune  RMSE        Rsquared   MAE         Selected
   53    no     0.05541388  0.9997903  0.02247063          
   79    yes    0.05141392  0.9998119  0.02171306          
   98    yes    0.05022613  0.9998153  0.02133500          
  165    yes    0.05018818  0.9998154  0.02131505          
  170    yes    0.05018818  0.9998154  0.02131505          
  172    yes    0.05018818  0.9998154  0.02131505          
  211    yes    0.05018818  0.9998154  0.02131505          
  224    yes    0.05018818  0.9998154  0.02131505          
  256    no     0.04643650  0.9998318  0.02147651          
  275    no     0.04623163  0.9998332  0.02139327          
  371    no     0.04566192  0.9998365  0.02125347          
  378    no     0.04564035  0.9998365  0.02127251          
  378    yes    0.05018818  0.9998154  0.02131505          
  383    yes    0.05018818  0.9998154  0.02131505          
  511    no     0.04529130  0.9998383  0.02144397          
  698    no     0.04509371  0.9998385  0.02172770  *       
  734    yes    0.05018818  0.9998154  0.02131505          
  738    yes    0.05018818  0.9998154  0.02131505          
  779    no     0.04512920  0.9998376  0.02184237          
  917    yes    0.05018818  0.9998154  0.02131505          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 698 and prune = no.
[1] "Sat Mar 10 04:08:58 2018"
Generalized Additive Model using Splines 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  df         RMSE       Rsquared   MAE        Selected
  0.2637359  0.4777390  0.9566552  0.3376073          
  0.3932665  0.4777390  0.9566552  0.3376073          
  0.4866008  0.4777390  0.9566552  0.3376073          
  0.8222393  0.4777390  0.9566552  0.3376073          
  0.8468778  0.4777390  0.9566552  0.3376073          
  0.8564762  0.4777390  0.9566552  0.3376073          
  1.0506594  0.4662939  0.9588694  0.3296309          
  1.1198487  0.4518480  0.9615961  0.3202906          
  1.2770136  0.4229707  0.9667988  0.3008898          
  1.3732394  0.4071867  0.9694844  0.2892901          
  1.8524173  0.3366904  0.9798310  0.2379698          
  1.8864186  0.3319022  0.9804385  0.2346905          
  1.8887179  0.3315754  0.9804798  0.2344757          
  1.9109656  0.3284577  0.9808700  0.2324253          
  2.5516933  0.2484628  0.9895366  0.1738761          
  3.4878025  0.1808866  0.9949954  0.1198669          
  3.6695463  0.1729174  0.9955061  0.1132906          
  3.6898952  0.1720958  0.9955570  0.1126058          
  3.8942035  0.1645613  0.9960065  0.1068386          
  4.5822269  0.1464289  0.9969618  0.0934681  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was df = 4.582227.
[1] "Sat Mar 10 04:09:13 2018"
Error in if (any(extras)) { : missing value where TRUE/FALSE needed
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:09:27 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "gaussprLinear"                  
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:09:46 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "gaussprPoly"                    
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:10:01 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "gaussprRadial"                  
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        3.4789             nan     0.0984    0.3173
     2        3.0417             nan     0.0984    0.0852
     3        2.8001             nan     0.0984    0.2426
     4        2.5778             nan     0.0984    0.2515
     5        2.4070             nan     0.0984    0.1732
     6        2.1458             nan     0.0984    0.1270
     7        2.0966             nan     0.0984    0.0112
     8        2.0674             nan     0.0984    0.0128
     9        1.8886             nan     0.0984    0.1015
    10        1.8977             nan     0.0984   -0.0880
    20        1.3374             nan     0.0984   -0.0397
    40        0.6976             nan     0.0984   -0.0008
    60        0.4465             nan     0.0984    0.0020
    80        0.3028             nan     0.0984   -0.0154
   100        0.2080             nan     0.0984   -0.0091
   120        0.1787             nan     0.0984    0.0017
   140        0.1402             nan     0.0984   -0.0030
   160        0.1162             nan     0.0984   -0.0028
   180        0.0873             nan     0.0984   -0.0027
   200        0.0759             nan     0.0984   -0.0052
   220        0.0637             nan     0.0984   -0.0019
   240        0.0576             nan     0.0984   -0.0023
   260        0.0490             nan     0.0984   -0.0024
   280        0.0429             nan     0.0984   -0.0012
   300        0.0378             nan     0.0984   -0.0009
   320        0.0335             nan     0.0984    0.0003
   340        0.0292             nan     0.0984   -0.0008
   360        0.0248             nan     0.0984   -0.0021
   380        0.0218             nan     0.0984   -0.0008
   400        0.0203             nan     0.0984   -0.0004
   420        0.0179             nan     0.0984   -0.0009
   440        0.0164             nan     0.0984   -0.0007
   460        0.0145             nan     0.0984   -0.0003
   480        0.0131             nan     0.0984   -0.0008
   500        0.0125             nan     0.0984   -0.0001
   520        0.0109             nan     0.0984   -0.0006
   540        0.0097             nan     0.0984   -0.0001
   560        0.0087             nan     0.0984   -0.0003
   580        0.0075             nan     0.0984   -0.0002
   600        0.0066             nan     0.0984   -0.0005
   620        0.0056             nan     0.0984   -0.0000
   640        0.0049             nan     0.0984   -0.0001
   660        0.0042             nan     0.0984   -0.0001
   680        0.0036             nan     0.0984   -0.0002
   700        0.0033             nan     0.0984   -0.0000
   720        0.0030             nan     0.0984   -0.0001
   740        0.0027             nan     0.0984   -0.0002
   760        0.0023             nan     0.0984   -0.0001
   780        0.0021             nan     0.0984   -0.0002
   800        0.0019             nan     0.0984   -0.0001
   820        0.0017             nan     0.0984   -0.0000
   840        0.0015             nan     0.0984   -0.0000
   847        0.0015             nan     0.0984   -0.0001

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        2.1987             nan     0.3816    0.5533
     2        1.7698             nan     0.3816    0.5148
     3        1.4445             nan     0.3816    0.3091
     4        1.1971             nan     0.3816   -0.0540
     5        1.2777             nan     0.3816   -0.4296
     6        1.0348             nan     0.3816    0.1666
     7        0.8265             nan     0.3816    0.0027
     8        0.7840             nan     0.3816   -0.0197
     9        0.6604             nan     0.3816   -0.0516
    10        0.6142             nan     0.3816    0.0329
    20        0.3817             nan     0.3816   -0.0299
    40        0.1441             nan     0.3816   -0.0182
    60        0.0945             nan     0.3816   -0.0189
    80        0.0739             nan     0.3816   -0.0275
   100        0.0359             nan     0.3816   -0.0074
   120        0.0266             nan     0.3816   -0.0026
   140        0.0190             nan     0.3816    0.0000
   160        0.0139             nan     0.3816   -0.0029
   180        0.0097             nan     0.3816   -0.0022
   200        0.0076             nan     0.3816   -0.0014
   220        0.0052             nan     0.3816   -0.0012
   240        0.0031             nan     0.3816   -0.0003
   260        0.0022             nan     0.3816   -0.0000
   280        0.0020             nan     0.3816   -0.0002
   300        0.0011             nan     0.3816   -0.0001
   320        0.0006             nan     0.3816   -0.0001
   340        0.0004             nan     0.3816   -0.0001
   360        0.0003             nan     0.3816   -0.0001
   380        0.0002             nan     0.3816   -0.0000
   400        0.0001             nan     0.3816   -0.0000
   420        0.0001             nan     0.3816   -0.0000
   440        0.0000             nan     0.3816   -0.0000
   460        0.0000             nan     0.3816    0.0000
   480        0.0000             nan     0.3816   -0.0000
   500        0.0000             nan     0.3816   -0.0000
   520        0.0000             nan     0.3816   -0.0000
   540        0.0000             nan     0.3816   -0.0000
   560        0.0000             nan     0.3816   -0.0000
   580        0.0000             nan     0.3816   -0.0000
   600        0.0000             nan     0.3816   -0.0000
   620        0.0000             nan     0.3816   -0.0000
   640        0.0000             nan     0.3816   -0.0000
   660        0.0000             nan     0.3816   -0.0000
   680        0.0000             nan     0.3816   -0.0000
   700        0.0000             nan     0.3816   -0.0000
   720        0.0000             nan     0.3816   -0.0000
   740        0.0000             nan     0.3816    0.0000
   760        0.0000             nan     0.3816    0.0000
   780        0.0000             nan     0.3816    0.0000
   800        0.0000             nan     0.3816   -0.0000
   820        0.0000             nan     0.3816    0.0000
   840        0.0000             nan     0.3816   -0.0000
   860        0.0000             nan     0.3816   -0.0000
   880        0.0000             nan     0.3816   -0.0000
   900        0.0000             nan     0.3816   -0.0000
   920        0.0000             nan     0.3816   -0.0000
   940        0.0000             nan     0.3816   -0.0000
   960        0.0000             nan     0.3816   -0.0000
   980        0.0000             nan     0.3816   -0.0000
  1000        0.0000             nan     0.3816   -0.0000
  1020        0.0000             nan     0.3816   -0.0000
  1040        0.0000             nan     0.3816   -0.0000
  1060        0.0000             nan     0.3816   -0.0000
  1080        0.0000             nan     0.3816   -0.0000
  1100        0.0000             nan     0.3816   -0.0000
  1120        0.0000             nan     0.3816   -0.0000
  1140        0.0000             nan     0.3816   -0.0000
  1160        0.0000             nan     0.3816   -0.0000
  1180        0.0000             nan     0.3816   -0.0000
  1200        0.0000             nan     0.3816   -0.0000
  1220        0.0000             nan     0.3816   -0.0000
  1240        0.0000             nan     0.3816   -0.0000
  1260        0.0000             nan     0.3816   -0.0000
  1280        0.0000             nan     0.3816   -0.0000
  1300        0.0000             nan     0.3816   -0.0000
  1320        0.0000             nan     0.3816   -0.0000
  1340        0.0000             nan     0.3816   -0.0000
  1360        0.0000             nan     0.3816   -0.0000
  1380        0.0000             nan     0.3816   -0.0000
  1400        0.0000             nan     0.3816   -0.0000
  1420        0.0000             nan     0.3816   -0.0000
  1440        0.0000             nan     0.3816   -0.0000
  1460        0.0000             nan     0.3816   -0.0000
  1480        0.0000             nan     0.3816   -0.0000
  1500        0.0000             nan     0.3816   -0.0000
  1520        0.0000             nan     0.3816   -0.0000
  1540        0.0000             nan     0.3816   -0.0000
  1560        0.0000             nan     0.3816    0.0000
  1580        0.0000             nan     0.3816    0.0000
  1600        0.0000             nan     0.3816   -0.0000
  1620        0.0000             nan     0.3816   -0.0000
  1640        0.0000             nan     0.3816   -0.0000
  1660        0.0000             nan     0.3816   -0.0000
  1680        0.0000             nan     0.3816   -0.0000
  1700        0.0000             nan     0.3816   -0.0000
  1720        0.0000             nan     0.3816   -0.0000
  1740        0.0000             nan     0.3816   -0.0000
  1760        0.0000             nan     0.3816   -0.0000
  1780        0.0000             nan     0.3816   -0.0000
  1800        0.0000             nan     0.3816    0.0000
  1820        0.0000             nan     0.3816   -0.0000
  1840        0.0000             nan     0.3816   -0.0000
  1853        0.0000             nan     0.3816   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        4.5888             nan     0.0984    0.4161
     2        4.0865             nan     0.0984    0.1195
     3        3.8369             nan     0.0984    0.2781
     4        3.4554             nan     0.0984    0.1000
     5        3.2291             nan     0.0984    0.1602
     6        3.0126             nan     0.0984   -0.0023
     7        2.8521             nan     0.0984    0.0064
     8        2.7017             nan     0.0984   -0.0176
     9        2.6125             nan     0.0984    0.0663
    10        2.5981             nan     0.0984   -0.0475
    20        2.1811             nan     0.0984   -0.0989
    40        1.6755             nan     0.0984   -0.0891
    60        1.4132             nan     0.0984   -0.0564
    80        1.2758             nan     0.0984   -0.0317
   100        1.0636             nan     0.0984   -0.0192
   120        0.9327             nan     0.0984   -0.0378
   140        0.7953             nan     0.0984   -0.0464
   160        0.7669             nan     0.0984    0.0016
   180        0.6107             nan     0.0984   -0.0435
   200        0.5029             nan     0.0984   -0.0254
   220        0.4168             nan     0.0984   -0.0038
   240        0.3647             nan     0.0984   -0.0181
   260        0.3282             nan     0.0984   -0.0018
   280        0.2867             nan     0.0984   -0.0101
   300        0.2583             nan     0.0984   -0.0085
   320        0.2290             nan     0.0984   -0.0085
   340        0.1925             nan     0.0984   -0.0013
   360        0.1738             nan     0.0984   -0.0048
   380        0.1548             nan     0.0984   -0.0013
   400        0.1380             nan     0.0984   -0.0061
   420        0.1196             nan     0.0984   -0.0028
   440        0.1093             nan     0.0984   -0.0067
   460        0.0911             nan     0.0984   -0.0011
   480        0.0805             nan     0.0984   -0.0042
   500        0.0717             nan     0.0984   -0.0019
   520        0.0593             nan     0.0984   -0.0013
   540        0.0544             nan     0.0984   -0.0032
   560        0.0485             nan     0.0984   -0.0006
   580        0.0455             nan     0.0984   -0.0004
   600        0.0372             nan     0.0984   -0.0013
   620        0.0333             nan     0.0984   -0.0021
   640        0.0304             nan     0.0984   -0.0018
   660        0.0259             nan     0.0984   -0.0015
   680        0.0241             nan     0.0984   -0.0008
   700        0.0216             nan     0.0984   -0.0013
   720        0.0181             nan     0.0984   -0.0004
   740        0.0160             nan     0.0984   -0.0002
   760        0.0143             nan     0.0984   -0.0009
   780        0.0122             nan     0.0984   -0.0005
   800        0.0110             nan     0.0984   -0.0004
   820        0.0098             nan     0.0984   -0.0006
   840        0.0080             nan     0.0984   -0.0002
   847        0.0077             nan     0.0984   -0.0004

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        3.3032             nan     0.3816    0.7037
     2        2.8145             nan     0.3816    0.5810
     3        2.4195             nan     0.3816    0.2774
     4        2.4706             nan     0.3816   -0.2218
     5        2.1021             nan     0.3816   -0.4800
     6        2.0448             nan     0.3816   -0.2148
     7        1.9687             nan     0.3816   -0.2688
     8        1.7764             nan     0.3816    0.0133
     9        1.6215             nan     0.3816   -0.0529
    10        1.5913             nan     0.3816   -0.2258
    20        1.3064             nan     0.3816   -0.2854
    40        0.9204             nan     0.3816   -0.1425
    60        0.5586             nan     0.3816   -0.0377
    80        0.2779             nan     0.3816   -0.0396
   100        0.1436             nan     0.3816   -0.0227
   120        0.0828             nan     0.3816   -0.0001
   140        0.0639             nan     0.3816   -0.0051
   160        0.0492             nan     0.3816   -0.0141
   180        0.0287             nan     0.3816   -0.0027
   200        0.0152             nan     0.3816   -0.0020
   220        0.0083             nan     0.3816   -0.0022
   240        0.0053             nan     0.3816   -0.0006
   260        0.0049             nan     0.3816   -0.0021
   280        0.0020             nan     0.3816   -0.0000
   300        0.0012             nan     0.3816   -0.0000
   320        0.0008             nan     0.3816   -0.0003
   340        0.0005             nan     0.3816   -0.0000
   360        0.0004             nan     0.3816   -0.0001
   380        0.0002             nan     0.3816   -0.0000
   400        0.0001             nan     0.3816   -0.0000
   420        0.0001             nan     0.3816   -0.0000
   440        0.0000             nan     0.3816   -0.0000
   460        0.0000             nan     0.3816   -0.0000
   480        0.0000             nan     0.3816   -0.0000
   500        0.0000             nan     0.3816   -0.0000
   520        0.0000             nan     0.3816   -0.0000
   540        0.0000             nan     0.3816   -0.0000
   560        0.0000             nan     0.3816   -0.0000
   580        0.0000             nan     0.3816   -0.0000
   600        0.0000             nan     0.3816   -0.0000
   620        0.0000             nan     0.3816   -0.0000
   640        0.0000             nan     0.3816   -0.0000
   660        0.0000             nan     0.3816   -0.0000
   680        0.0000             nan     0.3816   -0.0000
   700        0.0000             nan     0.3816   -0.0000
   720        0.0000             nan     0.3816   -0.0000
   740        0.0000             nan     0.3816   -0.0000
   760        0.0000             nan     0.3816   -0.0000
   780        0.0000             nan     0.3816   -0.0000
   800        0.0000             nan     0.3816   -0.0000
   820        0.0000             nan     0.3816   -0.0000
   840        0.0000             nan     0.3816   -0.0000
   860        0.0000             nan     0.3816   -0.0000
   880        0.0000             nan     0.3816   -0.0000
   900        0.0000             nan     0.3816   -0.0000
   920        0.0000             nan     0.3816   -0.0000
   940        0.0000             nan     0.3816   -0.0000
   960        0.0000             nan     0.3816   -0.0000
   980        0.0000             nan     0.3816   -0.0000
  1000        0.0000             nan     0.3816   -0.0000
  1020        0.0000             nan     0.3816   -0.0000
  1040        0.0000             nan     0.3816   -0.0000
  1060        0.0000             nan     0.3816   -0.0000
  1080        0.0000             nan     0.3816   -0.0000
  1100        0.0000             nan     0.3816   -0.0000
  1120        0.0000             nan     0.3816   -0.0000
  1140        0.0000             nan     0.3816   -0.0000
  1160        0.0000             nan     0.3816   -0.0000
  1180        0.0000             nan     0.3816   -0.0000
  1200        0.0000             nan     0.3816   -0.0000
  1220        0.0000             nan     0.3816   -0.0000
  1240        0.0000             nan     0.3816   -0.0000
  1260        0.0000             nan     0.3816   -0.0000
  1280        0.0000             nan     0.3816   -0.0000
  1300        0.0000             nan     0.3816   -0.0000
  1320        0.0000             nan     0.3816   -0.0000
  1340        0.0000             nan     0.3816   -0.0000
  1360        0.0000             nan     0.3816   -0.0000
  1380        0.0000             nan     0.3816   -0.0000
  1400        0.0000             nan     0.3816   -0.0000
  1420        0.0000             nan     0.3816   -0.0000
  1440        0.0000             nan     0.3816   -0.0000
  1460        0.0000             nan     0.3816   -0.0000
  1480        0.0000             nan     0.3816   -0.0000
  1500        0.0000             nan     0.3816    0.0000
  1520        0.0000             nan     0.3816   -0.0000
  1540        0.0000             nan     0.3816   -0.0000
  1560        0.0000             nan     0.3816   -0.0000
  1580        0.0000             nan     0.3816   -0.0000
  1600        0.0000             nan     0.3816   -0.0000
  1620        0.0000             nan     0.3816   -0.0000
  1640        0.0000             nan     0.3816   -0.0000
  1660        0.0000             nan     0.3816   -0.0000
  1680        0.0000             nan     0.3816   -0.0000
  1700        0.0000             nan     0.3816    0.0000
  1720        0.0000             nan     0.3816   -0.0000
  1740        0.0000             nan     0.3816   -0.0000
  1760        0.0000             nan     0.3816   -0.0000
  1780        0.0000             nan     0.3816   -0.0000
  1800        0.0000             nan     0.3816   -0.0000
  1820        0.0000             nan     0.3816   -0.0000
  1840        0.0000             nan     0.3816    0.0000
  1853        0.0000             nan     0.3816   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        4.5951             nan     0.0984    0.4696
     2        4.3524             nan     0.0984    0.2706
     3        3.8047             nan     0.0984    0.1727
     4        3.3775             nan     0.0984    0.1433
     5        3.0556             nan     0.0984    0.0515
     6        2.7817             nan     0.0984   -0.0124
     7        2.6096             nan     0.0984    0.0302
     8        2.5345             nan     0.0984   -0.2234
     9        2.4817             nan     0.0984   -0.0208
    10        2.4063             nan     0.0984    0.0408
    20        1.7404             nan     0.0984    0.0378
    40        1.1168             nan     0.0984    0.0041
    60        0.7786             nan     0.0984   -0.0261
    80        0.6064             nan     0.0984   -0.0148
   100        0.4488             nan     0.0984   -0.0081
   120        0.3639             nan     0.0984    0.0040
   140        0.3160             nan     0.0984   -0.0040
   160        0.2692             nan     0.0984   -0.0083
   180        0.2331             nan     0.0984   -0.0074
   200        0.2035             nan     0.0984   -0.0063
   220        0.1758             nan     0.0984   -0.0027
   240        0.1511             nan     0.0984   -0.0053
   260        0.1280             nan     0.0984   -0.0015
   280        0.1133             nan     0.0984   -0.0100
   300        0.1005             nan     0.0984   -0.0043
   320        0.0929             nan     0.0984   -0.0022
   340        0.0835             nan     0.0984   -0.0028
   360        0.0746             nan     0.0984   -0.0012
   380        0.0640             nan     0.0984   -0.0016
   400        0.0554             nan     0.0984   -0.0021
   420        0.0477             nan     0.0984   -0.0014
   440        0.0417             nan     0.0984   -0.0034
   460        0.0359             nan     0.0984   -0.0013
   480        0.0320             nan     0.0984   -0.0014
   500        0.0278             nan     0.0984   -0.0015
   520        0.0249             nan     0.0984   -0.0008
   540        0.0223             nan     0.0984   -0.0011
   560        0.0194             nan     0.0984   -0.0008
   580        0.0178             nan     0.0984   -0.0004
   600        0.0163             nan     0.0984   -0.0004
   620        0.0146             nan     0.0984   -0.0006
   640        0.0131             nan     0.0984   -0.0002
   660        0.0116             nan     0.0984   -0.0006
   680        0.0102             nan     0.0984   -0.0008
   700        0.0096             nan     0.0984   -0.0002
   720        0.0089             nan     0.0984   -0.0004
   740        0.0080             nan     0.0984   -0.0001
   760        0.0065             nan     0.0984   -0.0000
   780        0.0059             nan     0.0984   -0.0001
   800        0.0055             nan     0.0984   -0.0002
   820        0.0049             nan     0.0984   -0.0003
   840        0.0044             nan     0.0984   -0.0001
   847        0.0042             nan     0.0984   -0.0002

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        4.0589             nan     0.2768    1.0735
     2        3.6326             nan     0.2768    0.5568
     3        2.9603             nan     0.2768    0.1466
     4        2.7567             nan     0.2768   -0.0785
     5        2.6685             nan     0.2768    0.0686
     6        2.6089             nan     0.2768    0.0637
     7        2.4206             nan     0.2768   -0.4148
     8        2.3067             nan     0.2768    0.0821
     9        2.0624             nan     0.2768   -0.1676
    10        2.0953             nan     0.2768   -0.2415
    20        1.2304             nan     0.2768   -0.1251
    40        0.7846             nan     0.2768   -0.1212
    60        0.4154             nan     0.2768   -0.0194
    80        0.3406             nan     0.2768   -0.0260
   100        0.2312             nan     0.2768   -0.0515
   120        0.1867             nan     0.2768   -0.0173
   140        0.1527             nan     0.2768   -0.0042
   160        0.1190             nan     0.2768    0.0028
   180        0.0975             nan     0.2768   -0.0111
   200        0.0877             nan     0.2768   -0.0141
   220        0.0566             nan     0.2768   -0.0031
   240        0.0464             nan     0.2768   -0.0067
   260        0.0385             nan     0.2768   -0.0058
   280        0.0325             nan     0.2768   -0.0038
   300        0.0253             nan     0.2768   -0.0027
   320        0.0229             nan     0.2768   -0.0009
   340        0.0194             nan     0.2768   -0.0014
   360        0.0178             nan     0.2768   -0.0016
   380        0.0150             nan     0.2768   -0.0008
   400        0.0118             nan     0.2768   -0.0015
   420        0.0102             nan     0.2768   -0.0007
   440        0.0082             nan     0.2768   -0.0006
   460        0.0074             nan     0.2768   -0.0003
   480        0.0061             nan     0.2768   -0.0007
   500        0.0057             nan     0.2768   -0.0009
   520        0.0050             nan     0.2768   -0.0002
   540        0.0045             nan     0.2768   -0.0007
   560        0.0039             nan     0.2768   -0.0002
   580        0.0033             nan     0.2768   -0.0003
   600        0.0027             nan     0.2768   -0.0005
   620        0.0025             nan     0.2768   -0.0003
   640        0.0021             nan     0.2768   -0.0002
   660        0.0021             nan     0.2768   -0.0001
   680        0.0017             nan     0.2768   -0.0001
   700        0.0014             nan     0.2768   -0.0001
   720        0.0013             nan     0.2768   -0.0001
   740        0.0012             nan     0.2768   -0.0001
   760        0.0010             nan     0.2768   -0.0002
   780        0.0009             nan     0.2768   -0.0002
   800        0.0008             nan     0.2768   -0.0001
   820        0.0008             nan     0.2768   -0.0000
   840        0.0007             nan     0.2768   -0.0001
   860        0.0005             nan     0.2768   -0.0000
   880        0.0004             nan     0.2768   -0.0000
   900        0.0004             nan     0.2768   -0.0000
   920        0.0003             nan     0.2768   -0.0000
   940        0.0003             nan     0.2768   -0.0000
   960        0.0002             nan     0.2768   -0.0000
   980        0.0002             nan     0.2768   -0.0000
  1000        0.0002             nan     0.2768   -0.0000
  1020        0.0002             nan     0.2768   -0.0000
  1040        0.0002             nan     0.2768   -0.0000
  1060        0.0001             nan     0.2768   -0.0000
  1080        0.0001             nan     0.2768   -0.0000
  1100        0.0001             nan     0.2768   -0.0000
  1120        0.0001             nan     0.2768   -0.0000
  1140        0.0001             nan     0.2768   -0.0000
  1160        0.0000             nan     0.2768   -0.0000
  1180        0.0000             nan     0.2768   -0.0000
  1200        0.0000             nan     0.2768   -0.0000
  1220        0.0000             nan     0.2768   -0.0000
  1240        0.0000             nan     0.2768   -0.0000
  1260        0.0000             nan     0.2768   -0.0000
  1280        0.0000             nan     0.2768   -0.0000
  1300        0.0000             nan     0.2768   -0.0000
  1320        0.0000             nan     0.2768   -0.0000
  1340        0.0000             nan     0.2768   -0.0000
  1360        0.0000             nan     0.2768   -0.0000
  1380        0.0000             nan     0.2768   -0.0000
  1400        0.0000             nan     0.2768   -0.0000
  1420        0.0000             nan     0.2768   -0.0000
  1440        0.0000             nan     0.2768   -0.0000
  1460        0.0000             nan     0.2768   -0.0000
  1480        0.0000             nan     0.2768    0.0000
  1500        0.0000             nan     0.2768   -0.0000
  1520        0.0000             nan     0.2768   -0.0000
  1540        0.0000             nan     0.2768   -0.0000
  1560        0.0000             nan     0.2768   -0.0000
  1580        0.0000             nan     0.2768   -0.0000
  1600        0.0000             nan     0.2768   -0.0000
  1620        0.0000             nan     0.2768   -0.0000
  1640        0.0000             nan     0.2768   -0.0000
  1660        0.0000             nan     0.2768   -0.0000
  1680        0.0000             nan     0.2768   -0.0000
  1700        0.0000             nan     0.2768   -0.0000
  1720        0.0000             nan     0.2768   -0.0000
  1740        0.0000             nan     0.2768   -0.0000
  1760        0.0000             nan     0.2768   -0.0000
  1780        0.0000             nan     0.2768   -0.0000
  1800        0.0000             nan     0.2768   -0.0000
  1820        0.0000             nan     0.2768   -0.0000
  1840        0.0000             nan     0.2768   -0.0000
  1860        0.0000             nan     0.2768   -0.0000
  1880        0.0000             nan     0.2768   -0.0000
  1900        0.0000             nan     0.2768   -0.0000
  1920        0.0000             nan     0.2768   -0.0000
  1940        0.0000             nan     0.2768   -0.0000
  1960        0.0000             nan     0.2768   -0.0000
  1980        0.0000             nan     0.2768   -0.0000
  2000        0.0000             nan     0.2768   -0.0000
  2020        0.0000             nan     0.2768   -0.0000
  2040        0.0000             nan     0.2768   -0.0000
  2060        0.0000             nan     0.2768   -0.0000
  2080        0.0000             nan     0.2768   -0.0000
  2100        0.0000             nan     0.2768   -0.0000
  2120        0.0000             nan     0.2768   -0.0000
  2140        0.0000             nan     0.2768   -0.0000
  2160        0.0000             nan     0.2768   -0.0000
  2180        0.0000             nan     0.2768   -0.0000
  2200        0.0000             nan     0.2768   -0.0000
  2220        0.0000             nan     0.2768   -0.0000
  2240        0.0000             nan     0.2768   -0.0000
  2260        0.0000             nan     0.2768   -0.0000
  2280        0.0000             nan     0.2768   -0.0000
  2300        0.0000             nan     0.2768   -0.0000
  2320        0.0000             nan     0.2768   -0.0000
  2340        0.0000             nan     0.2768   -0.0000
  2360        0.0000             nan     0.2768   -0.0000
  2380        0.0000             nan     0.2768   -0.0000
  2400        0.0000             nan     0.2768   -0.0000
  2420        0.0000             nan     0.2768   -0.0000
  2440        0.0000             nan     0.2768   -0.0000
  2460        0.0000             nan     0.2768   -0.0000
  2480        0.0000             nan     0.2768   -0.0000
  2500        0.0000             nan     0.2768   -0.0000
  2520        0.0000             nan     0.2768   -0.0000
  2540        0.0000             nan     0.2768   -0.0000
  2560        0.0000             nan     0.2768   -0.0000
  2580        0.0000             nan     0.2768   -0.0000
  2600        0.0000             nan     0.2768   -0.0000
  2620        0.0000             nan     0.2768   -0.0000
  2640        0.0000             nan     0.2768   -0.0000
  2660        0.0000             nan     0.2768   -0.0000
  2680        0.0000             nan     0.2768   -0.0000
  2700        0.0000             nan     0.2768   -0.0000
  2720        0.0000             nan     0.2768   -0.0000
  2740        0.0000             nan     0.2768   -0.0000
  2760        0.0000             nan     0.2768   -0.0000
  2780        0.0000             nan     0.2768   -0.0000
  2800        0.0000             nan     0.2768   -0.0000
  2820        0.0000             nan     0.2768   -0.0000
  2840        0.0000             nan     0.2768   -0.0000
  2860        0.0000             nan     0.2768   -0.0000
  2880        0.0000             nan     0.2768   -0.0000
  2900        0.0000             nan     0.2768   -0.0000
  2920        0.0000             nan     0.2768   -0.0000
  2940        0.0000             nan     0.2768   -0.0000
  2960        0.0000             nan     0.2768   -0.0000
  2980        0.0000             nan     0.2768   -0.0000
  3000        0.0000             nan     0.2768   -0.0000
  3020        0.0000             nan     0.2768   -0.0000
  3040        0.0000             nan     0.2768   -0.0000
  3060        0.0000             nan     0.2768   -0.0000
  3080        0.0000             nan     0.2768   -0.0000
  3100        0.0000             nan     0.2768   -0.0000
  3120        0.0000             nan     0.2768   -0.0000
  3140        0.0000             nan     0.2768   -0.0000
  3160        0.0000             nan     0.2768   -0.0000
  3180        0.0000             nan     0.2768   -0.0000
  3200        0.0000             nan     0.2768   -0.0000
  3220        0.0000             nan     0.2768   -0.0000
  3240        0.0000             nan     0.2768   -0.0000
  3260        0.0000             nan     0.2768   -0.0000
  3280        0.0000             nan     0.2768   -0.0000
  3300        0.0000             nan     0.2768   -0.0000
  3320        0.0000             nan     0.2768   -0.0000
  3340        0.0000             nan     0.2768   -0.0000
  3360        0.0000             nan     0.2768   -0.0000
  3380        0.0000             nan     0.2768   -0.0000
  3400        0.0000             nan     0.2768   -0.0000
  3420        0.0000             nan     0.2768   -0.0000
  3440        0.0000             nan     0.2768   -0.0000
  3460        0.0000             nan     0.2768   -0.0000
  3480        0.0000             nan     0.2768   -0.0000
  3500        0.0000             nan     0.2768   -0.0000
  3520        0.0000             nan     0.2768   -0.0000
  3540        0.0000             nan     0.2768   -0.0000
  3560        0.0000             nan     0.2768   -0.0000
  3580        0.0000             nan     0.2768   -0.0000
  3600        0.0000             nan     0.2768   -0.0000
  3620        0.0000             nan     0.2768   -0.0000
  3640        0.0000             nan     0.2768   -0.0000
  3660        0.0000             nan     0.2768   -0.0000
  3680        0.0000             nan     0.2768   -0.0000
  3690        0.0000             nan     0.2768   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        4.1730             nan     0.3662    1.0787
     2        3.0297             nan     0.3662    0.5082
     3        2.7592             nan     0.3662    0.2128
     4        2.6891             nan     0.3662   -0.8067
     5        2.4501             nan     0.3662   -0.0115
     6        2.0023             nan     0.3662    0.3556
     7        1.9048             nan     0.3662   -0.1216
     8        1.5736             nan     0.3662    0.2120
     9        1.5533             nan     0.3662   -0.2246
    10        1.4075             nan     0.3662   -0.1897
    20        0.7901             nan     0.3662   -0.0205
    40        0.4103             nan     0.3662   -0.0125
    60        0.2965             nan     0.3662   -0.0195
    80        0.2163             nan     0.3662   -0.0162
   100        0.1510             nan     0.3662   -0.0063
   120        0.1056             nan     0.3662   -0.0121
   140        0.0793             nan     0.3662   -0.0172
   160        0.0576             nan     0.3662   -0.0098
   180        0.0445             nan     0.3662   -0.0042
   200        0.0337             nan     0.3662   -0.0041
   220        0.0249             nan     0.3662   -0.0023
   240        0.0221             nan     0.3662   -0.0038
   260        0.0156             nan     0.3662   -0.0019
   280        0.0132             nan     0.3662   -0.0030
   300        0.0098             nan     0.3662   -0.0004
   320        0.0087             nan     0.3662   -0.0012
   340        0.0074             nan     0.3662   -0.0008
   360        0.0055             nan     0.3662   -0.0005
   380        0.0047             nan     0.3662   -0.0013
   400        0.0039             nan     0.3662   -0.0006
   420        0.0031             nan     0.3662   -0.0005
   440        0.0027             nan     0.3662   -0.0001
   460        0.0022             nan     0.3662   -0.0005
   480        0.0014             nan     0.3662   -0.0001
   500        0.0012             nan     0.3662   -0.0002
   520        0.0009             nan     0.3662   -0.0001
   540        0.0008             nan     0.3662   -0.0001
   560        0.0006             nan     0.3662   -0.0002
   580        0.0006             nan     0.3662   -0.0000
   600        0.0004             nan     0.3662   -0.0000
   620        0.0004             nan     0.3662   -0.0001
   640        0.0004             nan     0.3662   -0.0001
   660        0.0003             nan     0.3662   -0.0000
   680        0.0003             nan     0.3662   -0.0000
   700        0.0002             nan     0.3662   -0.0000
   720        0.0002             nan     0.3662   -0.0001
   740        0.0001             nan     0.3662   -0.0000
   760        0.0001             nan     0.3662   -0.0000
   780        0.0001             nan     0.3662   -0.0000
   800        0.0001             nan     0.3662   -0.0000
   820        0.0001             nan     0.3662   -0.0000
   840        0.0001             nan     0.3662   -0.0000
   860        0.0000             nan     0.3662   -0.0000
   880        0.0000             nan     0.3662   -0.0000
   900        0.0000             nan     0.3662   -0.0000
   920        0.0000             nan     0.3662   -0.0000
   940        0.0000             nan     0.3662   -0.0000
   960        0.0000             nan     0.3662   -0.0000
   980        0.0000             nan     0.3662   -0.0000
  1000        0.0000             nan     0.3662   -0.0000
  1020        0.0000             nan     0.3662   -0.0000
  1040        0.0000             nan     0.3662   -0.0000
  1060        0.0000             nan     0.3662   -0.0000
  1080        0.0000             nan     0.3662   -0.0000
  1100        0.0000             nan     0.3662   -0.0000
  1120        0.0000             nan     0.3662   -0.0000
  1140        0.0000             nan     0.3662   -0.0000
  1160        0.0000             nan     0.3662   -0.0000
  1180        0.0000             nan     0.3662   -0.0000
  1200        0.0000             nan     0.3662   -0.0000
  1220        0.0000             nan     0.3662    0.0000
  1240        0.0000             nan     0.3662    0.0000
  1260        0.0000             nan     0.3662   -0.0000
  1280        0.0000             nan     0.3662   -0.0000
  1300        0.0000             nan     0.3662   -0.0000
  1320        0.0000             nan     0.3662   -0.0000
  1340        0.0000             nan     0.3662   -0.0000
  1360        0.0000             nan     0.3662   -0.0000
  1380        0.0000             nan     0.3662   -0.0000
  1400        0.0000             nan     0.3662   -0.0000
  1420        0.0000             nan     0.3662   -0.0000
  1440        0.0000             nan     0.3662   -0.0000
  1460        0.0000             nan     0.3662   -0.0000
  1480        0.0000             nan     0.3662   -0.0000
  1500        0.0000             nan     0.3662   -0.0000
  1520        0.0000             nan     0.3662   -0.0000
  1540        0.0000             nan     0.3662   -0.0000
  1560        0.0000             nan     0.3662   -0.0000
  1580        0.0000             nan     0.3662   -0.0000
  1600        0.0000             nan     0.3662   -0.0000
  1620        0.0000             nan     0.3662   -0.0000
  1640        0.0000             nan     0.3662   -0.0000
  1660        0.0000             nan     0.3662   -0.0000
  1680        0.0000             nan     0.3662   -0.0000
  1700        0.0000             nan     0.3662   -0.0000
  1720        0.0000             nan     0.3662   -0.0000
  1740        0.0000             nan     0.3662   -0.0000
  1760        0.0000             nan     0.3662   -0.0000
  1780        0.0000             nan     0.3662   -0.0000
  1800        0.0000             nan     0.3662   -0.0000
  1820        0.0000             nan     0.3662    0.0000
  1840        0.0000             nan     0.3662   -0.0000
  1860        0.0000             nan     0.3662   -0.0000
  1880        0.0000             nan     0.3662   -0.0000
  1900        0.0000             nan     0.3662   -0.0000
  1920        0.0000             nan     0.3662   -0.0000
  1940        0.0000             nan     0.3662   -0.0000
  1960        0.0000             nan     0.3662   -0.0000
  1980        0.0000             nan     0.3662   -0.0000
  2000        0.0000             nan     0.3662   -0.0000
  2020        0.0000             nan     0.3662   -0.0000
  2040        0.0000             nan     0.3662   -0.0000
  2060        0.0000             nan     0.3662   -0.0000
  2080        0.0000             nan     0.3662   -0.0000
  2100        0.0000             nan     0.3662   -0.0000
  2120        0.0000             nan     0.3662   -0.0000
  2140        0.0000             nan     0.3662   -0.0000
  2160        0.0000             nan     0.3662   -0.0000
  2180        0.0000             nan     0.3662   -0.0000
  2200        0.0000             nan     0.3662   -0.0000
  2220        0.0000             nan     0.3662   -0.0000
  2240        0.0000             nan     0.3662   -0.0000
  2260        0.0000             nan     0.3662   -0.0000
  2280        0.0000             nan     0.3662   -0.0000
  2300        0.0000             nan     0.3662   -0.0000
  2320        0.0000             nan     0.3662   -0.0000
  2340        0.0000             nan     0.3662   -0.0000
  2360        0.0000             nan     0.3662   -0.0000
  2380        0.0000             nan     0.3662   -0.0000
  2400        0.0000             nan     0.3662   -0.0000
  2420        0.0000             nan     0.3662   -0.0000
  2440        0.0000             nan     0.3662   -0.0000
  2460        0.0000             nan     0.3662    0.0000
  2480        0.0000             nan     0.3662   -0.0000
  2500        0.0000             nan     0.3662   -0.0000
  2520        0.0000             nan     0.3662   -0.0000
  2540        0.0000             nan     0.3662   -0.0000
  2560        0.0000             nan     0.3662   -0.0000
  2580        0.0000             nan     0.3662   -0.0000
  2600        0.0000             nan     0.3662    0.0000
  2620        0.0000             nan     0.3662   -0.0000
  2640        0.0000             nan     0.3662   -0.0000
  2660        0.0000             nan     0.3662   -0.0000
  2680        0.0000             nan     0.3662   -0.0000
  2700        0.0000             nan     0.3662   -0.0000
  2720        0.0000             nan     0.3662   -0.0000
  2740        0.0000             nan     0.3662   -0.0000
  2760        0.0000             nan     0.3662   -0.0000
  2780        0.0000             nan     0.3662   -0.0000
  2800        0.0000             nan     0.3662   -0.0000
  2820        0.0000             nan     0.3662   -0.0000
  2840        0.0000             nan     0.3662   -0.0000
  2860        0.0000             nan     0.3662   -0.0000
  2880        0.0000             nan     0.3662   -0.0000
  2900        0.0000             nan     0.3662   -0.0000
  2920        0.0000             nan     0.3662   -0.0000
  2940        0.0000             nan     0.3662   -0.0000
  2960        0.0000             nan     0.3662   -0.0000
  2980        0.0000             nan     0.3662   -0.0000
  3000        0.0000             nan     0.3662   -0.0000
  3020        0.0000             nan     0.3662   -0.0000
  3040        0.0000             nan     0.3662   -0.0000
  3060        0.0000             nan     0.3662   -0.0000
  3080        0.0000             nan     0.3662   -0.0000
  3100        0.0000             nan     0.3662   -0.0000
  3120        0.0000             nan     0.3662   -0.0000
  3140        0.0000             nan     0.3662   -0.0000
  3160        0.0000             nan     0.3662   -0.0000
  3180        0.0000             nan     0.3662   -0.0000
  3200        0.0000             nan     0.3662   -0.0000
  3220        0.0000             nan     0.3662   -0.0000
  3240        0.0000             nan     0.3662   -0.0000
  3260        0.0000             nan     0.3662   -0.0000
  3280        0.0000             nan     0.3662   -0.0000
  3300        0.0000             nan     0.3662   -0.0000
  3320        0.0000             nan     0.3662   -0.0000
  3340        0.0000             nan     0.3662   -0.0000
  3360        0.0000             nan     0.3662   -0.0000
  3380        0.0000             nan     0.3662   -0.0000
  3400        0.0000             nan     0.3662   -0.0000
  3420        0.0000             nan     0.3662   -0.0000
  3440        0.0000             nan     0.3662   -0.0000
  3460        0.0000             nan     0.3662   -0.0000
  3480        0.0000             nan     0.3662   -0.0000
  3500        0.0000             nan     0.3662   -0.0000
  3520        0.0000             nan     0.3662   -0.0000
  3540        0.0000             nan     0.3662   -0.0000
  3560        0.0000             nan     0.3662   -0.0000
  3580        0.0000             nan     0.3662   -0.0000
  3600        0.0000             nan     0.3662   -0.0000
  3620        0.0000             nan     0.3662   -0.0000
  3640        0.0000             nan     0.3662   -0.0000
  3660        0.0000             nan     0.3662   -0.0000
  3680        0.0000             nan     0.3662   -0.0000
  3700        0.0000             nan     0.3662   -0.0000
  3720        0.0000             nan     0.3662   -0.0000
  3740        0.0000             nan     0.3662   -0.0000
  3760        0.0000             nan     0.3662   -0.0000
  3780        0.0000             nan     0.3662   -0.0000
  3800        0.0000             nan     0.3662   -0.0000
  3820        0.0000             nan     0.3662   -0.0000
  3840        0.0000             nan     0.3662   -0.0000
  3860        0.0000             nan     0.3662   -0.0000
  3880        0.0000             nan     0.3662   -0.0000
  3900        0.0000             nan     0.3662   -0.0000
  3920        0.0000             nan     0.3662   -0.0000
  3940        0.0000             nan     0.3662   -0.0000
  3960        0.0000             nan     0.3662   -0.0000
  3980        0.0000             nan     0.3662   -0.0000
  4000        0.0000             nan     0.3662   -0.0000
  4020        0.0000             nan     0.3662   -0.0000
  4040        0.0000             nan     0.3662   -0.0000
  4060        0.0000             nan     0.3662   -0.0000
  4080        0.0000             nan     0.3662   -0.0000
  4100        0.0000             nan     0.3662   -0.0000
  4120        0.0000             nan     0.3662   -0.0000
  4140        0.0000             nan     0.3662   -0.0000
  4160        0.0000             nan     0.3662   -0.0000
  4180        0.0000             nan     0.3662   -0.0000
  4200        0.0000             nan     0.3662   -0.0000
  4220        0.0000             nan     0.3662   -0.0000
  4240        0.0000             nan     0.3662   -0.0000
  4260        0.0000             nan     0.3662   -0.0000
  4280        0.0000             nan     0.3662   -0.0000
  4300        0.0000             nan     0.3662   -0.0000
  4320        0.0000             nan     0.3662   -0.0000
  4340        0.0000             nan     0.3662   -0.0000
  4360        0.0000             nan     0.3662   -0.0000
  4380        0.0000             nan     0.3662   -0.0000
  4400        0.0000             nan     0.3662   -0.0000
  4420        0.0000             nan     0.3662    0.0000
  4440        0.0000             nan     0.3662   -0.0000
  4460        0.0000             nan     0.3662   -0.0000
  4480        0.0000             nan     0.3662   -0.0000
  4500        0.0000             nan     0.3662   -0.0000
  4520        0.0000             nan     0.3662   -0.0000
  4540        0.0000             nan     0.3662   -0.0000
  4560        0.0000             nan     0.3662   -0.0000
  4580        0.0000             nan     0.3662   -0.0000
  4582        0.0000             nan     0.3662   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        3.4651             nan     0.3816    1.5097
     2        2.6963             nan     0.3816    0.9701
     3        2.2485             nan     0.3816    0.4941
     4        2.1426             nan     0.3816    0.0365
     5        1.8923             nan     0.3816   -0.3844
     6        1.7010             nan     0.3816    0.2315
     7        1.7125             nan     0.3816   -0.1012
     8        1.5334             nan     0.3816   -0.3928
     9        1.3139             nan     0.3816    0.0968
    10        1.2395             nan     0.3816   -0.0756
    20        0.6574             nan     0.3816   -0.1327
    40        0.4637             nan     0.3816   -0.0759
    60        0.2645             nan     0.3816   -0.0309
    80        0.2006             nan     0.3816   -0.0307
   100        0.1469             nan     0.3816   -0.0241
   120        0.0902             nan     0.3816   -0.0104
   140        0.0658             nan     0.3816   -0.0084
   160        0.0548             nan     0.3816   -0.0122
   180        0.0388             nan     0.3816   -0.0061
   200        0.0232             nan     0.3816   -0.0019
   220        0.0183             nan     0.3816   -0.0020
   240        0.0142             nan     0.3816   -0.0030
   260        0.0096             nan     0.3816   -0.0005
   280        0.0065             nan     0.3816   -0.0006
   300        0.0054             nan     0.3816   -0.0012
   320        0.0039             nan     0.3816   -0.0008
   340        0.0023             nan     0.3816   -0.0001
   360        0.0016             nan     0.3816   -0.0002
   380        0.0010             nan     0.3816   -0.0000
   400        0.0007             nan     0.3816   -0.0002
   420        0.0006             nan     0.3816    0.0000
   440        0.0006             nan     0.3816   -0.0001
   460        0.0004             nan     0.3816   -0.0001
   480        0.0003             nan     0.3816   -0.0000
   500        0.0002             nan     0.3816   -0.0000
   520        0.0002             nan     0.3816   -0.0000
   540        0.0001             nan     0.3816   -0.0000
   560        0.0001             nan     0.3816   -0.0000
   580        0.0001             nan     0.3816   -0.0000
   600        0.0001             nan     0.3816   -0.0000
   620        0.0000             nan     0.3816   -0.0000
   640        0.0000             nan     0.3816   -0.0000
   660        0.0000             nan     0.3816   -0.0000
   680        0.0000             nan     0.3816   -0.0000
   700        0.0000             nan     0.3816    0.0000
   720        0.0000             nan     0.3816   -0.0000
   740        0.0000             nan     0.3816   -0.0000
   760        0.0000             nan     0.3816   -0.0000
   780        0.0000             nan     0.3816   -0.0000
   800        0.0000             nan     0.3816   -0.0000
   820        0.0000             nan     0.3816   -0.0000
   840        0.0000             nan     0.3816   -0.0000
   860        0.0000             nan     0.3816   -0.0000
   880        0.0000             nan     0.3816   -0.0000
   900        0.0000             nan     0.3816   -0.0000
   920        0.0000             nan     0.3816   -0.0000
   940        0.0000             nan     0.3816   -0.0000
   960        0.0000             nan     0.3816   -0.0000
   980        0.0000             nan     0.3816   -0.0000
  1000        0.0000             nan     0.3816   -0.0000
  1020        0.0000             nan     0.3816   -0.0000
  1040        0.0000             nan     0.3816   -0.0000
  1060        0.0000             nan     0.3816   -0.0000
  1080        0.0000             nan     0.3816   -0.0000
  1100        0.0000             nan     0.3816    0.0000
  1120        0.0000             nan     0.3816   -0.0000
  1140        0.0000             nan     0.3816   -0.0000
  1160        0.0000             nan     0.3816    0.0000
  1180        0.0000             nan     0.3816   -0.0000
  1200        0.0000             nan     0.3816   -0.0000
  1220        0.0000             nan     0.3816   -0.0000
  1240        0.0000             nan     0.3816   -0.0000
  1260        0.0000             nan     0.3816   -0.0000
  1280        0.0000             nan     0.3816   -0.0000
  1300        0.0000             nan     0.3816    0.0000
  1320        0.0000             nan     0.3816   -0.0000
  1340        0.0000             nan     0.3816   -0.0000
  1360        0.0000             nan     0.3816   -0.0000
  1380        0.0000             nan     0.3816   -0.0000
  1400        0.0000             nan     0.3816   -0.0000
  1420        0.0000             nan     0.3816   -0.0000
  1440        0.0000             nan     0.3816    0.0000
  1460        0.0000             nan     0.3816   -0.0000
  1480        0.0000             nan     0.3816   -0.0000
  1500        0.0000             nan     0.3816   -0.0000
  1520        0.0000             nan     0.3816   -0.0000
  1540        0.0000             nan     0.3816   -0.0000
  1560        0.0000             nan     0.3816   -0.0000
  1580        0.0000             nan     0.3816    0.0000
  1600        0.0000             nan     0.3816   -0.0000
  1620        0.0000             nan     0.3816   -0.0000
  1640        0.0000             nan     0.3816   -0.0000
  1660        0.0000             nan     0.3816   -0.0000
  1680        0.0000             nan     0.3816   -0.0000
  1700        0.0000             nan     0.3816   -0.0000
  1720        0.0000             nan     0.3816   -0.0000
  1740        0.0000             nan     0.3816   -0.0000
  1760        0.0000             nan     0.3816   -0.0000
  1780        0.0000             nan     0.3816   -0.0000
  1800        0.0000             nan     0.3816   -0.0000
  1820        0.0000             nan     0.3816   -0.0000
  1840        0.0000             nan     0.3816   -0.0000
  1853        0.0000             nan     0.3816   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        3.9350             nan     0.0984    0.4816
     2        3.5384             nan     0.0984    0.4493
     3        3.2085             nan     0.0984    0.3352
     4        3.0869             nan     0.0984    0.1575
     5        2.8432             nan     0.0984    0.2845
     6        2.5888             nan     0.0984    0.2387
     7        2.2919             nan     0.0984    0.1700
     8        2.2212             nan     0.0984    0.0991
     9        2.0662             nan     0.0984    0.1651
    10        1.8853             nan     0.0984    0.0910
    20        1.4031             nan     0.0984   -0.0107
    40        0.9294             nan     0.0984   -0.0169
    60        0.6514             nan     0.0984   -0.0105
    80        0.4587             nan     0.0984   -0.0091
   100        0.3532             nan     0.0984   -0.0056
   120        0.2760             nan     0.0984   -0.0019
   140        0.2181             nan     0.0984   -0.0055
   160        0.1692             nan     0.0984   -0.0081
   180        0.1402             nan     0.0984   -0.0037
   200        0.1178             nan     0.0984   -0.0001
   220        0.0895             nan     0.0984   -0.0019
   240        0.0678             nan     0.0984   -0.0025
   260        0.0496             nan     0.0984   -0.0018
   280        0.0347             nan     0.0984   -0.0013
   300        0.0274             nan     0.0984   -0.0006
   320        0.0223             nan     0.0984   -0.0013
   340        0.0179             nan     0.0984   -0.0004
   360        0.0139             nan     0.0984   -0.0002
   380        0.0113             nan     0.0984   -0.0002
   400        0.0096             nan     0.0984   -0.0004
   420        0.0076             nan     0.0984   -0.0004
   440        0.0068             nan     0.0984   -0.0003
   460        0.0055             nan     0.0984   -0.0001
   480        0.0051             nan     0.0984   -0.0002
   500        0.0042             nan     0.0984   -0.0002
   520        0.0032             nan     0.0984   -0.0001
   540        0.0025             nan     0.0984   -0.0001
   560        0.0020             nan     0.0984   -0.0000
   580        0.0016             nan     0.0984   -0.0000
   600        0.0014             nan     0.0984    0.0000
   620        0.0011             nan     0.0984   -0.0000
   640        0.0009             nan     0.0984   -0.0000
   660        0.0009             nan     0.0984   -0.0000
   680        0.0007             nan     0.0984   -0.0000
   700        0.0006             nan     0.0984   -0.0000
   720        0.0005             nan     0.0984   -0.0000
   740        0.0004             nan     0.0984   -0.0000
   760        0.0003             nan     0.0984   -0.0000
   780        0.0003             nan     0.0984   -0.0000
   800        0.0002             nan     0.0984   -0.0000
   820        0.0002             nan     0.0984   -0.0000
   840        0.0001             nan     0.0984   -0.0000
   847        0.0001             nan     0.0984   -0.0000

Stochastic Gradient Boosting 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  shrinkage    interaction.depth  n.minobsinnode  n.trees  RMSE      Rsquared 
  0.009233981   4                 16              1911          NaN        NaN
  0.013319140   8                 22              1887          NaN        NaN
  0.046669117   5                 11               394          NaN        NaN
  0.061972542   9                 12              3488          NaN        NaN
  0.098374100   2                  5               847     1.552432  0.5466916
  0.120007527   5                 16               823          NaN        NaN
  0.187308140   7                 18              2552          NaN        NaN
  0.217567193   3                 14              1120          NaN        NaN
  0.268249296   9                 15              1373          NaN        NaN
  0.270371927   2                  7              3669          NaN        NaN
  0.276785133   2                  6              3690     2.619915  0.1740184
  0.330251955   6                  9              1277          NaN        NaN
  0.354241170   1                 13               857          NaN        NaN
  0.366157481   2                  6              4582     2.300941  0.2127178
  0.381559306   8                  5              1853     1.878895  0.4744097
  0.463837563   4                 24              1889          NaN        NaN
  0.466149517   6                 14               264          NaN        NaN
  0.470180911   5                 12              1051          NaN        NaN
  0.483062579  10                 24              3894          NaN        NaN
  0.495746960   3                 22               487          NaN        NaN
  MAE       Selected
       NaN          
       NaN          
       NaN          
       NaN          
  1.217664  *       
       NaN          
       NaN          
       NaN          
       NaN          
       NaN          
  2.306219          
       NaN          
       NaN          
  1.935548          
  1.550323          
       NaN          
       NaN          
       NaN          
       NaN          
       NaN          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were n.trees = 847, interaction.depth =
 2, shrinkage = 0.0983741 and n.minobsinnode = 5.
[1] "Sat Mar 10 04:10:11 2018"
Error in relative.influence(object, n.trees = numTrees) : 
  could not find function "relative.influence"
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:13:32 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "gbm_h2o"                        
Multivariate Adaptive Regression Splines 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results:

  RMSE       Rsquared   MAE      
  0.2442225  0.9931293  0.1694902

Tuning parameter 'degree' was held constant at a value of 1
[1] "Sat Mar 10 04:13:41 2018"
Generalized Linear Model 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results:

  RMSE       Rsquared   MAE      
  0.4777437  0.9566545  0.3376107

[1] "Sat Mar 10 04:13:51 2018"
Negative Binomial Generalized Linear Model 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  link      RMSE       Rsquared   MAE        Selected
  identity  0.4531271  0.9667153  0.2938618          
  log       1.7539549  0.9056412  0.8441654          
  sqrt      0.2875203  0.9899614  0.2058999  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was link = sqrt.
[1] "Sat Mar 10 04:14:00 2018"
Boosted Generalized Linear Model 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  mstop  prune  RMSE       Rsquared   MAE        Selected
   53    no     0.3676959  0.9788199  0.2492393  *       
   79    yes    0.3681431  0.9782771  0.2526152          
   98    yes    0.3681431  0.9782771  0.2526152          
  165    yes    0.3681431  0.9782771  0.2526152          
  170    yes    0.3681431  0.9782771  0.2526152          
  172    yes    0.3681431  0.9782771  0.2526152          
  211    yes    0.3681431  0.9782771  0.2526152          
  224    yes    0.3681431  0.9782771  0.2526152          
  256    no     0.4401986  0.9651147  0.3106114          
  275    no     0.4438863  0.9643471  0.3135426          
  371    no     0.4576575  0.9613397  0.3241853          
  378    no     0.4585167  0.9611557  0.3247980          
  378    yes    0.3681431  0.9782771  0.2526152          
  383    yes    0.3681431  0.9782771  0.2526152          
  511    no     0.4688317  0.9588136  0.3326417          
  698    no     0.4727883  0.9579364  0.3350422          
  734    yes    0.3681431  0.9782771  0.2526152          
  738    yes    0.3681431  0.9782771  0.2526152          
  779    no     0.4738855  0.9576686  0.3357268          
  917    yes    0.3681431  0.9782771  0.2526152          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 53 and prune = no.
[1] "Sat Mar 10 04:14:12 2018"
glmnet 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  alpha       lambda       RMSE       Rsquared   MAE        Selected
  0.05274719  0.128389284  0.4740866  0.9585964  0.3274094          
  0.07865330  0.058017124  0.4647803  0.9592690  0.3229868          
  0.09732015  0.009284300  0.4727615  0.9575838  0.3320622          
  0.16444786  0.042534228  0.4534342  0.9617625  0.3142796          
  0.16937555  0.005159638  0.4696776  0.9583187  0.3297937          
  0.17129523  0.001744801  0.4696724  0.9583221  0.3298043          
  0.21013188  0.077673724  0.4293438  0.9670297  0.2954867          
  0.22396974  0.008537215  0.4685028  0.9585997  0.3290252          
  0.25540271  0.193717769  0.4069138  0.9761917  0.2518511  *       
  0.27464788  2.895099314  1.4735758  0.9802501  0.9636897          
  0.37048346  0.568822261  0.5735011  0.9803184  0.3307284          
  0.37728372  0.924947047  0.7827617  0.9802501  0.4721739          
  0.37774359  0.014910962  0.4555298  0.9614700  0.3186906          
  0.38219311  0.020688638  0.4475690  0.9631586  0.3118353          
  0.51033865  0.376929865  0.4697683  0.9802501  0.2579587          
  0.69756051  2.953591807  2.0591135  0.9826359  1.4136976          
  0.73390926  0.005539363  0.4610353  0.9603774  0.3240810          
  0.73797904  0.003501047  0.4639334  0.9597441  0.3262319          
  0.77884069  5.747058228  2.1244963        NaN  1.4646574          
  0.91644537  0.002869275  0.4630189  0.9600015  0.3255275          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alpha = 0.2554027 and lambda
 = 0.1937178.
[1] "Sat Mar 10 04:14:24 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:17:44 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "glmnet_h2o"                     
Start:  AIC=11.76
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V9    1    1.028   9.795
- V6    1    1.039  10.057
- V4    1    1.043  10.159
- V8    1    1.057  10.513
- V10   1    1.066  10.727
- V7    1    1.086  11.215
<none>       1.027  11.762
- V3    1    1.154  12.809
- V5    1    1.178  13.336
- V2    1   76.136 121.720

Step:  AIC=9.79
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V10

       Df Deviance     AIC
- V6    1    1.041   8.127
- V4    1    1.046   8.234
- V8    1    1.058   8.533
- V10   1    1.068   8.778
- V7    1    1.092   9.363
<none>       1.028   9.795
- V3    1    1.158  10.888
- V5    1    1.207  11.959
- V2    1   76.841 119.959

Step:  AIC=8.13
.outcome ~ V2 + V3 + V4 + V5 + V7 + V8 + V10

       Df Deviance     AIC
- V4    1    1.058   6.548
- V8    1    1.092   7.359
- V10   1    1.096   7.465
- V7    1    1.119   7.991
<none>       1.041   8.127
- V3    1    1.204   9.907
- V5    1    1.212  10.074
- V2    1   77.173 118.072

Step:  AIC=6.55
.outcome ~ V2 + V3 + V5 + V7 + V8 + V10

       Df Deviance     AIC
- V10   1    1.099   5.528
- V7    1    1.121   6.038
- V8    1    1.134   6.335
<none>       1.058   6.548
- V3    1    1.213   8.100
- V5    1    1.224   8.320
- V2    1   77.611 116.219

Step:  AIC=5.53
.outcome ~ V2 + V3 + V5 + V7 + V8

       Df Deviance     AIC
- V7    1    1.140   4.470
- V8    1    1.179   5.364
<none>       1.099   5.528
- V5    1    1.226   6.372
- V3    1    1.235   6.553
- V2    1   77.628 114.224

Step:  AIC=4.47
.outcome ~ V2 + V3 + V5 + V8

       Df Deviance     AIC
- V8    1    1.192   3.644
<none>       1.140   4.470
- V5    1    1.273   5.352
- V3    1    1.277   5.423
- V2    1   81.332 113.436

Step:  AIC=3.64
.outcome ~ V2 + V3 + V5

       Df Deviance     AIC
<none>       1.192   3.644
- V3    1    1.399   5.795
- V5    1    1.490   7.447
- V2    1   85.579 112.760
Start:  AIC=38.92
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V3    1    2.921  36.945
- V8    1    2.931  37.032
- V6    1    2.944  37.152
- V5    1    2.952  37.215
- V9    1    2.990  37.554
- V7    1    3.012  37.738
- V10   1    3.057  38.125
- V4    1    3.083  38.349
<none>       2.918  38.919
- V2    1   94.606 127.367

Step:  AIC=36.95
.outcome ~ V2 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V8    1    2.931  35.033
- V5    1    2.952  35.216
- V6    1    2.958  35.270
- V9    1    2.993  35.581
- V7    1    3.022  35.828
- V10   1    3.063  36.176
- V4    1    3.095  36.451
<none>       2.921  36.945
- V2    1  107.360 128.655

Step:  AIC=35.03
.outcome ~ V2 + V4 + V5 + V6 + V7 + V9 + V10

       Df Deviance     AIC
- V5    1    2.956  33.255
- V6    1    2.985  33.504
- V9    1    3.000  33.641
- V7    1    3.031  33.907
- V10   1    3.071  34.246
- V4    1    3.113  34.597
<none>       2.931  35.033
- V2    1  112.113 127.781

Step:  AIC=33.26
.outcome ~ V2 + V4 + V6 + V7 + V9 + V10

       Df Deviance     AIC
- V6    1    2.998  31.621
- V9    1    3.056  32.120
- V7    1    3.060  32.157
- V10   1    3.096  32.461
- V4    1    3.135  32.782
<none>       2.956  33.255
- V2    1  112.713 125.920

Step:  AIC=31.62
.outcome ~ V2 + V4 + V7 + V9 + V10

       Df Deviance     AIC
- V7    1    3.080  30.323
- V9    1    3.115  30.619
- V10   1    3.183  31.176
- V4    1    3.193  31.258
<none>       2.998  31.621
- V2    1  113.247 124.043

Step:  AIC=30.32
.outcome ~ V2 + V4 + V9 + V10

       Df Deviance     AIC
- V9    1    3.146  28.873
- V10   1    3.216  29.443
- V4    1    3.299  30.111
<none>       3.080  30.323
- V2    1  114.551 122.341

Step:  AIC=28.87
.outcome ~ V2 + V4 + V10

       Df Deviance     AIC
- V10   1    3.232  27.575
- V4    1    3.305  28.155
<none>       3.146  28.873
- V2    1  120.146 121.581

Step:  AIC=27.57
.outcome ~ V2 + V4

       Df Deviance     AIC
- V4    1    3.351  26.513
<none>       3.232  27.575
- V2    1  129.802 121.590

Step:  AIC=26.51
.outcome ~ V2

       Df Deviance     AIC
<none>       3.351  26.513
- V2    1  129.892 119.609
Start:  AIC=29.98
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V5    1    2.196  28.190
- V7    1    2.214  28.412
- V9    1    2.223  28.531
<none>       2.180  29.977
- V10   1    2.343  30.002
- V8    1    2.406  30.744
- V3    1    2.569  32.577
- V4    1    2.645  33.389
- V6    1    2.702  33.990
- V2    1  102.600 135.822

Step:  AIC=28.19
.outcome ~ V2 + V3 + V4 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V7    1    2.272  27.136
- V9    1    2.286  27.306
<none>       2.196  28.190
- V10   1    2.476  29.547
- V8    1    2.495  29.754
- V3    1    2.624  31.167
- V4    1    2.647  31.413
- V6    1    2.710  32.076
- V2    1  102.773 133.869

Step:  AIC=27.14
.outcome ~ V2 + V3 + V4 + V6 + V8 + V9 + V10

       Df Deviance     AIC
- V9    1    2.352  26.102
<none>       2.272  27.136
- V10   1    2.507  27.897
- V8    1    2.607  28.992
- V3    1    2.624  29.169
- V6    1    2.902  31.987
- V4    1    2.962  32.562
- V2    1  102.994 131.930

Step:  AIC=26.1
.outcome ~ V2 + V3 + V4 + V6 + V8 + V10

       Df Deviance     AIC
<none>       2.352  26.102
- V10   1    2.583  26.735
- V3    1    2.659  27.539
- V8    1    2.682  27.786
- V6    1    2.921  30.169
- V4    1    2.963  30.570
- V2    1  103.018 129.936
Start:  AIC=41.85
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance     AIC
- V9    1    3.850  39.879
- V7    1    3.864  40.029
- V8    1    3.867  40.063
- V5    1    3.883  40.228
- V10   1    3.917  40.575
- V3    1    3.948  40.891
- V4    1    4.000  41.416
<none>       3.846  41.846
- V6    1    4.055  41.956
- V2    1  149.038 186.128

Step:  AIC=39.88
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V10

       Df Deviance     AIC
- V7    1    3.866  38.047
- V8    1    3.871  38.097
- V5    1    3.898  38.374
- V10   1    3.918  38.586
- V3    1    3.952  38.928
- V4    1    4.006  39.475
<none>       3.850  39.879
- V6    1    4.075  40.155
- V2    1  151.600 184.810

Step:  AIC=38.05
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V10

       Df Deviance     AIC
- V8    1    3.884  36.236
- V5    1    3.908  36.486
- V10   1    3.934  36.748
- V3    1    3.965  37.057
<none>       3.866  38.047
- V4    1    4.080  38.201
- V6    1    4.084  38.241
- V2    1  153.285 183.252

Step:  AIC=36.24
.outcome ~ V2 + V3 + V4 + V5 + V6 + V10

       Df Deviance     AIC
- V5    1    3.911  34.515
- V10   1    3.936  34.765
- V3    1    3.968  35.092
- V4    1    4.080  36.201
<none>       3.884  36.236
- V6    1    4.090  36.297
- V2    1  164.336 184.036

Step:  AIC=34.52
.outcome ~ V2 + V3 + V4 + V6 + V10

       Df Deviance     AIC
- V10   1    3.949  32.895
- V3    1    3.991  33.319
<none>       3.911  34.515
- V6    1    4.123  34.619
- V4    1    4.134  34.727
- V2    1  165.011 182.200

Step:  AIC=32.9
.outcome ~ V2 + V3 + V4 + V6

       Df Deviance     AIC
- V3    1    4.008  31.492
- V4    1    4.140  32.785
<none>       3.949  32.895
- V6    1    4.226  33.610
- V2    1  167.317 180.756

Step:  AIC=31.49
.outcome ~ V2 + V4 + V6

       Df Deviance     AIC
- V4    1    4.211  31.468
<none>       4.008  31.492
- V6    1    4.367  32.925
- V2    1  184.031 182.564

Step:  AIC=31.47
.outcome ~ V2 + V6

       Df Deviance     AIC
<none>       4.211  31.468
- V6    1    4.538  32.456
- V2    1  184.165 180.593
Generalized Linear Model with Stepwise Feature Selection 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results:

  RMSE       Rsquared   MAE      
  0.4368565  0.9656231  0.3073659

[1] "Sat Mar 10 04:17:54 2018"
Independent Component Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  n.comp  RMSE       Rsquared   MAE        Selected
  1       1.9850261  0.1661350  1.3452668          
  2       1.6359578  0.4322175  1.0839403          
  3       1.5334178  0.5129545  1.1078008          
  4       1.5565106  0.5062272  1.1740135          
  5       1.3763468  0.6228163  1.0046293          
  7       0.7134378  0.8991942  0.5348386          
  8       0.5751311  0.9372510  0.4280503          
  9       0.4777437  0.9566545  0.3376107  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was n.comp = 9.
[1] "Sat Mar 10 04:18:06 2018"
Partial Least Squares 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      1.3918354  0.5963563  1.0454297          
  2      0.9019424  0.8330318  0.6932893          
  3      0.6579450  0.9159505  0.4751248          
  4      0.5443582  0.9413198  0.3871818          
  5      0.4988088  0.9524827  0.3557145          
  7      0.4813454  0.9561538  0.3418882          
  8      0.4794606  0.9562373  0.3382011          
  9      0.4777437  0.9566545  0.3376107  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 9.
[1] "Sat Mar 10 04:18:15 2018"
Error in MSEP(object) : could not find function "MSEP"
k-Nearest Neighbors 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  kmax  distance   kernel        RMSE      Rsquared   MAE        Selected
   1    1.6242915  inv           1.352881  0.6721599  0.9162460          
   2    0.7497713  inv           1.669377  0.5165218  0.9603023          
   2    1.3598360  rectangular   1.217322  0.7248810  0.8379345  *       
   3    0.1932181  triweight     2.339538  0.3642823  1.2261229          
   3    0.5541890  triangular    1.417196  0.7194985  0.7549603          
   3    0.7218418  epanechnikov  1.510742  0.6764454  0.7663444          
   3    1.2564849  triangular    1.322106  0.7254145  0.8613434          
   3    1.4569778  inv           1.500263  0.6260516  1.0224411          
   4    1.7612373  triweight     1.326201  0.7727563  0.9248534          
   4    2.6616030  biweight      1.504085  0.6147730  1.0586612          
   5    0.9075040  inv           1.495366  0.6137057  0.8722530          
   5    1.0165344  rectangular   1.470417  0.7330818  0.8824544          
   5    2.1198577  cos           1.343939  0.7593450  0.9399069          
   5    2.2817175  rectangular   1.244956  0.8319787  0.8496458          
   7    1.9828542  epanechnikov  1.318373  0.7671577  0.9234709          
  10    0.4250774  biweight      1.374560  0.7003068  0.7607536          
  10    0.5778313  biweight      1.369175  0.7171231  0.7629608          
  10    2.6682624  rectangular   1.388318  0.6800638  0.9240000          
  11    2.8898824  inv           1.273150  0.7216152  0.9167096          
  12    0.3588235  triweight     1.359521  0.7268252  0.7767861          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were kmax = 2, distance = 1.359836
 and kernel = rectangular.
[1] "Sat Mar 10 04:18:26 2018"
k-Nearest Neighbors 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  k   RMSE      Rsquared   MAE       Selected
   1  1.855750  0.3643829  1.180361          
   2  1.592377  0.5245518  1.013391  *       
   3  1.708229  0.3930723  1.078183          
   4  1.815860  0.3249219  1.079938          
   5  1.812047  0.3384117  1.156916          
   7  1.757580  0.4448165  1.143716          
  10  1.751015  0.5008638  1.133322          
  11  1.742791  0.5503810  1.141624          
  12  1.749366  0.5717575  1.133674          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 2.
[1] "Sat Mar 10 04:18:36 2018"
Polynomial Kernel Regularized Least Squares 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  lambda        degree  RMSE         Rsquared    MAE          Selected
  1.835422e-05  2          2.111953  0.47130998     1.452920  *       
  2.473241e-05  2          2.111953  0.47131009     1.452920          
  3.066202e-05  1       7966.489500  0.09341417  6785.783402          
  6.641089e-05  2          2.111953  0.47131080     1.452920          
  7.028745e-05  1       3475.217166  0.09341646  2960.219795          
  7.185817e-05  1       3399.251039  0.09341655  2895.513569          
  1.123723e-04  2          2.111953  0.47131160     1.452920          
  1.317798e-04  1       1853.525798  0.09342003  1578.899622          
  1.892403e-04  2          2.111953  0.47131292     1.452920          
  2.361780e-04  3          2.123834  0.12614935     1.464690          
  7.118972e-04  3          2.123834  0.12615495     1.464690          
  7.698721e-04  3          2.123834  0.12615563     1.464690          
  7.739590e-04  1        315.505677  0.09345729   268.844319          
  8.146397e-04  2          2.111953  0.47132369     1.452920          
  3.561995e-03  2          2.111955  0.47137091     1.452919          
  3.074698e-02  3          2.123831  0.12650944     1.464683          
  4.672468e-02  1          5.515486  0.09627648     4.726981          
  4.896607e-02  1          5.294197  0.09642164     4.528808          
  7.837967e-02  3          2.123826  0.12707476     1.464671          
  3.821438e-01  1          2.130360  0.12525144     1.403389          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 1.835422e-05 and degree = 2.
[1] "Sat Mar 10 04:18:46 2018"

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.620320452  0.065820157 -0.046834885  0.085077406 -0.086820265 -0.017478579 
          V8           V9          V10 
 0.026975732  0.088369183 -0.008276965 

 Quartiles of Marginal Effects:
 
          V2         V3          V4         V5          V6          V7
25% 2.482013 0.02288779 -0.07332220 0.04512978 -0.10165924 -0.04984472
50% 2.726728 0.07608945 -0.05340159 0.09486432 -0.07903162 -0.02310710
75% 2.853337 0.13799697 -0.02179566 0.13749108 -0.04678011  0.03749046
             V8         V9         V10
25% -0.02431783 0.05544415 -0.07293802
50%  0.02686479 0.09282232 -0.02374539
75%  0.08338037 0.12449560  0.05504997

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.92614806  0.05101727  0.03059435  0.09315989 -0.03467443  0.03880747 
         V8          V9         V10 
-0.01976273  0.02210664 -0.03354718 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5          V6         V7
25% 2.865604 0.02499833 0.01490532 0.06955563 -0.04677596 0.02260690
50% 2.932281 0.05417763 0.03314562 0.09733010 -0.02890448 0.04060924
75% 2.977033 0.07057383 0.04289587 0.11182899 -0.02317057 0.05681664
              V8         V9          V10
25% -0.038940866 0.00575760 -0.071458287
50% -0.019863539 0.02165112 -0.036279042
75% -0.003475664 0.03654132 -0.002139961

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.88727348  0.13293481 -0.09777635  0.10403584 -0.09509221 -0.02247037 
         V8          V9         V10 
 0.04460603  0.11315885  0.04224114 

 Quartiles of Marginal Effects:
 
          V2         V3          V4          V5          V6          V7
25% 1.336011 0.04430324 -0.16210879 -0.01088847 -0.18286609 -0.10141938
50% 1.952910 0.12961913 -0.09651141  0.10971789 -0.07958666 -0.01302060
75% 2.352921 0.22976623 -0.03729064  0.20782072 -0.01421398  0.03233633
             V8         V9         V10
25% -0.03769321 0.07748183 -0.09502753
50%  0.07725965 0.11695813  0.05411844
75%  0.13619143 0.21562815  0.19140088

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.88454217  0.05705439  0.02580857  0.09694342 -0.03449998  0.03995152 
         V8          V9         V10 
-0.01725690  0.02487625 -0.03239769 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5          V6         V7
25% 2.820631 0.03290072 0.01126691 0.07215463 -0.04569094 0.02526334
50% 2.908798 0.05901118 0.02903326 0.09947499 -0.03155198 0.04189792
75% 2.958340 0.07938525 0.03772381 0.11675030 -0.01959850 0.06038720
               V8          V9          V10
25% -0.0402345819 0.009778886 -0.070632334
50% -0.0177046777 0.028842609 -0.030984384
75% -0.0006789697 0.037542789  0.001553641

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.245402332  0.036377873 -0.049021786 -0.001292404  0.002341309  0.051536569 
          V8           V9          V10 
 0.021739275  0.013476114  0.058624606 

 Quartiles of Marginal Effects:
 
            V2          V3           V4           V5          V6          V7
25% 0.03734115 -0.01061357 -0.057171304 -0.045048398 -0.06088866 -0.01632704
50% 0.12889678  0.01286516 -0.034877912  0.003829579  0.03205397  0.03852682
75% 0.37236536  0.05425150 -0.004267796  0.039542906  0.11197108  0.11715709
             V8          V9        V10
25% -0.01484582 -0.03267691 0.00794350
50%  0.01399011  0.01307452 0.04258733
75%  0.06321925  0.06581462 0.10792187

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.10677687  0.13039782 -0.05129699  0.13190340  0.01552208  0.11593952 
         V8          V9         V10 
 0.00888996  0.09456261  0.08548763 

 Quartiles of Marginal Effects:
 
          V2        V3          V4        V5         V6        V7          V8
25% 1.103784 0.1300296 -0.05160175 0.1314402 0.01498323 0.1153984 0.008505087
50% 1.107786 0.1305086 -0.05138104 0.1322432 0.01557519 0.1162067 0.009016999
75% 1.110293 0.1310691 -0.05115878 0.1327263 0.01590809 0.1166480 0.009305681
            V9        V10
25% 0.09415344 0.08523254
50% 0.09487341 0.08584087
75% 0.09523573 0.08652747

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.49994828  0.07937460 -0.05611895  0.09357879 -0.09093931 -0.02248330 
         V8          V9         V10 
 0.03336191  0.09513162 -0.00478992 

 Quartiles of Marginal Effects:
 
          V2         V3          V4         V5          V6          V7
25% 2.371419 0.03214891 -0.08617445 0.04847209 -0.11526557 -0.05646472
50% 2.628209 0.08592922 -0.06734556 0.10773930 -0.07733243 -0.02947248
75% 2.812454 0.17545147 -0.02590841 0.15951899 -0.05219587  0.03350668
             V8        V9          V10
25% -0.02544598 0.0622848 -0.078863572
50%  0.03094143 0.1060176 -0.006118651
75%  0.09118096 0.1493042  0.068148580

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.226924822  0.033215212 -0.046105129 -0.001636254  0.003017545  0.049095794 
          V8           V9          V10 
 0.020707060  0.012270241  0.055364362 

 Quartiles of Marginal Effects:
 
            V2          V3           V4           V5          V6          V7
25% 0.03291132 -0.01119115 -0.054506390 -0.044500500 -0.05766739 -0.01619471
50% 0.11449296  0.01161072 -0.031582499  0.001887314  0.02952741  0.03570403
75% 0.34183669  0.04978671 -0.003499181  0.036266242  0.10787276  0.11035952
             V8          V9         V10
25% -0.01371375 -0.03051774 0.007816209
50%  0.01280044  0.01276000 0.038562971
75%  0.06035335  0.06209904 0.104400117

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.114301767  0.018310344 -0.007699004  0.018989284  0.002316858  0.019295622 
          V8           V9          V10 
 0.001698879  0.016565478  0.018765246 

 Quartiles of Marginal Effects:
 
           V2         V3           V4         V5          V6         V7
25% 0.1141232 0.01828990 -0.007720338 0.01895120 0.002299362 0.01926555
50% 0.1143501 0.01831601 -0.007703234 0.01901399 0.002315799 0.01931532
75% 0.1144908 0.01835737 -0.007691644 0.01904010 0.002338673 0.01933845
             V8         V9        V10
25% 0.001680087 0.01653940 0.01875721
50% 0.001709113 0.01658979 0.01879303
75% 0.001719335 0.01660390 0.01882402

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 1.415622204  0.145604105 -0.111928619  0.083226820 -0.076441332  0.009862429 
          V8           V9          V10 
 0.038995716  0.099466698  0.083381004 

 Quartiles of Marginal Effects:
 
           V2         V3          V4          V5          V6          V7
25% 0.7221331 0.02906696 -0.18114217 -0.05167049 -0.16000077 -0.06637433
50% 1.5088706 0.12247345 -0.11313728  0.06352041 -0.03734383  0.02385957
75% 1.9088672 0.27317377 -0.04617246  0.20811160  0.02790815  0.05697428
             V8         V9         V10
25% -0.02537112 0.03312289 -0.04668191
50%  0.05666248 0.09784468  0.08825384
75%  0.14282175 0.22457118  0.28787331

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.757470774  0.050473386 -0.033986798  0.073100284 -0.078753244 -0.007631430 
          V8           V9          V10 
 0.016758467  0.077232900 -0.009784856 

 Quartiles of Marginal Effects:
 
          V2          V3          V4         V5          V6          V7
25% 2.604063 0.008542458 -0.05725125 0.04346864 -0.08916567 -0.04036578
50% 2.819620 0.048124814 -0.03917944 0.06135271 -0.06742285 -0.01238678
75% 2.881758 0.102039829 -0.02030374 0.11832503 -0.05100244  0.04588311
             V8         V9          V10
25% -0.01725066 0.04645234 -0.064006748
50%  0.01380486 0.07577921 -0.004917513
75%  0.07436124 0.11590144  0.044018887

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.94532030  0.05271529  0.03522614  0.09846277 -0.03151232  0.04326426 
         V8          V9         V10 
-0.02464073  0.01787992 -0.03936974 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5          V6         V7
25% 2.895519 0.03017041 0.02256117 0.07584976 -0.04263468 0.03001159
50% 2.947806 0.05517974 0.04039476 0.10319282 -0.02623329 0.04305488
75% 2.985242 0.06889929 0.04613894 0.11650476 -0.02023106 0.05770149
             V8          V9         V10
25% -0.04139485 0.002723437 -0.07203303
50% -0.02527678 0.018400476 -0.04556619
75% -0.01032531 0.030302859 -0.01324560

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.18141254  0.11235672 -0.07765244  0.10565494 -0.09312087 -0.02297223 
         V8          V9         V10 
 0.04103905  0.10418695  0.01273920 

 Quartiles of Marginal Effects:
 
          V2         V3          V4         V5          V6          V7
25% 1.867189 0.04294268 -0.12476775 0.02223012 -0.14965524 -0.07268749
50% 2.241077 0.11282979 -0.09099926 0.12741003 -0.07787301 -0.02115709
75% 2.590262 0.22252205 -0.03465508 0.19355296 -0.03605964  0.03480929
             V8         V9         V10
25% -0.02723793 0.07424886 -0.10157405
50%  0.05161323 0.11637015  0.01558195
75%  0.11824558 0.16436370  0.12577886

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.721332255  0.055055677 -0.035117665  0.077102463 -0.079330711 -0.008081383 
          V8           V9          V10 
 0.019026625  0.077754760 -0.010834431 

 Quartiles of Marginal Effects:
 
          V2         V3          V4         V5          V6          V7
25% 2.573000 0.01310453 -0.05470153 0.03799142 -0.08468557 -0.04030267
50% 2.796697 0.05299710 -0.04001991 0.07147559 -0.06984098 -0.01149645
75% 2.866246 0.10650937 -0.01957491 0.12521344 -0.05163773  0.04212518
              V8         V9          V10
25% -0.008817278 0.04778540 -0.055922499
50%  0.011154266 0.07724069 -0.006606663
75%  0.074354723 0.12013127  0.036609619

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.892771766  0.042047242  0.006786668  0.072109753 -0.051139229  0.021300373 
          V8           V9          V10 
-0.004058233  0.041763922 -0.017209399 

 Quartiles of Marginal Effects:
 
          V2          V3           V4         V5          V6           V7
25% 2.782259 0.003545832 -0.016162979 0.03734050 -0.06483260 -0.008579528
50% 2.897871 0.045528127  0.009432477 0.07279711 -0.04327321  0.025216538
75% 2.959263 0.072792662  0.022750811 0.09632976 -0.02895117  0.052123386
               V8         V9         V10
25% -3.215953e-02 0.02299941 -0.08009602
50%  2.021595e-07 0.04276606 -0.01042262
75%  2.417147e-02 0.05856440  0.03013071

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.0264348754 -0.0004983028 -0.0066006994 -0.0014328112 -0.0004328022 
           V7            V8            V9           V10 
 0.0084154093  0.0049195262  0.0029803385  0.0098806903 

 Quartiles of Marginal Effects:
 
               V2            V3           V4            V5            V6
25% -0.0004432496 -2.743818e-03 -0.008209649 -0.0066695806 -0.0009697727
50%  0.0020144954 -1.032210e-06 -0.000990834  0.0006287655  0.0029513742
75%  0.0285638989  3.490622e-03  0.005975896  0.0048524843  0.0173054955
               V7            V8           V9          V10
25% -0.0030806414 -0.0022598694 -0.002639340 -0.008825401
50%  0.0008999735  0.0005956069  0.001622672  0.000880337
75%  0.0092868413  0.0074914341  0.010268995  0.007875917

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 2.7945336349  0.0471851086 -0.0258756000  0.0706903686 -0.0730871834 
           V7            V8            V9           V10 
-0.0008989028  0.0124561512  0.0696039264 -0.0112048077 

 Quartiles of Marginal Effects:
 
          V2         V3          V4         V5          V6           V7
25% 2.639422 0.00262818 -0.04408547 0.04286510 -0.08041018 -0.035266671
50% 2.840639 0.04803594 -0.02689885 0.06334818 -0.06414476 -0.002796611
75% 2.901558 0.09639703 -0.01433678 0.11036095 -0.04476224  0.049498992
             V8         V9          V10
25% -0.02485462 0.04048158 -0.069070249
50%  0.01232737 0.06798715 -0.009465739
75%  0.06303271 0.10846253  0.038070215

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.792903569  0.104633818 -0.042817129  0.105669190  0.012908452  0.098346570 
          V8           V9          V10 
 0.008488439  0.081897174  0.081361513 

 Quartiles of Marginal Effects:
 
           V2        V3          V4        V5         V6         V7          V8
25% 0.7914930 0.1044364 -0.04296831 0.1054470 0.01268608 0.09811863 0.008310942
50% 0.7933212 0.1046721 -0.04285042 0.1058310 0.01292878 0.09848159 0.008556551
75% 0.7944887 0.1049699 -0.04275716 0.1060315 0.01307791 0.09869505 0.008667750
            V9        V10
25% 0.08172110 0.08127262
50% 0.08207510 0.08153657
75% 0.08220399 0.08182152

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.97236809  0.07064843  0.03082879  0.12178624 -0.02394266  0.06651882 
         V8          V9         V10 
-0.03986304  0.01194587 -0.06212930 

 Quartiles of Marginal Effects:
 
          V2         V3         V4        V5          V6         V7          V8
25% 2.961484 0.06582473 0.02707837 0.1154853 -0.02694028 0.06397359 -0.04342179
50% 2.974506 0.07052843 0.03207555 0.1225148 -0.02340504 0.06645205 -0.04040781
75% 2.982789 0.07439398 0.03452951 0.1269258 -0.02101881 0.06991087 -0.03677504
             V9         V10
25% 0.007449076 -0.07021409
50% 0.012488613 -0.06208729
75% 0.015372867 -0.05594488

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.90429852  0.06127781  0.03099818  0.10571406 -0.02924941  0.04772919 
         V8          V9         V10 
-0.02410646  0.01976090 -0.04012709 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5          V6         V7
25% 2.862082 0.04248845 0.02036871 0.08753174 -0.03852066 0.03735458
50% 2.924637 0.06325047 0.03369576 0.10846120 -0.02676791 0.04688139
75% 2.963232 0.07812017 0.04040435 0.12379248 -0.01873755 0.06272515
             V8          V9         V10
25% -0.04307019 0.003914021 -0.06982579
50% -0.02471857 0.022458446 -0.04316864
75% -0.01120855 0.031090195 -0.01442369

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.783417900  0.029091693  0.067074463  0.050977278  0.005231808 -0.034282259 
          V8           V9          V10 
-0.065076772  0.063511619  0.046573626 

 Quartiles of Marginal Effects:
 
          V2          V3          V4          V5          V6          V7
25% 2.508495 -0.04677989 -0.05582545 0.003912074 -0.06505336 -0.11100972
50% 2.884340  0.03350492  0.06349869 0.084473359 -0.00650806 -0.01473447
75% 3.125272  0.11907683  0.21007007 0.161846808  0.09168241  0.06291711
              V8          V9          V10
25% -0.140621984 -0.01101178 -0.006434058
50% -0.074549975  0.05552461  0.056118480
75%  0.009062186  0.10093277  0.111146144

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 3.18858912  0.02359330  0.07711855  0.05420136 -0.04532371 -0.04907695 
         V8          V9         V10 
-0.01705081  0.04899659 -0.02962476 

 Quartiles of Marginal Effects:
 
          V2            V3         V4         V5          V6          V7
25% 3.045610 -0.0008628007 0.02309886 0.02539176 -0.06807449 -0.05864310
50% 3.181480  0.0213444388 0.07699503 0.06255191 -0.04674492 -0.04318160
75% 3.294008  0.0561108209 0.11246967 0.10761469 -0.01202732 -0.01680961
             V8           V9          V10
25% -0.05537139 -0.006873319 -0.060338490
50% -0.01862436  0.033265844 -0.026330781
75%  0.01413735  0.090408603  0.007782726

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 1.9571819780  0.0009383531  0.0616045577 -0.0511865586  0.0251339915 
           V7            V8            V9           V10 
 0.0056849332  0.0177205980  0.1392852357  0.0648698083 

 Quartiles of Marginal Effects:
 
          V2          V3          V4          V5          V6          V7
25% 1.374515 -0.15075671 -0.07496864 -0.07576095 -0.03901503 -0.02675796
50% 2.089226  0.00211411  0.07991061 -0.01572996  0.02710577  0.06539907
75% 2.378370  0.08763120  0.19344504  0.06524736  0.08866704  0.14207519
              V8         V9         V10
25% -0.070212920 0.05307117 -0.03301047
50% -0.008010749 0.09972796  0.09434349
75%  0.135820017 0.26630016  0.15259942

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 3.141760699  0.026879789  0.076525176  0.050832407 -0.037373313 -0.045174005 
          V8           V9          V10 
-0.008484749  0.056835263 -0.031051121 

 Quartiles of Marginal Effects:
 
          V2          V3         V4         V5            V6          V7
25% 3.009337 0.003511494 0.02270222 0.02765990 -0.0646388903 -0.05510994
50% 3.132752 0.023862490 0.07606574 0.06507909 -0.0370694750 -0.03801889
75% 3.278036 0.057386151 0.11253079 0.10020281 -0.0009490989 -0.01128860
              V8           V9          V10
25% -0.045748884 -0.002488457 -0.062940725
50% -0.005633316  0.037564058 -0.023847809
75%  0.025048437  0.099956829  0.004298171

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.23110494 -0.04621965 -0.03137419 -0.08907271  0.02571380  0.01243031 
         V8          V9         V10 
 0.04285360  0.03871501  0.02954469 

 Quartiles of Marginal Effects:
 
           V2           V3          V4           V5          V6          V7
25% 0.0239020 -0.111713370 -0.13045285 -0.160247186 -0.11285715 -0.02560668
50% 0.1608268 -0.003359061 -0.02945926 -0.060307986  0.01816505  0.02754621
75% 0.3101568  0.018275196  0.02987252 -0.007952054  0.11029349  0.07587449
             V8          V9           V10
25% -0.01347576 -0.05340653 -0.0577995670
50%  0.01932795  0.02593358 -0.0005102842
75%  0.12963748  0.18663230  0.0559084782

 Average Marginal Effects:
 
        V2         V3         V4         V5         V6         V7         V8 
1.17368313 0.13348706 0.02859183 0.01512530 0.06509178 0.08783230 0.15051623 
        V9        V10 
0.15473757 0.10992545 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5         V6         V7        V8
25% 1.170596 0.1329655 0.02791847 0.01468244 0.06465870 0.08745125 0.1500042
50% 1.175125 0.1335422 0.02844869 0.01516334 0.06529488 0.08834181 0.1508160
75% 1.176755 0.1340386 0.02930024 0.01582594 0.06588385 0.08850440 0.1514292
           V9       V10
25% 0.1544281 0.1093045
50% 0.1551492 0.1101036
75% 0.1554014 0.1106741

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.656109242  0.024236862  0.070014378  0.036021649  0.006269819 -0.030845989 
          V8           V9          V10 
-0.053167427  0.082435318  0.048493983 

 Quartiles of Marginal Effects:
 
          V2          V3          V4          V5          V6          V7
25% 2.372338 -0.05045557 -0.05235331 0.005962268 -0.05098320 -0.10523402
50% 2.816289  0.04270450  0.06732030 0.065602167 -0.01002477 -0.01708127
75% 3.069715  0.11964386  0.20366498 0.133685420  0.09822260  0.07690155
             V8           V9         V10
25% -0.11726828 -0.007945811 -0.02240966
50% -0.07005531  0.076116293  0.06898953
75%  0.01493567  0.132374360  0.11493094

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.21446555 -0.04410703 -0.02968458 -0.08391226  0.02528734  0.01124877 
         V8          V9         V10 
 0.04056684  0.03597158  0.02808945 

 Quartiles of Marginal Effects:
 
            V2           V3          V4           V5          V6          V7
25% 0.02170589 -0.106707513 -0.12427780 -0.150036270 -0.10277716 -0.02394204
50% 0.14764173 -0.002655363 -0.02932901 -0.059563223  0.01669681  0.02445982
75% 0.29145423  0.015956047  0.02675140 -0.004801538  0.10437362  0.06894964
             V8          V9           V10
25% -0.01614543 -0.05000673 -0.0582421229
50%  0.01899511  0.02266789 -0.0001042904
75%  0.12105376  0.17593290  0.0528947085

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
0.122645178 0.019727469 0.003216099 0.002081116 0.007411316 0.014348604 
         V8          V9         V10 
0.024159583 0.027125067 0.027228116 

 Quartiles of Marginal Effects:
 
           V2         V3          V4          V5          V6         V7
25% 0.1224717 0.01968912 0.003177910 0.002047330 0.007385862 0.01432473
50% 0.1227325 0.01973287 0.003204974 0.002080639 0.007437203 0.01437720
75% 0.1228100 0.01977031 0.003255716 0.002117908 0.007454055 0.01439463
            V8         V9        V10
25% 0.02413477 0.02708229 0.02718967
50% 0.02418481 0.02715511 0.02724412
75% 0.02421292 0.02718203 0.02728274

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.40909172 -0.02130853  0.03303718 -0.10900290  0.03475926  0.03146256 
         V8          V9         V10 
 0.06127789  0.14072703  0.07059835 

 Quartiles of Marginal Effects:
 
           V2          V3           V4          V5           V6          V7
25% 0.8448386 -0.13132120 -0.112190932 -0.21282415 -0.069740526 0.004792113
50% 1.5126601 -0.03320229 -0.007572408 -0.07430557  0.009507895 0.104441277
75% 1.8165047  0.07030431  0.174273157  0.05592244  0.133133764 0.194702648
             V8         V9         V10
25% -0.04756056 0.02115337 -0.03817282
50%  0.02506217 0.13720202  0.07151219
75%  0.19782639 0.29926251  0.17843469

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.922994036  0.033966771  0.062595022  0.065112517  0.007643061 -0.033373621 
          V8           V9          V10 
-0.070125158  0.041326304  0.044640311 

 Quartiles of Marginal Effects:
 
          V2          V3          V4           V5            V6          V7
25% 2.639599 -0.03607472 -0.06214634 -0.008049852 -0.0666844809 -0.09139626
50% 2.941857  0.02311142  0.06940586  0.102408647  0.0008264306 -0.01037511
75% 3.178023  0.11277416  0.18104029  0.184572732  0.0907855298  0.05106309
              V8          V9         V10
25% -0.153370194 -0.02684865 0.007793492
50% -0.073359475  0.02648977 0.051585053
75% -0.009796249  0.07689523 0.097155723

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 3.21926465  0.02140273  0.08509314  0.05291398 -0.05088624 -0.05481487 
         V8          V9         V10 
-0.01141154  0.05266492 -0.04564577 

 Quartiles of Marginal Effects:
 
          V2           V3         V4         V5          V6          V7
25% 3.097618 0.0006881894 0.03898816 0.03172244 -0.07211147 -0.06577917
50% 3.215443 0.0205681790 0.09012767 0.05788098 -0.05318861 -0.05234148
75% 3.303182 0.0489875984 0.11870413 0.10056388 -0.01901158 -0.03038084
             V8          V9         V10
25% -0.04492626 0.003086223 -0.07453705
50% -0.01400700 0.038369880 -0.04385726
75%  0.01805671 0.088063881 -0.01232559

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.300597917  0.012138428  0.071426823 -0.009370795  0.017588087 -0.010828801 
          V8           V9          V10 
-0.010456411  0.122641072  0.054556316 

 Quartiles of Marginal Effects:
 
          V2          V3          V4          V5          V6          V7
25% 1.898929 -0.08958461 -0.04569681 -0.02259995 -0.04717021 -0.05356179
50% 2.493092  0.03419184  0.08062247  0.03013252  0.02740542  0.02367628
75% 2.689074  0.08756301  0.19494799  0.08049556  0.06789040  0.13542430
             V8         V9         V10
25% -0.07726144 0.02122441 -0.03377590
50% -0.04124023 0.10765653  0.09045306
75%  0.08530121 0.21986792  0.13529013

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.888363117  0.031650035  0.064743670  0.060610665  0.006927832 -0.032249876 
          V8           V9          V10 
-0.064615422  0.049011837  0.044132902 

 Quartiles of Marginal Effects:
 
          V2          V3          V4           V5            V6           V7
25% 2.614626 -0.03594447 -0.05521766 -0.006381932 -0.0587237350 -0.095189643
50% 2.927697  0.02680895  0.06569222  0.095898532 -0.0001184508 -0.006808883
75% 3.165272  0.11726412  0.18414672  0.177314160  0.0881780105  0.058770836
               V8          V9         V10
25% -0.1384397864 -0.01933396 0.002732749
50% -0.0668143168  0.03555852 0.052281388
75%  0.0005729187  0.08162171 0.098839541

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 3.10057477  0.02967238  0.06044757  0.06539726 -0.01516376 -0.03474052 
         V8          V9         V10 
-0.03866531  0.03355147  0.01945939 

 Quartiles of Marginal Effects:
 
          V2          V3          V4           V5          V6           V7
25% 2.877569 -0.01338490 -0.01324590 -0.006574646 -0.04964667 -0.066638949
50% 3.098253  0.01847009  0.05540701  0.098549835 -0.01439466 -0.002813201
75% 3.244998  0.07946809  0.11845893  0.138760622  0.02198366  0.006401776
              V8          V9        V10
25% -0.082284987 -0.02518923 0.01066541
50% -0.043689392  0.01512203 0.02199241
75% -0.004271802  0.06844821 0.06654922

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.042602177 -0.013160000 -0.003117988 -0.027181011  0.012477628  0.002590946 
          V8           V9          V10 
 0.013396846  0.009299610  0.004673957 

 Quartiles of Marginal Effects:
 
               V2            V3            V4           V5            V6
25% -0.0004063937 -0.0515910923 -0.0111301950 -0.046452069 -0.0115621986
50%  0.0064662497 -0.0047667002 -0.0001050345 -0.004592479  0.0003641805
75%  0.0514105599  0.0008842118  0.0139360612  0.001473694  0.0208147883
               V7            V8            V9          V10
25% -0.0201267794 -0.0049004280 -0.0220198471 -0.025104736
50% -0.0006003376  0.0002201671  0.0002404331 -0.001465302
75%  0.0100434253  0.0255520392  0.0217782000  0.004743973

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.96329766  0.03341972  0.06210263  0.06683442  0.00794840 -0.03056585 
         V8          V9         V10 
-0.06219811  0.03798358  0.04223141 

 Quartiles of Marginal Effects:
 
          V2          V3          V4          V5           V6           V7
25% 2.691858 -0.03052066 -0.05142367 -0.01040466 -0.055720216 -0.081271815
50% 2.976415  0.02995303  0.06730985  0.10319440  0.003535722 -0.001683107
75% 3.192386  0.10983337  0.16459136  0.17317403  0.081200382  0.047074527
             V8          V9        V10
25% -0.13200283 -0.02326205 0.00904856
50% -0.05988943  0.01592928 0.05684331
75% -0.01891520  0.06900577 0.09489341

 Average Marginal Effects:
 
        V2         V3         V4         V5         V6         V7         V8 
0.84159931 0.10894921 0.02036208 0.01106962 0.04927757 0.07478463 0.12504065 
        V9        V10 
0.13294222 0.11060480 

 Quartiles of Marginal Effects:
 
           V2        V3         V4         V5         V6         V7        V8
25% 0.8401146 0.1086648 0.02006933 0.01084152 0.04907718 0.07460124 0.1248156
50% 0.8422324 0.1089926 0.02029535 0.01105477 0.04941048 0.07501852 0.1252107
75% 0.8429805 0.1092391 0.02069607 0.01138223 0.04963407 0.07512901 0.1254736
           V9       V10
25% 0.1327702 0.1103207
50% 0.1331451 0.1107454
75% 0.1332944 0.1109869

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 3.26952115  0.02002223  0.12258157  0.04997209 -0.04638037 -0.07543116 
         V8          V9         V10 
 0.02988411  0.08051485 -0.11807514 

 Quartiles of Marginal Effects:
 
          V2         V3        V4         V5          V6          V7         V8
25% 3.245352 0.01608474 0.1153060 0.04679803 -0.05167175 -0.07907495 0.02186080
50% 3.268981 0.02022965 0.1227982 0.05145794 -0.04709086 -0.07547007 0.02923727
75% 3.289073 0.02549606 0.1291107 0.06042296 -0.03826097 -0.07076832 0.03777952
            V9        V10
25% 0.07043954 -0.1251294
50% 0.07711447 -0.1173977
75% 0.08951431 -0.1105589

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 3.175243096  0.026269283  0.087903287  0.049290420 -0.041563496 -0.051679880 
          V8           V9          V10 
 0.003883855  0.063827802 -0.055720634 

 Quartiles of Marginal Effects:
 
          V2          V3         V4         V5          V6          V7
25% 3.076400 0.007530444 0.04893347 0.03628805 -0.06389663 -0.06118636
50% 3.168150 0.025117939 0.08905155 0.05868881 -0.04272503 -0.04852762
75% 3.278031 0.049379354 0.11822897 0.09003275 -0.01073949 -0.02745195
              V8         V9         V10
25% -0.024643077 0.01766316 -0.08341948
50%  0.005220024 0.04883200 -0.05027334
75%  0.030542825 0.09909097 -0.02755290

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.713839632  0.107549136  0.073413554  0.131878879 -0.087374284 -0.177970332 
          V8           V9          V10 
 0.109294101 -0.001738979 -0.109667669 

 Quartiles of Marginal Effects:
 
          V2        V3         V4         V5          V6         V7         V8
25% 2.578763 0.0493994 0.01079831 0.04315991 -0.10734718 -0.2446398 0.05106434
50% 2.792688 0.1030623 0.06094548 0.13148091 -0.08701827 -0.1233251 0.08832516
75% 2.915495 0.1879747 0.13861809 0.22182486 -0.01102417 -0.0535446 0.14285344
             V9         V10
25% -0.05373205 -0.15206622
50% -0.01459330 -0.11162274
75%  0.02971722 -0.06933444

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 3.09136332  0.15062194  0.18349599  0.01430954 -0.13552775 -0.10503231 
         V8          V9         V10 
 0.11461691 -0.06289759 -0.13304622 

 Quartiles of Marginal Effects:
 
          V2        V3        V4           V5         V6          V7         V8
25% 3.029634 0.1309892 0.1634575 -0.008792145 -0.1501774 -0.13559427 0.08255266
50% 3.108034 0.1506820 0.1845093  0.020905500 -0.1311507 -0.10261245 0.12193855
75% 3.155389 0.1601247 0.2090101  0.072787331 -0.1146181 -0.04680934 0.15075730
             V9        V10
25% -0.08240149 -0.1656067
50% -0.06307293 -0.1371794
75% -0.05218059 -0.1057904

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.98560295  0.06571929  0.02678091  0.08620793 -0.05409693 -0.10664399 
         V8          V9         V10 
 0.12584139  0.05372245 -0.07822504 

 Quartiles of Marginal Effects:
 
          V2          V3          V4           V5          V6          V7
25% 1.534731 -0.02202589 -0.11260629 -0.008107287 -0.11959427 -0.20724191
50% 2.119737  0.06044157 -0.01781731  0.097382340 -0.01520376 -0.08284181
75% 2.299137  0.16139706  0.09538655  0.242578473  0.06395090  0.07827217
            V8          V9          V10
25% 0.03514660 -0.03712525 -0.137381945
50% 0.09458398  0.05298760 -0.088397033
75% 0.21016153  0.13912510 -0.007917248

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 3.05045557  0.15301052  0.18161477  0.01473752 -0.13627850 -0.10312266 
         V8          V9         V10 
 0.12038602 -0.05762125 -0.13241214 

 Quartiles of Marginal Effects:
 
          V2        V3        V4          V5         V6          V7         V8
25% 3.000995 0.1349992 0.1619151 -0.01197330 -0.1494889 -0.13183017 0.08612717
50% 3.050604 0.1504429 0.1846307  0.02208139 -0.1332135 -0.10003757 0.12399595
75% 3.124444 0.1616063 0.2033196  0.06626624 -0.1173966 -0.04452094 0.15627726
             V9        V10
25% -0.07487247 -0.1624363
50% -0.05567177 -0.1352931
75% -0.04598101 -0.1052325

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.342707095 -0.002310585 -0.012758192 -0.078449967  0.036596244  0.053277203 
          V8           V9          V10 
 0.065149855  0.016064020  0.052005886 

 Quartiles of Marginal Effects:
 
            V2           V3           V4          V5          V6           V7
25% 0.02296382 -0.047271364 -0.081477463 -0.14878186 -0.02697255 -0.009581911
50% 0.22680955 -0.001855912 -0.005764259 -0.05257701  0.02503021  0.033022144
75% 0.49083755  0.066185462  0.029402047  0.01797574  0.10826992  0.167543341
             V8           V9         V10
25% -0.02997675 -0.056169606 -0.05009479
50%  0.03083064 -0.001512099  0.02701485
75%  0.09748729  0.113763130  0.08817125

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 1.2394637295  0.1970889911  0.0580643393 -0.0008806875 -0.0461013829 
           V7            V8            V9           V10 
 0.0474789536  0.1838405992  0.0619724833  0.0140241653 

 Quartiles of Marginal Effects:
 
          V2        V3         V4            V5          V6         V7
25% 1.236317 0.1966254 0.05759392 -1.459777e-03 -0.04641116 0.04750916
50% 1.241226 0.1972233 0.05799403 -5.178944e-04 -0.04609969 0.04783975
75% 1.242590 0.1978222 0.05844348  3.167134e-05 -0.04554237 0.04811129
           V8         V9        V10
25% 0.1834027 0.06151526 0.01346680
50% 0.1841410 0.06207937 0.01407283
75% 0.1846623 0.06253915 0.01455528

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.598708021  0.100685122  0.067256675  0.129003607 -0.085348198 -0.171289344 
          V8           V9          V10 
 0.115487447  0.008855384 -0.108861239 

 Quartiles of Marginal Effects:
 
          V2         V3          V4         V5           V6          V7
25% 2.360320 0.03403848 -0.02127371 0.05117463 -0.111174922 -0.22364675
50% 2.705108 0.10498046  0.04391446 0.13010831 -0.074534433 -0.13219722
75% 2.841217 0.19309763  0.12968785 0.23217979 -0.007995532 -0.05056906
            V8           V9         V10
25% 0.04397634 -0.051299845 -0.14903026
50% 0.09360469 -0.001130922 -0.11859247
75% 0.14577293  0.043408937 -0.05856628

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.320660501 -0.002753659 -0.012474379 -0.076563060  0.035265979  0.050013263 
          V8           V9          V10 
 0.061998438  0.014935737  0.050573803 

 Quartiles of Marginal Effects:
 
            V2           V3           V4          V5          V6           V7
25% 0.02009799 -0.046476365 -0.079223079 -0.14985911 -0.02815234 -0.007988122
50% 0.20732345 -0.001577599 -0.005750274 -0.04654601  0.02385378  0.033076017
75% 0.45741785  0.061492166  0.029231323  0.01716545  0.10398547  0.155680508
             V8          V9         V10
25% -0.02967036 -0.05467870 -0.04594505
50%  0.02888912 -0.00256799  0.02562703
75%  0.08800941  0.10842007  0.08303323

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.130757627  0.029667841  0.003049249  0.004421311 -0.002547776  0.012495723 
          V8           V9          V10 
 0.025741906  0.011315851  0.009238525 

 Quartiles of Marginal Effects:
 
           V2         V3          V4          V5           V6         V7
25% 0.1305943 0.02963049 0.003027075 0.004381504 -0.002563248 0.01248690
50% 0.1308590 0.02967720 0.003041554 0.004442802 -0.002545025 0.01252054
75% 0.1309251 0.02972866 0.003063698 0.004476933 -0.002515759 0.01254207
            V8         V9         V10
25% 0.02570205 0.01128622 0.009204986
50% 0.02576481 0.01132262 0.009244104
75% 0.02578355 0.01134865 0.009269859

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 1.502286621  0.043888214  0.005208672  0.027245783 -0.018741788 -0.032968295 
          V8           V9          V10 
 0.119233764  0.064711550 -0.036271667 

 Quartiles of Marginal Effects:
 
           V2          V3          V4          V5          V6          V7
25% 0.8965685 -0.05376705 -0.10585674 -0.11557841 -0.09657415 -0.12919978
50% 1.5826590  0.07571945 -0.02294481  0.05901374  0.02453511  0.05236204
75% 1.8070519  0.14325025  0.05147084  0.12809970  0.09422362  0.17443536
            V8          V9         V10
25% 0.01251396 -0.01515426 -0.10475395
50% 0.09315211  0.03705824 -0.04817126
75% 0.24284537  0.15185631  0.04789654

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.84766300  0.11546960  0.08559797  0.13039168 -0.08942014 -0.17884222 
         V8          V9         V10 
 0.10085560 -0.01492697 -0.10984294 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5          V6          V7
25% 2.655191 0.05983437 0.02393311 0.04746854 -0.12937891 -0.23894570
50% 2.884658 0.11429461 0.07130560 0.13293969 -0.07916391 -0.14122412
75% 2.989159 0.19512475 0.14025439 0.20956275 -0.03127680 -0.04391618
            V8           V9         V10
25% 0.04445336 -0.058721438 -0.14622384
50% 0.08323692 -0.035053978 -0.10876786
75% 0.12880012  0.007250087 -0.07443972

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 3.114879596  0.153693237  0.188426789  0.002060531 -0.139510334 -0.096433866 
          V8           V9          V10 
 0.117520335 -0.067015786 -0.132122966 

 Quartiles of Marginal Effects:
 
          V2        V3        V4           V5         V6          V7         V8
25% 3.067754 0.1418081 0.1710388 -0.018402322 -0.1520890 -0.12250867 0.09100717
50% 3.127088 0.1537957 0.1880442  0.007230943 -0.1360005 -0.09340425 0.12473161
75% 3.167547 0.1604836 0.2103346  0.048992819 -0.1214854 -0.04750216 0.14755771
             V9        V10
25% -0.08238311 -0.1602581
50% -0.06512491 -0.1362759
75% -0.05555160 -0.1090628

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.28794271  0.08378501  0.05591306  0.10848755 -0.07673662 -0.13767086 
         V8          V9         V10 
 0.12689721  0.03563247 -0.09806015 

 Quartiles of Marginal Effects:
 
          V2           V3          V4         V5          V6          V7
25% 1.960235 -0.006738512 -0.06695596 0.01595695 -0.12293153 -0.20666758
50% 2.410075  0.066902791  0.03279222 0.09644839 -0.04058219 -0.09695545
75% 2.550607  0.173708619  0.12256614 0.26411180  0.02787087  0.02729797
            V8          V9         V10
25% 0.03828897 -0.04264023 -0.14667004
50% 0.11070731  0.03421930 -0.10934785
75% 0.19931362  0.10025987 -0.03964965

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.81750178  0.11392476  0.08809207  0.12842288 -0.09084354 -0.17562673 
         V8          V9         V10 
 0.10392276 -0.01286170 -0.11178532 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5          V6          V7
25% 2.652343 0.06096458 0.03237198 0.03782499 -0.12537497 -0.23883520
50% 2.875368 0.11055567 0.07908758 0.13125045 -0.08417776 -0.13080662
75% 2.999951 0.18304351 0.13687347 0.20851129 -0.02618396 -0.05283179
            V8          V9         V10
25% 0.05090928 -0.05675577 -0.14721654
50% 0.08247866 -0.03176857 -0.10946577
75% 0.12902892  0.01589889 -0.07930067

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 3.02741048  0.13381171  0.14817247  0.07052891 -0.11520085 -0.14183835 
         V8          V9         V10 
 0.09960689 -0.04550931 -0.12687273 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5          V6          V7
25% 2.895274 0.09882954 0.09178783 0.04699610 -0.14934819 -0.19245283
50% 3.046258 0.12703558 0.14957194 0.07814781 -0.10301283 -0.13259986
75% 3.105354 0.16809904 0.18635615 0.12941735 -0.07673135 -0.05323349
            V8          V9         V10
25% 0.06299856 -0.06941296 -0.17332263
50% 0.10124847 -0.05754140 -0.13172767
75% 0.14235325 -0.02625217 -0.09031189

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.071709287 -0.001698541 -0.002987511 -0.030236142  0.015305569  0.006686369 
          V8           V9          V10 
 0.018517394  0.006763237  0.021739051 

 Quartiles of Marginal Effects:
 
               V2            V3            V4            V5            V6
25% -0.0001261146 -0.0160308013 -2.073883e-02 -0.0396496251 -0.0246756636
50%  0.0200203490 -0.0003102663  5.302045e-06 -0.0004635942 -0.0003318633
75%  0.0878068064  0.0247631006  3.008382e-02  0.0065859598  0.0317103948
               V7            V8           V9          V10
25% -0.0270681997 -5.620715e-03 -0.024280610 -0.003728229
50%  0.0003574534 -2.678046e-05 -0.002462112  0.003537901
75%  0.0156223260  1.457231e-02  0.033193036  0.023122354

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.89310652  0.11849520  0.09869215  0.12346549 -0.09307204 -0.17241038 
         V8          V9         V10 
 0.09904021 -0.02143419 -0.11304921 

 Quartiles of Marginal Effects:
 
          V2         V3         V4         V5          V6          V7
25% 2.713494 0.06686869 0.03312801 0.04264574 -0.13931185 -0.22681264
50% 2.916261 0.11228218 0.08821593 0.12800274 -0.07518753 -0.14503426
75% 3.014848 0.19380487 0.15192044 0.18872784 -0.04152897 -0.04265903
            V8           V9         V10
25% 0.04628008 -0.064687383 -0.15047992
50% 0.08863084 -0.040898903 -0.11298572
75% 0.13687952  0.002796504 -0.08073559

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.894737927  0.160298681  0.035794639  0.006882662 -0.028781901  0.048551885 
          V8           V9          V10 
 0.146835728  0.055238342  0.025080505 

 Quartiles of Marginal Effects:
 
           V2        V3         V4          V5          V6         V7        V8
25% 0.8933360 0.1600588 0.03558527 0.006595100 -0.02892439 0.04858807 0.1466090
50% 0.8955514 0.1603567 0.03575623 0.007021049 -0.02877445 0.04871419 0.1469999
75% 0.8960739 0.1606823 0.03595166 0.007304639 -0.02852613 0.04887696 0.1472144
            V9        V10
25% 0.05502273 0.02480890
50% 0.05529877 0.02512168
75% 0.05551969 0.02537156

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 3.14354324  0.16851616  0.19025282 -0.03958022 -0.15566953 -0.06993505 
         V8          V9         V10 
 0.13842230 -0.06481076 -0.12317272 

 Quartiles of Marginal Effects:
 
          V2        V3        V4          V5         V6          V7        V8
25% 3.132089 0.1649116 0.1875749 -0.04630402 -0.1570200 -0.07772138 0.1332726
50% 3.143895 0.1678348 0.1905252 -0.03694756 -0.1546992 -0.06827601 0.1394365
75% 3.155872 0.1732498 0.1941234 -0.02941623 -0.1523111 -0.05874043 0.1440659
             V9        V10
25% -0.06881368 -0.1293875
50% -0.06465406 -0.1238793
75% -0.05910176 -0.1180054

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 3.076002112  0.159609379  0.187901951 -0.003041869 -0.142021219 -0.089780404 
          V8           V9          V10 
 0.126961257 -0.062110848 -0.130606164 

 Quartiles of Marginal Effects:
 
          V2        V3        V4           V5         V6          V7        V8
25% 3.032139 0.1511563 0.1740670 -0.018856760 -0.1514608 -0.11189875 0.1007901
50% 3.076727 0.1575674 0.1885484  0.002133099 -0.1385221 -0.08649780 0.1299352
75% 3.132789 0.1665882 0.2056771  0.040719295 -0.1277956 -0.04568746 0.1536966
             V9        V10
25% -0.07480039 -0.1525144
50% -0.05887659 -0.1346394
75% -0.04997890 -0.1094757

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.804336016  0.019921832  0.005032309  0.052153171 -0.024526373 -0.110209603 
          V8           V9          V10 
-0.019871180  0.073898867  0.001063268 

 Quartiles of Marginal Effects:
 
          V2          V3          V4          V5          V6          V7
25% 2.544157 -0.05035386 -0.08469938 -0.01490533 -0.07000661 -0.15884779
50% 2.751705  0.02194484  0.01269404  0.05989371 -0.02397718 -0.09682819
75% 3.007848  0.07566952  0.06230595  0.13209170  0.02855358 -0.04230848
              V8         V9          V10
25% -0.060645857 0.01848899 -0.076321858
50% -0.009164784 0.04487533  0.003749631
75%  0.017242528 0.11010557  0.074974732
Radial Basis Function Kernel Regularized Least Squares 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  lambda        sigma        RMSE       Rsquared    MAE        Selected
  1.835422e-05    68.277552  0.4565052  0.96741580  0.2989576          
  2.473241e-05   153.774865  0.4210611  0.97113135  0.2794018          
  3.066202e-05  1000.702492  0.4236012  0.96859363  0.2816611          
  6.641089e-05   211.196512  0.4184080  0.97116806  0.2759377  *       
  7.028745e-05  1824.237417  0.4401094  0.96284761  0.2992095          
  7.185817e-05  5525.554472  0.4670216  0.95787285  0.3225813          
  1.123723e-04   114.120309  0.4259961  0.97039928  0.2795541          
  1.317798e-04  1090.295540  0.4351592  0.96390417  0.2945865          
  1.892403e-04    44.841861  0.5186021  0.96061568  0.3382454          
  2.361780e-04     2.826155  1.9432039  0.39860573  1.2396083          
  7.118972e-04    14.911562  0.9065614  0.89281527  0.5731872          
  7.698721e-04     9.072144  1.1975882  0.81514340  0.7416464          
  7.739590e-04   616.587515  0.4411952  0.96162983  0.2950085          
  8.146397e-04   441.185047  0.4332177  0.96339447  0.2907153          
  3.561995e-03    22.708814  0.7211598  0.93093242  0.4654839          
  3.074698e-02     2.768961  1.9555685  0.38481590  1.2532766          
  4.672468e-02  1696.517104  1.3364442  0.87181779  0.8714833          
  4.896607e-02  2711.628196  1.5400949  0.84957001  1.0219178          
  7.837967e-02     1.402244  2.0950730  0.08979626  1.4178186          
  3.821438e-01  3323.293006  2.0330209  0.79716638  1.3912252          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 6.641089e-05 and sigma
 = 211.1965.
[1] "Sat Mar 10 04:18:56 2018"
Least Angle Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  fraction    RMSE       Rsquared   MAE        Selected
  0.05274719  1.9861792  0.9802501  1.3586970          
  0.07865330  1.9183319  0.9802501  1.3066558          
  0.09732015  1.8694837  0.9802501  1.2691572          
  0.16444786  1.6941443  0.9802501  1.1343087          
  0.16937555  1.6812959  0.9802501  1.1244098          
  0.17129523  1.6762915  0.9802501  1.1205535          
  0.21013188  1.5751711  0.9802501  1.0425370          
  0.22396974  1.5392029  0.9802501  1.0147391          
  0.25540271  1.4576408  0.9802501  0.9522037          
  0.27464788  1.4078117  0.9802501  0.9147269          
  0.37048346  1.1613345  0.9802501  0.7343565          
  0.37728372  1.1439817  0.9802501  0.7217886          
  0.37774359  1.1428090  0.9802501  0.7209387          
  0.38219311  1.1314683  0.9802501  0.7127153          
  0.51033865  0.8113262  0.9802501  0.4905992          
  0.69756051  0.4384839  0.9800580  0.2536718          
  0.73390926  0.4051137  0.9796846  0.2372113          
  0.73797904  0.4021668  0.9796446  0.2366370          
  0.77884069  0.3883733  0.9781281  0.2527151  *       
  0.91644537  0.4283576  0.9673394  0.2990328          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.7788407.
[1] "Sat Mar 10 04:19:06 2018"
Least Angle Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  step  RMSE       Rsquared   MAE        Selected
  1     2.1244963        NaN  1.4646574          
  2     0.3736083  0.9802501  0.2382696          
  3     0.3734498  0.9800060  0.2409500          
  4     0.3680745  0.9796264  0.2430563  *       
  5     0.3688815  0.9788555  0.2490940          
  7     0.3877770  0.9753971  0.2694939          
  8     0.4001448  0.9733438  0.2803080          
  9     0.4232867  0.9690822  0.2946861          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was step = 4.
[1] "Sat Mar 10 04:19:16 2018"
The lasso 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  fraction    RMSE       Rsquared   MAE        Selected
  0.05274719  1.9861792  0.9802501  1.3586970          
  0.07865330  1.9183319  0.9802501  1.3066558          
  0.09732015  1.8694837  0.9802501  1.2691572          
  0.16444786  1.6941443  0.9802501  1.1343087          
  0.16937555  1.6812959  0.9802501  1.1244098          
  0.17129523  1.6762915  0.9802501  1.1205535          
  0.21013188  1.5751711  0.9802501  1.0425370          
  0.22396974  1.5392029  0.9802501  1.0147391          
  0.25540271  1.4576408  0.9802501  0.9522037          
  0.27464788  1.4078117  0.9802501  0.9147269          
  0.37048346  1.1613345  0.9802501  0.7343565          
  0.37728372  1.1439817  0.9802501  0.7217886          
  0.37774359  1.1428090  0.9802501  0.7209387          
  0.38219311  1.1314683  0.9802501  0.7127153          
  0.51033865  0.8113262  0.9802501  0.4905992          
  0.69756051  0.4384839  0.9800580  0.2536718          
  0.73390926  0.4051137  0.9796846  0.2372113          
  0.73797904  0.4021668  0.9796446  0.2366370          
  0.77884069  0.3883733  0.9781281  0.2527151  *       
  0.91644537  0.4283576  0.9673394  0.2990328          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.7788407.
[1] "Sat Mar 10 04:19:25 2018"
Linear Regression with Backwards Selection 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  nvmax  RMSE       Rsquared   MAE        Selected
  2      0.3814945  0.9765211  0.2697658  *       
  3      0.3887651  0.9757547  0.2675142          
  4      0.4155994  0.9707902  0.2844011          
  5      0.4419619  0.9664883  0.3085891          
  6      0.4586734  0.9619713  0.3257991          
  7      0.4728369  0.9586096  0.3318452          
  8      0.4768949  0.9571014  0.3361768          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sat Mar 10 04:19:34 2018"
Linear Regression with Forward Selection 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  nvmax  RMSE       Rsquared   MAE        Selected
  2      0.3794299  0.9769175  0.2727525  *       
  3      0.3838767  0.9766613  0.2721786          
  4      0.4241717  0.9691799  0.2954770          
  5      0.4638878  0.9618595  0.3312711          
  6      0.4748771  0.9586983  0.3426127          
  7      0.4786339  0.9570023  0.3472544          
  8      0.4794818  0.9564507  0.3419859          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sat Mar 10 04:19:44 2018"
Linear Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results:

  RMSE       Rsquared   MAE      
  0.4777437  0.9566545  0.3376107

Tuning parameter 'intercept' was held constant at a value of TRUE
[1] "Sat Mar 10 04:19:53 2018"
Start:  AIC=-64.02
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V9    1     0.001  1.028 -65.990
- V6    1     0.012  1.039 -65.728
- V4    1     0.016  1.043 -65.626
- V8    1     0.030  1.057 -65.272
- V10   1     0.039  1.066 -65.058
- V7    1     0.059  1.086 -64.570
<none>               1.027 -64.023
- V3    1     0.128  1.154 -62.976
- V5    1     0.151  1.178 -62.449
- V2    1    75.109 76.136  45.935

Step:  AIC=-65.99
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V10

       Df Sum of Sq    RSS     AIC
- V6    1     0.013  1.041 -67.657
- V4    1     0.018  1.046 -67.550
- V8    1     0.030  1.058 -67.252
- V10   1     0.040  1.068 -67.007
- V7    1     0.064  1.092 -66.422
<none>               1.028 -65.990
- V3    1     0.130  1.158 -64.897
- V5    1     0.179  1.207 -63.825
- V2    1    75.813 76.841  44.175

Step:  AIC=-67.66
.outcome ~ V2 + V3 + V4 + V5 + V7 + V8 + V10

       Df Sum of Sq    RSS     AIC
- V4    1     0.017  1.058 -69.237
- V8    1     0.051  1.092 -68.426
- V10   1     0.055  1.096 -68.319
- V7    1     0.077  1.119 -67.794
<none>               1.041 -67.657
- V3    1     0.163  1.204 -65.878
- V5    1     0.171  1.212 -65.710
- V2    1    76.132 77.173  42.287

Step:  AIC=-69.24
.outcome ~ V2 + V3 + V5 + V7 + V8 + V10

       Df Sum of Sq    RSS     AIC
- V10   1     0.041  1.099 -70.257
- V7    1     0.062  1.121 -69.747
- V8    1     0.075  1.134 -69.449
<none>               1.058 -69.237
- V3    1     0.155  1.213 -67.684
- V5    1     0.165  1.224 -67.465
- V2    1    76.553 77.611  40.434

Step:  AIC=-70.26
.outcome ~ V2 + V3 + V5 + V7 + V8

       Df Sum of Sq    RSS     AIC
- V7    1     0.041  1.140 -71.315
- V8    1     0.080  1.179 -70.421
<none>               1.099 -70.257
- V5    1     0.127  1.226 -69.413
- V3    1     0.136  1.235 -69.232
- V2    1    76.529 77.628  38.440

Step:  AIC=-71.31
.outcome ~ V2 + V3 + V5 + V8

       Df Sum of Sq    RSS     AIC
- V8    1     0.053  1.192 -72.141
<none>               1.140 -71.315
- V5    1     0.134  1.273 -70.433
- V3    1     0.137  1.277 -70.362
- V2    1    80.192 81.332  37.651

Step:  AIC=-72.14
.outcome ~ V2 + V3 + V5

       Df Sum of Sq    RSS     AIC
<none>               1.192 -72.141
- V3    1     0.206  1.399 -69.989
- V5    1     0.298  1.490 -68.338
- V2    1    84.387 85.579  36.975
Start:  AIC=-36.87
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V3    1     0.003  2.921 -38.839
- V8    1     0.013  2.931 -38.753
- V6    1     0.026  2.944 -38.633
- V5    1     0.033  2.952 -38.570
- V9    1     0.072  2.990 -38.230
- V7    1     0.093  3.012 -38.047
- V10   1     0.139  3.057 -37.660
- V4    1     0.165  3.083 -37.436
<none>               2.918 -36.866
- V2    1    91.688 94.606  51.582

Step:  AIC=-38.84
.outcome ~ V2 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq     RSS     AIC
- V8    1     0.010   2.931 -40.752
- V5    1     0.031   2.952 -40.568
- V6    1     0.037   2.958 -40.515
- V9    1     0.072   2.993 -40.204
- V7    1     0.101   3.022 -39.956
- V10   1     0.142   3.063 -39.609
- V4    1     0.174   3.095 -39.334
<none>                2.921 -38.839
- V2    1   104.439 107.360  52.870

Step:  AIC=-40.75
.outcome ~ V2 + V4 + V5 + V6 + V7 + V9 + V10

       Df Sum of Sq     RSS     AIC
- V5    1     0.025   2.956 -42.530
- V6    1     0.054   2.985 -42.281
- V9    1     0.069   3.000 -42.144
- V7    1     0.100   3.031 -41.878
- V10   1     0.140   3.071 -41.539
- V4    1     0.182   3.113 -41.188
<none>                2.931 -40.752
- V2    1   109.182 112.113  51.997

Step:  AIC=-42.53
.outcome ~ V2 + V4 + V6 + V7 + V9 + V10

       Df Sum of Sq     RSS     AIC
- V6    1     0.042   2.998 -44.164
- V9    1     0.100   3.056 -43.665
- V7    1     0.104   3.060 -43.628
- V10   1     0.140   3.096 -43.324
- V4    1     0.179   3.135 -43.003
<none>                2.956 -42.530
- V2    1   109.757 112.713  50.135

Step:  AIC=-44.16
.outcome ~ V2 + V4 + V7 + V9 + V10

       Df Sum of Sq     RSS     AIC
- V7    1     0.082   3.080 -45.461
- V9    1     0.117   3.115 -45.166
- V10   1     0.185   3.183 -44.609
- V4    1     0.195   3.193 -44.527
<none>                2.998 -44.164
- V2    1   110.249 113.247  48.258

Step:  AIC=-45.46
.outcome ~ V2 + V4 + V9 + V10

       Df Sum of Sq     RSS     AIC
- V9    1     0.066   3.146 -46.912
- V10   1     0.136   3.216 -46.342
- V4    1     0.219   3.299 -45.674
<none>                3.080 -45.461
- V2    1   111.471 114.551  46.556

Step:  AIC=-46.91
.outcome ~ V2 + V4 + V10

       Df Sum of Sq     RSS     AIC
- V10   1     0.086   3.232 -48.210
- V4    1     0.159   3.305 -47.630
<none>                3.146 -46.912
- V2    1   117.000 120.146  45.796

Step:  AIC=-48.21
.outcome ~ V2 + V4

       Df Sum of Sq     RSS     AIC
- V4    1     0.119   3.351 -49.272
<none>                3.232 -48.210
- V2    1   126.570 129.802  45.806

Step:  AIC=-49.27
.outcome ~ V2

       Df Sum of Sq     RSS     AIC
<none>                3.351 -49.272
- V2    1    126.54 129.892  43.824
Start:  AIC=-51.48
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq     RSS     AIC
- V5    1     0.017   2.196 -53.271
- V7    1     0.034   2.214 -53.049
- V9    1     0.044   2.223 -52.929
<none>                2.180 -51.484
- V10   1     0.163   2.343 -51.459
- V8    1     0.226   2.406 -50.717
- V3    1     0.389   2.569 -48.883
- V4    1     0.465   2.645 -48.071
- V6    1     0.522   2.702 -47.471
- V2    1   100.420 102.600  54.362

Step:  AIC=-53.27
.outcome ~ V2 + V3 + V4 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq     RSS     AIC
- V7    1     0.076   2.272 -54.324
- V9    1     0.089   2.286 -54.154
<none>                2.196 -53.271
- V10   1     0.280   2.476 -51.913
- V8    1     0.298   2.495 -51.707
- V3    1     0.427   2.624 -50.293
- V4    1     0.450   2.647 -50.047
- V6    1     0.514   2.710 -49.385
- V2    1   100.577 102.773  52.409

Step:  AIC=-54.32
.outcome ~ V2 + V3 + V4 + V6 + V8 + V9 + V10

       Df Sum of Sq     RSS     AIC
- V9    1     0.080   2.352 -55.359
<none>                2.272 -54.324
- V10   1     0.235   2.507 -53.563
- V8    1     0.335   2.607 -52.468
- V3    1     0.352   2.624 -52.291
- V6    1     0.630   2.902 -49.474
- V4    1     0.690   2.962 -48.898
- V2    1   100.722 102.994  50.469

Step:  AIC=-55.36
.outcome ~ V2 + V3 + V4 + V6 + V8 + V10

       Df Sum of Sq     RSS     AIC
<none>                2.352 -55.359
- V10   1     0.232   2.583 -54.726
- V3    1     0.307   2.659 -53.921
- V8    1     0.331   2.682 -53.675
- V6    1     0.569   2.921 -51.292
- V4    1     0.611   2.963 -50.890
- V2    1   100.666 103.018  48.476
Start:  AIC=-73.67
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq     RSS     AIC
- V9    1     0.003   3.850 -75.636
- V7    1     0.018   3.864 -75.486
- V8    1     0.021   3.867 -75.453
- V5    1     0.037   3.883 -75.287
- V10   1     0.071   3.917 -74.940
- V3    1     0.102   3.948 -74.624
- V4    1     0.154   4.000 -74.099
<none>                3.846 -73.669
- V6    1     0.208   4.055 -73.559
- V2    1   145.191 149.038  70.613

Step:  AIC=-75.64
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V10

       Df Sum of Sq     RSS     AIC
- V7    1     0.016   3.866 -77.469
- V8    1     0.021   3.871 -77.418
- V5    1     0.048   3.898 -77.141
- V10   1     0.069   3.918 -76.929
- V3    1     0.102   3.952 -76.587
- V4    1     0.157   4.006 -76.040
<none>                3.850 -75.636
- V6    1     0.225   4.075 -75.360
- V2    1   147.751 151.600  69.295

Step:  AIC=-77.47
.outcome ~ V2 + V3 + V4 + V5 + V6 + V8 + V10

       Df Sum of Sq     RSS     AIC
- V8    1     0.018   3.884 -79.279
- V5    1     0.043   3.908 -79.029
- V10   1     0.068   3.934 -78.767
- V3    1     0.099   3.965 -78.458
<none>                3.866 -77.469
- V4    1     0.214   4.080 -77.314
- V6    1     0.218   4.084 -77.274
- V2    1   149.419 153.285  67.737

Step:  AIC=-79.28
.outcome ~ V2 + V3 + V4 + V5 + V6 + V10

       Df Sum of Sq     RSS     AIC
- V5    1     0.027   3.911 -81.000
- V10   1     0.052   3.936 -80.750
- V3    1     0.084   3.968 -80.423
- V4    1     0.196   4.080 -79.314
<none>                3.884 -79.279
- V6    1     0.205   4.090 -79.218
- V2    1   160.452 164.336  68.521

Step:  AIC=-81
.outcome ~ V2 + V3 + V4 + V6 + V10

       Df Sum of Sq     RSS     AIC
- V10   1     0.037   3.949 -82.620
- V3    1     0.079   3.991 -82.196
<none>                3.911 -81.000
- V6    1     0.211   4.123 -80.896
- V4    1     0.222   4.134 -80.788
- V2    1   161.100 165.011  66.685

Step:  AIC=-82.62
.outcome ~ V2 + V3 + V4 + V6

       Df Sum of Sq     RSS     AIC
- V3    1     0.059   4.008 -84.023
- V4    1     0.191   4.140 -82.730
<none>                3.949 -82.620
- V6    1     0.277   4.226 -81.905
- V2    1   163.368 167.317  65.240

Step:  AIC=-84.02
.outcome ~ V2 + V4 + V6

       Df Sum of Sq     RSS     AIC
- V4    1     0.203   4.211 -84.047
<none>                4.008 -84.023
- V6    1     0.359   4.367 -82.590
- V2    1   180.023 184.031  67.049

Step:  AIC=-84.05
.outcome ~ V2 + V6

       Df Sum of Sq     RSS     AIC
<none>                4.211 -84.047
- V6    1     0.327   4.538 -83.059
- V2    1   179.954 184.165  65.078
Linear Regression with Stepwise Selection 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results:

  RMSE       Rsquared   MAE      
  0.4368565  0.9656231  0.3073659

[1] "Sat Mar 10 04:20:03 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:20:30 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "logreg"                         
Error : package RWeka is required
Error : package RWeka is required
Error : package RWeka is required
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:20:39 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "M5"                             
Error : package RWeka is required
Error : package RWeka is required
Error : package RWeka is required
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:20:48 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "M5Rules"                        
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:20:57 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "mlpKerasDecay"                  
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:21:06 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "mlpKerasDropout"                
Multi-Step Adaptive MCP-Net 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  alphas      nsteps  scale      RMSE       Rsquared   MAE        Selected
  0.09747247   6      3.1620379  0.3564089  0.9802501  0.2512137  *       
  0.12078797   6      0.5359085  0.3601391  0.9799120  0.2562956          
  0.13758814   4      3.3473307  0.3572645  0.9802501  0.2542573          
  0.19800308   5      0.9950388  0.3581787  0.9802501  0.2565515          
  0.20243800   3      0.8596041  0.3582305  0.9802501  0.2566755          
  0.20416571   2      2.4614431  0.3582503  0.9802501  0.2567224          
  0.23911869   6      3.1872762  0.3586017  0.9802501  0.2575269          
  0.25157277   4      1.6058046  0.3587080  0.9802501  0.2577600          
  0.27986244   7      2.3112602  0.3589202  0.9802501  0.2582130          
  0.29718309   9      1.9230966  0.3590333  0.9802501  0.2584481          
  0.38343511   8      2.6324664  0.3594621  0.9802501  0.2593042          
  0.38955535   8      0.3271232  0.3594861  0.9802501  0.2593506          
  0.38996923   4      3.1475640  0.3594877  0.9802501  0.2593537          
  0.39397380   5      0.3015483  0.3595030  0.9802501  0.2593833          
  0.50930479   7      1.4163698  0.3598491  0.9802501  0.2600354          
  0.67780446  10      0.6317146  0.3601557  0.9802501  0.2605905          
  0.71051834   3      1.9363852  0.3601993  0.9802501  0.2606678          
  0.71418114   3      1.9765346  0.3602039  0.9802501  0.2606760          
  0.75095662  10      3.2679210  0.3602482  0.9802501  0.2607541          
  0.87480084   3      2.5360443  0.3603710  0.9802501  0.2609690          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alphas = 0.09747247, nsteps = 6
 and scale = 3.162038.
[1] "Sat Mar 10 04:21:21 2018"
# weights:  23
initial  value 160.472302 
iter  10 value 136.595383
iter  20 value 136.197976
iter  30 value 135.916995
iter  40 value 135.433092
iter  50 value 135.302652
iter  60 value 135.269497
iter  70 value 135.267180
final  value 135.267135 
converged
# weights:  56
initial  value 179.603239 
iter  10 value 136.067883
iter  20 value 136.059407
iter  30 value 136.058105
iter  40 value 136.056571
iter  50 value 136.053739
iter  60 value 136.047473
iter  70 value 136.029997
iter  80 value 135.965014
iter  90 value 135.573694
iter 100 value 134.997243
final  value 134.997243 
stopped after 100 iterations
# weights:  89
initial  value 180.481707 
iter  10 value 137.590271
final  value 137.563129 
converged
# weights:  89
initial  value 194.598934 
iter  10 value 136.113814
iter  20 value 136.075322
iter  30 value 135.786541
iter  40 value 135.083987
iter  50 value 135.058993
iter  60 value 135.048491
iter  70 value 135.046961
iter  80 value 135.046536
iter  90 value 135.046300
iter 100 value 135.046194
final  value 135.046194 
stopped after 100 iterations
# weights:  67
initial  value 184.506637 
iter  10 value 145.391501
final  value 145.391116 
converged
# weights:  166
initial  value 195.547011 
iter  10 value 136.060550
iter  20 value 136.052682
iter  30 value 136.051488
iter  40 value 135.363782
iter  50 value 134.984429
iter  60 value 134.981363
iter  70 value 134.980880
final  value 134.980742 
converged
# weights:  67
initial  value 197.512062 
iter  10 value 137.385214
iter  20 value 136.516606
iter  30 value 136.183984
iter  40 value 136.179833
iter  50 value 136.179575
iter  60 value 136.179067
final  value 136.179064 
converged
# weights:  155
initial  value 198.027280 
iter  10 value 143.080016
iter  20 value 143.043486
final  value 143.043410 
converged
# weights:  210
initial  value 167.805687 
iter  10 value 136.053751
final  value 136.050652 
converged
# weights:  89
initial  value 205.948139 
iter  10 value 138.673931
final  value 138.673926 
converged
# weights:  23
initial  value 181.406560 
iter  10 value 136.317506
iter  20 value 136.190112
iter  30 value 135.429371
iter  40 value 135.321588
iter  50 value 134.542086
iter  60 value 134.509255
iter  70 value 134.508035
iter  80 value 134.507840
iter  90 value 134.507801
iter  90 value 134.507800
iter  90 value 134.507800
final  value 134.507800 
converged
# weights:  45
initial  value 190.680858 
iter  10 value 136.058863
iter  20 value 136.053683
iter  30 value 136.051034
iter  40 value 136.047257
iter  50 value 136.038852
iter  60 value 136.016332
iter  70 value 135.939277
iter  80 value 135.489302
iter  90 value 134.987298
iter 100 value 134.982095
final  value 134.982095 
stopped after 100 iterations
# weights:  122
initial  value 172.537501 
iter  10 value 136.922083
iter  20 value 136.871961
final  value 136.871959 
converged
# weights:  56
initial  value 169.420365 
iter  10 value 136.399083
iter  20 value 136.208892
iter  30 value 135.451008
iter  40 value 135.408318
iter  50 value 135.407638
iter  60 value 135.407439
final  value 135.407413 
converged
# weights:  23
initial  value 168.575079 
iter  10 value 136.064480
iter  20 value 136.064288
iter  30 value 136.064171
iter  40 value 136.064041
iter  50 value 136.063892
iter  60 value 136.063714
iter  70 value 136.063496
iter  80 value 136.063216
iter  90 value 136.062846
iter 100 value 136.062332
final  value 136.062332 
stopped after 100 iterations
# weights:  177
initial  value 361.316782 
iter  10 value 148.185267
iter  20 value 148.065758
final  value 148.065741 
converged
# weights:  45
initial  value 153.950108 
iter  10 value 136.151053
iter  20 value 136.123754
iter  30 value 136.081184
iter  40 value 135.764888
iter  50 value 135.209464
iter  60 value 135.185725
iter  70 value 135.184280
iter  80 value 135.183695
iter  90 value 135.183583
final  value 135.183562 
converged
# weights:  166
initial  value 194.090618 
iter  10 value 136.055190
iter  20 value 136.051299
iter  30 value 136.044183
iter  40 value 135.017334
iter  50 value 134.994022
iter  60 value 134.989049
iter  70 value 134.984540
iter  80 value 134.982440
iter  90 value 134.979261
iter 100 value 134.976926
final  value 134.976926 
stopped after 100 iterations
# weights:  45
initial  value 194.188428 
iter  10 value 136.053242
iter  20 value 136.051021
final  value 136.050845 
converged
# weights:  89
initial  value 180.628768 
iter  10 value 136.093459
iter  20 value 136.064738
iter  30 value 135.514866
iter  40 value 135.034052
iter  50 value 135.024287
iter  60 value 135.018958
iter  70 value 135.018415
final  value 135.018323 
converged
# weights:  23
initial  value 192.495582 
iter  10 value 161.167327
iter  20 value 160.691432
iter  30 value 159.721303
iter  40 value 159.711725
iter  50 value 159.711313
final  value 159.711303 
converged
# weights:  56
initial  value 186.505315 
iter  10 value 160.395880
iter  20 value 160.382034
iter  30 value 160.349853
iter  40 value 160.064612
iter  50 value 158.633718
iter  60 value 158.513436
iter  70 value 158.508273
iter  80 value 158.504677
iter  90 value 158.503970
iter 100 value 158.503719
final  value 158.503719 
stopped after 100 iterations
# weights:  89
initial  value 200.276454 
iter  10 value 161.875333
final  value 161.788101 
converged
# weights:  89
initial  value 191.534381 
iter  10 value 160.438480
iter  20 value 160.402059
iter  30 value 160.053540
iter  40 value 158.709424
iter  50 value 158.618167
iter  60 value 158.600760
iter  70 value 158.589195
iter  80 value 158.586122
iter  90 value 158.581795
iter 100 value 158.580110
final  value 158.580110 
stopped after 100 iterations
# weights:  67
initial  value 209.810413 
iter  10 value 169.136139
final  value 169.129411 
converged
# weights:  166
initial  value 216.061312 
iter  10 value 160.391301
iter  20 value 160.384541
iter  30 value 160.382786
iter  40 value 160.382675
iter  50 value 160.382544
iter  60 value 160.382389
iter  70 value 160.382201
iter  80 value 160.381965
iter  90 value 160.381662
iter 100 value 160.381254
final  value 160.381254 
stopped after 100 iterations
# weights:  67
initial  value 214.275350 
iter  10 value 161.768189
iter  20 value 160.807090
iter  30 value 160.158436
iter  40 value 160.150432
iter  50 value 160.150060
iter  50 value 160.150058
iter  50 value 160.150058
final  value 160.150058 
converged
# weights:  155
initial  value 225.352558 
iter  10 value 166.992391
iter  20 value 166.984999
final  value 166.984929 
converged
# weights:  210
initial  value 216.657148 
iter  10 value 160.385168
final  value 160.382159 
converged
# weights:  89
initial  value 187.808647 
iter  10 value 162.829804
final  value 162.829705 
converged
# weights:  23
initial  value 214.086535 
iter  10 value 160.736670
iter  20 value 160.459924
iter  30 value 159.258731
iter  40 value 159.057768
iter  50 value 159.019176
iter  60 value 159.005077
iter  70 value 159.000382
iter  80 value 158.999576
final  value 158.999526 
converged
# weights:  45
initial  value 196.089368 
iter  10 value 160.387504
iter  20 value 160.385759
iter  30 value 160.385671
iter  40 value 160.385570
iter  50 value 160.385453
iter  60 value 160.385316
iter  70 value 160.385150
iter  80 value 160.384947
iter  90 value 160.384690
iter 100 value 160.384355
final  value 160.384355 
stopped after 100 iterations
# weights:  122
initial  value 174.875337 
iter  10 value 161.154324
final  value 161.145625 
converged
# weights:  56
initial  value 191.594760 
iter  10 value 160.770430
iter  20 value 160.526552
iter  30 value 159.332648
iter  40 value 159.160291
iter  50 value 159.148588
iter  60 value 159.145513
iter  70 value 159.143217
iter  80 value 159.141680
iter  90 value 159.141228
iter 100 value 159.141130
final  value 159.141130 
stopped after 100 iterations
# weights:  23
initial  value 183.986115 
iter  10 value 160.397173
iter  20 value 160.375596
iter  30 value 160.354847
iter  40 value 160.301801
iter  50 value 159.972175
iter  60 value 158.600080
iter  70 value 158.522335
iter  80 value 158.518425
iter  90 value 158.515044
iter 100 value 158.513740
final  value 158.513740 
stopped after 100 iterations
# weights:  177
initial  value 383.678463 
iter  10 value 171.789072
iter  20 value 171.734708
final  value 171.734688 
converged
# weights:  45
initial  value 207.296117 
iter  10 value 160.559345
iter  20 value 160.446723
iter  30 value 160.071991
iter  40 value 158.868574
iter  50 value 158.821323
iter  60 value 158.810618
iter  70 value 158.809213
iter  80 value 158.807608
iter  90 value 158.806290
iter 100 value 158.806120
final  value 158.806120 
stopped after 100 iterations
# weights:  166
initial  value 220.266915 
iter  10 value 160.385290
iter  20 value 160.382330
iter  30 value 160.370074
iter  40 value 160.340276
iter  50 value 159.749182
iter  60 value 158.520236
iter  70 value 158.476310
iter  80 value 158.474504
iter  90 value 158.473666
iter 100 value 158.473147
final  value 158.473147 
stopped after 100 iterations
# weights:  45
initial  value 197.595547 
iter  10 value 160.381997
final  value 160.381992 
converged
# weights:  89
initial  value 221.699792 
iter  10 value 160.428272
iter  20 value 160.397088
iter  30 value 160.380615
iter  40 value 160.302102
iter  50 value 159.208922
iter  60 value 158.587891
iter  70 value 158.558699
iter  80 value 158.548186
iter  90 value 158.542738
iter 100 value 158.538907
final  value 158.538907 
stopped after 100 iterations
# weights:  23
initial  value 230.142958 
iter  10 value 181.692859
iter  20 value 181.330751
iter  30 value 180.723539
iter  40 value 180.578191
iter  50 value 180.574123
final  value 180.574105 
converged
# weights:  56
initial  value 244.639765 
iter  10 value 180.939613
iter  20 value 180.930006
iter  30 value 180.926352
iter  40 value 180.925451
iter  50 value 180.924117
iter  60 value 180.921964
iter  70 value 180.918086
iter  80 value 180.910039
iter  90 value 180.889547
iter 100 value 180.808152
final  value 180.808152 
stopped after 100 iterations
# weights:  89
initial  value 220.307145 
iter  10 value 182.535234
final  value 182.427788 
converged
# weights:  89
initial  value 222.246281 
iter  10 value 181.017640
iter  20 value 180.942097
iter  30 value 180.697714
iter  40 value 179.671328
iter  50 value 179.562597
iter  60 value 179.450566
iter  70 value 179.434472
iter  80 value 179.428824
iter  90 value 179.427576
iter 100 value 179.426807
final  value 179.426807 
stopped after 100 iterations
# weights:  67
initial  value 241.833761 
iter  10 value 190.435012
iter  20 value 190.425015
final  value 190.425002 
converged
# weights:  166
initial  value 237.208349 
iter  10 value 180.938695
iter  20 value 180.921134
iter  30 value 180.919767
iter  40 value 180.901029
iter  50 value 179.686776
iter  60 value 179.487872
iter  70 value 179.459826
iter  80 value 179.347930
iter  90 value 179.337058
iter 100 value 179.334101
final  value 179.334101 
stopped after 100 iterations
# weights:  67
initial  value 243.332686 
iter  10 value 182.502089
iter  20 value 181.383243
iter  30 value 181.373518
iter  40 value 181.373437
iter  40 value 181.373435
iter  40 value 181.373435
final  value 181.373435 
converged
# weights:  155
initial  value 276.952878 
iter  10 value 188.073299
iter  20 value 188.048068
final  value 188.048041 
converged
# weights:  210
initial  value 221.866528 
iter  10 value 180.925273
final  value 180.918875 
converged
# weights:  89
initial  value 235.178369 
iter  10 value 183.548714
final  value 183.548537 
converged
# weights:  23
initial  value 226.639624 
iter  10 value 181.175514
iter  20 value 181.077322
iter  30 value 180.945437
iter  40 value 180.103501
iter  50 value 179.833258
iter  60 value 179.820652
iter  70 value 179.816975
iter  80 value 179.815463
iter  90 value 179.815324
final  value 179.815310 
converged
# weights:  45
initial  value 201.972420 
iter  10 value 180.923379
iter  10 value 180.923379
iter  20 value 180.922478
iter  30 value 180.922057
iter  40 value 180.921883
iter  50 value 180.921681
iter  60 value 180.921442
iter  70 value 180.921155
iter  80 value 180.920799
iter  90 value 180.920344
iter 100 value 180.919739
final  value 180.919739 
stopped after 100 iterations
# weights:  122
initial  value 228.918459 
iter  10 value 181.828978
iter  20 value 181.736674
final  value 181.736668 
converged
# weights:  56
initial  value 216.403894 
iter  10 value 181.432176
iter  20 value 181.081710
iter  30 value 180.198623
iter  40 value 180.003034
iter  50 value 179.990843
iter  60 value 179.990284
iter  70 value 179.989835
iter  80 value 179.989665
iter  80 value 179.989664
iter  80 value 179.989664
final  value 179.989664 
converged
# weights:  23
initial  value 235.620149 
iter  10 value 180.944518
iter  20 value 180.937371
iter  30 value 180.930905
iter  40 value 180.929370
iter  50 value 180.927646
iter  60 value 180.925647
iter  70 value 180.923248
iter  80 value 180.920228
iter  90 value 180.916140
iter 100 value 180.909972
final  value 180.909972 
stopped after 100 iterations
# weights:  177
initial  value 388.554636 
iter  10 value 193.451491
iter  20 value 193.290209
final  value 193.289339 
converged
# weights:  45
initial  value 225.404460 
iter  10 value 181.096855
iter  20 value 180.996944
iter  30 value 179.903460
iter  40 value 179.702893
iter  50 value 179.634496
iter  60 value 179.629545
iter  70 value 179.628620
iter  80 value 179.628039
iter  90 value 179.627804
iter 100 value 179.627652
final  value 179.627652 
stopped after 100 iterations
# weights:  166
initial  value 235.987813 
iter  10 value 180.924141
iter  20 value 180.919800
iter  30 value 180.918432
iter  40 value 180.790126
iter  50 value 179.739951
iter  60 value 179.444561
iter  70 value 179.430038
iter  80 value 179.426140
iter  90 value 179.317082
iter 100 value 179.302104
final  value 179.302104 
stopped after 100 iterations
# weights:  45
initial  value 202.843784 
final  value 180.931612 
converged
# weights:  89
initial  value 236.851378 
iter  10 value 180.967921
iter  20 value 180.934604
iter  30 value 180.929124
iter  40 value 180.922907
iter  50 value 180.906384
iter  60 value 180.820524
iter  70 value 180.249014
iter  80 value 179.533202
iter  90 value 179.385530
iter 100 value 179.376080
final  value 179.376080 
stopped after 100 iterations
# weights:  166
initial  value 247.412170 
iter  10 value 238.677135
final  value 238.677129 
converged
Neural Network 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  size  decay         RMSE      Rsquared    MAE       Selected
   2    3.158948e-04  2.422941  0.14407075  1.435828          
   2    5.244112e-03  2.423786  0.08813593  1.433479          
   2    1.772487e-02  2.422061  0.10734826  1.423273          
   4    2.434648e-05  2.422042  0.19573333  1.430746          
   4    1.283447e-04  2.421405  0.13004396  1.427038          
   4    3.258141e-03  2.423362  0.08440315  1.430791          
   5    2.777689e-04  2.421869  0.08614177  1.431241          
   5    8.202677e-03  2.421976  0.08973701  1.424014          
   6    3.330232e-02  2.422765  0.15301426  1.423697          
   6    2.104776e+00  2.450745  0.69385701  1.462212          
   8    6.531424e-04  2.418989  0.08060230  1.415761          
   8    1.079118e-03  2.422509  0.08272884  1.428780          
   8    1.736662e-01  2.425094  0.57325621  1.434741          
   8    3.659612e-01  2.427892  0.62562760  1.438004          
  11    9.240773e-02  2.423572  0.55686031  1.432806          
  14    2.170325e+00  2.442491  0.70543886  1.452880          
  15    7.081982e-05  2.418766  0.06944401  1.416706          
  15    1.431076e-04  2.417841  0.14107817  1.409634  *       
  16    6.022333e+00  2.464578  0.71734644  1.474516          
  19    5.219718e-05  2.422042  0.46626707  1.430748          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 15 and decay = 0.0001431076.
[1] "Sat Mar 10 04:21:57 2018"
Non-Negative Least Squares 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results:

  RMSE       Rsquared   MAE     
  0.4276557  0.9712782  0.294919

[1] "Sat Mar 10 04:22:06 2018"
Non-Informative Model 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results:

  RMSE      Rsquared  MAE     
  2.124496  NaN       1.464657

[1] "Sat Mar 10 04:22:15 2018"
Parallel Random Forest 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE        Selected
  1     1.7616593  0.6111753  1.2337262          
  2     1.5425128  0.7606085  1.0759953          
  3     1.3552794  0.8296782  0.9355983          
  4     1.1634827  0.8893504  0.7879623          
  5     1.0155956  0.9183341  0.6857724          
  7     0.7716683  0.9435617  0.4507498          
  8     0.6547773  0.9587786  0.3623091          
  9     0.5824799  0.9615331  0.2885543  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 04:22:26 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: Warning messages:
1: package 'mxnet' is not available (for R version 3.4.3) 
2: package 'mxnet' is not available (for R version 3.4.3) 
3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
# weights:  23
initial  value 159.995991 
iter  10 value 136.618744
iter  20 value 136.146851
iter  30 value 135.228879
iter  40 value 135.217686
final  value 135.217648 
converged
# weights:  56
initial  value 180.586161 
iter  10 value 136.067049
iter  20 value 136.014920
iter  30 value 135.860822
iter  40 value 134.491499
iter  50 value 134.062464
iter  60 value 134.060185
iter  70 value 134.059398
iter  80 value 133.940775
iter  90 value 133.918151
iter 100 value 133.915537
final  value 133.915537 
stopped after 100 iterations
# weights:  89
initial  value 179.563623 
iter  10 value 137.635742
final  value 137.608054 
converged
# weights:  89
initial  value 189.989782 
iter  10 value 136.124414
iter  20 value 136.001780
iter  30 value 134.687619
iter  40 value 134.146189
iter  50 value 134.125137
iter  60 value 134.064400
iter  70 value 134.006985
iter  80 value 134.004361
iter  90 value 134.003087
iter 100 value 134.002636
final  value 134.002636 
stopped after 100 iterations
# weights:  67
initial  value 185.341452 
iter  10 value 145.550643
final  value 145.548222 
converged
# weights:  166
initial  value 196.702563 
iter  10 value 136.060695
iter  20 value 136.050959
iter  30 value 136.023997
iter  40 value 135.903567
iter  50 value 135.098761
iter  60 value 134.983647
iter  70 value 134.710564
iter  80 value 134.096176
iter  90 value 134.049487
iter 100 value 134.045226
final  value 134.045226 
stopped after 100 iterations
# weights:  67
initial  value 197.044741 
iter  10 value 137.378566
iter  20 value 136.446260
iter  30 value 135.647562
iter  40 value 135.638022
iter  50 value 135.637561
final  value 135.637555 
converged
# weights:  155
initial  value 199.289838 
iter  10 value 143.124159
iter  20 value 143.114898
final  value 143.114877 
converged
# weights:  210
initial  value 166.782131 
iter  10 value 136.055014
final  value 136.050478 
converged
# weights:  89
initial  value 200.364220 
iter  10 value 138.745883
final  value 138.745866 
converged
# weights:  23
initial  value 176.867384 
iter  10 value 136.295311
iter  20 value 136.133854
iter  30 value 134.510039
iter  40 value 134.494809
iter  50 value 134.421215
iter  60 value 134.385522
iter  70 value 134.383569
iter  80 value 134.383139
final  value 134.383118 
converged
# weights:  45
initial  value 191.126312 
iter  10 value 136.059937
iter  20 value 135.983513
iter  30 value 135.713856
iter  40 value 134.080142
iter  50 value 133.918089
iter  60 value 133.898989
iter  70 value 133.898025
iter  80 value 133.897256
iter  90 value 133.896648
iter 100 value 133.896115
final  value 133.896115 
stopped after 100 iterations
# weights:  122
initial  value 167.388127 
iter  10 value 136.934210
iter  20 value 136.894200
final  value 136.894197 
converged
# weights:  56
initial  value 169.508923 
iter  10 value 136.366393
iter  20 value 136.127958
iter  30 value 134.685018
iter  40 value 134.606663
iter  50 value 134.605153
iter  60 value 134.604322
iter  70 value 134.603985
final  value 134.603943 
converged
# weights:  23
initial  value 174.412562 
iter  10 value 136.066870
iter  20 value 136.038390
iter  30 value 135.968068
iter  40 value 135.667264
iter  50 value 134.093168
iter  60 value 134.077314
iter  70 value 134.073220
iter  80 value 134.071182
iter  90 value 134.041273
iter 100 value 133.944129
final  value 133.944129 
stopped after 100 iterations
# weights:  177
initial  value 350.864729 
iter  10 value 148.338505
iter  20 value 148.125459
iter  20 value 148.125459
iter  20 value 148.125459
final  value 148.125459 
converged
# weights:  45
initial  value 151.709983 
iter  10 value 136.153848
iter  20 value 135.563201
iter  30 value 134.373670
iter  40 value 134.321130
iter  50 value 134.311893
iter  60 value 134.310393
iter  70 value 134.306785
iter  80 value 134.305465
iter  90 value 134.305213
iter 100 value 134.305098
final  value 134.305098 
stopped after 100 iterations
# weights:  166
initial  value 184.050164 
iter  10 value 136.056503
iter  20 value 136.051472
iter  30 value 135.957495
iter  40 value 135.329470
iter  50 value 134.273283
iter  60 value 134.043508
iter  70 value 134.040943
iter  80 value 134.040198
iter  90 value 134.039602
iter 100 value 134.037780
final  value 134.037780 
stopped after 100 iterations
# weights:  45
initial  value 193.559387 
iter  10 value 136.052401
iter  20 value 136.051023
iter  30 value 136.044106
iter  40 value 136.042481
iter  50 value 136.040452
iter  60 value 136.037798
iter  70 value 136.034125
iter  80 value 136.028644
iter  90 value 136.019522
iter 100 value 136.001377
final  value 136.001377 
stopped after 100 iterations
# weights:  89
initial  value 181.803599 
iter  10 value 136.093045
iter  20 value 136.057910
iter  30 value 136.044958
iter  40 value 135.980306
iter  50 value 135.548179
iter  60 value 134.189486
iter  70 value 134.101043
iter  80 value 134.096360
iter  90 value 134.094890
iter 100 value 134.094598
final  value 134.094598 
stopped after 100 iterations
# weights:  23
initial  value 193.901119 
iter  10 value 161.115781
iter  20 value 160.040343
iter  30 value 159.391649
iter  40 value 159.386044
iter  50 value 159.385503
final  value 159.385481 
converged
# weights:  56
initial  value 189.171136 
iter  10 value 160.395596
iter  20 value 160.390016
iter  30 value 160.389274
iter  40 value 160.388219
iter  50 value 160.386590
iter  60 value 160.383777
iter  70 value 160.378147
iter  80 value 160.364727
iter  90 value 160.324685
iter 100 value 160.070651
final  value 160.070651 
stopped after 100 iterations
# weights:  89
initial  value 196.775501 
iter  10 value 161.866359
final  value 161.797392 
converged
# weights:  89
initial  value 190.568751 
iter  10 value 160.434718
iter  20 value 160.403626
iter  30 value 160.400485
iter  40 value 160.393672
iter  50 value 160.370382
iter  60 value 160.169390
iter  70 value 158.714295
iter  80 value 158.608965
iter  90 value 158.255423
iter 100 value 158.223436
final  value 158.223436 
stopped after 100 iterations
# weights:  67
initial  value 208.742703 
iter  10 value 169.259944
iter  20 value 169.245697
final  value 169.245671 
converged
# weights:  166
initial  value 210.831041 
iter  10 value 160.392828
iter  20 value 160.381615
iter  30 value 160.381012
iter  40 value 160.380071
iter  50 value 160.378424
iter  60 value 160.374956
iter  70 value 160.364609
iter  80 value 160.291742
iter  90 value 159.258378
iter 100 value 158.517961
final  value 158.517961 
stopped after 100 iterations
# weights:  67
initial  value 213.290479 
iter  10 value 161.698941
iter  20 value 160.568343
iter  30 value 160.007731
iter  40 value 159.884735
iter  50 value 159.883739
iter  60 value 159.883648
final  value 159.883630 
converged
# weights:  155
initial  value 227.975164 
iter  10 value 167.076920
iter  20 value 167.037527
final  value 167.037403 
converged
# weights:  210
initial  value 218.709297 
iter  10 value 160.384422
final  value 160.382002 
converged
# weights:  89
initial  value 188.240393 
iter  10 value 162.868310
final  value 162.868201 
converged
# weights:  23
initial  value 211.486544 
iter  10 value 160.745361
iter  20 value 160.425633
iter  30 value 159.247301
iter  40 value 158.580160
iter  50 value 158.573878
iter  60 value 158.573237
iter  70 value 158.572975
final  value 158.572918 
converged
# weights:  45
initial  value 199.941047 
iter  10 value 160.387359
iter  20 value 160.367500
iter  30 value 160.357140
iter  40 value 160.334978
iter  50 value 160.260664
iter  60 value 159.454146
iter  70 value 158.554076
iter  80 value 158.484336
iter  90 value 158.479784
iter 100 value 158.078172
final  value 158.078172 
stopped after 100 iterations
# weights:  122
initial  value 173.597662 
iter  10 value 161.154365
iter  20 value 161.108388
iter  30 value 161.096386
final  value 161.096373 
converged
# weights:  56
initial  value 189.525595 
iter  10 value 160.746144
iter  20 value 160.195084
iter  30 value 158.852412
iter  40 value 158.751361
iter  50 value 158.741733
iter  60 value 158.739490
iter  70 value 158.738350
iter  80 value 158.737849
iter  90 value 158.737382
iter 100 value 158.737175
final  value 158.737175 
stopped after 100 iterations
# weights:  23
initial  value 183.382160 
iter  10 value 160.385545
iter  20 value 159.954439
iter  30 value 158.520528
iter  40 value 158.426632
iter  50 value 158.383975
iter  60 value 158.336596
iter  70 value 158.189419
iter  80 value 158.128125
iter  90 value 158.113335
iter 100 value 158.094754
final  value 158.094754 
stopped after 100 iterations
# weights:  177
initial  value 381.497892 
iter  10 value 171.808390
iter  20 value 171.781736
iter  20 value 171.781735
iter  20 value 171.781735
final  value 171.781735 
converged
# weights:  45
initial  value 206.644664 
iter  10 value 160.547024
iter  20 value 160.033733
iter  30 value 158.669619
iter  40 value 158.407388
iter  50 value 158.376221
iter  60 value 158.369613
iter  70 value 158.366146
iter  80 value 158.365079
iter  90 value 158.364717
iter 100 value 158.364601
final  value 158.364601 
stopped after 100 iterations
# weights:  166
initial  value 219.860982 
iter  10 value 160.385135
final  value 160.382461 
converged
# weights:  45
initial  value 201.131613 
final  value 160.382085 
converged
# weights:  89
initial  value 222.487288 
iter  10 value 160.418157
iter  20 value 160.394335
iter  30 value 160.387681
iter  40 value 160.369925
iter  50 value 160.264872
iter  60 value 158.879342
iter  70 value 158.561002
iter  80 value 158.539227
iter  90 value 158.174402
iter 100 value 158.167396
final  value 158.167396 
stopped after 100 iterations
# weights:  23
initial  value 230.351862 
iter  10 value 181.664310
iter  20 value 181.241490
iter  30 value 180.253164
iter  40 value 180.046743
iter  50 value 180.045918
final  value 180.045906 
converged
# weights:  56
initial  value 246.498466 
iter  10 value 180.938418
iter  20 value 180.915766
iter  30 value 180.544604
iter  40 value 178.882650
iter  50 value 178.854672
iter  60 value 178.732309
iter  70 value 178.718739
iter  80 value 178.708505
iter  90 value 178.691712
iter 100 value 178.686645
final  value 178.686645 
stopped after 100 iterations
# weights:  89
initial  value 222.157841 
iter  10 value 182.607652
final  value 182.476687 
converged
# weights:  89
initial  value 222.197285 
iter  10 value 180.999950
iter  20 value 180.940619
iter  30 value 180.938252
iter  40 value 180.934051
iter  50 value 180.924237
iter  60 value 180.886544
iter  70 value 180.518336
iter  80 value 179.068911
iter  90 value 178.880296
iter 100 value 178.794664
final  value 178.794664 
stopped after 100 iterations
# weights:  67
initial  value 242.188949 
iter  10 value 190.597715
iter  20 value 190.591474
final  value 190.591424 
converged
# weights:  166
initial  value 235.254366 
iter  10 value 180.936745
iter  20 value 180.920766
iter  30 value 180.856898
iter  40 value 180.266448
iter  50 value 178.951475
iter  60 value 178.796291
iter  70 value 178.677336
iter  80 value 178.665860
iter  90 value 178.663309
iter 100 value 178.662440
final  value 178.662440 
stopped after 100 iterations
# weights:  67
initial  value 244.089449 
iter  10 value 182.386859
iter  20 value 181.319964
iter  30 value 180.555920
iter  40 value 180.552400
final  value 180.552313 
converged
# weights:  155
initial  value 276.060099 
iter  10 value 188.136434
iter  20 value 188.123454
final  value 188.123430 
converged
# weights:  210
initial  value 221.333759 
iter  10 value 180.924412
final  value 180.918826 
converged
# weights:  89
initial  value 231.376363 
iter  10 value 183.624462
final  value 183.624280 
converged
# weights:  23
initial  value 227.058505 
iter  10 value 181.168513
iter  20 value 180.974311
iter  30 value 179.423443
iter  40 value 179.222484
iter  50 value 179.215188
iter  60 value 179.213820
iter  70 value 179.213449
final  value 179.213402 
converged
# weights:  45
initial  value 204.743497 
iter  10 value 180.924153
iter  20 value 180.922190
iter  30 value 180.922008
iter  40 value 180.921792
iter  50 value 180.921532
iter  60 value 180.921213
iter  70 value 180.920813
iter  80 value 180.920302
iter  90 value 180.919628
iter 100 value 180.918713
final  value 180.918713 
stopped after 100 iterations
# weights:  122
initial  value 226.999240 
iter  10 value 181.836534
iter  20 value 181.761939
iter  20 value 181.761937
iter  20 value 181.761937
final  value 181.761937 
converged
# weights:  56
initial  value 214.036656 
iter  10 value 181.340080
iter  20 value 181.007387
iter  30 value 179.844834
iter  40 value 179.419703
iter  50 value 179.398495
iter  60 value 179.397013
iter  70 value 179.395074
iter  80 value 179.393733
iter  90 value 179.393509
iter 100 value 179.393427
final  value 179.393427 
stopped after 100 iterations
# weights:  23
initial  value 234.625004 
iter  10 value 180.943939
iter  20 value 180.929798
iter  30 value 179.819287
iter  40 value 179.118280
iter  50 value 179.074524
iter  60 value 178.933464
iter  70 value 178.910267
iter  80 value 178.777395
iter  90 value 178.716770
iter 100 value 178.709230
final  value 178.709230 
stopped after 100 iterations
# weights:  177
initial  value 395.308429 
iter  10 value 193.600629
iter  20 value 193.354343
final  value 193.354288 
converged
# weights:  45
initial  value 224.166907 
iter  10 value 181.102064
iter  20 value 180.896949
iter  30 value 179.805523
iter  40 value 179.242071
iter  50 value 179.098140
iter  60 value 179.017069
iter  70 value 179.008258
iter  80 value 179.007616
iter  90 value 179.007485
iter 100 value 179.007345
final  value 179.007345 
stopped after 100 iterations
# weights:  166
initial  value 239.832244 
iter  10 value 180.923310
iter  20 value 180.919601
final  value 180.918882 
converged
# weights:  45
initial  value 199.603867 
final  value 180.931745 
converged
# weights:  89
initial  value 239.938117 
iter  10 value 180.963700
iter  20 value 180.931023
iter  30 value 180.140791
iter  40 value 178.988137
iter  50 value 178.865179
iter  60 value 178.742528
iter  70 value 178.736526
iter  80 value 178.734690
iter  90 value 178.733796
iter 100 value 178.733457
final  value 178.733457 
stopped after 100 iterations
# weights:  23
initial  value 300.957921 
iter  10 value 238.697199
iter  20 value 238.671401
iter  30 value 238.657026
iter  40 value 238.628137
iter  50 value 238.510534
iter  60 value 235.862925
iter  70 value 235.331688
iter  80 value 235.320289
iter  90 value 235.319217
iter 100 value 235.318550
final  value 235.318550 
stopped after 100 iterations
Neural Networks with Feature Extraction 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  size  decay         RMSE      Rsquared   MAE       Selected
   2    3.158948e-04  2.406717  0.1524284  1.344109  *       
   2    5.244112e-03  2.408324  0.2054040  1.356396          
   2    1.772487e-02  2.414204  0.2107461  1.382811          
   4    2.434648e-05  2.421961  0.2431971  1.430382          
   4    1.283447e-04  2.413372  0.1682653  1.378628          
   4    3.258141e-03  2.409315  0.1826296  1.361117          
   5    2.777689e-04  2.410242  0.1786559  1.363301          
   5    8.202677e-03  2.411732  0.1977681  1.374458          
   6    3.330232e-02  2.418646  0.2110133  1.408792          
   6    2.104776e+00  2.451741  0.7643792  1.463581          
   8    6.531424e-04  2.412135  0.1703060  1.380228          
   8    1.079118e-03  2.415138  0.1561692  1.382405          
   8    1.736662e-01  2.425264  0.5848629  1.435218          
   8    3.659612e-01  2.428148  0.6721506  1.438612          
  11    9.240773e-02  2.423973  0.5132306  1.433167          
  14    2.170325e+00  2.442952  0.7724228  1.453550          
  15    7.081982e-05  2.419068  0.2854195  1.413056          
  15    1.431076e-04  2.417498  0.1352512  1.400646          
  16    6.022333e+00  2.465102  0.7841417  1.475143          
  19    5.219718e-05  2.422042  0.1599367  1.430748          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 2 and decay = 0.0003158948.
[1] "Sat Mar 10 04:22:37 2018"
Principal Component Analysis 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      2.0459466  0.1182168  1.4181019          
  2      2.0144078  0.1167529  1.3713886          
  3      1.7835356  0.3278226  1.2633497          
  4      1.8455164  0.2608352  1.2920589          
  5      1.9366563  0.2612283  1.3709703          
  6      2.0425213  0.2631859  1.5189630          
  7      1.6649211  0.5387418  1.3178137          
  8      0.7282483  0.8961194  0.5842633  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 8.
[1] "Sat Mar 10 04:22:47 2018"
____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.1413238435518) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0677689607720822) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 2 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0385220878757536) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
Warning : 1 2 < 10^{-12}
Warning only 2 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0239215683192015) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Component____ 4 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.132190277101472) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.192658825637773) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0283384929411113) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0605002636555582) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.1413238435518) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0677689607720822) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0385220878757536) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.177884160866961) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
Warning : 1 9 < 10^{-12}
Warning only 3 components could thus be extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.15211450275965) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.132190277101472) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.192658825637773) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0283384929411113) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0605002636555582) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.1413238435518) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0677689607720822) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0385220878757536) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.177884160866961) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0239215683192015) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.15211450275965) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.132190277101472) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.192658825637773) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0283384929411113) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0605002636555582) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Component____ 3 ____
No more significant predictors (<0.0283384929411113) found
Warning only 3 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

Partial Least Squares Generalized Linear Models  

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  nt  alpha.pvals.expli  RMSE       Rsquared   MAE        Selected
  1   0.04998475         1.0113894  0.7880230  0.7460740          
  1   0.09065573         1.0113894  0.7880230  0.7460740          
  1   0.10828610         1.0079881  0.7930299  0.7258058          
  2   0.01288121         0.5699547  0.9407740  0.4498954          
  2   0.03694593         0.5762021  0.9401530  0.4524885          
  2   0.08376566         0.4793790  0.9598237  0.3343286          
  2   0.09713185         0.4624758  0.9646452  0.3283085          
  3   0.04812279         0.3867526  0.9769950  0.2701536          
  3   0.11741582         0.4304933  0.9690348  0.3033001          
  3   0.17744020         0.4288300  0.9667625  0.3112854          
  4   0.06050026         0.4220663  0.9710981  0.2938288          
  4   0.06776896         0.4220663  0.9710981  0.2938288          
  4   0.14132384         0.4304933  0.9690348  0.3033001          
  4   0.15211450         0.4284437  0.9691491  0.3022002          
  5   0.13219028         0.4304933  0.9690348  0.3033001          
  7   0.02833849         0.3750487  0.9784573  0.2656966  *       
  7   0.03852209         0.3850271  0.9770608  0.2775522          
  7   0.17788416         0.4288300  0.9667625  0.3112854          
  8   0.19265883         0.4464905  0.9644007  0.3139289          
  9   0.02392157         0.3898095  0.9754385  0.2775334          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nt = 7 and alpha.pvals.expli
 = 0.02833849.
[1] "Sat Mar 10 04:23:21 2018"
Projection Pursuit Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  nterms  RMSE       Rsquared   MAE        Selected
   1      0.3631159  0.9854728  0.1600873  *       
   2      0.4215084  0.9781134  0.1945181          
   3      0.3934977  0.9827634  0.1869857          
   4      0.3910766  0.9828201  0.1779918          
   5      0.3897739  0.9831637  0.1779545          
   6      0.3897936  0.9828231  0.1748758          
   7      0.3899368  0.9828506  0.1754244          
   8      0.3899867  0.9828061  0.1752481          
   9      0.3894188  0.9828345  0.1755170          
  10      0.3888358  0.9828760  0.1749635          
  11      0.3889426  0.9828724  0.1749176          
  12      0.3889931  0.9828682  0.1749324          
  13      0.3889904  0.9828711  0.1749362          
  14      0.3889792  0.9828716  0.1749283          
  15      0.3889805  0.9828715  0.1749321          
  16      0.3889853  0.9828713  0.1749329          
  17      0.3889970  0.9828701  0.1749391          
  18      0.3889969  0.9828699  0.1749414          
  19      0.3890000  0.9828697  0.1749430          
  20      0.3889976  0.9828700  0.1749401          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nterms = 1.
[1] "Sat Mar 10 04:23:31 2018"
Quantile Random Forest 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  mtry  RMSE      Rsquared   MAE        Selected
  1     2.691061  0.1728220  1.7463837          
  2     2.567807  0.3257901  1.6292865          
  3     2.497354  0.4086339  1.5700452          
  4     2.385816  0.4184345  1.4571167          
  5     2.205272  0.5430047  1.3452587          
  7     1.862297  0.6526346  1.0072397          
  8     1.612924  0.7530551  0.8544056          
  9     1.521282  0.8001527  0.7885397  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 04:23:57 2018"
Random Forest 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  min.node.size  mtry  splitrule   RMSE       Rsquared   MAE        Selected
   2             3     maxstat     1.5333719  0.7162181  0.9413785          
   2             5     maxstat     1.3284794  0.7566965  0.7542783          
   2             5     variance    1.0198180  0.9072089  0.6630024          
   4             1     extratrees  1.7108285  0.7461747  1.1562822          
   4             2     variance    1.5324108  0.7415651  1.0826604          
   4             4     variance    1.1330485  0.8968618  0.7720872          
   5             3     extratrees  1.2746795  0.9149230  0.8176150          
   5             5     maxstat     1.3141444  0.7674196  0.7238931          
   6             6     extratrees  0.8930045  0.9526444  0.5221890          
   6             8     extratrees  0.6808740  0.9595645  0.3636289          
   8             3     maxstat     1.6462221  0.6409536  1.0117434          
   8             4     variance    1.1869051  0.8826745  0.8026126          
   8             7     extratrees  0.7924854  0.9631874  0.4394037          
   8             7     variance    0.7590858  0.9542980  0.4460201          
  11             6     variance    0.9130386  0.9329229  0.5846579          
  14             9     variance    0.6239507  0.9591841  0.3634065  *       
  15             2     extratrees  1.6597161  0.8580763  1.1253749          
  16             9     maxstat     1.5465341  0.5088074  0.9045964          
  19             2     extratrees  1.7375615  0.8566008  1.1741708          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 9, splitrule = variance
 and min.node.size = 14.
[1] "Sat Mar 10 04:24:10 2018"
Error in code$varImp(object$finalModel, ...) : 
  No importance values available
In addition: Warning messages:
1: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
2: package 'gpls' is not available (for R version 3.4.3) 
3: package 'rPython' is not available (for R version 3.4.3) 
Random Forest 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  predFixed  minNode  RMSE       Rsquared   MAE        Selected
  1           5       1.8269219  0.5130830  1.3017485          
  1          10       1.9021490  0.4554546  1.3640100          
  1          11       1.8842985  0.4421998  1.3392652          
  2           2       1.4520588  0.7721792  0.9890157          
  2           4       1.5570385  0.7579860  1.0811756          
  2           9       1.6733392  0.6874630  1.1640400          
  2          10       1.6405225  0.7129894  1.1733794          
  3           5       1.3356757  0.8338379  0.9017200          
  3          12       1.5112767  0.7685643  1.0516218          
  3          18       1.6564455  0.6551820  1.2163273          
  4           7       1.2168732  0.8701648  0.8139675          
  4          15       1.4201860  0.8074088  1.0101768          
  4          16       1.4522771  0.7783195  1.0604656          
  5          14       1.1590614  0.9009885  0.8108039          
  7           3       0.5600839  0.9764344  0.2794893          
  7           4       0.6129109  0.9696393  0.3202745          
  7          18       1.1207225  0.8733726  0.8610935          
  8          20       1.0350401  0.8843566  0.8013927          
  9           3       0.3670577  0.9844224  0.1396296  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were predFixed = 9 and minNode = 3.
[1] "Sat Mar 10 04:25:57 2018"
Relaxed Lasso 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  phi         lambda     RMSE       Rsquared   MAE        Selected
  0.06440603   5.263460  2.8096031  0.5200115  2.3985790          
  0.11960784  88.494698  3.0342869        NaN  2.1621905          
  0.14169246  45.016355  3.8341406  0.1723904  2.9136264          
  0.18472966   5.225331  2.4448112  0.5686958  2.0885809          
  0.19261044  44.327805  3.7263098  0.1878685  2.8241490          
  0.24061394   6.425575  2.2763813  0.5924277  1.9446024          
  0.24992375   3.977359  0.8121637  0.8608447  0.7155849          
  0.30250132  11.503869  2.0908280  0.6196236  1.7851576          
  0.33884480  11.699375  1.9824353  0.6360975  1.6915234          
  0.41882831   5.128715  3.4499738  0.4477599  2.8879236          
  0.45327866   3.705877  0.5347567  0.9471949  0.4640670          
  0.48565927   6.097489  1.5506249  0.7076880  1.3132751          
  0.54143052   3.359540  0.6192855  0.9323770  0.5201063          
  0.58707909   7.237922  1.2609933  0.7641791  1.0519802          
  0.66095139  19.008174  1.0573463  0.8106673  0.8665029          
  0.70661922  11.191860  0.9359070  0.8419038  0.7574347          
  0.76057251  11.483851  0.7984386  0.8806484  0.6303532          
  0.88720099   7.785185  0.5205146  0.9626523  0.3827054          
  0.88942080  38.626795  0.5166651  0.9636652  0.3791017  *       
  0.96329413  52.550981  3.0342869        NaN  2.1621905          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 38.62679 and phi = 0.8894208.
[1] "Sat Mar 10 04:26:08 2018"
Random Forest 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE        Selected
  1     1.8031251  0.5072538  1.2801386          
  2     1.5387265  0.7616671  1.0752871          
  3     1.2998891  0.8424080  0.9021428          
  4     1.1965156  0.8649584  0.8250189          
  5     1.0156665  0.9174939  0.6614392          
  7     0.7526019  0.9518810  0.4465632          
  8     0.6612163  0.9572790  0.3606660          
  9     0.5711504  0.9646743  0.2809928  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Sat Mar 10 04:26:18 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Ridge Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  lambda        RMSE       Rsquared   MAE        Selected
  2.072445e-05  0.4777451  0.9566535  0.3376140  *       
  2.964284e-05  0.4777457  0.9566531  0.3376154          
  3.836374e-05  0.4777463  0.9566527  0.3376167          
  9.698111e-05  0.4777502  0.9566499  0.3376259          
  1.038134e-04  0.4777507  0.9566496  0.3376270          
  1.066035e-04  0.4777509  0.9566494  0.3376275          
  1.823019e-04  0.4777560  0.9566459  0.3376393          
  2.207082e-04  0.4777586  0.9566440  0.3376454          
  3.407346e-04  0.4777668  0.9566383  0.3376642          
  4.445159e-04  0.4777739  0.9566334  0.3376804          
  1.670709e-03  0.4778623  0.9565746  0.3378717          
  1.835280e-03  0.4778747  0.9565667  0.3378972          
  1.846977e-03  0.4778756  0.9565661  0.3378990          
  1.964078e-03  0.4778845  0.9565604  0.3379172          
  1.153538e-02  0.4788203  0.9560690  0.3394552          
  1.532368e-01  0.5187811  0.9457537  0.3831197          
  2.531953e-01  0.5547438  0.9377069  0.4162848          
  2.678393e-01  0.5599105  0.9365494  0.4203748          
  4.710252e-01  0.6256271  0.9216018  0.4783457          
  3.152625e+00  0.9909320  0.8426806  0.8162016          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was lambda = 2.072445e-05.
[1] "Sat Mar 10 04:26:29 2018"
Robust Linear Model 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  intercept  psi           RMSE       Rsquared   MAE        Selected
  FALSE      psi.huber     0.4492765  0.9674803  0.2812929          
  FALSE      psi.hampel    0.4568109  0.9711262  0.2846406          
  FALSE      psi.bisquare  0.5136143  0.9736244  0.2667975          
   TRUE      psi.huber     0.4172686  0.9673133  0.2790852          
   TRUE      psi.hampel    0.4029542  0.9694653  0.2745910  *       
   TRUE      psi.bisquare  0.4855131  0.9701794  0.2754841          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were intercept = TRUE and psi = psi.hampel.
[1] "Sat Mar 10 04:26:39 2018"
CART 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  cp          RMSE      Rsquared   MAE        Selected
  0.00000000  1.426879  0.5833117  0.9757718          
  0.01903171  1.426879  0.5833117  0.9757718  *       
  0.08187064  1.465465  0.5580150  1.0289120          
  0.67626601  2.124496        NaN  1.4646574          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.01903171.
[1] "Sat Mar 10 04:26:48 2018"
CART 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results:

  RMSE      Rsquared   MAE      
  1.426879  0.5833117  0.9757718

[1] "Sat Mar 10 04:26:58 2018"
CART 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  maxdepth  RMSE      Rsquared   MAE        Selected
  1         1.465465  0.5580150  1.0289120          
  2         1.426879  0.5833117  0.9757718  *       
  3         1.426879  0.5833117  0.9757718          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was maxdepth = 2.
[1] "Sat Mar 10 04:27:07 2018"
Quantile Regression with LASSO penalty 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  lambda        RMSE       Rsquared   MAE        Selected
  2.072445e-05  0.4753751  0.9648723  0.3170569          
  2.964284e-05  0.4753751  0.9648723  0.3170569          
  3.836374e-05  0.4753751  0.9648723  0.3170569          
  9.698111e-05  0.4753751  0.9648723  0.3170569          
  1.038134e-04  0.4753751  0.9648723  0.3170569          
  1.066035e-04  0.4753751  0.9648723  0.3170569          
  1.823019e-04  0.4753751  0.9648723  0.3170569          
  2.207082e-04  0.4753751  0.9648723  0.3170569          
  3.407346e-04  0.4753751  0.9648723  0.3170569          
  4.445159e-04  0.4753751  0.9648723  0.3170569          
  1.670709e-03  0.4738700  0.9648869  0.3149772          
  1.835280e-03  0.4738700  0.9648869  0.3149772          
  1.846977e-03  0.4738700  0.9648869  0.3149772          
  1.964078e-03  0.4738700  0.9648869  0.3149772  *       
  1.153538e-02  0.5001293  0.9777402  0.2183049          
  1.532368e-01  0.6300118  0.9797497  0.2180154          
  2.531953e-01  2.2856354        NaN  1.3486825          
  2.678393e-01  2.2856354        NaN  1.3486825          
  4.710252e-01  2.2856354        NaN  1.3486825          
  3.152625e+00  2.2856354        NaN  1.3486825          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was lambda = 0.001964078.
[1] "Sat Mar 10 04:27:17 2018"
Non-Convex Penalized Quantile Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  lambda        penalty  RMSE       Rsquared   MAE        Selected
  2.072445e-05  SCAD     0.4753751  0.9648723  0.3170569          
  2.964284e-05  MCP      0.4753751  0.9648723  0.3170569          
  3.836374e-05  MCP      0.4753751  0.9648723  0.3170569          
  9.698111e-05  MCP      0.4753751  0.9648723  0.3170569          
  1.038134e-04  MCP      0.4753751  0.9648723  0.3170569          
  1.066035e-04  MCP      0.4753751  0.9648723  0.3170569          
  1.823019e-04  MCP      0.4753751  0.9648723  0.3170569          
  2.207082e-04  MCP      0.4753751  0.9648723  0.3170569          
  3.407346e-04  SCAD     0.4753751  0.9648723  0.3170569          
  4.445159e-04  SCAD     0.4753751  0.9648723  0.3170569          
  1.670709e-03  SCAD     0.4753751  0.9648723  0.3170569          
  1.835280e-03  SCAD     0.4753751  0.9648723  0.3170569          
  1.846977e-03  MCP      0.4753751  0.9648723  0.3170569          
  1.964078e-03  MCP      0.4753751  0.9648723  0.3170569  *       
  1.153538e-02  SCAD     0.4756823  0.9648177  0.3170238          
  1.532368e-01  SCAD     0.4789939  0.9802113  0.2021504          
  2.531953e-01  MCP      2.2856354        NaN  1.3486825          
  2.678393e-01  MCP      2.2856354        NaN  1.3486825          
  4.710252e-01  SCAD     2.2856354        NaN  1.3486825          
  3.152625e+00  MCP      2.2856354        NaN  1.3486825          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 0.001964078 and penalty = MCP.
[1] "Sat Mar 10 04:27:28 2018"
Regularized Random Forest 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  mtry  coefReg     coefImp     RMSE       Rsquared   MAE        Selected
  1     0.24992375  0.82595486  1.8926210  0.5294776  1.3389693          
  1     0.45327866  0.07624227  1.7535387  0.6348458  1.2450530          
  1     0.54143052  0.77654343  1.8056189  0.5759982  1.2951246          
  2     0.06440603  0.58971815  1.6187399  0.7390230  1.1594270          
  2     0.18472966  0.16256110  1.5555619  0.7521671  1.0853048          
  2     0.41882831  0.19867701  1.5310734  0.7399243  1.0692716          
  2     0.48565927  0.78327364  1.5610452  0.7196820  1.1032996          
  3     0.24061394  0.36154790  1.3341586  0.8522371  0.9017859          
  3     0.58707909  0.54966937  1.4159189  0.7908683  0.9858082          
  3     0.88720099  0.44615909  1.3497577  0.8411082  0.9296789          
  4     0.30250132  0.77268374  1.1526189  0.8781985  0.7615818          
  4     0.33884480  0.01374621  1.1219697  0.8936606  0.7604180          
  4     0.70661922  0.63532438  1.1971166  0.8611242  0.8049888          
  4     0.76057251  0.02056618  1.1949220  0.8732639  0.8058895          
  5     0.66095139  0.31103195  1.0302659  0.9087051  0.6806750          
  7     0.14169246  0.46040924  0.6760999  0.9585737  0.3650536          
  7     0.19261044  0.44970272  0.6535503  0.9709603  0.3682777          
  7     0.88942080  0.10179055  0.7789005  0.9483442  0.4671820          
  8     0.96329413  0.80477893  0.5565159  0.9678684  0.2727088          
  9     0.11960784  0.60961182  0.4510856  0.9802899  0.1919018  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 9, coefReg = 0.1196078
 and coefImp = 0.6096118.
[1] "Sat Mar 10 04:27:42 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 49 warnings (use warnings() to see them)
Regularized Random Forest 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  mtry  coefReg     RMSE       Rsquared   MAE        Selected
  1     0.24992375  1.7657198  0.5995955  1.2590484          
  1     0.45327866  1.7977695  0.5585537  1.2740177          
  1     0.54143052  1.7587824  0.6053571  1.2431150          
  2     0.06440603  1.5404942  0.7815778  1.0997713          
  2     0.18472966  1.6184571  0.6941364  1.1470175          
  2     0.41882831  1.5152357  0.7517065  1.0692359          
  2     0.48565927  1.5781227  0.7338547  1.0959532          
  3     0.24061394  1.3717625  0.8214487  0.9547756          
  3     0.58707909  1.3416102  0.8150830  0.9187508          
  3     0.88720099  1.3255786  0.8478666  0.9183388          
  4     0.30250132  1.1431257  0.8911039  0.7692638          
  4     0.33884480  1.1917681  0.8648422  0.8047575          
  4     0.70661922  1.1621225  0.8825988  0.7841163          
  4     0.76057251  1.1524644  0.8809749  0.7850314          
  5     0.66095139  1.0290403  0.9070730  0.6761082          
  7     0.14169246  0.6546303  0.9675828  0.3638564          
  7     0.19261044  0.6949858  0.9517359  0.3803908          
  7     0.88942080  0.7812448  0.9487171  0.4501448          
  8     0.96329413  0.6685513  0.9573273  0.3567865          
  9     0.11960784  0.4726576  0.9789220  0.1992936  *       

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 9 and coefReg = 0.1196078.
[1] "Sat Mar 10 04:27:53 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:28:07 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "rvmLinear"                      
Error in .local(object, ...) : test vector does not match model !
In addition: There were 38 warnings (use warnings() to see them)
Error in .local(object, ...) : test vector does not match model !
In addition: There were 11 warnings (use warnings() to see them)
Error in .local(object, ...) : test vector does not match model !
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:28:33 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "rvmPoly"                        
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
In addition: Warning messages:
1: In .local(x, ...) : Model might be overfitted
2: model fit failed for Resample03: sigma=0.07186 Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:28:49 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "rvmRadial"                      
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |=====                                                                 |   8%  |                                                                              |========                                                              |  12%  |                                                                              |===========                                                           |  15%  |                                                                              |=============                                                         |  19%  |                                                                              |================                                                      |  23%  |                                                                              |===================                                                   |  27%  |                                                                              |======================                                                |  31%  |                                                                              |========================                                              |  35%  |                                                                              |===========================                                           |  38%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
Error in inpvar.ctr[i, ] : incorrect number of dimensions
In addition: There were 50 or more warnings (use warnings() to see the first 50)
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%  |                                                                              |=================================================                     |  70%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%  |                                                                              |=================================================                     |  70%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%  |                                                                              |=================================================                     |  70%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%  |                                                                              |=================================================                     |  70%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%  |                                                                              |=================================================                     |  70%  |                                                                              |===================================================                   |  72%  |                                                                              |====================================================                  |  75%  |                                                                              |======================================================                |  78%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%  |                                                                              |=================================================                     |  70%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%  |                                                                              |=================================================                     |  70%  |                                                                              |===================================================                   |  72%  |                                                                              |====================================================                  |  75%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%  |                                                                              |=================================================                     |  70%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%  |                                                                              |=================================================                     |  70%  |                                                                              |===================================================                   |  72%  |                                                                              |====================================================                  |  75%  |                                                                              |======================================================                |  78%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%  |                                                                              |=================================================                     |  70%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |=========                                                             |  12%  |                                                                              |==========                                                            |  15%  |                                                                              |============                                                          |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |================                                                      |  22%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  32%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |=====================================                                 |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  60%  |                                                                              |============================================                          |  62%  |                                                                              |==============================================                        |  65%  |                                                                              |===============================================                       |  68%  |                                                                              |=================================================                     |  70%  |                                                                              |===================================================                   |  72%  |                                                                              |====================================================                  |  75%  |                                                                              |======================================================                |  78%  |                                                                              |========================================================              |  80%  |                                                                              |==========================================================            |  82%  |                                                                              |============================================================          |  85%  |                                                                              |=============================================================         |  88%
Subtractive Clustering and Fuzzy c-Means Rules 

40 samples
 9 predictor

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 40, 40, 40, 40, 40, 40, ... 
Resampling results across tuning parameters:

  r.a  RMSE      Rsquared   MAE        Selected
  0.0       NaN        NaN        NaN          
  0.5  1.417613  0.5916125  0.9628974          
  1.0  1.329864  0.6272582  0.8920808  *       

Tuning parameter 'eps.high' was held constant at a value of 0.5

Tuning parameter 'eps.low' was held constant at a value of 0
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were r.a = 1, eps.high = 0.5 and eps.low
 = 0.
[1] "Sat Mar 10 04:29:30 2018"
Partial Least Squares 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      1.3918354  0.5963563  1.0454297          
  2      0.9019424  0.8330318  0.6932893          
  3      0.6579450  0.9159505  0.4751248          
  4      0.5443582  0.9413198  0.3871818          
  5      0.4988088  0.9524827  0.3557145          
  6      0.4919816  0.9538344  0.3491503          
  7      0.4813454  0.9561538  0.3418882          
  8      0.4794606  0.9562373  0.3382011  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 8.
[1] "Sat Mar 10 04:29:40 2018"
Error in mvrValstats(object = object, estimate = "train") : 
  could not find function "mvrValstats"
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Spike and Slab Regression 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  vars  RMSE       Rsquared   MAE        Selected
  1     0.3731063  0.9802501  0.2263173          
  2     0.3709791  0.9802482  0.2263984          
  3     0.3700496  0.9802353  0.2271125          
  4     0.3686179  0.9802429  0.2284622          
  5     0.3648232  0.9801802  0.2295725          
  7     0.3633409  0.9801276  0.2300616          
  8     0.3627573  0.9800554  0.2307744          
  9     0.3617703  0.9798154  0.2324501  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was vars = 9.
[1] "Sat Mar 10 04:30:01 2018"
Sparse Partial Least Squares 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  kappa       eta         K  RMSE       Rsquared   MAE        Selected
  0.02637359  0.54143052  7  0.4784874  0.9565692  0.3371624          
  0.03932665  0.45327866  1  0.3604654  0.9802501  0.2611322          
  0.04866008  0.24992375  8  0.4784559  0.9565000  0.3376190          
  0.08222393  0.41882831  2  0.4097504  0.9721188  0.2911092          
  0.08468778  0.18472966  2  0.5561122  0.9367390  0.4098605          
  0.08564762  0.06440603  6  0.4846286  0.9551381  0.3420953          
  0.10506594  0.48565927  8  0.4784559  0.9565000  0.3376190          
  0.11198487  0.24061394  4  0.5060709  0.9493713  0.3596598          
  0.12770136  0.58707909  5  0.4705214  0.9586517  0.3381441          
  0.13732394  0.88720099  5  0.4526936  0.9638037  0.3211011          
  0.18524173  0.70661922  6  0.4805807  0.9563209  0.3432624          
  0.18864186  0.76057251  1  0.3604654  0.9802501  0.2611322          
  0.18887179  0.30250132  7  0.4784874  0.9565692  0.3371624          
  0.19109656  0.33884480  1  0.3604654  0.9802501  0.2611322          
  0.25516933  0.66095139  3  0.3874592  0.9758670  0.2647291          
  0.34878025  0.88942080  1  0.3604654  0.9802501  0.2611322  *       
  0.36695463  0.19261044  5  0.4970598  0.9522099  0.3549350          
  0.36898952  0.14169246  5  0.4970598  0.9522099  0.3549350          
  0.38942035  0.96329413  8  0.4794818  0.9564507  0.3419859          
  0.45822269  0.11960784  6  0.4846286  0.9551381  0.3420953          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were K = 1, eta = 0.8894208 and kappa
 = 0.3487803.
[1] "Sat Mar 10 04:30:12 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: Warning messages:
1: predictions failed for Fold1: threshold=0.7788, n.components=3 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
2: predictions failed for Fold2: threshold=0.7788, n.components=3 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
3: predictions failed for Fold3: threshold=0.7788, n.components=3 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Error in confusionMatrix.default(p[, 1], p[, 2]) : 
  The data must contain some levels that overlap the reference.
In addition: Warning messages:
1: predictions failed for Fold1: threshold=0.3644, n.components=3 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
2: predictions failed for Fold3: threshold=0.3644, n.components=3 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Error in rowMeans(x) : 'x' must be an array of at least two dimensions
In addition: Warning messages:
1: predictions failed for Resample04: n.components=3, threshold=0.9 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
2: predictions failed for Resample06: n.components=3, threshold=0.9 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
3: predictions failed for Resample10: n.components=3, threshold=0.9 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
4: predictions failed for Resample20: n.components=3, threshold=0.9 Error in rowMeans(x) : 'x' must be an array of at least two dimensions
 
5: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:30:27 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "superpc"                        
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:30:36 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "svmBoundrangeString"            
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:30:45 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "svmExpoString"                  
Support Vector Machines with Linear Kernel 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  C             RMSE       Rsquared   MAE        Selected
    0.05407883  0.8312942  0.9596915  0.4239140          
    0.07079540  0.7692672  0.9624597  0.3890592          
    0.08595958  0.6866303  0.9524918  0.3881785          
    0.17274525  0.5050723  0.9571824  0.3156698          
    0.18182636  0.5016946  0.9577457  0.3132719          
    0.18549195  0.5012461  0.9578752  0.3132585          
    0.27777298  0.4576909  0.9610172  0.3077807          
    0.32075554  0.4519297  0.9619009  0.3058164          
    0.44474204  0.4340880  0.9649278  0.2941716          
    0.54326132  0.4260971  0.9650424  0.2933624          
    1.47146357  0.4248139  0.9634959  0.2983326  *       
    1.57926776  0.4303734  0.9630501  0.2988440          
    1.58683684  0.4301886  0.9630797  0.2985874          
    1.66197267  0.4303780  0.9630437  0.2986532          
    6.29881200  0.4597440  0.9560770  0.3177754          
   44.12142850  0.4596276  0.9560793  0.3177703          
   64.38438592  0.4597094  0.9560681  0.3179100          
   67.16723018  0.4597785  0.9560726  0.3177785          
  102.72263909  0.4595954  0.9560860  0.3176903          
  429.54951035  0.4597891  0.9560601  0.3179273          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was C = 1.471464.
[1] "Sat Mar 10 04:31:02 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Linear Kernel 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  cost          RMSE       Rsquared   MAE        Selected
    0.05407883  0.8312942  0.9596915  0.4239140          
    0.07079540  0.7692672  0.9624597  0.3890592          
    0.08595958  0.6866303  0.9524918  0.3881785          
    0.17274525  0.5050723  0.9571824  0.3156698          
    0.18182636  0.5016946  0.9577457  0.3132719          
    0.18549195  0.5012461  0.9578752  0.3132585          
    0.27777298  0.4576909  0.9610172  0.3077807          
    0.32075554  0.4519297  0.9619009  0.3058164          
    0.44474204  0.4340880  0.9649278  0.2941716          
    0.54326132  0.4260971  0.9650424  0.2933624          
    1.47146357  0.4248139  0.9634959  0.2983326  *       
    1.57926776  0.4303734  0.9630501  0.2988440          
    1.58683684  0.4301886  0.9630797  0.2985874          
    1.66197267  0.4303780  0.9630437  0.2986532          
    6.29881200  0.4597440  0.9560770  0.3177754          
   44.12142850  0.4596276  0.9560793  0.3177703          
   64.38438592  0.4597094  0.9560681  0.3179100          
   67.16723018  0.4597785  0.9560726  0.3177785          
  102.72263909  0.4595954  0.9560860  0.3176903          
  429.54951035  0.4597891  0.9560601  0.3179273          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cost = 1.471464.
[1] "Sat Mar 10 04:31:18 2018"
L2 Regularized Support Vector Machine (dual) with Linear Kernel 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  cost          Loss  RMSE       Rsquared   MAE        Selected
  2.028943e-03  L2    2.9458422  0.5196431  2.0662684          
  2.905629e-03  L1    2.4586210  0.7457109  1.6314713          
  3.763793e-03  L1    2.3418879  0.7515340  1.5279752          
  9.544976e-03  L1    1.8364074  0.7872870  1.1280975          
  1.021980e-02  L1    1.7974804  0.7905267  1.1010884          
  1.049543e-02  L1    1.7823599  0.7918091  1.0906422          
  1.798126e-02  L1    1.4925988  0.8229133  0.8942559          
  2.178373e-02  L1    1.3971291  0.8346368  0.8258476          
  3.368043e-02  L2    2.0542635  0.7297677  1.2731162          
  4.397897e-02  L2    1.8972086  0.7892258  1.1538871          
  1.660476e-01  L2    0.9087877  0.9596077  0.4364796          
  1.824628e-01  L2    0.8363010  0.9629861  0.3794193          
  1.836298e-01  L1    0.7221569  0.9138804  0.4824520          
  1.953133e-01  L1    0.7115501  0.9155178  0.4783470          
  1.154104e+00  L2    0.4610376  0.9705671  0.2566193          
  1.546795e+01  L2    0.4403525  0.9679398  0.2798745          
  2.560201e+01  L1    0.5012392  0.9523477  0.3586351          
  2.708798e+01  L1    0.5012007  0.9523605  0.3586488          
  4.772965e+01  L2    0.4114530  0.9654317  0.2892937  *       
  3.215518e+02  L1    0.5048084  0.9521312  0.3616273          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were cost = 47.72965 and Loss = L2.
[1] "Sat Mar 10 04:31:28 2018"
Support Vector Machines with Polynomial Kernel 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  degree  scale         C             RMSE       Rsquared   MAE        Selected
  1       2.194918e-05   14.37781172  2.2251310  0.7454487  1.3424471          
  1       9.533612e-05    0.16938952  2.2398135  0.7454487  1.3527090          
  1       1.885826e-04    1.34091568  2.2282007  0.7454487  1.3445960          
  1       2.112775e-04  167.65165139  1.0537082  0.9480321  0.5329410          
  1       1.660433e-03    0.24658468  2.2205409  0.7454487  1.3392303          
  1       2.528388e-03    0.06904276  2.2320380  0.7454487  1.3472797          
  1       3.754001e-03  107.56795990  0.4340821  0.9646500  0.2956502          
  1       7.415482e-03  100.29817233  0.4220061  0.9642226  0.2935770  *       
  1       1.294565e-02    9.48100909  0.5746963  0.9551268  0.3479948          
  1       5.047532e-01    3.23190854  0.4303227  0.9630559  0.2987098          
  2       4.013919e-04   96.35289896  0.7188812  0.9619864  0.3677526          
  2       6.254988e-04    0.03605125  2.2383899  0.7455949  1.3517162          
  2       3.189504e-02    0.79304757  0.9085238  0.9524637  0.4622428          
  2       5.569416e-02   23.10071763  0.5851131  0.9418071  0.3953446          
  2       1.076013e-01    0.03870041  1.8252876  0.8256466  1.0696508          
  3       4.305710e-05   17.68160473  2.1318865  0.7838543  1.2796169          
  3       5.637882e-05    3.74804573  2.2096319  0.7454752  1.3315350          
  3       1.049624e-04    3.35320515  2.1890920  0.7454979  1.3168467          
  3       5.186165e-01    0.09004927  1.0991464  0.7982320  0.7718502          
  3       1.277765e+00  134.52065953  1.2587631  0.6779150  0.8765984          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 1, scale = 0.007415482 and
 C = 100.2982.
[1] "Sat Mar 10 04:31:39 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Support Vector Machines with Radial Basis Function Kernel 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  sigma       C             RMSE       Rsquared   MAE        Selected
  0.01781783    7.96395512  0.6479764  0.9326171  0.4344948          
  0.01812418  196.31541933  0.6432329  0.9314086  0.4324586  *       
  0.02083039    0.64511472  1.5507387  0.8359550  0.8719899          
  0.02220401    1.19053717  1.1852714  0.9196756  0.6061098          
  0.02584645    0.03631351  2.1876473  0.6996984  1.3149750          
  0.02828827    9.92002949  0.7324066  0.9195458  0.4929761          
  0.03746048   21.21321078  0.7953220  0.9093918  0.5278747          
  0.04250215    2.80273374  0.8719234  0.9036576  0.5473574          
  0.05251224    5.28192149  0.9149521  0.8833201  0.5980756          
  0.05297943    0.09504137  2.0710025  0.6719564  1.2298289          
  0.05441640    0.08013254  2.0974178  0.6700448  1.2482748          
  0.06801835    0.32818761  1.7683631  0.7212833  0.9975912          
  0.07517977    2.30098679  1.1078858  0.8466234  0.7061654          
  0.07901264    0.08206503  2.0998639  0.5990501  1.2480782          
  0.08425755    0.04238797  2.1730051  0.5840575  1.2988337          
  0.11877342  459.29785933  1.3425363  0.7463617  0.8820303          
  0.11992484    3.41409656  1.3485435  0.7440196  0.8861856          
  0.12195935    1.02956677  1.5683657  0.7665792  0.9098199          
  0.12869455  509.98462326  1.3932553  0.7265585  0.9165814          
  0.13568995  157.21529646  1.4269857  0.7135454  0.9389798          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were sigma = 0.01812418 and C = 196.3154.
[1] "Sat Mar 10 04:31:50 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  C             RMSE      Rsquared   MAE        Selected
    0.05407883  2.151540  0.5972339  1.2836572          
    0.07079540  2.120545  0.6186743  1.2625496          
    0.08595958  2.088878  0.6421789  1.2406872          
    0.17274525  1.957343  0.6126758  1.1403402          
    0.18182636  1.947358  0.6089070  1.1345465          
    0.18549195  1.946578  0.6059370  1.1295499          
    0.27777298  1.846732  0.6606382  1.0589619          
    0.32075554  1.789885  0.7038103  1.0085021          
    0.44474204  1.707737  0.7602383  0.9488207          
    0.54326132  1.619059  0.8000973  0.8884508          
    1.47146357  1.224369  0.8645994  0.7072295          
    1.57926776  1.205648  0.8541940  0.7253711          
    1.58683684  1.189802  0.8552655  0.7181416          
    1.66197267  1.150352  0.8700777  0.6870521          
    6.29881200  1.066320  0.8425942  0.6931694  *       
   44.12142850  1.092439  0.8369210  0.7072288          
   64.38438592  1.102538  0.8369970  0.7071018          
   67.16723018  1.114482  0.8319138  0.7172243          
  102.72263909  1.112240  0.8288547  0.7215411          
  429.54951035  1.136452  0.8253571  0.7289088          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was C = 6.298812.
[1] "Sat Mar 10 04:32:00 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  sigma       C             RMSE       Rsquared   MAE        Selected
  0.01781783    7.96395512  0.6479764  0.9326171  0.4344948          
  0.01812418  196.31541933  0.6432329  0.9314086  0.4324586  *       
  0.02083039    0.64511472  1.5507387  0.8359550  0.8719899          
  0.02220401    1.19053717  1.1852714  0.9196756  0.6061098          
  0.02584645    0.03631351  2.1876473  0.6996984  1.3149750          
  0.02828827    9.92002949  0.7324066  0.9195458  0.4929761          
  0.03746048   21.21321078  0.7953220  0.9093918  0.5278747          
  0.04250215    2.80273374  0.8719234  0.9036576  0.5473574          
  0.05251224    5.28192149  0.9149521  0.8833201  0.5980756          
  0.05297943    0.09504137  2.0710025  0.6719564  1.2298289          
  0.05441640    0.08013254  2.0974178  0.6700448  1.2482748          
  0.06801835    0.32818761  1.7683631  0.7212833  0.9975912          
  0.07517977    2.30098679  1.1078858  0.8466234  0.7061654          
  0.07901264    0.08206503  2.0998639  0.5990501  1.2480782          
  0.08425755    0.04238797  2.1730051  0.5840575  1.2988337          
  0.11877342  459.29785933  1.3425363  0.7463617  0.8820303          
  0.11992484    3.41409656  1.3485435  0.7440196  0.8861856          
  0.12195935    1.02956677  1.5683657  0.7665792  0.9098199          
  0.12869455  509.98462326  1.3932553  0.7265585  0.9165814          
  0.13568995  157.21529646  1.4269857  0.7135454  0.9389798          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were sigma = 0.01812418 and C = 196.3154.
[1] "Sat Mar 10 04:32:10 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:32:19 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "svmSpectrumString"              
Bagged CART 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results:

  RMSE      Rsquared   MAE      
  1.148439  0.7625807  0.7171633

[1] "Sat Mar 10 04:32:30 2018"
Partial Least Squares 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 26, 26, 28 
Resampling results across tuning parameters:

  ncomp  RMSE       Rsquared   MAE        Selected
  1      1.3918354  0.5963563  1.0454297          
  2      0.9019424  0.8330318  0.6932893          
  3      0.6579450  0.9159505  0.4751248          
  4      0.5443582  0.9413198  0.3871818          
  5      0.4988088  0.9524827  0.3557145          
  6      0.4919816  0.9538344  0.3491503          
  7      0.4813454  0.9561538  0.3418882          
  8      0.4794606  0.9562373  0.3382011  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 8.
[1] "Sat Mar 10 04:32:56 2018"
Error in mvrValstats(object = object, estimate = "train") : 
  could not find function "mvrValstats"
Error : package xgboost is required
Error : package xgboost is required
Error : package xgboost is required
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:33:06 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "xgbDART"                        
Error : package xgboost is required
Error : package xgboost is required
Error : package xgboost is required
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:33:17 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "xgbLinear"                      
Error : package xgboost is required
Error : package xgboost is required
Error : package xgboost is required
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:33:26 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "xgbTree"                        
Error : package kohonen is required
Error : package kohonen is required
Error : package kohonen is required
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:33:35 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "xyf"                            
Fitting Repeat 1 

# weights:  188
initial  value 158.070293 
iter  10 value 124.676345
iter  20 value 124.639770
final  value 124.637944 
converged
Fitting Repeat 2 

# weights:  188
initial  value 213.206793 
iter  10 value 189.150547
iter  20 value 189.127176
iter  30 value 189.125063
iter  40 value 189.124794
iter  50 value 189.124466
iter  60 value 189.124051
iter  70 value 189.123496
iter  80 value 189.122702
iter  90 value 189.121451
iter 100 value 189.119174
final  value 189.119174 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 135.092327 
iter  10 value 110.967326
iter  20 value 110.951053
iter  30 value 110.948878
iter  40 value 110.948392
iter  50 value 110.947691
iter  60 value 110.946581
iter  70 value 110.944554
iter  80 value 110.939769
iter  90 value 110.918825
iter 100 value 109.746385
final  value 109.746385 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 179.331268 
iter  10 value 153.214811
iter  20 value 153.195176
iter  30 value 153.189495
iter  40 value 153.184089
iter  50 value 153.124498
iter  60 value 152.524288
iter  70 value 150.774753
iter  80 value 150.476327
iter  90 value 150.473559
iter 100 value 150.469936
final  value 150.469936 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 58.918732 
iter  10 value 48.132058
iter  20 value 48.126609
iter  30 value 48.124177
iter  40 value 48.118439
iter  50 value 48.094543
iter  60 value 47.463407
iter  70 value 46.236835
iter  80 value 46.232054
iter  90 value 46.229300
iter 100 value 46.227763
final  value 46.227763 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  155
initial  value 291.809430 
iter  10 value 118.890015
iter  20 value 118.771500
final  value 118.771498 
converged
Fitting Repeat 2 

# weights:  155
initial  value 216.818899 
iter  10 value 36.190011
iter  20 value 35.969032
final  value 35.969027 
converged
Fitting Repeat 3 

# weights:  155
initial  value 284.484381 
iter  10 value 102.455625
iter  20 value 102.384434
iter  20 value 102.384434
iter  20 value 102.384433
final  value 102.384433 
converged
Fitting Repeat 4 

# weights:  155
initial  value 346.708053 
iter  10 value 158.412069
iter  20 value 158.386879
iter  20 value 158.386878
iter  20 value 158.386878
final  value 158.386878 
converged
Fitting Repeat 5 

# weights:  155
initial  value 368.046296 
iter  10 value 172.633916
iter  20 value 172.590079
final  value 172.590072 
converged
Fitting Repeat 1 

# weights:  45
initial  value 160.929982 
iter  10 value 112.979735
iter  20 value 112.954707
final  value 112.954704 
converged
Fitting Repeat 2 

# weights:  45
initial  value 159.185959 
iter  10 value 112.965211
iter  20 value 112.954723
final  value 112.954704 
converged
Fitting Repeat 3 

# weights:  45
initial  value 178.861413 
iter  10 value 112.991992
iter  20 value 112.954708
iter  20 value 112.954708
iter  20 value 112.954708
final  value 112.954708 
converged
Fitting Repeat 4 

# weights:  45
initial  value 176.113596 
iter  10 value 112.981835
iter  20 value 112.954704
iter  20 value 112.954704
iter  20 value 112.954703
final  value 112.954703 
converged
Fitting Repeat 5 

# weights:  45
initial  value 194.332404 
iter  10 value 113.047771
iter  20 value 112.954706
final  value 112.954704 
converged
Fitting Repeat 1 

# weights:  133
initial  value 222.138569 
iter  10 value 173.257845
iter  20 value 172.891234
iter  30 value 172.884329
iter  40 value 172.878888
iter  50 value 172.849125
iter  60 value 171.874310
iter  70 value 171.259730
iter  80 value 171.225423
iter  90 value 171.219665
final  value 171.219504 
converged
Fitting Repeat 2 

# weights:  133
initial  value 146.822696 
iter  10 value 91.846536
iter  20 value 91.765069
iter  30 value 90.826441
iter  40 value 90.748396
iter  50 value 90.737318
iter  60 value 90.736040
iter  70 value 90.328796
iter  80 value 90.224683
iter  90 value 90.220565
iter 100 value 90.220432
final  value 90.220432 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 187.286160 
iter  10 value 149.062074
iter  20 value 148.772421
iter  30 value 147.427033
iter  40 value 147.061190
iter  50 value 146.995938
iter  60 value 146.738777
iter  70 value 146.736069
iter  80 value 146.735254
iter  90 value 146.735144
iter 100 value 146.733376
final  value 146.733376 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  133
initial  value 72.159676 
iter  10 value 42.545987
iter  20 value 42.495972
iter  30 value 42.316469
iter  40 value 41.559448
iter  50 value 40.487115
iter  60 value 40.377667
iter  70 value 40.370333
iter  80 value 40.365086
iter  90 value 40.323327
iter 100 value 40.292199
final  value 40.292199 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  133
initial  value 64.429313 
iter  10 value 47.540390
iter  20 value 47.453647
iter  30 value 47.382954
iter  40 value 45.733297
iter  50 value 45.593194
iter  60 value 45.563200
iter  70 value 45.547535
iter  80 value 45.535457
iter  90 value 45.534108
iter 100 value 45.530854
final  value 45.530854 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 261.949942 
iter  10 value 93.049846
iter  20 value 92.951206
final  value 92.947648 
converged
Fitting Repeat 2 

# weights:  89
initial  value 359.001164 
iter  10 value 156.329134
final  value 156.328489 
converged
Fitting Repeat 3 

# weights:  89
initial  value 446.666436 
iter  10 value 251.437781
iter  20 value 251.394450
iter  20 value 251.394450
iter  20 value 251.394449
final  value 251.394449 
converged
Fitting Repeat 4 

# weights:  89
initial  value 382.699328 
iter  10 value 206.180365
iter  20 value 205.976943
final  value 205.976883 
converged
Fitting Repeat 5 

# weights:  89
initial  value 235.971305 
iter  10 value 76.787910
iter  20 value 76.753303
iter  20 value 76.753302
final  value 76.753299 
converged
Fitting Repeat 1 

# weights:  78
initial  value 99.227318 
iter  10 value 63.099985
final  value 63.099405 
converged
Fitting Repeat 2 

# weights:  78
initial  value 118.856601 
iter  10 value 88.200782
final  value 88.200597 
converged
Fitting Repeat 3 

# weights:  78
initial  value 229.928589 
iter  10 value 188.062814
final  value 188.046940 
converged
Fitting Repeat 4 

# weights:  78
initial  value 284.309793 
iter  10 value 213.718669
final  value 213.698572 
converged
Fitting Repeat 5 

# weights:  78
initial  value 85.136126 
iter  10 value 64.047697
final  value 64.047122 
converged
Fitting Repeat 1 

# weights:  199
initial  value 89.003205 
iter  10 value 48.149454
final  value 48.148945 
converged
Fitting Repeat 2 

# weights:  199
initial  value 63.465439 
iter  10 value 41.106170
final  value 41.105944 
converged
Fitting Repeat 3 

# weights:  199
initial  value 113.933240 
iter  10 value 71.173321
final  value 71.173246 
converged
Fitting Repeat 4 

# weights:  199
initial  value 93.092751 
iter  10 value 66.927192
final  value 66.926493 
converged
Fitting Repeat 5 

# weights:  199
initial  value 79.111760 
final  value 64.405012 
converged
Fitting Repeat 1 

# weights:  210
initial  value 95.927554 
iter  10 value 85.681885
iter  20 value 85.535023
iter  30 value 84.878708
iter  40 value 84.199818
iter  50 value 84.188903
iter  60 value 84.188348
iter  70 value 84.182568
iter  80 value 84.113126
iter  90 value 84.081761
iter 100 value 84.079628
final  value 84.079628 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  210
initial  value 78.198492 
iter  10 value 58.680274
iter  20 value 58.097451
iter  30 value 57.877341
iter  40 value 56.826480
iter  50 value 56.756577
iter  60 value 56.755707
iter  70 value 56.755311
iter  80 value 56.717004
iter  90 value 56.644122
iter 100 value 56.621715
final  value 56.621715 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  210
initial  value 112.200583 
iter  10 value 79.912993
iter  20 value 79.344049
iter  30 value 79.072766
iter  40 value 77.018066
iter  50 value 76.903194
iter  60 value 76.897054
iter  70 value 76.647782
iter  80 value 76.619955
iter  90 value 76.614990
iter 100 value 76.611173
final  value 76.611173 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  210
initial  value 101.622412 
iter  10 value 83.601097
iter  20 value 83.004720
iter  30 value 82.917719
iter  40 value 82.215637
iter  50 value 82.089731
iter  60 value 82.055136
iter  70 value 82.001668
iter  80 value 81.995660
iter  90 value 81.995275
iter 100 value 81.994981
final  value 81.994981 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  210
initial  value 113.663934 
iter  10 value 74.691149
iter  20 value 73.995742
iter  30 value 73.993656
final  value 73.993559 
converged
Fitting Repeat 1 

# weights:  177
initial  value 116.331304 
iter  10 value 96.357227
iter  20 value 96.215666
iter  30 value 95.895484
iter  40 value 95.448050
iter  50 value 94.961701
iter  60 value 94.926766
iter  70 value 94.918301
iter  80 value 94.869708
iter  90 value 94.335361
iter 100 value 94.207462
final  value 94.207462 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  177
initial  value 125.044136 
iter  10 value 96.459561
iter  20 value 96.217047
iter  30 value 96.170774
iter  40 value 95.750497
iter  50 value 95.431311
iter  60 value 95.264601
iter  70 value 95.123623
iter  80 value 95.123123
iter  90 value 95.122954
final  value 95.122951 
converged
Fitting Repeat 3 

# weights:  177
initial  value 157.592581 
iter  10 value 96.288251
iter  20 value 96.216335
iter  30 value 96.207310
iter  40 value 95.882265
iter  50 value 95.475685
iter  60 value 95.367538
iter  70 value 95.125200
iter  80 value 95.123237
iter  90 value 95.122829
final  value 95.122804 
converged
Fitting Repeat 4 

# weights:  177
initial  value 141.298373 
iter  10 value 96.524459
iter  20 value 96.219609
iter  30 value 96.153305
iter  40 value 95.472851
iter  50 value 95.080110
iter  60 value 94.927679
iter  70 value 94.918105
iter  80 value 94.338847
iter  90 value 94.210095
iter 100 value 94.185574
final  value 94.185574 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  177
initial  value 124.195265 
iter  10 value 96.450238
iter  20 value 96.217269
iter  30 value 96.211077
iter  40 value 96.208419
iter  50 value 96.201077
iter  60 value 96.154954
iter  70 value 95.708218
iter  80 value 95.429574
iter  90 value 95.323538
iter 100 value 95.127488
final  value 95.127488 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  78
initial  value 194.393105 
iter  10 value 98.692193
iter  20 value 98.663204
final  value 98.663198 
converged
Fitting Repeat 2 

# weights:  78
initial  value 180.427663 
iter  10 value 100.686799
final  value 100.615033 
converged
Fitting Repeat 3 

# weights:  78
initial  value 241.074267 
iter  10 value 119.950498
iter  20 value 119.918160
final  value 119.918148 
converged
Fitting Repeat 4 

# weights:  78
initial  value 191.459287 
iter  10 value 77.357944
iter  20 value 77.212003
final  value 77.211986 
converged
Fitting Repeat 5 

# weights:  78
initial  value 163.960477 
iter  10 value 80.968894
iter  20 value 80.916562
iter  20 value 80.916561
final  value 80.916556 
converged
Fitting Repeat 1 

# weights:  111
initial  value 111.620764 
iter  10 value 96.408732
iter  20 value 96.276068
iter  30 value 95.985843
iter  40 value 95.612920
iter  50 value 95.374761
iter  60 value 95.340854
iter  70 value 95.340420
final  value 95.340405 
converged
Fitting Repeat 2 

# weights:  111
initial  value 126.240230 
iter  10 value 96.645317
iter  20 value 96.277947
iter  30 value 96.228001
iter  40 value 95.906627
iter  50 value 95.607666
iter  60 value 95.354922
iter  70 value 95.340856
iter  80 value 95.340443
final  value 95.340421 
converged
Fitting Repeat 3 

# weights:  111
initial  value 130.343561 
iter  10 value 96.669429
iter  20 value 96.282626
iter  30 value 96.062466
iter  40 value 95.612068
iter  50 value 95.345488
iter  60 value 95.340829
iter  70 value 95.340453
final  value 95.340445 
converged
Fitting Repeat 4 

# weights:  111
initial  value 113.659543 
iter  10 value 96.428999
iter  20 value 96.277895
iter  30 value 96.169039
iter  40 value 95.653616
iter  50 value 95.468729
iter  60 value 95.341315
iter  70 value 95.340469
final  value 95.340416 
converged
Fitting Repeat 5 

# weights:  111
initial  value 137.551827 
iter  10 value 96.551432
iter  20 value 96.278685
iter  30 value 96.252998
iter  40 value 96.068760
iter  50 value 95.619295
iter  60 value 95.533458
iter  70 value 95.341305
iter  80 value 95.340441
final  value 95.340401 
converged
Fitting Repeat 1 

# weights:  166
initial  value 111.700789 
iter  10 value 74.509414
final  value 74.509118 
converged
Fitting Repeat 2 

# weights:  166
initial  value 148.004174 
iter  10 value 76.108505
final  value 76.107685 
converged
Fitting Repeat 3 

# weights:  166
initial  value 159.081281 
iter  10 value 106.390934
final  value 106.390746 
converged
Fitting Repeat 4 

# weights:  166
initial  value 218.567169 
iter  10 value 150.010603
final  value 150.010515 
converged
Fitting Repeat 5 

# weights:  166
initial  value 149.996983 
iter  10 value 93.598923
final  value 93.591717 
converged
Fitting Repeat 1 

# weights:  188
initial  value 69.995126 
iter  10 value 39.353663
iter  20 value 39.157540
iter  30 value 37.059495
iter  40 value 36.697056
iter  50 value 36.612870
iter  60 value 36.579085
iter  70 value 36.572745
iter  80 value 36.571876
iter  90 value 36.571790
iter 100 value 36.571688
final  value 36.571688 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 134.673625 
iter  10 value 121.788856
iter  20 value 121.727780
iter  30 value 121.700123
iter  40 value 121.430578
iter  50 value 120.784100
iter  60 value 120.698177
iter  70 value 120.692382
iter  80 value 120.691624
iter  80 value 120.691622
iter  80 value 120.691622
final  value 120.691622 
converged
Fitting Repeat 3 

# weights:  188
initial  value 155.130051 
iter  10 value 118.349561
iter  20 value 118.052242
iter  30 value 117.992349
iter  40 value 117.102563
iter  50 value 116.043513
iter  60 value 115.986815
iter  70 value 115.985324
iter  80 value 115.985167
iter  90 value 115.984851
iter 100 value 115.924566
final  value 115.924566 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 57.117012 
iter  10 value 44.349369
iter  20 value 39.731459
iter  30 value 39.391920
iter  40 value 39.348121
iter  50 value 39.325152
iter  60 value 39.323298
iter  70 value 38.500550
iter  80 value 38.484385
iter  90 value 38.484051
iter 100 value 38.483247
final  value 38.483247 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 97.995387 
iter  10 value 59.959034
iter  20 value 59.750151
iter  30 value 57.695043
iter  40 value 57.466699
iter  50 value 57.444853
iter  60 value 57.110841
iter  70 value 57.104132
iter  80 value 57.088637
iter  90 value 57.069880
iter 100 value 57.067255
final  value 57.067255 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  78
initial  value 155.600399 
iter  10 value 96.169019
iter  20 value 96.166801
iter  30 value 96.166659
iter  40 value 96.166479
iter  50 value 96.166244
iter  60 value 96.165928
iter  70 value 96.165482
iter  80 value 96.164826
iter  90 value 96.163800
iter 100 value 96.162083
final  value 96.162083 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  78
initial  value 122.993736 
iter  10 value 96.167820
final  value 96.167818 
converged
Fitting Repeat 3 

# weights:  78
initial  value 112.729468 
iter  10 value 96.167446
final  value 96.167439 
converged
Fitting Repeat 4 

# weights:  78
initial  value 119.800517 
iter  10 value 96.167682
final  value 96.167656 
converged
Fitting Repeat 5 

# weights:  78
initial  value 132.360123 
iter  10 value 96.168029
final  value 96.168018 
converged
Fitting Repeat 1 

# weights:  221
initial  value 126.503088 
iter  10 value 96.170140
final  value 96.167700 
converged
Fitting Repeat 2 

# weights:  221
initial  value 155.698908 
iter  10 value 96.168937
final  value 96.167682 
converged
Fitting Repeat 3 

# weights:  221
initial  value 117.485395 
iter  10 value 96.169658
final  value 96.167602 
converged
Fitting Repeat 4 

# weights:  221
initial  value 132.820362 
iter  10 value 96.170936
final  value 96.167628 
converged
Fitting Repeat 5 

# weights:  221
initial  value 165.071085 
iter  10 value 96.167897
final  value 96.167892 
converged
Fitting Repeat 1 

# weights:  12
initial  value 139.006967 
iter  10 value 96.388100
iter  20 value 96.267072
iter  30 value 95.877734
iter  40 value 95.202746
iter  50 value 94.669896
iter  60 value 94.510605
iter  70 value 94.392096
iter  80 value 94.390572
final  value 94.390384 
converged
Fitting Repeat 2 

# weights:  12
initial  value 124.436758 
iter  10 value 96.383628
iter  20 value 96.329824
iter  30 value 94.732741
iter  40 value 94.395266
iter  50 value 94.390598
iter  60 value 94.390402
iter  70 value 94.390380
iter  70 value 94.390380
iter  70 value 94.390380
final  value 94.390380 
converged
Fitting Repeat 3 

# weights:  12
initial  value 138.344596 
iter  10 value 96.360408
iter  20 value 96.288295
iter  30 value 96.183876
iter  40 value 95.448038
iter  50 value 95.170996
iter  60 value 94.441640
iter  70 value 94.395473
iter  80 value 94.390808
iter  90 value 94.390401
final  value 94.390383 
converged
Fitting Repeat 4 

# weights:  12
initial  value 135.790753 
iter  10 value 96.356128
iter  20 value 96.309032
iter  30 value 96.205870
iter  40 value 95.699802
iter  50 value 95.194030
iter  60 value 94.537546
iter  70 value 94.510693
iter  80 value 94.425780
iter  90 value 94.392965
iter 100 value 94.390394
final  value 94.390394 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  12
initial  value 127.782976 
iter  10 value 96.340609
iter  20 value 96.310677
iter  30 value 95.937430
iter  40 value 95.515189
iter  50 value 95.060210
iter  60 value 95.028681
iter  70 value 95.025519
iter  80 value 95.024707
iter  90 value 94.120257
iter 100 value 94.052816
final  value 94.052816 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  67
initial  value 110.189908 
iter  10 value 94.059873
iter  20 value 93.594530
iter  30 value 93.563820
final  value 93.563728 
converged
Fitting Repeat 2 

# weights:  67
initial  value 85.316685 
iter  10 value 64.473942
iter  20 value 64.467073
final  value 64.467056 
converged
Fitting Repeat 3 

# weights:  67
initial  value 226.071471 
iter  10 value 190.035314
iter  20 value 187.945652
iter  30 value 187.899041
iter  40 value 187.897739
final  value 187.897725 
converged
Fitting Repeat 4 

# weights:  67
initial  value 82.615862 
iter  10 value 44.627035
iter  20 value 44.508080
iter  30 value 44.336497
iter  40 value 44.256016
iter  50 value 44.125066
iter  60 value 43.978197
iter  70 value 43.915065
iter  80 value 43.911129
iter  90 value 43.911014
final  value 43.911013 
converged
Fitting Repeat 5 

# weights:  67
initial  value 180.491421 
iter  10 value 122.885336
iter  20 value 121.206616
iter  30 value 119.739322
iter  40 value 119.737243
final  value 119.737232 
converged
Fitting Repeat 1 

# weights:  78
initial  value 113.558066 
iter  10 value 86.931769
iter  10 value 86.931769
iter  10 value 86.931768
final  value 86.931768 
converged
Fitting Repeat 2 

# weights:  78
initial  value 161.250889 
iter  10 value 129.431099
final  value 129.430831 
converged
Fitting Repeat 3 

# weights:  78
initial  value 156.440401 
iter  10 value 120.608211
final  value 120.608084 
converged
Fitting Repeat 4 

# weights:  78
initial  value 121.213539 
iter  10 value 96.660346
final  value 96.660344 
converged
Fitting Repeat 5 

# weights:  78
initial  value 99.869463 
iter  10 value 70.015705
iter  20 value 70.013998
iter  30 value 70.013775
iter  40 value 70.013535
iter  50 value 70.013263
iter  60 value 70.012938
iter  70 value 70.012529
iter  80 value 70.011989
iter  90 value 70.011230
iter 100 value 70.010089
final  value 70.010089 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  34
initial  value 112.288786 
iter  10 value 96.171273
iter  20 value 96.169937
iter  30 value 96.169564
iter  40 value 96.169073
iter  50 value 96.168414
iter  60 value 96.167523
iter  70 value 96.166316
iter  80 value 96.164695
iter  90 value 96.162554
iter 100 value 96.159771
final  value 96.159771 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  34
initial  value 141.039406 
iter  10 value 96.178891
iter  20 value 96.173979
iter  30 value 96.172469
iter  40 value 96.172193
iter  50 value 96.171890
iter  60 value 96.171555
iter  70 value 96.171184
iter  80 value 96.170771
iter  90 value 96.170308
iter 100 value 96.169785
final  value 96.169785 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  34
initial  value 130.289929 
iter  10 value 96.174680
iter  20 value 96.171114
iter  30 value 96.170034
iter  40 value 96.169792
iter  50 value 96.169488
iter  60 value 96.169102
iter  70 value 96.168610
iter  80 value 96.167982
iter  90 value 96.167183
iter 100 value 96.166179
final  value 96.166179 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  34
initial  value 123.408767 
iter  10 value 96.172119
iter  20 value 96.168885
iter  30 value 96.167876
iter  40 value 96.166216
iter  50 value 96.163403
iter  60 value 96.158741
iter  70 value 96.151524
iter  80 value 96.140651
iter  90 value 96.122064
iter 100 value 96.078842
final  value 96.078842 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  34
initial  value 122.999101 
iter  10 value 96.173338
iter  20 value 96.170160
iter  30 value 96.161830
iter  40 value 96.144796
iter  50 value 96.110614
iter  60 value 96.013840
iter  70 value 95.415476
iter  80 value 94.840081
iter  90 value 94.839491
iter 100 value 94.839003
final  value 94.839003 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  34
initial  value 243.130682 
iter  10 value 211.016468
final  value 211.015469 
converged
Fitting Repeat 2 

# weights:  34
initial  value 81.260226 
iter  10 value 53.540337
iter  20 value 53.518119
iter  20 value 53.518119
iter  20 value 53.518119
final  value 53.518119 
converged
Fitting Repeat 3 

# weights:  34
initial  value 143.881651 
iter  10 value 109.432505
iter  20 value 109.409220
final  value 109.409217 
converged
Fitting Repeat 4 

# weights:  34
initial  value 178.672801 
iter  10 value 150.022049
final  value 150.020002 
converged
Fitting Repeat 5 

# weights:  34
initial  value 173.388963 
iter  10 value 120.814338
iter  20 value 120.753122
final  value 120.752960 
converged
Fitting Repeat 1 

# weights:  188
initial  value 163.694354 
iter  10 value 160.588526
iter  20 value 160.583011
iter  30 value 160.580731
iter  40 value 160.575399
iter  50 value 160.550845
iter  60 value 157.155604
iter  70 value 156.769704
iter  80 value 156.766570
iter  90 value 156.764949
iter 100 value 156.762363
final  value 156.762363 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 312.930141 
iter  10 value 238.142471
iter  20 value 238.071225
iter  30 value 238.068411
iter  40 value 238.055342
iter  50 value 238.027513
iter  60 value 237.325267
iter  70 value 236.746640
iter  80 value 236.742647
iter  90 value 236.740335
iter 100 value 236.738862
final  value 236.738862 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 156.892459 
iter  10 value 138.523074
iter  20 value 138.512075
iter  30 value 138.507973
iter  40 value 138.461977
iter  50 value 137.532254
iter  60 value 136.662383
iter  70 value 136.660365
iter  80 value 136.658625
iter  90 value 136.657328
iter 100 value 136.656739
final  value 136.656739 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 239.205223 
iter  10 value 191.424318
iter  20 value 191.367317
final  value 191.364953 
converged
Fitting Repeat 5 

# weights:  188
initial  value 239.798944 
iter  10 value 164.809454
iter  20 value 164.780286
iter  30 value 164.779751
final  value 164.779728 
converged
Fitting Repeat 1 

# weights:  155
initial  value 329.026169 
iter  10 value 137.796085
iter  20 value 137.737019
iter  20 value 137.737019
iter  20 value 137.737019
final  value 137.737019 
converged
Fitting Repeat 2 

# weights:  155
initial  value 275.972366 
iter  10 value 83.660324
iter  20 value 83.645781
final  value 83.645776 
converged
Fitting Repeat 3 

# weights:  155
initial  value 420.565821 
iter  10 value 210.400702
iter  20 value 209.431524
iter  30 value 209.428418
iter  30 value 209.428417
iter  30 value 209.428417
final  value 209.428417 
converged
Fitting Repeat 4 

# weights:  155
initial  value 412.587776 
iter  10 value 192.777814
final  value 192.776406 
converged
Fitting Repeat 5 

# weights:  155
initial  value 349.117046 
iter  10 value 165.206811
iter  20 value 165.135662
iter  20 value 165.135660
iter  20 value 165.135660
final  value 165.135660 
converged
Fitting Repeat 1 

# weights:  45
initial  value 246.566986 
iter  10 value 163.607532
iter  20 value 163.550698
final  value 163.550676 
converged
Fitting Repeat 2 

# weights:  45
initial  value 239.629436 
iter  10 value 163.607220
iter  20 value 163.550783
final  value 163.550676 
converged
Fitting Repeat 3 

# weights:  45
initial  value 226.914457 
iter  10 value 163.729673
iter  20 value 163.550951
final  value 163.550676 
converged
Fitting Repeat 4 

# weights:  45
initial  value 227.967457 
iter  10 value 163.670125
iter  20 value 163.550703
final  value 163.550678 
converged
Fitting Repeat 5 

# weights:  45
initial  value 218.019929 
iter  10 value 163.569847
iter  20 value 163.550681
iter  20 value 163.550680
iter  20 value 163.550679
final  value 163.550679 
converged
Fitting Repeat 1 

# weights:  133
initial  value 204.922273 
iter  10 value 164.140975
iter  20 value 163.879947
iter  30 value 163.862411
iter  40 value 163.783151
iter  50 value 162.799704
iter  60 value 162.281601
iter  70 value 162.276684
iter  80 value 161.205013
iter  90 value 161.157657
iter 100 value 161.140262
final  value 161.140262 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  133
initial  value 229.381702 
iter  10 value 174.347338
iter  20 value 174.000107
iter  30 value 172.740738
iter  40 value 172.528098
iter  50 value 171.426714
iter  60 value 171.405671
iter  70 value 171.399285
iter  80 value 171.394969
iter  90 value 171.392334
iter 100 value 171.388272
final  value 171.388272 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 86.059952 
iter  10 value 82.686441
iter  20 value 80.088768
iter  30 value 79.809637
iter  40 value 78.711114
iter  50 value 78.694058
iter  60 value 78.692430
iter  70 value 78.691367
iter  80 value 78.676156
iter  90 value 78.664950
iter 100 value 78.662466
final  value 78.662466 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  133
initial  value 78.673300 
iter  10 value 63.261241
iter  20 value 63.204624
iter  30 value 61.117013
iter  40 value 60.950361
iter  50 value 60.945883
iter  60 value 60.945332
iter  70 value 60.944803
iter  80 value 60.944691
final  value 60.944657 
converged
Fitting Repeat 5 

# weights:  133
initial  value 324.954912 
iter  10 value 245.268550
iter  20 value 244.656644
iter  30 value 242.460567
iter  40 value 242.314903
iter  50 value 242.284926
iter  60 value 242.281878
iter  70 value 242.281222
iter  80 value 242.278209
iter  90 value 242.262327
iter 100 value 242.250423
final  value 242.250423 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 340.733335 
iter  10 value 165.606432
iter  20 value 165.532014
final  value 165.531982 
converged
Fitting Repeat 2 

# weights:  89
initial  value 228.856801 
iter  10 value 96.339055
final  value 96.339004 
converged
Fitting Repeat 3 

# weights:  89
initial  value 240.074167 
iter  10 value 99.658361
iter  20 value 99.576568
final  value 99.576555 
converged
Fitting Repeat 4 

# weights:  89
initial  value 362.077676 
iter  10 value 179.332297
final  value 179.287111 
converged
Fitting Repeat 5 

# weights:  89
initial  value 277.029481 
iter  10 value 136.321109
iter  20 value 136.266193
final  value 136.266174 
converged
Fitting Repeat 1 

# weights:  78
initial  value 72.184706 
iter  10 value 38.900123
final  value 38.900117 
converged
Fitting Repeat 2 

# weights:  78
initial  value 220.571035 
iter  10 value 186.395500
final  value 186.395455 
converged
Fitting Repeat 3 

# weights:  78
initial  value 126.944900 
iter  10 value 101.351759
final  value 101.351673 
converged
Fitting Repeat 4 

# weights:  78
initial  value 140.204227 
iter  10 value 113.303829
final  value 113.303690 
converged
Fitting Repeat 5 

# weights:  78
initial  value 261.271670 
iter  10 value 182.656629
final  value 182.656370 
converged
Fitting Repeat 1 

# weights:  199
initial  value 224.039283 
iter  10 value 175.276055
final  value 175.274099 
converged
Fitting Repeat 2 

# weights:  199
initial  value 245.898150 
iter  10 value 191.555984
final  value 191.554169 
converged
Fitting Repeat 3 

# weights:  199
initial  value 156.899229 
final  value 136.804717 
converged
Fitting Repeat 4 

# weights:  199
initial  value 220.839881 
iter  10 value 194.941634
final  value 194.940517 
converged
Fitting Repeat 5 

# weights:  199
initial  value 79.916695 
iter  10 value 44.696476
final  value 44.696333 
converged
Fitting Repeat 1 

# weights:  210
initial  value 258.046137 
iter  10 value 214.690190
iter  20 value 214.640146
iter  30 value 214.635756
iter  40 value 214.475584
iter  50 value 214.247761
iter  60 value 214.246968
final  value 214.246954 
converged
Fitting Repeat 2 

# weights:  210
initial  value 245.810501 
iter  10 value 226.055645
iter  20 value 226.019663
iter  30 value 226.018718
final  value 226.018693 
converged
Fitting Repeat 3 

# weights:  210
initial  value 176.140134 
iter  10 value 150.337068
iter  20 value 150.315135
final  value 150.314750 
converged
Fitting Repeat 4 

# weights:  210
initial  value 210.045185 
iter  10 value 172.357143
iter  20 value 172.320261
iter  30 value 172.318148
final  value 172.318122 
converged
Fitting Repeat 5 

# weights:  210
initial  value 151.768214 
iter  10 value 123.182712
iter  20 value 122.088422
iter  30 value 122.083318
final  value 122.082994 
converged
Fitting Repeat 1 

# weights:  177
initial  value 193.019977 
iter  10 value 145.254469
iter  20 value 144.893431
iter  30 value 144.874330
iter  40 value 144.834535
iter  50 value 143.952042
iter  60 value 143.226246
iter  70 value 143.207114
iter  80 value 142.728964
iter  90 value 142.715074
iter 100 value 142.680911
final  value 142.680911 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  177
initial  value 159.890987 
iter  10 value 145.044783
iter  20 value 144.889040
iter  30 value 143.750878
iter  40 value 143.237545
iter  50 value 143.217341
iter  60 value 143.215773
iter  70 value 143.140862
iter  80 value 142.723574
iter  90 value 142.721473
iter 100 value 142.717185
final  value 142.717185 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  177
initial  value 191.282821 
iter  10 value 145.355525
iter  20 value 144.892290
iter  30 value 143.337209
iter  40 value 143.224366
iter  50 value 143.052806
iter  60 value 142.707389
iter  70 value 142.679653
iter  80 value 142.651456
iter  90 value 142.641841
iter 100 value 142.639575
final  value 142.639575 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  177
initial  value 215.896984 
iter  10 value 145.031574
iter  20 value 144.892970
iter  30 value 144.880660
iter  40 value 144.861696
iter  50 value 144.670030
iter  60 value 143.232837
iter  70 value 143.205424
iter  80 value 143.078343
iter  90 value 142.666553
iter 100 value 142.656712
final  value 142.656712 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  177
initial  value 193.681848 
iter  10 value 145.235202
iter  20 value 144.892827
iter  30 value 144.829564
iter  40 value 143.347441
iter  50 value 143.224508
iter  60 value 142.801773
iter  70 value 142.717447
iter  80 value 142.693680
iter  90 value 142.655483
iter 100 value 142.649067
final  value 142.649067 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  78
initial  value 248.151856 
iter  10 value 145.448324
iter  20 value 145.414812
iter  20 value 145.414811
iter  20 value 145.414810
final  value 145.414810 
converged
Fitting Repeat 2 

# weights:  78
initial  value 360.502078 
iter  10 value 243.655218
iter  20 value 243.541385
iter  20 value 243.541385
iter  20 value 243.541384
final  value 243.541384 
converged
Fitting Repeat 3 

# weights:  78
initial  value 310.119975 
iter  10 value 192.615405
final  value 192.607413 
converged
Fitting Repeat 4 

# weights:  78
initial  value 328.686419 
iter  10 value 220.784949
iter  20 value 220.597971
final  value 220.597726 
converged
Fitting Repeat 5 

# weights:  78
initial  value 175.082749 
iter  10 value 82.480020
iter  20 value 82.425604
iter  20 value 82.425604
iter  20 value 82.425604
final  value 82.425604 
converged
Fitting Repeat 1 

# weights:  111
initial  value 183.640446 
iter  10 value 145.485021
iter  20 value 144.955811
iter  30 value 144.774544
iter  40 value 143.418015
iter  50 value 143.400738
iter  60 value 142.951522
iter  70 value 142.946680
iter  80 value 142.943565
iter  90 value 142.915228
iter 100 value 142.864544
final  value 142.864544 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  111
initial  value 207.906268 
iter  10 value 145.412203
iter  20 value 144.956517
iter  30 value 144.340861
iter  40 value 143.410360
iter  50 value 143.400910
iter  60 value 142.957655
iter  70 value 142.939346
iter  80 value 142.878052
iter  90 value 142.863439
iter 100 value 142.862992
final  value 142.862992 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  111
initial  value 204.579148 
iter  10 value 145.500438
iter  20 value 144.953825
iter  30 value 144.753779
iter  40 value 143.452946
iter  50 value 143.400986
iter  60 value 143.307598
iter  70 value 142.948604
iter  80 value 142.947801
iter  90 value 142.947711
iter 100 value 142.945242
final  value 142.945242 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  111
initial  value 164.231950 
iter  10 value 145.180278
iter  20 value 144.948844
iter  30 value 143.757458
iter  40 value 143.232801
iter  50 value 142.947023
iter  60 value 142.939208
iter  70 value 142.880741
iter  80 value 142.855229
iter  90 value 142.849727
iter 100 value 142.847324
final  value 142.847324 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  111
initial  value 156.109217 
iter  10 value 144.998240
iter  20 value 144.930313
iter  30 value 143.426136
iter  40 value 143.399745
iter  50 value 142.946646
iter  60 value 142.939695
iter  70 value 142.878058
iter  80 value 142.850420
iter  90 value 142.847644
iter 100 value 142.846287
final  value 142.846287 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  166
initial  value 235.304553 
iter  10 value 163.124333
final  value 163.120353 
converged
Fitting Repeat 2 

# weights:  166
initial  value 218.483166 
iter  10 value 162.691739
final  value 162.685265 
converged
Fitting Repeat 3 

# weights:  166
initial  value 103.693541 
iter  10 value 75.124445
iter  10 value 75.124445
iter  10 value 75.124445
final  value 75.124445 
converged
Fitting Repeat 4 

# weights:  166
initial  value 192.474389 
iter  10 value 164.750337
final  value 164.750327 
converged
Fitting Repeat 5 

# weights:  166
initial  value 205.612260 
iter  10 value 163.771647
final  value 163.771496 
converged
Fitting Repeat 1 

# weights:  188
initial  value 74.792271 
iter  10 value 59.625033
iter  20 value 59.529846
iter  30 value 59.528302
iter  40 value 59.527483
iter  50 value 59.511036
iter  60 value 58.547991
iter  70 value 58.531626
iter  80 value 58.530184
final  value 58.530156 
converged
Fitting Repeat 2 

# weights:  188
initial  value 135.622370 
iter  10 value 104.742298
iter  20 value 104.467621
iter  30 value 104.456331
iter  40 value 103.003591
iter  50 value 102.986847
iter  60 value 102.986293
iter  70 value 102.986036
iter  80 value 102.963958
iter  90 value 102.942800
iter 100 value 102.935936
final  value 102.935936 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 338.353877 
iter  10 value 264.178815
iter  20 value 263.181718
iter  30 value 263.164651
iter  40 value 263.163804
final  value 263.163723 
converged
Fitting Repeat 4 

# weights:  188
initial  value 191.705074 
iter  10 value 171.465742
iter  20 value 171.198113
iter  30 value 171.184200
iter  40 value 171.132916
iter  50 value 170.714457
iter  60 value 170.617033
iter  70 value 170.045237
iter  80 value 170.017840
iter  90 value 169.995577
iter 100 value 169.990591
final  value 169.990591 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 262.466042 
iter  10 value 198.963293
iter  20 value 198.632489
iter  30 value 198.581866
iter  40 value 196.547235
iter  50 value 196.222934
iter  60 value 195.811586
iter  70 value 195.780636
iter  80 value 195.766973
iter  90 value 195.762725
iter 100 value 195.761758
final  value 195.761758 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  78
initial  value 192.667076 
iter  10 value 144.843593
final  value 144.843402 
converged
Fitting Repeat 2 

# weights:  78
initial  value 180.011579 
iter  10 value 144.843508
final  value 144.843060 
converged
Fitting Repeat 3 

# weights:  78
initial  value 187.283521 
iter  10 value 144.843531
final  value 144.843521 
converged
Fitting Repeat 4 

# weights:  78
initial  value 190.369582 
iter  10 value 144.843299
final  value 144.843283 
converged
Fitting Repeat 5 

# weights:  78
initial  value 177.810325 
iter  10 value 144.843763
final  value 144.843758 
converged
Fitting Repeat 1 

# weights:  221
initial  value 192.146035 
iter  10 value 144.847861
final  value 144.843093 
converged
Fitting Repeat 2 

# weights:  221
initial  value 180.406231 
iter  10 value 144.848431
final  value 144.842923 
converged
Fitting Repeat 3 

# weights:  221
initial  value 155.781734 
iter  10 value 144.842924
final  value 144.842836 
converged
Fitting Repeat 4 

# weights:  221
initial  value 154.769938 
iter  10 value 144.842893
final  value 144.842749 
converged
Fitting Repeat 5 

# weights:  221
initial  value 203.914271 
iter  10 value 144.846600
final  value 144.843062 
converged
Fitting Repeat 1 

# weights:  12
initial  value 164.731327 
iter  10 value 145.009783
iter  20 value 144.752565
iter  30 value 143.470773
iter  40 value 143.386199
iter  50 value 142.856711
iter  60 value 142.842929
iter  70 value 142.841796
iter  80 value 142.841749
iter  80 value 142.841748
iter  80 value 142.841748
final  value 142.841748 
converged
Fitting Repeat 2 

# weights:  12
initial  value 191.328363 
iter  10 value 145.092736
iter  20 value 145.020566
iter  30 value 144.388989
iter  40 value 143.467313
iter  50 value 143.388860
iter  60 value 142.843221
iter  70 value 142.841773
final  value 142.841748 
converged
Fitting Repeat 3 

# weights:  12
initial  value 186.462536 
iter  10 value 145.009821
iter  20 value 144.926801
iter  30 value 144.531020
iter  40 value 143.443134
iter  50 value 143.261075
iter  60 value 142.842501
iter  70 value 142.841786
final  value 142.841765 
converged
Fitting Repeat 4 

# weights:  12
initial  value 185.623542 
iter  10 value 145.015237
iter  20 value 144.965438
iter  30 value 144.899635
iter  40 value 144.543858
iter  50 value 143.419005
iter  60 value 142.992249
iter  70 value 142.842160
iter  80 value 142.841760
final  value 142.841751 
converged
Fitting Repeat 5 

# weights:  12
initial  value 194.760595 
iter  10 value 145.158184
iter  20 value 145.056632
iter  30 value 144.986461
iter  40 value 144.966319
iter  50 value 144.909396
iter  60 value 144.431430
iter  70 value 143.430571
iter  80 value 143.291849
iter  90 value 142.849526
iter 100 value 142.841857
final  value 142.841857 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  67
initial  value 219.675135 
iter  10 value 171.707410
iter  20 value 170.970718
iter  30 value 170.074171
final  value 170.073907 
converged
Fitting Repeat 2 

# weights:  67
initial  value 129.993411 
iter  10 value 95.223770
iter  20 value 93.917337
iter  30 value 93.908332
final  value 93.908302 
converged
Fitting Repeat 3 

# weights:  67
initial  value 183.423702 
iter  10 value 135.312739
iter  20 value 133.827061
iter  30 value 133.823846
final  value 133.823801 
converged
Fitting Repeat 4 

# weights:  67
initial  value 137.860218 
iter  10 value 115.686074
iter  20 value 114.514345
iter  30 value 113.874785
iter  40 value 113.872925
final  value 113.872889 
converged
Fitting Repeat 5 

# weights:  67
initial  value 161.964587 
iter  10 value 125.222249
iter  20 value 125.171405
final  value 125.171319 
converged
Fitting Repeat 1 

# weights:  78
initial  value 187.349713 
iter  10 value 150.812491
final  value 150.812289 
converged
Fitting Repeat 2 

# weights:  78
initial  value 115.485073 
final  value 109.010390 
converged
Fitting Repeat 3 

# weights:  78
initial  value 168.460776 
iter  10 value 116.971722
final  value 116.971695 
converged
Fitting Repeat 4 

# weights:  78
initial  value 122.165194 
iter  10 value 80.876140
iter  20 value 80.874709
iter  30 value 80.874690
iter  40 value 80.874670
iter  50 value 80.874649
iter  60 value 80.874625
iter  70 value 80.874600
iter  80 value 80.874571
iter  90 value 80.874540
iter 100 value 80.874506
final  value 80.874506 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  78
initial  value 199.058052 
iter  10 value 170.647914
final  value 170.647724 
converged
Fitting Repeat 1 

# weights:  34
initial  value 188.321395 
iter  10 value 144.849189
iter  20 value 144.844357
iter  30 value 144.841297
iter  40 value 144.835270
iter  50 value 144.820566
iter  60 value 144.774901
iter  70 value 144.403113
iter  80 value 143.004018
iter  90 value 142.985690
iter 100 value 142.984374
final  value 142.984374 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  34
initial  value 196.858694 
iter  10 value 144.849283
iter  20 value 144.842919
iter  30 value 143.811201
iter  40 value 143.003050
iter  50 value 142.992738
iter  60 value 142.990640
iter  70 value 142.987171
iter  80 value 142.985317
iter  90 value 142.984486
iter 100 value 142.984356
final  value 142.984356 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  34
initial  value 167.202165 
iter  10 value 144.846531
iter  20 value 144.843141
iter  30 value 144.842435
iter  40 value 144.841566
iter  50 value 144.840476
iter  60 value 144.839078
iter  70 value 144.837235
iter  80 value 144.834712
iter  90 value 144.831075
iter 100 value 144.825405
final  value 144.825405 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  34
initial  value 201.383387 
iter  10 value 144.852633
iter  20 value 144.849042
iter  30 value 144.374377
iter  40 value 143.755904
iter  50 value 143.731789
iter  60 value 143.426243
iter  70 value 143.181739
iter  80 value 143.171439
iter  90 value 143.131608
iter 100 value 143.016663
final  value 143.016663 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  34
initial  value 170.612935 
iter  10 value 144.849678
iter  20 value 144.846898
iter  30 value 143.240262
iter  40 value 143.021902
iter  50 value 143.003147
iter  60 value 142.993405
iter  70 value 142.988298
iter  80 value 142.917730
iter  90 value 142.443640
iter 100 value 142.417081
final  value 142.417081 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  34
initial  value 42.386436 
iter  10 value 31.953443
iter  20 value 31.949974
final  value 31.949973 
converged
Fitting Repeat 2 

# weights:  34
initial  value 179.986834 
iter  10 value 154.350220
iter  20 value 154.344978
final  value 154.344971 
converged
Fitting Repeat 3 

# weights:  34
initial  value 97.389245 
iter  10 value 70.466306
iter  20 value 70.378294
final  value 70.378275 
converged
Fitting Repeat 4 

# weights:  34
initial  value 262.874040 
iter  10 value 209.194370
final  value 209.181657 
converged
Fitting Repeat 5 

# weights:  34
initial  value 274.513789 
iter  10 value 222.580816
iter  20 value 222.521453
final  value 222.521388 
converged
Fitting Repeat 1 

# weights:  188
initial  value 172.998166 
iter  10 value 133.402056
iter  20 value 133.368120
iter  30 value 133.363683
iter  40 value 133.361408
iter  50 value 133.355993
iter  60 value 133.338894
iter  70 value 133.268526
iter  80 value 132.305455
iter  90 value 132.278634
iter 100 value 132.276454
final  value 132.276454 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 54.195161 
iter  10 value 32.522207
iter  20 value 32.512900
iter  30 value 30.622211
iter  40 value 30.602288
iter  50 value 30.600530
iter  60 value 30.599384
iter  70 value 30.598264
iter  80 value 30.597215
iter  90 value 30.595543
iter 100 value 30.594792
final  value 30.594792 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 146.749224 
iter  10 value 126.849175
iter  20 value 126.835756
iter  30 value 124.986071
iter  40 value 124.550564
iter  50 value 124.536142
iter  60 value 124.426395
iter  70 value 124.417102
iter  80 value 124.415043
iter  90 value 124.411203
iter 100 value 124.410276
final  value 124.410276 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 162.481059 
iter  10 value 110.046068
iter  20 value 110.013469
iter  30 value 108.302676
iter  40 value 107.474007
iter  50 value 107.472276
iter  60 value 107.471584
iter  70 value 107.471090
iter  80 value 107.470726
iter  90 value 107.470579
iter 100 value 107.470396
final  value 107.470396 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 200.006070 
iter  10 value 168.831149
iter  20 value 168.795687
iter  30 value 168.792801
iter  40 value 168.792324
iter  50 value 168.791619
iter  60 value 168.790441
iter  70 value 168.788027
iter  80 value 168.780723
iter  90 value 168.710977
iter 100 value 166.618132
final  value 166.618132 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  155
initial  value 206.571174 
iter  10 value 41.583494
iter  20 value 41.491679
final  value 41.491646 
converged
Fitting Repeat 2 

# weights:  155
initial  value 244.186229 
iter  10 value 51.980838
iter  20 value 51.921999
final  value 51.921993 
converged
Fitting Repeat 3 

# weights:  155
initial  value 357.547021 
iter  10 value 148.632519
iter  20 value 148.544623
final  value 148.544579 
converged
Fitting Repeat 4 

# weights:  155
initial  value 243.098229 
iter  10 value 92.806687
iter  20 value 92.256732
final  value 92.256491 
converged
Fitting Repeat 5 

# weights:  155
initial  value 290.872283 
iter  10 value 79.866212
iter  20 value 79.683566
final  value 79.683522 
converged
Fitting Repeat 1 

# weights:  45
initial  value 146.341578 
iter  10 value 102.504794
final  value 102.501587 
converged
Fitting Repeat 2 

# weights:  45
initial  value 170.589673 
iter  10 value 102.652876
iter  20 value 102.501621
final  value 102.501586 
converged
Fitting Repeat 3 

# weights:  45
initial  value 158.536507 
iter  10 value 102.513337
final  value 102.501586 
converged
Fitting Repeat 4 

# weights:  45
initial  value 162.611441 
iter  10 value 102.519491
iter  20 value 102.501589
final  value 102.501587 
converged
Fitting Repeat 5 

# weights:  45
initial  value 163.474498 
iter  10 value 102.574980
iter  20 value 102.501589
final  value 102.501586 
converged
Fitting Repeat 1 

# weights:  133
initial  value 96.897413 
iter  10 value 81.244717
iter  20 value 78.612020
iter  30 value 77.787887
iter  40 value 77.747983
iter  50 value 77.704366
iter  60 value 77.676141
iter  70 value 77.673741
iter  80 value 77.673178
iter  90 value 77.672968
iter 100 value 77.672900
final  value 77.672900 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  133
initial  value 164.996129 
iter  10 value 136.668798
iter  20 value 136.397708
iter  30 value 134.450681
iter  40 value 134.211632
iter  50 value 134.035411
iter  60 value 134.032613
iter  70 value 134.031224
iter  80 value 134.015648
iter  90 value 134.003815
iter 100 value 133.999468
final  value 133.999468 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  133
initial  value 63.430086 
iter  10 value 58.297409
iter  20 value 54.357609
iter  30 value 54.134349
iter  40 value 54.118419
iter  50 value 54.113695
iter  60 value 54.112299
iter  70 value 54.111440
iter  80 value 54.110145
iter  90 value 54.108774
iter 100 value 54.108363
final  value 54.108363 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  133
initial  value 157.524982 
iter  10 value 117.184520
iter  20 value 116.998427
iter  30 value 116.827393
iter  40 value 114.182384
iter  50 value 113.970962
iter  60 value 113.946798
iter  70 value 113.930895
iter  80 value 113.925762
iter  90 value 113.924840
iter 100 value 113.924463
final  value 113.924463 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  133
initial  value 57.795199 
iter  10 value 36.140425
iter  20 value 35.938651
iter  30 value 34.865762
iter  40 value 33.996705
iter  50 value 33.578132
iter  60 value 33.557549
iter  70 value 33.538178
iter  80 value 33.520209
iter  90 value 33.513355
iter 100 value 33.511456
final  value 33.511456 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  89
initial  value 268.602066 
iter  10 value 118.324450
iter  20 value 118.240062
final  value 118.240055 
converged
Fitting Repeat 2 

# weights:  89
initial  value 237.446120 
iter  10 value 62.141262
final  value 62.138451 
converged
Fitting Repeat 3 

# weights:  89
initial  value 260.017835 
iter  10 value 98.078065
final  value 98.076696 
converged
Fitting Repeat 4 

# weights:  89
initial  value 185.552031 
iter  10 value 55.736611
final  value 55.720135 
converged
Fitting Repeat 5 

# weights:  89
initial  value 309.261496 
iter  10 value 150.140115
iter  20 value 149.985121
iter  20 value 149.985121
iter  20 value 149.985121
final  value 149.985121 
converged
Fitting Repeat 1 

# weights:  78
initial  value 48.977412 
iter  10 value 37.636696
final  value 37.636694 
converged
Fitting Repeat 2 

# weights:  78
initial  value 222.796490 
iter  10 value 190.093917
final  value 190.092072 
converged
Fitting Repeat 3 

# weights:  78
initial  value 129.777735 
iter  10 value 108.605564
final  value 108.604898 
converged
Fitting Repeat 4 

# weights:  78
initial  value 225.921173 
iter  10 value 175.364700
final  value 175.361999 
converged
Fitting Repeat 5 

# weights:  78
initial  value 139.383072 
iter  10 value 88.068159
final  value 88.068134 
converged
Fitting Repeat 1 

# weights:  199
initial  value 178.752456 
iter  10 value 136.113186
final  value 136.111945 
converged
Fitting Repeat 2 

# weights:  199
initial  value 124.925589 
iter  10 value 82.769989
final  value 82.769631 
converged
Fitting Repeat 3 

# weights:  199
initial  value 144.871315 
final  value 84.633253 
converged
Fitting Repeat 4 

# weights:  199
initial  value 234.550780 
final  value 162.598992 
converged
Fitting Repeat 5 

# weights:  199
initial  value 84.411175 
iter  10 value 51.825281
final  value 51.825228 
converged
Fitting Repeat 1 

# weights:  210
initial  value 142.766089 
iter  10 value 100.186991
iter  20 value 99.309139
iter  30 value 97.071460
iter  40 value 96.628311
iter  50 value 96.347601
iter  60 value 96.294972
iter  70 value 96.209932
iter  80 value 96.200786
iter  90 value 96.200194
iter 100 value 96.185364
final  value 96.185364 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  210
initial  value 192.034104 
iter  10 value 136.403484
iter  20 value 135.204221
iter  30 value 134.843701
iter  40 value 134.002521
iter  50 value 133.739612
iter  60 value 133.633597
iter  70 value 133.550202
iter  80 value 133.535461
iter  90 value 133.529768
iter 100 value 133.527056
final  value 133.527056 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  210
initial  value 46.543706 
iter  10 value 40.199220
iter  20 value 40.162985
iter  30 value 40.161414
final  value 40.161392 
converged
Fitting Repeat 4 

# weights:  210
initial  value 127.206029 
iter  10 value 101.271803
iter  20 value 100.294805
iter  30 value 97.460281
iter  40 value 97.219109
iter  50 value 97.171557
iter  60 value 97.008606
iter  70 value 96.965409
iter  80 value 96.957416
iter  90 value 96.953400
iter 100 value 96.950101
final  value 96.950101 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  210
initial  value 88.855042 
iter  10 value 65.232344
iter  20 value 64.479053
iter  30 value 64.444502
iter  40 value 63.978982
iter  50 value 62.445204
iter  60 value 62.411156
iter  70 value 62.240350
iter  80 value 62.219060
iter  90 value 62.215249
iter 100 value 62.211903
final  value 62.211903 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  177
initial  value 117.907184 
iter  10 value 87.237478
iter  20 value 86.994887
iter  30 value 86.961581
iter  40 value 86.666588
iter  50 value 85.302329
iter  60 value 85.255933
iter  70 value 85.145400
iter  80 value 85.118162
iter  90 value 85.082059
iter 100 value 85.070045
final  value 85.070045 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  177
initial  value 124.959477 
iter  10 value 87.207162
iter  20 value 86.997293
iter  30 value 86.784776
iter  40 value 85.432486
iter  50 value 85.209755
iter  60 value 85.156718
iter  70 value 85.129234
iter  80 value 85.095501
iter  90 value 85.074581
iter 100 value 85.062697
final  value 85.062697 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  177
initial  value 104.876257 
iter  10 value 87.111028
iter  20 value 86.986616
iter  30 value 86.857026
iter  40 value 85.681773
iter  50 value 85.287907
iter  60 value 85.202256
iter  70 value 85.137929
iter  80 value 85.101207
iter  90 value 85.077160
iter 100 value 85.067449
final  value 85.067449 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  177
initial  value 116.580872 
iter  10 value 87.184206
iter  20 value 86.994495
iter  30 value 85.830006
iter  40 value 85.285071
iter  50 value 85.186754
iter  60 value 85.160006
iter  70 value 85.155430
iter  80 value 85.133505
iter  90 value 85.079036
iter 100 value 85.055347
final  value 85.055347 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  177
initial  value 104.282549 
iter  10 value 87.100561
iter  20 value 86.982145
iter  30 value 86.908392
iter  40 value 86.147242
iter  50 value 85.289565
iter  60 value 85.254054
iter  70 value 85.139629
iter  80 value 85.108876
iter  90 value 85.086423
iter 100 value 85.079343
final  value 85.079343 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  78
initial  value 224.365680 
iter  10 value 113.461200
final  value 113.440581 
converged
Fitting Repeat 2 

# weights:  78
initial  value 310.747346 
iter  10 value 166.319074
iter  20 value 165.727072
final  value 165.726782 
converged
Fitting Repeat 3 

# weights:  78
initial  value 142.286378 
iter  10 value 57.147601
final  value 57.122926 
converged
Fitting Repeat 4 

# weights:  78
initial  value 197.075018 
iter  10 value 101.259647
iter  20 value 101.236755
final  value 101.236741 
converged
Fitting Repeat 5 

# weights:  78
initial  value 272.209233 
iter  10 value 163.926155
iter  20 value 163.876859
final  value 163.876820 
converged
Fitting Repeat 1 

# weights:  111
initial  value 131.899533 
iter  10 value 87.276796
iter  20 value 87.029693
iter  30 value 86.648605
iter  40 value 85.598580
iter  50 value 85.474182
iter  60 value 85.386229
iter  70 value 85.382412
iter  80 value 85.350469
iter  90 value 85.301234
iter 100 value 85.289774
final  value 85.289774 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  111
initial  value 103.970094 
iter  10 value 87.196728
iter  20 value 86.971716
iter  30 value 85.639452
iter  40 value 85.427292
iter  50 value 85.386054
iter  60 value 85.384301
iter  70 value 85.350348
iter  80 value 85.293004
iter  90 value 85.288249
iter 100 value 85.282907
final  value 85.282907 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  111
initial  value 113.239734 
iter  10 value 87.287160
iter  20 value 87.033525
iter  30 value 86.722903
iter  40 value 85.914002
iter  50 value 85.447146
iter  60 value 85.385717
iter  70 value 85.384113
iter  80 value 85.355053
iter  90 value 85.284825
iter 100 value 85.274367
final  value 85.274367 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  111
initial  value 131.533195 
iter  10 value 87.272166
iter  20 value 87.034971
iter  30 value 86.777185
iter  40 value 85.963415
iter  50 value 85.437087
iter  60 value 85.385896
iter  70 value 85.385082
iter  80 value 85.380847
iter  90 value 85.315542
iter 100 value 85.275565
final  value 85.275565 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  111
initial  value 97.327365 
iter  10 value 87.096279
iter  20 value 86.975238
iter  30 value 85.506901
iter  40 value 85.412560
iter  50 value 85.386079
iter  60 value 85.382768
iter  70 value 85.326836
iter  80 value 85.289597
iter  90 value 85.273683
iter 100 value 85.270342
final  value 85.270342 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  166
initial  value 99.595169 
iter  10 value 51.620481
final  value 51.620393 
converged
Fitting Repeat 2 

# weights:  166
initial  value 179.829208 
iter  10 value 110.942858
final  value 110.935221 
converged
Fitting Repeat 3 

# weights:  166
initial  value 144.080682 
iter  10 value 79.547563
final  value 79.546050 
converged
Fitting Repeat 4 

# weights:  166
initial  value 57.115527 
iter  10 value 22.869513
final  value 22.868686 
converged
Fitting Repeat 5 

# weights:  166
initial  value 144.789150 
iter  10 value 106.181944
final  value 106.181485 
converged
Fitting Repeat 1 

# weights:  188
initial  value 218.761374 
iter  10 value 194.141637
iter  20 value 193.891229
iter  30 value 192.316128
iter  40 value 191.872192
iter  50 value 191.837440
iter  60 value 191.830531
iter  70 value 191.824723
iter  80 value 191.782318
iter  90 value 191.755137
iter 100 value 191.746063
final  value 191.746063 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  188
initial  value 65.009268 
iter  10 value 29.306743
iter  20 value 29.258312
iter  30 value 28.031879
iter  40 value 27.909204
iter  50 value 27.892527
iter  60 value 27.876588
iter  70 value 27.721949
iter  80 value 27.644620
iter  90 value 27.021167
iter 100 value 26.921179
final  value 26.921179 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  188
initial  value 215.882244 
iter  10 value 144.058542
iter  20 value 143.928351
iter  30 value 142.294494
iter  40 value 141.969700
iter  50 value 141.886725
iter  60 value 141.840558
iter  70 value 141.808697
iter  80 value 141.800149
iter  90 value 141.798350
iter 100 value 141.796767
final  value 141.796767 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  188
initial  value 114.753189 
iter  10 value 89.247998
iter  20 value 88.945491
iter  30 value 88.922233
iter  40 value 88.705368
iter  50 value 87.577724
iter  60 value 87.502142
iter  70 value 87.477261
iter  80 value 87.460998
iter  90 value 87.458756
iter 100 value 87.457725
final  value 87.457725 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  188
initial  value 131.589442 
iter  10 value 97.825117
iter  20 value 97.508037
iter  30 value 96.925477
iter  40 value 94.819635
iter  50 value 94.483360
iter  60 value 94.431317
iter  70 value 94.398885
iter  80 value 94.392919
iter  90 value 94.392085
iter 100 value 94.391730
final  value 94.391730 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  78
initial  value 108.568766 
iter  10 value 86.950594
final  value 86.950589 
converged
Fitting Repeat 2 

# weights:  78
initial  value 125.304558 
iter  10 value 86.951078
final  value 86.951067 
converged
Fitting Repeat 3 

# weights:  78
initial  value 118.470640 
iter  10 value 86.950629
final  value 86.950615 
converged
Fitting Repeat 4 

# weights:  78
initial  value 118.786537 
iter  10 value 86.950368
final  value 86.950364 
converged
Fitting Repeat 5 

# weights:  78
initial  value 111.977625 
iter  10 value 86.949906
iter  20 value 86.947418
iter  30 value 85.887885
iter  40 value 85.186151
iter  50 value 85.179673
iter  60 value 84.860703
iter  70 value 84.847318
iter  80 value 84.846487
iter  90 value 84.843615
iter 100 value 84.843229
final  value 84.843229 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  221
initial  value 97.461323 
iter  10 value 86.950159
final  value 86.950076 
converged
Fitting Repeat 2 

# weights:  221
initial  value 118.994778 
iter  10 value 86.952030
final  value 86.950279 
converged
Fitting Repeat 3 

# weights:  221
initial  value 124.488289 
iter  10 value 86.952389
final  value 86.950299 
converged
Fitting Repeat 4 

# weights:  221
initial  value 134.152163 
iter  10 value 86.951425
final  value 86.950446 
converged
Fitting Repeat 5 

# weights:  221
initial  value 102.286528 
iter  10 value 86.950691
final  value 86.950050 
converged
Fitting Repeat 1 

# weights:  12
initial  value 118.789190 
iter  10 value 87.112100
iter  20 value 87.009927
iter  30 value 86.082165
iter  40 value 85.308813
iter  50 value 85.293654
iter  60 value 85.292959
iter  70 value 85.269481
iter  80 value 85.233810
iter  90 value 85.232709
iter 100 value 85.232527
final  value 85.232527 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  12
initial  value 109.460942 
iter  10 value 87.174689
iter  20 value 87.162279
iter  30 value 87.118685
iter  40 value 85.975241
iter  50 value 85.303302
iter  60 value 85.293724
iter  70 value 85.292198
iter  80 value 85.244975
iter  90 value 85.233535
iter 100 value 85.232635
final  value 85.232635 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  12
initial  value 114.829677 
iter  10 value 87.180740
iter  20 value 87.097208
iter  30 value 86.857448
iter  40 value 85.403570
iter  50 value 85.297037
iter  60 value 85.293352
iter  70 value 85.284844
iter  80 value 85.236984
iter  90 value 85.232940
iter 100 value 85.232513
final  value 85.232513 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  12
initial  value 118.982649 
iter  10 value 87.194534
iter  20 value 87.122917
iter  30 value 86.643934
iter  40 value 85.318298
iter  50 value 85.244794
iter  60 value 85.232855
iter  70 value 85.232548
iter  80 value 85.232490
final  value 85.232489 
converged
Fitting Repeat 5 

# weights:  12
initial  value 125.218731 
iter  10 value 87.139487
iter  20 value 87.070250
iter  30 value 86.967521
iter  40 value 86.088770
iter  50 value 85.299393
iter  60 value 85.292854
iter  70 value 85.243570
iter  80 value 85.233170
iter  90 value 85.232523
iter 100 value 85.232492
final  value 85.232492 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  67
initial  value 211.891544 
iter  10 value 147.426499
iter  20 value 146.144079
iter  30 value 145.566684
iter  40 value 145.396064
iter  50 value 145.395905
iter  50 value 145.395905
iter  50 value 145.395905
final  value 145.395905 
converged
Fitting Repeat 2 

# weights:  67
initial  value 66.854885 
iter  10 value 40.626142
iter  20 value 39.412755
iter  30 value 38.364614
iter  40 value 38.354533
iter  50 value 38.353184
iter  60 value 38.352564
final  value 38.352557 
converged
Fitting Repeat 3 

# weights:  67
initial  value 192.013438 
iter  10 value 141.510378
iter  20 value 140.842199
iter  30 value 140.588169
iter  40 value 140.587789
final  value 140.587785 
converged
Fitting Repeat 4 

# weights:  67
initial  value 124.179721 
iter  10 value 100.723515
iter  20 value 100.037998
iter  30 value 99.671608
iter  40 value 99.598919
iter  50 value 99.598005
final  value 99.598001 
converged
Fitting Repeat 5 

# weights:  67
initial  value 175.495441 
iter  10 value 141.052208
iter  20 value 139.673003
iter  30 value 139.669639
final  value 139.669526 
converged
Fitting Repeat 1 

# weights:  78
initial  value 84.752802 
iter  10 value 46.338695
iter  20 value 46.338005
final  value 46.337987 
converged
Fitting Repeat 2 

# weights:  78
initial  value 106.729363 
iter  10 value 84.808234
iter  20 value 84.808203
iter  30 value 84.808185
iter  40 value 84.808167
iter  50 value 84.808147
iter  60 value 84.808125
iter  70 value 84.808102
iter  80 value 84.808078
iter  90 value 84.808051
iter 100 value 84.808021
final  value 84.808021 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  78
initial  value 86.853882 
iter  10 value 53.110566
iter  20 value 53.108863
iter  30 value 53.108761
iter  40 value 53.108648
iter  50 value 53.108520
iter  60 value 53.108369
iter  70 value 53.108184
iter  80 value 53.107948
iter  90 value 53.107631
iter 100 value 53.107176
final  value 53.107176 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  78
initial  value 80.280282 
iter  10 value 54.145183
iter  20 value 54.142710
iter  30 value 54.142058
iter  40 value 54.140994
iter  50 value 54.139168
iter  60 value 54.135928
iter  70 value 54.130199
iter  80 value 54.120431
iter  90 value 54.103769
iter 100 value 54.071689
final  value 54.071689 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  78
initial  value 35.759694 
iter  10 value 18.718167
iter  20 value 18.714843
iter  30 value 18.713380
iter  40 value 18.710524
iter  50 value 18.703569
iter  60 value 18.684924
iter  70 value 18.649035
iter  80 value 18.561013
iter  90 value 16.984041
iter 100 value 16.632753
final  value 16.632753 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  34
initial  value 118.093008 
iter  10 value 86.960066
iter  20 value 86.954388
iter  30 value 86.950934
iter  40 value 86.947839
iter  50 value 86.943667
iter  60 value 86.937482
iter  70 value 86.926968
iter  80 value 86.904292
iter  90 value 86.818231
iter 100 value 85.032765
final  value 85.032765 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  34
initial  value 126.636361 
iter  10 value 86.956836
iter  20 value 86.952633
iter  30 value 86.952284
iter  40 value 86.951858
iter  50 value 86.951323
iter  60 value 86.950634
iter  70 value 86.949719
iter  80 value 86.948460
iter  90 value 86.946674
iter 100 value 86.944091
final  value 86.944091 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  34
initial  value 115.371765 
iter  10 value 86.958702
iter  20 value 86.954225
iter  30 value 86.947750
iter  40 value 86.946276
iter  50 value 86.943842
iter  60 value 86.938367
iter  70 value 86.923064
iter  80 value 86.887686
iter  90 value 86.766244
iter 100 value 86.122412
final  value 86.122412 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  34
initial  value 113.859626 
iter  10 value 86.955978
iter  20 value 86.953572
iter  30 value 86.953490
iter  40 value 86.953400
iter  50 value 86.953301
iter  60 value 86.953193
iter  70 value 86.953073
iter  80 value 86.952942
iter  90 value 86.952796
iter 100 value 86.952635
final  value 86.952635 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  34
initial  value 115.241760 
iter  10 value 86.961349
iter  20 value 86.956455
iter  30 value 86.953777
iter  40 value 86.953410
iter  50 value 86.953005
iter  60 value 86.952555
iter  70 value 86.952052
iter  80 value 86.951485
iter  90 value 86.950842
iter 100 value 86.950104
final  value 86.950104 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  34
initial  value 137.418851 
iter  10 value 95.685592
iter  20 value 95.483481
final  value 95.483318 
converged
Fitting Repeat 2 

# weights:  34
initial  value 70.122101 
iter  10 value 47.785926
iter  20 value 47.774839
final  value 47.774833 
converged
Fitting Repeat 3 

# weights:  34
initial  value 194.761702 
iter  10 value 168.589821
final  value 168.585309 
converged
Fitting Repeat 4 

# weights:  34
initial  value 53.280035 
iter  10 value 30.058101
iter  20 value 30.000860
final  value 30.000810 
converged
Fitting Repeat 5 

# weights:  34
initial  value 113.907789 
iter  10 value 87.931527
iter  20 value 87.910042
final  value 87.910038 
converged
Fitting Repeat 1 

# weights:  12
initial  value 196.324593 
iter  10 value 164.165286
iter  20 value 164.132598
iter  30 value 164.111706
iter  40 value 164.074435
iter  50 value 163.919442
iter  60 value 161.793591
iter  70 value 161.634531
iter  80 value 161.631451
iter  90 value 161.629310
iter 100 value 161.172473
final  value 161.172473 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  12
initial  value 200.283064 
iter  10 value 164.190077
iter  20 value 164.141431
iter  30 value 164.104551
iter  40 value 164.042246
iter  50 value 163.424382
iter  60 value 161.670807
iter  70 value 161.635828
iter  80 value 161.631692
iter  90 value 161.627352
iter 100 value 161.142508
final  value 161.142508 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  12
initial  value 202.640109 
iter  10 value 164.261945
iter  20 value 164.137137
iter  30 value 162.407109
iter  40 value 161.756110
iter  50 value 161.646114
iter  60 value 161.631814
iter  70 value 161.630742
iter  80 value 161.160438
iter  90 value 160.968516
iter 100 value 160.737451
final  value 160.737451 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  12
initial  value 209.825991 
iter  10 value 164.277079
iter  20 value 164.193841
iter  30 value 164.137080
iter  40 value 164.122271
iter  50 value 164.102976
iter  60 value 164.060345
iter  70 value 163.809951
iter  80 value 161.681245
iter  90 value 161.634758
iter 100 value 161.631613
final  value 161.631613 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  12
initial  value 208.692393 
iter  10 value 164.227445
iter  20 value 164.135309
iter  30 value 162.026202
iter  40 value 161.644594
iter  50 value 161.632258
iter  60 value 161.629255
iter  70 value 161.147921
iter  80 value 160.794365
iter  90 value 160.734935
iter 100 value 160.733403
final  value 160.733403 
stopped after 100 iterations
Model Averaged Neural Network 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 27, 26, 27 
Resampling results across tuning parameters:

  size  decay         bag    RMSE      Rsquared    MAE       Selected
   1    3.838894e-03  FALSE  1.961773  0.09151787  1.282413  *       
   3    9.672654e-05  FALSE  1.963368  0.02784432  1.285192          
   3    2.225735e+00   TRUE  2.018127  0.42368357  1.336651          
   4    6.357005e+00  FALSE  2.056177  0.47814364  1.372855          
   6    4.534382e-02   TRUE  1.968077  0.16569636  1.291256          
   7    2.423686e-05  FALSE  1.962551  0.07199631  1.284422          
   7    2.967943e-05   TRUE  1.963579  0.16209188  1.284221          
   7    2.932025e-01   TRUE  1.970136  0.34992612  1.291503          
   7    6.702776e+00   TRUE  2.045822  0.44596778  1.362795          
   8    9.897070e+00   TRUE  2.061534  0.43630281  1.377273          
  10    7.543968e-03  FALSE  1.964214  0.06615692  1.278812          
  12    3.057252e-03   TRUE  1.966709  0.13633196  1.290262          
  14    6.477716e+00   TRUE  2.023291  0.45335646  1.341278          
  15    9.459945e-01   TRUE  1.978321  0.41734668  1.298952          
  16    3.592783e-03  FALSE  1.962198  0.10031060  1.284044          
  17    2.885849e-04   TRUE  1.971053  0.02349633  1.295569          
  17    3.873557e-03   TRUE  1.970894  0.06677000  1.293198          
  18    1.341816e-05   TRUE  1.962427  0.15413683  1.284257          
  19    1.045569e-02   TRUE  1.963469  0.08431481  1.278856          
  20    4.060306e-05  FALSE  1.962428  0.11576940  1.284258          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 1, decay = 0.003838894 and
 bag = FALSE.
[1] "Sat Mar 10 04:33:53 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=9 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:34:04 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "bag"                            
Bagged MARS 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 27, 26, 27 
Resampling results across tuning parameters:

  degree  nprune  RMSE       Rsquared   MAE        Selected
  1       2       0.3652268  0.9383356  0.2848917          
  1       3       0.1650934  0.9935297  0.1119922  *       
  1       4       0.1706323  0.9930207  0.1132017          
  2       2       0.3714839  0.9337719  0.2859679          
  2       3       0.1669923  0.9934628  0.1110976          
  2       4       0.2264270  0.9908940  0.1146584          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nprune = 3 and degree = 1.
[1] "Sat Mar 10 04:34:30 2018"
Bagged MARS using gCV Pruning 

40 samples
 9 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 27, 26, 27 
Resampling results:

  RMSE       Rsquared   MAE      
  0.1721832  0.9923508  0.1264697

Tuning parameter 'degree' was held constant at a value of 1
[1] "Sat Mar 10 04:35:12 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :6     NA's   :6     NA's   :6    
Error : Stopping
In addition: There were 19 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :5     NA's   :5     NA's   :5    
Error : Stopping
In addition: There were 16 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                          "failed"                         
 [3] "Sat Mar 10 04:35:56 2018"        "poly aC1^1.3 + bC1^2.1 + dC1^.7"
 [5] "ignore"                          "none"                           
 [7] "YeoJohnson"                      "HOPPER"                         
 [9] "14th20hp3cv"                     "bam"                            
bartMachine initializing with 84 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 280.9/477.6MB
Iteration 200/1250  mem: 180.8/481.3MB
Iteration 300/1250  mem: 213.9/481.3MB
Iteration 400/1250  mem: 246/481.3MB
Iteration 500/1250  mem: 185.9/478.2MB
Iteration 600/1250  mem: 218.7/478.2MB
Iteration 700/1250  mem: 251.4/478.2MB
Iteration 800/1250  mem: 195.4/479.7MB
Iteration 900/1250  mem: 229.4/479.7MB
Iteration 1000/1250  mem: 261.7/479.7MB
Iteration 1100/1250  mem: 207.2/479.2MB
Iteration 1200/1250  mem: 240.5/479.2MB
done building BART in 0.832 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 72 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 290.7/479.2MB
Iteration 200/1250  mem: 225/481.3MB
Iteration 300/1250  mem: 255.5/481.3MB
Iteration 400/1250  mem: 287.6/481.3MB
Iteration 500/1250  mem: 319.7/481.3MB
Iteration 600/1250  mem: 257.9/481.3MB
Iteration 700/1250  mem: 288.1/481.3MB
Iteration 800/1250  mem: 320.2/481.3MB
Iteration 900/1250  mem: 259.7/483.9MB
Iteration 1000/1250  mem: 294.2/483.9MB
Iteration 1100/1250  mem: 326.6/483.9MB
Iteration 1200/1250  mem: 359.3/483.9MB
done building BART in 0.869 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 24 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 294.3/485.5MB
Iteration 200/1250  mem: 303.4/485.5MB
Iteration 300/1250  mem: 312.6/485.5MB
Iteration 400/1250  mem: 319.5/485.5MB
Iteration 500/1250  mem: 328.7/485.5MB
Iteration 600/1250  mem: 337.9/485.5MB
Iteration 700/1250  mem: 344.8/485.5MB
Iteration 800/1250  mem: 354/485.5MB
Iteration 900/1250  mem: 360.9/485.5MB
Iteration 1000/1250  mem: 370/485.5MB
Iteration 1100/1250  mem: 376.9/485.5MB
Iteration 1200/1250  mem: 282.1/486.5MB
done building BART in 0.226 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 61 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 312.5/486.5MB
Iteration 200/1250  mem: 333.2/486.5MB
Iteration 300/1250  mem: 356.1/486.5MB
Iteration 400/1250  mem: 376.8/486.5MB
Iteration 500/1250  mem: 292.9/485.5MB
Iteration 600/1250  mem: 315.1/485.5MB
Iteration 700/1250  mem: 337.3/485.5MB
Iteration 800/1250  mem: 357.4/485.5MB
Iteration 900/1250  mem: 379.6/485.5MB
Iteration 1000/1250  mem: 401.8/485.5MB
Iteration 1100/1250  mem: 322.3/486MB
Iteration 1200/1250  mem: 342.6/486MB
done building BART in 0.545 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 42 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 371/486MB
Iteration 200/1250  mem: 387.2/486MB
Iteration 300/1250  mem: 401.1/486MB
Iteration 400/1250  mem: 311/487.6MB
Iteration 500/1250  mem: 325.6/487.6MB
Iteration 600/1250  mem: 340.1/487.6MB
Iteration 700/1250  mem: 354.6/487.6MB
Iteration 800/1250  mem: 369.1/487.6MB
Iteration 900/1250  mem: 385.5/487.6MB
Iteration 1000/1250  mem: 400/487.6MB
Iteration 1100/1250  mem: 414.5/487.6MB
Iteration 1200/1250  mem: 323.3/487.1MB
done building BART in 0.396 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 40 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 351.4/487.1MB
Iteration 200/1250  mem: 370.5/487.1MB
Iteration 300/1250  mem: 387.1/487.1MB
Iteration 400/1250  mem: 408.6/487.1MB
Iteration 500/1250  mem: 427.6/487.1MB
Iteration 600/1250  mem: 38.5/493.4MB
Iteration 700/1250  mem: 57.2/493.4MB
Iteration 800/1250  mem: 75.8/493.4MB
Iteration 900/1250  mem: 94.4/493.4MB
Iteration 1000/1250  mem: 113.1/493.4MB
Iteration 1100/1250  mem: 134/493.4MB
Iteration 1200/1250  mem: 59.6/492.8MB
done building BART in 0.563 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 90 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 103/492.8MB
Iteration 200/1250  mem: 134.7/492.8MB
Iteration 300/1250  mem: 165.3/492.8MB
Iteration 400/1250  mem: 82.1/493.4MB
Iteration 500/1250  mem: 112.2/493.4MB
Iteration 600/1250  mem: 142.4/493.4MB
Iteration 700/1250  mem: 172.5/493.4MB
Iteration 800/1250  mem: 98/478.2MB
Iteration 900/1250  mem: 128.9/478.2MB
Iteration 1000/1250  mem: 157.8/478.2MB
Iteration 1100/1250  mem: 188.7/478.2MB
Iteration 1200/1250  mem: 114.1/492.8MB
done building BART in 0.799 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 96 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 168.7/492.8MB
Iteration 200/1250  mem: 207.5/492.8MB
Iteration 300/1250  mem: 131.2/491.8MB
Iteration 400/1250  mem: 170.2/491.8MB
Iteration 500/1250  mem: 211.3/491.8MB
Iteration 600/1250  mem: 149.2/494.4MB
Iteration 700/1250  mem: 191.5/494.4MB
Iteration 800/1250  mem: 233.9/494.4MB
Iteration 900/1250  mem: 172.5/490.7MB
Iteration 1000/1250  mem: 214.1/490.7MB
Iteration 1100/1250  mem: 253.5/490.7MB
Iteration 1200/1250  mem: 192.3/491.8MB
done building BART in 1.058 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 79 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 244.8/491.8MB
Iteration 200/1250  mem: 270.9/491.8MB
Iteration 300/1250  mem: 190.3/492.8MB
Iteration 400/1250  mem: 218/492.8MB
Iteration 500/1250  mem: 243.9/492.8MB
Iteration 600/1250  mem: 271.6/492.8MB
Iteration 700/1250  mem: 192.2/491.8MB
Iteration 800/1250  mem: 218.9/491.8MB
Iteration 900/1250  mem: 245.6/491.8MB
Iteration 1000/1250  mem: 272.2/491.8MB
Iteration 1100/1250  mem: 298.9/491.8MB
Iteration 1200/1250  mem: 220.6/492.3MB
done building BART in 0.705 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 38 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 251.4/492.3MB
Iteration 200/1250  mem: 268.2/492.3MB
Iteration 300/1250  mem: 282.6/492.3MB
Iteration 400/1250  mem: 299.5/492.3MB
Iteration 500/1250  mem: 313.9/492.3MB
Iteration 600/1250  mem: 220.5/493.9MB
Iteration 700/1250  mem: 235/493.9MB
Iteration 800/1250  mem: 249.6/493.9MB
Iteration 900/1250  mem: 266.2/493.9MB
Iteration 1000/1250  mem: 282.8/493.9MB
Iteration 1100/1250  mem: 297.3/493.9MB
Iteration 1200/1250  mem: 311.8/493.9MB
done building BART in 0.382 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 51 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 236.3/493.9MB
Iteration 200/1250  mem: 255/493.9MB
Iteration 300/1250  mem: 273.7/493.9MB
Iteration 400/1250  mem: 292.4/493.9MB
Iteration 500/1250  mem: 311.1/493.9MB
Iteration 600/1250  mem: 329.8/493.9MB
Iteration 700/1250  mem: 348.5/493.9MB
Iteration 800/1250  mem: 255.9/494.9MB
Iteration 900/1250  mem: 275.2/494.9MB
Iteration 1000/1250  mem: 294.6/494.9MB
Iteration 1100/1250  mem: 312.2/494.9MB
Iteration 1200/1250  mem: 331.6/494.9MB
done building BART in 0.472 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 75 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
building BART with mem-cache speedup...
Iteration 100/1250  mem: 260.9/495.5MB
Iteration 200/1250  mem: 287.2/495.5MB
Iteration 300/1250  mem: 314.9/495.5MB
Iteration 400/1250  mem: 341.1/495.5MB
Iteration 500/1250  mem: 368.9/495.5MB
Iteration 600/1250  mem: 278.8/496.5MB
Iteration 700/1250  mem: 306.5/496.5MB
Iteration 800/1250  mem: 332.4/496.5MB
Iteration 900/1250  mem: 360.1/496.5MB
Iteration 1000/1250  mem: 386/496.5MB
Iteration 1100/1250  mem: 301/485MB
Iteration 1200/1250  mem: 328.1/485MB
done building BART in 0.691 sec 

burning and aggregating chains from all threads... done
evaluating in sample data...done
bartMachine initializing with 84 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 10 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...
Could not calculate inverse cum prob density for chi sq df = 0.0032804894726723433 with q = 0.9

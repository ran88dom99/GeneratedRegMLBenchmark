
R version 3.4.1 (2017-06-30) -- "Single Candle"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> options(repos=structure(c(CRAN="https://rweb.crmda.ku.edu/cran/")))
> ## capture messages and errors to a file.https://rweb.crmda.ku.edu/cran/
> #zz <- file("all.Rout", open="wt")https://cran.cnr.berkeley.edu
> #sink(zz, type="message") edit for rebaseless
> #chek for R package updates
> #try(log("a")) ## test --no-edit
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> task.subject<-"14th20hp3cv"
> pc.mlr<-c("ACE")#"ALTA","HOPPER"
> which.computer<-Sys.info()[['nodename']]
> out.file<-paste("out",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,".csv",sep="")
> importance.file<-paste("importance",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,sep="")
> 
> if(exists("base.folder")){setwd(base.folder)}
> base.folder<-getwd()
> cpout.folder<-paste(base.folder,"/",which.computer,sep = "")
> setwd(cpout.folder)
> 
> if(length(which(list.files() == out.file))<1) write.table( "0.01,0.01,100,100,100,Wed Aug 02 16:37:25 2017,dummy,8,1,basic latent features,ignore,none,asis,1.12784979099243,random,333,53,adaptive_cv,16,5,2,2,19,0.0107744822639878,FALSE,,,,,,,,,," ,file =,out.file,  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,".csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,".csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,"mlr.csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,"mlr.csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> 
> cv.iters=3
> tuneLength=20
> tuneLength2=8
> normings=c("YeoJohnson","ICA", "centernscale","expoTrans","range01","asis","quantile")#,"centernscale"
> 
> gensTTesto<-c(56,53,4,12,13,14,15,20,45,54,55, 44,3,1,52)#,  51,c(4)#c(1:40)#c(5,10,11,13,14,15,16,17,18,19,20,21,24,28,38,39,40)
> gensTTest<-vector()
> write.table( t(gensTTesto),file = "initial tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
> try({
+   gensTTest<-t(read.csv("tasks to test.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   gensTTest<-as.vector(gensTTest)
+ })
> if(!exists("gensTTest")) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> gensTTesto<-c(gensTTesto[length(gensTTesto):1])
> if(length(gensTTest)<1) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> 
> 
> ########packages install check######
> 
> #list.of.packages <- c("caret","caretEnsemble","mlr","MLmetrics","tgp")
> #list.of.packages <- c("gower","dimRed","DEoptimR","caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> #new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> #if(length(new.packages)) install.packages(new.packages, dep = TRUE)
> 
> 
> #install.packages("mlr", dependencies = c("Depends", "Suggests"))
> #install.packages("caret", dependencies = c("Depends", "Suggests"))
> #install.packages("caret",repos = "http://cran.r-project.org",dependencies = c("Depends", "Imports", "Suggests"))
> #install.packages("SuperLearner", dependencies = c("Depends", "Suggests"))
> #install.packages("rattle", dependencies = c("Depends", "Suggests"))
> 
> # Load libraries
> #library(mlbench)
> 
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> #library(caretEnsemble)
> library(MLmetrics)

Attaching package: 'MLmetrics'

The following object is masked from 'package:caret':

    RMSE

The following object is masked from 'package:base':

    Recall

> 
> ########error no repeat#########
> 
> 
> try({
+   before.last.alg<-as.matrix(read.csv("beforelast algorithm.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.alg<-as.matrix(read.csv("last algorithm tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   #write.table(paste(date(), last.alg,.Platform$OS.type,.Platform$r_arch,which.computer,sep=" "),file = "algos after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.alg==before.last.alg){print("algorithm may be broken")}
+   write.table(last.alg,file = "beforelast algorithm.csv",  quote = F, row.names = F,col.names = F)
+ })
[1] "algorithm may be broken"
> try({
+   before.last.tsk<-as.matrix(read.csv("beforelast task.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.tsk<-as.matrix(read.csv("last task tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   write.table(paste(date(),last.alg, last.tsk,cv.iters,tuneLength,.Platform$OS.type,.Platform$r_arch,which.computer,sep=","),file = "test after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.tsk==before.last.tsk){print("task may be broken")}
+   write.table(last.tsk,file = "beforelast task.csv",  quote = F, row.names = F,col.names = F)
+ })
[1] "task may be broken"
> bad.models=c("spaccceeee")
> previous.fails<-(read.csv("test after which reset.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
> previous.fails<-previous.fails[previous.fails[,8]==which.computer,]
> lgf<-length(previous.fails[,2])
> for(lt in 2:lgf)  {
+   if(previous.fails[lt,2]==previous.fails[lt-1,2])  {
+     bad.models=union(bad.models,c(paste(previous.fails[lt,2])))  }}
> 
> #######not to redo a test function#####
> check.redundant<-function(df=df.previous.calcs,norming="asis",trans.y=1,withextra="missing",missingdata="leaveempty",datasource="mean" ,column.to.predict=200,allmodel="ctree")
+ {
+   for(intern in 1:length(df[,1])){
+     if((any(df[intern,] == norming, na.rm=T))&&
+        (any(df[intern,] == withextra, na.rm=T))&&
+        (any(df[intern,] == missingdata, na.rm=T))&&
+        (any(df[intern,] == datasource, na.rm=T))&&
+        (any(df[intern,] == column.to.predict, na.rm=T))&&
+        (any(df[intern,] == allmodel, na.rm=T))&&
+        (  (df[intern,9] == trans.y)))
+     {return(TRUE)}
+   }
+   return(FALSE)
+ }
> #####caret init#####
> best.ranged <- c("avNNet", "nnet", "pcaNNet", "glm.nb")
> best.asis <- c("svmLinear3", "relaxo", "superpc", "xgbTree")
> best.cns <- c("gam", "bam", "svmLinear2", "msaenet", "BstLm", "gbm")
> 
> cv6hp5 <- c( "BstLm", "qrnn")#earth
> cv3hp32 <- c("Rborist", "pcaNNet", "SBC")
> cv7x5hp32 <- c("gbm", "krlsPoly", "kknn", "xgbLinear","RRF", "cubist", "rlm" )
> cv6hp5.avoid <- c("pcaNNet")
> cv3hp32.avoid <- c("glm.nb", "gamboost", "ctree2","glmboost", "leapSeq","ctree","svmLinear2")
> cv7x5hp32.avoid <- c("SBC","bagearthgcv","gcvearth","lmStepAIC","glmStepAIC","bridge","lm","glm","bayesglm","blassoAveraged","treebag","rpart1SE")
> 
> allmodels <- c("avNNet", "bagEarth", "bagEarthGCV",
+                "bayesglm", "bdk", "blackboost", "Boruta", "brnn", "BstLm" ,
+                "bstTree", "cforest", "ctree", "ctree2", "cubist", "DENFIS",
+                "dnn", "earth", "elm", "enet",   "evtree",
+                "extraTrees",  "gamLoess",  "gaussprLinear", "gaussprPoly", "gaussprRadial",
+                "gcvEarth","glm", "glmboost",  "icr", "kernelpls",
+                "kknn", "knn",  "krlsRadial", "lars" , "lasso",
+                "leapBackward", "leapForward", "leapSeq", "lm", "M5", "M5Rules",
+                "mlpWeightDecay", "neuralnet" , "partDSA",
+                "pcaNNet", "pcr", "penalized", "pls", "plsRglm", "ppr",
+                "qrf" , "ranger",  "rf")
> allmodels <- c("rlm", "rpart", "rpart2",
+                "RRF", "RRFglobal",  "simpls",
+                "svmLinear", "svmPoly", "svmRadial", "svmRadialCost",
+                "widekernelpls",  "xgbLinear",
+                "xgbTree")
> allmodels <- c("avNNet","BstLm","bstTree","cforest","ctree","ctree2",
+                "cubist","earth","enet","evtree","glmboost",
+                "icr","kernelpls","kknn","lasso","pcaNNet",
+                "pcr","pls","qrf","ranger","rf")
> 
> allmodels <- c("kknn", "cubist", "avNNet", "xgbLinear", "RRF", "pcaNNet","earth","nnet","gbm","enet","lasso","BstLm",
+                "foba", "leapBackward", "gcvEarth", "SBC","glm.nb","gamboost","ctree2","relaxo",
+                "bartMachine","extraTrees","bam","gam","randomGLM")
> #allmodels <- c("bam")
> #allmodels <- c("rf")"rqlasso",, "xyf" "rvmPoly", "rvmRadial",    "spls", "superpc" ,   "treebag",  "svmLinear2",  "SBC",
> #allmodels <- c("bartMachine", "xgbLinear", "pcaNNet","svmLinear","glmnet","cforest","cubist","rf","ranger")"glmnet",
> #wow rfRules is really slow "rfRules","WM", takes 50min
> # brak everythig "rbfDDA","ridge","rqnc",
> # use "rf" to test all
> library(caret)
> allmodels <- unique(modelLookup()[modelLookup()$forReg,c(1)])
> #allmodels <-c("avNNet", "nnet", "pcaNNet",  "glm.nb", "gam" ,
> #              "bam","msaenet", "svmLinear2","svmLinear3",
> #              "relaxo",  "superpc", "xgbTree", "BstLm")
> #allmodels<- c("svmLinear","svmPoly","svmRadial")
> #library(doParallel); cl <- makeCluster(detectCores()); registerDoParallel(cl)
> #allmodels<-c("bartMachine","extraTrees")#,"randomGLM"
> 
> 
> adaptControl <- trainControl(method = "adaptive_cv",
+                              number = 7, repeats = 5,
+                              adaptive = list(min = 4, alpha = 0.05,
+                                              method = "gls", complete = FALSE),
+                              search = "random")
> adaptControl <-trainControl(method = "cv", number = cv.iters,  search = "random")
> simpleControl <- trainControl(method = "cv",
+                               number = cv.iters,
+                               search = "random")
> 
> 
> #########MLR init######
> #R.utils::gcDLLs()
> #list.of.packages <- c("ParamHelpers","devtools","mlrMBO","RJSONIO","plot3D","plotly")
> #install.packages("mlrMBO", dependencies = c("Depends", "Suggests"))
> list.of.packages <- c("caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> if(length(new.packages)) install.packages(new.packages, dep = TRUE)
Installing packages into 'C:/Users/irina grishina/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Warning message:
packages ' RWeka', 'rPython', 'rrf' are not available (for R version 3.4.1) 
> 
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> tuneLengthMLR<-tuneLength
> mlr.iters<-cv.iters
> #######data read process start#####
> seed.var =222+round(runif(1,min=0,max=100))
> column.to.predict=1
> print(date());
[1] "Sun Mar 11 15:31:35 2018"
> 
> setwd(base.folder)
> if(!exists("gen.count")){gen.count=56}
> gens.names<-as.matrix(read.table("gens names.csv", sep = ",",header = FALSE,row.names=1,fill=TRUE, quote="",dec="."))
> count.toy.data.passed<-1
> for(gend.data in gensTTest){
+   count.toy.data.passed<-count.toy.data.passed+1
+   setwd(base.folder)
+   data.source<-as.matrix(read.csv(paste("Generats/",gens.names[gend.data],".csv", sep = ""), sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   datasource<-gens.names[gend.data,1]
+   setwd(cpout.folder)
+   missingdatas=c("ignore")
+   for(missingdata in missingdatas){
+     withextras=c("none")
+     for(withextra in withextras){
+       ################data wrestling###############
+ 
+       dependant.selection=complete.cases(data.source[,column.to.predict])
+       df.previous.calcs=as.data.frame(read.csv(file=out.file, header = FALSE, sep = ",", quote = "",
+                                                dec = ".", fill = TRUE, comment.char = ""))
+       unimportant.computations<-vector(mode = "logical",length=length(df.previous.calcs[,1])  )
+       for(intern in 1:length(df.previous.calcs[,1])){
+         if((any(df.previous.calcs[intern,] == withextra, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == missingdata, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == datasource, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == column.to.predict, na.rm=T)))
+         {unimportant.computations[intern]<-T}}
+ 
+       df.previous.calcs<-df.previous.calcs[unimportant.computations,]
+ 
+       #data.source=data.frame( data.source[,column.to.predict],data.source[,1:2], data.source[,4:(column.to.predict-1)], data.source[,(column.to.predict+1):length( data.source[1,])])
+ 
+         for(norming in normings) {
+         for(trans.y in 1:2) {
+           df.toprocess=data.source
+           y.untransformed<-df.toprocess[,1]
+ 
+           if(norming=="centernscale"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("center", "scale"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="range01"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="expoTrans"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("expoTrans"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="YeoJohnson"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("YeoJohnson"))#"center", "scale",
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+ 
+           if((norming=="asis")&&(trans.y==2)){next}
+ 
+ 
+           ################preprocess###########
+           df.toprocess=data.frame(df.toprocess[dependant.selection,])
+           y.untransformed=y.untransformed[dependant.selection]
+           if(norming=="quantile"){
+             for(Clol in trans.y:length(data.source[1,])){
+               df.toprocess[,Clol]<- (rank(df.toprocess[,Clol],na.last = "keep",ties.method = "average")-1) }
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+ 
+           loess.model<-loess(y.untransformed~ df.toprocess[,1],span = 0.21, degree = 1)
+ 
+           #df.toprocess = data.frame(df.toprocess,)
+           nzv <- nearZeroVar(df.toprocess[,])#, saveMetrics= TRUE
+           #nzv[nzv$nzv,][1:10,]
+           if(length(nzv)>1){
+             df.toprocess = (df.toprocess[, -nzv])}
+           df.toprocess = signif(df.toprocess,digits = 3)
+ 
+           seed.var =222+round(runif(1,min=0,max=100))
+           set.seed(seed.var)
+           inTrain <- createDataPartition(y = df.toprocess[,1],
+                                          p = .75,
+                                          list = FALSE)
+           training <- df.toprocess[ inTrain,]
+           testing  <- df.toprocess[-inTrain,]
+           write.table(df.toprocess,file = "sanity check 1.csv",  quote = F, row.names = F,col.names = F)
+ 
+           ###########for all models#################
+           setwd(base.folder)
+           if(max(which.computer==pc.mlr)>0)
+             source("MLR part.R")
+           else
+             source("Caret part.R")
+ 
+          setwd(cpout.folder)
+           if(norming == normings[length(normings)]){
+             if(count.toy.data.passed>length(gensTTest)){gensTTest<-c(gensTTesto)}
+             write.table( t(gensTTest[count.toy.data.passed:length(gensTTest)]),file = "tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
+ 
+             }
+ 
+         }
+       }
+     }
+   }
+ 
+ }
Loading required package: h2o

----------------------------------------------------------------------

Your next step is to start H2O:
    > h2o.init()

For H2O package documentation, ask for help:
    > ??h2o

After starting H2O, you can use the Web UI at http://localhost:54321
For more information visit http://docs.h2o.ai

----------------------------------------------------------------------


Attaching package: 'h2o'

The following objects are masked from 'package:stats':

    cor, sd, var

The following objects are masked from 'package:base':

    %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,
    colnames<-, ifelse, is.character, is.factor, is.numeric, log,
    log10, log1p, log2, round, signif, trunc

Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:35:40 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "gbm_h2o"                 
Loading required package: earth
Loading required package: plotmo
Loading required package: plotrix
Loading required package: TeachingDemos
Multivariate Adaptive Regression Splines 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results:

  RMSE         Rsquared 
  0.003156257  0.9999955

Tuning parameter 'degree' was held constant at a value of 1
[1] "Sun Mar 11 15:35:43 2018"
Generalized Linear Model 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results:

  RMSE         Rsquared 
  0.003109473  0.9999956

[1] "Sun Mar 11 15:35:45 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.351,  : 
  could not find function "glm.nb"

2: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.351,  : 
  could not find function "glm.nb"

3: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.351,  : 
  could not find function "glm.nb"

4: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

5: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

6: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

7: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

8: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

9: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

2: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

3: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

4: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

5: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

6: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.12,  : 
  could not find function "glm.nb"

7: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.305,  : 
  could not find function "glm.nb"

8: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.305,  : 
  could not find function "glm.nb"

9: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(-0.305,  : 
  could not find function "glm.nb"

10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:35:48 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "glm.nb"                  
Loading required package: plyr
Loading required package: mboost
Loading required package: parallel
Loading required package: stabs
This is mboost 2.8-1. See 'package?mboost' and 'news(package  = "mboost")'
for a complete list of changes.


Attaching package: 'mboost'

The following object is masked from 'package:MLmetrics':

    AUC

The following object is masked from 'package:ggplot2':

    %+%

Boosted Generalized Linear Model 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  mstop  prune  RMSE         Rsquared   Selected
   86    yes    0.013647281  0.9999956          
  204    no     0.003109612  0.9999956          
  254    yes    0.003109478  0.9999956          
  267    no     0.003109475  0.9999956          
  306    no     0.003109473  0.9999956          
  310    yes    0.003109473  0.9999956          
  450    no     0.003109473  0.9999956          
  491    yes    0.003109473  0.9999956          
  526    no     0.003109473  0.9999956          
  554    no     0.003109473  0.9999956          
  583    no     0.003109473  0.9999956          
  611    no     0.003109473  0.9999956          
  636    yes    0.003109473  0.9999956          
  667    no     0.003109473  0.9999956  *       
  685    no     0.003109473  0.9999956          
  726    yes    0.003109473  0.9999956          
  730    no     0.003109473  0.9999956          
  735    no     0.003109473  0.9999956          
  803    no     0.003109473  0.9999956          
  908    yes    0.003109473  0.9999956          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mstop = 667 and prune = no.
[1] "Sun Mar 11 15:35:56 2018"
Loading required package: glmnet
Loading required package: Matrix
Loading required package: foreach
Loaded glmnet 2.0-10

glmnet 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  alpha      lambda       RMSE        Rsquared   Selected
  0.0852882  0.015329067  0.04482193  0.9999956          
  0.2035726  0.135994290  0.13331871  0.9999952          
  0.2535576  0.002686154  0.04525603  0.9999955          
  0.2665269  2.002979901  1.09656360  0.9994014          
  0.3050035  1.027283062  0.76406601  0.9998967          
  0.3092305  0.007589096  0.04308323  0.9999955  *       
  0.4496882  6.634828593  1.46482781        NaN          
  0.4902568  0.001178149  0.04512502  0.9999954          
  0.5256768  0.784826294  0.74858477  0.9997543          
  0.5531831  0.334776387  0.36555062  0.9999692          
  0.5828214  0.343726377  0.38070641  0.9999634          
  0.6103412  0.711516859  0.73633696  0.9997054          
  0.6351519  0.001065464  0.04436613  0.9999953          
  0.6668849  1.912867807  1.46482781        NaN          
  0.6841167  5.881601282  1.46482781        NaN          
  0.7258268  0.009489803  0.04405480  0.9999952          
  0.7290284  3.863695930  1.46482781        NaN          
  0.7347418  3.511594872  1.46482781        NaN          
  0.8020587  0.148789116  0.19061638  0.9999858          
  0.9074311  0.002340257  0.04521855  0.9999951          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were alpha = 0.3092305 and lambda
 = 0.007589096.
[1] "Sun Mar 11 15:35:59 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:37:02 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "glmnet_h2o"              
Loading required package: MASS
Start:  AIC=-4339.05
.outcome ~ V2 + V3

       Df Deviance     AIC
<none>        0.01 -4339.0
- V2    1   479.94  1406.3
- V3    1   572.04  1494.2
Start:  AIC=-4379.34
.outcome ~ V2 + V3

       Df Deviance     AIC
<none>        0.00 -4379.3
- V2    1   525.61  1453.7
- V3    1   531.53  1459.3
Start:  AIC=-4381.67
.outcome ~ V2 + V3

       Df Deviance     AIC
<none>        0.00 -4381.7
- V2    1   486.13  1412.7
- V3    1   507.48  1434.2
Start:  AIC=-6551.27
.outcome ~ V2 + V3

       Df Deviance     AIC
<none>        0.01 -6551.3
- V2    1   746.29  2134.4
- V3    1   805.95  2192.2
Generalized Linear Model with Stepwise Feature Selection 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results:

  RMSE         Rsquared 
  0.003109473  0.9999956

[1] "Sun Mar 11 15:37:04 2018"
Loading required package: fastICA
Independent Component Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  n.comp  RMSE         Rsquared   Selected
  1       0.027106652  0.9994177          
  2       0.003109473  0.9999956  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was n.comp = 2.
[1] "Sun Mar 11 15:37:06 2018"
Loading required package: pls

Attaching package: 'pls'

The following object is masked from 'package:caret':

    R2

The following object is masked from 'package:stats':

    loadings

Partial Least Squares 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  ncomp  RMSE         Rsquared   Selected
  1      0.051967695  0.9977691          
  2      0.003109473  0.9999956  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 2.
[1] "Sun Mar 11 15:37:08 2018"
k-Nearest Neighbors 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  k    RMSE       Rsquared   Selected
   22  0.2372110  0.9855943  *       
   51  0.3621196  0.9768493          
   64  0.4061761  0.9738610          
   67  0.4160399  0.9732869          
   77  0.4500919  0.9712311          
   78  0.4532107  0.9708344          
  113  0.5585249  0.9637100          
  123  0.5853610  0.9620082          
  132  0.6094049  0.9603139          
  139  0.6285370  0.9587879          
  146  0.6466273  0.9570302          
  153  0.6644667  0.9557916          
  159  0.6782249  0.9548107          
  167  0.6961570  0.9534905          
  172  0.7082171  0.9523082          
  182  0.7307992  0.9500611          
  183  0.7327371  0.9498975          
  184  0.7348685  0.9497921          
  201  0.7716148  0.9468198          
  227  0.8260840  0.9423585          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was k = 22.
[1] "Sun Mar 11 15:37:11 2018"
Loading required package: KRLS
## KRLS Package for Kernel-based Regularized Least Squares.

## See Hainmueller and Hazlett (2014) for details.

Polynomial Kernel Regularized Least Squares 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  lambda        degree  RMSE        Rsquared    Selected
  2.669568e-05  1       525.352720  0.14030962          
  1.041989e-04  2        97.889699  0.18004302          
  1.852626e-04  1        75.935408  0.13956462          
  2.150971e-04  3        34.390241  0.03948507          
  3.349789e-04  3        22.031444  0.03999492          
  3.516840e-04  1        40.140305  0.13878556          
  1.771908e-03  3         4.213280  0.04637355          
  2.826729e-03  1         5.376851  0.12757580          
  4.249951e-03  3         2.059024  0.05845312          
  5.833315e-03  2         2.446480  0.17097652          
  8.205538e-03  2         2.052525  0.16746898          
  1.126434e-02  3         1.436223  0.09855257          
  1.498855e-02  1         1.751088  0.08815778          
  2.159854e-02  3         1.368024  0.16665785          
  2.633804e-02  3         1.362853  0.19889452          
  4.257295e-02  1         1.411021  0.14960814          
  4.417148e-02  3         1.360730  0.31199814          
  4.717471e-02  3         1.361026  0.32889327          
  1.023984e-01  2         1.414978  0.29984525          
  3.444733e-01  1         1.318763  0.77180625  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 0.3444733 and degree = 1.
[1] "Sun Mar 11 15:38:11 2018"
Loading required package: kernlab

Attaching package: 'kernlab'

The following object is masked from 'package:ggplot2':

    alpha


 Average Marginal Effects:
 
       V2        V3 
0.9997829 1.0003945 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9979658 0.999308
50% 1.0013851 1.002187
75% 1.0032921 1.003581

 Average Marginal Effects:
 
       V2        V3 
0.9776532 0.9778263 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9774648 0.9777076
50% 0.9778407 0.9780429
75% 0.9780399 0.9782003

 Average Marginal Effects:
 
       V2        V3 
0.9993487 0.9996740 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9988245 0.9993397
50% 0.9997815 1.0002088
75% 1.0003198 1.0005608

 Average Marginal Effects:
 
       V2        V3 
0.8814360 0.8813095 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.8812871 0.8812104
50% 0.8815890 0.8814861
75% 0.8817491 0.8816135

 Average Marginal Effects:
 
       V2        V3 
0.9990880 0.9970844 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9865514 0.9880573
50% 0.9987957 0.9990971
75% 1.0161540 1.0148246

 Average Marginal Effects:
 
       V2        V3 
0.9994519 0.9999721 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9979204 0.9989519
50% 1.0007260 1.0015177
75% 1.0022456 1.0025265

 Average Marginal Effects:
 
       V2        V3 
0.9996774 0.9992935 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9967218 0.9970802
50% 1.0005397 1.0003252
75% 1.0037920 1.0034503

 Average Marginal Effects:
 
       V2        V3 
0.9990450 0.9985784 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9967648 0.997690
50% 1.0012368 1.000765
75% 1.0057797 1.006261

 Average Marginal Effects:
 
       V2        V3 
0.9999888 1.0000066 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9989991 0.9992891
50% 1.0000286 1.0000338
75% 1.0013252 1.0015765

 Average Marginal Effects:
 
       V2        V3 
0.9981359 0.9985147 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9975571 0.9976029
50% 1.0001362 0.9997309
75% 1.0021233 1.0030997

 Average Marginal Effects:
 
       V2        V3 
0.9603450 0.9607005 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9587985 0.9597336
50% 0.9618896 0.9624882
75% 0.9635488 0.9637919

 Average Marginal Effects:
 
       V2        V3 
0.9980624 0.9949078 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9870035 0.9855956
50% 1.0013890 1.0011679
75% 1.0146496 1.0128371

 Average Marginal Effects:
 
       V2        V3 
0.9890501 0.9938392 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9651981 0.9766484
50% 1.0120415 1.0197358
75% 1.0368263 1.0387234

 Average Marginal Effects:
 
       V2        V3 
0.9917032 0.9889989 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9899061 0.9873072
50% 1.0012505 1.0007061
75% 1.0081018 1.0085483

 Average Marginal Effects:
 
       V2        V3 
0.9994903 0.9983886 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9949019 0.9944577
50% 0.9985401 0.9991500
75% 1.0067446 1.0067629

 Average Marginal Effects:
 
       V2        V3 
0.9853846 0.9849855 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9919135 0.9905258
50% 0.9996287 0.9998877
75% 1.0070332 1.0088026

 Average Marginal Effects:
 
       V2        V3 
0.9872162 0.9887546 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9954785 0.9965586
50% 0.9999123 1.0002163
75% 1.0036392 1.0036504

 Average Marginal Effects:
 
       V2        V3 
0.9993480 0.9987646 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9959558 0.9967751
50% 1.0008295 1.0000267
75% 1.0059138 1.0061087

 Average Marginal Effects:
 
       V2        V3 
0.9903949 0.9883312 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9876713 0.9889643
50% 0.9998088 0.9993100
75% 1.0080684 1.0098638

 Average Marginal Effects:
 
       V2        V3 
0.4192672 0.4185810 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.4191096 0.4184751
50% 0.4194302 0.4187686
75% 0.4195997 0.4189039

 Average Marginal Effects:
 
      V2       V3 
1.000030 1.000508 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9987099 0.9993338
50% 1.0019724 1.0021870
75% 1.0033980 1.0034969

 Average Marginal Effects:
 
       V2        V3 
0.9781006 0.9782501 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9779405 0.9781180
50% 0.9783099 0.9784915
75% 0.9784910 0.9786355

 Average Marginal Effects:
 
       V2        V3 
0.9994888 0.9997137 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9991334 0.9993749
50% 0.9999659 1.0002780
75% 1.0004110 1.0006390

 Average Marginal Effects:
 
       V2        V3 
0.8832369 0.8833271 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.8831061 0.8832193
50% 0.8834076 0.8835229
75% 0.8835552 0.8836413

 Average Marginal Effects:
 
       V2        V3 
0.9980043 0.9965414 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9859076 0.9861530
50% 1.0006548 0.9977317
75% 1.0173471 1.0129124

 Average Marginal Effects:
 
       V2        V3 
0.9996786 1.0001080 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9986732 0.9991704
50% 1.0010243 1.0017081
75% 1.0023292 1.0027552

 Average Marginal Effects:
 
       V2        V3 
0.9997114 0.9991714 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9970112 0.997102
50% 1.0003155 1.000048
75% 1.0036993 1.003494

 Average Marginal Effects:
 
       V2        V3 
0.9986967 0.9984678 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9969889 0.9960329
50% 1.0018586 0.9998567
75% 1.0060530 1.0063983

 Average Marginal Effects:
 
       V2        V3 
1.0000168 0.9999022 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9988711 0.9987390
50% 1.0001052 0.9997306
75% 1.0014609 1.0015857

 Average Marginal Effects:
 
       V2        V3 
0.9986577 0.9983184 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9983145 0.9977795
50% 0.9995751 0.9997635
75% 1.0017311 1.0022932

 Average Marginal Effects:
 
       V2        V3 
0.9611500 0.9616047 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9598326 0.9605191
50% 0.9628804 0.9635935
75% 0.9643826 0.9648015

 Average Marginal Effects:
 
       V2        V3 
0.9971331 0.9943678 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9889786 0.9867198
50% 1.0012511 1.0010902
75% 1.0141382 1.0135403

 Average Marginal Effects:
 
       V2        V3 
0.9911591 0.9965509 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9708994 0.9788691
50% 1.0151647 1.0226545
75% 1.0385311 1.0421959

 Average Marginal Effects:
 
       V2        V3 
0.9912169 0.9869899 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9860624 0.9901846
50% 1.0023647 0.9997793
75% 1.0089919 1.0080426

 Average Marginal Effects:
 
       V2        V3 
0.9989557 0.9979976 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9934862 0.9944964
50% 0.9993093 0.9986131
75% 1.0088987 1.0054082

 Average Marginal Effects:
 
       V2        V3 
0.9868149 0.9838520 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9929824 0.9919473
50% 0.9999360 0.9998041
75% 1.0064189 1.0061907

 Average Marginal Effects:
 
       V2        V3 
0.9915235 0.9906804 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9961585 0.9966145
50% 1.0005289 0.9997538
75% 1.0027960 1.0024239

 Average Marginal Effects:
 
       V2        V3 
0.9989702 0.9985954 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9963632 0.9953384
50% 1.0011893 0.9993315
75% 1.0061472 1.0060060

 Average Marginal Effects:
 
       V2        V3 
0.9901194 0.9861018 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9887809 0.9886644
50% 1.0001843 0.9989859
75% 1.0076140 1.0087202

 Average Marginal Effects:
 
       V2        V3 
0.4233523 0.4233009 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.4232119 0.4231853
50% 0.4235350 0.4235104
75% 0.4236932 0.4236374

 Average Marginal Effects:
 
      V2       V3 
0.999217 1.000194 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9967828 0.9989062
50% 1.0011939 1.0023636
75% 1.0033332 1.0037907

 Average Marginal Effects:
 
       V2        V3 
0.9785112 0.9787713 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9783104 0.9786309
50% 0.9787014 0.9790030
75% 0.9789032 0.9791578

 Average Marginal Effects:
 
       V2        V3 
0.9992414 0.9996702 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9987523 0.9993465
50% 0.9996869 1.0002150
75% 1.0001530 1.0005879

 Average Marginal Effects:
 
       V2        V3 
0.8858385 0.8858231 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.8856764 0.8857081
50% 0.8859932 0.8860129
75% 0.8861579 0.8861382

 Average Marginal Effects:
 
       V2        V3 
0.9949405 0.9950514 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9875925 0.9866791
50% 0.9987002 1.0007176
75% 1.0133763 1.0147515

 Average Marginal Effects:
 
       V2        V3 
0.9991115 0.9998443 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9975928 0.998835
50% 1.0004197 1.001434
75% 1.0017758 1.002480

 Average Marginal Effects:
 
       V2        V3 
0.9993496 0.9990470 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9978243 0.9967252
50% 1.0004481 1.0001114
75% 1.0022612 1.0029923

 Average Marginal Effects:
 
       V2        V3 
0.9963696 0.9979803 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9985997 1.001145
50% 1.0036049 1.003100
75% 1.0046853 1.005627

 Average Marginal Effects:
 
       V2        V3 
0.9996463 1.0000046 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9997342 0.9995774
50% 1.0001894 1.0002087
75% 1.0007038 1.0010950

 Average Marginal Effects:
 
       V2        V3 
0.9991325 0.9990275 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9982491 0.9974693
50% 0.9999892 1.0001610
75% 1.0017935 1.0024934

 Average Marginal Effects:
 
       V2        V3 
0.9615222 0.9621682 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9598548 0.9610110
50% 0.9631025 0.9640925
75% 0.9647609 0.9653681

 Average Marginal Effects:
 
       V2        V3 
0.9934978 0.9915008 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9919690 0.990278
50% 0.9993966 0.998829
75% 1.0082309 1.008498

 Average Marginal Effects:
 
       V2        V3 
0.9835210 0.9917445 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9533589 0.971608
50% 1.0071949 1.020224
75% 1.0329075 1.039924

 Average Marginal Effects:
 
       V2        V3 
0.9885214 0.9842246 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9797278 0.9812064
50% 1.0003176 1.0001198
75% 1.0217706 1.0143283

 Average Marginal Effects:
 
       V2        V3 
0.9968014 0.9966035 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9903543 0.9912301
50% 0.9988682 0.9998585
75% 1.0111785 1.0092075

 Average Marginal Effects:
 
       V2        V3 
0.9871858 0.9840568 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9853247 0.9877984
50% 0.9997161 0.9986519
75% 1.0132992 1.0098099

 Average Marginal Effects:
 
       V2        V3 
0.9936571 0.9933267 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9935438 0.9956297
50% 0.9985200 1.0002950
75% 1.0048756 1.0036250

 Average Marginal Effects:
 
       V2        V3 
0.9970249 0.9982560 

 Quartiles of Marginal Effects:
 
          V2        V3
25% 1.000435 0.9997141
50% 1.002220 1.0022540
75% 1.003700 1.0049848

 Average Marginal Effects:
 
       V2        V3 
0.9884292 0.9841462 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9805657 0.9841472
50% 0.9998114 1.0011304
75% 1.0183238 1.0093589

 Average Marginal Effects:
 
       V2        V3 
0.4298281 0.4292727 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.4296523 0.4291478
50% 0.4299957 0.4294789
75% 0.4301743 0.4296137

 Average Marginal Effects:
 
       V2        V3 
0.9995838 0.9998998 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9991608 0.9995644
50% 1.0000280 1.0004395
75% 1.0005086 1.0008114
Radial Basis Function Kernel Regularized Least Squares 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  lambda        sigma        RMSE         Rsquared   Selected
  2.669568e-05   599.402919  0.005512613  0.9999861          
  1.041989e-04    64.377325  0.004263770  0.9999915          
  1.852626e-04  3555.034594  0.003517141  0.9999948  *       
  2.150971e-04     4.118354  0.019342037  0.9997114          
  3.349789e-04     8.149446  0.012808096  0.9998805          
  3.516840e-04  1229.707735  0.004692073  0.9999903          
  1.771908e-03     1.210762  0.089221020  0.9936300          
  2.826729e-03  8254.602378  0.032369242  0.9999954          
  4.249951e-03    10.730803  0.021924900  0.9996580          
  5.833315e-03    25.635400  0.014447594  0.9998740          
  8.205538e-03    24.953328  0.016082313  0.9998418          
  1.126434e-02    11.862142  0.030530828  0.9994036          
  1.498855e-02  9147.949659  0.170912746  0.9999955          
  2.159854e-02     4.316759  0.050760189  0.9979844          
  2.633804e-02     1.369465  0.110238886  0.9903134          
  4.257295e-02   978.557869  0.057215819  0.9999879          
  4.417148e-02     2.104179  0.089519906  0.9934622          
  4.717471e-02     2.320062  0.085545086  0.9940330          
  1.023984e-01    58.724345  0.056856383  0.9985046          
  3.444733e-01  4092.947343  0.843925823  0.9999946          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 0.0001852626 and sigma
 = 3555.035.
[1] "Sun Mar 11 15:39:54 2018"
Loading required package: lars
Loaded lars 1.2

Least Angle Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  fraction   RMSE       Rsquared   Selected
  0.0852882  1.3419153  0.9380588          
  0.2035726  1.1683896  0.9909482          
  0.2535576  1.0950608  0.9948608          
  0.2665269  1.0760345  0.9955078          
  0.3050035  1.0195887  0.9969191          
  0.3092305  1.0133875  0.9970391          
  0.4496882  0.8073339  0.9991102          
  0.4902568  0.7478192  0.9993568          
  0.5256768  0.6958577  0.9995147          
  0.5531831  0.6555057  0.9996103          
  0.5828214  0.6120261  0.9996931          
  0.6103412  0.5716545  0.9997549          
  0.6351519  0.5352571  0.9998008          
  0.6668849  0.4887049  0.9998482          
  0.6841167  0.4634259  0.9998697          
  0.7258268  0.4022378  0.9999113          
  0.7290284  0.3975411  0.9999139          
  0.7347418  0.3891596  0.9999185          
  0.8020587  0.2904084  0.9999595          
  0.9074311  0.1358434  0.9999893  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was fraction = 0.9074311.
[1] "Sun Mar 11 15:39:56 2018"
Least Angle Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  step  RMSE      Rsquared   Selected
  1     1.464828        NaN          
  2     1.438772  0.5392154  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was step = 2.
[1] "Sun Mar 11 15:39:58 2018"
Loading required package: elasticnet
The lasso 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  fraction   RMSE       Rsquared   Selected
  0.0852882  1.3419153  0.9380588          
  0.2035726  1.1683896  0.9909482          
  0.2535576  1.0950608  0.9948608          
  0.2665269  1.0760345  0.9955078          
  0.3050035  1.0195887  0.9969191          
  0.3092305  1.0133875  0.9970391          
  0.4496882  0.8073339  0.9991102          
  0.4902568  0.7478192  0.9993568          
  0.5256768  0.6958577  0.9995147          
  0.5531831  0.6555057  0.9996103          
  0.5828214  0.6120261  0.9996931          
  0.6103412  0.5716545  0.9997549          
  0.6351519  0.5352571  0.9998008          
  0.6668849  0.4887049  0.9998482          
  0.6841167  0.4634259  0.9998697          
  0.7258268  0.4022378  0.9999113          
  0.7290284  0.3975411  0.9999139          
  0.7347418  0.3891596  0.9999185          
  0.8020587  0.2904084  0.9999595          
  0.9074311  0.1358434  0.9999893  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was fraction = 0.9074311.
[1] "Sun Mar 11 15:40:00 2018"
Loading required package: leaps
Linear Regression with Backwards Selection 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  nvmax  RMSE         Rsquared   Selected
  1      0.996813654  0.5392154          
  2      0.003109473  0.9999956  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sun Mar 11 15:40:02 2018"
Linear Regression with Forward Selection 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  nvmax  RMSE         Rsquared   Selected
  1      0.996813654  0.5392154          
  2      0.003109473  0.9999956  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sun Mar 11 15:40:03 2018"
Linear Regression with Stepwise Selection 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  nvmax  RMSE         Rsquared   Selected
  1      0.996813654  0.5392154          
  2      0.003109473  0.9999956  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nvmax = 2.
[1] "Sun Mar 11 15:40:05 2018"
Linear Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results:

  RMSE         Rsquared 
  0.003109473  0.9999956

Tuning parameter 'intercept' was held constant at a value of TRUE
[1] "Sun Mar 11 15:40:06 2018"
Start:  AIC=-5762.82
.outcome ~ V2 + V3

       Df Sum of Sq    RSS     AIC
<none>                0.01 -5762.8
- V2    1    479.93 479.94   -17.5
- V3    1    572.03 572.04    70.4
Start:  AIC=-5805.96
.outcome ~ V2 + V3

       Df Sum of Sq    RSS     AIC
<none>                0.00 -5806.0
- V2    1    525.60 525.61    27.1
- V3    1    531.52 531.53    32.7
Start:  AIC=-5805.44
.outcome ~ V2 + V3

       Df Sum of Sq    RSS     AIC
<none>                0.00 -5805.4
- V2    1    486.13 486.13   -11.1
- V3    1    507.48 507.48    10.4
Start:  AIC=-8687.35
.outcome ~ V2 + V3

       Df Sum of Sq    RSS     AIC
<none>                0.01 -8687.4
- V2    1    746.28 746.29    -1.7
- V3    1    805.95 805.95    56.1
Linear Regression with Stepwise Selection 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results:

  RMSE         Rsquared 
  0.003109473  0.9999956

[1] "Sun Mar 11 15:40:08 2018"
Loading required package: logicFS
Loading required package: LogicReg
Loading required package: survival

Attaching package: 'survival'

The following object is masked from 'package:caret':

    cluster

Loading required package: mcbiopi
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :18    NA's   :18   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :6     NA's   :6    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:40:11 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "logicBag"                
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:40:15 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "logreg"                  
Loading required package: RWeka
Error: package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Loading required package: RWeka
Error: package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Loading required package: RWeka
Error: package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:40:17 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "M5"                      
Loading required package: RWeka
Error: package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Loading required package: RWeka
Error: package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Loading required package: RWeka
Error: package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package or namespace load failed for 'RWeka':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:40:18 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "M5Rules"                 
Loading required package: msaenet
Multi-Step Adaptive MCP-Net 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  alphas     nsteps  scale      RMSE         Rsquared   Selected
  0.1267594   4      2.6023996  0.010829113  0.9999956          
  0.2332154   6      3.4449009  0.005866293  0.9999956          
  0.2782018   3      1.6757152  0.005011692  0.9999956          
  0.2898742   9      2.1345227  0.004842524  0.9999956          
  0.3245031   8      3.9087673  0.004431358  0.9999956          
  0.3283075   4      2.9923695  0.004393142  0.9999956          
  0.4547194  10      3.7655153  0.003605275  0.9999956          
  0.4912312   2      0.8960751  0.003486691  0.9999956          
  0.5231091   8      2.7819839  0.003405889  0.9999956          
  0.5478648   7      1.0827993  0.003354666  0.9999956          
  0.5745393   7      2.1414188  0.003308653  0.9999956          
  0.5993071   8      1.8453820  0.003273028  0.9999956          
  0.6216367   2      3.4697338  0.003245831  0.9999956          
  0.6501964   9      2.2041859  0.003216761  0.9999956          
  0.6657050  10      2.5345341  0.003203284  0.9999956          
  0.7032441   4      3.9953945  0.003176276  0.9999956          
  0.7061256  10      0.4289555  0.003174493  0.9999956          
  0.7112677  10      2.5283356  0.003171404  0.9999956          
  0.7718528   7      1.6033660  0.003142779  0.9999956          
  0.8666880   2      3.6043202  0.003118816  0.9999956  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were alphas = 0.866688, nsteps = 2
 and scale = 3.60432.
[1] "Sun Mar 11 15:40:26 2018"
Loading required package: nnet
# weights:  9
initial  value 1189.431611 
iter  10 value 1072.414916
iter  20 value 1071.406875
iter  30 value 1071.394067
iter  40 value 1071.370453
iter  50 value 1071.299065
iter  60 value 1002.965650
iter  70 value 694.802802
iter  80 value 690.026876
iter  90 value 689.612353
iter 100 value 689.529770
final  value 689.529770 
stopped after 100 iterations
# weights:  41
initial  value 1326.865908 
iter  10 value 1071.365260
final  value 1071.313169 
converged
# weights:  25
initial  value 1186.906920 
iter  10 value 1071.408818
iter  20 value 1071.311817
iter  30 value 1071.277379
iter  40 value 707.968651
iter  50 value 699.218829
iter  60 value 693.523491
iter  70 value 689.995432
iter  80 value 689.360343
iter  90 value 689.199346
iter 100 value 689.180562
final  value 689.180562 
stopped after 100 iterations
# weights:  53
initial  value 1236.727710 
iter  10 value 1071.361424
iter  20 value 1071.312862
iter  20 value 1071.312861
iter  20 value 1071.312861
final  value 1071.312861 
converged
# weights:  53
initial  value 1201.123942 
iter  10 value 791.876733
iter  20 value 708.921917
iter  30 value 706.027187
iter  40 value 705.610423
iter  50 value 705.565209
iter  60 value 705.563058
iter  70 value 705.561229
iter  70 value 705.561226
iter  70 value 705.561225
final  value 705.561225 
converged
# weights:  29
initial  value 1174.842960 
iter  10 value 1071.967575
iter  20 value 1000.697344
iter  30 value 696.666146
iter  40 value 691.820831
iter  50 value 690.291487
iter  60 value 689.961192
iter  70 value 689.669036
iter  80 value 689.370755
iter  90 value 689.296548
iter 100 value 689.278537
final  value 689.278537 
stopped after 100 iterations
# weights:  29
initial  value 1191.566448 
iter  10 value 828.960065
iter  20 value 719.199644
iter  30 value 716.712763
iter  40 value 716.632407
iter  50 value 716.613465
iter  50 value 716.613461
iter  50 value 716.613461
final  value 716.613461 
converged
# weights:  49
initial  value 1156.037201 
iter  10 value 718.894016
iter  20 value 696.795212
iter  30 value 696.469013
iter  40 value 696.459671
iter  50 value 696.458907
final  value 696.458877 
converged
# weights:  21
initial  value 1157.165349 
iter  10 value 1073.516571
iter  20 value 721.442879
iter  30 value 695.006862
iter  40 value 693.081191
iter  50 value 692.727979
iter  60 value 692.392025
iter  70 value 692.293285
iter  80 value 692.290815
final  value 692.290403 
converged
# weights:  25
initial  value 1189.778923 
iter  10 value 754.605423
iter  20 value 750.057764
iter  30 value 750.011136
final  value 750.010852 
converged
# weights:  61
initial  value 1273.687776 
iter  10 value 1073.952919
iter  20 value 1071.342086
iter  30 value 735.338751
iter  40 value 703.752959
iter  50 value 702.826917
iter  60 value 694.843317
iter  70 value 690.455295
iter  80 value 689.798595
iter  90 value 689.685002
iter 100 value 689.550132
final  value 689.550132 
stopped after 100 iterations
# weights:  57
initial  value 1379.853895 
iter  10 value 929.231851
iter  20 value 770.466255
iter  30 value 745.839575
iter  40 value 742.831363
iter  50 value 742.227328
iter  60 value 742.031197
iter  70 value 741.991859
iter  80 value 741.984424
iter  90 value 741.982025
iter 100 value 741.964068
final  value 741.964068 
stopped after 100 iterations
# weights:  69
initial  value 1123.922233 
iter  10 value 1074.445895
iter  20 value 714.033535
iter  30 value 692.617106
iter  40 value 691.862338
iter  50 value 691.754551
iter  60 value 691.730210
iter  70 value 691.718328
iter  80 value 691.716595
iter  90 value 691.716015
iter 100 value 691.715506
final  value 691.715506 
stopped after 100 iterations
# weights:  61
initial  value 1162.354799 
iter  10 value 795.783532
iter  20 value 792.542994
iter  30 value 792.504002
final  value 792.503366 
converged
# weights:  45
initial  value 1190.417973 
iter  10 value 724.602195
iter  20 value 709.094114
iter  30 value 707.883748
iter  40 value 707.794026
iter  50 value 707.774603
final  value 707.774524 
converged
# weights:  57
initial  value 1327.161449 
iter  10 value 987.310901
iter  20 value 882.749068
iter  30 value 869.939784
iter  40 value 868.163597
iter  50 value 867.932663
iter  60 value 867.912753
iter  70 value 867.837255
final  value 867.834774 
converged
# weights:  37
initial  value 1185.004166 
iter  10 value 906.404210
iter  20 value 895.960479
iter  30 value 895.725837
iter  40 value 894.806382
iter  50 value 893.175104
final  value 893.167271 
converged
# weights:  49
initial  value 1158.961577 
iter  10 value 967.135756
iter  20 value 705.305601
iter  30 value 698.866334
iter  40 value 696.684431
iter  50 value 695.821632
iter  60 value 695.677938
iter  70 value 695.654126
iter  80 value 695.652104
iter  90 value 695.651258
iter 100 value 695.650219
final  value 695.650219 
stopped after 100 iterations
# weights:  61
initial  value 1367.023269 
iter  10 value 861.270797
iter  20 value 804.534497
iter  30 value 803.649527
iter  40 value 803.636263
final  value 803.636229 
converged
# weights:  77
initial  value 1130.446157 
iter  10 value 1071.504017
iter  20 value 1071.313852
iter  30 value 694.548509
iter  40 value 689.978515
iter  50 value 689.848880
iter  60 value 689.448197
iter  70 value 689.136462
iter  80 value 689.105474
iter  90 value 689.096125
iter 100 value 689.094127
final  value 689.094127 
stopped after 100 iterations
# weights:  9
initial  value 1261.435569 
iter  10 value 1095.857648
iter  20 value 1095.071022
iter  30 value 1093.852823
iter  40 value 715.026613
iter  50 value 711.743615
iter  60 value 711.165211
iter  70 value 711.103351
iter  80 value 711.089349
final  value 711.088072 
converged
# weights:  41
initial  value 1191.908945 
iter  10 value 1095.097781
final  value 1095.073618 
converged
# weights:  25
initial  value 1316.849618 
iter  10 value 1095.195348
final  value 1095.079198 
converged
# weights:  53
initial  value 1151.854406 
iter  10 value 1095.090885
final  value 1095.073433 
converged
# weights:  53
initial  value 1260.349788 
iter  10 value 907.063699
iter  20 value 733.125573
iter  30 value 727.577067
iter  40 value 727.313001
iter  50 value 727.220736
iter  60 value 727.199327
iter  70 value 727.192065
final  value 727.192027 
converged
# weights:  29
initial  value 1319.625738 
iter  10 value 1095.820077
iter  20 value 1095.091916
iter  30 value 758.566746
iter  40 value 733.544118
iter  50 value 728.278785
iter  60 value 727.455660
iter  70 value 727.082979
iter  80 value 726.050546
iter  90 value 725.534336
iter 100 value 725.254402
final  value 725.254402 
stopped after 100 iterations
# weights:  29
initial  value 1170.816533 
iter  10 value 745.613990
iter  20 value 743.663450
iter  30 value 740.397188
iter  40 value 738.586134
iter  50 value 738.572920
final  value 738.568731 
converged
# weights:  49
initial  value 1101.457774 
iter  10 value 719.399446
iter  20 value 718.219632
iter  30 value 718.145628
iter  40 value 717.866133
iter  50 value 717.778909
iter  60 value 717.770535
iter  70 value 717.769650
iter  80 value 717.769348
final  value 717.769299 
converged
# weights:  21
initial  value 1153.318342 
iter  10 value 1096.491208
iter  20 value 766.066737
iter  30 value 723.285372
iter  40 value 715.187219
iter  50 value 714.035873
iter  60 value 713.771186
iter  70 value 713.754817
iter  80 value 713.752808
final  value 713.752628 
converged
# weights:  25
initial  value 1336.463164 
iter  10 value 900.874800
iter  20 value 781.149434
iter  30 value 776.087799
iter  40 value 775.744267
iter  50 value 775.548764
iter  50 value 775.548759
iter  50 value 775.548759
final  value 775.548759 
converged
# weights:  61
initial  value 1346.939779 
iter  10 value 1097.569080
iter  20 value 1095.100534
iter  30 value 1095.072520
iter  40 value 1088.323248
iter  50 value 716.643486
iter  60 value 711.600109
iter  70 value 711.319859
iter  80 value 711.035534
iter  90 value 710.989246
iter 100 value 710.958366
final  value 710.958366 
stopped after 100 iterations
# weights:  57
initial  value 1214.212808 
iter  10 value 775.979601
iter  20 value 766.352088
iter  30 value 765.709459
final  value 765.703030 
converged
# weights:  69
initial  value 1374.929333 
iter  10 value 1100.746959
iter  20 value 763.728025
iter  30 value 718.801845
iter  40 value 714.597847
iter  50 value 713.582391
iter  60 value 713.432297
iter  70 value 713.413294
iter  80 value 713.406969
iter  90 value 713.402746
iter 100 value 713.400572
final  value 713.400572 
stopped after 100 iterations
# weights:  61
initial  value 1396.179475 
iter  10 value 885.898927
iter  20 value 819.958957
iter  30 value 815.125560
iter  40 value 814.495596
iter  50 value 814.455305
iter  60 value 814.453480
iter  70 value 814.452807
final  value 814.452758 
converged
# weights:  45
initial  value 1332.705185 
iter  10 value 862.220231
iter  20 value 732.220008
iter  30 value 729.908106
iter  40 value 729.862749
iter  50 value 729.862171
final  value 729.862100 
converged
# weights:  57
initial  value 1590.127482 
iter  10 value 987.501751
iter  20 value 917.704799
iter  30 value 895.917192
iter  40 value 891.033693
iter  50 value 889.645482
iter  60 value 889.608028
iter  70 value 889.591282
final  value 889.590993 
converged
# weights:  37
initial  value 1217.633973 
iter  10 value 982.625421
iter  20 value 920.245976
iter  30 value 915.794647
iter  40 value 915.464636
iter  50 value 915.449134
iter  50 value 915.449133
iter  50 value 915.449132
final  value 915.449132 
converged
# weights:  49
initial  value 1297.953905 
iter  10 value 1049.662780
iter  20 value 739.670319
iter  30 value 719.054661
iter  40 value 717.804396
iter  50 value 717.526248
iter  60 value 717.381744
iter  70 value 717.367727
iter  80 value 717.353739
iter  90 value 717.342817
iter 100 value 717.340310
final  value 717.340310 
stopped after 100 iterations
# weights:  61
initial  value 1125.954625 
iter  10 value 857.609206
iter  20 value 826.207173
iter  30 value 825.726508
iter  40 value 825.587104
iter  50 value 825.566828
final  value 825.566804 
converged
# weights:  77
initial  value 1336.728645 
iter  10 value 1095.277331
iter  20 value 1095.074173
final  value 1095.074133 
converged
# weights:  9
initial  value 1252.827346 
iter  10 value 1062.191061
iter  20 value 1060.708791
iter  30 value 703.488161
iter  40 value 699.891435
iter  50 value 690.848736
iter  60 value 684.783714
iter  70 value 683.715892
iter  80 value 683.451922
iter  90 value 682.922071
iter 100 value 682.748877
final  value 682.748877 
stopped after 100 iterations
# weights:  41
initial  value 1122.836331 
iter  10 value 1060.986109
final  value 1060.965694 
converged
# weights:  25
initial  value 1238.338077 
iter  10 value 1061.115138
final  value 1060.969871 
converged
# weights:  53
initial  value 1322.380515 
iter  10 value 1061.030178
iter  20 value 1060.965893
final  value 1060.965726 
converged
# weights:  53
initial  value 1109.507918 
iter  10 value 877.471961
iter  20 value 704.703204
iter  30 value 699.315462
iter  40 value 698.811940
iter  50 value 698.769528
iter  60 value 698.767024
iter  70 value 698.766766
iter  80 value 698.766574
final  value 698.766545 
converged
# weights:  29
initial  value 1207.051748 
iter  10 value 1061.529024
iter  20 value 1060.870660
iter  30 value 691.496121
iter  40 value 685.248197
iter  50 value 683.241144
iter  60 value 682.906848
iter  70 value 682.847900
iter  80 value 682.750896
iter  90 value 682.643646
iter 100 value 682.591817
final  value 682.591817 
stopped after 100 iterations
# weights:  29
initial  value 1360.548071 
iter  10 value 724.286414
iter  20 value 709.827311
iter  30 value 709.614590
final  value 709.613968 
converged
# weights:  49
initial  value 1357.570092 
iter  10 value 696.109901
iter  20 value 689.957163
iter  30 value 689.452248
iter  40 value 689.198968
iter  50 value 689.129984
iter  60 value 689.127209
iter  70 value 689.125612
final  value 689.125176 
converged
# weights:  21
initial  value 1174.059637 
iter  10 value 694.932070
iter  20 value 688.664005
iter  30 value 686.100969
iter  40 value 685.616949
iter  50 value 685.417853
iter  60 value 685.346655
final  value 685.345985 
converged
# weights:  25
initial  value 1097.224935 
iter  10 value 753.923397
iter  20 value 747.310819
iter  30 value 747.212239
final  value 747.211015 
converged
# weights:  61
initial  value 1423.627666 
iter  10 value 1061.844428
iter  20 value 1060.989321
iter  30 value 1060.921879
iter  40 value 721.187666
iter  50 value 696.315464
iter  60 value 694.722417
iter  70 value 685.560248
iter  80 value 683.389820
iter  90 value 683.084200
iter 100 value 682.881733
final  value 682.881733 
stopped after 100 iterations
# weights:  57
initial  value 1127.502488 
iter  10 value 739.025297
iter  20 value 735.665551
iter  30 value 735.460128
final  value 735.458748 
converged
# weights:  69
initial  value 1232.209971 
iter  10 value 712.209086
iter  20 value 687.463923
iter  30 value 685.251320
iter  40 value 685.050484
iter  50 value 684.996633
iter  60 value 684.985716
iter  70 value 684.982191
iter  80 value 684.981076
final  value 684.980920 
converged
# weights:  61
initial  value 1367.110553 
iter  10 value 886.265362
iter  20 value 801.024715
iter  30 value 788.032760
iter  40 value 786.682856
iter  50 value 786.514960
iter  60 value 786.441105
iter  70 value 786.439476
iter  80 value 786.434119
final  value 786.433517 
converged
# weights:  45
initial  value 1298.214996 
iter  10 value 810.148763
iter  20 value 706.154711
iter  30 value 701.301908
iter  40 value 701.162279
iter  50 value 701.161008
final  value 701.160654 
converged
# weights:  57
initial  value 1242.914919 
iter  10 value 909.503268
iter  20 value 864.220157
iter  30 value 861.532642
iter  40 value 860.847089
iter  50 value 860.804378
final  value 860.804141 
converged
# weights:  37
initial  value 1149.083719 
iter  10 value 918.675499
iter  20 value 890.395744
iter  30 value 887.801497
iter  40 value 887.665323
final  value 887.663263 
converged
# weights:  49
initial  value 1198.435829 
iter  10 value 927.543023
iter  20 value 695.262434
iter  30 value 689.255740
iter  40 value 689.022820
iter  50 value 689.004401
iter  60 value 688.985014
iter  70 value 688.978338
final  value 688.977871 
converged
# weights:  61
initial  value 1249.183237 
iter  10 value 905.066318
iter  20 value 805.081140
iter  30 value 799.624308
iter  40 value 798.380504
iter  50 value 798.194076
iter  60 value 798.190776
iter  60 value 798.190773
iter  60 value 798.190773
final  value 798.190773 
converged
# weights:  77
initial  value 1319.485241 
iter  10 value 1061.150660
iter  20 value 1060.967296
final  value 1060.967125 
converged
# weights:  61
initial  value 1977.313804 
iter  10 value 1618.417857
iter  20 value 1613.728816
iter  30 value 1144.094548
iter  40 value 1061.860662
iter  50 value 1042.088884
iter  60 value 1041.628776
iter  70 value 1041.433089
iter  80 value 1041.380545
iter  90 value 1041.331053
iter 100 value 1041.253648
final  value 1041.253648 
stopped after 100 iterations
Neural Network 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  size  decay         RMSE      Rsquared   Selected
   2    6.814307e-04  1.176460  0.7191522          
   5    1.935979e-02  1.177522  0.7104224          
   6    4.717743e-05  1.369463  0.6949814          
   6    1.196505e+00  1.190982  0.7938033          
   7    2.318981e-04  1.179507  0.6834101          
   7    4.298411e-01  1.181588  0.7584305          
   9    7.506058e+00  1.241442  0.8934247          
  10    1.333386e-05  1.464724  0.4192760          
  11    2.844800e-01  1.179721  0.7493876          
  12    7.704418e-02  1.177698  0.7251749          
  12    8.022455e-02  1.177791  0.7254246          
  13    1.142916e-05  1.464724  0.2167413          
  13    2.447684e-01  1.179232  0.7463583          
  14    1.114970e+00  1.187799  0.7943927          
  14    6.239839e+00  1.229960  0.8823555          
  15    3.266783e-04  1.176424  0.7197353  *       
  15    2.829767e+00  1.202434  0.8381381          
  15    3.276243e+00  1.206168  0.8458775          
  17    2.222147e-02  1.177275  0.7144302          
  19    3.818966e-05  1.369468  0.6943622          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 15 and decay = 0.0003266783.
[1] "Sun Mar 11 15:40:34 2018"
Loading required package: nnls
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  predictions failed for Fold1: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments

2: In eval(xpr, envir = envir) :
  predictions failed for Fold2: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments

3: In eval(xpr, envir = envir) :
  predictions failed for Fold3: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments

4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  predictions failed for Fold1: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments

2: In eval(xpr, envir = envir) :
  predictions failed for Fold2: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments

3: In eval(xpr, envir = envir) :
  predictions failed for Fold3: parameter=none Error in .Primitive("%*%")(x, y) : 
  requires numeric/complex matrix/vector arguments

4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:40:36 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "nnls"                    
Loading required package: ordinalNet
Error : The tuning parameter grid should have columns alpha, criteria, link
Error : The tuning parameter grid should have columns alpha, criteria, link
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :12    NA's   :12   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:40:39 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "ordinalNet"              
Loading required package: e1071
Loading required package: randomForest
randomForest 4.6-12
Type rfNews() to see new features/changes/bug fixes.

Attaching package: 'randomForest'

The following object is masked from 'package:ggplot2':

    margin

Parallel Random Forest 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   Selected
  1     0.1925353  0.9866965          
  2     0.1620888  0.9894842  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 2.
[1] "Sun Mar 11 15:40:48 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: Warning message:
executing %dopar% sequentially: no parallel backend registered 
# weights:  9
initial  value 1208.439802 
iter  10 value 1072.226676
iter  20 value 1071.359972
iter  30 value 1071.303520
iter  40 value 1069.948636
iter  50 value 691.420027
iter  60 value 689.626207
iter  70 value 689.552801
iter  80 value 689.475413
iter  90 value 689.446671
final  value 689.446338 
converged
# weights:  41
initial  value 1333.548473 
iter  10 value 1071.365893
final  value 1071.313100 
converged
# weights:  25
initial  value 1224.536377 
iter  10 value 1071.425909
final  value 1071.317144 
converged
# weights:  53
initial  value 1167.059784 
iter  10 value 1071.352684
final  value 1071.312125 
converged
# weights:  53
initial  value 1171.413167 
iter  10 value 954.194803
iter  20 value 740.655474
iter  30 value 710.788539
iter  40 value 707.747770
iter  50 value 707.060261
iter  60 value 706.623579
iter  70 value 706.228954
iter  80 value 706.014968
iter  90 value 705.926321
iter 100 value 705.901240
final  value 705.901240 
stopped after 100 iterations
# weights:  29
initial  value 1159.296692 
iter  10 value 1071.895498
iter  20 value 738.813068
iter  30 value 689.984433
iter  40 value 689.319161
iter  50 value 689.279314
iter  60 value 689.252039
iter  70 value 689.211619
iter  80 value 689.198987
iter  90 value 689.189096
iter 100 value 689.186077
final  value 689.186077 
stopped after 100 iterations
# weights:  29
initial  value 1166.250315 
iter  10 value 930.249046
iter  20 value 729.156747
iter  30 value 718.064946
iter  40 value 717.462862
iter  50 value 717.253770
iter  60 value 717.241965
iter  70 value 717.239337
final  value 717.239151 
converged
# weights:  49
initial  value 1175.632844 
iter  10 value 738.606019
iter  20 value 697.015692
iter  30 value 696.197011
iter  40 value 696.037152
iter  50 value 696.002174
iter  60 value 695.968581
iter  70 value 695.966693
final  value 695.966620 
converged
# weights:  21
initial  value 1166.018258 
iter  10 value 1073.447035
iter  20 value 704.162773
iter  30 value 694.867074
iter  40 value 692.659821
iter  50 value 692.260060
iter  60 value 692.136196
iter  70 value 692.116187
final  value 692.115977 
converged
# weights:  25
initial  value 1188.580199 
iter  10 value 752.920268
iter  20 value 750.756565
iter  30 value 750.622708
final  value 750.620750 
converged
# weights:  61
initial  value 1239.617673 
iter  10 value 1073.796505
iter  20 value 1071.340283
iter  30 value 718.777143
iter  40 value 690.194189
iter  50 value 689.923105
iter  60 value 689.702675
iter  70 value 689.435774
iter  80 value 689.387111
iter  90 value 689.327728
iter 100 value 689.299771
final  value 689.299771 
stopped after 100 iterations
# weights:  57
initial  value 1422.792573 
iter  10 value 891.234546
iter  20 value 768.162181
iter  30 value 746.476265
iter  40 value 743.245247
iter  50 value 742.987132
iter  60 value 742.908580
iter  70 value 742.865673
iter  80 value 742.864024
final  value 742.863836 
converged
# weights:  69
initial  value 1098.901891 
iter  10 value 1072.829474
iter  20 value 708.645802
iter  30 value 693.985992
iter  40 value 692.284167
iter  50 value 692.064821
iter  60 value 691.942267
iter  70 value 691.881817
iter  80 value 691.837696
iter  90 value 691.793710
iter 100 value 691.778159
final  value 691.778159 
stopped after 100 iterations
# weights:  61
initial  value 1168.897728 
iter  10 value 807.385945
iter  20 value 794.284818
iter  30 value 794.174958
final  value 794.174755 
converged
# weights:  45
initial  value 1120.790743 
iter  10 value 708.665594
iter  20 value 708.158373
iter  30 value 708.090811
final  value 708.090763 
converged
# weights:  57
initial  value 1274.885074 
iter  10 value 970.286702
iter  20 value 884.938134
iter  30 value 872.989934
iter  40 value 869.551697
iter  50 value 869.136418
iter  60 value 869.076658
iter  70 value 869.027856
final  value 869.027552 
converged
# weights:  37
initial  value 1196.870679 
iter  10 value 925.497916
iter  20 value 908.987810
iter  30 value 898.263653
iter  40 value 896.683232
iter  50 value 896.609512
final  value 896.609213 
converged
# weights:  49
initial  value 1193.366891 
iter  10 value 1042.259970
iter  20 value 705.256474
iter  30 value 698.302267
iter  40 value 696.258765
iter  50 value 695.962066
iter  60 value 695.849709
iter  70 value 695.830176
iter  80 value 695.821612
iter  90 value 695.819709
iter 100 value 695.816270
final  value 695.816270 
stopped after 100 iterations
# weights:  61
initial  value 1333.234205 
iter  10 value 828.280161
iter  20 value 809.415021
iter  30 value 807.499926
iter  40 value 807.368304
iter  50 value 807.347480
iter  60 value 807.343957
final  value 807.343935 
converged
# weights:  77
initial  value 1200.990874 
iter  10 value 1071.586537
iter  20 value 1071.314803
final  value 1071.313789 
converged
# weights:  9
initial  value 1246.027082 
iter  10 value 1096.029012
iter  20 value 1095.148821
iter  30 value 1095.124566
iter  40 value 1095.043446
iter  50 value 772.275375
iter  60 value 712.020106
iter  70 value 711.253513
iter  80 value 711.116000
iter  90 value 711.094476
final  value 711.093272 
converged
# weights:  41
initial  value 1204.442383 
iter  10 value 1095.099295
final  value 1095.073540 
converged
# weights:  25
initial  value 1338.078287 
iter  10 value 1095.182945
iter  20 value 1095.080099
final  value 1095.076941 
converged
# weights:  53
initial  value 1162.290985 
iter  10 value 1095.092376
iter  20 value 1095.070811
iter  30 value 1095.070103
iter  40 value 1095.068820
iter  50 value 1095.065864
iter  60 value 1095.053356
iter  70 value 713.710026
iter  80 value 711.630795
iter  90 value 711.438275
iter 100 value 710.913149
final  value 710.913149 
stopped after 100 iterations
# weights:  53
initial  value 1229.454128 
iter  10 value 877.552014
iter  20 value 731.120900
iter  30 value 727.850704
iter  40 value 727.424708
iter  50 value 727.405356
iter  60 value 727.403178
final  value 727.402990 
converged
# weights:  29
initial  value 1297.556015 
iter  10 value 1095.734039
iter  20 value 1095.089761
iter  30 value 1095.082219
iter  40 value 1095.077215
iter  50 value 1095.048658
iter  60 value 720.986541
iter  70 value 715.988125
iter  80 value 712.037074
iter  90 value 711.593759
iter 100 value 711.447251
final  value 711.447251 
stopped after 100 iterations
# weights:  29
initial  value 1185.912903 
iter  10 value 774.645964
iter  20 value 744.639724
iter  30 value 738.788637
iter  40 value 738.469596
iter  50 value 738.468057
final  value 738.468009 
converged
# weights:  49
initial  value 1108.937495 
iter  10 value 733.959743
iter  20 value 718.974927
iter  30 value 717.997778
iter  40 value 717.854123
iter  50 value 717.733318
iter  60 value 717.678509
iter  70 value 717.663500
iter  80 value 717.661896
iter  90 value 717.661646
final  value 717.661633 
converged
# weights:  21
initial  value 1121.964985 
iter  10 value 1096.389936
iter  20 value 773.938734
iter  30 value 720.109084
iter  40 value 715.414224
iter  50 value 714.974917
iter  60 value 714.880402
iter  70 value 714.831506
final  value 714.830812 
converged
# weights:  25
initial  value 1305.073742 
iter  10 value 959.139463
iter  20 value 777.713072
iter  30 value 773.600911
iter  40 value 772.818862
final  value 772.806178 
converged
# weights:  61
initial  value 1396.989610 
iter  10 value 1096.780806
iter  20 value 1095.092731
iter  30 value 1095.081171
iter  40 value 1095.076327
iter  50 value 1095.037047
iter  60 value 712.143135
iter  70 value 711.305321
iter  80 value 711.014106
iter  90 value 710.965430
iter 100 value 710.934204
final  value 710.934204 
stopped after 100 iterations
# weights:  57
initial  value 1198.855856 
iter  10 value 776.566884
iter  20 value 765.101526
iter  30 value 764.656070
final  value 764.647195 
converged
# weights:  69
initial  value 1340.028414 
iter  10 value 1101.599605
iter  20 value 732.262410
iter  30 value 719.097549
iter  40 value 713.978014
iter  50 value 713.569329
iter  60 value 713.492369
iter  70 value 713.451698
iter  80 value 713.428308
iter  90 value 713.424748
iter 100 value 713.421856
final  value 713.421856 
stopped after 100 iterations
# weights:  61
initial  value 1354.991167 
iter  10 value 825.737883
iter  20 value 816.794320
iter  30 value 816.307450
iter  40 value 816.263846
final  value 816.260081 
converged
# weights:  45
initial  value 1338.771335 
iter  10 value 972.915015
iter  20 value 734.057294
iter  30 value 730.057452
iter  40 value 729.955744
iter  50 value 729.929645
iter  60 value 729.920492
iter  60 value 729.920487
iter  60 value 729.920487
final  value 729.920487 
converged
# weights:  57
initial  value 1577.973903 
iter  10 value 1043.342479
iter  20 value 924.340853
iter  30 value 899.267912
iter  40 value 893.953773
iter  50 value 892.620811
iter  60 value 891.200751
iter  70 value 890.780321
iter  80 value 890.653228
iter  90 value 890.605708
final  value 890.531129 
converged
# weights:  37
initial  value 1210.827219 
iter  10 value 1023.027530
iter  20 value 936.730720
iter  30 value 921.735832
iter  40 value 918.113293
iter  50 value 917.920310
iter  60 value 917.840042
final  value 917.839944 
converged
# weights:  49
initial  value 1201.013990 
iter  10 value 968.369331
iter  20 value 727.662462
iter  30 value 718.303955
iter  40 value 717.627857
iter  50 value 717.479955
iter  60 value 717.460231
iter  70 value 717.450018
iter  80 value 717.448517
final  value 717.448063 
converged
# weights:  61
initial  value 1130.230318 
iter  10 value 840.369365
iter  20 value 828.514300
iter  30 value 827.653100
iter  40 value 827.548898
iter  50 value 827.543740
final  value 827.543723 
converged
# weights:  77
initial  value 1400.353649 
iter  10 value 1095.240646
iter  20 value 1095.074704
final  value 1095.074676 
converged
# weights:  9
initial  value 1252.996211 
iter  10 value 1061.883689
iter  20 value 1061.043368
iter  30 value 1060.954057
iter  40 value 1045.060648
iter  50 value 695.551952
iter  60 value 690.945331
iter  70 value 687.695268
iter  80 value 686.856268
iter  90 value 686.296301
iter 100 value 683.971042
final  value 683.971042 
stopped after 100 iterations
# weights:  41
initial  value 1150.808762 
iter  10 value 1060.989633
iter  20 value 1060.963734
iter  30 value 1060.962897
iter  40 value 1060.961320
iter  50 value 1060.957303
iter  60 value 1060.931236
iter  70 value 686.223659
iter  80 value 683.078884
iter  90 value 682.638074
iter 100 value 682.404938
final  value 682.404938 
stopped after 100 iterations
# weights:  25
initial  value 1231.329872 
iter  10 value 1061.128889
iter  20 value 1060.969507
iter  30 value 685.769928
iter  40 value 682.810135
iter  50 value 682.394262
iter  60 value 682.373328
iter  70 value 682.365930
iter  80 value 682.362034
iter  90 value 682.360642
iter 100 value 682.358963
final  value 682.358963 
stopped after 100 iterations
# weights:  53
initial  value 1340.805780 
iter  10 value 1061.019859
iter  20 value 1060.965785
final  value 1060.965766 
converged
# weights:  53
initial  value 1085.398348 
iter  10 value 702.054447
iter  20 value 698.834498
iter  30 value 698.769684
final  value 698.768746 
converged
# weights:  29
initial  value 1249.558947 
iter  10 value 1061.438743
iter  20 value 1060.893844
iter  30 value 684.646780
iter  40 value 682.779420
iter  50 value 682.720609
iter  60 value 682.635013
iter  70 value 682.588231
iter  80 value 682.563822
iter  90 value 682.546773
iter 100 value 682.530891
final  value 682.530891 
stopped after 100 iterations
# weights:  29
initial  value 1386.413958 
iter  10 value 789.226585
iter  20 value 713.870335
iter  30 value 709.845345
iter  40 value 709.677789
iter  50 value 709.619367
final  value 709.618650 
converged
# weights:  49
initial  value 1371.932754 
iter  10 value 710.581567
iter  20 value 689.960424
iter  30 value 689.403567
iter  40 value 689.162677
iter  50 value 689.137576
iter  60 value 689.129254
iter  70 value 689.126536
iter  80 value 689.126167
iter  90 value 689.125937
final  value 689.125881 
converged
# weights:  21
initial  value 1159.225680 
iter  10 value 715.792077
iter  20 value 687.915307
iter  30 value 686.181487
iter  40 value 685.650065
iter  50 value 685.452366
iter  60 value 685.348165
final  value 685.346547 
converged
# weights:  25
initial  value 1107.287528 
iter  10 value 769.006985
iter  20 value 746.422021
iter  30 value 744.039657
iter  40 value 743.300579
final  value 743.283022 
converged
# weights:  61
initial  value 1457.979967 
iter  10 value 1061.662554
iter  20 value 1060.991151
iter  30 value 1060.684703
iter  40 value 708.550516
iter  50 value 706.914493
iter  60 value 696.334708
iter  70 value 694.846800
iter  80 value 692.865497
iter  90 value 689.835595
iter 100 value 687.348298
final  value 687.348298 
stopped after 100 iterations
# weights:  57
initial  value 1102.586413 
iter  10 value 737.068306
iter  20 value 735.579124
iter  30 value 735.508170
final  value 735.507214 
converged
# weights:  69
initial  value 1249.180366 
iter  10 value 750.010802
iter  20 value 687.266069
iter  30 value 685.301957
iter  40 value 685.106545
iter  50 value 685.024837
iter  60 value 685.006205
iter  70 value 684.998764
iter  80 value 684.996139
iter  90 value 684.993740
iter 100 value 684.991830
final  value 684.991830 
stopped after 100 iterations
# weights:  61
initial  value 1395.679405 
iter  10 value 919.276550
iter  20 value 791.773490
iter  30 value 787.224754
iter  40 value 786.930164
iter  50 value 786.779284
iter  60 value 786.428571
iter  70 value 786.388752
final  value 786.387972 
converged
# weights:  45
initial  value 1299.561580 
iter  10 value 843.098350
iter  20 value 704.575861
iter  30 value 701.108694
iter  40 value 701.025865
final  value 701.025084 
converged
# weights:  57
initial  value 1249.547695 
iter  10 value 927.485581
iter  20 value 867.186383
iter  30 value 862.235894
iter  40 value 862.157695
final  value 862.156528 
converged
# weights:  37
initial  value 1168.250470 
iter  10 value 910.011938
iter  20 value 891.112795
iter  30 value 889.064533
iter  40 value 888.170030
iter  50 value 888.135943
iter  50 value 888.135941
iter  50 value 888.135941
final  value 888.135941 
converged
# weights:  49
initial  value 1292.682997 
iter  10 value 961.518505
iter  20 value 696.917048
iter  30 value 689.989381
iter  40 value 689.087667
iter  50 value 689.030588
iter  60 value 688.967900
iter  70 value 688.921112
iter  80 value 688.919950
iter  90 value 688.917932
iter 100 value 688.917140
final  value 688.917140 
stopped after 100 iterations
# weights:  61
initial  value 1201.969556 
iter  10 value 922.888026
iter  20 value 816.107789
iter  30 value 799.914066
iter  40 value 798.667318
iter  50 value 798.401980
iter  60 value 798.125061
iter  70 value 798.115359
final  value 798.115324 
converged
# weights:  77
initial  value 1298.908907 
iter  10 value 1061.182764
iter  20 value 1060.967384
iter  30 value 1060.440540
iter  40 value 722.924439
iter  50 value 691.341512
iter  60 value 688.730228
iter  70 value 683.345027
iter  80 value 682.597567
iter  90 value 682.532340
iter 100 value 682.512209
final  value 682.512209 
stopped after 100 iterations
# weights:  29
initial  value 1714.208056 
iter  10 value 1614.320644
iter  20 value 1613.689755
iter  30 value 1613.679554
iter  40 value 1587.685581
iter  50 value 1042.630794
iter  60 value 1041.695000
iter  70 value 1041.609472
iter  80 value 1041.484069
iter  90 value 1041.370816
iter 100 value 1041.287695
final  value 1041.287695 
stopped after 100 iterations
Neural Networks with Feature Extraction 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  size  decay         RMSE      Rsquared   Selected
   2    6.814307e-04  1.176704  0.7144336          
   5    1.935979e-02  1.177590  0.7105767          
   6    4.717743e-05  1.368569  0.5072789          
   6    1.196505e+00  1.190106  0.7943708          
   7    2.318981e-04  1.176425  0.7198421  *       
   7    4.298411e-01  1.181621  0.7590975          
   9    7.506058e+00  1.242427  0.8944257          
  10    1.333386e-05  1.368606  0.5519927          
  11    2.844800e-01  1.179715  0.7500150          
  12    7.704418e-02  1.177702  0.7254012          
  12    8.022455e-02  1.177722  0.7259914          
  13    1.142916e-05  1.367846  0.5704765          
  13    2.447684e-01  1.179252  0.7467297          
  14    1.114970e+00  1.187734  0.7953499          
  14    6.239839e+00  1.230234  0.8837906          
  15    3.266783e-04  1.177402  0.7099709          
  15    2.829767e+00  1.202791  0.8392244          
  15    3.276243e+00  1.206826  0.8466005          
  17    2.222147e-02  1.177277  0.7145244          
  19    3.818966e-05  1.368593  0.5724573          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 7 and decay = 0.0002318981.
[1] "Sun Mar 11 15:40:58 2018"
Principal Component Analysis 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results:

  RMSE       Rsquared
  0.4196538  0.862089

Tuning parameter 'ncomp' was held constant at a value of 1
[1] "Sun Mar 11 15:41:00 2018"
Installing package into 'C:/Users/irina grishina/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Loading required package: plsRglm
____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.00193381169810891) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0504706743173301) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0193981921300292) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

Partial Least Squares Generalized Linear Models  

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  nt  alpha.pvals.expli  RMSE         Rsquared   Selected
  1   0.004165192        0.003496351  0.9999944          
  1   0.022457808        0.003496351  0.9999944          
  1   0.045509905        0.003496351  0.9999944          
  1   0.061114057        0.003496351  0.9999944          
  1   0.109563354        0.003496351  0.9999944          
  1   0.154443596        0.003496351  0.9999944          
  1   0.169263815        0.003496351  0.9999944          
  1   0.195847065        0.003496351  0.9999944          
  2   0.001933812        0.003117766  0.9999956          
  2   0.019398192        0.003117766  0.9999956          
  2   0.050470674        0.003117766  0.9999956          
  2   0.111559091        0.003109473  0.9999956          
  2   0.129557995        0.003109473  0.9999956          
  2   0.130143576        0.003109473  0.9999956          
  2   0.146291844        0.003109473  0.9999956          
  2   0.148468389        0.003109473  0.9999956          
  2   0.168242110        0.003109473  0.9999956          
  2   0.181725022        0.003109473  0.9999956          
  2   0.183845870        0.003109473  0.9999956          
  2   0.193172446        0.003109473  0.9999956  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nt = 2 and alpha.pvals.expli
 = 0.1931724.
[1] "Sun Mar 11 15:41:42 2018"
Projection Pursuit Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  nterms  RMSE        Rsquared   Selected
   1      0.03150031  0.9989048  *       
   2      0.03151258  0.9989046          
   3      0.03152937  0.9989046          
   4      0.03152210  0.9989046          
   5      0.03154115  0.9989046          
   6      0.03155893  0.9989045          
   7      0.03154905  0.9989045          
   8      0.03159016  0.9989044          
   9      0.03159154  0.9989044          
  10      0.03159154  0.9989044          
  11      0.03159154  0.9989044          
  12      0.03159154  0.9989044          
  13      0.03159154  0.9989044          
  14      0.03159154  0.9989044          
  15      0.03159154  0.9989044          
  16      0.03159154  0.9989044          
  17      0.03159154  0.9989044          
  18      0.03159154  0.9989044          
  19      0.03159154  0.9989044          
  20      0.03159154  0.9989044          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was nterms = 1.
[1] "Sun Mar 11 15:41:45 2018"
Installing package into 'C:/Users/irina grishina/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Loading required package: quantregForest
Loading required package: RColorBrewer
Quantile Random Forest 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   Selected
  1     0.5587092  0.9555576          
  2     0.3955439  0.9717222  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 2.
[1] "Sun Mar 11 15:41:57 2018"
Loading required package: ranger

Attaching package: 'ranger'

The following object is masked from 'package:randomForest':

    importance

Random Forest 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   Selected
  1     0.1941831  0.9864436          
  2     0.1643228  0.9891920  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 2.
[1] "Sun Mar 11 15:42:02 2018"
Error in code$varImp(object$finalModel, ...) : 
  No importance values available
In addition: Warning messages:
1: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
2: package 'gpls' is not available (for R version 3.4.1) 
3: package 'rPython' is not available (for R version 3.4.1) 
Loading required package: Rborist
Loading required package: Rcpp
Rborist 0.1-7
Type RboristNews() to see new features/changes/bug fixes.
Random Forest 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  predFixed  RMSE       Rsquared   Selected
  1          0.1963449  0.9848529          
  2          0.1704290  0.9879524  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was predFixed = 2.
[1] "Sun Mar 11 15:42:23 2018"
Loading required package: relaxo
Relaxed Lasso 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  phi          lambda    RMSE      Rsquared  Selected
  0.009669058  825.2369  1.464724  NaN       *       
  0.020825960  820.8162  1.464724  NaN               
  0.096990961  833.6085  1.464724  NaN               
  0.112289042  813.6455  1.464724  NaN               
  0.227549524  815.3264  1.464724  NaN               
  0.252353372  828.0154  1.464724  NaN               
  0.305570287  808.5860  1.464724  NaN               
  0.547816768  812.1393  1.464724  NaN               
  0.557795454  830.3586  1.464724  NaN               
  0.647789975  822.7331  1.464724  NaN               
  0.650717882  823.6375  1.464724  NaN               
  0.731459219  824.4782  1.464724  NaN               
  0.742341945  821.8946  1.464724  NaN               
  0.772217982  815.1987  1.464724  NaN               
  0.841210552  826.2082  1.464724  NaN               
  0.846319073  814.0368  1.464724  NaN               
  0.908625109  828.2891  1.464724  NaN               
  0.919229351  828.1137  1.464724  NaN               
  0.965862230  826.7361  1.464724  NaN               
  0.979235323  819.5827  1.464724  NaN               

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 825.2369 and phi
 = 0.009669058.
[1] "Sun Mar 11 15:42:25 2018"
Random Forest 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   Selected
  1     0.1958326  0.9861637          
  2     0.1655520  0.9891388  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 2.
[1] "Sun Mar 11 15:42:31 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Ridge Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  lambda        RMSE         Rsquared   Selected
  3.248846e-05  0.003109470  0.9999956          
  1.665083e-04  0.003109470  0.9999956  *       
  3.321585e-04  0.003109497  0.9999956          
  3.973393e-04  0.003109517  0.9999956          
  6.761155e-04  0.003109654  0.9999956          
  7.167755e-04  0.003109681  0.9999956          
  4.990330e-03  0.003122837  0.9999956          
  8.740596e-03  0.003150634  0.9999956          
  1.425807e-02  0.003217074  0.9999956          
  2.084964e-02  0.003331548  0.9999956          
  3.139992e-02  0.003578087  0.9999956          
  4.592478e-02  0.004006065  0.9999956          
  6.470108e-02  0.004648135  0.9999956          
  1.003019e-01  0.005981890  0.9999956          
  1.272624e-01  0.007018067  0.9999956          
  2.264439e-01  0.010673793  0.9999956          
  2.366848e-01  0.011029095  0.9999956          
  2.561245e-01  0.011691023  0.9999956          
  6.491604e-01  0.022051665  0.9999954          
  2.783471e+00  0.041514490  0.9999950          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was lambda = 0.0001665083.
[1] "Sun Mar 11 15:42:33 2018"
Robust Linear Model 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  intercept  psi           RMSE         Rsquared   Selected
  FALSE      psi.huber     0.003092361  0.9999956          
  FALSE      psi.hampel    0.003091158  0.9999956  *       
  FALSE      psi.bisquare  0.003101158  0.9999956          
   TRUE      psi.huber     0.003106507  0.9999956          
   TRUE      psi.hampel    0.003109228  0.9999956          
   TRUE      psi.bisquare  0.003115431  0.9999956          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were intercept = FALSE and psi = psi.hampel.
[1] "Sun Mar 11 15:42:35 2018"
Loading required package: rpart
CART 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  cp            RMSE       Rsquared   Selected
  2.279532e-05  0.3713736  0.9362427  *       
  1.113629e-04  0.3714765  0.9362173          
  2.073448e-04  0.3715187  0.9362130          
  4.740632e-04  0.3769594  0.9344222          
  6.440388e-04  0.3833264  0.9323831          
  7.017135e-04  0.3852642  0.9317446          
  7.186826e-04  0.3852798  0.9316840          
  1.403935e-03  0.4015428  0.9257971          
  1.927278e-03  0.4237694  0.9169986          
  2.075088e-03  0.4237694  0.9169986          
  2.270385e-03  0.4264902  0.9159933          
  2.833081e-03  0.4470141  0.9073686          
  4.974673e-03  0.4982157  0.8849214          
  7.748815e-03  0.5698410  0.8489718          
  9.072251e-03  0.5872576  0.8399898          
  3.207127e-02  0.7778238  0.7191522          
  2.139145e-01  1.1920889  0.3406540          
  3.274758e-01  1.2580694  0.2688801          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was cp = 2.279532e-05.
[1] "Sun Mar 11 15:42:37 2018"
CART 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results:

  RMSE       Rsquared 
  0.5986962  0.8336042

[1] "Sun Mar 11 15:42:39 2018"
CART 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  maxdepth  RMSE       Rsquared   Selected
   1        1.2580694  0.2688801          
   3        0.9421286  0.5904251          
   6        0.7581024  0.7337401          
   7        0.7251234  0.7563854          
   9        0.6351551  0.8133060          
  11        0.5986962  0.8336042  *       
  13        0.5986962  0.8336042          
  14        0.5986962  0.8336042          
  15        0.5986962  0.8336042          
  17        0.5986962  0.8336042          
  20        0.5986962  0.8336042          
  21        0.5986962  0.8336042          
  22        0.5986962  0.8336042          
  23        0.5986962  0.8336042          
  27        0.5986962  0.8336042          
  29        0.5986962  0.8336042          
  30        0.5986962  0.8336042          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was maxdepth = 11.
[1] "Sun Mar 11 15:42:41 2018"
Loading required package: rqPen
Loading required package: quantreg
Loading required package: SparseM

Attaching package: 'SparseM'

The following object is masked from 'package:base':

    backsolve


Attaching package: 'quantreg'

The following object is masked from 'package:survival':

    untangle.specials

Loading required package: regpro
Loading required package: denpro
Quantile Regression with LASSO penalty 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  lambda        RMSE         Rsquared   Selected
  3.248846e-05  0.003102412  0.9999956  *       
  1.665083e-04  0.003102412  0.9999956          
  3.321585e-04  0.003102412  0.9999956          
  3.973393e-04  0.003102412  0.9999956          
  6.761155e-04  0.003102412  0.9999956          
  7.167755e-04  0.003102412  0.9999956          
  4.990330e-03  0.003102412  0.9999956          
  8.740596e-03  0.003102412  0.9999956          
  1.425807e-02  0.003102412  0.9999956          
  2.084964e-02  0.003102412  0.9999956          
  3.139992e-02  0.003102412  0.9999956          
  4.592478e-02  0.003102412  0.9999956          
  6.470108e-02  0.003102412  0.9999956          
  1.003019e-01  0.003102412  0.9999956          
  1.272624e-01  0.003375625  0.9999956          
  2.264439e-01  0.005106500  0.9999956          
  2.366848e-01  0.005472630  0.9999956          
  2.561245e-01  0.006208516  0.9999956          
  6.491604e-01  1.466215430        NaN          
  2.783471e+00  1.466215565        NaN          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was lambda = 3.248846e-05.
[1] "Sun Mar 11 15:42:43 2018"
Non-Convex Penalized Quantile Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  lambda        penalty  RMSE         Rsquared   Selected
  3.248846e-05  MCP      0.003102412  0.9999956          
  1.665083e-04  SCAD     0.003102412  0.9999956          
  3.321585e-04  MCP      0.003102412  0.9999956          
  3.973393e-04  SCAD     0.003102412  0.9999956          
  6.761155e-04  SCAD     0.003102412  0.9999956          
  7.167755e-04  MCP      0.003102412  0.9999956          
  4.990330e-03  SCAD     0.003102412  0.9999956          
  8.740596e-03  MCP      0.003102412  0.9999956          
  1.425807e-02  SCAD     0.003102412  0.9999956          
  2.084964e-02  SCAD     0.003102412  0.9999956          
  3.139992e-02  SCAD     0.003102412  0.9999956          
  4.592478e-02  SCAD     0.003102412  0.9999956          
  6.470108e-02  MCP      0.003102412  0.9999956          
  1.003019e-01  SCAD     0.003102412  0.9999956          
  1.272624e-01  SCAD     0.003102412  0.9999956          
  2.264439e-01  MCP      0.003102412  0.9999956          
  2.366848e-01  SCAD     0.003102412  0.9999956          
  2.561245e-01  SCAD     0.003102412  0.9999956  *       
  6.491604e-01  SCAD     1.466215430        NaN          
  2.783471e+00  MCP      1.466215565        NaN          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 0.2561245 and penalty = SCAD.
[1] "Sun Mar 11 15:42:46 2018"
Loading required package: RRF
RRF 1.7
Type rrfNews() to see new features/changes/bug fixes.

Attaching package: 'RRF'

The following object is masked from 'package:ranger':

    importance

The following objects are masked from 'package:randomForest':

    classCenter, combine, getTree, grow, importance, margin, MDSplot,
    na.roughfix, outlier, partialPlot, treesize, varImpPlot, varUsed

The following object is masked from 'package:ggplot2':

    margin

Regularized Random Forest 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  mtry  coefReg      coefImp     RMSE       Rsquared   Selected
  1     0.020825960  0.17228669  0.1958934  0.9863066          
  1     0.112289042  0.38019073  0.1944402  0.9865680          
  1     0.227549524  0.73129853  0.1976269  0.9858751          
  1     0.305570287  0.62730656  0.1963452  0.9862843          
  1     0.547816768  0.85197357  0.1962887  0.9861301          
  1     0.772217982  0.97567128  0.1985683  0.9858917          
  1     0.846319073  0.50253938  0.1947487  0.9865664          
  1     0.979235323  0.93747076  0.1971458  0.9862436          
  2     0.009669058  0.85859567  0.1614807  0.9896049          
  2     0.096990961  0.89448540  0.1636758  0.9893984          
  2     0.252353372  0.99877187  0.1640722  0.9893623          
  2     0.557795454  0.36089761  0.1621472  0.9895784          
  2     0.647789975  0.22207982  0.1642687  0.9893139          
  2     0.650717882  0.50437835  0.1641720  0.9892821          
  2     0.731459219  0.42543520  0.1647350  0.9892325          
  2     0.742341945  0.67519571  0.1624810  0.9895403          
  2     0.841210552  0.52111625  0.1652314  0.9891317          
  2     0.908625109  0.60755615  0.1631929  0.9894682          
  2     0.919229351  0.04772146  0.1613165  0.9896433  *       
  2     0.965862230  0.60920909  0.1629328  0.9894004          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mtry = 2, coefReg = 0.9192294
 and coefImp = 0.04772146.
[1] "Sun Mar 11 15:44:09 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: Warning messages:
1: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
2: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Regularized Random Forest 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  mtry  coefReg      RMSE       Rsquared   Selected
  1     0.020825960  0.1949318  0.9863321          
  1     0.112289042  0.1966564  0.9860810          
  1     0.227549524  0.1949834  0.9863376          
  1     0.305570287  0.1968778  0.9861429          
  1     0.547816768  0.2028890  0.9853170          
  1     0.772217982  0.1950538  0.9866282          
  1     0.846319073  0.1969567  0.9863636          
  1     0.979235323  0.1986719  0.9855489          
  2     0.009669058  0.1637481  0.9893381          
  2     0.096990961  0.1631192  0.9894638          
  2     0.252353372  0.1627366  0.9895094          
  2     0.557795454  0.1638848  0.9893913          
  2     0.647789975  0.1629805  0.9894826          
  2     0.650717882  0.1655219  0.9890795          
  2     0.731459219  0.1641512  0.9893109          
  2     0.742341945  0.1631534  0.9894091          
  2     0.841210552  0.1619773  0.9895576  *       
  2     0.908625109  0.1626106  0.9894498          
  2     0.919229351  0.1632008  0.9895250          
  2     0.965862230  0.1634807  0.9893488          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mtry = 2 and coefReg = 0.8412106.
[1] "Sun Mar 11 15:44:50 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  model fit failed for Resample09: parameter=none Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) : 
  the leading minor of order 21 is not positive definite

2: In eval(xpr, envir = envir) :
  model fit failed for Resample20: parameter=none Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) : 
  the leading minor of order 248 is not positive definite

3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:47:17 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "rvmLinear"               
Partial Least Squares 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results:

  RMSE       Rsquared 
  0.0519677  0.9977691

Tuning parameter 'ncomp' was held constant at a value of 1
[1] "Sun Mar 11 15:47:19 2018"
Loading required package: spikeslab

 spikeslab 1.1.5 
 
 Type spikeslab.news() to see new features, changes, and bug fixes. 
 

Spike and Slab Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  vars  RMSE         Rsquared   Selected
  1     1.438772153  0.5392154          
  2     0.003109509  0.9999956  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was vars = 2.
[1] "Sun Mar 11 15:47:23 2018"
Loading required package: spls
Sparse Partial Least Squares (SPLS) Regression and
Classification (version 2.2-1)


Attaching package: 'spls'

The following object is masked from 'package:caret':

    splsda

Sparse Partial Least Squares 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  kappa      eta          K  RMSE         Rsquared   Selected
  0.0426441  0.305570287  2  0.003109473  0.9999956          
  0.1017863  0.547816768  2  0.003109473  0.9999956          
  0.1267788  0.112289042  1  0.003496351  0.9999944          
  0.1332635  0.846319073  2  0.003109473  0.9999956          
  0.1525017  0.772217982  2  0.003109473  0.9999956          
  0.1546153  0.227549524  2  0.003109473  0.9999956          
  0.2248441  0.979235323  2  0.003109473  0.9999956  *       
  0.2451284  0.020825960  1  0.003496351  0.9999944          
  0.2628384  0.742341945  2  0.003109473  0.9999956          
  0.2765915  0.647789975  1  0.003496351  0.9999944          
  0.2914107  0.650717882  2  0.003109473  0.9999956          
  0.3051706  0.731459219  1  0.003496351  0.9999944          
  0.3175760  0.009669058  2  0.003109473  0.9999956          
  0.3334424  0.841210552  2  0.003109473  0.9999956          
  0.3420584  0.965862230  2  0.003109473  0.9999956          
  0.3629134  0.252353372  2  0.003109473  0.9999956          
  0.3645142  0.919229351  1  0.345963878  0.8369905          
  0.3673709  0.908625109  2  0.003109473  0.9999956          
  0.4010293  0.557795454  1  0.003496351  0.9999944          
  0.4537156  0.096990961  2  0.003109473  0.9999956          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were K = 2, eta = 0.9792353 and kappa
 = 0.2248441.
[1] "Sun Mar 11 15:47:26 2018"
Loading required package: superpc
Supervised Principal Component Analysis 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  threshold  n.components  RMSE        Rsquared   Selected
  0.0852882  1             0.41970935  0.8620890          
  0.2035726  2             0.00755875  0.9999956  *       
  0.2535576  1             0.41970935  0.8620890          
  0.2665269  3             0.00755875  0.9999956          
  0.3050035  3             0.00755875  0.9999956          
  0.3092305  1             0.41970935  0.8620890          
  0.4496882  3             0.00755875  0.9999956          
  0.4902568  1             0.41970935  0.8620890          
  0.5256768  3             0.00755875  0.9999956          
  0.5531831  2             0.00755875  0.9999956          
  0.5828214  2             0.00755875  0.9999956          
  0.6103412  3             0.00755875  0.9999956          
  0.6351519  1             0.41970935  0.8620890          
  0.6668849  3             0.00755875  0.9999956          
  0.6841167  3             0.00755875  0.9999956          
  0.7258268  1             0.41970935  0.8620890          
  0.7290284  3             0.00755875  0.9999956          
  0.7347418  3             0.00755875  0.9999956          
  0.8020587  2             0.00755875  0.9999956          
  0.9074311  1             0.41970935  0.8620890          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were threshold = 0.2035726 and
 n.components = 2.
[1] "Sun Mar 11 15:47:29 2018"
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:47:30 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "svmBoundrangeString"     
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:47:31 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "svmExpoString"           
Support Vector Machines with Linear Kernel 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  C             RMSE        Rsquared   Selected
    0.07585158  0.05774795  0.9999282          
    0.25946099  0.04895485  0.9999478  *       
    0.43629128  0.04895485  0.9999478          
    0.49927403  0.04895485  0.9999478          
    0.74486567  0.04895485  0.9999478          
    0.77833220  0.04895485  0.9999478          
    3.35269951  0.04895485  0.9999478          
    5.11187540  0.04895485  0.9999478          
    7.38783584  0.04895485  0.9999478          
    9.83378184  0.04895485  0.9999478          
   13.38292190  0.04895485  0.9999478          
   17.81619932  0.04895485  0.9999478          
   23.05933279  0.04895485  0.9999478          
   32.07268170  0.04895485  0.9999478          
   38.36584240  0.04895485  0.9999478          
   59.19491601  0.04895485  0.9999478          
   61.19856162  0.04895485  0.9999478          
   64.94414835  0.04895485  0.9999478          
  130.76929375  0.04895485  0.9999478          
  391.11982269  0.04895485  0.9999478          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was C = 0.259461.
[1] "Sun Mar 11 15:47:34 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Linear Kernel 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  cost          RMSE        Rsquared   Selected
    0.07585158  0.05774795  0.9999282          
    0.25946099  0.04895485  0.9999478  *       
    0.43629128  0.04895485  0.9999478          
    0.49927403  0.04895485  0.9999478          
    0.74486567  0.04895485  0.9999478          
    0.77833220  0.04895485  0.9999478          
    3.35269951  0.04895485  0.9999478          
    5.11187540  0.04895485  0.9999478          
    7.38783584  0.04895485  0.9999478          
    9.83378184  0.04895485  0.9999478          
   13.38292190  0.04895485  0.9999478          
   17.81619932  0.04895485  0.9999478          
   23.05933279  0.04895485  0.9999478          
   32.07268170  0.04895485  0.9999478          
   38.36584240  0.04895485  0.9999478          
   59.19491601  0.04895485  0.9999478          
   61.19856162  0.04895485  0.9999478          
   64.94414835  0.04895485  0.9999478          
  130.76929375  0.04895485  0.9999478          
  391.11982269  0.04895485  0.9999478          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was cost = 0.259461.
[1] "Sun Mar 11 15:47:37 2018"
Loading required package: LiblineaR
L2 Regularized Support Vector Machine (dual) with Linear Kernel 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  cost          Loss  RMSE        Rsquared   Selected
  3.185563e-03  L1    0.39355383  0.9997651          
  1.641835e-02  L2    0.05806980  0.9999863          
  3.282984e-02  L1    0.11403194  0.9997390          
  3.929633e-02  L2    0.04560589  0.9999572          
  6.698907e-02  L2    0.04411612  0.9999317          
  7.103189e-02  L1    0.09015231  0.9995524          
  4.978435e-01  L2    0.03700837  0.9999576          
  8.736558e-01  L1    0.06419414  0.9994439          
  1.427545e+00  L2    0.03622042  0.9999429          
  2.090230e+00  L2    0.03420740  0.9999420  *       
  3.152351e+00  L2    0.03567904  0.9999617          
  4.616577e+00  L2    0.03544592  0.9999484          
  6.511719e+00  L1    0.06257102  0.9994391          
  1.010990e+01  L2    0.03826977  0.9999330          
  1.283787e+01  L2    0.03577186  0.9999455          
  2.288825e+01  L1    0.06237911  0.9994396          
  2.392700e+01  L2    0.03842751  0.9999487          
  2.589922e+01  L2    0.03560349  0.9999620          
  6.585282e+01  L2    0.03509751  0.9999381          
  2.837787e+02  L1    0.06227192  0.9994394          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were cost = 2.09023 and Loss = L2.
[1] "Sun Mar 11 15:47:40 2018"
Support Vector Machines with Polynomial Kernel 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  degree  scale         C             RMSE        Rsquared   Selected
  1       3.937745e-05  1.627729e+00  1.42685572  0.9990041          
  1       1.607855e-04  6.266022e+01  0.08302806  0.9999740          
  1       4.167132e-04  2.125306e+01  0.08689696  0.9999688          
  1       8.016654e-03  2.197324e+02  0.04888236  0.9999513  *       
  1       1.240368e-01  7.951423e+02  0.04894986  0.9999486          
  1       3.064522e-01  5.808198e+00  0.04895483  0.9999478          
  2       1.125268e-05  2.353942e+02  0.11254136  0.9999771          
  2       1.289434e-05  1.874140e-01  1.46256765  0.9990041          
  2       2.716152e-02  3.145143e-01  0.07377124  0.9999635          
  2       2.814978e-02  5.920320e+00  0.06185812  0.9998667          
  2       7.542013e-02  2.605442e+00  0.07009914  0.9998324          
  2       8.613446e-02  3.496733e+01  0.06971916  0.9998260          
  2       1.552229e+00  5.345056e+02  0.07188936  0.9997424          
  3       3.267021e-05  3.418645e+02  0.06419636  0.9999305          
  3       2.176370e-04  1.011007e+03  0.04901298  0.9999447          
  3       9.055041e-03  1.331880e+00  0.06386776  0.9999650          
  3       2.879270e-01  7.045703e+00  0.09505183  0.9981212          
  3       6.556151e-01  1.730770e+01  0.09343346  0.9979368          
  3       7.462122e-01  5.132558e-02  0.09448483  0.9979286          
  3       1.318453e+00  1.760772e+01  0.09315391  0.9979104          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were degree = 1, scale = 0.008016654 and
 C = 219.7324.
[1] "Sun Mar 11 15:47:42 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Support Vector Machines with Radial Basis Function Kernel 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  sigma        C             RMSE        Rsquared   Selected
   0.01345330  1.812515e+02  0.07907596  0.9988128  *       
   0.01469894  8.266619e+00  0.08194258  0.9985483          
   0.02452343  4.436892e+02  0.08619129  0.9981120          
   0.04545823  2.554887e+00  0.10416556  0.9958367          
   0.05510499  1.663098e+02  0.08933352  0.9977020          
   0.09323462  5.376164e+01  0.08437667  0.9971173          
   0.15257379  4.360915e-01  0.15609922  0.9879033          
   0.27083288  4.551201e-01  0.17550841  0.9832510          
   0.36732780  1.010503e+03  0.12715611  0.9913054          
   0.44752812  6.066379e-02  0.45765117  0.9124665          
   0.63976617  2.798004e+00  0.16566294  0.9859408          
   1.47873695  1.051402e+02  0.23481924  0.9723507          
   1.51263443  3.033169e+01  0.23708834  0.9718770          
   1.53914874  6.701721e+01  0.23896885  0.9714925          
   1.73032774  7.596162e+00  0.25026425  0.9690147          
   2.19952356  4.506702e+01  0.27229048  0.9636629          
   3.35507848  5.820709e+01  0.31572214  0.9528147          
   9.21198053  1.382083e+00  0.50616830  0.8890677          
  13.17377644  4.255679e+00  0.55435916  0.8691492          
  17.43836887  2.044282e+01  0.62558950  0.8343703          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were sigma = 0.0134533 and C = 181.2515.
[1] "Sun Mar 11 15:47:45 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  C             RMSE       Rsquared   Selected
    0.07585158  0.5918892  0.8720995          
    0.25946099  0.3730528  0.9414395          
    0.43629128  0.2916300  0.9576238          
    0.49927403  0.3078516  0.9575540          
    0.74486567  0.2711898  0.9645119          
    0.77833220  0.2661183  0.9665309          
    3.35269951  0.2209108  0.9748163          
    5.11187540  0.2184260  0.9748521          
    7.38783584  0.2341374  0.9705031          
    9.83378184  0.2071650  0.9789508          
   13.38292190  0.2069550  0.9797954          
   17.81619932  0.2148187  0.9777140          
   23.05933279  0.1981748  0.9805442  *       
   32.07268170  0.2097631  0.9768968          
   38.36584240  0.2247154  0.9740629          
   59.19491601  0.2155518  0.9760361          
   61.19856162  0.2100142  0.9772625          
   64.94414835  0.2131141  0.9776899          
  130.76929375  0.2199808  0.9749232          
  391.11982269  0.2255335  0.9731648          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was C = 23.05933.
[1] "Sun Mar 11 15:47:48 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  sigma        C             RMSE        Rsquared   Selected
   0.01345330  1.812515e+02  0.07907596  0.9988128  *       
   0.01469894  8.266619e+00  0.08194258  0.9985483          
   0.02452343  4.436892e+02  0.08619129  0.9981120          
   0.04545823  2.554887e+00  0.10416556  0.9958367          
   0.05510499  1.663098e+02  0.08933352  0.9977020          
   0.09323462  5.376164e+01  0.08437667  0.9971173          
   0.15257379  4.360915e-01  0.15609922  0.9879033          
   0.27083288  4.551201e-01  0.17550841  0.9832510          
   0.36732780  1.010503e+03  0.12715611  0.9913054          
   0.44752812  6.066379e-02  0.45765117  0.9124665          
   0.63976617  2.798004e+00  0.16566294  0.9859408          
   1.47873695  1.051402e+02  0.23481924  0.9723507          
   1.51263443  3.033169e+01  0.23708834  0.9718770          
   1.53914874  6.701721e+01  0.23896885  0.9714925          
   1.73032774  7.596162e+00  0.25026425  0.9690147          
   2.19952356  4.506702e+01  0.27229048  0.9636629          
   3.35507848  5.820709e+01  0.31572214  0.9528147          
   9.21198053  1.382083e+00  0.50616830  0.8890677          
  13.17377644  4.255679e+00  0.55435916  0.8691492          
  17.43836887  2.044282e+01  0.62558950  0.8343703          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were sigma = 0.0134533 and C = 181.2515.
[1] "Sun Mar 11 15:47:51 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:47:53 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "svmSpectrumString"       
Loading required package: ipred

Attaching package: 'ipred'

The following object is masked from 'package:mboost':

    cv

Bagged CART 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results:

  RMSE       Rsquared 
  0.4087668  0.9334881

[1] "Sun Mar 11 15:47:55 2018"
Partial Least Squares 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results:

  RMSE       Rsquared 
  0.0519677  0.9977691

Tuning parameter 'ncomp' was held constant at a value of 1
[1] "Sun Mar 11 15:48:02 2018"
Loading required package: xgboost
eXtreme Gradient Boosting 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  lambda        alpha         nrounds  eta        RMSE       Rsquared 
  2.669568e-05  3.371719e-04   63      0.6300166  0.1544935  0.9893944
  1.041989e-04  5.483828e-03   86      1.1858117  0.1535729  0.9894148
  1.852626e-04  3.642883e-05   39      1.6989581  0.1538257  0.9894858
  2.150971e-04  1.704494e-01   51      1.9640616  0.1587201  0.9891761
  3.349789e-04  7.262563e-02   98      1.0875088  0.1550218  0.9893542
  3.516840e-04  1.373244e-04   74      1.5425973  0.1533787  0.9895879
  1.771908e-03  7.873659e-01   94      0.6571099  0.1802252  0.9853934
  2.826729e-03  1.270954e-05   18      0.3466052  0.1573100  0.9890961
  4.249951e-03  5.148844e-02   68      2.9259196  0.1603280  0.9888630
  5.833315e-03  1.733604e-02   23      2.3918571  0.1576488  0.9890716
  8.205538e-03  1.793038e-02   51      0.3541262  0.1577058  0.9890077
  1.126434e-02  4.542512e-02   43      1.2811671  0.1549744  0.9895667
  1.498855e-02  1.117752e-05   86      0.5303836  0.1564097  0.9892263
  2.159854e-02  1.607136e-01   53      2.3704035  0.1552181  0.9896389
  2.633804e-02  6.750115e-01   61      0.1093841  0.1761463  0.9860259
  4.257295e-02  1.827119e-04  100      2.1530037  0.1568315  0.9891475
  4.417148e-02  3.945906e-01    5      2.9062267  0.3833891  0.9764702
  4.717471e-02  3.492413e-01   61      1.6557358  0.1625525  0.9884465
  1.023984e-01  6.151447e-03   37      0.5235673  0.1519280  0.9897194
  3.444733e-01  3.054603e-05   90      2.9127949  0.1505217  0.9901374
  Selected
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 90, lambda =
 0.3444733, alpha = 3.054603e-05 and eta = 2.912795.
[1] "Sun Mar 11 15:48:17 2018"
eXtreme Gradient Boosting 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  eta         max_depth  gamma      colsample_bytree  min_child_weight
  0.02958515  10         9.6874223  0.4041860          4              
  0.10419973   1         1.1553506  0.4157241          7              
  0.13402581   7         7.9728571  0.4329648         16              
  0.21717767   6         1.7452242  0.5648307         12              
  0.22873424   2         5.6631937  0.5146850         14              
  0.25583569   8         4.2705570  0.6864826          3              
  0.30202109   9         6.5468721  0.4862384         10              
  0.30312263   7         1.1804206  0.3665378          9              
  0.31314863   9         7.9013449  0.4946658         15              
  0.36492613  10         5.5191194  0.6771944         15              
  0.36591625  10         0.3646137  0.5107000          5              
  0.37675663   4         2.1000555  0.5580639          8              
  0.40544223   8         9.7530654  0.6243242          5              
  0.43904782   3         5.1419911  0.6787978         15              
  0.51133217   6         3.9527057  0.3231082          6              
  0.51529881   1         1.7679452  0.3729363         14              
  0.53679675   1         9.7093164  0.6644930         16              
  0.56254498  10         2.1903665  0.5466018         13              
  0.58542710   8         3.6250293  0.3637066          7              
  0.59926435   3         7.1766791  0.4580078          5              
  subsample  nrounds  RMSE       Rsquared   Selected
  0.7437407  730            NaN        NaN          
  0.7422573  491            NaN        NaN          
  0.6914419  554            NaN        NaN          
  0.3145611  803      0.2410812  0.9747258          
  0.2638774  254      0.3564932  0.9554582          
  0.6671609  611      0.2711658  0.9789836          
  0.3483807  267            NaN        NaN          
  0.9744347  583            NaN        NaN          
  0.5097010  667            NaN        NaN          
  0.6437293  735      0.2885809  0.9719440          
  0.6372188  685      0.2350609  0.9758852  *       
  0.6192287   86      0.2542905  0.9740205          
  0.4724055  526      0.3627183  0.9525292          
  0.7895007  310      0.2919209  0.9685915          
  0.9168857  204            NaN        NaN          
  0.6004051  636            NaN        NaN          
  0.8637994  908      0.4149514  0.9222286          
  0.8663366  450      0.2416099  0.9755300          
  0.9302933  306            NaN        NaN          
  0.7944445  726            NaN        NaN          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 685, max_depth = 10, eta
 = 0.3659162, gamma = 0.3646137, colsample_bytree = 0.5107, min_child_weight
 = 5 and subsample = 0.6372188.
[1] "Sun Mar 11 15:49:03 2018"
Loading required package: kohonen
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :24    NA's   :24   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:49:16 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "xyf"                     
Fitting Repeat 1 

# weights:  45
initial  value 1248.073041 
iter  10 value 1050.201070
iter  20 value 1049.206238
iter  30 value 1049.159521
iter  40 value 704.290238
iter  50 value 698.203752
iter  60 value 698.135459
iter  70 value 698.114369
iter  80 value 698.057085
iter  90 value 698.034648
iter 100 value 698.028996
final  value 698.028996 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 1143.469595 
iter  10 value 1049.778154
iter  20 value 1049.151326
iter  30 value 706.979443
iter  40 value 704.171042
iter  50 value 699.358765
iter  60 value 698.422485
iter  70 value 698.329889
iter  80 value 698.259134
iter  90 value 698.132983
iter 100 value 698.070699
final  value 698.070699 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  45
initial  value 1299.546400 
iter  10 value 1050.270210
iter  20 value 1049.210970
iter  30 value 1049.195061
iter  40 value 1049.181217
iter  50 value 795.208060
iter  60 value 699.391425
iter  70 value 698.850286
iter  80 value 698.384228
iter  90 value 698.059291
iter 100 value 698.037698
final  value 698.037698 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 1174.066838 
iter  10 value 1049.888545
iter  20 value 1049.188367
iter  30 value 1029.785055
iter  40 value 706.940255
iter  50 value 700.138607
iter  60 value 698.576904
iter  70 value 698.403087
iter  80 value 698.312539
iter  90 value 698.202026
iter 100 value 698.138637
final  value 698.138637 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 1380.618946 
iter  10 value 1050.089432
iter  20 value 1049.211274
iter  30 value 1049.203501
iter  40 value 1049.203162
iter  50 value 1049.202735
iter  60 value 1049.202165
iter  70 value 1049.201334
iter  80 value 1049.199952
iter  90 value 1049.197050
iter 100 value 1049.186518
final  value 1049.186518 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  81
initial  value 1336.858236 
iter  10 value 719.682828
iter  20 value 702.868651
iter  30 value 702.201058
iter  40 value 702.155875
iter  50 value 702.141804
iter  60 value 702.128409
iter  70 value 702.125647
final  value 702.125596 
converged
Fitting Repeat 2 

# weights:  81
initial  value 1241.881236 
iter  10 value 1008.986679
iter  20 value 709.960384
iter  30 value 704.072243
iter  40 value 702.457218
iter  50 value 702.265067
iter  60 value 702.211747
iter  70 value 702.170037
iter  80 value 702.149912
iter  90 value 702.144605
iter 100 value 702.140910
final  value 702.140910 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  81
initial  value 1350.079229 
iter  10 value 1022.945458
iter  20 value 734.373415
iter  30 value 705.594189
iter  40 value 703.547728
iter  50 value 702.883151
iter  60 value 702.540803
iter  70 value 702.273763
iter  80 value 702.189959
iter  90 value 702.160156
iter 100 value 702.142866
final  value 702.142866 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  81
initial  value 1481.931837 
iter  10 value 779.060436
iter  20 value 710.403624
iter  30 value 703.161386
iter  40 value 702.423111
iter  50 value 702.214210
iter  60 value 702.159344
iter  70 value 702.132956
iter  80 value 702.128435
iter  90 value 702.126970
iter 100 value 702.125829
final  value 702.125829 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  81
initial  value 1149.463240 
iter  10 value 1049.266664
iter  20 value 725.515240
iter  30 value 707.044210
iter  40 value 703.010280
iter  50 value 702.631878
iter  60 value 702.362311
iter  70 value 702.236410
iter  80 value 702.200887
iter  90 value 702.192493
iter 100 value 702.189716
final  value 702.189716 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  73
initial  value 1168.209165 
iter  10 value 1049.346621
iter  20 value 1049.194121
iter  30 value 739.316131
iter  40 value 708.996441
iter  50 value 708.053351
iter  60 value 707.418416
iter  70 value 702.297062
iter  80 value 698.318266
iter  90 value 698.094036
iter 100 value 698.041421
final  value 698.041421 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  73
initial  value 1257.631747 
iter  10 value 1049.407365
iter  20 value 1049.194848
final  value 1049.194173 
converged
Fitting Repeat 3 

# weights:  73
initial  value 1169.141583 
iter  10 value 1049.369627
iter  20 value 1049.194413
final  value 1049.193856 
converged
Fitting Repeat 4 

# weights:  73
initial  value 1055.504502 
iter  10 value 1049.115276
iter  20 value 699.084890
iter  30 value 698.851115
iter  40 value 698.644419
iter  50 value 698.024593
iter  60 value 697.969406
iter  70 value 697.955467
iter  80 value 697.939857
iter  90 value 697.924155
iter 100 value 697.915941
final  value 697.915941 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  73
initial  value 1504.701958 
iter  10 value 1049.246211
iter  20 value 1049.194369
iter  20 value 1049.194367
iter  20 value 1049.194366
final  value 1049.194366 
converged
Fitting Repeat 1 

# weights:  25
initial  value 1088.191560 
iter  10 value 759.119016
iter  20 value 755.590296
final  value 755.580325 
converged
Fitting Repeat 2 

# weights:  25
initial  value 1131.327311 
iter  10 value 748.351117
iter  20 value 741.250738
iter  30 value 741.228965
final  value 741.228919 
converged
Fitting Repeat 3 

# weights:  25
initial  value 1128.662018 
iter  10 value 759.070541
iter  20 value 741.323206
iter  30 value 741.229165
final  value 741.228923 
converged
Fitting Repeat 4 

# weights:  25
initial  value 1139.830193 
iter  10 value 754.576197
iter  20 value 741.433969
iter  30 value 741.234889
iter  40 value 741.228982
final  value 741.228917 
converged
Fitting Repeat 5 

# weights:  25
initial  value 1139.940702 
iter  10 value 750.501221
iter  20 value 741.489763
iter  30 value 741.230103
iter  40 value 741.228968
final  value 741.228932 
converged
Fitting Repeat 1 

# weights:  69
initial  value 1335.723304 
iter  10 value 827.201690
iter  20 value 680.316828
iter  30 value 677.080931
iter  40 value 675.867898
iter  50 value 675.674859
iter  60 value 675.591507
iter  70 value 675.539746
iter  80 value 675.509134
iter  90 value 675.493036
iter 100 value 675.481780
final  value 675.481780 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  69
initial  value 1214.339856 
iter  10 value 1152.753117
iter  20 value 793.255503
iter  30 value 785.704180
iter  40 value 785.135262
iter  50 value 784.935975
iter  60 value 784.878577
iter  70 value 784.863489
iter  80 value 784.857863
iter  90 value 784.853038
iter 100 value 784.846385
final  value 784.846385 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  69
initial  value 1172.857335 
iter  10 value 1032.008753
iter  20 value 670.340821
iter  30 value 669.757716
iter  40 value 669.613541
iter  50 value 669.577573
iter  60 value 669.558155
iter  70 value 669.553393
iter  80 value 669.552494
iter  90 value 669.551773
iter 100 value 669.550865
final  value 669.550865 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  69
initial  value 1533.874489 
iter  10 value 1097.504473
iter  20 value 801.679173
iter  30 value 754.050582
iter  40 value 744.467645
iter  50 value 742.373045
iter  60 value 741.741287
iter  70 value 741.597690
iter  80 value 741.547595
iter  90 value 741.523058
iter 100 value 741.516556
final  value 741.516556 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  69
initial  value 1233.137465 
iter  10 value 1084.068024
iter  20 value 741.303970
iter  30 value 735.289633
iter  40 value 734.060648
iter  50 value 733.277828
iter  60 value 733.041201
iter  70 value 732.955225
iter  80 value 732.924844
iter  90 value 732.916159
iter 100 value 732.911865
final  value 732.911865 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  21
initial  value 1166.693347 
iter  10 value 1049.240702
final  value 1049.200429 
converged
Fitting Repeat 2 

# weights:  21
initial  value 1328.022188 
iter  10 value 1049.261297
final  value 1049.196753 
converged
Fitting Repeat 3 

# weights:  21
initial  value 1338.577964 
iter  10 value 1049.281752
final  value 1049.198277 
converged
Fitting Repeat 4 

# weights:  21
initial  value 1109.252247 
iter  10 value 1049.215854
final  value 1049.195634 
converged
Fitting Repeat 5 

# weights:  21
initial  value 1123.067748 
iter  10 value 1049.216475
final  value 1049.195814 
converged
Fitting Repeat 1 

# weights:  41
initial  value 1312.269281 
iter  10 value 1050.923743
iter  20 value 1049.214250
iter  30 value 1049.206999
iter  40 value 1049.204551
iter  50 value 1049.198440
iter  60 value 1049.146486
iter  70 value 700.336118
iter  80 value 698.332003
iter  90 value 698.184516
iter 100 value 698.140921
final  value 698.140921 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  41
initial  value 1087.858827 
iter  10 value 1049.720863
iter  20 value 1049.221878
iter  30 value 1049.218641
iter  40 value 1049.211467
iter  50 value 1049.175688
iter  60 value 708.778012
iter  70 value 699.264150
iter  80 value 698.275730
iter  90 value 698.135482
iter 100 value 698.103958
final  value 698.103958 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  41
initial  value 1373.395422 
iter  10 value 1050.629576
iter  20 value 1049.213395
iter  30 value 851.739099
iter  40 value 708.198537
iter  50 value 705.286857
iter  60 value 701.122417
iter  70 value 699.208467
iter  80 value 698.757442
iter  90 value 698.600501
iter 100 value 698.259678
final  value 698.259678 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  41
initial  value 1311.282079 
iter  10 value 1050.813240
iter  20 value 1049.217259
iter  30 value 1047.751768
iter  40 value 706.962274
iter  50 value 701.580470
iter  60 value 698.411957
iter  70 value 698.297223
iter  80 value 698.180556
iter  90 value 698.116666
iter 100 value 698.099874
final  value 698.099874 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  41
initial  value 1225.470394 
iter  10 value 1050.829348
iter  20 value 1049.204357
iter  30 value 1049.194498
iter  40 value 1047.685089
iter  50 value 709.828706
iter  60 value 700.687969
iter  70 value 699.795291
iter  80 value 699.416212
iter  90 value 698.803111
iter 100 value 698.484225
final  value 698.484225 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  81
initial  value 1042.235912 
iter  10 value 707.292174
iter  20 value 705.819910
iter  30 value 705.759555
iter  40 value 705.746465
iter  50 value 705.737345
iter  60 value 705.733470
final  value 705.733241 
converged
Fitting Repeat 2 

# weights:  81
initial  value 1059.564380 
iter  10 value 719.718960
iter  20 value 706.851444
iter  30 value 705.835457
iter  40 value 705.781097
iter  50 value 705.766871
iter  60 value 705.764381
final  value 705.764354 
converged
Fitting Repeat 3 

# weights:  81
initial  value 1122.120425 
iter  10 value 943.306273
iter  20 value 722.783842
iter  30 value 708.337412
iter  40 value 706.420537
iter  50 value 705.961276
iter  60 value 705.838850
iter  70 value 705.791097
iter  80 value 705.769711
iter  90 value 705.741299
iter 100 value 705.734499
final  value 705.734499 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  81
initial  value 1131.053906 
iter  10 value 1014.730022
iter  20 value 711.282730
iter  30 value 706.088426
iter  40 value 705.783535
iter  50 value 705.751746
iter  60 value 705.743028
iter  70 value 705.741933
iter  80 value 705.741637
final  value 705.741624 
converged
Fitting Repeat 5 

# weights:  81
initial  value 1152.275232 
iter  10 value 860.879037
iter  20 value 714.211971
iter  30 value 707.260591
iter  40 value 706.141242
iter  50 value 705.939781
iter  60 value 705.860005
iter  70 value 705.813276
iter  80 value 705.797101
iter  90 value 705.772653
iter 100 value 705.751621
final  value 705.751621 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  37
initial  value 1299.822332 
iter  10 value 1161.735412
iter  20 value 1158.984105
iter  30 value 765.865158
iter  40 value 751.886314
iter  50 value 751.572914
iter  60 value 751.519150
iter  70 value 751.504604
iter  80 value 751.493070
iter  90 value 751.487275
iter 100 value 751.479843
final  value 751.479843 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  37
initial  value 964.082851 
iter  10 value 910.591104
iter  20 value 910.092965
iter  30 value 621.323605
iter  40 value 618.669681
iter  50 value 618.525555
iter  60 value 618.498937
iter  70 value 618.464635
iter  80 value 618.443709
iter  90 value 618.413762
iter 100 value 618.404120
final  value 618.404120 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  37
initial  value 1030.876198 
iter  10 value 936.827031
iter  20 value 695.712944
iter  30 value 689.981364
iter  40 value 688.799504
iter  50 value 687.031629
iter  60 value 686.042108
iter  70 value 683.397508
iter  80 value 679.615353
iter  90 value 678.344597
iter 100 value 675.426480
final  value 675.426480 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  37
initial  value 1212.678028 
iter  10 value 1033.545716
iter  20 value 1031.785587
iter  30 value 1031.536962
iter  40 value 663.270535
iter  50 value 661.367621
iter  60 value 658.282434
iter  70 value 654.881331
iter  80 value 654.052967
iter  90 value 652.828838
iter 100 value 652.686524
final  value 652.686524 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  37
initial  value 1311.848665 
iter  10 value 1029.072288
iter  20 value 1026.614149
iter  30 value 1026.593826
iter  40 value 1026.530484
iter  50 value 691.778287
iter  60 value 689.175071
iter  70 value 689.003973
iter  80 value 688.804276
iter  90 value 688.457958
iter 100 value 688.391968
final  value 688.391968 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  9
initial  value 1102.046504 
iter  10 value 1017.969126
final  value 1017.959760 
converged
Fitting Repeat 2 

# weights:  9
initial  value 1232.148460 
iter  10 value 1050.706254
final  value 1050.682285 
converged
Fitting Repeat 3 

# weights:  9
initial  value 1280.028863 
iter  10 value 1157.948698
iter  20 value 1157.930131
iter  30 value 1157.929195
iter  40 value 1157.928104
iter  50 value 1157.926747
iter  60 value 1157.924925
iter  70 value 1157.922246
iter  80 value 1157.917791
iter  90 value 1157.908763
iter 100 value 1157.881404
final  value 1157.881404 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  9
initial  value 1159.313452 
iter  10 value 1061.483247
iter  20 value 694.291135
iter  30 value 693.837250
iter  40 value 692.086170
iter  50 value 689.554916
iter  60 value 685.341613
iter  70 value 684.322432
iter  80 value 684.319874
iter  90 value 684.273462
iter 100 value 684.257393
final  value 684.257393 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  9
initial  value 1108.435331 
iter  10 value 1030.077762
final  value 1030.063292 
converged
Fitting Repeat 1 

# weights:  45
initial  value 1210.499479 
iter  10 value 708.343227
iter  20 value 702.243784
iter  30 value 700.709001
iter  40 value 700.040744
iter  50 value 699.864234
iter  60 value 699.797334
iter  70 value 699.780094
iter  80 value 699.774958
iter  90 value 699.772274
iter 100 value 699.766629
final  value 699.766629 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 1369.862320 
iter  10 value 706.563010
iter  20 value 702.021518
iter  30 value 700.867167
iter  40 value 700.156934
iter  50 value 699.875451
iter  60 value 699.814883
iter  70 value 699.782162
iter  80 value 699.769662
iter  90 value 699.766121
final  value 699.765647 
converged
Fitting Repeat 3 

# weights:  45
initial  value 1099.944140 
iter  10 value 1051.130690
iter  20 value 754.217311
iter  30 value 714.440364
iter  40 value 702.384718
iter  50 value 700.235969
iter  60 value 699.986311
iter  70 value 699.912166
iter  80 value 699.847592
iter  90 value 699.802993
iter 100 value 699.786605
final  value 699.786605 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 1193.608799 
iter  10 value 1051.164991
iter  20 value 800.750979
iter  30 value 718.700976
iter  40 value 702.059578
iter  50 value 700.342324
iter  60 value 699.995158
iter  70 value 699.937092
iter  80 value 699.903756
iter  90 value 699.851111
iter 100 value 699.827835
final  value 699.827835 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 1052.559450 
iter  10 value 700.165175
iter  20 value 699.823001
iter  30 value 699.803698
iter  40 value 699.800447
iter  50 value 699.799951
final  value 699.799829 
converged
Fitting Repeat 1 

# weights:  25
initial  value 1270.519130 
iter  10 value 1103.456569
iter  20 value 1101.288919
iter  30 value 703.729752
iter  40 value 699.724033
iter  50 value 699.526389
iter  60 value 699.251223
iter  70 value 699.163368
iter  80 value 699.127870
iter  90 value 699.111890
iter 100 value 699.100966
final  value 699.100966 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  25
initial  value 1087.458570 
iter  10 value 1044.940887
iter  20 value 726.791427
iter  30 value 681.669974
iter  40 value 678.284922
iter  50 value 677.666578
iter  60 value 677.584754
iter  70 value 677.556030
iter  80 value 677.539018
iter  90 value 677.534645
iter 100 value 677.523783
final  value 677.523783 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  25
initial  value 1138.782482 
iter  10 value 1075.121911
iter  20 value 1072.311542
iter  30 value 688.700262
iter  40 value 686.285688
iter  50 value 685.831517
iter  60 value 685.614571
iter  70 value 685.535016
iter  80 value 685.510480
iter  90 value 685.494717
iter 100 value 685.490825
final  value 685.490825 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  25
initial  value 1159.746537 
iter  10 value 975.595630
iter  20 value 973.461646
iter  30 value 973.392533
iter  40 value 663.422039
iter  50 value 658.953388
iter  60 value 658.831451
iter  70 value 658.805017
iter  80 value 658.764356
iter  90 value 658.735178
iter 100 value 658.707495
final  value 658.707495 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  25
initial  value 1112.274660 
iter  10 value 1009.857499
iter  20 value 1007.855833
iter  30 value 684.381655
iter  40 value 676.300175
iter  50 value 674.989812
iter  60 value 673.794970
iter  70 value 673.439681
iter  80 value 673.234188
iter  90 value 673.201071
iter 100 value 673.157027
final  value 673.157027 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  53
initial  value 1123.266273 
iter  10 value 788.871700
iter  20 value 775.926374
iter  30 value 775.470649
iter  40 value 775.443345
final  value 775.431908 
converged
Fitting Repeat 2 

# weights:  53
initial  value 1263.515547 
iter  10 value 978.933604
iter  20 value 794.870484
iter  30 value 777.243236
iter  40 value 775.769028
iter  50 value 775.514634
iter  60 value 775.480510
iter  70 value 775.473403
iter  80 value 775.465195
iter  90 value 775.431916
final  value 775.431903 
converged
Fitting Repeat 3 

# weights:  53
initial  value 1347.805754 
iter  10 value 966.923726
iter  20 value 814.003905
iter  30 value 778.022032
iter  40 value 776.006618
iter  50 value 775.526232
iter  60 value 775.457158
iter  70 value 775.437561
iter  80 value 775.435482
iter  90 value 775.432235
final  value 775.431902 
converged
Fitting Repeat 4 

# weights:  53
initial  value 1340.415195 
iter  10 value 777.285489
iter  20 value 775.484094
iter  30 value 775.480850
iter  30 value 775.480846
iter  30 value 775.480846
final  value 775.480846 
converged
Fitting Repeat 5 

# weights:  53
initial  value 1254.422985 
iter  10 value 796.290216
iter  20 value 777.219895
iter  30 value 776.107708
iter  40 value 776.104622
final  value 776.104580 
converged
Fitting Repeat 1 

# weights:  53
initial  value 1193.305088 
iter  10 value 959.842280
iter  20 value 959.331311
iter  30 value 959.325646
iter  40 value 959.323792
iter  50 value 959.319308
iter  60 value 959.296111
iter  70 value 738.924217
iter  80 value 685.027186
iter  90 value 679.260268
iter 100 value 679.120232
final  value 679.120232 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  53
initial  value 1050.515317 
iter  10 value 971.570971
iter  20 value 971.398126
iter  30 value 971.392125
iter  40 value 971.344603
iter  50 value 623.881578
iter  60 value 622.471294
iter  70 value 622.364774
iter  80 value 622.146692
iter  90 value 621.600710
iter 100 value 621.530131
final  value 621.530131 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  53
initial  value 1222.048416 
iter  10 value 1152.618190
iter  20 value 1152.311580
iter  30 value 1152.310171
iter  40 value 1152.306890
iter  50 value 1152.290704
iter  60 value 775.623143
iter  70 value 768.833671
iter  80 value 768.767004
iter  90 value 768.704745
iter 100 value 768.372978
final  value 768.372978 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  53
initial  value 1294.852350 
iter  10 value 1171.154530
final  value 1170.730715 
converged
Fitting Repeat 5 

# weights:  53
initial  value 1176.395022 
iter  10 value 984.770082
iter  20 value 984.258231
iter  30 value 983.363589
iter  40 value 654.001535
iter  50 value 653.514744
iter  60 value 648.719748
iter  70 value 642.125190
iter  80 value 640.323705
iter  90 value 638.479361
iter 100 value 637.913636
final  value 637.913636 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  41
initial  value 1397.042980 
iter  10 value 907.832462
iter  20 value 740.777931
iter  30 value 735.890470
iter  40 value 735.490054
iter  50 value 735.378236
iter  60 value 735.360523
final  value 735.360453 
converged
Fitting Repeat 2 

# weights:  41
initial  value 1137.283318 
iter  10 value 868.723817
iter  20 value 749.008456
iter  30 value 740.472999
iter  40 value 736.872293
iter  50 value 735.823208
iter  60 value 735.765520
final  value 735.748046 
converged
Fitting Repeat 3 

# weights:  41
initial  value 1434.173358 
iter  10 value 816.040273
iter  20 value 742.473091
iter  30 value 736.111844
iter  40 value 735.353033
iter  50 value 735.322740
iter  60 value 735.311866
iter  70 value 735.289897
final  value 735.284254 
converged
Fitting Repeat 4 

# weights:  41
initial  value 1260.941510 
iter  10 value 758.681556
iter  20 value 735.669557
iter  30 value 735.361007
final  value 735.360459 
converged
Fitting Repeat 5 

# weights:  41
initial  value 1202.052357 
iter  10 value 841.254783
iter  20 value 744.177559
iter  30 value 736.266916
iter  40 value 735.845511
iter  50 value 735.776488
iter  60 value 735.750804
iter  70 value 735.748283
final  value 735.748042 
converged
Fitting Repeat 1 

# weights:  57
initial  value 1360.781376 
iter  10 value 1003.694295
iter  20 value 995.461426
iter  30 value 994.352421
iter  40 value 994.341557
final  value 994.341421 
converged
Fitting Repeat 2 

# weights:  57
initial  value 1344.167406 
iter  10 value 1075.062399
iter  20 value 950.525949
iter  30 value 929.857650
iter  40 value 926.233371
iter  50 value 924.544534
iter  60 value 924.007627
iter  70 value 923.789326
iter  80 value 923.684371
final  value 923.505789 
converged
Fitting Repeat 3 

# weights:  57
initial  value 1350.976296 
iter  10 value 1004.509441
iter  20 value 963.088376
iter  30 value 958.465865
iter  40 value 958.218150
iter  50 value 957.985470
iter  60 value 957.642562
iter  70 value 957.625566
final  value 957.625471 
converged
Fitting Repeat 4 

# weights:  57
initial  value 1198.179451 
iter  10 value 913.136023
iter  20 value 902.537969
iter  30 value 900.833866
iter  40 value 900.465741
iter  50 value 900.408539
final  value 900.407962 
converged
Fitting Repeat 5 

# weights:  57
initial  value 1165.298941 
iter  10 value 913.668097
iter  20 value 885.732286
iter  30 value 882.725626
iter  40 value 880.664219
iter  50 value 880.219752
iter  60 value 880.133355
final  value 880.131348 
converged
Fitting Repeat 1 

# weights:  57
initial  value 1387.183967 
iter  10 value 1055.384190
iter  20 value 726.007911
iter  30 value 706.539298
iter  40 value 701.092237
iter  50 value 700.869607
iter  60 value 700.839393
iter  70 value 700.827654
iter  80 value 700.798234
iter  90 value 700.794506
iter 100 value 700.794097
final  value 700.794097 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  57
initial  value 1221.155971 
iter  10 value 726.902160
iter  20 value 706.679845
iter  30 value 702.264972
iter  40 value 701.085244
iter  50 value 700.878943
iter  60 value 700.829169
iter  70 value 700.809952
iter  80 value 700.800952
iter  90 value 700.798404
iter 100 value 700.797012
final  value 700.797012 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  57
initial  value 1402.240920 
iter  10 value 734.962439
iter  20 value 702.981311
iter  30 value 701.360065
iter  40 value 700.902999
iter  50 value 700.816289
iter  60 value 700.805164
iter  70 value 700.796578
iter  80 value 700.792088
iter  90 value 700.790139
iter 100 value 700.789013
final  value 700.789013 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  57
initial  value 1126.865045 
iter  10 value 1054.140353
iter  20 value 709.134018
iter  30 value 701.621815
iter  40 value 700.941483
iter  50 value 700.843340
iter  60 value 700.806905
iter  70 value 700.800356
iter  80 value 700.797850
iter  90 value 700.795547
iter 100 value 700.793306
final  value 700.793306 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  57
initial  value 1310.435378 
iter  10 value 1054.258136
iter  20 value 701.412048
iter  30 value 700.899465
iter  40 value 700.828415
iter  50 value 700.815374
iter  60 value 700.804043
iter  70 value 700.798360
iter  80 value 700.794562
iter  90 value 700.793953
final  value 700.793892 
converged
Fitting Repeat 1 

# weights:  81
initial  value 1250.652162 
iter  10 value 744.190256
iter  20 value 742.079986
iter  30 value 742.004005
final  value 742.000638 
converged
Fitting Repeat 2 

# weights:  81
initial  value 1093.219482 
iter  10 value 749.333204
iter  20 value 743.243189
iter  30 value 742.295973
iter  40 value 742.174773
iter  50 value 742.170190
iter  60 value 742.165857
final  value 742.165212 
converged
Fitting Repeat 3 

# weights:  81
initial  value 1239.468292 
iter  10 value 771.803855
iter  20 value 742.569502
iter  30 value 742.154331
iter  40 value 742.019654
final  value 742.001232 
converged
Fitting Repeat 4 

# weights:  81
initial  value 1380.344483 
iter  10 value 1079.192576
iter  20 value 804.872619
iter  30 value 749.718073
iter  40 value 744.239177
iter  50 value 742.608858
iter  60 value 742.251143
iter  70 value 742.223325
iter  80 value 742.194781
iter  90 value 742.180789
iter 100 value 742.169949
final  value 742.169949 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  81
initial  value 1371.850265 
iter  10 value 905.368027
iter  20 value 758.035522
iter  30 value 746.034049
iter  40 value 743.335123
iter  50 value 742.979261
iter  60 value 742.942812
final  value 742.942130 
converged
Fitting Repeat 1 

# weights:  5
initial  value 1320.532998 
iter  10 value 982.549987
iter  20 value 981.590297
iter  30 value 981.542187
iter  40 value 981.531161
iter  50 value 981.513515
iter  60 value 981.471105
iter  70 value 981.101891
iter  80 value 748.648979
iter  90 value 717.051402
iter 100 value 714.612098
final  value 714.612098 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  5
initial  value 1277.292938 
iter  10 value 1146.044631
iter  20 value 929.213603
iter  30 value 714.053326
iter  40 value 708.146683
iter  50 value 707.822672
final  value 707.822663 
converged
Fitting Repeat 3 

# weights:  5
initial  value 1089.292060 
iter  10 value 1000.668827
iter  20 value 1000.264188
iter  30 value 661.280855
iter  40 value 647.006381
iter  50 value 646.713923
final  value 646.698386 
converged
Fitting Repeat 4 

# weights:  5
initial  value 1070.392489 
iter  10 value 1028.275512
iter  20 value 1028.212073
iter  30 value 1028.206666
iter  40 value 1028.199990
iter  50 value 1028.191127
iter  60 value 1028.177953
iter  70 value 1028.154116
iter  80 value 1028.087180
iter  90 value 1024.066842
iter 100 value 649.514057
final  value 649.514057 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  5
initial  value 1116.261070 
iter  10 value 977.869299
iter  20 value 977.473216
iter  30 value 977.445462
iter  40 value 977.418698
iter  50 value 977.289934
iter  60 value 649.246879
iter  70 value 640.232211
iter  80 value 636.948169
iter  90 value 633.284709
final  value 633.159306 
converged
Fitting Repeat 1 

# weights:  65
initial  value 1196.266124 
iter  10 value 1050.524327
iter  20 value 1049.207725
iter  30 value 815.853086
iter  40 value 704.316804
iter  50 value 698.622665
iter  60 value 698.446426
iter  70 value 698.313959
iter  80 value 698.215715
iter  90 value 698.125946
iter 100 value 698.056121
final  value 698.056121 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  65
initial  value 1313.933305 
iter  10 value 1050.506776
iter  20 value 1049.207523
iter  30 value 937.858157
iter  40 value 709.346283
iter  50 value 703.887565
iter  60 value 701.231755
iter  70 value 700.011527
iter  80 value 699.532222
iter  90 value 699.341569
iter 100 value 698.846970
final  value 698.846970 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  65
initial  value 1050.364151 
iter  10 value 699.553423
iter  20 value 698.666256
iter  30 value 698.207521
iter  40 value 698.029169
iter  50 value 698.011592
iter  60 value 698.005992
iter  70 value 697.997062
iter  80 value 697.990896
iter  90 value 697.988026
iter 100 value 697.985684
final  value 697.985684 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  65
initial  value 1170.518696 
iter  10 value 1049.657921
iter  20 value 1049.197702
iter  30 value 709.551878
iter  40 value 709.209169
iter  50 value 705.971563
iter  60 value 699.310928
iter  70 value 699.152611
iter  80 value 698.810350
iter  90 value 698.358914
iter 100 value 698.280897
final  value 698.280897 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  65
initial  value 1046.970304 
iter  10 value 699.867786
iter  20 value 699.202276
iter  30 value 699.070514
iter  40 value 698.848909
iter  50 value 698.285246
iter  60 value 698.129186
iter  70 value 698.092774
iter  80 value 698.058893
iter  90 value 698.031757
iter 100 value 698.016588
final  value 698.016588 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  45
initial  value 1107.014968 
iter  10 value 1041.991448
iter  20 value 1041.690457
iter  30 value 1040.410204
iter  40 value 666.769506
iter  50 value 665.723895
iter  60 value 659.780580
iter  70 value 658.948864
iter  80 value 658.798273
iter  90 value 658.465230
iter 100 value 658.215922
final  value 658.215922 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 1329.535727 
iter  10 value 1042.454601
iter  20 value 1041.709047
iter  30 value 1041.705298
iter  40 value 1041.704908
iter  50 value 1041.704388
iter  60 value 1041.703635
iter  70 value 1041.702399
iter  80 value 1041.699878
iter  90 value 1041.691516
iter 100 value 1040.886856
final  value 1040.886856 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  45
initial  value 1098.854649 
iter  10 value 1041.992376
iter  20 value 703.369920
iter  30 value 677.410816
iter  40 value 671.665973
iter  50 value 658.835987
iter  60 value 658.066368
iter  70 value 657.985622
iter  80 value 657.737275
iter  90 value 657.657372
iter 100 value 657.608466
final  value 657.608466 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 1119.025646 
iter  10 value 1042.147018
iter  20 value 1041.651900
iter  30 value 661.576611
iter  40 value 657.991236
iter  50 value 657.853091
iter  60 value 657.715075
iter  70 value 657.627178
iter  80 value 657.570123
iter  90 value 657.547687
iter 100 value 657.534728
final  value 657.534728 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 1048.667801 
iter  10 value 1041.457642
iter  20 value 662.133362
iter  30 value 657.738404
iter  40 value 657.680206
iter  50 value 657.625507
iter  60 value 657.584056
iter  70 value 657.560188
iter  80 value 657.538379
iter  90 value 657.518602
iter 100 value 657.512120
final  value 657.512120 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  81
initial  value 1098.656904 
iter  10 value 1021.381636
iter  20 value 695.469482
iter  30 value 664.733587
iter  40 value 662.360632
iter  50 value 661.753416
iter  60 value 661.645280
iter  70 value 661.618295
iter  80 value 661.592893
iter  90 value 661.569598
iter 100 value 661.563467
final  value 661.563467 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  81
initial  value 1192.148109 
iter  10 value 1027.851923
iter  20 value 702.685617
iter  30 value 665.141785
iter  40 value 662.057522
iter  50 value 661.653472
iter  60 value 661.580122
iter  70 value 661.572482
iter  80 value 661.570182
iter  90 value 661.569262
iter 100 value 661.569019
final  value 661.569019 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  81
initial  value 1192.197644 
iter  10 value 689.112303
iter  20 value 668.849679
iter  30 value 662.368152
iter  40 value 661.727990
iter  50 value 661.603160
iter  60 value 661.581733
iter  70 value 661.576511
iter  80 value 661.568671
iter  90 value 661.561989
iter 100 value 661.560863
final  value 661.560863 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  81
initial  value 1275.682695 
iter  10 value 987.724264
iter  20 value 675.424257
iter  30 value 664.003038
iter  40 value 662.021499
iter  50 value 661.778687
iter  60 value 661.681584
iter  70 value 661.637731
iter  80 value 661.604786
iter  90 value 661.590226
iter 100 value 661.586818
final  value 661.586818 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  81
initial  value 1248.662833 
iter  10 value 1033.511544
iter  20 value 670.954293
iter  30 value 663.250009
iter  40 value 662.196200
iter  50 value 661.890857
iter  60 value 661.684835
iter  70 value 661.630123
iter  80 value 661.601541
iter  90 value 661.586605
iter 100 value 661.583710
final  value 661.583710 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  73
initial  value 1122.423948 
iter  10 value 1041.767762
iter  20 value 1041.695309
iter  30 value 1041.695025
iter  40 value 1041.694622
iter  50 value 1041.694013
iter  60 value 1041.692983
iter  70 value 1041.690859
iter  80 value 1041.684244
iter  90 value 1041.512125
iter 100 value 659.999192
final  value 659.999192 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  73
initial  value 1051.646644 
iter  10 value 1041.730376
iter  20 value 659.559421
iter  30 value 658.333559
iter  40 value 658.257475
iter  50 value 658.150090
iter  60 value 657.979637
iter  70 value 657.536348
iter  80 value 657.442762
iter  90 value 657.429389
iter 100 value 657.417428
final  value 657.417428 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  73
initial  value 1348.186821 
iter  10 value 1041.846205
iter  20 value 1041.696510
final  value 1041.696475 
converged
Fitting Repeat 4 

# weights:  73
initial  value 1111.328003 
iter  10 value 1041.747194
final  value 1041.696783 
converged
Fitting Repeat 5 

# weights:  73
initial  value 1049.558594 
iter  10 value 1041.695816
iter  20 value 1041.695664
iter  30 value 1041.695460
iter  40 value 1041.695166
iter  50 value 1041.694714
iter  60 value 1041.693906
iter  70 value 1041.692049
iter  80 value 1041.683652
iter  90 value 668.855981
iter 100 value 658.952566
final  value 658.952566 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  25
initial  value 1067.157769 
iter  10 value 702.162684
iter  20 value 700.815040
iter  30 value 700.755560
iter  30 value 700.755558
iter  30 value 700.755558
final  value 700.755558 
converged
Fitting Repeat 2 

# weights:  25
initial  value 1223.308528 
iter  10 value 713.682034
iter  20 value 700.650977
iter  30 value 700.535631
final  value 700.531943 
converged
Fitting Repeat 3 

# weights:  25
initial  value 1208.069963 
iter  10 value 739.710003
iter  20 value 715.790268
iter  30 value 715.598114
final  value 715.597866 
converged
Fitting Repeat 4 

# weights:  25
initial  value 1248.639310 
iter  10 value 872.470790
iter  20 value 719.060121
iter  30 value 702.163442
iter  40 value 701.189301
iter  50 value 701.097540
iter  60 value 700.758538
final  value 700.755594 
converged
Fitting Repeat 5 

# weights:  25
initial  value 1138.913056 
iter  10 value 855.717801
iter  20 value 709.976326
iter  30 value 701.957193
iter  40 value 700.674430
iter  50 value 700.560563
iter  60 value 700.532016
final  value 700.531961 
converged
Fitting Repeat 1 

# weights:  69
initial  value 1406.945816 
iter  10 value 722.679386
iter  20 value 703.431198
iter  30 value 701.735667
iter  40 value 701.521032
iter  50 value 701.449307
iter  60 value 701.396569
iter  70 value 701.366503
iter  80 value 701.350155
iter  90 value 701.346623
iter 100 value 701.345847
final  value 701.345847 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  69
initial  value 1374.312807 
iter  10 value 1041.052076
iter  20 value 653.812137
iter  30 value 650.941237
iter  40 value 650.646682
iter  50 value 650.530733
iter  60 value 650.459792
iter  70 value 650.432308
iter  80 value 650.426801
iter  90 value 650.425456
final  value 650.425153 
converged
Fitting Repeat 3 

# weights:  69
initial  value 1145.011169 
iter  10 value 1114.719905
iter  20 value 704.589404
iter  30 value 699.750061
iter  40 value 699.039527
iter  50 value 698.676819
iter  60 value 698.524710
iter  70 value 698.480338
iter  80 value 698.462773
iter  90 value 698.436770
iter 100 value 698.410845
final  value 698.410845 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  69
initial  value 1391.612105 
iter  10 value 683.033577
iter  20 value 680.972477
iter  30 value 680.679656
iter  40 value 680.606576
iter  50 value 680.573738
iter  60 value 680.555758
iter  70 value 680.546466
iter  80 value 680.541726
iter  90 value 680.539055
iter 100 value 680.537605
final  value 680.537605 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  69
initial  value 1292.791258 
iter  10 value 999.359740
iter  20 value 769.253102
iter  30 value 659.379189
iter  40 value 647.776389
iter  50 value 646.294933
iter  60 value 645.956634
iter  70 value 645.892699
iter  80 value 645.838733
iter  90 value 645.789208
iter 100 value 645.770196
final  value 645.770196 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  21
initial  value 1194.746716 
iter  10 value 1041.757729
final  value 1041.700273 
converged
Fitting Repeat 2 

# weights:  21
initial  value 1092.584627 
iter  10 value 1041.731549
final  value 1041.700269 
converged
Fitting Repeat 3 

# weights:  21
initial  value 1100.945589 
iter  10 value 1041.709401
final  value 1041.700497 
converged
Fitting Repeat 4 

# weights:  21
initial  value 1117.696525 
iter  10 value 1041.747250
iter  20 value 1041.699476
iter  30 value 1041.699327
iter  40 value 1041.699135
iter  50 value 1041.698872
iter  60 value 1041.698486
iter  70 value 1041.697867
iter  80 value 1041.696714
iter  90 value 1041.693891
iter 100 value 1041.680986
final  value 1041.680986 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  21
initial  value 1215.559607 
iter  10 value 1041.801524
iter  20 value 1041.698444
iter  30 value 661.814564
iter  40 value 657.916639
iter  50 value 657.474370
iter  60 value 657.432548
iter  70 value 657.412831
iter  80 value 657.408441
iter  90 value 657.404299
iter 100 value 657.395469
final  value 657.395469 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  41
initial  value 1156.698014 
iter  10 value 1042.552999
iter  20 value 1041.713981
iter  30 value 1035.459765
iter  40 value 665.590457
iter  50 value 658.425411
iter  60 value 658.132091
iter  70 value 657.954159
iter  80 value 657.836104
iter  90 value 657.701818
iter 100 value 657.658050
final  value 657.658050 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  41
initial  value 1277.119891 
iter  10 value 1043.314566
iter  20 value 1041.716122
iter  30 value 821.040756
iter  40 value 670.391322
iter  50 value 669.007920
iter  60 value 663.274661
iter  70 value 660.800505
iter  80 value 659.411300
iter  90 value 658.917528
iter 100 value 658.412042
final  value 658.412042 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  41
initial  value 1123.662825 
iter  10 value 1042.595710
iter  20 value 1041.700033
iter  30 value 1040.249619
iter  40 value 670.918236
iter  50 value 669.772359
iter  60 value 669.473699
iter  70 value 668.394862
iter  80 value 664.562915
iter  90 value 660.608290
iter 100 value 659.546701
final  value 659.546701 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  41
initial  value 1165.334726 
iter  10 value 1042.702442
iter  20 value 1041.693210
iter  30 value 665.289870
iter  40 value 661.624064
iter  50 value 658.497833
iter  60 value 658.178694
iter  70 value 658.010241
iter  80 value 657.871343
iter  90 value 657.670974
iter 100 value 657.637429
final  value 657.637429 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  41
initial  value 1272.787585 
iter  10 value 1042.790429
iter  20 value 1041.715977
iter  30 value 1041.695001
iter  40 value 1039.202033
iter  50 value 659.682341
iter  60 value 657.722905
iter  70 value 657.650039
iter  80 value 657.624425
iter  90 value 657.606667
iter 100 value 657.601205
final  value 657.601205 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  81
initial  value 1192.317986 
iter  10 value 974.439663
iter  20 value 674.674227
iter  30 value 665.580556
iter  40 value 665.196219
iter  50 value 665.174152
iter  60 value 665.144019
iter  70 value 665.140492
iter  80 value 665.139041
iter  90 value 665.137419
final  value 665.137193 
converged
Fitting Repeat 2 

# weights:  81
initial  value 1137.719628 
iter  10 value 930.975676
iter  20 value 684.528665
iter  30 value 667.860664
iter  40 value 665.942035
iter  50 value 665.350024
iter  60 value 665.148025
iter  70 value 665.129515
iter  80 value 665.128424
final  value 665.128337 
converged
Fitting Repeat 3 

# weights:  81
initial  value 1095.794449 
iter  10 value 887.831563
iter  20 value 673.473856
iter  30 value 666.584976
iter  40 value 665.504464
iter  50 value 665.300360
iter  60 value 665.209126
iter  70 value 665.177162
iter  80 value 665.169443
iter  90 value 665.163465
iter 100 value 665.160922
final  value 665.160922 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  81
initial  value 1118.508510 
iter  10 value 968.319066
iter  20 value 669.942113
iter  30 value 665.902412
iter  40 value 665.400001
iter  50 value 665.219634
iter  60 value 665.152761
iter  70 value 665.127529
iter  80 value 665.124133
iter  90 value 665.123152
iter 100 value 665.122774
final  value 665.122774 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  81
initial  value 1332.072059 
iter  10 value 706.012096
iter  20 value 670.492836
iter  30 value 665.500696
iter  40 value 665.192426
iter  50 value 665.142356
iter  60 value 665.133174
iter  70 value 665.128749
iter  80 value 665.128194
final  value 665.128163 
converged
Fitting Repeat 1 

# weights:  37
initial  value 1187.261732 
iter  10 value 1091.284277
iter  20 value 1089.863148
iter  30 value 698.356749
iter  40 value 696.048076
iter  50 value 695.889288
iter  60 value 695.778523
iter  70 value 695.732301
iter  80 value 695.699837
iter  90 value 695.680715
iter 100 value 695.652925
final  value 695.652925 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  37
initial  value 1185.622771 
iter  10 value 1029.878623
iter  20 value 995.125685
iter  30 value 634.864631
iter  40 value 630.911782
iter  50 value 628.042585
iter  60 value 627.343279
iter  70 value 626.902637
iter  80 value 626.671541
iter  90 value 626.593858
iter 100 value 626.528506
final  value 626.528506 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  37
initial  value 1130.014179 
iter  10 value 1105.243941
iter  20 value 702.078393
iter  30 value 700.290670
iter  40 value 699.949732
iter  50 value 699.753731
iter  60 value 699.690187
iter  70 value 699.669041
iter  80 value 699.660353
iter  90 value 699.653763
iter 100 value 699.649750
final  value 699.649750 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  37
initial  value 1206.460721 
iter  10 value 1034.280929
iter  20 value 1030.837570
iter  30 value 691.681557
iter  40 value 684.865107
iter  50 value 684.364169
iter  60 value 684.180568
iter  70 value 684.112177
iter  80 value 684.073890
iter  90 value 684.042820
iter 100 value 684.009520
final  value 684.009520 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  37
initial  value 1260.876038 
iter  10 value 1059.029652
iter  20 value 1056.368824
iter  30 value 1056.315995
iter  40 value 680.371916
iter  50 value 674.719401
iter  60 value 674.640510
iter  70 value 674.247130
iter  80 value 674.108394
iter  90 value 674.072165
iter 100 value 674.053074
final  value 674.053074 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  9
initial  value 1147.193330 
iter  10 value 1068.647668
final  value 1068.639067 
converged
Fitting Repeat 2 

# weights:  9
initial  value 1252.017081 
iter  10 value 1143.995807
final  value 1143.985638 
converged
Fitting Repeat 3 

# weights:  9
initial  value 1068.977042 
iter  10 value 962.674487
iter  20 value 962.656793
iter  30 value 962.656334
iter  40 value 962.655511
iter  50 value 962.653784
iter  60 value 962.649520
iter  70 value 962.638596
iter  80 value 962.611536
iter  90 value 962.500547
iter 100 value 685.267747
final  value 685.267747 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  9
initial  value 1063.017555 
iter  10 value 969.146811
final  value 969.142316 
converged
Fitting Repeat 5 

# weights:  9
initial  value 1194.735646 
iter  10 value 1086.863253
final  value 1086.844354 
converged
Fitting Repeat 1 

# weights:  45
initial  value 1077.948613 
iter  10 value 1043.423867
iter  20 value 740.735337
iter  30 value 665.692283
iter  40 value 660.904655
iter  50 value 659.892350
iter  60 value 659.564918
iter  70 value 659.421523
iter  80 value 659.326143
iter  90 value 659.280074
iter 100 value 659.260122
final  value 659.260122 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 1029.363973 
iter  10 value 704.705635
iter  20 value 669.302919
iter  30 value 661.448312
iter  40 value 659.782491
iter  50 value 659.447399
iter  60 value 659.298302
iter  70 value 659.250347
iter  80 value 659.241169
iter  90 value 659.236883
iter 100 value 659.231686
final  value 659.231686 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  45
initial  value 1201.906991 
iter  10 value 664.878004
iter  20 value 661.181252
iter  30 value 659.999684
iter  40 value 659.475487
iter  50 value 659.309937
iter  60 value 659.277614
iter  70 value 659.259782
iter  80 value 659.250608
iter  90 value 659.244356
iter 100 value 659.232463
final  value 659.232463 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 1059.359958 
iter  10 value 717.178437
iter  20 value 664.192333
iter  30 value 660.392679
iter  40 value 659.595517
iter  50 value 659.373957
iter  60 value 659.263697
iter  70 value 659.230980
iter  80 value 659.223908
iter  90 value 659.221380
final  value 659.220857 
converged
Fitting Repeat 5 

# weights:  45
initial  value 1049.925220 
iter  10 value 661.538149
iter  20 value 659.597824
iter  30 value 659.355100
iter  40 value 659.268398
iter  50 value 659.227862
iter  60 value 659.221314
iter  70 value 659.219974
iter  80 value 659.219731
final  value 659.219718 
converged
Fitting Repeat 1 

# weights:  25
initial  value 1161.241138 
iter  10 value 998.105028
iter  20 value 995.318569
iter  30 value 995.267991
iter  40 value 720.673446
iter  50 value 682.850523
iter  60 value 682.524336
iter  70 value 682.487446
iter  80 value 682.477186
iter  90 value 682.463295
iter 100 value 682.451185
final  value 682.451185 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  25
initial  value 1198.640141 
iter  10 value 1040.744505
iter  20 value 1036.898895
iter  30 value 689.435019
iter  40 value 685.820320
iter  50 value 678.261361
iter  60 value 674.214829
iter  70 value 672.869715
iter  80 value 672.076410
iter  90 value 671.760753
iter 100 value 671.435978
final  value 671.435978 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  25
initial  value 959.716891 
iter  10 value 937.186787
iter  20 value 640.344023
iter  30 value 621.941283
iter  40 value 606.118374
iter  50 value 603.788009
iter  60 value 603.516727
iter  70 value 603.292641
iter  80 value 603.179175
iter  90 value 603.097913
iter 100 value 603.078323
final  value 603.078323 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  25
initial  value 993.868815 
iter  10 value 954.330827
iter  20 value 609.397733
iter  30 value 602.047540
iter  40 value 601.543467
iter  50 value 601.289192
iter  60 value 601.158183
iter  70 value 601.086369
iter  80 value 601.049451
iter  90 value 601.034809
iter 100 value 601.021310
final  value 601.021310 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  25
initial  value 1218.803381 
iter  10 value 935.777603
iter  20 value 933.609036
iter  30 value 613.757045
iter  40 value 600.595413
iter  50 value 596.551067
iter  60 value 595.515118
iter  70 value 595.342858
iter  80 value 595.192989
iter  90 value 595.094635
iter 100 value 595.077998
final  value 595.077998 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  53
initial  value 1113.746102 
iter  10 value 752.074163
iter  20 value 735.061178
iter  30 value 734.941208
iter  40 value 734.935192
final  value 734.934470 
converged
Fitting Repeat 2 

# weights:  53
initial  value 1059.993561 
iter  10 value 778.341326
iter  20 value 737.231786
iter  30 value 735.352094
iter  40 value 735.060990
iter  50 value 735.011428
iter  60 value 734.998892
iter  70 value 734.988497
final  value 734.988466 
converged
Fitting Repeat 3 

# weights:  53
initial  value 1239.770483 
iter  10 value 743.732688
iter  20 value 735.719866
iter  30 value 735.640094
final  value 735.640039 
converged
Fitting Repeat 4 

# weights:  53
initial  value 1240.297100 
iter  10 value 743.041784
iter  20 value 734.964641
iter  30 value 734.934950
final  value 734.934485 
converged
Fitting Repeat 5 

# weights:  53
initial  value 1201.604299 
iter  10 value 764.335760
iter  20 value 737.671447
iter  30 value 735.461353
iter  40 value 735.190709
iter  50 value 735.052468
iter  60 value 734.991645
iter  70 value 734.988463
iter  70 value 734.988462
iter  70 value 734.988462
final  value 734.988462 
converged
Fitting Repeat 1 

# weights:  53
initial  value 1042.427453 
iter  10 value 956.493324
iter  20 value 956.223849
iter  30 value 956.220343
iter  40 value 956.219446
iter  50 value 956.217849
iter  60 value 956.214187
iter  70 value 956.198034
iter  80 value 604.512480
iter  90 value 603.416560
iter 100 value 600.678345
final  value 600.678345 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  53
initial  value 1026.344532 
iter  10 value 874.901851
iter  20 value 874.519134
iter  30 value 874.518610
iter  40 value 874.517861
iter  50 value 874.516634
iter  60 value 874.514144
iter  70 value 874.506129
iter  80 value 874.027825
iter  90 value 536.589265
iter 100 value 535.032602
final  value 535.032602 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  53
initial  value 1062.534171 
iter  10 value 882.798885
iter  20 value 882.278397
iter  30 value 634.919395
iter  40 value 618.509124
iter  50 value 575.174939
iter  60 value 567.394121
iter  70 value 566.731651
iter  80 value 566.549778
iter  90 value 566.133336
iter 100 value 562.149252
final  value 562.149252 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  53
initial  value 1336.035986 
iter  10 value 982.814470
final  value 982.556292 
converged
Fitting Repeat 5 

# weights:  53
initial  value 1161.271990 
iter  10 value 974.785063
iter  20 value 974.103495
iter  30 value 974.058002
iter  40 value 631.316724
iter  50 value 630.845002
iter  60 value 627.441680
iter  70 value 623.015175
iter  80 value 621.207272
iter  90 value 620.100504
iter 100 value 619.981580
final  value 619.981580 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  41
initial  value 1162.490155 
iter  10 value 777.424528
iter  20 value 695.640170
iter  30 value 694.685021
iter  40 value 694.662198
final  value 694.661098 
converged
Fitting Repeat 2 

# weights:  41
initial  value 1067.849833 
iter  10 value 696.474128
iter  20 value 694.800147
iter  30 value 694.664921
final  value 694.661040 
converged
Fitting Repeat 3 

# weights:  41
initial  value 1173.205854 
iter  10 value 852.820436
iter  20 value 708.537187
iter  30 value 700.258170
iter  40 value 700.160831
final  value 700.160591 
converged
Fitting Repeat 4 

# weights:  41
initial  value 1124.041308 
iter  10 value 700.828333
iter  20 value 695.331281
iter  30 value 695.308730
final  value 695.307021 
converged
Fitting Repeat 5 

# weights:  41
initial  value 1062.714251 
iter  10 value 706.170985
iter  20 value 698.720799
iter  30 value 694.984704
iter  40 value 694.770095
iter  50 value 694.755978
iter  60 value 694.671066
final  value 694.661034 
converged
Fitting Repeat 1 

# weights:  57
initial  value 1192.912106 
iter  10 value 848.137436
iter  20 value 823.552288
iter  30 value 820.833417
iter  40 value 820.590263
iter  50 value 820.451163
iter  60 value 820.429397
final  value 820.429331 
converged
Fitting Repeat 2 

# weights:  57
initial  value 1160.348561 
iter  10 value 913.999811
iter  20 value 893.727413
iter  30 value 892.902546
iter  40 value 892.846958
iter  50 value 892.843149
final  value 892.842820 
converged
Fitting Repeat 3 

# weights:  57
initial  value 1222.996642 
iter  10 value 896.069386
iter  20 value 813.082307
iter  30 value 801.131316
iter  40 value 799.883564
iter  50 value 799.390608
iter  60 value 799.134278
iter  70 value 799.094188
final  value 799.093654 
converged
Fitting Repeat 4 

# weights:  57
initial  value 1074.508000 
iter  10 value 861.421425
iter  20 value 848.295441
iter  30 value 847.240090
iter  40 value 846.161725
iter  50 value 845.816281
iter  60 value 845.613303
final  value 845.604867 
converged
Fitting Repeat 5 

# weights:  57
initial  value 1296.762961 
iter  10 value 874.821090
iter  20 value 849.450651
iter  30 value 847.934981
iter  40 value 847.629959
iter  50 value 847.613491
final  value 847.613417 
converged
Fitting Repeat 1 

# weights:  57
initial  value 1253.971021 
iter  10 value 1048.371859
iter  20 value 666.117604
iter  30 value 661.027178
iter  40 value 660.713290
iter  50 value 660.529550
iter  60 value 660.435404
iter  70 value 660.373390
iter  80 value 660.338212
iter  90 value 660.323836
iter 100 value 660.316115
final  value 660.316115 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  57
initial  value 957.353848 
iter  10 value 694.114577
iter  20 value 661.114180
iter  30 value 660.337119
iter  40 value 660.256618
iter  50 value 660.240579
iter  60 value 660.239973
final  value 660.239908 
converged
Fitting Repeat 3 

# weights:  57
initial  value 1111.086786 
iter  10 value 1045.615911
iter  20 value 674.497286
iter  30 value 664.468557
iter  40 value 660.594646
iter  50 value 660.285256
iter  60 value 660.267108
iter  70 value 660.262859
iter  80 value 660.259933
iter  90 value 660.258270
iter 100 value 660.257639
final  value 660.257639 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  57
initial  value 1131.617100 
iter  10 value 1031.087012
iter  20 value 683.523612
iter  30 value 663.147904
iter  40 value 661.580545
iter  50 value 660.847936
iter  60 value 660.607177
iter  70 value 660.423417
iter  80 value 660.356312
iter  90 value 660.331227
iter 100 value 660.285566
final  value 660.285566 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  57
initial  value 1446.899791 
iter  10 value 707.203547
iter  20 value 665.422845
iter  30 value 660.472658
iter  40 value 660.270978
iter  50 value 660.243911
iter  60 value 660.241015
iter  70 value 660.240028
iter  80 value 660.239800
final  value 660.239757 
converged
Fitting Repeat 1 

# weights:  81
initial  value 1079.903452 
iter  10 value 720.289697
iter  20 value 702.498178
iter  30 value 701.397129
iter  40 value 701.329665
iter  50 value 701.318816
final  value 701.314065 
converged
Fitting Repeat 2 

# weights:  81
initial  value 1088.422348 
iter  10 value 713.282465
iter  20 value 701.769436
iter  30 value 701.341040
iter  40 value 701.288944
final  value 701.288346 
converged
Fitting Repeat 3 

# weights:  81
initial  value 1048.169675 
iter  10 value 703.046708
iter  20 value 701.397857
iter  30 value 701.365602
final  value 701.365537 
converged
Fitting Repeat 4 

# weights:  81
initial  value 1162.319542 
iter  10 value 712.743726
iter  20 value 701.713446
iter  30 value 701.306019
iter  40 value 701.289477
iter  50 value 701.288498
final  value 701.288354 
converged
Fitting Repeat 5 

# weights:  81
initial  value 1234.239812 
iter  10 value 712.032303
iter  20 value 701.700731
iter  30 value 701.316901
iter  40 value 701.314036
iter  40 value 701.314031
iter  40 value 701.314030
final  value 701.314030 
converged
Fitting Repeat 1 

# weights:  5
initial  value 1201.472111 
iter  10 value 1051.708590
iter  20 value 1051.283075
iter  30 value 1051.277392
iter  40 value 1051.270238
iter  50 value 1051.260457
iter  60 value 1051.245192
iter  70 value 1051.214781
iter  80 value 1051.100943
iter  90 value 750.346058
iter 100 value 689.697083
final  value 689.697083 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  5
initial  value 1166.623960 
iter  10 value 1029.214570
iter  20 value 1028.500005
iter  30 value 1028.268915
iter  40 value 648.097863
iter  50 value 646.031029
iter  60 value 640.092513
iter  70 value 639.416683
iter  80 value 639.373878
final  value 639.368564 
converged
Fitting Repeat 3 

# weights:  5
initial  value 1293.436484 
iter  10 value 1127.712286
iter  20 value 1126.985082
iter  30 value 1126.611847
iter  40 value 773.089388
iter  50 value 755.734400
iter  60 value 749.202890
iter  70 value 743.920663
iter  80 value 743.636170
final  value 743.630333 
converged
Fitting Repeat 4 

# weights:  5
initial  value 1157.046835 
iter  10 value 1067.714437
iter  20 value 709.158783
iter  30 value 698.484896
iter  40 value 697.524996
iter  50 value 694.490321
iter  60 value 691.964554
iter  70 value 691.786215
iter  80 value 691.694135
iter  90 value 691.688826
iter  90 value 691.688823
iter  90 value 691.688823
final  value 691.688823 
converged
Fitting Repeat 5 

# weights:  5
initial  value 1363.108895 
iter  10 value 1190.560235
iter  20 value 1189.779630
iter  30 value 1189.630313
iter  40 value 783.437103
iter  50 value 764.314148
iter  60 value 763.604202
final  value 763.603991 
converged
Fitting Repeat 1 

# weights:  65
initial  value 1080.944489 
iter  10 value 1042.220881
iter  20 value 1041.702656
iter  30 value 1041.694378
iter  40 value 990.767466
iter  50 value 658.359433
iter  60 value 658.301195
iter  70 value 658.155801
iter  80 value 657.626899
iter  90 value 657.581079
iter 100 value 657.546845
final  value 657.546845 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  65
initial  value 1209.893495 
iter  10 value 1042.609146
iter  20 value 1041.708156
iter  20 value 1041.708150
iter  30 value 1041.704777
iter  40 value 1041.704521
iter  50 value 1041.704214
iter  60 value 1041.703843
iter  70 value 1041.703366
iter  80 value 1041.702687
iter  90 value 1041.701564
iter 100 value 1041.699367
final  value 1041.699367 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  65
initial  value 1096.173377 
iter  10 value 1042.288768
iter  20 value 1041.700227
iter  30 value 1041.682101
iter  40 value 660.565415
iter  50 value 657.933185
iter  60 value 657.573694
iter  70 value 657.549492
iter  80 value 657.532723
iter  90 value 657.524023
iter 100 value 657.517504
final  value 657.517504 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  65
initial  value 1096.065712 
iter  10 value 1042.117213
iter  20 value 1041.704145
iter  30 value 1041.702028
iter  40 value 1041.696192
iter  50 value 1041.613278
iter  60 value 671.364055
iter  70 value 670.966105
iter  80 value 668.705561
iter  90 value 667.769418
iter 100 value 665.892742
final  value 665.892742 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  65
initial  value 1294.987168 
iter  10 value 1042.713821
iter  20 value 1041.706895
iter  30 value 1041.696907
iter  40 value 1041.660604
iter  50 value 679.280432
iter  60 value 671.984660
iter  70 value 669.368913
iter  80 value 666.929299
iter  90 value 662.841888
iter 100 value 661.080314
final  value 661.080314 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  45
initial  value 1274.630995 
iter  10 value 1010.309305
iter  20 value 1009.259202
iter  30 value 1009.193972
iter  40 value 656.411738
iter  50 value 648.915172
iter  60 value 648.373675
iter  70 value 644.819044
iter  80 value 642.958228
iter  90 value 641.784634
iter 100 value 640.977322
final  value 640.977322 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 1070.512770 
iter  10 value 1009.702869
iter  20 value 1009.237547
iter  30 value 669.902600
iter  40 value 650.186104
iter  50 value 642.146169
iter  60 value 641.364772
iter  70 value 640.817726
iter  80 value 640.373623
iter  90 value 640.028931
iter 100 value 639.428274
final  value 639.428274 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  45
initial  value 1146.044913 
iter  10 value 1010.074616
iter  20 value 1009.252039
iter  30 value 1009.214900
iter  40 value 641.975149
iter  50 value 639.715392
iter  60 value 639.286130
iter  70 value 638.895254
iter  80 value 638.873234
iter  90 value 638.849883
iter 100 value 638.843283
final  value 638.843283 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 1173.512177 
iter  10 value 1010.308322
iter  20 value 1009.262072
iter  30 value 696.934866
iter  40 value 642.572146
iter  50 value 639.641616
iter  60 value 639.427717
iter  70 value 639.280529
iter  80 value 639.087443
iter  90 value 639.008708
iter 100 value 638.968299
final  value 638.968299 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 1196.809999 
iter  10 value 1010.019086
iter  20 value 1009.266643
iter  30 value 726.577553
iter  40 value 673.353410
iter  50 value 652.320649
iter  60 value 651.700860
iter  70 value 649.376112
iter  80 value 641.353290
iter  90 value 639.291867
iter 100 value 639.171258
final  value 639.171258 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  81
initial  value 1125.909137 
iter  10 value 953.463343
iter  20 value 656.279196
iter  30 value 645.264016
iter  40 value 643.719866
iter  50 value 643.281586
iter  60 value 643.152932
iter  70 value 643.117952
iter  80 value 643.017804
iter  90 value 642.991479
iter 100 value 642.983766
final  value 642.983766 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  81
initial  value 1058.873625 
iter  10 value 1010.268522
iter  20 value 685.201302
iter  30 value 646.515159
iter  40 value 644.355388
iter  50 value 643.536947
iter  60 value 643.177035
iter  70 value 643.049207
iter  80 value 642.972349
iter  90 value 642.931878
iter 100 value 642.921408
final  value 642.921408 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  81
initial  value 1163.342439 
iter  10 value 978.724251
iter  20 value 681.655375
iter  30 value 644.778383
iter  40 value 643.410682
iter  50 value 643.077063
iter  60 value 643.023080
iter  70 value 642.984447
iter  80 value 642.951340
iter  90 value 642.939071
iter 100 value 642.931202
final  value 642.931202 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  81
initial  value 1191.981705 
iter  10 value 1006.622201
iter  20 value 659.116452
iter  30 value 646.557824
iter  40 value 643.896969
iter  50 value 643.131273
iter  60 value 643.083545
iter  70 value 643.055965
iter  80 value 643.040738
iter  90 value 643.033709
iter 100 value 643.030407
final  value 643.030407 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  81
initial  value 1113.080932 
iter  10 value 1003.655249
iter  20 value 664.828283
iter  30 value 646.125835
iter  40 value 643.725734
iter  50 value 643.266237
iter  60 value 643.076351
iter  70 value 643.002108
iter  80 value 642.956968
iter  90 value 642.923241
iter 100 value 642.914370
final  value 642.914370 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  73
initial  value 1295.648874 
iter  10 value 1009.392990
iter  20 value 1009.247052
final  value 1009.246999 
converged
Fitting Repeat 2 

# weights:  73
initial  value 1205.135945 
iter  10 value 1009.402083
iter  20 value 1009.247969
final  value 1009.247774 
converged
Fitting Repeat 3 

# weights:  73
initial  value 1108.368936 
iter  10 value 1009.310620
final  value 1009.246653 
converged
Fitting Repeat 4 

# weights:  73
initial  value 1034.437405 
iter  10 value 1009.260921
final  value 1009.247170 
converged
Fitting Repeat 5 

# weights:  73
initial  value 1102.893479 
iter  10 value 1009.387550
iter  20 value 1009.247339
final  value 1009.247272 
converged
Fitting Repeat 1 

# weights:  25
initial  value 1135.140013 
iter  10 value 848.797179
iter  20 value 694.504129
iter  30 value 685.983698
iter  40 value 684.911988
iter  50 value 684.834321
final  value 684.834017 
converged
Fitting Repeat 2 

# weights:  25
initial  value 1165.755180 
iter  10 value 795.154942
iter  20 value 682.296349
iter  30 value 682.063131
final  value 682.058882 
converged
Fitting Repeat 3 

# weights:  25
initial  value 1235.349803 
iter  10 value 739.236099
iter  20 value 686.252725
iter  30 value 683.574041
iter  40 value 683.476452
final  value 683.476441 
converged
Fitting Repeat 4 

# weights:  25
initial  value 1163.074565 
iter  10 value 793.527481
iter  20 value 701.418751
iter  30 value 687.802604
iter  40 value 684.892966
iter  50 value 684.834092
final  value 684.834006 
converged
Fitting Repeat 5 

# weights:  25
initial  value 1267.565764 
iter  10 value 776.349941
iter  20 value 690.446431
iter  30 value 685.197016
iter  40 value 682.370745
iter  50 value 682.061457
final  value 682.058863 
converged
Fitting Repeat 1 

# weights:  69
initial  value 1365.729774 
iter  10 value 1097.311611
iter  20 value 1075.355875
iter  30 value 701.627227
iter  40 value 692.417388
iter  50 value 690.092455
iter  60 value 689.625246
iter  70 value 689.519083
iter  80 value 689.477082
iter  90 value 689.449829
iter 100 value 689.435552
final  value 689.435552 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  69
initial  value 1124.448016 
iter  10 value 1028.521576
iter  20 value 831.311565
iter  30 value 706.074335
iter  40 value 697.113809
iter  50 value 695.408806
iter  60 value 695.075600
iter  70 value 694.919865
iter  80 value 694.879932
iter  90 value 694.852635
iter 100 value 694.835412
final  value 694.835412 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  69
initial  value 1003.057935 
iter  10 value 695.002626
iter  20 value 676.022329
iter  30 value 673.049825
iter  40 value 672.376768
iter  50 value 672.255007
iter  60 value 672.216605
iter  70 value 672.196658
iter  80 value 672.176358
iter  90 value 672.166374
iter 100 value 672.162927
final  value 672.162927 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  69
initial  value 1006.151713 
iter  10 value 961.982173
iter  20 value 660.761902
iter  30 value 619.454918
iter  40 value 605.493521
iter  50 value 600.135113
iter  60 value 597.709102
iter  70 value 596.332046
iter  80 value 596.048934
iter  90 value 595.963075
iter 100 value 595.941040
final  value 595.941040 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  69
initial  value 1446.257935 
iter  10 value 831.584265
iter  20 value 802.001493
iter  30 value 790.591705
iter  40 value 786.801105
iter  50 value 785.919780
iter  60 value 785.737563
iter  70 value 785.694409
iter  80 value 785.679771
iter  90 value 785.672977
iter 100 value 785.671000
final  value 785.671000 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  21
initial  value 1124.903126 
iter  10 value 1009.303582
iter  20 value 1009.239750
iter  30 value 1009.169883
iter  40 value 926.045991
iter  50 value 647.724599
iter  60 value 647.025598
iter  70 value 646.118047
iter  80 value 644.621385
iter  90 value 639.249310
iter 100 value 639.068378
final  value 639.068378 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  21
initial  value 1052.979627 
iter  10 value 1009.277361
iter  20 value 648.729131
iter  30 value 638.970166
iter  40 value 638.839156
iter  50 value 638.833722
iter  60 value 638.821853
iter  70 value 638.813368
iter  80 value 638.809171
iter  90 value 638.760816
iter 100 value 638.750332
final  value 638.750332 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  21
initial  value 1274.502933 
iter  10 value 1009.297930
final  value 1009.252623 
converged
Fitting Repeat 4 

# weights:  21
initial  value 1086.743891 
iter  10 value 1009.315426
iter  20 value 1009.249148
iter  30 value 1009.248921
iter  40 value 1009.248590
iter  50 value 1009.248065
iter  60 value 1009.247141
iter  70 value 1009.245289
iter  80 value 1009.240831
iter  90 value 1009.226391
iter 100 value 1009.072848
final  value 1009.072848 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  21
initial  value 1149.012605 
iter  10 value 1009.303936
final  value 1009.251910 
converged
Fitting Repeat 1 

# weights:  41
initial  value 1211.608297 
iter  10 value 1010.702040
iter  20 value 1009.271133
iter  30 value 1009.197286
iter  40 value 641.864886
iter  50 value 639.764135
iter  60 value 639.365474
iter  70 value 638.972310
iter  80 value 638.938022
iter  90 value 638.903086
iter 100 value 638.897189
final  value 638.897189 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  41
initial  value 1093.247797 
iter  10 value 1009.860534
iter  20 value 1008.382767
iter  30 value 641.220799
iter  40 value 639.073623
iter  50 value 638.935688
iter  60 value 638.925007
iter  70 value 638.917827
iter  80 value 638.910328
iter  90 value 638.905139
iter 100 value 638.900898
final  value 638.900898 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  41
initial  value 1054.348323 
iter  10 value 1009.814036
iter  20 value 1009.260235
iter  30 value 1009.231413
iter  40 value 649.046642
iter  50 value 642.902423
iter  60 value 639.981167
iter  70 value 639.655486
iter  80 value 639.313197
iter  90 value 639.171311
iter 100 value 639.035683
final  value 639.035683 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  41
initial  value 1045.625522 
iter  10 value 1009.544561
iter  20 value 1009.143300
iter  30 value 640.239669
iter  40 value 639.689302
iter  50 value 639.537493
iter  60 value 639.100416
iter  70 value 638.986207
iter  80 value 638.927638
iter  90 value 638.913348
iter 100 value 638.903889
final  value 638.903889 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  41
initial  value 1277.001650 
iter  10 value 1010.377351
iter  20 value 1009.267813
iter  30 value 1009.148263
iter  40 value 650.317350
iter  50 value 647.138566
iter  60 value 640.999402
iter  70 value 640.544749
iter  80 value 640.044274
iter  90 value 639.555911
iter 100 value 639.324757
final  value 639.324757 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  81
initial  value 1042.315517 
iter  10 value 647.424923
iter  20 value 646.553057
iter  30 value 646.530438
iter  40 value 646.503916
iter  50 value 646.500217
final  value 646.499975 
converged
Fitting Repeat 2 

# weights:  81
initial  value 1306.676479 
iter  10 value 771.462010
iter  20 value 647.957630
iter  30 value 646.617573
iter  40 value 646.550526
iter  50 value 646.542115
iter  60 value 646.537045
iter  70 value 646.536772
iter  70 value 646.536767
iter  70 value 646.536767
final  value 646.536767 
converged
Fitting Repeat 3 

# weights:  81
initial  value 1152.935228 
iter  10 value 670.029324
iter  20 value 648.901516
iter  30 value 646.659164
iter  40 value 646.559371
iter  50 value 646.512719
iter  60 value 646.507054
iter  70 value 646.506101
final  value 646.506068 
converged
Fitting Repeat 4 

# weights:  81
initial  value 1180.310350 
iter  10 value 872.890192
iter  20 value 649.256838
iter  30 value 646.744290
iter  40 value 646.584937
iter  50 value 646.555411
iter  60 value 646.526411
iter  70 value 646.502466
iter  80 value 646.500222
final  value 646.500088 
converged
Fitting Repeat 5 

# weights:  81
initial  value 1364.250769 
iter  10 value 844.716114
iter  20 value 649.717496
iter  30 value 646.947279
iter  40 value 646.590187
iter  50 value 646.563834
iter  60 value 646.559767
iter  70 value 646.555687
iter  80 value 646.553937
iter  90 value 646.553331
iter 100 value 646.552287
final  value 646.552287 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  37
initial  value 1191.380668 
iter  10 value 1066.472912
iter  20 value 1063.745379
iter  30 value 706.236396
iter  40 value 702.597010
iter  50 value 699.208862
iter  60 value 698.087376
iter  70 value 696.817464
iter  80 value 695.756753
iter  90 value 695.437368
iter 100 value 695.169526
final  value 695.169526 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  37
initial  value 1216.022627 
iter  10 value 937.940933
iter  20 value 935.305272
iter  30 value 712.819420
iter  40 value 608.013800
iter  50 value 606.248514
iter  60 value 606.070821
iter  70 value 606.001226
iter  80 value 605.991760
iter  90 value 605.978622
iter 100 value 605.975274
final  value 605.975274 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  37
initial  value 1220.788825 
iter  10 value 1076.674257
iter  20 value 1074.149220
iter  30 value 740.123124
iter  40 value 713.382858
iter  50 value 710.317428
iter  60 value 709.277062
iter  70 value 707.830059
iter  80 value 707.681262
iter  90 value 707.596439
iter 100 value 707.528022
final  value 707.528022 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  37
initial  value 1197.149542 
iter  10 value 990.397619
iter  20 value 986.407095
iter  30 value 986.012203
iter  40 value 684.116577
iter  50 value 680.727102
iter  60 value 675.555871
iter  70 value 674.404563
iter  80 value 673.705175
iter  90 value 673.373154
iter 100 value 673.086159
final  value 673.086159 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  37
initial  value 1052.914585 
iter  10 value 992.772504
iter  20 value 991.985234
iter  30 value 991.979337
iter  40 value 991.960366
iter  50 value 748.687551
iter  60 value 646.438804
iter  70 value 646.017768
iter  80 value 645.940498
iter  90 value 645.914547
iter 100 value 645.873223
final  value 645.873223 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  9
initial  value 1059.631467 
final  value 1035.436659 
converged
Fitting Repeat 2 

# weights:  9
initial  value 1160.850644 
iter  10 value 958.125551
final  value 958.098932 
converged
Fitting Repeat 3 

# weights:  9
initial  value 1149.298427 
iter  10 value 900.076908
final  value 900.042666 
converged
Fitting Repeat 4 

# weights:  9
initial  value 984.612116 
iter  10 value 913.716025
iter  20 value 913.697574
iter  30 value 913.696374
iter  40 value 913.694862
iter  50 value 913.692896
iter  60 value 913.690235
iter  70 value 913.686431
iter  80 value 913.680538
iter  90 value 913.670191
iter 100 value 913.647359
final  value 913.647359 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  9
initial  value 1156.536231 
iter  10 value 1006.665633
final  value 1006.645541 
converged
Fitting Repeat 1 

# weights:  45
initial  value 1339.499070 
iter  10 value 1011.468632
iter  20 value 688.973434
iter  30 value 659.255823
iter  40 value 645.480402
iter  50 value 642.109827
iter  60 value 641.101325
iter  70 value 640.793912
iter  80 value 640.666785
iter  90 value 640.625130
iter 100 value 640.608860
final  value 640.608860 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  45
initial  value 1221.529510 
iter  10 value 1011.821058
iter  20 value 779.986046
iter  30 value 652.033277
iter  40 value 641.608279
iter  50 value 640.904714
iter  60 value 640.661681
iter  70 value 640.595929
iter  80 value 640.583136
iter  90 value 640.576514
iter 100 value 640.570363
final  value 640.570363 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  45
initial  value 1046.066192 
iter  10 value 667.223464
iter  20 value 645.429190
iter  30 value 641.374540
iter  40 value 640.816537
iter  50 value 640.655483
iter  60 value 640.599027
iter  70 value 640.577634
iter  80 value 640.565535
iter  90 value 640.556008
iter 100 value 640.551789
final  value 640.551789 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  45
initial  value 1067.887264 
iter  10 value 1010.695303
iter  20 value 667.482323
iter  30 value 643.178969
iter  40 value 641.250640
iter  50 value 640.686355
iter  60 value 640.588944
iter  70 value 640.553752
iter  80 value 640.551051
iter  90 value 640.550480
iter 100 value 640.550191
final  value 640.550191 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  45
initial  value 1162.789638 
iter  10 value 1011.646881
iter  20 value 650.831476
iter  30 value 642.625547
iter  40 value 640.793901
iter  50 value 640.589014
iter  60 value 640.556994
iter  70 value 640.552332
iter  80 value 640.551217
iter  90 value 640.550489
iter 100 value 640.549737
final  value 640.549737 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  25
initial  value 1122.129218 
iter  10 value 924.816628
iter  20 value 922.780515
iter  30 value 922.500130
iter  40 value 601.652502
iter  50 value 598.101048
iter  60 value 592.595533
iter  70 value 591.085765
iter  80 value 590.535861
iter  90 value 590.172858
iter 100 value 590.022179
final  value 590.022179 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  25
initial  value 1104.751445 
iter  10 value 1063.360582
iter  20 value 1060.357804
iter  30 value 735.218346
iter  40 value 718.277396
iter  50 value 715.632480
iter  60 value 711.401214
iter  70 value 709.203053
iter  80 value 707.317153
iter  90 value 705.422358
iter 100 value 703.876550
final  value 703.876550 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  25
initial  value 1261.485898 
iter  10 value 1057.533510
iter  20 value 1055.318502
iter  30 value 713.326494
iter  40 value 708.310561
iter  50 value 703.157224
iter  60 value 702.171267
iter  70 value 701.675112
iter  80 value 701.478735
iter  90 value 701.401129
iter 100 value 701.327074
final  value 701.327074 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  25
initial  value 1011.443767 
iter  10 value 621.059978
iter  20 value 616.496978
iter  30 value 615.803657
iter  40 value 615.498764
iter  50 value 615.368431
iter  60 value 615.306916
iter  70 value 615.259091
iter  80 value 615.220371
iter  90 value 615.203827
iter 100 value 615.199486
final  value 615.199486 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  25
initial  value 1151.642546 
iter  10 value 981.574552
iter  20 value 979.280630
iter  30 value 648.653517
iter  40 value 644.966516
iter  50 value 644.848658
iter  60 value 644.788874
iter  70 value 644.751031
iter  80 value 644.744507
iter  90 value 644.740003
iter 100 value 644.738682
final  value 644.738682 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  53
initial  value 1416.340158 
iter  10 value 808.792340
iter  20 value 735.215360
iter  30 value 717.616531
iter  40 value 716.874716
iter  50 value 716.869174
iter  60 value 716.851378
iter  70 value 716.837850
final  value 716.837262 
converged
Fitting Repeat 2 

# weights:  53
initial  value 1051.231489 
iter  10 value 718.766169
iter  20 value 716.186223
iter  30 value 716.137454
final  value 716.135061 
converged
Fitting Repeat 3 

# weights:  53
initial  value 1274.147105 
iter  10 value 734.300392
iter  20 value 717.655166
iter  30 value 716.967949
iter  40 value 716.856605
iter  50 value 716.838908
iter  60 value 716.837289
final  value 716.837266 
converged
Fitting Repeat 4 

# weights:  53
initial  value 1191.123241 
iter  10 value 823.735365
iter  20 value 725.943751
iter  30 value 716.807115
iter  40 value 716.247911
iter  50 value 716.183884
iter  60 value 716.150477
iter  70 value 716.135832
final  value 716.135061 
converged
Fitting Repeat 5 

# weights:  53
initial  value 1056.385319 
iter  10 value 720.043230
iter  20 value 716.351807
iter  30 value 716.190204
iter  40 value 716.189135
final  value 716.189037 
converged
Fitting Repeat 1 

# weights:  53
initial  value 1037.782544 
iter  10 value 971.011816
iter  20 value 665.106874
iter  30 value 661.231106
iter  40 value 630.079573
iter  50 value 629.756516
iter  60 value 628.220840
iter  70 value 626.504554
iter  80 value 621.776811
iter  90 value 620.801956
iter 100 value 620.733404
final  value 620.733404 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  53
initial  value 1160.974807 
iter  10 value 1018.053091
iter  20 value 1017.777800
iter  30 value 1017.766989
iter  40 value 1017.751930
iter  50 value 704.673663
iter  60 value 673.792890
iter  70 value 673.398153
iter  80 value 673.111866
iter  90 value 672.704241
iter 100 value 672.648507
final  value 672.648507 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  53
initial  value 1043.851695 
iter  10 value 973.311911
iter  20 value 973.147274
iter  30 value 641.835138
iter  40 value 636.339263
iter  50 value 635.885186
iter  60 value 635.867603
iter  70 value 635.843446
iter  80 value 635.834540
iter  90 value 635.824092
iter 100 value 635.812259
final  value 635.812259 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  53
initial  value 1071.725180 
iter  10 value 983.956056
iter  20 value 983.617055
iter  30 value 983.616946
iter  40 value 983.616822
iter  50 value 983.616678
iter  60 value 983.616509
iter  70 value 983.616300
iter  80 value 983.616030
iter  90 value 983.615670
iter 100 value 983.615150
final  value 983.615150 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  53
initial  value 1320.696947 
iter  10 value 1037.708483
iter  20 value 1037.168291
iter  30 value 672.201138
iter  40 value 669.348089
iter  50 value 668.110338
iter  60 value 668.065331
iter  70 value 667.990386
iter  80 value 667.873339
iter  90 value 667.711083
iter 100 value 667.668596
final  value 667.668596 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  41
initial  value 1256.477678 
iter  10 value 881.943455
iter  20 value 698.054754
iter  30 value 680.492206
iter  40 value 677.056753
iter  50 value 676.535388
iter  60 value 676.195792
iter  70 value 676.170472
iter  80 value 676.112736
iter  90 value 676.099062
final  value 676.098844 
converged
Fitting Repeat 2 

# weights:  41
initial  value 1094.560419 
iter  10 value 682.457267
iter  20 value 677.082695
iter  30 value 676.052556
iter  40 value 676.027660
final  value 676.025979 
converged
Fitting Repeat 3 

# weights:  41
initial  value 1187.874977 
iter  10 value 721.364617
iter  20 value 682.928889
iter  30 value 676.992412
iter  40 value 676.512773
iter  50 value 676.504525
final  value 676.504365 
converged
Fitting Repeat 4 

# weights:  41
initial  value 1085.773472 
iter  10 value 682.190055
iter  20 value 676.407047
iter  30 value 676.064441
iter  40 value 676.026030
final  value 676.025983 
converged
Fitting Repeat 5 

# weights:  41
initial  value 1058.638334 
iter  10 value 757.986214
iter  20 value 680.336858
iter  30 value 676.732233
iter  40 value 676.516743
iter  50 value 676.505069
final  value 676.504356 
converged
Fitting Repeat 1 

# weights:  57
initial  value 1149.342163 
iter  10 value 863.600377
iter  20 value 839.354006
iter  30 value 837.025989
iter  40 value 836.718260
iter  50 value 836.571075
iter  60 value 836.562386
final  value 836.562174 
converged
Fitting Repeat 2 

# weights:  57
initial  value 1101.261909 
iter  10 value 774.338210
iter  20 value 734.653483
iter  30 value 729.774800
iter  40 value 728.990314
iter  50 value 728.941378
iter  60 value 728.937979
final  value 728.937914 
converged
Fitting Repeat 3 

# weights:  57
initial  value 1319.125930 
iter  10 value 917.776338
iter  20 value 868.838366
iter  30 value 862.917806
iter  40 value 860.998184
iter  50 value 860.892499
iter  60 value 860.881288
iter  70 value 860.876803
final  value 860.876775 
converged
Fitting Repeat 4 

# weights:  57
initial  value 1203.006741 
iter  10 value 868.071869
iter  20 value 771.989414
iter  30 value 761.461514
iter  40 value 760.732875
iter  50 value 760.685943
iter  60 value 760.657120
final  value 760.656426 
converged
Fitting Repeat 5 

# weights:  57
initial  value 1096.991384 
iter  10 value 808.168640
iter  20 value 791.825461
iter  30 value 790.983727
iter  40 value 790.970754
iter  50 value 790.968073
final  value 790.967941 
converged
Fitting Repeat 1 

# weights:  57
initial  value 1342.316083 
iter  10 value 1013.812846
iter  20 value 675.355339
iter  30 value 646.768102
iter  40 value 641.881325
iter  50 value 641.674008
iter  60 value 641.615133
iter  70 value 641.591875
iter  80 value 641.583185
iter  90 value 641.581189
iter 100 value 641.580128
final  value 641.580128 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  57
initial  value 1018.476902 
iter  10 value 643.143597
iter  20 value 641.632314
iter  30 value 641.593168
iter  40 value 641.590245
iter  50 value 641.589609
iter  60 value 641.589242
final  value 641.589062 
converged
Fitting Repeat 3 

# weights:  57
initial  value 1236.273098 
iter  10 value 649.099993
iter  20 value 642.595608
iter  30 value 641.665841
iter  40 value 641.587565
iter  50 value 641.580991
iter  60 value 641.580236
iter  70 value 641.579784
iter  80 value 641.579430
iter  90 value 641.579228
final  value 641.579189 
converged
Fitting Repeat 4 

# weights:  57
initial  value 1157.544290 
iter  10 value 1014.146292
iter  20 value 652.814686
iter  30 value 643.538763
iter  40 value 641.853508
iter  50 value 641.723866
iter  60 value 641.669208
iter  70 value 641.653875
iter  80 value 641.615105
iter  90 value 641.594768
iter 100 value 641.592856
final  value 641.592856 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  57
initial  value 1045.322618 
iter  10 value 814.505378
iter  20 value 646.579276
iter  30 value 642.587029
iter  40 value 641.743747
iter  50 value 641.626959
iter  60 value 641.605322
iter  70 value 641.593277
iter  80 value 641.589013
iter  90 value 641.579769
iter 100 value 641.576694
final  value 641.576694 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  81
initial  value 1081.378901 
iter  10 value 685.357001
iter  20 value 682.951746
iter  30 value 682.695598
final  value 682.676889 
converged
Fitting Repeat 2 

# weights:  81
initial  value 1023.644799 
iter  10 value 687.742781
iter  20 value 683.184464
iter  30 value 682.672547
iter  40 value 682.651538
iter  50 value 682.651018
iter  50 value 682.651012
iter  50 value 682.651011
final  value 682.651011 
converged
Fitting Repeat 3 

# weights:  81
initial  value 1144.534876 
iter  10 value 687.246754
iter  20 value 683.124736
iter  30 value 682.779062
iter  40 value 682.700066
iter  50 value 682.678868
final  value 682.676919 
converged
Fitting Repeat 4 

# weights:  81
initial  value 1118.597787 
iter  10 value 710.233404
iter  20 value 683.241899
iter  30 value 682.682806
iter  40 value 682.677024
final  value 682.676886 
converged
Fitting Repeat 5 

# weights:  81
initial  value 1068.410098 
iter  10 value 781.446494
iter  20 value 687.843159
iter  30 value 683.733084
iter  40 value 682.889829
iter  50 value 682.826335
iter  60 value 682.820530
iter  70 value 682.819857
final  value 682.819831 
converged
Fitting Repeat 1 

# weights:  5
initial  value 1083.443325 
iter  10 value 1047.354514
iter  20 value 607.785929
iter  30 value 605.110656
iter  40 value 594.808093
iter  50 value 593.177967
iter  60 value 593.029685
final  value 593.029573 
converged
Fitting Repeat 2 

# weights:  5
initial  value 1130.412687 
iter  10 value 1010.619958
iter  20 value 1010.190398
iter  30 value 680.508409
iter  40 value 676.857286
iter  50 value 675.989155
iter  60 value 670.304188
iter  70 value 668.315558
iter  80 value 668.164586
final  value 668.157092 
converged
Fitting Repeat 3 

# weights:  5
initial  value 1292.233925 
iter  10 value 1066.911475
iter  20 value 1066.139315
iter  30 value 1066.056111
iter  40 value 1065.936035
iter  50 value 798.125229
iter  60 value 681.195130
iter  70 value 679.702296
iter  80 value 675.203910
iter  90 value 675.018834
final  value 675.005280 
converged
Fitting Repeat 4 

# weights:  5
initial  value 1055.979072 
iter  10 value 955.373890
iter  20 value 955.107048
iter  30 value 955.101002
iter  40 value 955.093196
iter  50 value 955.082103
iter  60 value 955.063622
iter  70 value 955.021348
iter  80 value 954.741611
iter  90 value 625.182978
iter 100 value 621.599575
final  value 621.599575 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  5
initial  value 1178.102783 
iter  10 value 930.443020
iter  20 value 929.657438
iter  30 value 929.646770
iter  40 value 929.632756
iter  50 value 929.630662
iter  60 value 929.628491
iter  70 value 929.626210
iter  80 value 929.623767
iter  90 value 929.621093
iter 100 value 929.618079
final  value 929.618079 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  65
initial  value 1036.308881 
iter  10 value 1009.409198
iter  20 value 696.629115
iter  30 value 650.191329
iter  40 value 649.267646
iter  50 value 648.885109
iter  60 value 648.113704
iter  70 value 645.446658
iter  80 value 643.611734
iter  90 value 641.878392
iter 100 value 640.329929
final  value 640.329929 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  65
initial  value 1400.151278 
iter  10 value 1009.673264
iter  20 value 1009.256796
iter  30 value 656.618554
iter  40 value 641.355604
iter  50 value 639.597123
iter  60 value 639.462200
iter  70 value 639.331691
iter  80 value 639.205146
iter  90 value 639.081802
iter 100 value 639.002860
final  value 639.002860 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  65
initial  value 1073.490952 
iter  10 value 1010.156771
iter  20 value 1009.254463
iter  30 value 1009.253985
iter  40 value 1009.253292
iter  50 value 1009.252156
iter  60 value 1009.249793
iter  70 value 1009.241300
iter  80 value 683.568725
iter  90 value 639.777596
iter 100 value 639.629368
final  value 639.629368 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  65
initial  value 1207.533244 
iter  10 value 1010.845731
iter  20 value 1009.263801
iter  30 value 640.264540
iter  40 value 639.572593
iter  50 value 639.105100
iter  60 value 638.891902
iter  70 value 638.867283
iter  80 value 638.850965
iter  90 value 638.845310
iter 100 value 638.841795
final  value 638.841795 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  65
initial  value 1138.086026 
iter  10 value 1010.308425
iter  20 value 1009.257625
iter  30 value 1009.112971
iter  40 value 648.948330
iter  50 value 641.776746
iter  60 value 640.170633
iter  70 value 639.935755
iter  80 value 639.696618
iter  90 value 639.319189
iter 100 value 639.164344
final  value 639.164344 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  41
initial  value 1571.451050 
iter  10 value 1550.689934
iter  20 value 1548.452741
iter  30 value 998.755513
iter  40 value 998.041316
iter  50 value 997.300179
iter  60 value 997.221988
iter  70 value 997.178688
iter  80 value 997.166762
iter  90 value 997.162500
iter 100 value 997.156754
final  value 997.156754 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  41
initial  value 1888.743989 
iter  10 value 1552.565934
iter  20 value 1550.097981
iter  30 value 1550.063499
iter  40 value 1534.789686
iter  50 value 1002.249882
iter  60 value 998.592306
iter  70 value 997.910507
iter  80 value 997.329891
iter  90 value 997.257396
iter 100 value 997.229334
final  value 997.229334 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  41
initial  value 1836.619154 
iter  10 value 1552.970202
iter  20 value 1550.099711
iter  30 value 1015.286958
iter  40 value 1013.112388
iter  50 value 1007.033556
iter  60 value 1000.026407
iter  70 value 999.101040
iter  80 value 998.692267
iter  90 value 998.287052
iter 100 value 997.780351
final  value 997.780351 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  41
initial  value 1595.303547 
iter  10 value 1551.348412
iter  20 value 1550.064468
iter  30 value 1021.157555
iter  40 value 998.535546
iter  50 value 998.043844
iter  60 value 997.297947
iter  70 value 997.242955
iter  80 value 997.209427
iter  90 value 997.183148
iter 100 value 997.167416
final  value 997.167416 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  41
initial  value 1921.602606 
iter  10 value 1552.334617
iter  20 value 1550.098961
iter  30 value 1195.078571
iter  40 value 999.940951
iter  50 value 997.673670
iter  60 value 997.297115
iter  70 value 997.261033
iter  80 value 997.246722
iter  90 value 997.235817
iter 100 value 997.212023
final  value 997.212023 
stopped after 100 iterations
Model Averaged Neural Network 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 501, 501 
Resampling results across tuning parameters:

  size  decay         bag    RMSE      Rsquared   Selected
   1    5.657025e-04   TRUE  1.164686  0.6951998          
   2    1.965207e-05   TRUE  1.394557  0.5985719          
   5    3.139264e-05  FALSE  1.369467  0.4999766          
   6    9.263327e-04   TRUE  1.150656  0.7206785          
   6    7.533109e-01  FALSE  1.160582  0.7780643          
   9    6.217310e-04   TRUE  1.150647  0.7200704          
  10    3.191744e-04  FALSE  1.150612  0.7204057  *       
  10    6.767904e-01  FALSE  1.158221  0.7757678          
  11    2.115869e-04  FALSE  1.178869  0.7195098          
  11    9.888626e-03  FALSE  1.151497  0.7104248          
  13    9.066458e-05   TRUE  1.192093  0.7046824          
  13    1.847884e+00  FALSE  1.169120  0.8180559          
  14    2.414492e-02  FALSE  1.151532  0.7147407          
  14    7.487350e+00   TRUE  1.215610  0.8978499          
  16    1.941086e-04  FALSE  1.162434  0.7153746          
  17    6.786236e-03   TRUE  1.151429  0.7105474          
  18    2.782907e-05  FALSE  1.326299  0.5594494          
  20    4.441388e-02  FALSE  1.151604  0.7201837          
  20    9.982902e-02  FALSE  1.152011  0.7304084          
  20    8.913664e-01  FALSE  1.159669  0.7875404          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 10, decay = 0.0003191744
 and bag = FALSE.
[1] "Sun Mar 11 15:49:57 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  model fit failed for Fold1: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

2: In eval(xpr, envir = envir) :
  model fit failed for Fold2: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

3: In eval(xpr, envir = envir) :
  model fit failed for Fold3: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  model fit failed for Fold1: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

2: In eval(xpr, envir = envir) :
  model fit failed for Fold2: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

3: In eval(xpr, envir = envir) :
  model fit failed for Fold3: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:49:59 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "bag"                     
Bagged MARS 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 501, 501 
Resampling results across tuning parameters:

  degree  nprune  RMSE         Rsquared   Selected
  1       2       0.800684633  0.7998573          
  1       3       0.152385235  0.9925708          
  1       4       0.064987846  0.9981621          
  1       5       0.003150964  0.9999953          
  2       3       0.178028732  0.9869852          
  2       4       0.075926563  0.9975250          
  2       5       0.003140661  0.9999953  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nprune = 5 and degree = 2.
[1] "Sun Mar 11 15:50:11 2018"
Bagged MARS using gCV Pruning 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 501, 501 
Resampling results:

  RMSE         Rsquared 
  0.003146028  0.9999953

Tuning parameter 'degree' was held constant at a value of 1
[1] "Sun Mar 11 15:50:17 2018"
Loading required package: mgcv
Loading required package: nlme
This is mgcv 1.8-17. For overview type 'help("mgcv-package")'.

Attaching package: 'mgcv'

The following object is masked from 'package:nnet':

    multinom

Generalized Additive Model using Splines 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 501, 501 
Resampling results across tuning parameters:

  select  method  RMSE         Rsquared   Selected
  FALSE   GCV.Cp  0.003137913  0.9999953          
  FALSE   ML      0.003124552  0.9999953  *       
  FALSE   REML    0.003126617  0.9999953          
   TRUE   GCV.Cp  0.003170024  0.9999952          
   TRUE   REML    0.003126625  0.9999953          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were select = FALSE and method = ML.
[1] "Sun Mar 11 15:50:57 2018"
Loading required package: bartMachine
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Failed with error:  'package 'rJava' could not be loaded'
In addition: Warning message:
In max(c(NaN, NaN), na.rm = TRUE) :
  no non-missing arguments to max; returning -Inf
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package 'rJava' could not be loaded
Loading required package: bartMachine
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Failed with error:  'package 'rJava' could not be loaded'
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package 'rJava' could not be loaded
Loading required package: bartMachine
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Failed with error:  'package 'rJava' could not be loaded'
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package 'rJava' could not be loaded
 [1] "failed"                   "failed"                  
 [3] "Sun Mar 11 15:50:59 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "bartMachine"             
Loading required package: arm
Loading required package: lme4

Attaching package: 'lme4'

The following object is masked from 'package:nlme':

    lmList


arm (Version 1.9-3, built: 2016-11-21)

Working directory is C:/Users/irina grishina/Desktop/generated data test/LAPTOP-1SBQTC5I


Attaching package: 'arm'

The following objects are masked from 'package:pls':

    coefplot, corrplot

The following object is masked from 'package:plotrix':

    rescale

Bayesian Generalized Linear Model 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 501, 501 
Resampling results:

  RMSE         Rsquared 
  0.003117646  0.9999954

[1] "Sun Mar 11 15:51:02 2018"
Loading required package: monomvn
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
The Bayesian lasso 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 501, 501 
Resampling results across tuning parameters:

  sparsity    RMSE         Rsquared   Selected
  0.04757576  0.004448527  0.9999952          
  0.07869760  0.004448527  0.9999952          
  0.20772399  0.004448527  0.9999952          
  0.25109595  0.004448527  0.9999952          
  0.26113834  0.004448527  0.9999952          
  0.41359011  0.004448527  0.9999952          
  0.45065580  0.004448527  0.9999952          
  0.47029005  0.004448527  0.9999952          
  0.53170463  0.004448527  0.9999952          
  0.53789397  0.004448527  0.9999952          
  0.63966445  0.004448527  0.9999952          
  0.64388162  0.004448527  0.9999952          
  0.66866923  0.004448527  0.9999952          
  0.67525475  0.004448527  0.9999952          
  0.79775398  0.004448527  0.9999952          
  0.81850293  0.004448527  0.9999952          
  0.86503280  0.004448527  0.9999952          
  0.96122680  0.004448527  0.9999952          
  0.97122970  0.004448527  0.9999952          
  0.97566439  0.004448527  0.9999952  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was sparsity = 0.9756644.
[1] "Sun Mar 11 15:51:05 2018"
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
Bayesian Ridge Regression (Model Averaged) 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 501, 501 
Resampling results:

  RMSE         Rsquared 
  0.004747036  0.9999954

[1] "Sun Mar 11 15:51:07 2018"
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
Bayesian Ridge Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 501, 501 
Resampling results:

  RMSE         Rsquared 
  0.003565576  0.9999954

[1] "Sun Mar 11 15:51:09 2018"
Loading required package: bst
Loading required package: gbm
Loading required package: splines
Loaded gbm 2.1.3

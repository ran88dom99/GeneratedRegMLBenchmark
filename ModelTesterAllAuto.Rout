
R version 3.4.3 (2017-11-30) -- "Kite-Eating Tree"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> options(repos=structure(c(CRAN="https://rweb.crmda.ku.edu/cran/")))
> ## capture messages and errors to a file.https://rweb.crmda.ku.edu/cran/
> #zz <- file("all.Rout", open="wt")https://cran.cnr.berkeley.edu
> #sink(zz, type="message")
> #chek for R package updates
> #try(log("a"))
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> which.computer<-Sys.info()[['nodename']]
> task.subject<-"14th20hp3cv"
> out.file<-paste("out",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,".csv",sep="")
> importance.file<-paste("importance",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,sep="")
> 
> base.folder<-getwd()
> cpout.folder<-paste(base.folder,"/",which.computer,sep = "")
> setwd(cpout.folder)
> 
> if(length(which(list.files() == out.file))<1) write.table( "0.01,0.01,100,100,100,Wed Aug 02 16:37:25 2017,dummy,8,1,basic latent features,ignore,none,asis,1.12784979099243,random,333,53,adaptive_cv,16,5,2,2,19,0.0107744822639878,FALSE,,,,,,,,,," ,file =,out.file,  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,".csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,".csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,"mlr.csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,"mlr.csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> 
> cv.iters=3
> tuneLength=20
> tuneLength2=8
> normings=c("YeoJohnson","ICA", "centernscale","expoTrans","range01","asis","quantile")#,"centernscale"
> 
> gensTTesto<-c(56,53,4,12,13,14,15,20,45,54,55, 44,52,1,3)#,  51,c(4)#c(1:40)#c(5,10,11,13,14,15,16,17,18,19,20,21,24,28,38,39,40)
> write.table( t(gensTTesto),file = "initial tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
> try({
+   gensTTest<-(read.csv("tasks to test.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+ })
> if(length(gensTTest)<1) gensTTest<-gensTTesto
> 
> ########packages install check######
> 
> #list.of.packages <- c("caret","caretEnsemble","mlr","MLmetrics","tgp")
> #list.of.packages <- c("gower","dimRed","DEoptimR","caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> #new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> #if(length(new.packages)) install.packages(new.packages, dep = TRUE)
> 
> 
> #install.packages("mlr", dependencies = c("Depends", "Suggests"))
> #install.packages("caret", dependencies = c("Depends", "Suggests"))
> #install.packages("caret",repos = "http://cran.r-project.org",dependencies = c("Depends", "Imports", "Suggests"))
> #install.packages("SuperLearner", dependencies = c("Depends", "Suggests"))
> #install.packages("rattle", dependencies = c("Depends", "Suggests"))
> 
> # Load libraries
> #library(mlbench)
> 
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> #library(caretEnsemble)
> library(MLmetrics)

Attaching package: 'MLmetrics'

The following objects are masked from 'package:caret':

    MAE, RMSE

The following object is masked from 'package:base':

    Recall

> 
> ########error no repeat#########
> 
> 
> try({
+   before.last.alg<-as.matrix(read.csv("beforelast algorithm.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.alg<-as.matrix(read.csv("last algorithm tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   #write.table(paste(date(), last.alg,.Platform$OS.type,.Platform$r_arch,which.computer,sep=" "),file = "algos after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.alg==before.last.alg){print("algorithm may be broken")}
+   write.table(last.alg,file = "beforelast algorithm.csv",  quote = F, row.names = F,col.names = F)
+ })
> try({
+   before.last.tsk<-as.matrix(read.csv("beforelast task.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.tsk<-as.matrix(read.csv("last task tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   write.table(paste(date(),last.alg, last.tsk,cv.iters,tuneLength,.Platform$OS.type,.Platform$r_arch,which.computer,sep=","),file = "test after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.tsk==before.last.tsk){print("task may be broken")}
+   write.table(last.tsk,file = "beforelast task.csv",  quote = F, row.names = F,col.names = F)
+ })
[1] "task may be broken"
> bad.models=c("spaccceeee")
> previous.fails<-(read.csv("test after which reset.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
> previous.fails<-previous.fails[previous.fails[,8]==which.computer,]
> lgf<-length(previous.fails[,2])
> for(lt in 2:lgf)  {
+   if(previous.fails[lt,2]==previous.fails[lt-1,2])  {
+     bad.models=union(bad.models,c(paste(previous.fails[lt,2])))  }}
> 
> #######not to redo a test function#####
> check.redundant<-function(df=df.previous.calcs,norming="asis",trans.y=1,withextra="missing",missingdata="leaveempty",datasource="mean" ,column.to.predict=200,allmodel="ctree")
+ {
+   for(intern in 1:length(df[,1])){
+     if((any(df[intern,] == norming, na.rm=T))&&
+        (any(df[intern,] == withextra, na.rm=T))&&
+        (any(df[intern,] == missingdata, na.rm=T))&&
+        (any(df[intern,] == datasource, na.rm=T))&&
+        (any(df[intern,] == column.to.predict, na.rm=T))&&
+        (any(df[intern,] == allmodel, na.rm=T))&&
+        (  (df[intern,9] == trans.y)))
+     {return(TRUE)}
+   }
+   return(FALSE)
+ }
> #####caret init#####
> best.ranged <- c("avNNet", "nnet", "pcaNNet", "glm.nb")
> best.asis <- c("svmLinear3", "relaxo", "superpc", "xgbTree")
> best.cns <- c("gam", "bam", "svmLinear2", "msaenet", "BstLm", "gbm") 
> 
> cv6hp5 <- c( "BstLm", "qrnn")#earth
> cv3hp32 <- c("Rborist", "pcaNNet", "SBC")
> cv7x5hp32 <- c("gbm", "krlsPoly", "kknn", "xgbLinear","RRF", "cubist", "rlm" )
> cv6hp5.avoid <- c("pcaNNet")
> cv3hp32.avoid <- c("glm.nb", "gamboost", "ctree2","glmboost", "leapSeq","ctree","svmLinear2")
> cv7x5hp32.avoid <- c("SBC","bagearthgcv","gcvearth","lmStepAIC","glmStepAIC","bridge","lm","glm","bayesglm","blassoAveraged","treebag","rpart1SE")
> 
> allmodels <- c("avNNet", "bagEarth", "bagEarthGCV",
+                "bayesglm", "bdk", "blackboost", "Boruta", "brnn", "BstLm" ,
+                "bstTree", "cforest", "ctree", "ctree2", "cubist", "DENFIS",
+                "dnn", "earth", "elm", "enet",   "evtree",
+                "extraTrees",  "gamLoess",  "gaussprLinear", "gaussprPoly", "gaussprRadial",
+                "gcvEarth","glm", "glmboost",  "icr", "kernelpls",
+                "kknn", "knn",  "krlsRadial", "lars" , "lasso",
+                "leapBackward", "leapForward", "leapSeq", "lm", "M5", "M5Rules",
+                "mlpWeightDecay", "neuralnet" , "partDSA",
+                "pcaNNet", "pcr", "penalized", "pls", "plsRglm", "ppr",
+                "qrf" , "ranger",  "rf")
> allmodels <- c("rlm", "rpart", "rpart2",
+                "RRF", "RRFglobal",  "simpls",
+                "svmLinear", "svmPoly", "svmRadial", "svmRadialCost",
+                "widekernelpls",  "xgbLinear",
+                "xgbTree")
> allmodels <- c("avNNet","BstLm","bstTree","cforest","ctree","ctree2",
+                "cubist","earth","enet","evtree","glmboost",
+                "icr","kernelpls","kknn","lasso","pcaNNet",
+                "pcr","pls","qrf","ranger","rf")
> 
> allmodels <- c("kknn", "cubist", "avNNet", "xgbLinear", "RRF", "pcaNNet","earth","nnet","gbm","enet","lasso","BstLm",
+                "foba", "leapBackward", "gcvEarth", "SBC","glm.nb","gamboost","ctree2","relaxo", 
+                "bartMachine","extraTrees","bam","gam","randomGLM")
> #allmodels <- c("bam")
> #allmodels <- c("rf")"rqlasso",, "xyf" "rvmPoly", "rvmRadial",    "spls", "superpc" ,   "treebag",  "svmLinear2",  "SBC",
> #allmodels <- c("bartMachine", "xgbLinear", "pcaNNet","svmLinear","glmnet","cforest","cubist","rf","ranger")"glmnet",
> #wow rfRules is really slow "rfRules","WM", takes 50min
> # brak everythig "rbfDDA","ridge","rqnc",
> # use "rf" to test all
> library(caret)
> allmodels <- unique(modelLookup()[modelLookup()$forReg,c(1)])
> #allmodels <-c("avNNet", "nnet", "pcaNNet",  "glm.nb", "gam" ,
> #              "bam","msaenet", "svmLinear2","svmLinear3",
> #              "relaxo",  "superpc", "xgbTree", "BstLm")
> #allmodels<- c("svmLinear","svmPoly","svmRadial")
> #library(doParallel); cl <- makeCluster(detectCores()); registerDoParallel(cl)
> #allmodels<-c("bartMachine","extraTrees")#,"randomGLM"
> 
> 
> adaptControl <- trainControl(method = "adaptive_cv",
+                              number = 7, repeats = 5,
+                              adaptive = list(min = 4, alpha = 0.05,
+                                              method = "gls", complete = FALSE),
+                              search = "random")
> adaptControl <-trainControl(method = "cv", number = cv.iters,  search = "random")
> simpleControl <- trainControl(method = "cv",
+                               number = cv.iters,
+                               search = "random")
> 
> 
> #########MLR init######
> #R.utils::gcDLLs()
> #list.of.packages <- c("ParamHelpers","devtools","mlrMBO","RJSONIO","plot3D","plotly")
> #install.packages("mlrMBO", dependencies = c("Depends", "Suggests"))
> list.of.packages <- c("caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> if(length(new.packages)) install.packages(new.packages, dep = TRUE)
Warning message:
packages 'logicFS', ' RWeka', 'rPython', 'rrf' are not available (for R version 3.4.3) 
> 
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> tuneLengthMLR<-tuneLength
> mlr.iters<-cv.iters
> #######data read process start#####
> seed.var =222+round(runif(1,min=0,max=100))
> column.to.predict=1
> print(date());
[1] "Fri Jan 12 20:32:54 2018"
> 
> setwd(base.folder)
> if(!exists("gen.count")){gen.count=56}
> gens.names<-as.matrix(read.table("gens names.csv", sep = ",",header = FALSE,row.names=1,fill=TRUE, quote="",dec="."))
> for(gend.data in gensTTest){
+   setwd(base.folder)
+   data.source<-as.matrix(read.csv(paste("Generats/",gens.names[gend.data],".csv", sep = ""), sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   datasource<-gens.names[gend.data,1]
+   setwd(cpout.folder)
+   missingdatas=c("ignore")
+   for(missingdata in missingdatas){
+     withextras=c("none")
+     for(withextra in withextras){
+       ################data wrestling###############
+       
+       dependant.selection=complete.cases(data.source[,column.to.predict])
+       df.previous.calcs=as.data.frame(read.csv(file=out.file, header = FALSE, sep = ",", quote = "",
+                                                dec = ".", fill = TRUE, comment.char = ""))
+       unimportant.computations<-vector(mode = "logical",length=length(df.previous.calcs[,1])  )
+       for(intern in 1:length(df.previous.calcs[,1])){
+         if((any(df.previous.calcs[intern,] == withextra, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == missingdata, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == datasource, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == column.to.predict, na.rm=T)))
+         {unimportant.computations[intern]<-T}}
+       
+       df.previous.calcs<-df.previous.calcs[unimportant.computations,]
+       
+       
+       
+       #data.source=data.frame( data.source[,column.to.predict],data.source[,1:2], data.source[,4:(column.to.predict-1)], data.source[,(column.to.predict+1):length( data.source[1,])])
+       
+       
+         for(norming in normings) {
+         for(trans.y in 1:2) {
+           df.toprocess=data.source
+           y.untransformed<-df.toprocess[,1]
+           
+           if(norming=="centernscale"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("center", "scale"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="range01"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="expoTrans"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("expoTrans"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="YeoJohnson"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("YeoJohnson"))#"center", "scale",
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           if((norming=="asis")&&(trans.y==2)){next}
+           
+           
+           ################preprocess###########
+           df.toprocess=data.frame(df.toprocess[dependant.selection,])
+           y.untransformed=y.untransformed[dependant.selection]
+           if(norming=="quantile"){
+             for(Clol in trans.y:length(data.source[1,])){
+               df.toprocess[,Clol]<- (rank(df.toprocess[,Clol],na.last = "keep",ties.method = "average")-1) }
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           
+           loess.model<-loess(y.untransformed~ df.toprocess[,1],span = 0.21, degree = 1)
+           
+           
+           #df.toprocess = data.frame(df.toprocess,)
+           nzv <- nearZeroVar(df.toprocess[,])#, saveMetrics= TRUE
+           #nzv[nzv$nzv,][1:10,]
+           if(length(nzv)>1){
+             df.toprocess = (df.toprocess[, -nzv])}
+           
+           seed.var =222+round(runif(1,min=0,max=100))
+           set.seed(seed.var)
+           inTrain <- createDataPartition(y = df.toprocess[,1],
+                                          p = .75,
+                                          list = FALSE)
+           training <- df.toprocess[ inTrain,]
+           testing  <- df.toprocess[-inTrain,]
+           write.table(df.toprocess,file = "sanity check 1.csv",  quote = F, row.names = F,col.names = F)
+           
+           
+           
+           ###########for all models#################
+           setwd(base.folder)
+           if(max(which.computer==c("ALTA","HOPPER"))>0)
+             source("MLR part.R")
+           else
+             source("Caret part.R")
+           
+          setwd(cpout.folder)
+           if(norming == normings[length(normings)]){
+             write.table( gensTTest[-1],file = "tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)}
+           
+         }
+       }
+     }
+   }
+   
+ }

Attaching package: 'mlr'

The following object is masked from 'package:caret':

    train

Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
replacing previous import 'BBmisc::isFALSE' by 'backports::isFALSE' when loading 'mlr' 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  6.916344 8.037717 4.8946 5.60977 24.04801 42.83385 3.99756 3.99756 4.247041 4.14941 6.604495 5.202464 8.146007 4.455497 8.890634 6.584955 7.91525 8.837383 5.537444 8.944944 9.173486 7.481885 9.114579 8.491638 8.733924 5.814787 5.394467 7.451764 6.508818 3.99869 4.343312 13.01522 12.58088 9.138194 5.812431 4.838335 5.037523 5.567975 5.588714 7.638953 4.18422 5.74317 5.731567 5.652859 6.747334 4.205231 5.851932 4.336629 4.072879 7.051768 17.67309 4.238293 4.12017 6.664383 6.110547 6.664383 4.033661 4.024729 7.16777 4.033574 4.014111 7.42458 5.518349 6.843315 7.362419 4.252496 5.530165 6.549321 4.38946 9.004517 6.868495 4.353849 24.74906 6.22627 10.57332 27.85469 3.999084 5.385471 4.392101 8.105988 8.105988 8.143407 7.143376 22.54598 9.46101 20.20247 7.708457 16.75374 10.89096 7.954099 19.61318 7.91525 4.078773 3.998724 14.76566 7.91525 9.30714 3.997739 4.069193 4.143031 8.179604 4.449646 8.910589 4.585431 4.33835 4.028919 10.70499 13.26443 8.3129 7.279324 13.67494 15.78724 4.09878 4.09878 3.741657 4.901846 5.067017 4.020698 6.835681 8.13722 6.023665 7.224804 5.902923 7.878865 5.210207 7.145596 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 
  - best initial criterion value(s) :  -535.8198 

N = 166, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       535.82  |proj g|=       1.0149
At iterate     1  f =       524.67  |proj g|=        2.5124
ys=-4.924e+00  -gs= 8.445e+00, BFGS update SKIPPED
At iterate     2  f =       495.27  |proj g|=        1.9843
ys=-6.600e+00  -gs= 2.524e+01, BFGS update SKIPPED
At iterate     3  f =       436.31  |proj g|=        2.6391
ys=-1.333e+01  -gs= 4.951e+01, BFGS update SKIPPED
At iterate     4  f =       369.41  |proj g|=        1.5766
At iterate     5  f =       335.74  |proj g|=        1.6268
At iterate     6  f =       296.77  |proj g|=        6.1937
ys=-1.609e+00  -gs= 3.781e+01, BFGS update SKIPPED
At iterate     7  f =       266.24  |proj g|=        2.4323
At iterate     8  f =       244.74  |proj g|=        2.7793
At iterate     9  f =       232.28  |proj g|=        1.9515
At iterate    10  f =       217.75  |proj g|=       0.42494
At iterate    11  f =       213.04  |proj g|=       0.28621
At iterate    12  f =       207.13  |proj g|=       0.20836
At iterate    13  f =       205.69  |proj g|=       0.12649
At iterate    14  f =       205.22  |proj g|=      0.089831
At iterate    15  f =       204.37  |proj g|=       0.55479
At iterate    16  f =       204.09  |proj g|=      0.064387
At iterate    17  f =       204.02  |proj g|=       0.06193
At iterate    18  f =       203.77  |proj g|=       0.17738
At iterate    19  f =       203.13  |proj g|=        2.2763
ys=-9.897e-01  -gs= 2.541e-01, BFGS update SKIPPED
At iterate    20  f =       189.15  |proj g|=       0.30394
At iterate    21  f =       189.08  |proj g|=       0.23251
At iterate    22  f =       189.01  |proj g|=      0.063984
At iterate    23  f =          189  |proj g|=      0.053224
At iterate    24  f =       188.86  |proj g|=      0.031116
At iterate    25  f =        188.6  |proj g|=      0.045853
At iterate    26  f =       188.47  |proj g|=       0.45815
ys=-2.018e-01  -gs= 3.935e-02, BFGS update SKIPPED
At iterate    27  f =       183.37  |proj g|=        0.1184
ys=-1.782e+00  -gs= 1.863e+00, BFGS update SKIPPED
At iterate    28  f =       183.36  |proj g|=      0.016036
At iterate    29  f =       183.36  |proj g|=     0.0086488
At iterate    30  f =       183.36  |proj g|=     0.0065026
At iterate    31  f =       183.36  |proj g|=      0.027866
At iterate    32  f =       183.35  |proj g|=      0.029266
ys=-2.066e-04  -gs= 9.946e-04, BFGS update SKIPPED
At iterate    33  f =        182.3  |proj g|=       0.89145
ys=-6.911e-01  -gs= 9.675e-02, BFGS update SKIPPED
At iterate    34  f =       178.17  |proj g|=       0.52988
At iterate    35  f =       177.84  |proj g|=       0.52452
At iterate    36  f =       177.43  |proj g|=       0.33958
At iterate    37  f =       177.33  |proj g|=      0.064222
At iterate    38  f =       177.33  |proj g|=      0.030853
At iterate    39  f =       177.33  |proj g|=     0.0028642
At iterate    40  f =       177.33  |proj g|=     0.0028674
At iterate    41  f =       177.33  |proj g|=     0.0057305
At iterate    42  f =       177.33  |proj g|=      0.012536
At iterate    43  f =       177.32  |proj g|=      0.024747
At iterate    44  f =       177.32  |proj g|=      0.041924
At iterate    45  f =       177.32  |proj g|=      0.063876
At iterate    46  f =       177.32  |proj g|=      0.046586
At iterate    47  f =       177.32  |proj g|=     0.0022272
At iterate    48  f =       177.32  |proj g|=    7.5053e-05
At iterate    49  f =       177.32  |proj g|=    1.2685e-07

iterations 49
function evaluations 60
segments explored during Cauchy searches 187
BFGS updates skipped 9
active bounds at final generalized Cauchy point 146
norm of the final projected gradient 1.26849e-07
final function value 177.32

F = 177.32
final  value 177.319680 
converged
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.km: Error in checkNames(X1 = X, X2 = newdata, X1.name = "the design", X2.name = "newdata") : 
  the design and newdata must have the same numbers of columns

[1] "Fri Jan 12 21:51:06 2018"
[Tune] Started tuning learner regr.ksvm for parameter set:
         Type len  Def    Constr Req Tunable Trafo
C     numeric   -    0  -5 to 10   -    TRUE     Y
sigma numeric   - TRUE -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: C=22.3; sigma=46.9
[Tune-y] 1: rmse.test.rmse=0.401; time: 0.1 min
[Tune-x] 2: C=162; sigma=0.0345
[Tune-y] 2: rmse.test.rmse=0.33; time: 0.0 min
[Tune-x] 3: C=1.25; sigma=206
[Tune-y] 3: rmse.test.rmse=0.401; time: 0.0 min
[Tune-x] 4: C=19.5; sigma=5.55
[Tune-y] 4: rmse.test.rmse=0.401; time: 0.0 min
[Tune-x] 5: C=40.2; sigma=0.552
[Tune-y] 5: rmse.test.rmse=0.401; time: 0.0 min
[Tune-x] 6: C=0.895; sigma=0.000842
[Tune-y] 6: rmse.test.rmse=0.137; time: 0.0 min
[Tune-x] 7: C=14.4; sigma=0.0378
[Tune-y] 7: rmse.test.rmse=0.343; time: 0.0 min
[Tune-x] 8: C=3.31; sigma=0.00401
[Tune-y] 8: rmse.test.rmse=0.144; time: 0.0 min
[Tune-x] 9: C=5.39; sigma=80
[Tune-y] 9: rmse.test.rmse=0.401; time: 0.0 min
[Tune-x] 10: C=171; sigma=0.000268
[Tune-y] 10: rmse.test.rmse=0.15; time: 0.1 min
[Tune-x] 11: C=271; sigma=1.57e+04
[Tune-y] 11: rmse.test.rmse=0.401; time: 0.0 min
[Tune-x] 12: C=25; sigma=0.00102
[Tune-y] 12: rmse.test.rmse=0.14; time: 0.0 min
[Tune-x] 13: C=7.12; sigma=656
[Tune-y] 13: rmse.test.rmse=0.401; time: 0.0 min
[Tune-x] 14: C=47.3; sigma=0.205
[Tune-y] 14: rmse.test.rmse=0.401; time: 0.0 min
[Tune-x] 15: C=4.56; sigma=0.851
[Tune-y] 15: rmse.test.rmse=0.401; time: 0.0 min
[Tune-x] 16: C=0.105; sigma=0.00314
[Tune-y] 16: rmse.test.rmse=0.172; time: 0.0 min
[Tune-x] 17: C=438; sigma=5.88e-05
[Tune-y] 17: rmse.test.rmse=0.153; time: 0.1 min
[Tune-x] 18: C=207; sigma=65.8
[Tune-y] 18: rmse.test.rmse=0.401; time: 0.0 min
[Tune-x] 19: C=36.4; sigma=0.000174
[Tune-y] 19: rmse.test.rmse=0.144; time: 0.0 min
[Tune-x] 20: C=101; sigma=2.06
[Tune-y] 20: rmse.test.rmse=0.401; time: 0.0 min
[Tune] Result: C=0.895; sigma=0.000842 : rmse.test.rmse=0.137
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.ksvm: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Fri Jan 12 21:52:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
In addition: There were 20 warnings (use warnings() to see them)
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.laGP: Error in (function (X, Z, XX, start = 6, end = 50, d = NULL, g = 1/10000,  : 
  mismatch XX and X cols

[1] "Fri Jan 12 21:52:15 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.LiblineaRL2L1SVR: Error in predict.LiblineaR(.model$learner.model, newx = .newdata, ...) : 
  columns of 'test' and 'train' differ

[1] "Fri Jan 12 21:52:28 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.LiblineaRL2L2SVR: Error in predict.LiblineaR(.model$learner.model, newx = .newdata, ...) : 
  columns of 'test' and 'train' differ

[1] "Fri Jan 12 21:52:37 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.lm: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Fri Jan 12 21:52:46 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Jan 12 21:53:41 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
Error in root.matrix(crossprod(process)) : 
  matrix is not positive semidefinite
Warning in train(allmodel, regr.task) :
  Could not train learner regr.mob: Error in trainLearner.regr.mob(.learner = structure(list(id = "regr.mob",  : 
  Failed to fit party::mob. Some coefficients are estimated as NA

[1] "Fri Jan 12 21:53:52 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=12; decay=5.95
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2017) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2017) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2017) weights

[Tune-y] 1: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 2: size=18; decay=0.000537
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3025) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3025) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3025) weights

[Tune-y] 2: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 3: size=19; decay=0.826
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3193) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3193) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3193) weights

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: size=1; decay=0.00506
# weights:  169
initial  value 111077.128924 
iter  10 value 231.927869
iter  20 value 166.996153
iter  30 value 128.771265
iter  40 value 104.782109
iter  50 value 96.570505
iter  60 value 87.830096
iter  70 value 78.407079
iter  80 value 67.855036
iter  90 value 60.639213
iter 100 value 53.611798
final  value 53.611798 
stopped after 100 iterations
# weights:  169
initial  value 100303.974768 
iter  10 value 137.403846
iter  20 value 113.119055
iter  30 value 107.248424
iter  40 value 101.623773
iter  50 value 96.969711
iter  60 value 91.206870
iter  70 value 88.470492
iter  80 value 84.071199
iter  90 value 77.645631
iter 100 value 68.548693
final  value 68.548693 
stopped after 100 iterations
# weights:  169
initial  value 101559.506391 
iter  10 value 1166.843949
iter  20 value 117.000424
iter  30 value 104.255471
iter  40 value 51.872824
iter  50 value 49.153732
iter  60 value 47.000360
iter  70 value 44.334722
iter  80 value 40.184227
iter  90 value 34.714791
iter 100 value 28.651867
final  value 28.651867 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=0.375; time: 0.0 min
[Tune-x] 5: size=11; decay=2.33
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1849) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1849) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1849) weights

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: size=15; decay=0.223
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2521) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2521) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2521) weights

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: size=19; decay=0.000125
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3193) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3193) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3193) weights

[Tune-y] 7: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 8: size=8; decay=0.00459
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1345) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1345) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1345) weights

[Tune-y] 8: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 9: size=13; decay=5.87
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2185) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2185) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2185) weights

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: size=17; decay=2.05e-05
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2857) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2857) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2857) weights

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: size=7; decay=2.41e-05
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1177) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1177) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1177) weights

[Tune-y] 11: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 12: size=18; decay=0.00398
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3025) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3025) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3025) weights

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: size=11; decay=0.000878
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1849) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1849) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1849) weights

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: size=10; decay=0.696
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

[Tune-y] 14: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 15: size=14; decay=0.111
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: size=20; decay=0.873
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3361) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3361) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3361) weights

[Tune-y] 16: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 17: size=1; decay=0.000728
# weights:  169
initial  value 96672.854219 
iter  10 value 122.475566
iter  20 value 122.475164
iter  30 value 122.473534
iter  40 value 122.459606
final  value 122.454563 
converged
# weights:  169
initial  value 101266.501847 
iter  10 value 114.336023
iter  20 value 114.319784
iter  30 value 114.284924
iter  40 value 114.251405
iter  50 value 114.219412
iter  60 value 113.329469
iter  70 value 111.750327
iter  80 value 109.733507
iter  90 value 108.897360
iter 100 value 106.244717
final  value 106.244717 
stopped after 100 iterations
# weights:  169
initial  value 105335.714203 
iter  10 value 60.005359
iter  20 value 53.706364
iter  30 value 52.954805
iter  40 value 52.516625
iter  50 value 51.992185
iter  60 value 51.747933
iter  70 value 51.271116
iter  80 value 51.125249
iter  90 value 46.547067
iter 100 value 45.058039
final  value 45.058039 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.397; time: 0.0 min
[Tune-x] 18: size=16; decay=1.04
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2689) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2689) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2689) weights

[Tune-y] 18: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 19: size=6; decay=0.0639
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1009) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1009) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1009) weights

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: size=4; decay=0.00306
# weights:  673
initial  value 102459.123855 
iter  10 value 66.232982
iter  20 value 24.618017
iter  30 value 17.546926
iter  40 value 15.293506
iter  50 value 12.852744
iter  60 value 11.729660
iter  70 value 10.968535
iter  80 value 10.303068
iter  90 value 9.671517
iter 100 value 9.066623
final  value 9.066623 
stopped after 100 iterations
# weights:  673
initial  value 114351.259163 
iter  10 value 63.127970
iter  20 value 28.594408
iter  30 value 23.915096
iter  40 value 21.674505
iter  50 value 19.413132
iter  60 value 18.009065
iter  70 value 16.437554
iter  80 value 15.380089
iter  90 value 14.758266
iter 100 value 13.926902
final  value 13.926902 
stopped after 100 iterations
# weights:  673
initial  value 114019.492826 
iter  10 value 64.482152
iter  20 value 37.173800
iter  30 value 31.128978
iter  40 value 27.921568
iter  50 value 26.601327
iter  60 value 25.473157
iter  70 value 23.091678
iter  80 value 21.921785
iter  90 value 20.517779
iter 100 value 19.112635
final  value 19.112635 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.221; time: 0.1 min
[Tune] Result: size=4; decay=0.00306 : rmse.test.rmse=0.221
# weights:  673
initial  value 172233.743514 
iter  10 value 405.077714
iter  20 value 218.754623
iter  30 value 134.642173
iter  40 value 105.643936
iter  50 value 78.035420
iter  60 value 69.709853
iter  70 value 68.176025
iter  80 value 64.213248
iter  90 value 59.358419
iter 100 value 55.495429
final  value 55.495429 
stopped after 100 iterations
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.nnet: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Fri Jan 12 21:54:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 538 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 284
 number of selected nodes                               : 69 
[1] "Fri Jan 12 21:54:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.pcr: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Fri Jan 12 21:54:50 2018"
Loading required package: penalized
Loading required package: survival

Attaching package: 'survival'

The following object is masked from 'package:caret':

    cluster

Welcome to penalized. For extended examples, see vignette("penalized").
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.plsr: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Fri Jan 12 21:55:28 2018"
[Tune] Started tuning learner regr.randomForest for parameter set:
            Type len Def   Constr Req Tunable Trafo
nodesize integer   -   1  1 to 10   -    TRUE     -
mtry     integer   -  55 1 to 166   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nodesize=6; mtry=160
[Tune-y] 1: rmse.test.rmse=0.156; time: 2.1 min
[Tune-x] 2: nodesize=9; mtry=48
[Tune-y] 2: rmse.test.rmse=0.148; time: 0.6 min
[Tune-x] 3: nodesize=10; mtry=137
[Tune-y] 3: rmse.test.rmse=0.154; time: 1.4 min
[Tune-x] 4: nodesize=1; mtry=75
[Tune-y] 4: rmse.test.rmse=0.148; time: 2.0 min
[Tune-x] 5: nodesize=6; mtry=149
[Tune-y] 5: rmse.test.rmse=0.155; time: 2.0 min
[Tune-x] 6: nodesize=8; mtry=121
[Tune-y] 6: rmse.test.rmse=0.152; time: 1.5 min
[Tune-x] 7: nodesize=10; mtry=31
[Tune-y] 7: rmse.test.rmse=0.149; time: 0.4 min
[Tune-x] 8: nodesize=4; mtry=74
[Tune-y] 8: rmse.test.rmse=0.147; time: 1.3 min
[Tune-x] 9: nodesize=7; mtry=160
[Tune-y] 9: rmse.test.rmse=0.157; time: 2.0 min
[Tune-x] 10: nodesize=9; mtry=9
[Tune-y] 10: rmse.test.rmse=0.158; time: 0.2 min
[Tune-x] 11: nodesize=4; mtry=11
[Tune-y] 11: rmse.test.rmse=0.154; time: 0.3 min
[Tune-x] 12: nodesize=9; mtry=72
[Tune-y] 12: rmse.test.rmse=0.149; time: 0.9 min
[Tune-x] 13: nodesize=6; mtry=54
[Tune-y] 13: rmse.test.rmse=0.148; time: 0.8 min
[Tune-x] 14: nodesize=5; mtry=134
[Tune-y] 14: rmse.test.rmse=0.153; time: 1.9 min
[Tune-x] 15: nodesize=7; mtry=112
[Tune-y] 15: rmse.test.rmse=0.152; time: 1.4 min
[Tune-x] 16: nodesize=10; mtry=137
[Tune-y] 16: rmse.test.rmse=0.154; time: 1.4 min
[Tune-x] 17: nodesize=1; mtry=52
[Tune-y] 17: rmse.test.rmse=0.148; time: 1.4 min
[Tune-x] 18: nodesize=8; mtry=139
[Tune-y] 18: rmse.test.rmse=0.154; time: 1.7 min
[Tune-x] 19: nodesize=3; mtry=106
[Tune-y] 19: rmse.test.rmse=0.15; time: 1.9 min
[Tune-x] 20: nodesize=2; mtry=69
[Tune-y] 20: rmse.test.rmse=0.148; time: 1.5 min
[Tune] Result: nodesize=4; mtry=74 : rmse.test.rmse=0.147
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.randomForest: Error in predict.randomForest(.model$learner.model, newdata = .newdata,  : 
  variables in the training data missing in newdata

[1] "Fri Jan 12 22:23:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.randomForestSRC: Error in generic.predict.rfsrc(object, newdata, outcome.target = outcome.target,  : 
  x-variables in test data do not match original training data

[1] "Fri Jan 12 22:23:27 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def   Constr Req Tunable Trafo
mtry          integer   -  55 1 to 166   -    TRUE     -
min.node.size integer   -   5  1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=99; min.node.size=10
[Tune-y] 1: rmse.test.rmse=0.15; time: 0.5 min
[Tune-x] 2: mtry=144; min.node.size=3
[Tune-y] 2: rmse.test.rmse=0.154; time: 1.0 min
[Tune-x] 3: mtry=155; min.node.size=9
[Tune-y] 3: rmse.test.rmse=0.156; time: 0.8 min
[Tune-x] 4: mtry=6; min.node.size=5
[Tune-y] 4: rmse.test.rmse=0.161; time: 0.1 min
[Tune-x] 5: mtry=86; min.node.size=9
[Tune-y] 5: rmse.test.rmse=0.149; time: 0.4 min
[Tune-x] 6: mtry=124; min.node.size=8
[Tune-y] 6: rmse.test.rmse=0.152; time: 0.6 min
[Tune-x] 7: mtry=150; min.node.size=2
[Tune-y] 7: rmse.test.rmse=0.155; time: 1.1 min
[Tune-x] 8: mtry=65; min.node.size=5
[Tune-y] 8: rmse.test.rmse=0.148; time: 0.4 min
[Tune-x] 9: mtry=104; min.node.size=10
[Tune-y] 9: rmse.test.rmse=0.151; time: 0.5 min
[Tune-x] 10: mtry=138; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.152; time: 1.1 min
[Tune-x] 11: mtry=55; min.node.size=1
[Tune-y] 11: rmse.test.rmse=0.148; time: 0.5 min
[Tune-x] 12: mtry=146; min.node.size=5
[Tune-y] 12: rmse.test.rmse=0.154; time: 0.9 min
[Tune-x] 13: mtry=87; min.node.size=4
[Tune-y] 13: rmse.test.rmse=0.148; time: 0.6 min
[Tune-x] 14: mtry=82; min.node.size=9
[Tune-y] 14: rmse.test.rmse=0.148; time: 0.4 min
[Tune-x] 15: mtry=116; min.node.size=7
[Tune-y] 15: rmse.test.rmse=0.151; time: 0.7 min
[Tune-x] 16: mtry=161; min.node.size=9
[Tune-y] 16: rmse.test.rmse=0.156; time: 0.8 min
[Tune-x] 17: mtry=1; min.node.size=4
[Tune-y] 17: rmse.test.rmse=0.258; time: 0.0 min
[Tune-x] 18: mtry=125; min.node.size=9
[Tune-y] 18: rmse.test.rmse=0.152; time: 0.6 min
[Tune-x] 19: mtry=42; min.node.size=7
[Tune-y] 19: rmse.test.rmse=0.147; time: 0.3 min
[Tune-x] 20: mtry=28; min.node.size=5
[Tune-y] 20: rmse.test.rmse=0.149; time: 0.2 min
[Tune] Result: mtry=42; min.node.size=7 : rmse.test.rmse=0.147
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.ranger: Error in `[.data.frame`(data, , forest$independent.variable.names, drop = FALSE) : 
  undefined columns selected

[1] "Fri Jan 12 22:35:23 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.rknn: Error in knn.reg(train = data[, fset], test = newdata[, fset], y = y,  : 
  too many ties in knn

[1] "Fri Jan 12 22:35:33 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.0588; maxdepth=29; minbucket=44; minsplit=18
[Tune-y] 1: rmse.test.rmse=0.259; time: 0.0 min
[Tune-x] 2: cp=0.613; maxdepth=25; minbucket=6; minsplit=25
[Tune-y] 2: rmse.test.rmse=0.401; time: 0.0 min
[Tune-x] 3: cp=0.0354; maxdepth=28; minbucket=39; minsplit=38
[Tune-y] 3: rmse.test.rmse=0.25; time: 0.0 min
[Tune-x] 4: cp=0.505; maxdepth=8; minbucket=22; minsplit=25
[Tune-y] 4: rmse.test.rmse=0.401; time: 0.0 min
[Tune-x] 5: cp=0.075; maxdepth=29; minbucket=43; minsplit=7
[Tune-y] 5: rmse.test.rmse=0.268; time: 0.0 min
[Tune-x] 6: cp=0.00965; maxdepth=4; minbucket=45; minsplit=24
[Tune-y] 6: rmse.test.rmse=0.226; time: 0.0 min
[Tune-x] 7: cp=0.0363; maxdepth=12; minbucket=27; minsplit=42
[Tune-y] 7: rmse.test.rmse=0.251; time: 0.0 min
[Tune-x] 8: cp=0.123; maxdepth=21; minbucket=49; minsplit=42
[Tune-y] 8: rmse.test.rmse=0.292; time: 0.0 min
[Tune-x] 9: cp=0.000997; maxdepth=11; minbucket=39; minsplit=43
[Tune-y] 9: rmse.test.rmse=0.221; time: 0.0 min
[Tune-x] 10: cp=0.00554; maxdepth=20; minbucket=12; minsplit=24
[Tune-y] 10: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 11: cp=0.0958; maxdepth=21; minbucket=40; minsplit=6
[Tune-y] 11: rmse.test.rmse=0.292; time: 0.0 min
[Tune-x] 12: cp=0.0976; maxdepth=20; minbucket=49; minsplit=21
[Tune-y] 12: rmse.test.rmse=0.292; time: 0.0 min
[Tune-x] 13: cp=0.201; maxdepth=16; minbucket=22; minsplit=50
[Tune-y] 13: rmse.test.rmse=0.292; time: 0.0 min
[Tune-x] 14: cp=0.23; maxdepth=21; minbucket=9; minsplit=25
[Tune-y] 14: rmse.test.rmse=0.292; time: 0.0 min
[Tune-x] 15: cp=0.012; maxdepth=20; minbucket=38; minsplit=44
[Tune-y] 15: rmse.test.rmse=0.227; time: 0.0 min
[Tune-x] 16: cp=0.0441; maxdepth=5; minbucket=8; minsplit=44
[Tune-y] 16: rmse.test.rmse=0.259; time: 0.0 min
[Tune-x] 17: cp=0.00166; maxdepth=24; minbucket=9; minsplit=10
[Tune-y] 17: rmse.test.rmse=0.204; time: 0.0 min
[Tune-x] 18: cp=0.00368; maxdepth=6; minbucket=7; minsplit=37
[Tune-y] 18: rmse.test.rmse=0.208; time: 0.0 min
[Tune-x] 19: cp=0.422; maxdepth=23; minbucket=27; minsplit=26
[Tune-y] 19: rmse.test.rmse=0.292; time: 0.0 min
[Tune-x] 20: cp=0.181; maxdepth=26; minbucket=12; minsplit=36
[Tune-y] 20: rmse.test.rmse=0.292; time: 0.0 min
[Tune] Result: cp=0.00166; maxdepth=24; minbucket=9; minsplit=10 : rmse.test.rmse=0.204
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.rpart: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Fri Jan 12 22:35:52 2018"
[Tune] Started tuning learner regr.RRF for parameter set:
           Type len Def   Constr Req Tunable Trafo
mtry    integer   -  55 1 to 166   -    TRUE     -
coefReg numeric   - 0.8   0 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=99; coefReg=0.962
[Tune-y] 1: rmse.test.rmse=0.15; time: 1.5 min
[Tune-x] 2: mtry=144; coefReg=0.288
[Tune-y] 2: rmse.test.rmse=0.158; time: 2.1 min
[Tune-x] 3: mtry=155; coefReg=0.82
[Tune-y] 3: rmse.test.rmse=0.155; time: 2.2 min
[Tune-x] 4: mtry=6; coefReg=0.451
[Tune-y] 4: rmse.test.rmse=0.161; time: 0.2 min
[Tune-x] 5: mtry=86; coefReg=0.895
[Tune-y] 5: rmse.test.rmse=0.149; time: 1.3 min
[Tune-x] 6: mtry=124; coefReg=0.725
[Tune-y] 6: rmse.test.rmse=0.152; time: 1.8 min
[Tune-x] 7: mtry=150; coefReg=0.183
[Tune-y] 7: rmse.test.rmse=0.163; time: 2.2 min
[Tune-x] 8: mtry=65; coefReg=0.444
[Tune-y] 8: rmse.test.rmse=0.147; time: 1.0 min
[Tune-x] 9: mtry=104; coefReg=0.961
[Tune-y] 9: rmse.test.rmse=0.151; time: 1.5 min
[Tune-x] 10: mtry=138; coefReg=0.052
[Tune-y] 10: rmse.test.rmse=0.17; time: 2.0 min
[Tune-x] 11: mtry=55; coefReg=0.0635
[Tune-y] 11: rmse.test.rmse=0.151; time: 0.9 min
[Tune-x] 12: mtry=146; coefReg=0.433
[Tune-y] 12: rmse.test.rmse=0.156; time: 2.1 min
[Tune-x] 13: mtry=87; coefReg=0.324
[Tune-y] 13: rmse.test.rmse=0.151; time: 1.3 min
[Tune-x] 14: mtry=82; coefReg=0.807
[Tune-y] 14: rmse.test.rmse=0.149; time: 1.2 min
[Tune-x] 15: mtry=116; coefReg=0.674
[Tune-y] 15: rmse.test.rmse=0.152; time: 1.7 min
[Tune-x] 16: mtry=161; coefReg=0.823
[Tune-y] 16: rmse.test.rmse=0.156; time: 2.3 min
[Tune-x] 17: mtry=1; coefReg=0.31
[Tune-y] 17: rmse.test.rmse=0.263; time: 0.0 min
[Tune-x] 18: mtry=125; coefReg=0.836
[Tune-y] 18: rmse.test.rmse=0.152; time: 1.8 min
[Tune-x] 19: mtry=42; coefReg=0.634
[Tune-y] 19: rmse.test.rmse=0.147; time: 0.7 min
[Tune-x] 20: mtry=28; coefReg=0.414
[Tune-y] 20: rmse.test.rmse=0.148; time: 0.5 min
[Tune] Result: mtry=42; coefReg=0.634 : rmse.test.rmse=0.147
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.RRF: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Fri Jan 12 23:04:42 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.rsm: Error in lapply(X = X, FUN = FUN, ...) : object 'V198.dummy' not found

[1] "Fri Jan 12 23:04:52 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
In addition: Warning message:
In (function (formula, data, ...)  :
  Some coefficients are aliased - cannot use 'rsm' methods.
  Returning an 'lm' object.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.rvm: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Fri Jan 12 23:06:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.8160 0.4390 0.2360 0.1270 0.0683
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 1 -----> 71 
Runtime: 40.44531 secs 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.slim: Error in newdata %*% object$beta[, lambda.idx] : 
  non-conformable arguments

[1] "Fri Jan 12 23:07:25 2018"
[Tune] Started tuning learner regr.svm for parameter set:
         Type len   Def    Constr Req Tunable Trafo
cost  numeric   -     0 -15 to 15   -    TRUE     Y
gamma numeric   - -7.38 -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cost=6.65; gamma=1.5e+04
[Tune-y] 1: rmse.test.rmse=0.401; time: 0.1 min
[Tune-x] 2: cost=1.9e+03; gamma=0.0123
[Tune-y] 2: rmse.test.rmse=0.187; time: 0.1 min
[Tune-x] 3: cost=7.53e+03; gamma=768
[Tune-y] 3: rmse.test.rmse=0.401; time: 0.1 min
[Tune-x] 4: cost=5.93e-05; gamma=0.359
[Tune-y] 4: rmse.test.rmse=0.402; time: 0.1 min
[Tune-x] 5: cost=1.45; gamma=3.67e+03
[Tune-y] 5: rmse.test.rmse=0.401; time: 0.1 min
[Tune-x] 6: cost=159; gamma=107
[Tune-y] 6: rmse.test.rmse=0.401; time: 0.1 min
[Tune-x] 7: cost=4.21e+03; gamma=0.00136
[Tune-y] 7: rmse.test.rmse=0.143; time: 0.1 min
[Tune-x] 8: cost=0.104; gamma=0.31
[Tune-y] 8: rmse.test.rmse=0.402; time: 0.1 min
[Tune-x] 9: cost=13.8; gamma=1.47e+04
[Tune-y] 9: rmse.test.rmse=0.401; time: 0.1 min
[Tune-x] 10: cost=963; gamma=8.99e-05
[Tune-y] 10: rmse.test.rmse=0.14; time: 0.1 min
[Tune-x] 11: cost=0.0295; gamma=0.000114
[Tune-y] 11: rmse.test.rmse=0.372; time: 0.1 min
[Tune-x] 12: cost=2.64e+03; gamma=0.25
[Tune-y] 12: rmse.test.rmse= 0.4; time: 0.1 min
[Tune-x] 13: cost=1.56; gamma=0.0257
[Tune-y] 13: rmse.test.rmse=0.286; time: 0.1 min
[Tune-x] 14: cost=0.857; gamma=594
[Tune-y] 14: rmse.test.rmse=0.401; time: 0.1 min
[Tune-x] 15: cost=61.6; gamma=37.5
[Tune-y] 15: rmse.test.rmse=0.401; time: 0.1 min
[Tune-x] 16: cost=1.69e+04; gamma=834
[Tune-y] 16: rmse.test.rmse=0.401; time: 0.1 min
[Tune-x] 17: cost=3.25e-05; gamma=0.0194
[Tune-y] 17: rmse.test.rmse=0.402; time: 0.1 min
[Tune-x] 18: cost=186; gamma=1.09e+03
[Tune-y] 18: rmse.test.rmse=0.401; time: 0.1 min
[Tune-x] 19: cost=0.00558; gamma=16.3
[Tune-y] 19: rmse.test.rmse=0.402; time: 0.1 min
[Tune-x] 20: cost=0.000989; gamma=0.168
[Tune-y] 20: rmse.test.rmse=0.402; time: 0.1 min
[Tune] Result: cost=963; gamma=8.99e-05 : rmse.test.rmse=0.14
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.svm: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Fri Jan 12 23:09:20 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=345; max_depth=10; eta=0.518; gamma=2.88; colsample_bytree=0.672; min_child_weight=16.4; subsample=0.274
[Tune-y] 1: rmse.test.rmse=0.214; time: 0.2 min
[Tune-x] 2: nrounds=149; max_depth=6; eta=0.537; gamma=7.44; colsample_bytree=0.59; min_child_weight=18; subsample=0.387
[Tune-y] 2: rmse.test.rmse=0.261; time: 0.1 min
[Tune-x] 3: nrounds=104; max_depth=5; eta=0.376; gamma=9.61; colsample_bytree=0.632; min_child_weight=1.04; subsample=0.498
[Tune-y] 3: rmse.test.rmse=0.253; time: 0.1 min
[Tune-x] 4: nrounds=15; max_depth=9; eta=0.261; gamma=5.22; colsample_bytree=0.43; min_child_weight=9.85; subsample=0.855
[Tune-y] 4: rmse.test.rmse=0.258; time: 0.0 min
[Tune-x] 5: nrounds=656; max_depth=7; eta=0.581; gamma=8.23; colsample_bytree=0.301; min_child_weight=6.21; subsample=0.813
[Tune-y] 5: rmse.test.rmse=0.237; time: 0.3 min
[Tune-x] 6: nrounds=1.5e+03; max_depth=3; eta=0.381; gamma=1.67; colsample_bytree=0.466; min_child_weight=13.2; subsample=0.743
[Tune-y] 6: rmse.test.rmse=0.181; time: 0.4 min
[Tune-x] 7: nrounds=998; max_depth=1; eta=0.399; gamma=6.18; colsample_bytree=0.69; min_child_weight=7.11; subsample=0.826
[Tune-y] 7: rmse.test.rmse=0.225; time: 0.1 min
[Tune-x] 8: nrounds=171; max_depth=4; eta=0.59; gamma=7.88; colsample_bytree=0.562; min_child_weight=2.08; subsample=0.581
[Tune-y] 8: rmse.test.rmse=0.247; time: 0.1 min
[Tune-x] 9: nrounds=87; max_depth=7; eta=0.436; gamma=8.61; colsample_bytree=0.52; min_child_weight=1.93; subsample=0.305
[Tune-y] 9: rmse.test.rmse=0.295; time: 0.1 min
[Tune-x] 10: nrounds=1.65e+03; max_depth=1; eta=0.468; gamma=1.05; colsample_bytree=0.351; min_child_weight=3.83; subsample=0.346
[Tune-y] 10: rmse.test.rmse=0.181; time: 0.2 min
[Tune-x] 11: nrounds=15; max_depth=8; eta=0.525; gamma=7.19; colsample_bytree=0.493; min_child_weight=9.19; subsample=0.815
[Tune-y] 11: rmse.test.rmse=0.24; time: 0.0 min
[Tune-x] 12: nrounds=1.42e+03; max_depth=2; eta=0.415; gamma=8.54; colsample_bytree=0.541; min_child_weight=4.31; subsample=0.509
[Tune-y] 12: rmse.test.rmse=0.246; time: 0.3 min
[Tune-x] 13: nrounds=44; max_depth=7; eta=0.546; gamma=9.12; colsample_bytree=0.435; min_child_weight=5.86; subsample=0.307
[Tune-y] 13: rmse.test.rmse=0.279; time: 0.0 min
[Tune-x] 14: nrounds=2.99e+03; max_depth=8; eta=0.179; gamma=0.38; colsample_bytree=0.415; min_child_weight=0.0431; subsample=0.486
[Tune-y] 14: rmse.test.rmse=0.153; time: 2.1 min
[Tune-x] 15: nrounds=24; max_depth=8; eta=0.232; gamma=6.5; colsample_bytree=0.645; min_child_weight=16.8; subsample=0.368
[Tune-y] 15: rmse.test.rmse=0.265; time: 0.0 min
[Tune-x] 16: nrounds=2.58e+03; max_depth=9; eta=0.00683; gamma=8.88; colsample_bytree=0.652; min_child_weight=15.4; subsample=0.283
[Tune-y] 16: rmse.test.rmse=0.304; time: 1.2 min
[Tune-x] 17: nrounds=17; max_depth=8; eta=0.451; gamma=3.79; colsample_bytree=0.494; min_child_weight=13.2; subsample=0.816
[Tune-y] 17: rmse.test.rmse=0.213; time: 0.0 min
[Tune-x] 18: nrounds=59; max_depth=6; eta=0.289; gamma=7.01; colsample_bytree=0.354; min_child_weight=17.1; subsample=0.797
[Tune-y] 18: rmse.test.rmse=0.232; time: 0.0 min
[Tune-x] 19: nrounds=185; max_depth=10; eta=0.51; gamma=8.58; colsample_bytree=0.59; min_child_weight=6.89; subsample=0.969
[Tune-y] 19: rmse.test.rmse=0.24; time: 0.2 min
[Tune-x] 20: nrounds=2.3e+03; max_depth=9; eta=0.354; gamma=5.26; colsample_bytree=0.38; min_child_weight=4.22; subsample=0.653
[Tune-y] 20: rmse.test.rmse=0.22; time: 1.7 min
[Tune] Result: nrounds=2.99e+03; max_depth=8; eta=0.179; gamma=0.38; colsample_bytree=0.415; min_child_weight=0.0431; subsample=0.486 : rmse.test.rmse=0.153
[1] "Fri Jan 12 23:17:19 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
In addition: There were 20 warnings (use warnings() to see them)
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Jan 12 23:17:30 2018"
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  Expo. transform induced infinite values in several predictors and is ommitted: V175, V183, V184, V204, V237, V273
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  Expo. transform induced infinite values in several predictors and is ommitted: V175, V183, V184, V204, V237, V273
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [40,0], n=(713,384)
**GROW** @depth 1: [101,0], n=(468,245)
**GROW** @depth 2: [75,0.170213], n=(233,235)
r=1000 d=[0] [0] [0] [0]; n=(234,234,338,291)
r=2000 d=[0] [0] [0] [0]; n=(252,216,338,291)

Sampling @ nn=0 pred locs:
r=1000 d=[0] [0] [0] [0]; mh=4 n=(252,216,338,291)
r=2000 d=[0] [0] [0] [0]; mh=4 n=(252,216,338,291)
r=3000 d=[0] [0] [0] [0]; mh=4 n=(256,212,338,291)
r=4000 d=[0] [0] [0] [0]; mh=4 n=(261,207,338,291)
r=5000 d=[0] [0] [0] [0]; mh=4 n=(269,199,338,291)
Grow: 0.8021%, Prune: 0%, Change: 18.59%, Swap: 0.2907%

Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.bcart: Error in predict.tgp(.model$learner.model, XX = .newdata, pred.n = FALSE,  : 
  mismatched column dimension of object$X and XX

[1] "Fri Jan 12 23:21:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Jan 12 23:21:33 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=1097

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=1097
r=2000 d=[0]; mh=1 n=1097
r=3000 d=[0]; mh=1 n=1097

Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.blm: Error in predict.tgp(.model$learner.model, XX = .newdata, pred.n = FALSE,  : 
  mismatched column dimension of object$X and XX

[1] "Fri Jan 12 23:37:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 332 
Nguyen-Widrow method
Scaling factor= 0.7004424 
gamma= 202.5502 	 alpha= 45.8813 	 beta= 111.675 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.brnn: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Fri Jan 12 23:38:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Jan 12 23:38:16 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=1097
r=2000 d=[0]; n=1097

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=1097
r=2000 d=[0]; mh=1 n=1097
r=3000 d=[0]; mh=1 n=1097
r=4000 d=[0]; mh=1 n=1097
r=5000 d=[0]; mh=1 n=1097
Grow: 0%, 

Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.btlm: Error in predict.tgp(.model$learner.model, XX = .newdata, pred.n = FALSE,  : 
  mismatched column dimension of object$X and XX

[1] "Sat Jan 13 00:00:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cforest no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.cforest: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 00:00:28 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.ctree: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 00:00:42 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
cubist code called exit with value 1
Warning in train(allmodel, regr.task) :
  Could not train learner regr.cubist: Error in strsplit(tmp, "\"")[[1]] : subscript out of bounds

[1] "Sat Jan 13 00:00:52 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.cvglmnet: Error in cbind2(1, newx) %*% nbeta : 
  Cholmod error 'X and/or Y have wrong dimensions' at file ../MatrixOps/cholmod_sdmult.c, line 90

[1] "Sat Jan 13 00:01:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Sat Jan 13 00:01:12 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
Loading required package: MASS
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.elmNN: Error in inpweight %*% TV.P : non-conformable arguments

[1] "Sat Jan 13 00:01:20 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
[Tune] Started tuning learner regr.extraTrees for parameter set:
                 Type len Def   Constr Req Tunable Trafo
mtry          integer   -  54 1 to 164   -    TRUE     -
numRandomCuts integer   -   1  1 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=117; numRandomCuts=14
[Tune-y] 1: rmse.test.rmse=0.151; time: 8.2 min
[Tune-x] 2: mtry=128; numRandomCuts=17
[Tune-y] 2: rmse.test.rmse=0.154; time: 10.4 min
[Tune-x] 3: mtry=17; numRandomCuts=25
[Tune-y] 3: rmse.test.rmse=0.142; time: 2.5 min
[Tune-x] 4: mtry=125; numRandomCuts=13
[Tune-y] 4: rmse.test.rmse=0.154; time: 8.9 min
[Tune-x] 5: mtry=30; numRandomCuts=8
[Tune-y] 5: rmse.test.rmse=0.142; time: 1.6 min
[Tune-x] 6: mtry=117; numRandomCuts=24
[Tune-y] 6: rmse.test.rmse=0.153; time: 13.8 min
[Tune-x] 7: mtry=154; numRandomCuts=6
[Tune-y] 7: rmse.test.rmse=0.154; time: 4.4 min
[Tune-x] 8: mtry=121; numRandomCuts=15
[Tune-y] 8: rmse.test.rmse=0.153; time: 9.1 min
[Tune-x] 9: mtry=30; numRandomCuts=23
[Tune-y] 9: rmse.test.rmse=0.141; time: 4.0 min
[Tune-x] 10: mtry=136; numRandomCuts=6
[Tune-y] 10: rmse.test.rmse=0.153; time: 4.3 min
[Tune-x] 11: mtry=12; numRandomCuts=5
Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: GC overhead limit exceeded

[Tune-y] 11: rmse.test.rmse=  NA; time: 1.3 min
[Tune-x] 12: mtry=113; numRandomCuts=14
Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: GC overhead limit exceeded

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: GC overhead limit exceeded

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: GC overhead limit exceeded

[Tune-y] 12: rmse.test.rmse=  NA; time: 14.7 min
[Tune-x] 13: mtry=83; numRandomCuts=18
Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: GC overhead limit exceeded

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: GC overhead limit exceeded

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: GC overhead limit exceeded

[Tune-y] 13: rmse.test.rmse=  NA; time: 12.7 min
[Tune-x] 14: mtry=87; numRandomCuts=11
[Tune-y] 14: rmse.test.rmse=0.147; time: 5.6 min
[Tune-x] 15: mtry=29; numRandomCuts=24
[Tune-y] 15: rmse.test.rmse=0.141; time: 4.0 min
[Tune-x] 16: mtry=5; numRandomCuts=4
[Tune-y] 16: rmse.test.rmse=0.154; time: 0.2 min
[Tune-x] 17: mtry=52; numRandomCuts=15
[Tune-y] 17: rmse.test.rmse=0.143; time: 4.7 min
[Tune-x] 18: mtry=21; numRandomCuts=9
[Tune-y] 18: rmse.test.rmse=0.141; time: 1.3 min
[Tune-x] 19: mtry=6; numRandomCuts=1
Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: Java heap space

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: Java heap space

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: Java heap space

[Tune-y] 19: rmse.test.rmse=  NA; time: 14.0 min
[Tune-x] 20: mtry=19; numRandomCuts=25
Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: Java heap space

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: Java heap space

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: Java heap space

[Tune-y] 20: rmse.test.rmse=  NA; time: 73.4 min
[Tune] Result: mtry=30; numRandomCuts=23 : rmse.test.rmse=0.141
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.extraTrees: Error in predict.extraTrees(.model$learner.model, as.matrix(.newdata),  : 
  newdata(ncol=162) does not have the same dimensions as the original x (ncol=164)

[1] "Sat Jan 13 04:08:04 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Sat Jan 13 04:08:17 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.fnn: Error in get.knnx(train, test, k, algorithm) : 
  Number of columns must be same!.

[1] "Sat Jan 13 04:08:27 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.frbs: Error in rep(1:num.varinput, num.labels.input) : invalid 'times' argument

[1] "Sat Jan 13 04:09:46 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
In addition: Warning messages:
1: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
2: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
3: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
4: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
5: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.gausspr: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 04:10:03 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=267; interaction.depth=6; shrinkage=0.468; n.minobsinnode=18
[Tune-y] 1: rmse.test.rmse=0.18; time: 0.3 min
[Tune-x] 2: n.trees=16; interaction.depth=10; shrinkage=0.455; n.minobsinnode=15
[Tune-y] 2: rmse.test.rmse=0.159; time: 0.0 min
[Tune-x] 3: n.trees=23; interaction.depth=3; shrinkage=0.426; n.minobsinnode=24
[Tune-y] 3: rmse.test.rmse=0.157; time: 0.0 min
[Tune-x] 4: n.trees=738; interaction.depth=3; shrinkage=0.439; n.minobsinnode=17
[Tune-y] 4: rmse.test.rmse=0.163; time: 0.4 min
[Tune-x] 5: n.trees=23; interaction.depth=10; shrinkage=0.495; n.minobsinnode=9
[Tune-y] 5: rmse.test.rmse=0.169; time: 0.1 min
[Tune-x] 6: n.trees=14; interaction.depth=2; shrinkage=0.413; n.minobsinnode=16
[Tune-y] 6: rmse.test.rmse=0.163; time: 0.0 min
[Tune-x] 7: n.trees=102; interaction.depth=8; shrinkage=0.318; n.minobsinnode=13
[Tune-y] 7: rmse.test.rmse=0.154; time: 0.1 min
[Tune-x] 8: n.trees=22; interaction.depth=10; shrinkage=0.0189; n.minobsinnode=7
[Tune-y] 8: rmse.test.rmse=0.298; time: 0.0 min
[Tune-x] 9: n.trees=42; interaction.depth=6; shrinkage=0.0768; n.minobsinnode=12
[Tune-y] 9: rmse.test.rmse=0.149; time: 0.1 min
[Tune-x] 10: n.trees=12; interaction.depth=1; shrinkage=0.0671; n.minobsinnode=25
[Tune-y] 10: rmse.test.rmse=0.301; time: 0.0 min
[Tune-x] 11: n.trees=106; interaction.depth=2; shrinkage=0.297; n.minobsinnode=12
[Tune-y] 11: rmse.test.rmse=0.143; time: 0.1 min
[Tune-x] 12: n.trees=135; interaction.depth=2; shrinkage=0.537; n.minobsinnode=17
[Tune-y] 12: rmse.test.rmse=0.158; time: 0.1 min
[Tune-x] 13: n.trees=22; interaction.depth=1; shrinkage=0.566; n.minobsinnode=19
[Tune-y] 13: rmse.test.rmse=0.172; time: 0.0 min
[Tune-x] 14: n.trees=422; interaction.depth=7; shrinkage=0.27; n.minobsinnode=11
[Tune-y] 14: rmse.test.rmse=0.156; time: 0.5 min
[Tune-x] 15: n.trees=78; interaction.depth=1; shrinkage=0.373; n.minobsinnode=24
[Tune-y] 15: rmse.test.rmse=0.148; time: 0.0 min
[Tune-x] 16: n.trees=16; interaction.depth=3; shrinkage=0.357; n.minobsinnode=14
[Tune-y] 16: rmse.test.rmse=0.155; time: 0.0 min
[Tune-x] 17: n.trees=377; interaction.depth=1; shrinkage=0.192; n.minobsinnode=22
[Tune-y] 17: rmse.test.rmse=0.138; time: 0.1 min
[Tune-x] 18: n.trees=189; interaction.depth=2; shrinkage=0.328; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=0.146; time: 0.1 min
[Tune-x] 19: n.trees=64; interaction.depth=7; shrinkage=0.321; n.minobsinnode=22
[Tune-y] 19: rmse.test.rmse=0.153; time: 0.1 min
[Tune-x] 20: n.trees=170; interaction.depth=6; shrinkage=0.0786; n.minobsinnode=23
[Tune-y] 20: rmse.test.rmse=0.135; time: 0.2 min
[Tune] Result: n.trees=170; interaction.depth=6; shrinkage=0.0786; n.minobsinnode=23 : rmse.test.rmse=0.135
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.gbm: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 04:12:27 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.glm: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 04:12:35 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.713; lambda=0.107
[Tune-y] 1: rmse.test.rmse=0.206; time: 0.0 min
[Tune-x] 2: alpha=0.78; lambda=0.395
[Tune-y] 2: rmse.test.rmse=0.386; time: 0.0 min
[Tune-x] 3: alpha=0.1; lambda=7.69
[Tune-y] 3: rmse.test.rmse=0.393; time: 0.0 min
[Tune-x] 4: alpha=0.758; lambda=0.0754
[Tune-y] 4: rmse.test.rmse=0.19; time: 0.0 min
[Tune-x] 5: alpha=0.183; lambda=0.0127
[Tune-y] 5: rmse.test.rmse=0.151; time: 0.0 min
[Tune-x] 6: alpha=0.709; lambda=3.98
[Tune-y] 6: rmse.test.rmse=0.393; time: 0.0 min
[Tune-x] 7: alpha=0.934; lambda=0.00633
[Tune-y] 7: rmse.test.rmse=0.151; time: 0.0 min
[Tune-x] 8: alpha=0.732; lambda=0.186
[Tune-y] 8: rmse.test.rmse=0.259; time: 0.0 min
[Tune-x] 9: alpha=0.182; lambda=3.62
[Tune-y] 9: rmse.test.rmse=0.393; time: 0.0 min
[Tune-x] 10: alpha=0.825; lambda=0.00658
[Tune-y] 10: rmse.test.rmse=0.151; time: 0.0 min
[Tune-x] 11: alpha=0.0691; lambda=0.00414
[Tune-y] 11: rmse.test.rmse=2.87e+43; time: 0.0 min
[Tune-x] 12: alpha=0.688; lambda=0.116
[Tune-y] 12: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 13: alpha=0.503; lambda=0.614
[Tune-y] 13: rmse.test.rmse=0.388; time: 0.0 min
[Tune-x] 14: alpha=0.53; lambda=0.0409
[Tune-y] 14: rmse.test.rmse=0.161; time: 0.0 min
[Tune-x] 15: alpha=0.171; lambda=4.82
[Tune-y] 15: rmse.test.rmse=0.393; time: 0.0 min
[Tune-x] 16: alpha=0.0299; lambda=0.00344
[Tune-y] 16: rmse.test.rmse=3.63e+43; time: 0.0 min
[Tune-x] 17: alpha=0.312; lambda=0.211
[Tune-y] 17: rmse.test.rmse=0.205; time: 0.0 min
[Tune-x] 18: alpha=0.127; lambda=0.0214
[Tune-y] 18: rmse.test.rmse=0.15; time: 0.0 min
[Tune-x] 19: alpha=0.0311; lambda=0.000979
[Tune-y] 19: rmse.test.rmse=2.27e+43; time: 0.0 min
[Tune-x] 20: alpha=0.11; lambda=6.83
[Tune-y] 20: rmse.test.rmse=0.393; time: 0.0 min
[Tune] Result: alpha=0.127; lambda=0.0214 : rmse.test.rmse=0.15
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.glmnet: Error in cbind2(1, newx) %*% nbeta : 
  Cholmod error 'X and/or Y have wrong dimensions' at file ../MatrixOps/cholmod_sdmult.c, line 90

[1] "Sat Jan 13 04:12:53 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    C:\Users\gvg\AppData\Local\Temp\RtmpKeh50C/h2o_gvg_started_from_r.out
    C:\Users\gvg\AppData\Local\Temp\RtmpKeh50C/h2o_gvg_started_from_r.err

java version "1.8.0_25"
Java(TM) SE Runtime Environment (build 1.8.0_25-b18)
Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)

Starting H2O JVM and connecting: ..... Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         10 seconds 57 milliseconds 
    H2O cluster version:        3.16.0.2 
    H2O cluster version age:    1 month and 13 days  
    H2O cluster name:           H2O_started_from_R_gvg_jnl453 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   0.77 GB 
    H2O cluster total cores:    4 
    H2O cluster allowed cores:  4 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    H2O API Extensions:         Algos, AutoML, Core V3, Core V4 
    R Version:                  R version 3.4.3 (2017-11-30) 

  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==============                                                        |  20%  |                                                                              |============================                                          |  40%  |                                                                              |==========================================                            |  60%  |                                                                              |========================================================              |  80%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 04:13:48 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning messages:
1: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V198.dummy': substituting in a column of NaN
2: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V202.dummy': substituting in a column of NaN
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |========                                                              |  12%  |                                                                              |====================================                                  |  52%  |                                                                              |===================================================================== |  98%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 04:14:06 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning messages:
1: In .h2o.startModelJob(algo, params, h2oRestApiVersion) :
  Dropping columns with too large numeric values: [V257, V256, V2, V242, V141].

2: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V198.dummy': substituting in a column of NaN
3: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V202.dummy': substituting in a column of NaN
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 04:14:19 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning messages:
1: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V198.dummy': substituting in a column of NaN
2: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V202.dummy': substituting in a column of NaN
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======                                                                |   8%  |                                                                              |===============                                                       |  22%  |                                                                              |===========================                                           |  38%  |                                                                              |======================================                                |  54%  |                                                                              |==================================================                    |  72%  |                                                                              |==============================================================        |  88%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 04:14:39 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
In addition: Warning messages:
1: In .h2o.startModelJob(algo, params, h2oRestApiVersion) :
  Dropping columns with too large numeric values: [V257, V256, V2, V242, V141].

2: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V198.dummy': substituting in a column of NaN
3: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V202.dummy': substituting in a column of NaN
Loading required package: kknn

Attaching package: 'kknn'

The following object is masked from 'package:caret':

    contr.dummy

Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  5.1582e+167 340 2 2 427890 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 13.72952 7.993672 276 120 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1.639458e+156 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 11288 2 4672 12220 2 2 2 2 2 2 2 8640 4130 10244 1.765722 0.4998323 5.708863 1.240471 13.27343 2 2 2 8.161928 2 2.933966 2 2 2 2 2 4020 2 2 2 10.84527 1.49385e+178 2 2 1.703617e+214 6.369043e+214 2 2 2 2 2 2 2 18.01447 8 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 
  - best initial criterion value(s) :  -489.0925 

N = 164, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       489.09  |proj g|=       3.5187
At iterate     1  f =       353.25  |proj g|=        2.6495
ys=-2.701e+01  -gs= 8.686e+01, BFGS update SKIPPED
At iterate     2  f =       314.03  |proj g|=        1.9777
At iterate     3  f =       304.89  |proj g|=        1.9905
At iterate     4  f =       299.71  |proj g|=        1.9978
At iterate     5  f =       292.67  |proj g|=        2.0078
ys=-6.861e-01  -gs= 6.228e+00, BFGS update SKIPPED
At iterate     6  f =       284.58  |proj g|=        2.0854
At iterate     7  f =       279.23  |proj g|=        2.0855
ys=-4.721e-02  -gs= 5.260e+00, BFGS update SKIPPED
At iterate     8  f =        277.2  |proj g|=        2.0892
ys=-6.486e-02  -gs= 1.978e+00, BFGS update SKIPPED
At iterate     9  f =       268.92  |proj g|=        2.0876
At iterate    10  f =       254.67  |proj g|=        1.9979
At iterate    11  f =       251.46  |proj g|=        1.8663
At iterate    12  f =       239.68  |proj g|=         1.175
At iterate    13  f =        232.5  |proj g|=       0.75978
At iterate    14  f =       225.69  |proj g|=        1.5558
At iterate    15  f =       212.37  |proj g|=       0.87416
At iterate    16  f =       208.19  |proj g|=       0.38962
At iterate    17  f =       205.76  |proj g|=       0.36439
At iterate    18  f =       204.57  |proj g|=       0.30493
At iterate    19  f =       200.85  |proj g|=       0.26143
At iterate    20  f =       181.55  |proj g|=       0.47139
ys=-4.518e+00  -gs= 4.299e+00, BFGS update SKIPPED
At iterate    21  f =       179.15  |proj g|=      0.059072
At iterate    22  f =       178.56  |proj g|=      0.058893
At iterate    23  f =        176.4  |proj g|=       0.80498
At iterate    24  f =        173.5  |proj g|=       0.83923
At iterate    25  f =        171.5  |proj g|=       0.38234
At iterate    26  f =       170.65  |proj g|=         0.092
At iterate    27  f =        170.3  |proj g|=        0.0899
At iterate    28  f =       170.03  |proj g|=      0.054461
At iterate    29  f =       169.97  |proj g|=      0.054375
At iterate    30  f =       169.62  |proj g|=       0.29871
At iterate    31  f =       164.28  |proj g|=       0.34656
At iterate    32  f =        157.4  |proj g|=      0.030485
At iterate    33  f =       157.27  |proj g|=      0.070287
At iterate    34  f =       157.07  |proj g|=        0.2217
At iterate    35  f =       156.53  |proj g|=       0.45991
At iterate    36  f =        154.5  |proj g|=        1.0201
At iterate    37  f =        149.2  |proj g|=       0.76501
At iterate    38  f =       144.64  |proj g|=       0.26805
At iterate    39  f =       141.85  |proj g|=      0.062767
At iterate    40  f =       139.52  |proj g|=      0.067712
At iterate    41  f =       114.97  |proj g|=       0.89958
At iterate    42  f =        111.9  |proj g|=       0.60477
At iterate    43  f =       109.05  |proj g|=       0.34813
At iterate    44  f =       108.54  |proj g|=       0.34358
At iterate    45  f =       107.86  |proj g|=      0.075535
At iterate    46  f =       107.85  |proj g|=       0.07541
At iterate    47  f =       107.62  |proj g|=       0.21063
At iterate    48  f =       107.55  |proj g|=      0.073345
At iterate    49  f =       107.53  |proj g|=       0.14526
At iterate    50  f =       107.42  |proj g|=       0.11422
At iterate    51  f =       106.81  |proj g|=       0.25792
At iterate    52  f =       106.37  |proj g|=        0.3046
At iterate    53  f =        106.2  |proj g|=       0.28543
At iterate    54  f =       105.96  |proj g|=       0.22591
At iterate    55  f =       105.78  |proj g|=      0.048889
At iterate    56  f =       105.74  |proj g|=      0.057031
At iterate    57  f =       105.74  |proj g|=      0.019537
At iterate    58  f =       105.74  |proj g|=     0.0078903
At iterate    59  f =       105.74  |proj g|=     0.0062617
At iterate    60  f =       105.74  |proj g|=      0.012688
At iterate    61  f =       105.74  |proj g|=       0.01008
At iterate    62  f =       105.73  |proj g|=     0.0066104
At iterate    63  f =       105.73  |proj g|=       0.00591
At iterate    64  f =       105.73  |proj g|=     0.0090357
At iterate    65  f =       105.73  |proj g|=     0.0085808
At iterate    66  f =       105.73  |proj g|=     0.0037061
At iterate    67  f =       105.73  |proj g|=     0.0018289
At iterate    68  f =       105.73  |proj g|=     0.0053193
At iterate    69  f =       105.73  |proj g|=      0.011378
At iterate    70  f =       105.73  |proj g|=      0.021339
At iterate    71  f =       105.73  |proj g|=      0.034863
At iterate    72  f =       105.73  |proj g|=      0.042387
At iterate    73  f =       105.73  |proj g|=       0.02588
At iterate    74  f =       105.73  |proj g|=     0.0030606
At iterate    75  f =       105.73  |proj g|=     0.0031041
At iterate    76  f =       105.73  |proj g|=     0.0081617
At iterate    77  f =       105.72  |proj g|=     0.0098142
At iterate    78  f =       105.72  |proj g|=      0.007609
At iterate    79  f =       105.72  |proj g|=       0.01071
At iterate    80  f =       105.72  |proj g|=       0.01218
At iterate    81  f =       105.72  |proj g|=      0.016427
At iterate    82  f =       105.72  |proj g|=      0.025203
At iterate    83  f =       105.72  |proj g|=      0.023578
At iterate    84  f =       105.72  |proj g|=      0.010782
At iterate    85  f =       105.72  |proj g|=     0.0072436
At iterate    86  f =       105.72  |proj g|=     0.0095652
At iterate    87  f =       105.72  |proj g|=       0.01639
At iterate    88  f =       105.72  |proj g|=       0.02165
At iterate    89  f =       105.71  |proj g|=      0.020563
At iterate    90  f =       105.71  |proj g|=      0.018519
At iterate    91  f =       105.71  |proj g|=      0.015231
At iterate    92  f =       105.71  |proj g|=     0.0093539
At iterate    93  f =       105.71  |proj g|=     0.0057146
At iterate    94  f =        105.7  |proj g|=     0.0048574
At iterate    95  f =        105.7  |proj g|=     0.0047296
At iterate    96  f =        105.7  |proj g|=      0.010561
At iterate    97  f =       105.54  |proj g|=        1.0237
At iterate    98  f =       105.51  |proj g|=        1.2684
At iterate    99  f =       105.35  |proj g|=        1.2202
At iterate   100  f =       104.96  |proj g|=        0.9062
At iterate   101  f =       104.05  |proj g|=      0.056697
final  value 104.045796 
stopped after 101 iterations
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.km: Error in checkNames(X1 = X, X2 = newdata, X1.name = "the design", X2.name = "newdata") : 
  the design and newdata must have the same numbers of columns

[1] "Sat Jan 13 06:17:02 2018"
[Tune] Started tuning learner regr.ksvm for parameter set:
         Type len  Def    Constr Req Tunable Trafo
C     numeric   -    0  -5 to 10   -    TRUE     Y
sigma numeric   - TRUE -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: C=0.273; sigma=0.0148
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 1: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 2: C=77.9; sigma=0.0018
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 2: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 3: C=0.18; sigma=0.0237
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: C=0.0585; sigma=0.0082
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: C=0.146; sigma=807
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: C=111; sigma=485
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: C=7.74; sigma=0.0128
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 7: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 8: C=0.13; sigma=0.000408
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 8: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 9: C=0.146; sigma=2.1
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: C=403; sigma=0.000359
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: C=0.0601; sigma=0.0126
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 11: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 12: C=263; sigma=0.0649
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: C=1.1; sigma=0.201
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: C=0.0959; sigma=0.000142
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 14: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 15: C=4.91; sigma=1.3e+03
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: C=992; sigma=0.000131
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 16: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 17: C=0.416; sigma=12.3
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 17: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 18: C=22.9; sigma=8.93
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 18: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 19: C=18.6; sigma=0.286
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: C=0.0346; sigma=1.06e+03
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.ksvm: Error in .local(x, ...) : 
  No Support Vectors found. You may want to change your parameters

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: C=111; sigma=485 : rmse.test.rmse=  NA
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.ksvm: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 06:17:36 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: There were 40 warnings (use warnings() to see them)
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.laGP: Error in (function (X, Z, XX, start = 6, end = 50, d = NULL, g = 1/10000,  : 
  mismatch XX and X cols

[1] "Sat Jan 13 06:17:45 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.LiblineaRL2L1SVR: Error in predict.LiblineaR(.model$learner.model, newx = .newdata, ...) : 
  columns of 'test' and 'train' differ

[1] "Sat Jan 13 06:17:56 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.lm: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 06:18:04 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Sat Jan 13 06:18:48 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning message:
In storage.mode(tagx) <- "integer" :
  NAs introduced by coercion to integer range
Error in eigen(X, symmetric = TRUE) : infinite or missing values in 'x'
Warning in train(allmodel, regr.task) :
  Could not train learner regr.mob: Error in trainLearner.regr.mob(.learner = structure(list(id = "regr.mob",  : 
  Failed to fit party::mob. Some coefficients are estimated as NA

[1] "Sat Jan 13 06:18:58 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=15; decay=0.0133
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2491) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2491) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2491) weights

[Tune-y] 1: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 2: size=16; decay=0.0994
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2657) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2657) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2657) weights

[Tune-y] 2: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 3: size=3; decay=9.4
# weights:  499
initial  value 116135.320558 
iter  10 value 1070.158416
iter  20 value 910.925929
iter  20 value 910.925927
iter  30 value 910.726138
final  value 910.725873 
converged
# weights:  499
initial  value 117243.589399 
iter  10 value 1276.387716
iter  20 value 1268.973415
iter  30 value 1268.905057
iter  40 value 1268.809881
final  value 1268.807422 
converged
# weights:  499
initial  value 122078.734976 
iter  10 value 2048.265652
iter  20 value 1096.091125
iter  30 value 1031.203959
iter  40 value 1030.208119
iter  40 value 1030.208118
iter  50 value 1030.179434
iter  60 value 1029.887284
iter  70 value 1029.693139
iter  80 value 1029.690701
iter  80 value 1029.690700
final  value 1029.690700 
converged
[Tune-y] 3: rmse.test.rmse=0.578; time: 0.1 min
[Tune-x] 4: size=16; decay=0.00784
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2657) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2657) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2657) weights

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: size=4; decay=0.00051
# weights:  665
initial  value 104411.856302 
iter  10 value 98.529076
iter  20 value 98.476079
iter  30 value 98.475814
iter  30 value 98.475814
final  value 98.475814 
converged
# weights:  665
initial  value 110686.853617 
iter  10 value 111.871059
iter  20 value 111.781936
iter  30 value 111.774968
final  value 111.774888 
converged
# weights:  665
initial  value 97373.002717 
iter  10 value 109.172668
iter  20 value 109.117723
iter  30 value 109.114657
final  value 109.114622 
converged
[Tune-y] 5: rmse.test.rmse=0.382; time: 0.0 min
[Tune-x] 6: size=15; decay=3.43
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2491) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2491) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2491) weights

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: size=19; decay=0.000176
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3155) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3155) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3155) weights

[Tune-y] 7: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 8: size=15; decay=0.0312
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2491) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2491) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2491) weights

[Tune-y] 8: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 9: size=4; decay=2.97
# weights:  665
initial  value 110319.687618 
iter  10 value 633.576799
iter  20 value 492.043298
iter  30 value 491.130396
iter  40 value 367.434859
iter  50 value 321.240919
iter  60 value 321.239271
iter  70 value 319.458894
iter  80 value 318.776521
iter  90 value 314.970039
iter 100 value 314.965978
final  value 314.965978 
stopped after 100 iterations
# weights:  665
initial  value 111455.802166 
iter  10 value 501.338158
iter  20 value 492.021108
final  value 492.021010 
converged
# weights:  665
initial  value 118413.278082 
iter  10 value 439.324191
iter  20 value 377.339501
iter  30 value 328.715148
iter  40 value 326.055514
iter  50 value 326.048112
final  value 326.048006 
converged
[Tune-y] 9: rmse.test.rmse=0.418; time: 0.1 min
[Tune-x] 10: size=17; decay=0.000186
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2823) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2823) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2823) weights

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: size=2; decay=9.16e-05
# weights:  333
initial  value 106010.956035 
iter  10 value 103.199604
iter  20 value 103.194325
iter  20 value 103.194325
iter  20 value 103.194325
final  value 103.194325 
converged
# weights:  333
initial  value 111804.368386 
iter  10 value 112.048170
iter  20 value 112.046941
final  value 112.046938 
converged
# weights:  333
initial  value 122094.410033 
iter  10 value 108.910787
iter  20 value 108.906295
final  value 108.906190 
converged
[Tune-y] 11: rmse.test.rmse=0.389; time: 0.0 min
[Tune-x] 12: size=14; decay=0.0151
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2325) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2325) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2325) weights

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: size=11; decay=0.195
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1827) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1827) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1827) weights

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: size=11; decay=0.00307
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1827) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1827) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1827) weights

[Tune-y] 14: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 15: size=4; decay=4.6
# weights:  665
initial  value 93365.275776 
iter  10 value 697.562425
iter  20 value 560.907098
iter  30 value 551.006125
final  value 551.006052 
converged
# weights:  665
initial  value 101028.233500 
iter  10 value 518.024615
final  value 518.024546 
converged
# weights:  665
initial  value 123063.504085 
iter  10 value 803.561888
iter  20 value 617.119059
iter  30 value 610.592248
iter  40 value 610.204150
iter  50 value 609.565260
final  value 609.556353 
converged
[Tune-y] 15: rmse.test.rmse=0.452; time: 0.0 min
[Tune-x] 16: size=1; decay=6.89e-05
# weights:  167
initial  value 101718.352651 
final  value 98.482646 
converged
# weights:  167
initial  value 113484.858912 
final  value 118.471199 
converged
# weights:  167
initial  value 117478.118202 
final  value 109.266301 
converged
[Tune-y] 16: rmse.test.rmse=0.388; time: 0.0 min
[Tune-x] 17: size=7; decay=0.0379
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1163) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1163) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1163) weights

[Tune-y] 17: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 18: size=3; decay=0.00114
# weights:  499
initial  value 121195.101786 
iter  10 value 98.997712
iter  20 value 98.914985
final  value 98.914970 
converged
# weights:  499
initial  value 107615.886916 
iter  10 value 112.795725
iter  20 value 112.717989
iter  30 value 112.717286
final  value 112.717278 
converged
# weights:  499
initial  value 95782.574757 
iter  10 value 108.674519
iter  20 value 108.635807
iter  30 value 108.635530
final  value 108.635528 
converged
[Tune-y] 18: rmse.test.rmse=0.384; time: 0.0 min
[Tune-x] 19: size=1; decay=1e-05
# weights:  167
initial  value 105007.716494 
final  value 99.716876 
converged
# weights:  167
initial  value 85615.135437 
final  value 118.314889 
converged
# weights:  167
initial  value 117742.325504 
final  value 116.936008 
converged
[Tune-y] 19: rmse.test.rmse=0.388; time: 0.0 min
[Tune-x] 20: size=3; decay=7.84
# weights:  499
initial  value 122162.428389 
iter  10 value 901.945403
iter  20 value 884.686313
iter  30 value 884.515713
iter  40 value 613.259687
iter  50 value 542.592367
iter  60 value 478.377525
iter  70 value 474.470820
iter  80 value 474.056046
iter  90 value 473.724340
final  value 473.595358 
converged
# weights:  499
initial  value 114803.747315 
iter  10 value 1227.391041
iter  20 value 1170.409562
iter  30 value 1163.557062
iter  40 value 1072.247787
iter  50 value 1005.841334
final  value 991.311919 
converged
# weights:  499
initial  value 95895.285674 
iter  10 value 2961.031515
iter  20 value 2629.869480
iter  30 value 1903.833798
iter  40 value 1256.021217
iter  50 value 1131.839615
iter  60 value 1128.430910
iter  70 value 1125.134111
final  value 1124.675728 
converged
[Tune-y] 20: rmse.test.rmse=0.497; time: 0.1 min
[Tune] Result: size=4; decay=0.00051 : rmse.test.rmse=0.382
# weights:  665
initial  value 142658.020286 
iter  10 value 159.587821
iter  20 value 159.532661
final  value 159.532635 
converged
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.nnet: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 06:19:33 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 435 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 222
 number of selected nodes                               : 56 
[1] "Sat Jan 13 06:19:51 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.pcr: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 06:20:01 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.plsr: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 06:20:36 2018"
[Tune] Started tuning learner regr.randomForest for parameter set:
            Type len Def   Constr Req Tunable Trafo
nodesize integer   -   1  1 to 10   -    TRUE     -
mtry     integer   -  54 1 to 164   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nodesize=8; mtry=86
[Tune-y] 1: rmse.test.rmse=0.147; time: 1.1 min
[Tune-x] 2: nodesize=8; mtry=110
[Tune-y] 2: rmse.test.rmse=0.148; time: 1.3 min
[Tune-x] 3: nodesize=2; mtry=164
[Tune-y] 3: rmse.test.rmse=0.151; time: 3.4 min
[Tune-x] 4: nodesize=8; mtry=80
[Tune-y] 4: rmse.test.rmse=0.147; time: 1.0 min
[Tune-x] 5: nodesize=2; mtry=47
[Tune-y] 5: rmse.test.rmse=0.145; time: 1.1 min
[Tune-x] 6: nodesize=8; mtry=152
[Tune-y] 6: rmse.test.rmse=0.151; time: 1.8 min
[Tune-x] 7: nodesize=10; mtry=35
[Tune-y] 7: rmse.test.rmse=0.146; time: 0.4 min
[Tune-x] 8: nodesize=8; mtry=96
[Tune-y] 8: rmse.test.rmse=0.148; time: 1.2 min
[Tune-x] 9: nodesize=2; mtry=150
[Tune-y] 9: rmse.test.rmse=0.15; time: 3.1 min
[Tune-x] 10: nodesize=9; mtry=35
[Tune-y] 10: rmse.test.rmse=0.147; time: 0.4 min
[Tune-x] 11: nodesize=1; mtry=27
[Tune-y] 11: rmse.test.rmse=0.146; time: 0.8 min
[Tune-x] 12: nodesize=7; mtry=87
[Tune-y] 12: rmse.test.rmse=0.147; time: 1.1 min
[Tune-x] 13: nodesize=6; mtry=118
[Tune-y] 13: rmse.test.rmse=0.149; time: 1.6 min
[Tune-x] 14: nodesize=6; mtry=68
[Tune-y] 14: rmse.test.rmse=0.145; time: 1.0 min
[Tune-x] 15: nodesize=2; mtry=155
[Tune-y] 15: rmse.test.rmse=0.151; time: 3.2 min
[Tune-x] 16: nodesize=1; mtry=23
[Tune-y] 16: rmse.test.rmse=0.147; time: 0.7 min
[Tune-x] 17: nodesize=4; mtry=98
[Tune-y] 17: rmse.test.rmse=0.147; time: 1.6 min
[Tune-x] 18: nodesize=2; mtry=57
[Tune-y] 18: rmse.test.rmse=0.145; time: 1.3 min
[Tune-x] 19: nodesize=1; mtry=1
[Tune-y] 19: rmse.test.rmse=0.261; time: 0.0 min
[Tune-x] 20: nodesize=2; mtry=162
[Tune-y] 20: rmse.test.rmse=0.152; time: 3.3 min
[Tune] Result: nodesize=2; mtry=47 : rmse.test.rmse=0.145
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.randomForest: Error in predict.randomForest(.model$learner.model, newdata = .newdata,  : 
  variables in the training data missing in newdata

[1] "Sat Jan 13 06:51:01 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.randomForestSRC: Error in generic.predict.rfsrc(object, newdata, outcome.target = outcome.target,  : 
  x-variables in test data do not match original training data

[1] "Sat Jan 13 06:51:27 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def   Constr Req Tunable Trafo
mtry          integer   -  54 1 to 164   -    TRUE     -
min.node.size integer   -   5  1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=117; min.node.size=6
[Tune-y] 1: rmse.test.rmse=0.148; time: 0.7 min
[Tune-x] 2: mtry=128; min.node.size=7
[Tune-y] 2: rmse.test.rmse=0.149; time: 0.7 min
[Tune-x] 3: mtry=17; min.node.size=10
[Tune-y] 3: rmse.test.rmse=0.149; time: 0.1 min
[Tune-x] 4: mtry=125; min.node.size=5
[Tune-y] 4: rmse.test.rmse=0.148; time: 0.7 min
[Tune-x] 5: mtry=30; min.node.size=3
[Tune-y] 5: rmse.test.rmse=0.146; time: 0.3 min
[Tune-x] 6: mtry=117; min.node.size=10
[Tune-y] 6: rmse.test.rmse=0.149; time: 0.6 min
[Tune-x] 7: mtry=154; min.node.size=3
[Tune-y] 7: rmse.test.rmse=0.15; time: 1.1 min
[Tune-x] 8: mtry=121; min.node.size=6
[Tune-y] 8: rmse.test.rmse=0.148; time: 0.7 min
[Tune-x] 9: mtry=30; min.node.size=10
[Tune-y] 9: rmse.test.rmse=0.147; time: 0.2 min
[Tune-x] 10: mtry=136; min.node.size=3
[Tune-y] 10: rmse.test.rmse=0.149; time: 0.9 min
[Tune-x] 11: mtry=12; min.node.size=2
[Tune-y] 11: rmse.test.rmse=0.151; time: 0.2 min
[Tune-x] 12: mtry=113; min.node.size=6
[Tune-y] 12: rmse.test.rmse=0.148; time: 0.6 min
[Tune-x] 13: mtry=83; min.node.size=8
[Tune-y] 13: rmse.test.rmse=0.147; time: 0.4 min
[Tune-x] 14: mtry=87; min.node.size=5
[Tune-y] 14: rmse.test.rmse=0.146; time: 0.5 min
[Tune-x] 15: mtry=29; min.node.size=10
[Tune-y] 15: rmse.test.rmse=0.147; time: 0.2 min
[Tune-x] 16: mtry=5; min.node.size=2
[Tune-y] 16: rmse.test.rmse=0.163; time: 0.1 min
[Tune-x] 17: mtry=52; min.node.size=6
[Tune-y] 17: rmse.test.rmse=0.146; time: 0.3 min
[Tune-x] 18: mtry=21; min.node.size=4
[Tune-y] 18: rmse.test.rmse=0.148; time: 0.2 min
[Tune-x] 19: mtry=6; min.node.size=1
[Tune-y] 19: rmse.test.rmse=0.159; time: 0.1 min
[Tune-x] 20: mtry=19; min.node.size=10
[Tune-y] 20: rmse.test.rmse=0.15; time: 0.1 min
[Tune] Result: mtry=52; min.node.size=6 : rmse.test.rmse=0.146
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.ranger: Error in `[.data.frame`(data, , forest$independent.variable.names, drop = FALSE) : 
  undefined columns selected

[1] "Sat Jan 13 07:00:42 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.rknn: Error in knn.reg(train = data[, fset], test = newdata[, fset], y = y,  : 
  too many ties in knn

[1] "Sat Jan 13 07:00:50 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.137; maxdepth=17; minbucket=40; minsplit=35
[Tune-y] 1: rmse.test.rmse=0.288; time: 0.0 min
[Tune-x] 2: cp=0.00195; maxdepth=30; minbucket=39; minsplit=27
[Tune-y] 2: rmse.test.rmse=0.211; time: 0.0 min
[Tune-x] 3: cp=0.00347; maxdepth=10; minbucket=37; minsplit=47
[Tune-y] 3: rmse.test.rmse=0.213; time: 0.0 min
[Tune-x] 4: cp=0.633; maxdepth=8; minbucket=38; minsplit=31
[Tune-y] 4: rmse.test.rmse=0.393; time: 0.0 min
[Tune-x] 5: cp=0.00344; maxdepth=28; minbucket=42; minsplit=14
[Tune-y] 5: rmse.test.rmse=0.213; time: 0.0 min
[Tune-x] 6: cp=0.00158; maxdepth=7; minbucket=36; minsplit=29
[Tune-y] 6: rmse.test.rmse=0.212; time: 0.0 min
[Tune-x] 7: cp=0.032; maxdepth=23; minbucket=29; minsplit=24
[Tune-y] 7: rmse.test.rmse=0.249; time: 0.0 min
[Tune-x] 8: cp=0.00319; maxdepth=29; minbucket=6; minsplit=11
[Tune-y] 8: rmse.test.rmse=0.194; time: 0.0 min
[Tune-x] 9: cp=0.00852; maxdepth=19; minbucket=10; minsplit=20
[Tune-y] 9: rmse.test.rmse=0.209; time: 0.0 min
[Tune-x] 10: cp=0.00121; maxdepth=3; minbucket=10; minsplit=50
[Tune-y] 10: rmse.test.rmse=0.227; time: 0.0 min
[Tune-x] 11: cp=0.0342; maxdepth=7; minbucket=27; minsplit=21
[Tune-y] 11: rmse.test.rmse=0.249; time: 0.0 min
[Tune-x] 12: cp=0.0491; maxdepth=8; minbucket=46; minsplit=31
[Tune-y] 12: rmse.test.rmse=0.251; time: 0.0 min
[Tune-x] 13: cp=0.0032; maxdepth=3; minbucket=48; minsplit=37
[Tune-y] 13: rmse.test.rmse=0.228; time: 0.0 min
[Tune-x] 14: cp=0.273; maxdepth=20; minbucket=25; minsplit=19
[Tune-y] 14: rmse.test.rmse=0.288; time: 0.0 min
[Tune-x] 15: cp=0.0214; maxdepth=3; minbucket=33; minsplit=47
[Tune-y] 15: rmse.test.rmse=0.233; time: 0.0 min
[Tune-x] 16: cp=0.00202; maxdepth=10; minbucket=32; minsplit=26
[Tune-y] 16: rmse.test.rmse=0.207; time: 0.0 min
[Tune-x] 17: cp=0.23; maxdepth=3; minbucket=19; minsplit=43
[Tune-y] 17: rmse.test.rmse=0.288; time: 0.0 min
[Tune-x] 18: cp=0.0817; maxdepth=7; minbucket=30; minsplit=44
[Tune-y] 18: rmse.test.rmse=0.288; time: 0.0 min
[Tune-x] 19: cp=0.016; maxdepth=22; minbucket=29; minsplit=42
[Tune-y] 19: rmse.test.rmse=0.224; time: 0.0 min
[Tune-x] 20: cp=0.0696; maxdepth=17; minbucket=10; minsplit=45
[Tune-y] 20: rmse.test.rmse=0.259; time: 0.0 min
[Tune] Result: cp=0.00319; maxdepth=29; minbucket=6; minsplit=11 : rmse.test.rmse=0.194
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.rpart: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 07:01:08 2018"
[Tune] Started tuning learner regr.RRF for parameter set:
           Type len Def   Constr Req Tunable Trafo
mtry    integer   -  54 1 to 164   -    TRUE     -
coefReg numeric   - 0.8   0 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=117; coefReg=0.521
[Tune-y] 1: rmse.test.rmse=0.149; time: 1.7 min
[Tune-x] 2: mtry=128; coefReg=0.666
[Tune-y] 2: rmse.test.rmse=0.149; time: 1.8 min
[Tune-x] 3: mtry=17; coefReg=0.996
[Tune-y] 3: rmse.test.rmse=0.149; time: 0.3 min
[Tune-x] 4: mtry=125; coefReg=0.482
[Tune-y] 4: rmse.test.rmse=0.15; time: 1.8 min
[Tune-x] 5: mtry=30; coefReg=0.285
[Tune-y] 5: rmse.test.rmse=0.146; time: 0.5 min
[Tune-x] 6: mtry=117; coefReg=0.922
[Tune-y] 6: rmse.test.rmse=0.148; time: 1.7 min
[Tune-x] 7: mtry=154; coefReg=0.207
[Tune-y] 7: rmse.test.rmse=0.161; time: 2.3 min
[Tune-x] 8: mtry=121; coefReg=0.582
[Tune-y] 8: rmse.test.rmse=0.148; time: 1.7 min
[Tune-x] 9: mtry=30; coefReg=0.912
[Tune-y] 9: rmse.test.rmse=0.146; time: 0.5 min
[Tune-x] 10: mtry=136; coefReg=0.212
[Tune-y] 10: rmse.test.rmse=0.157; time: 2.0 min
[Tune-x] 11: mtry=12; coefReg=0.16
[Tune-y] 11: rmse.test.rmse=0.151; time: 0.3 min
[Tune-x] 12: mtry=113; coefReg=0.53
[Tune-y] 12: rmse.test.rmse=0.148; time: 1.6 min
[Tune-x] 13: mtry=83; coefReg=0.715
[Tune-y] 13: rmse.test.rmse=0.146; time: 1.2 min
[Tune-x] 14: mtry=87; coefReg=0.415
[Tune-y] 14: rmse.test.rmse=0.146; time: 1.3 min
[Tune-x] 15: mtry=29; coefReg=0.944
[Tune-y] 15: rmse.test.rmse=0.146; time: 0.5 min
[Tune-x] 16: mtry=5; coefReg=0.14
[Tune-y] 16: rmse.test.rmse=0.163; time: 0.1 min
[Tune-x] 17: mtry=52; coefReg=0.596
[Tune-y] 17: rmse.test.rmse=0.145; time: 0.8 min
[Tune-x] 18: mtry=21; coefReg=0.343
[Tune-y] 18: rmse.test.rmse=0.147; time: 0.4 min
[Tune-x] 19: mtry=6; coefReg=0.00025
[Tune-y] 19: rmse.test.rmse=0.159; time: 0.2 min
[Tune-x] 20: mtry=19; coefReg=0.982
[Tune-y] 20: rmse.test.rmse=0.148; time: 0.4 min
[Tune] Result: mtry=52; coefReg=0.596 : rmse.test.rmse=0.145
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.RRF: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 07:23:05 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.rsm: Error in lapply(X = X, FUN = FUN, ...) : object 'V198.dummy' not found

[1] "Sat Jan 13 07:23:14 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning message:
In (function (formula, data, ...)  :
  Some coefficients are aliased - cannot use 'rsm' methods.
  Returning an 'lm' object.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.rvm: Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) : 
  the leading minor of order 18 is not positive definite

[1] "Sat Jan 13 07:23:27 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.8060 0.4350 0.2340 0.1260 0.0682
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 49 
Runtime: 33.75593 secs 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.slim: Error in newdata %*% object$beta[, lambda.idx] : 
  non-conformable arguments

[1] "Sat Jan 13 07:24:10 2018"
[Tune] Started tuning learner regr.svm for parameter set:
         Type len   Def    Constr Req Tunable Trafo
cost  numeric   -     0 -15 to 15   -    TRUE     Y
gamma numeric   - -7.36 -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cost=84.5; gamma=1.54
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 1: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 2: cost=335; gamma=31.7
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 2: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 3: cost=0.000244; gamma=2.99e+04
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: cost=215; gamma=0.693
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: cost=0.00137; gamma=0.0114
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: cost=77.1; gamma=6.53e+03
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: cost=8.31e+03; gamma=0.00228
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 7: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 8: cost=124; gamma=5.54
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 8: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 9: cost=0.00134; gamma=5.26e+03
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: cost=867; gamma=0.00249
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: cost=0.000128; gamma=0.000856
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 11: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 12: cost=50.3; gamma=1.86
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: cost=1.07; gamma=87.6
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: cost=1.86; gamma=0.169
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 14: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 15: cost=0.00107; gamma=1.02e+04
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: cost=5.68e-05; gamma=0.000557
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 16: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 17: cost=0.0203; gamma=7.43
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 17: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 18: cost=0.000424; gamma=0.0381
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 18: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 19: cost=5.83e-05; gamma=3.07e-05
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: cost=0.000302; gamma=2.27e+04
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.svm: Error in predict.svm(ret, xhold, decision.values = TRUE) : 
  Model is empty!

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: cost=0.000128; gamma=0.000856 : rmse.test.rmse=  NA
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.svm: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 07:25:02 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=718; max_depth=6; eta=0.468; gamma=6.66; colsample_bytree=0.34; min_child_weight=19.9; subsample=0.819
[Tune-y] 1: rmse.test.rmse=0.227; time: 0.3 min
[Tune-x] 2: nrounds=180; max_depth=2; eta=0.172; gamma=7.09; colsample_bytree=0.669; min_child_weight=18.7; subsample=0.406
[Tune-y] 2: rmse.test.rmse=0.258; time: 0.1 min
[Tune-x] 3: nrounds=802; max_depth=6; eta=0.11; gamma=9.12; colsample_bytree=0.63; min_child_weight=4.23; subsample=0.302
[Tune-y] 3: rmse.test.rmse=0.291; time: 0.5 min
[Tune-x] 4: nrounds=26; max_depth=7; eta=0.318; gamma=5.03; colsample_bytree=0.586; min_child_weight=10.6; subsample=0.561
[Tune-y] 4: rmse.test.rmse=0.232; time: 0.0 min
[Tune-x] 5: nrounds=28; max_depth=10; eta=0.0189; gamma=1.4; colsample_bytree=0.425; min_child_weight=11.9; subsample=0.345
[Tune-y] 5: rmse.test.rmse=6.78; time: 0.0 min
[Tune-x] 6: nrounds=78; max_depth=1; eta=0.00115; gamma=1.1; colsample_bytree=0.693; min_child_weight=10.3; subsample=0.373
[Tune-y] 6: rmse.test.rmse=10.5; time: 0.0 min
[Tune-x] 7: nrounds=193; max_depth=4; eta=0.34; gamma=1.79; colsample_bytree=0.658; min_child_weight=11.6; subsample=0.378
[Tune-y] 7: rmse.test.rmse=0.187; time: 0.1 min
[Tune-x] 8: nrounds=12; max_depth=10; eta=0.42; gamma=8.12; colsample_bytree=0.544; min_child_weight=8.99; subsample=0.486
[Tune-y] 8: rmse.test.rmse=0.273; time: 0.0 min
[Tune-x] 9: nrounds=144; max_depth=1; eta=0.373; gamma=9.31; colsample_bytree=0.342; min_child_weight=5.32; subsample=0.695
[Tune-y] 9: rmse.test.rmse=0.254; time: 0.0 min
[Tune-x] 10: nrounds=156; max_depth=8; eta=0.00893; gamma=3.2; colsample_bytree=0.634; min_child_weight=12.8; subsample=0.383
[Tune-y] 10: rmse.test.rmse=2.88; time: 0.0 min
[Tune-x] 11: nrounds=264; max_depth=9; eta=0.242; gamma=6.91; colsample_bytree=0.513; min_child_weight=16.5; subsample=0.712
[Tune-y] 11: rmse.test.rmse=0.233; time: 0.2 min
[Tune-x] 12: nrounds=244; max_depth=2; eta=0.533; gamma=4; colsample_bytree=0.633; min_child_weight=18.7; subsample=0.843
[Tune-y] 12: rmse.test.rmse=0.217; time: 0.1 min
[Tune-x] 13: nrounds=18; max_depth=9; eta=0.295; gamma=4.15; colsample_bytree=0.337; min_child_weight=2.77; subsample=0.945
[Tune-y] 13: rmse.test.rmse=0.212; time: 0.0 min
[Tune-x] 14: nrounds=1.12e+03; max_depth=5; eta=0.381; gamma=6.96; colsample_bytree=0.341; min_child_weight=19.9; subsample=0.605
[Tune-y] 14: rmse.test.rmse=0.23; time: 0.4 min
[Tune-x] 15: nrounds=293; max_depth=4; eta=0.278; gamma=2.24; colsample_bytree=0.691; min_child_weight=3.68; subsample=0.325
[Tune-y] 15: rmse.test.rmse=0.196; time: 0.1 min
[Tune-x] 16: nrounds=55; max_depth=2; eta=0.223; gamma=7.39; colsample_bytree=0.662; min_child_weight=13.1; subsample=0.677
[Tune-y] 16: rmse.test.rmse=0.243; time: 0.0 min
[Tune-x] 17: nrounds=314; max_depth=10; eta=0.57; gamma=6.73; colsample_bytree=0.559; min_child_weight=12.8; subsample=0.795
[Tune-y] 17: rmse.test.rmse=0.242; time: 0.3 min
[Tune-x] 18: nrounds=12; max_depth=8; eta=0.124; gamma=2.21; colsample_bytree=0.319; min_child_weight=7.74; subsample=0.364
[Tune-y] 18: rmse.test.rmse=2.39; time: 0.0 min
[Tune-x] 19: nrounds=522; max_depth=5; eta=0.0277; gamma=9.67; colsample_bytree=0.593; min_child_weight=6.81; subsample=0.645
[Tune-y] 19: rmse.test.rmse=0.27; time: 0.2 min
[Tune-x] 20: nrounds=264; max_depth=8; eta=0.333; gamma=4.83; colsample_bytree=0.37; min_child_weight=9.01; subsample=0.772
[Tune-y] 20: rmse.test.rmse=0.219; time: 0.2 min
[Tune] Result: nrounds=193; max_depth=4; eta=0.34; gamma=1.79; colsample_bytree=0.658; min_child_weight=11.6; subsample=0.378 : rmse.test.rmse=0.187
[1] "Sat Jan 13 07:27:39 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: There were 40 warnings (use warnings() to see them)
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Sat Jan 13 07:27:48 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

burn in:
**GROW** @depth 0: [125,0], n=(197,900)
**GROW** @depth 1: [151,0], n=(477,423)
r=1000 d=[0] [0] [0]; n=(197,477,423)
**GROW** @depth 2: [100,0], n=(240,183)
r=2000 d=[0] [0] [0] [0]; n=(197,477,240,183)

Sampling @ nn=0 pred locs:
r=1000 d=[0] [0] [0] [0]; mh=4 n=(197,477,240,183)
**GROW** @depth 2: [60,0], n=(223,254)
r=2000 d=[0] [0] [0] [0] [0]; mh=4 n=(197,223,254,240,183)
r=3000 d=[0] [0] [0] [0] [0]; mh=4 n=(197,223,254,240,183)
r=4000 d=[0] [0] [0] [0] [0]; mh=4 n=(197,223,254,240,183)
r=5000 d=[0] [0] [0] [0] [0]; mh=4 n=(197,223,254,240,183)
Grow: 1.201%, Prune: 0%, Change: 0%, Swap: 0%

Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.bcart: Error in predict.tgp(.model$learner.model, XX = .newdata, pred.n = FALSE,  : 
  mismatched column dimension of object$X and XX

[1] "Sat Jan 13 07:32:04 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Sat Jan 13 07:32:12 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

Warning in train(allmodel, regr.task) :
  Could not train learner regr.blm: Error in tgp(X, Z, XX, BTE, R, m0r1, FALSE, params, itemps, pred.n, krige,  : 
  X[,1:166]-matrix is not of full rank

[1] "Sat Jan 13 07:32:24 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Number of parameters (weights and biases) to estimate: 336 
Nguyen-Widrow method
Scaling factor= 0.7004424 
gamma= 194.005 	 alpha= 77.6675 	 beta= 107.2528 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.brnn: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 07:32:48 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Sat Jan 13 07:32:57 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

Warning in train(allmodel, regr.task) :
  Could not train learner regr.btlm: Error in tgp(X, Z, XX, BTE, R, m0r1, FALSE, params, itemps, pred.n, krige,  : 
  X[,1:166]-matrix is not of full rank

[1] "Sat Jan 13 07:33:06 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.cforest: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 07:33:23 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.ctree: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 07:33:36 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.cubist: Error in `[.data.frame`(newdata, , object$vars$all, drop = FALSE) : 
  undefined columns selected

[1] "Sat Jan 13 07:33:47 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.cvglmnet: Error in cbind2(1, newx) %*% nbeta : 
  Cholmod error 'X and/or Y have wrong dimensions' at file ../MatrixOps/cholmod_sdmult.c, line 90

[1] "Sat Jan 13 07:33:57 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Sat Jan 13 07:34:08 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.elmNN: Error in inpweight %*% TV.P : non-conformable arguments

[1] "Sat Jan 13 07:34:17 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
[Tune] Started tuning learner regr.extraTrees for parameter set:
                 Type len Def   Constr Req Tunable Trafo
mtry          integer   -  55 1 to 166   -    TRUE     -
numRandomCuts integer   -   1  1 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=6; numRandomCuts=12
[Tune-y] 1: rmse.test.rmse=0.146; time: 0.6 min
[Tune-x] 2: mtry=50; numRandomCuts=23
[Tune-y] 2: rmse.test.rmse=0.137; time: 6.5 min
[Tune-x] 3: mtry=53; numRandomCuts=3
[Tune-y] 3: rmse.test.rmse=0.139; time: 1.2 min
[Tune-x] 4: mtry=33; numRandomCuts=4
[Tune-y] 4: rmse.test.rmse=0.138; time: 1.0 min
[Tune-x] 5: mtry=148; numRandomCuts=24
[Tune-y] 5: rmse.test.rmse=0.155; time: 16.1 min
[Tune-x] 6: mtry=140; numRandomCuts=23
[Tune-y] 6: rmse.test.rmse=0.152; time: 14.8 min
[Tune-x] 7: mtry=10; numRandomCuts=22
[Tune-y] 7: rmse.test.rmse=0.14; time: 1.3 min
[Tune-x] 8: mtry=16; numRandomCuts=20
[Tune-y] 8: rmse.test.rmse=0.137; time: 2.0 min
[Tune-x] 9: mtry=120; numRandomCuts=19
[Tune-y] 9: rmse.test.rmse=0.146; time: 11.4 min
[Tune-x] 10: mtry=20; numRandomCuts=17
[Tune-y] 10: rmse.test.rmse=0.137; time: 2.0 min
[Tune-x] 11: mtry=145; numRandomCuts=10
[Tune-y] 11: rmse.test.rmse=0.149; time: 6.9 min
[Tune-x] 12: mtry=127; numRandomCuts=6
[Tune-y] 12: rmse.test.rmse=0.143; time: 4.1 min
[Tune-x] 13: mtry=163; numRandomCuts=25
[Tune-y] 13: rmse.test.rmse=0.158; time: 16.8 min
[Tune-x] 14: mtry=43; numRandomCuts=12
[Tune-y] 14: rmse.test.rmse=0.137; time: 3.1 min
[Tune-x] 15: mtry=129; numRandomCuts=5
[Tune-y] 15: rmse.test.rmse=0.144; time: 3.5 min
[Tune-x] 16: mtry=83; numRandomCuts=2
[Tune-y] 16: rmse.test.rmse=0.141; time: 1.4 min
[Tune-x] 17: mtry=113; numRandomCuts=10
[Tune-y] 17: rmse.test.rmse=0.143; time: 6.1 min
[Tune-x] 18: mtry=116; numRandomCuts=1
[Tune-y] 18: rmse.test.rmse=0.146; time: 1.0 min
[Tune-x] 19: mtry=34; numRandomCuts=15
[Tune-y] 19: rmse.test.rmse=0.136; time: 3.0 min
[Tune-x] 20: mtry=153; numRandomCuts=8
[Tune-y] 20: rmse.test.rmse=0.149; time: 5.7 min
[Tune] Result: mtry=34; numRandomCuts=15 : rmse.test.rmse=0.136
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.extraTrees: Error in predict.extraTrees(.model$learner.model, as.matrix(.newdata),  : 
  newdata(ncol=164) does not have the same dimensions as the original x (ncol=166)

[1] "Sat Jan 13 09:24:55 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Sat Jan 13 09:25:29 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.fnn: Error in get.knnx(train, test, k, algorithm) : 
  Number of columns must be same!.

[1] "Sat Jan 13 09:25:37 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.frbs: Error in rep(1:num.varinput, num.labels.input) : invalid 'times' argument

[1] "Sat Jan 13 09:26:30 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
In addition: There were 15 warnings (use warnings() to see them)
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.gausspr: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 09:26:49 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=12; interaction.depth=5; shrinkage=0.18; n.minobsinnode=23
[Tune-y] 1: rmse.test.rmse=0.166; time: 0.1 min
[Tune-x] 2: n.trees=43; interaction.depth=2; shrinkage=0.118; n.minobsinnode=7
[Tune-y] 2: rmse.test.rmse=0.147; time: 0.0 min
[Tune-x] 3: n.trees=599; interaction.depth=10; shrinkage=0.504; n.minobsinnode=24
[Tune-y] 3: rmse.test.rmse=0.168; time: 1.0 min
[Tune-x] 4: n.trees=13; interaction.depth=9; shrinkage=0.057; n.minobsinnode=21
[Tune-y] 4: rmse.test.rmse=0.247; time: 0.0 min
[Tune-x] 5: n.trees=276; interaction.depth=8; shrinkage=0.073; n.minobsinnode=18
[Tune-y] 5: rmse.test.rmse=0.126; time: 0.4 min
[Tune-x] 6: n.trees=552; interaction.depth=4; shrinkage=0.458; n.minobsinnode=9
[Tune-y] 6: rmse.test.rmse=0.161; time: 0.4 min
[Tune-x] 7: n.trees=895; interaction.depth=10; shrinkage=0.153; n.minobsinnode=14
[Tune-y] 7: rmse.test.rmse=0.132; time: 1.5 min
[Tune-x] 8: n.trees=350; interaction.depth=2; shrinkage=0.298; n.minobsinnode=6
[Tune-y] 8: rmse.test.rmse=0.136; time: 0.1 min
[Tune-x] 9: n.trees=225; interaction.depth=4; shrinkage=0.419; n.minobsinnode=5
[Tune-y] 9: rmse.test.rmse=0.159; time: 0.2 min
[Tune-x] 10: n.trees=25; interaction.depth=6; shrinkage=0.551; n.minobsinnode=11
[Tune-y] 10: rmse.test.rmse=0.168; time: 0.0 min
[Tune-x] 11: n.trees=26; interaction.depth=8; shrinkage=0.0295; n.minobsinnode=9
[Tune-y] 11: rmse.test.rmse=0.248; time: 0.0 min
[Tune-x] 12: n.trees=23; interaction.depth=2; shrinkage=0.41; n.minobsinnode=6
[Tune-y] 12: rmse.test.rmse=0.154; time: 0.0 min
[Tune-x] 13: n.trees=42; interaction.depth=6; shrinkage=0.195; n.minobsinnode=9
[Tune-y] 13: rmse.test.rmse=0.133; time: 0.1 min
[Tune-x] 14: n.trees=430; interaction.depth=2; shrinkage=0.452; n.minobsinnode=21
[Tune-y] 14: rmse.test.rmse=0.144; time: 0.2 min
[Tune-x] 15: n.trees=51; interaction.depth=7; shrinkage=0.199; n.minobsinnode=19
[Tune-y] 15: rmse.test.rmse=0.133; time: 0.1 min
[Tune-x] 16: n.trees=113; interaction.depth=2; shrinkage=0.539; n.minobsinnode=18
[Tune-y] 16: rmse.test.rmse=0.148; time: 0.1 min
[Tune-x] 17: n.trees=41; interaction.depth=1; shrinkage=0.348; n.minobsinnode=5
[Tune-y] 17: rmse.test.rmse=0.154; time: 0.0 min
[Tune-x] 18: n.trees=87; interaction.depth=5; shrinkage=0.0893; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=0.128; time: 0.1 min
[Tune-x] 19: n.trees=472; interaction.depth=7; shrinkage=0.133; n.minobsinnode=20
[Tune-y] 19: rmse.test.rmse=0.13; time: 0.6 min
[Tune-x] 20: n.trees=132; interaction.depth=7; shrinkage=0.0477; n.minobsinnode=10
[Tune-y] 20: rmse.test.rmse=0.126; time: 0.2 min
[Tune] Result: n.trees=132; interaction.depth=7; shrinkage=0.0477; n.minobsinnode=10 : rmse.test.rmse=0.126
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.gbm: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 09:32:00 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.glm: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 09:32:08 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.0353; lambda=0.0613
[Tune-y] 1: rmse.test.rmse=0.127; time: 0.0 min
[Tune-x] 2: alpha=0.299; lambda=2.8
[Tune-y] 2: rmse.test.rmse=0.393; time: 0.0 min
[Tune-x] 3: alpha=0.315; lambda=0.00246
[Tune-y] 3: rmse.test.rmse=0.128; time: 0.0 min
[Tune-x] 4: alpha=0.195; lambda=0.0029
[Tune-y] 4: rmse.test.rmse=0.129; time: 0.0 min
[Tune-x] 5: alpha=0.889; lambda=5.35
[Tune-y] 5: rmse.test.rmse=0.393; time: 0.0 min
[Tune-x] 6: alpha=0.84; lambda=3.71
[Tune-y] 6: rmse.test.rmse=0.393; time: 0.0 min
[Tune-x] 7: alpha=0.0583; lambda=2.42
[Tune-y] 7: rmse.test.rmse=0.318; time: 0.0 min
[Tune-x] 8: alpha=0.0934; lambda=0.963
[Tune-y] 8: rmse.test.rmse=0.243; time: 0.0 min
[Tune-x] 9: alpha=0.721; lambda=0.878
[Tune-y] 9: rmse.test.rmse=0.393; time: 0.0 min
[Tune-x] 10: alpha=0.12; lambda=0.351
[Tune-y] 10: rmse.test.rmse=0.175; time: 0.0 min
[Tune-x] 11: alpha=0.871; lambda=0.0312
[Tune-y] 11: rmse.test.rmse=0.147; time: 0.0 min
[Tune-x] 12: alpha=0.763; lambda=0.00649
[Tune-y] 12: rmse.test.rmse=0.127; time: 0.0 min
[Tune-x] 13: alpha=0.976; lambda=6.26
[Tune-y] 13: rmse.test.rmse=0.393; time: 0.0 min
[Tune-x] 14: alpha=0.254; lambda=0.057
[Tune-y] 14: rmse.test.rmse=0.135; time: 0.0 min
[Tune-x] 15: alpha=0.772; lambda=0.00472
[Tune-y] 15: rmse.test.rmse=0.127; time: 0.0 min
[Tune-x] 16: alpha=0.496; lambda=0.00161
[Tune-y] 16: rmse.test.rmse=0.129; time: 0.0 min
[Tune-x] 17: alpha=0.676; lambda=0.0253
[Tune-y] 17: rmse.test.rmse=0.136; time: 0.0 min
[Tune-x] 18: alpha=0.697; lambda=0.001
[Tune-y] 18: rmse.test.rmse=0.129; time: 0.0 min
[Tune-x] 19: alpha=0.203; lambda=0.171
[Tune-y] 19: rmse.test.rmse=0.16; time: 0.0 min
[Tune-x] 20: alpha=0.919; lambda=0.0129
[Tune-y] 20: rmse.test.rmse=0.131; time: 0.0 min
[Tune] Result: alpha=0.763; lambda=0.00649 : rmse.test.rmse=0.127
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.glmnet: Error in cbind2(1, newx) %*% nbeta : 
  Cholmod error 'X and/or Y have wrong dimensions' at file ../MatrixOps/cholmod_sdmult.c, line 90

[1] "Sat Jan 13 09:32:28 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==============                                                        |  20%  |                                                                              |============================                                          |  40%  |                                                                              |==========================================                            |  60%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 09:33:02 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning messages:
1: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V198.dummy': substituting in a column of NaN
2: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V202.dummy': substituting in a column of NaN
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=                                                                     |   2%  |                                                                              |================================                                      |  46%  |                                                                              |===================================================================== |  98%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 09:33:18 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning messages:
1: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V198.dummy': substituting in a column of NaN
2: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V202.dummy': substituting in a column of NaN
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=                                                                     |   2%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 09:33:33 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning messages:
1: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V198.dummy': substituting in a column of NaN
2: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V202.dummy': substituting in a column of NaN
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==========                                                            |  14%  |                                                                              |====================                                                  |  28%  |                                                                              |=============================                                         |  42%  |                                                                              |=========================================                             |  58%  |                                                                              |=================================================                     |  70%  |                                                                              |============================================================          |  86%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 09:33:54 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
In addition: Warning messages:
1: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V198.dummy': substituting in a column of NaN
2: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V202.dummy': substituting in a column of NaN
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  1.998629 2 2 2 2 1.527121 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1.777778 1.75 1.985507 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0.8008505 2 2 1.049427 2 2 2 2 2 2 2 1.310693 1.813075 1.596835 1.333333 2 2 2 2 2 2 2 1.833333 2 2 2 2 2 2 2 2 2 2 2 2 1.760226 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 
  - best initial criterion value(s) :  -516.0711 

N = 166, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       516.07  |proj g|=       1.8892
At iterate     1  f =       423.99  |proj g|=        1.7312
At iterate     2  f =       411.13  |proj g|=        1.5793
ys=-3.224e+00  -gs= 1.092e+01, BFGS update SKIPPED
At iterate     3  f =       391.12  |proj g|=        1.4479
At iterate     4  f =       331.98  |proj g|=        1.7155
At iterate     5  f =       315.66  |proj g|=        1.6054
At iterate     6  f =       293.81  |proj g|=        1.6828
At iterate     7  f =       278.38  |proj g|=        1.5043
ys=-1.119e+00  -gs= 1.393e+01, BFGS update SKIPPED
At iterate     8  f =        268.2  |proj g|=        1.5586
At iterate     9  f =       247.82  |proj g|=        1.6449
At iterate    10  f =        241.1  |proj g|=        1.3381
ys=-1.767e+00  -gs= 5.219e+00, BFGS update SKIPPED
At iterate    11  f =       233.92  |proj g|=        1.1004
At iterate    12  f =        219.2  |proj g|=        0.7271
At iterate    13  f =       215.34  |proj g|=       0.76308
At iterate    14  f =       212.23  |proj g|=       0.62279
At iterate    15  f =       211.01  |proj g|=        1.5406
ys=-6.979e-01  -gs= 9.233e-01, BFGS update SKIPPED
At iterate    16  f =       201.75  |proj g|=        1.0113
ys=-6.186e+00  -gs= 4.044e+00, BFGS update SKIPPED
At iterate    17  f =       191.28  |proj g|=        0.2619
At iterate    18  f =       190.85  |proj g|=       0.29305
At iterate    19  f =       190.51  |proj g|=       0.37814
ys=-5.881e-02  -gs= 3.081e-01, BFGS update SKIPPED
At iterate    20  f =       189.01  |proj g|=       0.71556
At iterate    21  f =       188.99  |proj g|=       0.56603
At iterate    22  f =       188.94  |proj g|=       0.45896
At iterate    23  f =       188.89  |proj g|=       0.11077
At iterate    24  f =       188.86  |proj g|=       0.30112
At iterate    25  f =       188.85  |proj g|=       0.10315
At iterate    26  f =       188.78  |proj g|=       0.16611
At iterate    27  f =       188.57  |proj g|=       0.18436
At iterate    28  f =       188.55  |proj g|=        0.1623
At iterate    29  f =       188.53  |proj g|=      0.089777
At iterate    30  f =       188.49  |proj g|=       0.14909
At iterate    31  f =       188.44  |proj g|=       0.10972
At iterate    32  f =       188.43  |proj g|=      0.062382
At iterate    33  f =       188.43  |proj g|=      0.059646
At iterate    34  f =       188.43  |proj g|=       0.15856
At iterate    35  f =       188.42  |proj g|=       0.23478
At iterate    36  f =       188.42  |proj g|=       0.13416
At iterate    37  f =       188.42  |proj g|=      0.081515
At iterate    38  f =       174.97  |proj g|=       0.66836
ys=-7.073e+00  -gs= 1.026e-01, BFGS update SKIPPED
At iterate    39  f =       172.41  |proj g|=       0.37643
At iterate    40  f =       172.32  |proj g|=       0.38999
At iterate    41  f =       171.44  |proj g|=       0.42917
At iterate    42  f =       171.44  |proj g|=       0.44446
At iterate    43  f =       171.41  |proj g|=       0.35543
At iterate    44  f =       171.35  |proj g|=       0.16538
At iterate    45  f =       171.32  |proj g|=      0.035787
At iterate    46  f =       171.32  |proj g|=     0.0088529
At iterate    47  f =       171.32  |proj g|=    0.00027091
At iterate    48  f =       171.32  |proj g|=    0.00031038

iterations 48
function evaluations 68
segments explored during Cauchy searches 174
BFGS updates skipped 7
active bounds at final generalized Cauchy point 147
norm of the final projected gradient 0.000310381
final function value 171.321

F = 171.321
final  value 171.320594 
converged
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.km: Error in checkNames(X1 = X, X2 = newdata, X1.name = "the design", X2.name = "newdata") : 
  the design and newdata must have the same numbers of columns

[1] "Sat Jan 13 10:36:16 2018"
[Tune] Started tuning learner regr.ksvm for parameter set:
         Type len  Def    Constr Req Tunable Trafo
C     numeric   -    0  -5 to 10   -    TRUE     Y
sigma numeric   - TRUE -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: C=10.6; sigma=0.313
[Tune-y] 1: rmse.test.rmse=0.345; time: 0.2 min
[Tune-x] 2: C=187; sigma=8.32
[Tune-y] 2: rmse.test.rmse=0.394; time: 0.1 min
[Tune-x] 3: C=22.2; sigma=1.15
[Tune-y] 3: rmse.test.rmse=0.392; time: 0.0 min
[Tune-x] 4: C=35.6; sigma=0.0034
[Tune-y] 4: rmse.test.rmse=0.129; time: 0.1 min
[Tune-x] 5: C=0.174; sigma=8.13e-05
[Tune-y] 5: rmse.test.rmse=0.35; time: 0.1 min
[Tune-x] 6: C=4.31; sigma=38.6
[Tune-y] 6: rmse.test.rmse=0.394; time: 0.0 min
[Tune-x] 7: C=4.43; sigma=5.88e+03
[Tune-y] 7: rmse.test.rmse=0.394; time: 0.0 min
[Tune-x] 8: C=7.17; sigma=0.00168
[Tune-y] 8: rmse.test.rmse=0.131; time: 0.1 min
[Tune-x] 9: C=0.124; sigma=1.31e+04
[Tune-y] 9: rmse.test.rmse=0.395; time: 0.0 min
[Tune-x] 10: C=0.504; sigma=1.55
[Tune-y] 10: rmse.test.rmse=0.393; time: 0.0 min
[Tune-x] 11: C=0.461; sigma=0.31
[Tune-y] 11: rmse.test.rmse=0.347; time: 0.0 min
[Tune-x] 12: C=28.2; sigma=2.89e+04
[Tune-y] 12: rmse.test.rmse=0.394; time: 0.0 min
[Tune-x] 13: C=0.0418; sigma=6.3e+03
[Tune-y] 13: rmse.test.rmse=0.395; time: 0.0 min
[Tune-x] 14: C=0.0976; sigma=19.6
[Tune-y] 14: rmse.test.rmse=0.395; time: 0.0 min
[Tune-x] 15: C=2.82; sigma=0.0476
[Tune-y] 15: rmse.test.rmse=0.247; time: 0.1 min
[Tune-x] 16: C=0.0816; sigma=0.00287
[Tune-y] 16: rmse.test.rmse=0.224; time: 0.0 min
[Tune-x] 17: C=36.8; sigma=4.88e-05
[Tune-y] 17: rmse.test.rmse=0.154; time: 0.0 min
[Tune-x] 18: C=6.95; sigma=0.736
[Tune-y] 18: rmse.test.rmse=0.388; time: 0.0 min
[Tune-x] 19: C=2.56; sigma=1.15e+03
[Tune-y] 19: rmse.test.rmse=0.394; time: 0.0 min
[Tune-x] 20: C=56.4; sigma=0.445
[Tune-y] 20: rmse.test.rmse=0.372; time: 0.0 min
[Tune] Result: C=35.6; sigma=0.0034 : rmse.test.rmse=0.129
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.ksvm: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 10:37:46 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
In addition: There were 40 warnings (use warnings() to see them)
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.laGP: Error in (function (X, Z, XX, start = 6, end = 50, d = NULL, g = 1/10000,  : 
  mismatch XX and X cols

[1] "Sat Jan 13 10:37:58 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.LiblineaRL2L1SVR: Error in predict.LiblineaR(.model$learner.model, newx = .newdata, ...) : 
  columns of 'test' and 'train' differ

[1] "Sat Jan 13 10:38:23 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.LiblineaRL2L2SVR: Error in predict.LiblineaR(.model$learner.model, newx = .newdata, ...) : 
  columns of 'test' and 'train' differ

[1] "Sat Jan 13 10:38:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.lm: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 10:38:40 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Sat Jan 13 10:39:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
Error in root.matrix(crossprod(process)) : 
  matrix is not positive semidefinite
Warning in train(allmodel, regr.task) :
  Could not train learner regr.mob: Error in trainLearner.regr.mob(.learner = structure(list(id = "regr.mob",  : 
  Failed to fit party::mob. Some coefficients are estimated as NA

[1] "Sat Jan 13 10:40:00 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=1; decay=0.0057
# weights:  169
initial  value 107606.481677 
iter  10 value 123.088125
iter  20 value 122.886335
iter  30 value 100.757720
iter  40 value 41.926269
iter  50 value 22.585611
iter  60 value 16.833929
iter  70 value 14.277027
iter  80 value 12.106977
iter  90 value 10.656410
iter 100 value 9.806488
final  value 9.806488 
stopped after 100 iterations
# weights:  169
initial  value 97217.774997 
iter  10 value 111.182045
iter  20 value 52.040283
iter  30 value 23.096068
iter  40 value 16.310708
iter  50 value 13.104439
iter  60 value 11.743521
iter  70 value 11.004421
iter  80 value 10.434818
iter  90 value 10.050374
iter 100 value 9.772387
final  value 9.772387 
stopped after 100 iterations
# weights:  169
initial  value 107793.404254 
iter  10 value 42.745267
iter  20 value 32.529571
iter  30 value 28.550636
iter  40 value 23.850380
iter  50 value 18.829438
iter  60 value 15.931409
iter  70 value 14.388087
iter  80 value 13.665555
iter  90 value 13.094799
iter 100 value 12.876875
final  value 12.876875 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=0.136; time: 0.0 min
[Tune-x] 2: size=6; decay=2
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1009) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1009) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1009) weights

[Tune-y] 2: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 3: size=7; decay=4.12e-05
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1177) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1177) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1177) weights

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: size=4; decay=5.3e-05
# weights:  673
initial  value 106949.380913 
iter  10 value 124.455104
iter  20 value 95.237773
iter  30 value 41.308815
iter  40 value 22.248908
iter  50 value 17.530175
iter  60 value 15.877389
iter  70 value 14.545891
iter  80 value 12.543415
iter  90 value 11.669591
iter 100 value 11.455765
final  value 11.455765 
stopped after 100 iterations
# weights:  673
initial  value 115229.490685 
iter  10 value 108.748583
iter  20 value 53.743819
iter  30 value 31.871122
iter  40 value 22.318851
iter  50 value 15.519260
iter  60 value 12.563318
iter  70 value 11.058786
iter  80 value 10.357011
iter  90 value 9.718681
iter 100 value 9.293261
final  value 9.293261 
stopped after 100 iterations
# weights:  673
initial  value 112081.781383 
iter  10 value 115.809708
iter  20 value 21.446165
iter  30 value 11.609475
iter  40 value 8.145188
iter  50 value 6.555280
iter  60 value 5.653956
iter  70 value 5.337112
iter  80 value 5.146499
iter  90 value 5.022474
iter 100 value 4.900857
final  value 4.900857 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=0.205; time: 0.1 min
[Tune-x] 5: size=18; decay=5.39
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3025) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3025) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3025) weights

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: size=17; decay=3.08
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2857) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2857) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2857) weights

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: size=2; decay=1.6
# weights:  337
initial  value 106655.360110 
iter  10 value 235.558391
iter  20 value 147.026063
iter  30 value 121.871556
iter  40 value 110.567454
iter  50 value 106.620659
iter  60 value 105.302753
iter  70 value 104.617131
iter  80 value 104.344845
iter  90 value 104.247877
iter 100 value 104.196175
final  value 104.196175 
stopped after 100 iterations
# weights:  337
initial  value 110805.257019 
iter  10 value 658.854487
iter  20 value 311.291797
iter  30 value 268.682172
iter  40 value 254.475463
iter  50 value 238.175158
iter  60 value 214.454877
iter  70 value 197.772219
iter  80 value 188.279817
iter  90 value 185.764560
iter 100 value 183.304072
final  value 183.304072 
stopped after 100 iterations
# weights:  337
initial  value 101636.305885 
iter  10 value 1374.198047
iter  20 value 319.586139
iter  30 value 210.775983
iter  40 value 192.635312
iter  50 value 178.383083
iter  60 value 161.138396
iter  70 value 151.484722
iter  80 value 142.298940
iter  90 value 137.658162
iter 100 value 134.567952
final  value 134.567952 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.133; time: 0.1 min
[Tune-x] 8: size=2; decay=0.389
# weights:  337
initial  value 111953.862197 
iter  10 value 219.566971
iter  20 value 148.251097
iter  30 value 57.754719
iter  40 value 43.089092
iter  50 value 39.826172
iter  60 value 38.178086
iter  70 value 36.881606
iter  80 value 36.064280
iter  90 value 35.498658
iter 100 value 35.180896
final  value 35.180896 
stopped after 100 iterations
# weights:  337
initial  value 109171.527092 
iter  10 value 210.537817
iter  20 value 125.524142
iter  30 value 64.029522
iter  40 value 47.003769
iter  50 value 42.302372
iter  60 value 41.032824
iter  70 value 40.316301
iter  80 value 39.748779
iter  90 value 39.349979
iter 100 value 39.078751
final  value 39.078751 
stopped after 100 iterations
# weights:  337
initial  value 121377.157595 
iter  10 value 151.814514
iter  20 value 97.576884
iter  30 value 53.370799
iter  40 value 44.225558
iter  50 value 40.497812
iter  60 value 39.405475
iter  70 value 38.265792
iter  80 value 37.478893
iter  90 value 37.179722
iter 100 value 36.962500
final  value 36.962500 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.127; time: 0.1 min
[Tune-x] 9: size=15; decay=0.338
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2521) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2521) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2521) weights

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: size=3; decay=0.0829
# weights:  505
initial  value 93540.870944 
iter  10 value 69.096126
iter  20 value 38.359307
iter  30 value 26.923655
iter  40 value 21.913527
iter  50 value 17.682891
iter  60 value 14.591195
iter  70 value 13.108533
iter  80 value 12.379157
iter  90 value 11.921898
iter 100 value 11.461947
final  value 11.461947 
stopped after 100 iterations
# weights:  505
initial  value 106084.233759 
iter  10 value 77.650757
iter  20 value 35.234222
iter  30 value 30.100833
iter  40 value 26.386564
iter  50 value 23.864306
iter  60 value 22.397952
iter  70 value 21.127358
iter  80 value 19.854106
iter  90 value 19.221943
iter 100 value 18.750104
final  value 18.750104 
stopped after 100 iterations
# weights:  505
initial  value 107384.182391 
iter  10 value 1011.633627
iter  20 value 664.629210
iter  30 value 404.566963
iter  40 value 285.166839
iter  50 value 200.447163
iter  60 value 177.781818
iter  70 value 154.692075
iter  80 value 137.100416
iter  90 value 106.225104
iter 100 value 89.428849
final  value 89.428849 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.215; time: 0.1 min
[Tune-x] 11: size=18; decay=0.00203
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3025) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3025) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3025) weights

[Tune-y] 11: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 12: size=16; decay=0.000183
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2689) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2689) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2689) weights

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: size=20; decay=6.86
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3361) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3361) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3361) weights

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: size=6; decay=0.00511
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1009) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1009) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1009) weights

[Tune-y] 14: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 15: size=16; decay=0.000112
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2689) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2689) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2689) weights

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: size=10; decay=2.15e-05
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

[Tune-y] 16: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 17: size=14; decay=0.00147
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

[Tune-y] 17: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 18: size=14; decay=1.04e-05
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

[Tune-y] 18: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 19: size=5; decay=0.0275
# weights:  841
initial  value 100716.362054 
iter  10 value 65.164067
iter  20 value 27.754219
iter  30 value 16.974943
iter  40 value 12.656291
iter  50 value 9.603631
iter  60 value 7.986924
iter  70 value 7.243402
iter  80 value 6.707633
iter  90 value 6.296727
iter 100 value 6.006913
final  value 6.006913 
stopped after 100 iterations
# weights:  841
initial  value 110663.845035 
iter  10 value 146.747915
iter  20 value 90.496402
iter  30 value 60.410900
iter  40 value 41.308101
iter  50 value 31.476901
iter  60 value 23.562128
iter  70 value 17.822420
iter  80 value 15.414894
iter  90 value 14.303033
iter 100 value 13.937595
final  value 13.937595 
stopped after 100 iterations
# weights:  841
initial  value 127058.186470 
iter  10 value 113.097416
iter  20 value 76.172956
iter  30 value 31.965918
iter  40 value 19.883228
iter  50 value 12.933808
iter  60 value 8.957392
iter  70 value 7.176632
iter  80 value 6.291770
iter  90 value 5.316963
iter 100 value 4.675721
final  value 4.675721 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.145; time: 0.1 min
[Tune-x] 20: size=19; decay=0.000523
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3193) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3193) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3193) weights

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: size=2; decay=0.389 : rmse.test.rmse=0.127
# weights:  337
initial  value 143016.382261 
iter  10 value 232.974723
iter  20 value 174.602107
iter  30 value 134.955839
iter  40 value 98.662241
iter  50 value 84.559303
iter  60 value 67.825688
iter  70 value 57.780982
iter  80 value 53.178230
iter  90 value 51.389545
iter 100 value 50.233512
final  value 50.233512 
stopped after 100 iterations
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.nnet: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 10:40:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 494 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 256
 number of selected nodes                               : 52 
[1] "Sat Jan 13 10:40:58 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.pcr: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 10:41:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.plsr: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 10:41:45 2018"
[Tune] Started tuning learner regr.randomForest for parameter set:
            Type len Def   Constr Req Tunable Trafo
nodesize integer   -   1  1 to 10   -    TRUE     -
mtry     integer   -  55 1 to 166   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nodesize=1; mtry=77
[Tune-y] 1: rmse.test.rmse=0.139; time: 2.1 min
[Tune-x] 2: nodesize=3; mtry=147
[Tune-y] 2: rmse.test.rmse=0.144; time: 2.6 min
[Tune-x] 3: nodesize=4; mtry=17
[Tune-y] 3: rmse.test.rmse=0.142; time: 0.4 min
[Tune-x] 4: nodesize=2; mtry=21
[Tune-y] 4: rmse.test.rmse=0.141; time: 0.5 min
[Tune-x] 5: nodesize=9; mtry=159
[Tune-y] 5: rmse.test.rmse=0.147; time: 1.8 min
[Tune-x] 6: nodesize=9; mtry=152
[Tune-y] 6: rmse.test.rmse=0.146; time: 1.7 min
[Tune-x] 7: nodesize=1; mtry=145
[Tune-y] 7: rmse.test.rmse=0.144; time: 3.7 min
[Tune-x] 8: nodesize=1; mtry=127
[Tune-y] 8: rmse.test.rmse=0.143; time: 3.3 min
[Tune-x] 9: nodesize=8; mtry=126
[Tune-y] 9: rmse.test.rmse=0.144; time: 1.5 min
[Tune-x] 10: nodesize=2; mtry=109
[Tune-y] 10: rmse.test.rmse=0.142; time: 2.3 min
[Tune-x] 11: nodesize=9; mtry=64
[Tune-y] 11: rmse.test.rmse=0.14; time: 0.8 min
[Tune-x] 12: nodesize=8; mtry=35
[Tune-y] 12: rmse.test.rmse=0.139; time: 0.5 min
[Tune-x] 13: nodesize=10; mtry=162
[Tune-y] 13: rmse.test.rmse=0.147; time: 1.7 min
[Tune-x] 14: nodesize=3; mtry=75
[Tune-y] 14: rmse.test.rmse=0.139; time: 1.4 min
[Tune-x] 15: nodesize=8; mtry=30
[Tune-y] 15: rmse.test.rmse=0.14; time: 0.4 min
[Tune-x] 16: nodesize=5; mtry=10
[Tune-y] 16: rmse.test.rmse=0.146; time: 0.2 min
[Tune-x] 17: nodesize=7; mtry=60
[Tune-y] 17: rmse.test.rmse=0.139; time: 0.8 min
[Tune-x] 18: nodesize=7; mtry=1
[Tune-y] 18: rmse.test.rmse=0.26; time: 0.0 min
[Tune-x] 19: nodesize=3; mtry=96
[Tune-y] 19: rmse.test.rmse=0.141; time: 1.7 min
[Tune-x] 20: nodesize=10; mtry=48
[Tune-y] 20: rmse.test.rmse=0.14; time: 0.6 min
[Tune] Result: nodesize=8; mtry=35 : rmse.test.rmse=0.139
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.randomForest: Error in predict.randomForest(.model$learner.model, newdata = .newdata,  : 
  variables in the training data missing in newdata

[1] "Sat Jan 13 11:10:06 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.randomForestSRC: Error in generic.predict.rfsrc(object, newdata, outcome.target = outcome.target,  : 
  x-variables in test data do not match original training data

[1] "Sat Jan 13 11:10:35 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def   Constr Req Tunable Trafo
mtry          integer   -  55 1 to 166   -    TRUE     -
min.node.size integer   -   5  1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=6; min.node.size=5
[Tune-y] 1: rmse.test.rmse=0.154; time: 0.1 min
[Tune-x] 2: mtry=50; min.node.size=9
[Tune-y] 2: rmse.test.rmse=0.139; time: 0.3 min
[Tune-x] 3: mtry=53; min.node.size=2
[Tune-y] 3: rmse.test.rmse=0.139; time: 0.5 min
[Tune-x] 4: mtry=33; min.node.size=2
[Tune-y] 4: rmse.test.rmse=0.139; time: 0.3 min
[Tune-x] 5: mtry=148; min.node.size=10
[Tune-y] 5: rmse.test.rmse=0.145; time: 0.8 min
[Tune-x] 6: mtry=140; min.node.size=10
[Tune-y] 6: rmse.test.rmse=0.145; time: 0.7 min
[Tune-x] 7: mtry=10; min.node.size=9
[Tune-y] 7: rmse.test.rmse=0.149; time: 0.1 min
[Tune-x] 8: mtry=16; min.node.size=8
[Tune-y] 8: rmse.test.rmse=0.144; time: 0.1 min
[Tune-x] 9: mtry=120; min.node.size=8
[Tune-y] 9: rmse.test.rmse=0.143; time: 0.6 min
[Tune-x] 10: mtry=20; min.node.size=7
[Tune-y] 10: rmse.test.rmse=0.142; time: 0.2 min
[Tune-x] 11: mtry=145; min.node.size=4
[Tune-y] 11: rmse.test.rmse=0.144; time: 0.9 min
[Tune-x] 12: mtry=127; min.node.size=3
[Tune-y] 12: rmse.test.rmse=0.143; time: 0.9 min
[Tune-x] 13: mtry=163; min.node.size=10
[Tune-y] 13: rmse.test.rmse=0.147; time: 0.8 min
[Tune-x] 14: mtry=43; min.node.size=5
[Tune-y] 14: rmse.test.rmse=0.139; time: 0.3 min
[Tune-x] 15: mtry=129; min.node.size=2
[Tune-y] 15: rmse.test.rmse=0.143; time: 1.0 min
[Tune-x] 16: mtry=83; min.node.size=1
[Tune-y] 16: rmse.test.rmse=0.139; time: 0.7 min
[Tune-x] 17: mtry=113; min.node.size=4
[Tune-y] 17: rmse.test.rmse=0.143; time: 0.8 min
[Tune-x] 18: mtry=116; min.node.size=1
[Tune-y] 18: rmse.test.rmse=0.142; time: 0.9 min
[Tune-x] 19: mtry=34; min.node.size=6
[Tune-y] 19: rmse.test.rmse=0.139; time: 0.2 min
[Tune-x] 20: mtry=153; min.node.size=3
[Tune-y] 20: rmse.test.rmse=0.145; time: 1.0 min
[Tune] Result: mtry=53; min.node.size=2 : rmse.test.rmse=0.139
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.ranger: Error in `[.data.frame`(data, , forest$independent.variable.names, drop = FALSE) : 
  undefined columns selected

[1] "Sat Jan 13 11:22:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.rknn: Error in knn.reg(train = data[, fset], test = newdata[, fset], y = y,  : 
  too many ties in knn

[1] "Sat Jan 13 11:22:20 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.00125; maxdepth=15; minbucket=18; minsplit=45
[Tune-y] 1: rmse.test.rmse=0.203; time: 0.0 min
[Tune-x] 2: cp=0.00868; maxdepth=5; minbucket=13; minsplit=10
[Tune-y] 2: rmse.test.rmse=0.214; time: 0.0 min
[Tune-x] 3: cp=0.462; maxdepth=29; minbucket=43; minsplit=47
[Tune-y] 3: rmse.test.rmse=0.288; time: 0.0 min
[Tune-x] 4: cp=0.00146; maxdepth=27; minbucket=9; minsplit=40
[Tune-y] 4: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 5: cp=0.144; maxdepth=24; minbucket=10; minsplit=35
[Tune-y] 5: rmse.test.rmse=0.288; time: 0.0 min
[Tune-x] 6: cp=0.409; maxdepth=13; minbucket=40; minsplit=14
[Tune-y] 6: rmse.test.rmse=0.288; time: 0.0 min
[Tune-x] 7: cp=0.847; maxdepth=30; minbucket=16; minsplit=25
[Tune-y] 7: rmse.test.rmse=0.393; time: 0.0 min
[Tune-x] 8: cp=0.206; maxdepth=7; minbucket=27; minsplit=7
[Tune-y] 8: rmse.test.rmse=0.288; time: 0.0 min
[Tune-x] 9: cp=0.106; maxdepth=13; minbucket=37; minsplit=5
[Tune-y] 9: rmse.test.rmse=0.288; time: 0.0 min
[Tune-x] 10: cp=0.00399; maxdepth=19; minbucket=47; minsplit=18
[Tune-y] 10: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 11: cp=0.00411; maxdepth=23; minbucket=7; minsplit=15
[Tune-y] 11: rmse.test.rmse=0.205; time: 0.0 min
[Tune-x] 12: cp=0.00334; maxdepth=8; minbucket=36; minsplit=8
[Tune-y] 12: rmse.test.rmse=0.209; time: 0.0 min
[Tune-x] 13: cp=0.00847; maxdepth=18; minbucket=19; minsplit=15
[Tune-y] 13: rmse.test.rmse=0.216; time: 0.0 min
[Tune-x] 14: cp=0.281; maxdepth=7; minbucket=39; minsplit=41
[Tune-y] 14: rmse.test.rmse=0.288; time: 0.0 min
[Tune-x] 15: cp=0.0113; maxdepth=21; minbucket=20; minsplit=36
[Tune-y] 15: rmse.test.rmse=0.219; time: 0.0 min
[Tune-x] 16: cp=0.0376; maxdepth=8; minbucket=46; minsplit=35
[Tune-y] 16: rmse.test.rmse=0.233; time: 0.0 min
[Tune-x] 17: cp=0.00824; maxdepth=4; minbucket=31; minsplit=6
[Tune-y] 17: rmse.test.rmse=0.215; time: 0.0 min
[Tune-x] 18: cp=0.0252; maxdepth=16; minbucket=11; minsplit=44
[Tune-y] 18: rmse.test.rmse=0.23; time: 0.0 min
[Tune-x] 19: cp=0.323; maxdepth=22; minbucket=15; minsplit=39
[Tune-y] 19: rmse.test.rmse=0.288; time: 0.0 min
[Tune-x] 20: cp=0.0475; maxdepth=21; minbucket=8; minsplit=15
[Tune-y] 20: rmse.test.rmse=0.242; time: 0.0 min
[Tune] Result: cp=0.00146; maxdepth=27; minbucket=9; minsplit=40 : rmse.test.rmse= 0.2
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.rpart: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 11:22:38 2018"
[Tune] Started tuning learner regr.RRF for parameter set:
           Type len Def   Constr Req Tunable Trafo
mtry    integer   -  55 1 to 166   -    TRUE     -
coefReg numeric   - 0.8   0 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=6; coefReg=0.459
[Tune-y] 1: rmse.test.rmse=0.154; time: 0.2 min
[Tune-x] 2: mtry=50; coefReg=0.883
[Tune-y] 2: rmse.test.rmse=0.139; time: 0.8 min
[Tune-x] 3: mtry=53; coefReg=0.102
[Tune-y] 3: rmse.test.rmse=0.141; time: 0.9 min
[Tune-x] 4: mtry=33; coefReg=0.121
[Tune-y] 4: rmse.test.rmse=0.14; time: 0.5 min
[Tune-x] 5: mtry=148; coefReg=0.955
[Tune-y] 5: rmse.test.rmse=0.145; time: 2.1 min
[Tune-x] 6: mtry=140; coefReg=0.915
[Tune-y] 6: rmse.test.rmse=0.145; time: 2.0 min
[Tune-x] 7: mtry=10; coefReg=0.867
[Tune-y] 7: rmse.test.rmse=0.147; time: 0.2 min
[Tune-x] 8: mtry=16; coefReg=0.765
[Tune-y] 8: rmse.test.rmse=0.143; time: 0.4 min
[Tune-x] 9: mtry=120; coefReg=0.755
[Tune-y] 9: rmse.test.rmse=0.143; time: 1.7 min
[Tune-x] 10: mtry=20; coefReg=0.653
[Tune-y] 10: rmse.test.rmse=0.141; time: 0.4 min
[Tune-x] 11: mtry=145; coefReg=0.385
[Tune-y] 11: rmse.test.rmse=0.146; time: 2.1 min
[Tune-x] 12: mtry=127; coefReg=0.21
[Tune-y] 12: rmse.test.rmse=0.153; time: 1.9 min
[Tune-x] 13: mtry=163; coefReg=0.973
[Tune-y] 13: rmse.test.rmse=0.147; time: 2.3 min
[Tune-x] 14: mtry=43; coefReg=0.451
[Tune-y] 14: rmse.test.rmse=0.138; time: 0.7 min
[Tune-x] 15: mtry=129; coefReg=0.175
[Tune-y] 15: rmse.test.rmse=0.155; time: 1.9 min
[Tune-x] 16: mtry=83; coefReg=0.0553
[Tune-y] 16: rmse.test.rmse=0.145; time: 1.3 min
[Tune-x] 17: mtry=113; coefReg=0.361
[Tune-y] 17: rmse.test.rmse=0.145; time: 1.6 min
[Tune-x] 18: mtry=116; coefReg=0.00288
[Tune-y] 18: rmse.test.rmse=0.159; time: 1.7 min
[Tune-x] 19: mtry=34; coefReg=0.573
[Tune-y] 19: rmse.test.rmse=0.139; time: 0.6 min
[Tune-x] 20: mtry=153; coefReg=0.286
[Tune-y] 20: rmse.test.rmse=0.147; time: 2.2 min
[Tune] Result: mtry=43; coefReg=0.451 : rmse.test.rmse=0.138
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.RRF: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 11:48:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.rsm: Error in lapply(X = X, FUN = FUN, ...) : object 'V198.dummy' not found

[1] "Sat Jan 13 11:48:42 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
In addition: Warning message:
In (function (formula, data, ...)  :
  Some coefficients are aliased - cannot use 'rsm' methods.
  Returning an 'lm' object.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.rvm: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 11:50:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.8170 0.4390 0.2360 0.1270 0.0683
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 50 
Runtime: 26.54952 secs 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.slim: Error in newdata %*% object$beta[, lambda.idx] : 
  non-conformable arguments

[1] "Sat Jan 13 11:51:01 2018"
[Tune] Started tuning learner regr.svm for parameter set:
         Type len   Def    Constr Req Tunable Trafo
cost  numeric   -     0 -15 to 15   -    TRUE     Y
gamma numeric   - -7.38 -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cost=6.36e-05; gamma=0.43
[Tune-y] 1: rmse.test.rmse=0.394; time: 0.1 min
[Tune-x] 2: cost=0.0153; gamma=2.9e+03
[Tune-y] 2: rmse.test.rmse=0.394; time: 0.1 min
[Tune-x] 3: cost=0.0214; gamma=0.000257
[Tune-y] 3: rmse.test.rmse=0.368; time: 0.1 min
[Tune-x] 4: cost=0.00175; gamma=0.000376
[Tune-y] 4: rmse.test.rmse=0.391; time: 0.1 min
[Tune-x] 5: cost=3.24e+03; gamma=1.29e+04
[Tune-y] 5: rmse.test.rmse=0.393; time: 0.1 min
[Tune-x] 6: cost=1.19e+03; gamma=5.58e+03
[Tune-y] 6: rmse.test.rmse=0.393; time: 0.1 min
[Tune-x] 7: cost=0.000103; gamma=2.08e+03
[Tune-y] 7: rmse.test.rmse=0.394; time: 0.1 min
[Tune-x] 8: cost=0.000213; gamma=247
[Tune-y] 8: rmse.test.rmse=0.394; time: 0.1 min
[Tune-x] 9: cost=98.7; gamma=200
[Tune-y] 9: rmse.test.rmse=0.393; time: 0.1 min
[Tune-x] 10: cost=0.000371; gamma=24.1
[Tune-y] 10: rmse.test.rmse=0.394; time: 0.1 min
[Tune-x] 11: cost=2.25e+03; gamma=0.0907
[Tune-y] 11: rmse.test.rmse=0.333; time: 0.1 min
[Tune-x] 12: cost=238; gamma=0.00242
[Tune-y] 12: rmse.test.rmse=0.131; time: 0.1 min
[Tune-x] 13: cost=1.99e+04; gamma=1.86e+04
[Tune-y] 13: rmse.test.rmse=0.393; time: 0.1 min
[Tune-x] 14: cost=0.00598; gamma=0.364
[Tune-y] 14: rmse.test.rmse=0.393; time: 0.1 min
[Tune-x] 15: cost=288; gamma=0.00116
[Tune-y] 15: rmse.test.rmse=0.13; time: 0.1 min
[Tune-x] 16: cost=0.922; gamma=9.64e-05
[Tune-y] 16: rmse.test.rmse=0.221; time: 0.1 min
[Tune-x] 17: cost=38.8; gamma=0.0555
[Tune-y] 17: rmse.test.rmse=0.318; time: 0.1 min
[Tune-x] 18: cost=60.3; gamma=3.24e-05
[Tune-y] 18: rmse.test.rmse=0.144; time: 0.1 min
[Tune-x] 19: cost=0.00208; gamma=4.59
[Tune-y] 19: rmse.test.rmse=0.394; time: 0.1 min
[Tune-x] 20: cost=6.06e+03; gamma=0.0118
[Tune-y] 20: rmse.test.rmse=0.166; time: 0.1 min
[Tune] Result: cost=288; gamma=0.00116 : rmse.test.rmse=0.13
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.svm: Error in eval(predvars, data, env) : object 'V198.dummy' not found

[1] "Sat Jan 13 11:52:48 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=12; max_depth=5; eta=0.18; gamma=8.83; colsample_bytree=0.426; min_child_weight=2.05; subsample=0.396
[Tune-y] 1: rmse.test.rmse=1.12; time: 0.0 min
[Tune-x] 2: nrounds=21; max_depth=9; eta=0.573; gamma=8.4; colsample_bytree=0.666; min_child_weight=1.17; subsample=0.901
[Tune-y] 2: rmse.test.rmse=0.239; time: 0.0 min
[Tune-x] 3: nrounds=18; max_depth=8; eta=0.433; gamma=7.55; colsample_bytree=0.348; min_child_weight=13.1; subsample=0.903
[Tune-y] 3: rmse.test.rmse=0.231; time: 0.0 min
[Tune-x] 4: nrounds=100; max_depth=8; eta=0.127; gamma=9.76; colsample_bytree=0.689; min_child_weight=5.08; subsample=0.589
[Tune-y] 4: rmse.test.rmse=0.263; time: 0.1 min
[Tune-x] 5: nrounds=1.02e+03; max_depth=2; eta=0.298; gamma=0.553; colsample_bytree=0.57; min_child_weight=7.22; subsample=0.773
[Tune-y] 5: rmse.test.rmse=0.151; time: 0.2 min
[Tune-x] 6: nrounds=10; max_depth=3; eta=0.344; gamma=9.19; colsample_bytree=0.415; min_child_weight=4.15; subsample=0.802
[Tune-y] 6: rmse.test.rmse=0.305; time: 0.0 min
[Tune-x] 7: nrounds=13; max_depth=3; eta=0.107; gamma=1.92; colsample_bytree=0.573; min_child_weight=1.41; subsample=0.484
[Tune-y] 7: rmse.test.rmse=2.67; time: 0.0 min
[Tune-x] 8: nrounds=264; max_depth=4; eta=0.134; gamma=8.17; colsample_bytree=0.365; min_child_weight=15.1; subsample=0.842
[Tune-y] 8: rmse.test.rmse=0.238; time: 0.1 min
[Tune-x] 9: nrounds=83; max_depth=7; eta=0.199; gamma=6.89; colsample_bytree=0.511; min_child_weight=3.57; subsample=0.923
[Tune-y] 9: rmse.test.rmse=0.229; time: 0.1 min
[Tune-x] 10: nrounds=540; max_depth=4; eta=0.0385; gamma=5.79; colsample_bytree=0.309; min_child_weight=9.38; subsample=0.6
[Tune-y] 10: rmse.test.rmse=0.234; time: 0.2 min
[Tune-x] 11: nrounds=24; max_depth=9; eta=0.502; gamma=6.91; colsample_bytree=0.388; min_child_weight=15.1; subsample=0.67
[Tune-y] 11: rmse.test.rmse=0.239; time: 0.0 min
[Tune-x] 12: nrounds=537; max_depth=1; eta=0.144; gamma=6.75; colsample_bytree=0.619; min_child_weight=3.73; subsample=0.668
[Tune-y] 12: rmse.test.rmse=0.234; time: 0.1 min
[Tune-x] 13: nrounds=21; max_depth=9; eta=0.434; gamma=0.719; colsample_bytree=0.449; min_child_weight=15.7; subsample=0.729
[Tune-y] 13: rmse.test.rmse=0.167; time: 0.0 min
[Tune-x] 14: nrounds=55; max_depth=2; eta=0.556; gamma=1.36; colsample_bytree=0.662; min_child_weight=8.71; subsample=0.379
[Tune-y] 14: rmse.test.rmse=0.199; time: 0.0 min
[Tune-x] 15: nrounds=470; max_depth=1; eta=0.336; gamma=2.09; colsample_bytree=0.511; min_child_weight=0.614; subsample=0.322
[Tune-y] 15: rmse.test.rmse=0.192; time: 0.1 min
[Tune-x] 16: nrounds=19; max_depth=6; eta=0.383; gamma=0.127; colsample_bytree=0.465; min_child_weight=19.4; subsample=0.752
[Tune-y] 16: rmse.test.rmse=0.147; time: 0.0 min
[Tune-x] 17: nrounds=126; max_depth=4; eta=0.124; gamma=3.22; colsample_bytree=0.363; min_child_weight=9.07; subsample=0.598
[Tune-y] 17: rmse.test.rmse=0.203; time: 0.0 min
[Tune-x] 18: nrounds=44; max_depth=2; eta=0.576; gamma=2.8; colsample_bytree=0.53; min_child_weight=19.4; subsample=0.361
[Tune-y] 18: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 19: nrounds=325; max_depth=1; eta=0.42; gamma=5.02; colsample_bytree=0.548; min_child_weight=5.86; subsample=0.633
[Tune-y] 19: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 20: nrounds=1.03e+03; max_depth=9; eta=0.481; gamma=7.34; colsample_bytree=0.591; min_child_weight=20; subsample=0.897
[Tune-y] 20: rmse.test.rmse=0.223; time: 0.8 min
[Tune] Result: nrounds=19; max_depth=6; eta=0.383; gamma=0.127; colsample_bytree=0.465; min_child_weight=19.4; subsample=0.752 : rmse.test.rmse=0.147
[1] "Sat Jan 13 11:54:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
In addition: There were 20 warnings (use warnings() to see them)
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Sat Jan 13 11:54:48 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [165,0], n=(203,894)
**GROW** @depth 1: [94,0], n=(485,409)
**GROW** @depth 2: [76,0.547536], n=(179,306)
r=1000 d=[0] [0] [0] [0]; n=(203,179,306,409)
r=2000 d=[0] [0] [0] [0]; n=(203,182,303,409)

Sampling @ nn=0 pred locs:
r=1000 d=[0] [0] [0] [0]; mh=4 n=(203,185,300,409)
r=2000 d=[0] [0] [0] [0]; mh=4 n=(203,181,304,409)
**GROW** @depth 2: [152,0], n=(176,233)
r=3000 d=[0] [0] [0] [0] [0]; mh=4 n=(203,189,296,176,233)
r=4000 d=[0] [0] [0] [0] [0]; mh=4 n=(203,192,293,176,233)
r=5000 d=[0] [0] [0] [0] [0]; mh=4 n=(203,190,295,176,233)
Grow: 1.072%, Prune: 0%, Change: 16.47%, Swap: 0%

Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.bcart: Error in predict.tgp(.model$learner.model, XX = .newdata, pred.n = FALSE,  : 
  mismatched column dimension of object$X and XX

[1] "Sat Jan 13 12:02:51 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Sat Jan 13 12:03:00 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

Warning in train(allmodel, regr.task) :
  Could not train learner regr.blm: Error in tgp(X, Z, XX, BTE, R, m0r1, FALSE, params, itemps, pred.n, krige,  : 
  X[,1:166]-matrix is not of full rank

[1] "Sat Jan 13 12:03:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 336 
Nguyen-Widrow method
Scaling factor= 0.7004424 
gamma= 210.3412 	 alpha= 78.6321 	 beta= 100.392 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.brnn: Error in eval(predvars, data, env) : object 'V138.dummy' not found

[1] "Sat Jan 13 12:03:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Sat Jan 13 12:03:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

Warning in train(allmodel, regr.task) :
  Could not train learner regr.btlm: Error in tgp(X, Z, XX, BTE, R, m0r1, FALSE, params, itemps, pred.n, krige,  : 
  X[,1:166]-matrix is not of full rank

[1] "Sat Jan 13 12:03:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cforest no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.cforest: Error in eval(predvars, data, env) : object 'V138.dummy' not found

[1] "Sat Jan 13 12:04:10 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.ctree: Error in eval(predvars, data, env) : object 'V138.dummy' not found

[1] "Sat Jan 13 12:04:24 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.cubist: Error in `[.data.frame`(newdata, , object$vars$all, drop = FALSE) : 
  undefined columns selected

[1] "Sat Jan 13 12:04:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.cvglmnet: Error in cbind2(1, newx) %*% nbeta : 
  Cholmod error 'X and/or Y have wrong dimensions' at file ../MatrixOps/cholmod_sdmult.c, line 90

[1] "Sat Jan 13 12:04:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Sat Jan 13 12:04:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.elmNN: Error in inpweight %*% TV.P : non-conformable arguments

[1] "Sat Jan 13 12:05:02 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
[Tune] Started tuning learner regr.extraTrees for parameter set:
                 Type len Def   Constr Req Tunable Trafo
mtry          integer   -  55 1 to 166   -    TRUE     -
numRandomCuts integer   -   1  1 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=4; numRandomCuts=20
[Tune-y] 1: rmse.test.rmse=0.158; time: 0.9 min
[Tune-x] 2: mtry=113; numRandomCuts=19
[Tune-y] 2: rmse.test.rmse=0.151; time: 11.0 min
[Tune-x] 3: mtry=56; numRandomCuts=1
[Tune-y] 3: rmse.test.rmse=0.153; time: 0.7 min
[Tune-x] 4: mtry=106; numRandomCuts=14
[Tune-y] 4: rmse.test.rmse=0.15; time: 7.8 min
[Tune-x] 5: mtry=113; numRandomCuts=13
[Tune-y] 5: rmse.test.rmse=0.15; time: 8.4 min
[Tune-x] 6: mtry=144; numRandomCuts=14
[Tune-y] 6: rmse.test.rmse=0.157; time: 9.3 min
[Tune-x] 7: mtry=101; numRandomCuts=10
[Tune-y] 7: rmse.test.rmse=0.15; time: 5.6 min
[Tune-x] 8: mtry=85; numRandomCuts=10
[Tune-y] 8: rmse.test.rmse=0.147; time: 4.9 min
[Tune-x] 9: mtry=77; numRandomCuts=21
[Tune-y] 9: rmse.test.rmse=0.147; time: 8.9 min
[Tune-x] 10: mtry=71; numRandomCuts=25
[Tune-y] 10: rmse.test.rmse=0.147; time: 11.3 min
[Tune-x] 11: mtry=78; numRandomCuts=5
Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: GC overhead limit exceeded

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: GC overhead limit exceeded

[Tune-y] 11: rmse.test.rmse=  NA; time: 2.2 min
[Tune-x] 12: mtry=104; numRandomCuts=24
[Tune-y] 12: rmse.test.rmse=0.15; time: 12.8 min
[Tune-x] 13: mtry=28; numRandomCuts=5
[Tune-y] 13: rmse.test.rmse=0.145; time: 1.0 min
[Tune-x] 14: mtry=116; numRandomCuts=24
[Tune-y] 14: rmse.test.rmse=0.152; time: 13.8 min
[Tune-x] 15: mtry=118; numRandomCuts=14
[Tune-y] 15: rmse.test.rmse=0.152; time: 8.4 min
[Tune-x] 16: mtry=110; numRandomCuts=25
[Tune-y] 16: rmse.test.rmse=0.151; time: 14.1 min
[Tune-x] 17: mtry=75; numRandomCuts=17
Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: Java heap space

[Tune-y] 17: rmse.test.rmse=  NA; time: 45.7 min
[Tune-x] 18: mtry=37; numRandomCuts=22
Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: Java heap space

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: Java heap space

[Tune-y] 18: rmse.test.rmse=  NA; time: 36.4 min
[Tune-x] 19: mtry=119; numRandomCuts=20
[Tune-y] 19: rmse.test.rmse=0.152; time: 11.9 min
[Tune-x] 20: mtry=79; numRandomCuts=20
[Tune-y] 20: rmse.test.rmse=0.147; time: 8.7 min
[Tune] Result: mtry=28; numRandomCuts=5 : rmse.test.rmse=0.145
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.extraTrees: Error in predict.extraTrees(.model$learner.model, as.matrix(.newdata),  : 
  newdata(ncol=160) does not have the same dimensions as the original x (ncol=166)

[1] "Sat Jan 13 15:49:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Sat Jan 13 15:50:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.fnn: Error in get.knnx(train, test, k, algorithm) : 
  Number of columns must be same!.

[1] "Sat Jan 13 15:50:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.frbs no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.frbs: Error in rep(1:num.varinput, num.labels.input) : invalid 'times' argument

[1] "Sat Jan 13 15:51:21 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
In addition: Warning messages:
1: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
2: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
3: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
4: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
5: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
6: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.gausspr: Error in eval(predvars, data, env) : object 'V138.dummy' not found

[1] "Sat Jan 13 15:51:40 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=11; interaction.depth=8; shrinkage=0.407; n.minobsinnode=20
[Tune-y] 1: rmse.test.rmse=0.156; time: 0.0 min
[Tune-x] 2: n.trees=47; interaction.depth=1; shrinkage=0.381; n.minobsinnode=16
[Tune-y] 2: rmse.test.rmse=0.156; time: 0.0 min
[Tune-x] 3: n.trees=227; interaction.depth=6; shrinkage=0.52; n.minobsinnode=16
[Tune-y] 3: rmse.test.rmse=0.18; time: 0.2 min
[Tune-x] 4: n.trees=163; interaction.depth=4; shrinkage=0.306; n.minobsinnode=13
[Tune-y] 4: rmse.test.rmse=0.155; time: 0.1 min
[Tune-x] 5: n.trees=84; interaction.depth=9; shrinkage=0.256; n.minobsinnode=25
[Tune-y] 5: rmse.test.rmse=0.153; time: 0.2 min
[Tune-x] 6: n.trees=86; interaction.depth=2; shrinkage=0.375; n.minobsinnode=24
[Tune-y] 6: rmse.test.rmse=0.152; time: 0.1 min
[Tune-x] 7: n.trees=22; interaction.depth=2; shrinkage=0.419; n.minobsinnode=24
[Tune-y] 7: rmse.test.rmse=0.156; time: 0.0 min
[Tune-x] 8: n.trees=264; interaction.depth=6; shrinkage=0.395; n.minobsinnode=25
[Tune-y] 8: rmse.test.rmse=0.17; time: 0.3 min
[Tune-x] 9: n.trees=80; interaction.depth=7; shrinkage=0.132; n.minobsinnode=22
[Tune-y] 9: rmse.test.rmse=0.141; time: 0.1 min
[Tune-x] 10: n.trees=266; interaction.depth=8; shrinkage=0.284; n.minobsinnode=21
[Tune-y] 10: rmse.test.rmse=0.16; time: 0.4 min
[Tune-x] 11: n.trees=11; interaction.depth=3; shrinkage=0.102; n.minobsinnode=9
[Tune-y] 11: rmse.test.rmse=0.241; time: 0.0 min
[Tune-x] 12: n.trees=24; interaction.depth=9; shrinkage=0.379; n.minobsinnode=16
[Tune-y] 12: rmse.test.rmse=0.155; time: 0.1 min
[Tune-x] 13: n.trees=19; interaction.depth=9; shrinkage=0.475; n.minobsinnode=25
[Tune-y] 13: rmse.test.rmse=0.166; time: 0.0 min
[Tune-x] 14: n.trees=77; interaction.depth=3; shrinkage=0.363; n.minobsinnode=5
[Tune-y] 14: rmse.test.rmse=0.158; time: 0.1 min
[Tune-x] 15: n.trees=54; interaction.depth=9; shrinkage=0.0751; n.minobsinnode=10
[Tune-y] 15: rmse.test.rmse=0.139; time: 0.1 min
[Tune-x] 16: n.trees=526; interaction.depth=4; shrinkage=0.219; n.minobsinnode=11
[Tune-y] 16: rmse.test.rmse=0.15; time: 0.4 min
[Tune-x] 17: n.trees=22; interaction.depth=10; shrinkage=0.552; n.minobsinnode=16
[Tune-y] 17: rmse.test.rmse=0.177; time: 0.1 min
[Tune-x] 18: n.trees=57; interaction.depth=6; shrinkage=0.111; n.minobsinnode=22
[Tune-y] 18: rmse.test.rmse=0.14; time: 0.1 min
[Tune-x] 19: n.trees=22; interaction.depth=5; shrinkage=0.297; n.minobsinnode=22
[Tune-y] 19: rmse.test.rmse=0.153; time: 0.0 min
[Tune-x] 20: n.trees=24; interaction.depth=3; shrinkage=0.359; n.minobsinnode=19
[Tune-y] 20: rmse.test.rmse=0.154; time: 0.0 min
[Tune] Result: n.trees=54; interaction.depth=9; shrinkage=0.0751; n.minobsinnode=10 : rmse.test.rmse=0.139
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.gbm: Error in eval(predvars, data, env) : object 'V138.dummy' not found

[1] "Sat Jan 13 15:54:16 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.glm: Error in eval(predvars, data, env) : object 'V138.dummy' not found

[1] "Sat Jan 13 15:54:26 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.0227; lambda=1.1
[Tune-y] 1: rmse.test.rmse=0.189; time: 0.0 min
[Tune-x] 2: alpha=0.678; lambda=0.852
[Tune-y] 2: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 3: alpha=0.336; lambda=0.00139
[Tune-y] 3: rmse.test.rmse=0.146; time: 0.0 min
[Tune-x] 4: alpha=0.634; lambda=0.117
[Tune-y] 4: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 5: alpha=0.678; lambda=0.0983
[Tune-y] 5: rmse.test.rmse=0.192; time: 0.0 min
[Tune-x] 6: alpha=0.867; lambda=0.116
[Tune-y] 6: rmse.test.rmse=0.219; time: 0.0 min
[Tune-x] 7: alpha=0.607; lambda=0.0294
[Tune-y] 7: rmse.test.rmse=0.146; time: 0.0 min
[Tune-x] 8: alpha=0.509; lambda=0.0313
[Tune-y] 8: rmse.test.rmse=0.144; time: 0.0 min
[Tune-x] 9: alpha=0.461; lambda=1.45
[Tune-y] 9: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 10: alpha=0.426; lambda=6.55
[Tune-y] 10: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 11: alpha=0.466; lambda=0.00587
[Tune-y] 11: rmse.test.rmse=0.141; time: 0.0 min
[Tune-x] 12: alpha=0.625; lambda=5.2
[Tune-y] 12: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 13: alpha=0.167; lambda=0.00531
[Tune-y] 13: rmse.test.rmse=0.143; time: 0.0 min
[Tune-x] 14: alpha=0.698; lambda=4.53
[Tune-y] 14: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 15: alpha=0.71; lambda=0.126
[Tune-y] 15: rmse.test.rmse=0.212; time: 0.0 min
[Tune-x] 16: alpha=0.657; lambda=5.87
[Tune-y] 16: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 17: alpha=0.452; lambda=0.425
[Tune-y] 17: rmse.test.rmse=0.319; time: 0.0 min
[Tune-x] 18: alpha=0.218; lambda=1.96
[Tune-y] 18: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 19: alpha=0.712; lambda=1.14
[Tune-y] 19: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 20: alpha=0.472; lambda=1.09
[Tune-y] 20: rmse.test.rmse= 0.4; time: 0.0 min
[Tune] Result: alpha=0.466; lambda=0.00587 : rmse.test.rmse=0.141
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.glmnet: Error in cbind2(1, newx) %*% nbeta : 
  Cholmod error 'X and/or Y have wrong dimensions' at file ../MatrixOps/cholmod_sdmult.c, line 90

[1] "Sat Jan 13 15:54:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==============                                                        |  20%  |                                                                              |============================                                          |  40%  |                                                                              |==========================================                            |  60%  |                                                                              |========================================================              |  80%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 15:55:27 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
In addition: Warning messages:
1: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V138.dummy': substituting in a column of NaN
2: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V139.dummy': substituting in a column of NaN
3: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V140.dummy': substituting in a column of NaN
4: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V141.dummy': substituting in a column of NaN
5: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V198.dummy': substituting in a column of NaN
6: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V202.dummy': substituting in a column of NaN
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   6%  |                                                                              |=========================                                             |  36%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 15:55:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
In addition: Warning messages:
1: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V138.dummy': substituting in a column of NaN
2: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V139.dummy': substituting in a column of NaN
3: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V140.dummy': substituting in a column of NaN
4: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V141.dummy': substituting in a column of NaN
5: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V198.dummy': substituting in a column of NaN
6: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V202.dummy': substituting in a column of NaN
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 15:56:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
In addition: Warning messages:
1: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V138.dummy': substituting in a column of NaN
2: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V139.dummy': substituting in a column of NaN
3: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V140.dummy': substituting in a column of NaN
4: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V141.dummy': substituting in a column of NaN
5: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V198.dummy': substituting in a column of NaN
6: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V202.dummy': substituting in a column of NaN
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |=============                                                         |  18%  |                                                                              |====================                                                  |  28%  |                                                                              |============================                                          |  40%  |                                                                              |====================================                                  |  52%  |                                                                              |=============================================                         |  64%  |                                                                              |=======================================================               |  78%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 15:56:31 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
In addition: Warning messages:
1: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V138.dummy': substituting in a column of NaN
2: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V139.dummy': substituting in a column of NaN
3: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V140.dummy': substituting in a column of NaN
4: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V141.dummy': substituting in a column of NaN
5: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V198.dummy': substituting in a column of NaN
6: In doTryCatch(return(expr), name, parentenv, handler) :
  Test/Validation dataset is missing column 'V202.dummy': substituting in a column of NaN
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  2 2 2 2 2 1.995888 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1.998629 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1.998629 2 1.998629 2 2 2 2 2 2 2 2 1.987663 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 
  - best initial criterion value(s) :  -541.2709 

N = 166, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       541.27  |proj g|=        1.603
At iterate     1  f =       437.55  |proj g|=        1.6636
ys=-2.627e+01  -gs= 7.278e+01, BFGS update SKIPPED
At iterate     2  f =       272.33  |proj g|=         1.666
At iterate     3  f =       266.42  |proj g|=        1.6325
At iterate     4  f =       246.36  |proj g|=        1.3252
At iterate     5  f =       233.26  |proj g|=        1.5561
At iterate     6  f =       219.16  |proj g|=        1.4964
At iterate     7  f =       211.36  |proj g|=        1.0913
At iterate     8  f =       206.13  |proj g|=        1.1525
At iterate     9  f =       203.01  |proj g|=        1.3281
At iterate    10  f =       201.82  |proj g|=       0.95077
ys=-3.908e-02  -gs= 1.151e+00, BFGS update SKIPPED
At iterate    11  f =       200.54  |proj g|=       0.65831
ys=-2.571e-01  -gs= 1.135e+00, BFGS update SKIPPED
At iterate    12  f =       198.69  |proj g|=       0.36703
ys=-1.856e-01  -gs= 1.734e+00, BFGS update SKIPPED
At iterate    13  f =       196.63  |proj g|=       0.30161
At iterate    14  f =       196.33  |proj g|=       0.27152
At iterate    15  f =       196.26  |proj g|=       0.10237
At iterate    16  f =       196.23  |proj g|=       0.20454
At iterate    17  f =       196.18  |proj g|=        0.0761
At iterate    18  f =       196.16  |proj g|=      0.042935
At iterate    19  f =       196.15  |proj g|=      0.038816
At iterate    20  f =       196.15  |proj g|=      0.033536
At iterate    21  f =       196.12  |proj g|=       0.03862
At iterate    22  f =       196.11  |proj g|=       0.03475
At iterate    23  f =       193.48  |proj g|=        1.5831
ys=-6.598e+00  -gs= 4.040e-02, BFGS update SKIPPED
At iterate    24  f =       171.38  |proj g|=       0.49195
At iterate    25  f =       122.05  |proj g|=        1.4639
At iterate    26  f =       122.02  |proj g|=        1.4463
At iterate    27  f =       104.25  |proj g|=        1.4229
At iterate    28  f =       103.79  |proj g|=         1.166
At iterate    29  f =       103.62  |proj g|=       0.36719
At iterate    30  f =       103.61  |proj g|=      0.025362
At iterate    31  f =       103.61  |proj g|=       0.01097
At iterate    32  f =       103.61  |proj g|=      0.016395
At iterate    33  f =       103.61  |proj g|=      0.069835
At iterate    34  f =       103.61  |proj g|=       0.13216
At iterate    35  f =       103.61  |proj g|=       0.23503
At iterate    36  f =        103.6  |proj g|=       0.32777
At iterate    37  f =        103.6  |proj g|=       0.23805
At iterate    38  f =        103.6  |proj g|=     0.0069594
At iterate    39  f =        103.6  |proj g|=     0.0067964
At iterate    40  f =        103.6  |proj g|=     0.0045336
At iterate    41  f =        103.6  |proj g|=     0.0014906
At iterate    42  f =        103.6  |proj g|=    0.00064793
At iterate    43  f =        103.6  |proj g|=     0.0039332
At iterate    44  f =        103.6  |proj g|=     0.0059345
At iterate    45  f =        103.6  |proj g|=     0.0045865
At iterate    46  f =        103.6  |proj g|=      0.001494
At iterate    47  f =        103.6  |proj g|=    0.00015377

iterations 47
function evaluations 64
segments explored during Cauchy searches 185
BFGS updates skipped 5
active bounds at final generalized Cauchy point 146
norm of the final projected gradient 0.000153766
final function value 103.6

F = 103.6
final  value 103.599872 
converged
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.km: Error in checkNames(X1 = X, X2 = newdata, X1.name = "the design", X2.name = "newdata") : 
  the design and newdata must have the same numbers of columns

[1] "Sat Jan 13 17:14:47 2018"
[Tune] Started tuning learner regr.ksvm for parameter set:
         Type len  Def    Constr Req Tunable Trafo
C     numeric   -    0  -5 to 10   -    TRUE     Y
sigma numeric   - TRUE -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: C=999; sigma=0.00117
[Tune-y] 1: rmse.test.rmse=0.147; time: 0.2 min
[Tune-x] 2: C=26.3; sigma=0.02
[Tune-y] 2: rmse.test.rmse=0.183; time: 0.0 min
[Tune-x] 3: C=0.641; sigma=18.2
[Tune-y] 3: rmse.test.rmse= 0.4; time: 0.1 min
[Tune-x] 4: C=297; sigma=0.000127
[Tune-y] 4: rmse.test.rmse=0.137; time: 0.1 min
[Tune-x] 5: C=69.5; sigma=0.00163
[Tune-y] 5: rmse.test.rmse=0.137; time: 0.0 min
[Tune-x] 6: C=10.3; sigma=0.00415
[Tune-y] 6: rmse.test.rmse=0.136; time: 0.0 min
[Tune-x] 7: C=410; sigma=46.5
[Tune-y] 7: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 8: C=271; sigma=688
[Tune-y] 8: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 9: C=4.47; sigma=2.24
[Tune-y] 9: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 10: C=26.4; sigma=992
[Tune-y] 10: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 11: C=0.04; sigma=2.52
[Tune-y] 11: rmse.test.rmse=0.401; time: 0.0 min
[Tune-x] 12: C=0.156; sigma=4.97e-05
[Tune-y] 12: rmse.test.rmse=0.371; time: 0.0 min
[Tune-x] 13: C=561; sigma=0.0605
[Tune-y] 13: rmse.test.rmse=0.26; time: 0.0 min
[Tune-x] 14: C=0.575; sigma=2.84e+04
[Tune-y] 14: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 15: C=8.65; sigma=494
[Tune-y] 15: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 16: C=0.268; sigma=8.2
[Tune-y] 16: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 17: C=0.0326; sigma=0.000259
[Tune-y] 17: rmse.test.rmse=0.37; time: 0.0 min
[Tune-x] 18: C=6.69; sigma=1.94e+04
[Tune-y] 18: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 19: C=0.163; sigma=0.000549
[Tune-y] 19: rmse.test.rmse=0.248; time: 0.0 min
[Tune-x] 20: C=89.3; sigma=0.000153
[Tune-y] 20: rmse.test.rmse=0.136; time: 0.0 min
[Tune] Result: C=89.3; sigma=0.000153 : rmse.test.rmse=0.136
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.ksvm: Error in eval(predvars, data, env) : object 'V138.dummy' not found

[1] "Sat Jan 13 17:16:18 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
In addition: There were 40 warnings (use warnings() to see them)
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.laGP: Error in (function (X, Z, XX, start = 6, end = 50, d = NULL, g = 1/10000,  : 
  mismatch XX and X cols

[1] "Sat Jan 13 17:16:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.LiblineaRL2L1SVR: Error in predict.LiblineaR(.model$learner.model, newx = .newdata, ...) : 
  columns of 'test' and 'train' differ

[1] "Sat Jan 13 17:16:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.LiblineaRL2L2SVR: Error in predict.LiblineaR(.model$learner.model, newx = .newdata, ...) : 
  columns of 'test' and 'train' differ

[1] "Sat Jan 13 17:16:57 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.lm: Error in eval(predvars, data, env) : object 'V138.dummy' not found

[1] "Sat Jan 13 17:17:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Sat Jan 13 17:18:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
Error in root.matrix(crossprod(process)) : 
  matrix is not positive semidefinite
Warning in train(allmodel, regr.task) :
  Could not train learner regr.mob: Error in trainLearner.regr.mob(.learner = structure(list(id = "regr.mob",  : 
  Failed to fit party::mob. Some coefficients are estimated as NA

[1] "Sat Jan 13 17:18:22 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=1; decay=0.478
# weights:  169
initial  value 111967.271833 
iter  10 value 186.488360
iter  20 value 150.730911
iter  30 value 105.458197
iter  40 value 76.351028
iter  50 value 63.723322
iter  60 value 59.349636
iter  70 value 55.841271
iter  80 value 54.311265
iter  90 value 53.531472
iter 100 value 53.012630
final  value 53.012630 
stopped after 100 iterations
# weights:  169
initial  value 114372.703763 
iter  10 value 153.956026
iter  20 value 128.364123
iter  30 value 122.923389
iter  40 value 117.211558
iter  50 value 112.886113
iter  60 value 108.674496
iter  70 value 93.599677
iter  80 value 77.449999
iter  90 value 68.007384
iter 100 value 58.812896
final  value 58.812896 
stopped after 100 iterations
# weights:  169
initial  value 99519.597086 
iter  10 value 135.933063
iter  20 value 108.051561
iter  30 value 89.484853
iter  40 value 80.428398
iter  50 value 71.375056
iter  60 value 65.474209
iter  70 value 58.650334
iter  80 value 55.282583
iter  90 value 52.740576
iter 100 value 51.483022
final  value 51.483022 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=0.17; time: 0.0 min
[Tune-x] 2: size=14; decay=0.322
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

[Tune-y] 2: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 3: size=7; decay=1.71e-05
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1177) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1177) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1177) weights

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: size=13; decay=0.0154
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2185) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2185) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2185) weights

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: size=14; decay=0.0118
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: size=18; decay=0.0151
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3025) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3025) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (3025) weights

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: size=13; decay=0.00185
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2185) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2185) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2185) weights

[Tune-y] 7: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 8: size=11; decay=0.00204
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1849) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1849) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1849) weights

[Tune-y] 8: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 9: size=10; decay=0.729
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: size=9; decay=7.35
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1513) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1513) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1513) weights

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: size=10; decay=0.000156
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

[Tune-y] 11: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 12: size=13; decay=5.17
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2185) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2185) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2185) weights

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: size=4; decay=0.000134
# weights:  673
initial  value 111950.523412 
iter  10 value 101.816463
iter  20 value 65.174120
iter  30 value 44.285022
iter  40 value 32.112593
iter  50 value 26.836070
iter  60 value 22.760942
iter  70 value 19.850592
iter  80 value 18.270677
iter  90 value 15.666932
iter 100 value 13.035548
final  value 13.035548 
stopped after 100 iterations
# weights:  673
initial  value 99553.087873 
iter  10 value 62.185816
iter  20 value 38.796835
iter  30 value 32.784366
iter  40 value 25.561512
iter  50 value 19.827464
iter  60 value 17.239753
iter  70 value 16.774189
iter  80 value 16.490133
iter  90 value 16.053968
iter 100 value 15.524047
final  value 15.524047 
stopped after 100 iterations
# weights:  673
initial  value 113646.191988 
iter  10 value 36.560231
iter  20 value 20.447704
iter  30 value 16.243573
iter  40 value 13.960497
iter  50 value 13.399896
iter  60 value 12.565801
iter  70 value 12.097832
iter  80 value 12.019423
iter  90 value 11.981567
iter 100 value 11.957236
final  value 11.957236 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.176; time: 0.1 min
[Tune-x] 14: size=14; decay=4.19
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

[Tune-y] 14: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 15: size=15; decay=0.0172
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2521) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2521) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2521) weights

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: size=14; decay=6.22
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2353) weights

[Tune-y] 16: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 17: size=10; decay=0.111
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

[Tune-y] 17: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 18: size=5; decay=1.16
# weights:  841
initial  value 86011.466653 
iter  10 value 466.485114
iter  20 value 237.811054
iter  30 value 114.307922
iter  40 value 71.807112
iter  50 value 66.607408
iter  60 value 64.785155
iter  70 value 64.064128
iter  80 value 63.040409
iter  90 value 62.158626
iter 100 value 61.060788
final  value 61.060788 
stopped after 100 iterations
# weights:  841
initial  value 115335.642492 
iter  10 value 405.003717
iter  20 value 341.628699
iter  30 value 215.161898
iter  40 value 112.702968
iter  50 value 103.438204
iter  60 value 99.147371
iter  70 value 94.801420
iter  80 value 89.583206
iter  90 value 86.095740
iter 100 value 83.720169
final  value 83.720169 
stopped after 100 iterations
# weights:  841
initial  value 99002.120586 
iter  10 value 172.490718
iter  20 value 95.308100
iter  30 value 80.953332
iter  40 value 74.628937
iter  50 value 72.214782
iter  60 value 70.927765
iter  70 value 69.331845
iter  80 value 68.221733
iter  90 value 67.005540
iter 100 value 66.170786
final  value 66.170786 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.146; time: 0.2 min
[Tune-x] 19: size=15; decay=0.507
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2521) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2521) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (2521) weights

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: size=10; decay=0.473
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.nnet: Error in nnet.default(x, y, w, ...) : too many (1681) weights

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: size=5; decay=1.16 : rmse.test.rmse=0.146
# weights:  841
initial  value 168375.986616 
iter  10 value 2810.747032
iter  20 value 905.655976
iter  30 value 765.530930
iter  40 value 702.859671
iter  50 value 637.073249
iter  60 value 553.728431
iter  70 value 510.338689
iter  80 value 473.384103
iter  90 value 444.281515
iter 100 value 383.583109
final  value 383.583109 
stopped after 100 iterations
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.nnet: Error in eval(predvars, data, env) : object 'V138.dummy' not found

[1] "Sat Jan 13 17:18:58 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 523 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 279
 number of selected nodes                               : 57 
[1] "Sat Jan 13 17:19:19 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.pcr: Error in eval(predvars, data, env) : object 'V138.dummy' not found

[1] "Sat Jan 13 17:19:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.plsr: Error in eval(predvars, data, env) : object 'V138.dummy' not found

[1] "Sat Jan 13 17:20:09 2018"
[Tune] Started tuning learner regr.randomForest for parameter set:
            Type len Def   Constr Req Tunable Trafo
nodesize integer   -   1  1 to 10   -    TRUE     -
mtry     integer   -  55 1 to 166   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nodesize=1; mtry=130
[Tune-y] 1: rmse.test.rmse=0.148; time: 3.7 min
[Tune-x] 2: nodesize=7; mtry=125
[Tune-y] 2: rmse.test.rmse=0.148; time: 1.7 min
[Tune-x] 3: nodesize=4; mtry=7
[Tune-y] 3: rmse.test.rmse=0.157; time: 0.2 min
[Tune-x] 4: nodesize=7; mtry=89
[Tune-y] 4: rmse.test.rmse=0.146; time: 1.2 min
[Tune-x] 5: nodesize=7; mtry=85
[Tune-y] 5: rmse.test.rmse=0.146; time: 1.2 min
[Tune-x] 6: nodesize=9; mtry=88
[Tune-y] 6: rmse.test.rmse=0.147; time: 1.0 min
[Tune-x] 7: nodesize=7; mtry=63
[Tune-y] 7: rmse.test.rmse=0.146; time: 0.9 min
[Tune-x] 8: nodesize=6; mtry=64
[Tune-y] 8: rmse.test.rmse=0.145; time: 0.9 min
[Tune-x] 9: nodesize=5; mtry=135
[Tune-y] 9: rmse.test.rmse=0.149; time: 2.1 min
[Tune-x] 10: nodesize=5; mtry=163
[Tune-y] 10: rmse.test.rmse=0.151; time: 2.4 min
[Tune-x] 11: nodesize=5; mtry=34
[Tune-y] 11: rmse.test.rmse=0.146; time: 0.6 min
[Tune-x] 12: nodesize=7; mtry=159
[Tune-y] 12: rmse.test.rmse=0.151; time: 2.1 min
[Tune-x] 13: nodesize=2; mtry=32
[Tune-y] 13: rmse.test.rmse=0.146; time: 0.8 min
[Tune-x] 14: nodesize=7; mtry=156
[Tune-y] 14: rmse.test.rmse=0.15; time: 2.0 min
[Tune-x] 15: nodesize=8; mtry=90
[Tune-y] 15: rmse.test.rmse=0.147; time: 1.1 min
[Tune-x] 16: nodesize=7; mtry=161
[Tune-y] 16: rmse.test.rmse=0.151; time: 2.1 min
[Tune-x] 17: nodesize=5; mtry=112
[Tune-y] 17: rmse.test.rmse=0.148; time: 1.7 min
[Tune-x] 18: nodesize=3; mtry=141
[Tune-y] 18: rmse.test.rmse=0.15; time: 2.7 min
[Tune-x] 19: nodesize=8; mtry=131
[Tune-y] 19: rmse.test.rmse=0.149; time: 1.6 min
[Tune-x] 20: nodesize=5; mtry=130
[Tune-y] 20: rmse.test.rmse=0.149; time: 2.0 min
[Tune] Result: nodesize=6; mtry=64 : rmse.test.rmse=0.145
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.randomForest: Error in predict.randomForest(.model$learner.model, newdata = .newdata,  : 
  variables in the training data missing in newdata

[1] "Sat Jan 13 17:53:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.randomForestSRC: Error in generic.predict.rfsrc(object, newdata, outcome.target = outcome.target,  : 
  x-variables in test data do not match original training data

[1] "Sat Jan 13 17:53:31 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def   Constr Req Tunable Trafo
mtry          integer   -  55 1 to 166   -    TRUE     -
min.node.size integer   -   5  1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=4; min.node.size=8
[Tune-y] 1: rmse.test.rmse=0.17; time: 0.1 min
[Tune-x] 2: mtry=113; min.node.size=8
[Tune-y] 2: rmse.test.rmse=0.148; time: 0.7 min
[Tune-x] 3: mtry=56; min.node.size=1
[Tune-y] 3: rmse.test.rmse=0.145; time: 0.6 min
[Tune-x] 4: mtry=106; min.node.size=6
[Tune-y] 4: rmse.test.rmse=0.147; time: 0.7 min
[Tune-x] 5: mtry=113; min.node.size=6
[Tune-y] 5: rmse.test.rmse=0.147; time: 0.7 min
[Tune-x] 6: mtry=144; min.node.size=6
[Tune-y] 6: rmse.test.rmse=0.15; time: 0.9 min
[Tune-x] 7: mtry=101; min.node.size=4
[Tune-y] 7: rmse.test.rmse=0.147; time: 0.8 min
[Tune-x] 8: mtry=85; min.node.size=4
[Tune-y] 8: rmse.test.rmse=0.146; time: 0.6 min
[Tune-x] 9: mtry=77; min.node.size=9
[Tune-y] 9: rmse.test.rmse=0.147; time: 0.4 min
[Tune-x] 10: mtry=71; min.node.size=10
[Tune-y] 10: rmse.test.rmse=0.146; time: 0.4 min
[Tune-x] 11: mtry=78; min.node.size=2
[Tune-y] 11: rmse.test.rmse=0.145; time: 0.7 min
[Tune-x] 12: mtry=104; min.node.size=10
[Tune-y] 12: rmse.test.rmse=0.147; time: 0.6 min
[Tune-x] 13: mtry=28; min.node.size=2
[Tune-y] 13: rmse.test.rmse=0.146; time: 0.3 min
[Tune-x] 14: mtry=116; min.node.size=10
[Tune-y] 14: rmse.test.rmse=0.148; time: 0.6 min
[Tune-x] 15: mtry=118; min.node.size=6
[Tune-y] 15: rmse.test.rmse=0.148; time: 0.7 min
[Tune-x] 16: mtry=110; min.node.size=10
[Tune-y] 16: rmse.test.rmse=0.148; time: 0.6 min
[Tune-x] 17: mtry=75; min.node.size=7
[Tune-y] 17: rmse.test.rmse=0.146; time: 0.5 min
[Tune-x] 18: mtry=37; min.node.size=9
[Tune-y] 18: rmse.test.rmse=0.147; time: 0.2 min
[Tune-x] 19: mtry=119; min.node.size=8
[Tune-y] 19: rmse.test.rmse=0.148; time: 0.7 min
[Tune-x] 20: mtry=79; min.node.size=8
[Tune-y] 20: rmse.test.rmse=0.145; time: 0.5 min
[Tune] Result: mtry=56; min.node.size=1 : rmse.test.rmse=0.145
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.ranger: Error in `[.data.frame`(data, , forest$independent.variable.names, drop = FALSE) : 
  undefined columns selected

[1] "Sat Jan 13 18:05:15 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.rknn: Error in knn.reg(train = data[, fset], test = newdata[, fset], y = y,  : 
  too many ties in knn

[1] "Sat Jan 13 18:05:25 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.00114; maxdepth=24; minbucket=36; minsplit=39
[Tune-y] 1: rmse.test.rmse=0.211; time: 0.0 min
[Tune-x] 2: cp=0.0101; maxdepth=4; minbucket=34; minsplit=29
[Tune-y] 2: rmse.test.rmse=0.227; time: 0.0 min
[Tune-x] 3: cp=0.107; maxdepth=17; minbucket=44; minsplit=29
[Tune-y] 3: rmse.test.rmse=0.292; time: 0.0 min
[Tune-x] 4: cp=0.0655; maxdepth=13; minbucket=28; minsplit=22
[Tune-y] 4: rmse.test.rmse=0.257; time: 0.0 min
[Tune-x] 5: cp=0.0238; maxdepth=25; minbucket=24; minsplit=49
[Tune-y] 5: rmse.test.rmse=0.242; time: 0.0 min
[Tune-x] 6: cp=0.0248; maxdepth=8; minbucket=33; minsplit=48
[Tune-y] 6: rmse.test.rmse=0.242; time: 0.0 min
[Tune-x] 7: cp=0.00311; maxdepth=8; minbucket=37; minsplit=48
[Tune-y] 7: rmse.test.rmse=0.215; time: 0.0 min
[Tune-x] 8: cp=0.134; maxdepth=18; minbucket=35; minsplit=49
[Tune-y] 8: rmse.test.rmse=0.292; time: 0.0 min
[Tune-x] 9: cp=0.0223; maxdepth=21; minbucket=15; minsplit=43
[Tune-y] 9: rmse.test.rmse=0.242; time: 0.0 min
[Tune-x] 10: cp=0.136; maxdepth=24; minbucket=26; minsplit=40
[Tune-y] 10: rmse.test.rmse=0.292; time: 0.0 min
[Tune-x] 11: cp=0.00109; maxdepth=9; minbucket=12; minsplit=15
[Tune-y] 11: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 12: cp=0.00369; maxdepth=25; minbucket=34; minsplit=29
[Tune-y] 12: rmse.test.rmse=0.213; time: 0.0 min
[Tune-x] 13: cp=0.00259; maxdepth=27; minbucket=41; minsplit=49
[Tune-y] 13: rmse.test.rmse=0.219; time: 0.0 min
[Tune-x] 14: cp=0.0209; maxdepth=10; minbucket=32; minsplit=5
[Tune-y] 14: rmse.test.rmse=0.24; time: 0.0 min
[Tune-x] 15: cp=0.0124; maxdepth=25; minbucket=10; minsplit=17
[Tune-y] 15: rmse.test.rmse=0.229; time: 0.0 min
[Tune-x] 16: cp=0.38; maxdepth=11; minbucket=21; minsplit=19
[Tune-y] 16: rmse.test.rmse=0.292; time: 0.0 min
[Tune-x] 17: cp=0.00311; maxdepth=30; minbucket=47; minsplit=30
[Tune-y] 17: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 18: cp=0.0134; maxdepth=17; minbucket=13; minsplit=42
[Tune-y] 18: rmse.test.rmse=0.232; time: 0.0 min
[Tune-x] 19: cp=0.00311; maxdepth=15; minbucket=27; minsplit=44
[Tune-y] 19: rmse.test.rmse=0.211; time: 0.0 min
[Tune-x] 20: cp=0.00365; maxdepth=9; minbucket=32; minsplit=36
[Tune-y] 20: rmse.test.rmse=0.213; time: 0.0 min
[Tune] Result: cp=0.00109; maxdepth=9; minbucket=12; minsplit=15 : rmse.test.rmse= 0.2
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.rpart: Error in eval(predvars, data, env) : object 'V138.dummy' not found

[1] "Sat Jan 13 18:05:45 2018"
[Tune] Started tuning learner regr.RRF for parameter set:
           Type len Def   Constr Req Tunable Trafo
mtry    integer   -  55 1 to 166   -    TRUE     -
coefReg numeric   - 0.8   0 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=4; coefReg=0.78
[Tune-y] 1: rmse.test.rmse=0.169; time: 0.2 min
[Tune-x] 2: mtry=113; coefReg=0.751
[Tune-y] 2: rmse.test.rmse=0.147; time: 1.8 min
[Tune-x] 3: mtry=56; coefReg=0.0388
[Tune-y] 3: rmse.test.rmse=0.152; time: 1.0 min
[Tune-x] 4: mtry=106; coefReg=0.531
[Tune-y] 4: rmse.test.rmse=0.148; time: 1.6 min
[Tune-x] 5: mtry=113; coefReg=0.512
[Tune-y] 5: rmse.test.rmse=0.148; time: 1.8 min
[Tune-x] 6: mtry=144; coefReg=0.53
[Tune-y] 6: rmse.test.rmse=0.149; time: 2.2 min
[Tune-x] 7: mtry=101; coefReg=0.378
[Tune-y] 7: rmse.test.rmse=0.149; time: 1.5 min
[Tune-x] 8: mtry=85; coefReg=0.385
[Tune-y] 8: rmse.test.rmse=0.147; time: 1.3 min
[Tune-x] 9: mtry=77; coefReg=0.81
[Tune-y] 9: rmse.test.rmse=0.146; time: 1.2 min
[Tune-x] 10: mtry=71; coefReg=0.978
[Tune-y] 10: rmse.test.rmse=0.145; time: 1.1 min
[Tune-x] 11: mtry=78; coefReg=0.199
[Tune-y] 11: rmse.test.rmse=0.149; time: 1.2 min
[Tune-x] 12: mtry=104; coefReg=0.952
[Tune-y] 12: rmse.test.rmse=0.147; time: 1.6 min
[Tune-x] 13: mtry=28; coefReg=0.188
[Tune-y] 13: rmse.test.rmse=0.147; time: 0.5 min
[Tune-x] 14: mtry=116; coefReg=0.937
[Tune-y] 14: rmse.test.rmse=0.148; time: 1.8 min
[Tune-x] 15: mtry=118; coefReg=0.539
[Tune-y] 15: rmse.test.rmse=0.148; time: 1.8 min
[Tune-x] 16: mtry=110; coefReg=0.966
[Tune-y] 16: rmse.test.rmse=0.147; time: 1.8 min
[Tune-x] 17: mtry=75; coefReg=0.674
[Tune-y] 17: rmse.test.rmse=0.146; time: 1.2 min
[Tune-x] 18: mtry=37; coefReg=0.844
[Tune-y] 18: rmse.test.rmse=0.145; time: 0.6 min
[Tune-x] 19: mtry=119; coefReg=0.784
[Tune-y] 19: rmse.test.rmse=0.148; time: 1.8 min
[Tune-x] 20: mtry=79; coefReg=0.779
[Tune-y] 20: rmse.test.rmse=0.146; time: 1.2 min
[Tune] Result: mtry=71; coefReg=0.978 : rmse.test.rmse=0.145
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.RRF: Error in eval(predvars, data, env) : object 'V138.dummy' not found

[1] "Sat Jan 13 18:33:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.rsm: Error in lapply(X = X, FUN = FUN, ...) : object 'V138.dummy' not found

[1] "Sat Jan 13 18:33:59 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
In addition: Warning message:
In (function (formula, data, ...)  :
  Some coefficients are aliased - cannot use 'rsm' methods.
  Returning an 'lm' object.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.rvm: Error in eval(predvars, data, env) : object 'V138.dummy' not found

[1] "Sat Jan 13 18:35:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.7930 0.4300 0.2330 0.1260 0.0683
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 1 -----> 50 
Runtime: 32.46486 secs 
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.slim: Error in newdata %*% object$beta[, lambda.idx] : 
  non-conformable arguments

[1] "Sat Jan 13 18:36:30 2018"
[Tune] Started tuning learner regr.svm for parameter set:
         Type len   Def    Constr Req Tunable Trafo
cost  numeric   -     0 -15 to 15   -    TRUE     Y
gamma numeric   - -7.38 -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cost=4.89e-05; gamma=337
[Tune-y] 1: rmse.test.rmse=0.401; time: 0.1 min
[Tune-x] 2: cost=40.6; gamma=186
[Tune-y] 2: rmse.test.rmse= 0.4; time: 0.1 min
[Tune-x] 3: cost=0.0333; gamma=6.84e-05
[Tune-y] 3: rmse.test.rmse=0.388; time: 0.1 min
[Tune-x] 4: cost=16.2; gamma=1.91
[Tune-y] 4: rmse.test.rmse= 0.4; time: 0.1 min
[Tune-x] 5: cost=40.6; gamma=1.28
[Tune-y] 5: rmse.test.rmse= 0.4; time: 0.1 min
[Tune-x] 6: cost=2.06e+03; gamma=1.86
[Tune-y] 6: rmse.test.rmse= 0.4; time: 0.1 min
[Tune-x] 7: cost=9.19; gamma=0.0786
[Tune-y] 7: rmse.test.rmse=0.342; time: 0.1 min
[Tune-x] 8: cost=1.21; gamma=0.0913
[Tune-y] 8: rmse.test.rmse=0.345; time: 0.1 min
[Tune-x] 9: cost=0.444; gamma=637
[Tune-y] 9: rmse.test.rmse=0.401; time: 0.1 min
[Tune-x] 10: cost=0.215; gamma=2.06e+04
[Tune-y] 10: rmse.test.rmse=0.401; time: 0.1 min
[Tune-x] 11: cost=0.498; gamma=0.00192
[Tune-y] 11: rmse.test.rmse=0.153; time: 0.1 min
[Tune-x] 12: cost=13.4; gamma=1.21e+04
[Tune-y] 12: rmse.test.rmse= 0.4; time: 0.1 min
[Tune-x] 13: cost=0.000984; gamma=0.00152
[Tune-y] 13: rmse.test.rmse=0.395; time: 0.1 min
[Tune-x] 14: cost=60.8; gamma=8.84e+03
[Tune-y] 14: rmse.test.rmse= 0.4; time: 0.1 min
[Tune-x] 15: cost=79.6; gamma=2.26
[Tune-y] 15: rmse.test.rmse= 0.4; time: 0.1 min
[Tune-x] 16: cost=26.3; gamma=1.6e+04
[Tune-y] 16: rmse.test.rmse= 0.4; time: 0.1 min
[Tune-x] 17: cost=0.365; gamma=37.4
[Tune-y] 17: rmse.test.rmse=0.401; time: 0.1 min
[Tune-x] 18: cost=0.00286; gamma=1.28e+03
[Tune-y] 18: rmse.test.rmse=0.401; time: 0.1 min
[Tune-x] 19: cost=82.4; gamma=369
[Tune-y] 19: rmse.test.rmse= 0.4; time: 0.1 min
[Tune-x] 20: cost=0.564; gamma=332
[Tune-y] 20: rmse.test.rmse=0.401; time: 0.1 min
[Tune] Result: cost=0.498; gamma=0.00192 : rmse.test.rmse=0.153
Warning in predict.WrappedModel(m, newdata = (testing)) :
  Could not predict with learner regr.svm: Error in eval(predvars, data, env) : object 'V138.dummy' not found

[1] "Sat Jan 13 18:38:27 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=11; max_depth=8; eta=0.407; gamma=7.51; colsample_bytree=0.435; min_child_weight=0.776; subsample=0.726
[Tune-y] 1: rmse.test.rmse=0.254; time: 0.0 min
[Tune-x] 2: nrounds=241; max_depth=7; eta=0.308; gamma=8.67; colsample_bytree=0.512; min_child_weight=12.1; subsample=0.533
[Tune-y] 2: rmse.test.rmse=0.249; time: 0.1 min
[Tune-x] 3: nrounds=211; max_depth=4; eta=0.277; gamma=8.1; colsample_bytree=0.47; min_child_weight=19.6; subsample=0.6
[Tune-y] 3: rmse.test.rmse=0.247; time: 0.1 min
[Tune-x] 4: nrounds=33; max_depth=7; eta=0.571; gamma=1.67; colsample_bytree=0.375; min_child_weight=14; subsample=0.953
[Tune-y] 4: rmse.test.rmse=0.195; time: 0.0 min
[Tune-x] 5: nrounds=706; max_depth=6; eta=0.395; gamma=9.66; colsample_bytree=0.481; min_child_weight=13.5; subsample=0.414
[Tune-y] 5: rmse.test.rmse=0.263; time: 0.3 min
[Tune-x] 6: nrounds=1.57e+03; max_depth=8; eta=0.471; gamma=4.72; colsample_bytree=0.612; min_child_weight=0.312; subsample=0.43
[Tune-y] 6: rmse.test.rmse=0.227; time: 1.4 min
[Tune-x] 7: nrounds=27; max_depth=3; eta=0.116; gamma=8.15; colsample_bytree=0.553; min_child_weight=10.8; subsample=0.355
[Tune-y] 7: rmse.test.rmse=0.517; time: 0.0 min
[Tune-x] 8: nrounds=1.84e+03; max_depth=8; eta=0.58; gamma=4.42; colsample_bytree=0.404; min_child_weight=12.1; subsample=0.259
[Tune-y] 8: rmse.test.rmse=0.227; time: 0.9 min
[Tune-x] 9: nrounds=90; max_depth=9; eta=0.0751; gamma=2.62; colsample_bytree=0.644; min_child_weight=6.22; subsample=0.524
[Tune-y] 9: rmse.test.rmse=0.203; time: 0.1 min
[Tune-x] 10: nrounds=69; max_depth=2; eta=0.586; gamma=9.19; colsample_bytree=0.518; min_child_weight=7.56; subsample=0.633
[Tune-y] 10: rmse.test.rmse=0.251; time: 0.0 min
[Tune-x] 11: nrounds=30; max_depth=9; eta=0.101; gamma=4.56; colsample_bytree=0.498; min_child_weight=17; subsample=0.393
[Tune-y] 11: rmse.test.rmse=0.541; time: 0.0 min
[Tune-x] 12: nrounds=42; max_depth=6; eta=0.416; gamma=4.82; colsample_bytree=0.483; min_child_weight=16.7; subsample=0.506
[Tune-y] 12: rmse.test.rmse=0.23; time: 0.0 min
[Tune-x] 13: nrounds=122; max_depth=2; eta=0.44; gamma=5.76; colsample_bytree=0.602; min_child_weight=15.8; subsample=0.51
[Tune-y] 13: rmse.test.rmse=0.23; time: 0.0 min
[Tune-x] 14: nrounds=568; max_depth=3; eta=0.115; gamma=9.23; colsample_bytree=0.654; min_child_weight=4.21; subsample=0.36
[Tune-y] 14: rmse.test.rmse=0.279; time: 0.2 min
[Tune-x] 15: nrounds=35; max_depth=6; eta=0.0555; gamma=9.58; colsample_bytree=0.409; min_child_weight=16.3; subsample=0.799
[Tune-y] 15: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 16: nrounds=2.31e+03; max_depth=8; eta=0.409; gamma=1.84; colsample_bytree=0.471; min_child_weight=2.83; subsample=0.699
[Tune-y] 16: rmse.test.rmse=0.178; time: 1.8 min
[Tune-x] 17: nrounds=734; max_depth=2; eta=0.0274; gamma=7.21; colsample_bytree=0.331; min_child_weight=6.07; subsample=0.345
[Tune-y] 17: rmse.test.rmse=0.273; time: 0.1 min
[Tune-x] 18: nrounds=54; max_depth=6; eta=0.572; gamma=0.052; colsample_bytree=0.613; min_child_weight=8.39; subsample=0.858
[Tune-y] 18: rmse.test.rmse=0.168; time: 0.0 min
[Tune-x] 19: nrounds=783; max_depth=1; eta=0.0645; gamma=4.02; colsample_bytree=0.426; min_child_weight=18.4; subsample=0.393
[Tune-y] 19: rmse.test.rmse=0.223; time: 0.1 min
[Tune-x] 20: nrounds=30; max_depth=1; eta=0.023; gamma=5.27; colsample_bytree=0.677; min_child_weight=12.7; subsample=0.482
[Tune-y] 20: rmse.test.rmse=5.76; time: 0.0 min
[Tune] Result: nrounds=54; max_depth=6; eta=0.572; gamma=0.052; colsample_bytree=0.613; min_child_weight=8.39; subsample=0.858 : rmse.test.rmse=0.168
[1] "Sat Jan 13 18:43:51 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
In addition: There were 20 warnings (use warnings() to see them)
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Sat Jan 13 18:44:01 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [5,0.510314], n=(40,36)
**PRUNE** @depth 0: [5,0.510314]
**GROW** @depth 0: [2,0.413882], n=(50,26)
**PRUNE** @depth 0: [2,0.445244]
**GROW** @depth 0: [8,0.623823], n=(53,23)
**PRUNE** @depth 0: [8,0.662429]
**GROW** @depth 0: [2,0.512339], n=(59,17)
**PRUNE** @depth 0: [2,0.512339]
**GROW** @depth 0: [5,0.510314], n=(40,36)
**PRUNE** @depth 0: [5,0.518834]
**GROW** @depth 0: [3,0.444066], n=(48,28)
**PRUNE** @depth 0: [3,0.444066]
**GROW** @depth 0: [2,0.413882], n=(50,26)
r=1000 d=[0] [0]; n=(21,55)
**PRUNE** @depth 0: [2,0.37892]
**GROW** @depth 0: [2,0.413882], n=(50,26)
**PRUNE** @depth 0: [2,0.458098]
**GROW** @depth 0: [4,0.5545], n=(50,26)
**PRUNE** @depth 0: [4,0.5545]
**GROW** @depth 0: [1,0.539441], n=(16,60)
**PRUNE** @depth 0: [1,0.539441]
**GROW** @depth 0: [3,0.629028], n=(60,16)
**PRUNE** @depth 0: [3,0.545717]
r=2000 d=[0]; n=76

Sampling @ nn=0 pred locs:
**GROW** @depth 0: [3,0.504323], n=(52,24)
**PRUNE** @depth 0: [3,0.517422]
**GROW** @depth 0: [2,0.579949], n=(63,13)
**PRUNE** @depth 0: [2,0.579949]
**GROW** @depth 0: [3,0.662562], n=(63,13)
**PRUNE** @depth 0: [3,0.644485]
**GROW** @depth 0: [3,0.535237], n=(54,22)
**PRUNE** @depth 0: [3,0.545717]
**GROW** @depth 0: [3,0.572963], n=(58,18)
**PRUNE** @depth 0: [3,0.597066]
**GROW** @depth 0: [5,0.510314], n=(40,36)
**PRUNE** @depth 0: [5,0.517713]
**GROW** @depth 0: [4,0.370018], n=(15,61)
**PRUNE** @depth 0: [4,0.381367]
r=1000 d=[0]; mh=2 n=76
**GROW** @depth 0: [3,0.517422], n=(53,23)
**PRUNE** @depth 0: [3,0.517422]
**GROW** @depth 0: [1,0.439606], n=(12,64)
**PRUNE** @depth 0: [1,0.520953]
**GROW** @depth 0: [1,0.524856], n=(15,61)
**PRUNE** @depth 0: [1,0.524856]
**GROW** @depth 0: [3,0.481006], n=(51,25)
**PRUNE** @depth 0: [3,0.535237]
**GROW** @depth 0: [3,0.629814], n=(61,15)
**PRUNE** @depth 0: [3,0.629028]
**GROW** @depth 0: [3,0.629814], n=(61,15)
r=2000 d=[0] [0]; mh=2 n=(59,17)
**PRUNE** @depth 0: [3,0.535237]
**GROW** @depth 0: [2,0.48072], n=(55,21)
**PRUNE** @depth 0: [2,0.48072]
**GROW** @depth 0: [2,0.413882], n=(50,26)
**PRUNE** @depth 0: [2,0.445244]
r=3000 d=[0]; mh=2 n=76
**GROW** @depth 0: [3,0.419701], n=(23,53)
**GROW** @depth 1: [1,0.637634], n=(18,42)
**PRUNE** @depth 1: [1,0.547658]
r=4000 d=[0] [0]; mh=3 n=(19,57)
**PRUNE** @depth 0: [3,0.393503]
**GROW** @depth 0: [2,0.413882], n=(50,26)
**GROW** @depth 1: [4,0.428873], n=(18,42)
**PRUNE** @depth 1: [4,0.428873]
**GROW** @depth 1: [3,0.444066], n=(36,22)
**PRUNE** @depth 1: [3,0.44328]
**PRUNE** @depth 0: [2,0.301285]
**GROW** @depth 0: [2,0.458098], n=(53,23)
**PRUNE** @depth 0: [2,0.477892]
**GROW** @depth 0: [1,0.671939], n=(53,23)
**PRUNE** @depth 0: [1,0.671939]
**GROW** @depth 0: [5,0.510314], n=(40,36)
**PRUNE** @depth 0: [5,0.517713]
r=5000 d=[0]; mh=3 n=76
Grow: 9.659%, Prune: 19.43%, Change: 67.13%, Swap: 72.73%

[1] "Sat Jan 13 18:46:04 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Sat Jan 13 18:46:10 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=76

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=76
r=2000 d=[0]; mh=1 n=76
r=3000 d=[0]; mh=1 n=76

[1] "Sat Jan 13 18:46:20 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7064135 
gamma= 14.5455 	 alpha= 1.8032 	 beta= 10.7333 
[1] "Sat Jan 13 18:46:26 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Sat Jan 13 18:46:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=76
r=2000 d=[0]; n=76

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=76
r=2000 d=[0]; mh=1 n=76
r=3000 d=[0]; mh=1 n=76
r=4000 d=[0]; mh=1 n=76
r=5000 d=[0]; mh=1 n=76
Grow: 0%, 

[1] "Sat Jan 13 18:46:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cforest no default is available.
[1] "Sat Jan 13 18:46:46 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Sat Jan 13 18:46:56 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Sat Jan 13 18:47:02 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Sat Jan 13 18:47:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Sat Jan 13 18:47:15 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Sat Jan 13 18:47:21 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
[Tune] Started tuning learner regr.extraTrees for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
numRandomCuts integer   -   1 1 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=9; numRandomCuts=18
[Tune-y] 1: rmse.test.rmse=1.62; time: 0.2 min
[Tune-x] 2: mtry=6; numRandomCuts=11
[Tune-y] 2: rmse.test.rmse=1.55; time: 0.1 min
[Tune-x] 3: mtry=9; numRandomCuts=3
[Tune-y] 3: rmse.test.rmse=1.55; time: 0.5 min
[Tune-x] 4: mtry=7; numRandomCuts=20
[Tune-y] 4: rmse.test.rmse=1.58; time: 0.1 min
[Tune-x] 5: mtry=4; numRandomCuts=20
[Tune-y] 5: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 6: mtry=6; numRandomCuts=16
[Tune-y] 6: rmse.test.rmse=1.55; time: 0.1 min
[Tune-x] 7: mtry=4; numRandomCuts=16
[Tune-y] 7: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 8: mtry=6; numRandomCuts=19
[Tune-y] 8: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 9: mtry=8; numRandomCuts=16
[Tune-y] 9: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 10: mtry=2; numRandomCuts=10
[Tune-y] 10: rmse.test.rmse=1.51; time: 0.1 min
[Tune-x] 11: mtry=4; numRandomCuts=7
[Tune-y] 11: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 12: mtry=1; numRandomCuts=9
[Tune-y] 12: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 13: mtry=6; numRandomCuts=23
[Tune-y] 13: rmse.test.rmse=1.55; time: 0.1 min
[Tune-x] 14: mtry=9; numRandomCuts=4
[Tune-y] 14: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 15: mtry=9; numRandomCuts=25
[Tune-y] 15: rmse.test.rmse=1.63; time: 0.1 min
[Tune-x] 16: mtry=7; numRandomCuts=13
[Tune-y] 16: rmse.test.rmse=1.58; time: 0.1 min
[Tune-x] 17: mtry=2; numRandomCuts=20
[Tune-y] 17: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 18: mtry=5; numRandomCuts=20
[Tune-y] 18: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 19: mtry=9; numRandomCuts=15
[Tune-y] 19: rmse.test.rmse= 1.6; time: 0.5 min
[Tune-x] 20: mtry=9; numRandomCuts=10
[Tune-y] 20: rmse.test.rmse= 1.6; time: 0.0 min
[Tune] Result: mtry=2; numRandomCuts=10 : rmse.test.rmse=1.51
[1] "Sat Jan 13 18:49:26 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Sat Jan 13 18:49:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Sat Jan 13 18:49:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.frbs no default is available.
[1] "Sat Jan 13 18:49:46 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
In addition: Warning messages:
1: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
2: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
3: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
4: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
5: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
6: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Sat Jan 13 18:49:55 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=892; interaction.depth=7; shrinkage=0.388; n.minobsinnode=13
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 1: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 2: n.trees=630; interaction.depth=2; shrinkage=0.428; n.minobsinnode=20
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 2: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 3: n.trees=75; interaction.depth=8; shrinkage=0.381; n.minobsinnode=18
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: n.trees=57; interaction.depth=7; shrinkage=0.349; n.minobsinnode=20
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: n.trees=431; interaction.depth=7; shrinkage=0.113; n.minobsinnode=12
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: n.trees=63; interaction.depth=3; shrinkage=0.0567; n.minobsinnode=12
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: n.trees=205; interaction.depth=10; shrinkage=0.587; n.minobsinnode=8
[Tune-y] 7: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 8: n.trees=682; interaction.depth=10; shrinkage=0.465; n.minobsinnode=15
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 8: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 9: n.trees=19; interaction.depth=8; shrinkage=0.273; n.minobsinnode=21
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: n.trees=658; interaction.depth=6; shrinkage=0.56; n.minobsinnode=13
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: n.trees=20; interaction.depth=4; shrinkage=0.325; n.minobsinnode=13
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 11: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 12: n.trees=577; interaction.depth=7; shrinkage=0.32; n.minobsinnode=14
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: n.trees=818; interaction.depth=7; shrinkage=0.297; n.minobsinnode=14
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: n.trees=197; interaction.depth=1; shrinkage=0.569; n.minobsinnode=15
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 14: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 15: n.trees=206; interaction.depth=3; shrinkage=0.459; n.minobsinnode=16
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: n.trees=13; interaction.depth=5; shrinkage=0.507; n.minobsinnode=25
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 16: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 17: n.trees=60; interaction.depth=5; shrinkage=0.522; n.minobsinnode=6
[Tune-y] 17: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 18: n.trees=235; interaction.depth=6; shrinkage=0.362; n.minobsinnode=20
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 18: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 19: n.trees=34; interaction.depth=4; shrinkage=0.497; n.minobsinnode=12
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: n.trees=78; interaction.depth=6; shrinkage=0.313; n.minobsinnode=10
[Tune-y] 20: rmse.test.rmse=1.62; time: 0.0 min
[Tune] Result: n.trees=205; interaction.depth=10; shrinkage=0.587; n.minobsinnode=8 : rmse.test.rmse=1.58
[1] "Sat Jan 13 18:50:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Sat Jan 13 18:50:09 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.975; lambda=0.529
[Tune-y] 1: rmse.test.rmse=1.66; time: 0.0 min
[Tune-x] 2: alpha=0.646; lambda=0.0441
[Tune-y] 2: rmse.test.rmse=0.974; time: 0.0 min
[Tune-x] 3: alpha=0.9; lambda=0.00268
[Tune-y] 3: rmse.test.rmse=0.982; time: 0.0 min
[Tune-x] 4: alpha=0.712; lambda=0.921
[Tune-y] 4: rmse.test.rmse=1.84; time: 0.0 min
[Tune-x] 5: alpha=0.439; lambda=1.2
[Tune-y] 5: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 6: alpha=0.634; lambda=0.269
[Tune-y] 6: rmse.test.rmse=1.09; time: 0.0 min
[Tune-x] 7: alpha=0.377; lambda=0.283
[Tune-y] 7: rmse.test.rmse=1.07; time: 0.0 min
[Tune-x] 8: alpha=0.581; lambda=0.87
[Tune-y] 8: rmse.test.rmse=1.69; time: 0.0 min
[Tune-x] 9: alpha=0.817; lambda=0.293
[Tune-y] 9: rmse.test.rmse=1.16; time: 0.0 min
[Tune-x] 10: alpha=0.187; lambda=0.0257
[Tune-y] 10: rmse.test.rmse=0.979; time: 0.0 min
[Tune-x] 11: alpha=0.4; lambda=0.00984
[Tune-y] 11: rmse.test.rmse=0.98; time: 0.0 min
[Tune-x] 12: alpha=0.093; lambda=0.0227
[Tune-y] 12: rmse.test.rmse=0.98; time: 0.0 min
[Tune-x] 13: alpha=0.656; lambda=3.27
[Tune-y] 13: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 14: alpha=0.978; lambda=0.0037
[Tune-y] 14: rmse.test.rmse=0.981; time: 0.0 min
[Tune-x] 15: alpha=0.917; lambda=7.26
[Tune-y] 15: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 16: alpha=0.775; lambda=0.0829
[Tune-y] 16: rmse.test.rmse=0.983; time: 0.0 min
[Tune-x] 17: alpha=0.143; lambda=1.14
[Tune-y] 17: rmse.test.rmse=1.37; time: 0.0 min
[Tune-x] 18: alpha=0.454; lambda=1.3
[Tune-y] 18: rmse.test.rmse=1.81; time: 0.0 min
[Tune-x] 19: alpha=0.909; lambda=0.168
[Tune-y] 19: rmse.test.rmse=1.04; time: 0.0 min
[Tune-x] 20: alpha=0.933; lambda=0.0339
[Tune-y] 20: rmse.test.rmse=0.973; time: 0.0 min
[Tune] Result: alpha=0.933; lambda=0.0339 : rmse.test.rmse=0.973
[1] "Sat Jan 13 18:50:21 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=================================================                     |  70%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 18:50:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 18:50:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=                                                                     |   2%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 18:50:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 18:51:03 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  9.736 7.78 7.634 7.578 8.92 9.47 10.93 8.496 11.16 
  - best initial criterion value(s) :  -149.9705 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       149.97  |proj g|=       2.7724
At iterate     1  f =       145.57  |proj g|=        2.0505
At iterate     2  f =       144.86  |proj g|=        5.2794
At iterate     3  f =       142.41  |proj g|=        2.7181
At iterate     4  f =       141.28  |proj g|=        1.9084
At iterate     5  f =        138.6  |proj g|=        1.6995
At iterate     6  f =       138.14  |proj g|=        3.4349
At iterate     7  f =       136.64  |proj g|=        4.1614
At iterate     8  f =       136.04  |proj g|=        1.3678
At iterate     9  f =       135.74  |proj g|=        1.0771
At iterate    10  f =       135.48  |proj g|=         1.073
At iterate    11  f =       135.25  |proj g|=       0.79521
At iterate    12  f =        135.1  |proj g|=       0.55408
At iterate    13  f =       134.72  |proj g|=       0.95509
At iterate    14  f =       134.21  |proj g|=        1.8224
At iterate    15  f =       132.86  |proj g|=        3.6392
At iterate    16  f =          132  |proj g|=        2.1701
At iterate    17  f =       131.58  |proj g|=        1.1291
At iterate    18  f =       130.45  |proj g|=        1.0898
At iterate    19  f =       130.13  |proj g|=        1.3243
At iterate    20  f =       129.74  |proj g|=       0.75936
At iterate    21  f =       129.55  |proj g|=       0.54496
At iterate    22  f =       129.46  |proj g|=       0.35884
At iterate    23  f =       129.44  |proj g|=       0.13002
At iterate    24  f =       129.44  |proj g|=       0.13049
At iterate    25  f =        129.4  |proj g|=       0.21095
At iterate    26  f =       129.37  |proj g|=       0.19169
At iterate    27  f =       129.36  |proj g|=       0.13624
At iterate    28  f =       129.35  |proj g|=      0.062464
At iterate    29  f =       129.34  |proj g|=       0.16879
At iterate    30  f =       129.33  |proj g|=       0.22897
At iterate    31  f =       129.32  |proj g|=       0.25352
At iterate    32  f =       129.29  |proj g|=       0.14301
At iterate    33  f =       129.28  |proj g|=       0.15426
At iterate    34  f =       129.27  |proj g|=         0.015
At iterate    35  f =       129.27  |proj g|=     0.0080249
At iterate    36  f =       129.27  |proj g|=     0.0056065
At iterate    37  f =       129.27  |proj g|=      0.007967
At iterate    38  f =       129.27  |proj g|=     0.0036654
At iterate    39  f =       129.27  |proj g|=     0.0017342
At iterate    40  f =       129.27  |proj g|=     0.0009629
At iterate    41  f =       129.27  |proj g|=    0.00077232
At iterate    42  f =       129.27  |proj g|=    0.00012085

iterations 42
function evaluations 56
segments explored during Cauchy searches 44
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0.000120849
final function value 129.274

F = 129.274
final  value 129.273822 
converged
[1] "Sat Jan 13 18:51:40 2018"
[Tune] Started tuning learner regr.ksvm for parameter set:
         Type len  Def    Constr Req Tunable Trafo
C     numeric   -    0  -5 to 10   -    TRUE     Y
sigma numeric   - TRUE -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: C=7.07; sigma=0.00809
[Tune-y] 1: rmse.test.rmse=0.982; time: 0.0 min
[Tune-x] 2: C=20.6; sigma=9.2e+03
[Tune-y] 2: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 3: C=0.913; sigma=1.23e+04
[Tune-y] 3: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 4: C=102; sigma=9.82e+03
[Tune-y] 4: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 5: C=7.7; sigma=9.36
[Tune-y] 5: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 6: C=97.8; sigma=1.08e+04
[Tune-y] 6: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 7: C=369; sigma=1.94e+03
[Tune-y] 7: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 8: C=0.066; sigma=3.42e-05
[Tune-y] 8: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 9: C=27.1; sigma=5.42e+03
[Tune-y] 9: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 10: C=1.13; sigma=0.00421
[Tune-y] 10: rmse.test.rmse=1.47; time: 0.0 min
[Tune-x] 11: C=5.19; sigma=0.0553
[Tune-y] 11: rmse.test.rmse=1.26; time: 0.0 min
[Tune-x] 12: C=12.5; sigma=10.4
[Tune-y] 12: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 13: C=80.9; sigma=194
[Tune-y] 13: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 14: C=4.67; sigma=0.00807
[Tune-y] 14: rmse.test.rmse=0.992; time: 0.0 min
[Tune-x] 15: C=0.132; sigma=0.564
[Tune-y] 15: rmse.test.rmse=1.86; time: 0.0 min
[Tune-x] 16: C=1.37; sigma=0.338
[Tune-y] 16: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 17: C=0.0342; sigma=0.000473
[Tune-y] 17: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 18: C=0.621; sigma=9.11
[Tune-y] 18: rmse.test.rmse=1.88; time: 0.0 min
[Tune-x] 19: C=0.294; sigma=120
[Tune-y] 19: rmse.test.rmse=1.88; time: 0.0 min
[Tune-x] 20: C=0.356; sigma=1.37
[Tune-y] 20: rmse.test.rmse=1.86; time: 0.0 min
[Tune] Result: C=7.07; sigma=0.00809 : rmse.test.rmse=0.982
[1] "Sat Jan 13 18:51:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 24), d = 11.0365, its = 6
i = 2 (of 24), d = 21.3599, its = 8
i = 3 (of 24), d = 11.0402, its = 6
i = 4 (of 24), d = 12.0106, its = 6
i = 5 (of 24), d = 18.9726, its = 7
i = 6 (of 24), d = 10.1756, its = 6
i = 7 (of 24), d = 10.2008, its = 6
i = 8 (of 24), d = 9.51942, its = 6
i = 9 (of 24), d = 9.66985, its = 6
i = 10 (of 24), d = 16.5496, its = 7
i = 11 (of 24), d = 10.702, its = 6
i = 12 (of 24), d = 10.6441, its = 6
i = 13 (of 24), d = 16.6391, its = 7
i = 14 (of 24), d = 17.1385, its = 7
i = 15 (of 24), d = 9.15645, its = 6
i = 16 (of 24), d = 9.40389, its = 6
i = 17 (of 24), d = 15.4759, its = 7
i = 18 (of 24), d = 10.1202, its = 6
i = 19 (of 24), d = 15.6861, its = 7
i = 20 (of 24), d = 9.41145, its = 6
i = 21 (of 24), d = 12.3981, its = 7
i = 22 (of 24), d = 8.92932, its = 6
i = 23 (of 24), d = 18.1304, its = 7
i = 24 (of 24), d = 18.0867, its = 7
[1] "Sat Jan 13 18:51:56 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Sat Jan 13 18:52:02 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Sat Jan 13 18:52:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Sat Jan 13 18:52:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Sat Jan 13 18:52:20 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Sat Jan 13 18:52:26 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=20; decay=0.155
# weights:  221
initial  value 3691.371862 
iter  10 value 195.421575
iter  20 value 84.635129
iter  30 value 40.490262
iter  40 value 30.367377
iter  50 value 26.295356
iter  60 value 22.788123
iter  70 value 21.203047
iter  80 value 20.075921
iter  90 value 19.717898
iter 100 value 19.392285
final  value 19.392285 
stopped after 100 iterations
# weights:  221
initial  value 2471.648087 
iter  10 value 113.873369
iter  20 value 64.079555
iter  30 value 52.811259
iter  40 value 47.970975
iter  50 value 43.241739
iter  60 value 39.701755
iter  70 value 37.075445
iter  80 value 34.901652
iter  90 value 32.742556
iter 100 value 31.508539
final  value 31.508539 
stopped after 100 iterations
# weights:  221
initial  value 2024.752021 
iter  10 value 161.390143
iter  20 value 86.480630
iter  30 value 54.765702
iter  40 value 38.318130
iter  50 value 32.248013
iter  60 value 29.167548
iter  70 value 26.999463
iter  80 value 25.810617
iter  90 value 24.786483
iter 100 value 23.969526
final  value 23.969526 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=1.35; time: 0.0 min
[Tune-x] 2: size=13; decay=0.00345
# weights:  144
initial  value 5042.884240 
iter  10 value 56.379636
iter  20 value 22.816349
iter  30 value 6.300836
iter  40 value 2.334639
iter  50 value 1.587920
iter  60 value 1.384611
iter  70 value 1.235719
iter  80 value 1.122177
iter  90 value 1.048738
iter 100 value 0.991654
final  value 0.991654 
stopped after 100 iterations
# weights:  144
initial  value 4481.463725 
iter  10 value 151.813207
iter  20 value 108.191698
iter  30 value 42.991560
iter  40 value 36.414660
iter  50 value 35.612012
iter  60 value 33.923280
iter  70 value 29.562540
iter  80 value 19.261234
iter  90 value 16.412986
iter 100 value 11.752299
final  value 11.752299 
stopped after 100 iterations
# weights:  144
initial  value 3404.240197 
iter  10 value 98.611561
iter  20 value 61.056083
iter  30 value 23.324220
iter  40 value 7.917631
iter  50 value 3.829383
iter  60 value 2.298067
iter  70 value 1.888987
iter  80 value 1.608455
iter  90 value 1.417511
iter 100 value 1.330478
final  value 1.330478 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 3: size=18; decay=4.69e-05
# weights:  199
initial  value 4884.264055 
iter  10 value 49.382641
iter  20 value 16.960068
iter  30 value 6.236510
iter  40 value 1.980679
iter  50 value 0.431315
iter  60 value 0.076959
iter  70 value 0.020560
iter  80 value 0.017793
iter  90 value 0.017636
iter 100 value 0.016349
final  value 0.016349 
stopped after 100 iterations
# weights:  199
initial  value 5248.414354 
iter  10 value 91.557915
iter  20 value 50.156608
iter  30 value 28.341200
iter  40 value 23.162741
iter  50 value 17.177262
iter  60 value 12.819634
iter  70 value 9.828654
iter  80 value 8.125075
iter  90 value 7.405156
iter 100 value 6.801104
final  value 6.801104 
stopped after 100 iterations
# weights:  199
initial  value 4135.451549 
iter  10 value 86.133382
iter  20 value 30.735395
iter  30 value 11.062609
iter  40 value 4.180318
iter  50 value 1.259940
iter  60 value 0.258363
iter  70 value 0.116990
iter  80 value 0.057752
iter  90 value 0.054921
iter 100 value 0.052161
final  value 0.052161 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=2.23; time: 0.0 min
[Tune-x] 4: size=15; decay=0.363
# weights:  166
initial  value 3532.245750 
iter  10 value 168.535936
iter  20 value 96.284079
iter  30 value 79.575780
iter  40 value 59.100319
iter  50 value 50.436643
iter  60 value 45.468461
iter  70 value 42.070340
iter  80 value 38.580242
iter  90 value 37.218671
iter 100 value 36.537836
final  value 36.537836 
stopped after 100 iterations
# weights:  166
initial  value 3064.854898 
iter  10 value 92.560782
iter  20 value 64.668186
iter  30 value 54.031648
iter  40 value 51.598103
iter  50 value 50.523984
iter  60 value 49.607614
iter  70 value 47.539279
iter  80 value 46.846297
iter  90 value 46.515647
iter 100 value 46.380644
final  value 46.380644 
stopped after 100 iterations
# weights:  166
initial  value 3501.161196 
iter  10 value 115.803353
iter  20 value 79.693289
iter  30 value 58.662858
iter  40 value 50.187346
iter  50 value 46.581556
iter  60 value 45.370145
iter  70 value 45.053978
iter  80 value 44.975916
iter  90 value 44.337140
iter 100 value 44.002130
final  value 44.002130 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 5: size=9; decay=0.542
# weights:  100
initial  value 4497.989070 
iter  10 value 103.185642
iter  20 value 67.393844
iter  30 value 55.724330
iter  40 value 50.063815
iter  50 value 48.075149
iter  60 value 47.665898
iter  70 value 46.874829
iter  80 value 46.452760
iter  90 value 46.337845
iter 100 value 46.300665
final  value 46.300665 
stopped after 100 iterations
# weights:  100
initial  value 2710.176719 
iter  10 value 108.727259
iter  20 value 83.232415
iter  30 value 71.273764
iter  40 value 67.584254
iter  50 value 64.009627
iter  60 value 62.700601
iter  70 value 62.286656
iter  80 value 62.091894
iter  90 value 62.008781
iter 100 value 61.917268
final  value 61.917268 
stopped after 100 iterations
# weights:  100
initial  value 2847.354099 
iter  10 value 148.288553
iter  20 value 116.095144
iter  30 value 90.755361
iter  40 value 77.827269
iter  50 value 68.971917
iter  60 value 63.785571
iter  70 value 62.857270
iter  80 value 62.242586
iter  90 value 61.846602
iter 100 value 61.590305
final  value 61.590305 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=1.22; time: 0.0 min
[Tune-x] 6: size=13; decay=0.0551
# weights:  144
initial  value 5110.695961 
iter  10 value 63.108221
iter  20 value 24.913136
iter  30 value 16.037654
iter  40 value 12.297926
iter  50 value 10.757347
iter  60 value 10.241555
iter  70 value 10.001340
iter  80 value 9.609924
iter  90 value 9.120060
iter 100 value 8.957101
final  value 8.957101 
stopped after 100 iterations
# weights:  144
initial  value 3835.503953 
iter  10 value 92.635882
iter  20 value 52.074312
iter  30 value 31.658444
iter  40 value 21.684958
iter  50 value 18.313189
iter  60 value 15.713765
iter  70 value 13.508401
iter  80 value 12.779283
iter  90 value 12.300964
iter 100 value 12.018620
final  value 12.018620 
stopped after 100 iterations
# weights:  144
initial  value 4441.629268 
iter  10 value 111.773978
iter  20 value 64.452843
iter  30 value 40.962309
iter  40 value 27.366551
iter  50 value 17.755472
iter  60 value 15.239776
iter  70 value 13.815002
iter  80 value 12.916906
iter  90 value 11.962266
iter 100 value 11.530938
final  value 11.530938 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 7: size=8; decay=0.0595
# weights:  89
initial  value 3923.066644 
iter  10 value 232.285178
iter  20 value 122.676227
iter  30 value 83.023462
iter  40 value 47.843206
iter  50 value 29.735134
iter  60 value 21.820563
iter  70 value 17.807318
iter  80 value 16.138290
iter  90 value 14.171619
iter 100 value 12.475906
final  value 12.475906 
stopped after 100 iterations
# weights:  89
initial  value 4670.825856 
iter  10 value 152.267700
iter  20 value 68.399841
iter  30 value 45.549816
iter  40 value 34.516164
iter  50 value 27.972539
iter  60 value 25.269764
iter  70 value 23.052138
iter  80 value 21.393391
iter  90 value 20.139196
iter 100 value 18.803312
final  value 18.803312 
stopped after 100 iterations
# weights:  89
initial  value 3425.911052 
iter  10 value 107.527754
iter  20 value 45.163214
iter  30 value 30.734662
iter  40 value 23.370516
iter  50 value 19.831212
iter  60 value 18.104954
iter  70 value 16.430154
iter  80 value 15.193015
iter  90 value 14.363707
iter 100 value 13.546231
final  value 13.546231 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 8: size=12; decay=0.333
# weights:  133
initial  value 3692.005902 
iter  10 value 114.748682
iter  20 value 70.377162
iter  30 value 49.927912
iter  40 value 39.244631
iter  50 value 37.002451
iter  60 value 35.990261
iter  70 value 35.454852
iter  80 value 34.906681
iter  90 value 34.544595
iter 100 value 34.314013
final  value 34.314013 
stopped after 100 iterations
# weights:  133
initial  value 4003.876262 
iter  10 value 85.288843
iter  20 value 61.763351
iter  30 value 52.986812
iter  40 value 49.639205
iter  50 value 46.554525
iter  60 value 45.609058
iter  70 value 45.180882
iter  80 value 45.007220
iter  90 value 44.953313
iter 100 value 44.802016
final  value 44.802016 
stopped after 100 iterations
# weights:  133
initial  value 3002.913053 
iter  10 value 155.679768
iter  20 value 104.092927
iter  30 value 76.154028
iter  40 value 62.339553
iter  50 value 52.118577
iter  60 value 48.881934
iter  70 value 46.451814
iter  80 value 45.203977
iter  90 value 44.602689
iter 100 value 44.008846
final  value 44.008846 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=1.22; time: 0.0 min
[Tune-x] 9: size=17; decay=0.0627
# weights:  188
initial  value 3336.734039 
iter  10 value 92.739842
iter  20 value 36.869937
iter  30 value 22.744914
iter  40 value 16.906048
iter  50 value 14.121322
iter  60 value 12.590856
iter  70 value 11.765086
iter  80 value 11.225165
iter  90 value 10.916702
iter 100 value 10.633698
final  value 10.633698 
stopped after 100 iterations
# weights:  188
initial  value 1464.427541 
iter  10 value 86.932539
iter  20 value 50.627141
iter  30 value 36.762360
iter  40 value 29.468158
iter  50 value 25.388307
iter  60 value 22.139860
iter  70 value 20.122629
iter  80 value 18.271896
iter  90 value 17.249356
iter 100 value 16.770057
final  value 16.770057 
stopped after 100 iterations
# weights:  188
initial  value 4365.948462 
iter  10 value 191.870589
iter  20 value 122.568116
iter  30 value 79.126755
iter  40 value 53.671794
iter  50 value 36.279457
iter  60 value 24.324422
iter  70 value 17.208649
iter  80 value 14.739691
iter  90 value 13.628604
iter 100 value 12.728540
final  value 12.728540 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 10: size=4; decay=0.00151
# weights:  45
initial  value 4657.498950 
iter  10 value 135.128368
iter  20 value 74.828866
iter  30 value 33.153062
iter  40 value 21.612358
iter  50 value 13.915652
iter  60 value 10.004499
iter  70 value 8.265901
iter  80 value 6.921848
iter  90 value 6.128767
iter 100 value 5.957736
final  value 5.957736 
stopped after 100 iterations
# weights:  45
initial  value 3824.267463 
iter  10 value 144.012420
iter  20 value 101.713258
iter  30 value 84.892948
iter  40 value 55.454653
iter  50 value 43.828332
iter  60 value 35.520078
iter  70 value 34.908683
iter  80 value 34.865302
iter  90 value 34.812081
iter 100 value 30.663350
final  value 30.663350 
stopped after 100 iterations
# weights:  45
initial  value 4333.269609 
iter  10 value 164.606194
iter  20 value 87.250745
iter  30 value 44.181056
iter  40 value 26.405562
iter  50 value 24.703156
iter  60 value 23.466930
iter  70 value 23.243168
iter  80 value 22.778545
iter  90 value 20.589929
iter 100 value 18.205155
final  value 18.205155 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=2.45; time: 0.0 min
[Tune-x] 11: size=9; decay=0.000345
# weights:  100
initial  value 3818.643445 
iter  10 value 173.747558
iter  20 value 173.579572
iter  30 value 173.459563
iter  40 value 172.762533
iter  50 value 102.317774
iter  60 value 49.880215
iter  70 value 29.772756
iter  80 value 22.071291
iter  90 value 20.849580
iter 100 value 20.771853
final  value 20.771853 
stopped after 100 iterations
# weights:  100
initial  value 2618.212964 
iter  10 value 75.387961
iter  20 value 32.760711
iter  30 value 16.802507
iter  40 value 7.202489
iter  50 value 2.364759
iter  60 value 0.884829
iter  70 value 0.517684
iter  80 value 0.392751
iter  90 value 0.360464
iter 100 value 0.321126
final  value 0.321126 
stopped after 100 iterations
# weights:  100
initial  value 5180.606561 
iter  10 value 174.185443
iter  20 value 130.080479
iter  30 value 122.016420
iter  40 value 121.487618
iter  50 value 121.384632
iter  60 value 121.290496
iter  70 value 116.717646
iter  80 value 57.113184
iter  90 value 30.972839
iter 100 value 22.891093
final  value 22.891093 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=1.47; time: 0.0 min
[Tune-x] 12: size=2; decay=0.00124
# weights:  23
initial  value 4822.207697 
iter  10 value 173.345395
iter  20 value 168.687722
iter  30 value 77.325890
iter  40 value 58.780667
iter  50 value 28.435316
iter  60 value 21.131374
iter  70 value 20.787651
iter  80 value 20.770202
iter  90 value 20.769978
iter 100 value 20.765299
final  value 20.765299 
stopped after 100 iterations
# weights:  23
initial  value 3007.748102 
iter  10 value 149.320928
iter  20 value 137.947209
iter  30 value 121.368808
iter  40 value 44.523321
iter  50 value 34.978058
iter  60 value 33.240966
iter  70 value 28.711372
iter  80 value 26.656018
iter  90 value 25.368367
iter 100 value 25.251560
final  value 25.251560 
stopped after 100 iterations
# weights:  23
initial  value 2872.188796 
iter  10 value 121.033607
iter  20 value 40.929377
iter  30 value 29.489445
iter  40 value 28.610233
iter  50 value 28.588921
iter  60 value 28.540897
iter  70 value 23.394825
iter  80 value 20.624576
iter  90 value 19.121273
iter 100 value 18.043021
final  value 18.043021 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=1.17; time: 0.0 min
[Tune-x] 13: size=14; decay=2.54
# weights:  155
initial  value 3071.104879 
iter  10 value 144.224938
iter  20 value 123.621762
iter  30 value 111.770215
iter  40 value 106.743871
iter  50 value 102.123336
iter  60 value 100.538456
iter  70 value 99.713752
iter  80 value 99.217272
iter  90 value 98.822151
iter 100 value 98.569445
final  value 98.569445 
stopped after 100 iterations
# weights:  155
initial  value 5450.712694 
iter  10 value 495.732601
iter  20 value 166.323745
iter  30 value 131.123810
iter  40 value 123.751129
iter  50 value 117.992238
iter  60 value 114.510031
iter  70 value 112.703766
iter  80 value 110.280498
iter  90 value 108.157925
iter 100 value 107.084864
final  value 107.084864 
stopped after 100 iterations
# weights:  155
initial  value 3335.044656 
iter  10 value 199.856879
iter  20 value 159.451108
iter  30 value 131.726087
iter  40 value 124.907234
iter  50 value 119.803041
iter  60 value 117.541585
iter  70 value 116.445145
iter  80 value 115.911284
iter  90 value 115.676318
iter 100 value 115.555469
final  value 115.555469 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 14: size=20; decay=7.69e-05
# weights:  221
initial  value 4221.147924 
iter  10 value 60.805086
iter  20 value 22.298811
iter  30 value 13.880122
iter  40 value 6.429338
iter  50 value 2.242818
iter  60 value 0.688585
iter  70 value 0.139179
iter  80 value 0.064049
iter  90 value 0.049979
iter 100 value 0.048880
final  value 0.048880 
stopped after 100 iterations
# weights:  221
initial  value 4020.791917 
iter  10 value 127.396736
iter  20 value 80.742283
iter  30 value 37.536289
iter  40 value 18.669798
iter  50 value 13.527379
iter  60 value 8.542041
iter  70 value 7.833999
iter  80 value 7.476704
iter  90 value 7.321339
iter 100 value 7.248408
final  value 7.248408 
stopped after 100 iterations
# weights:  221
initial  value 3614.421031 
iter  10 value 166.358806
iter  20 value 112.738938
iter  30 value 66.932956
iter  40 value 40.150422
iter  50 value 30.905063
iter  60 value 18.360826
iter  70 value 9.528134
iter  80 value 8.257918
iter  90 value 7.157348
iter 100 value 6.262739
final  value 6.262739 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=1.81; time: 0.0 min
[Tune-x] 15: size=19; decay=8.61
# weights:  210
initial  value 3516.796474 
iter  10 value 278.343924
iter  20 value 229.688790
iter  30 value 198.276468
iter  40 value 187.721610
iter  50 value 179.959514
iter  60 value 173.098493
iter  70 value 165.287860
iter  80 value 162.288000
iter  90 value 161.592202
iter 100 value 160.716821
final  value 160.716821 
stopped after 100 iterations
# weights:  210
initial  value 4272.885239 
iter  10 value 507.111875
iter  20 value 383.839833
iter  30 value 295.710081
iter  40 value 225.826886
iter  50 value 195.824823
iter  60 value 183.401209
iter  70 value 173.435266
iter  80 value 167.568693
iter  90 value 162.890931
iter 100 value 159.953552
final  value 159.953552 
stopped after 100 iterations
# weights:  210
initial  value 4776.226452 
iter  10 value 1509.204521
iter  20 value 606.115285
iter  30 value 294.361075
iter  40 value 252.105402
iter  50 value 233.062356
iter  60 value 220.524828
iter  70 value 210.832584
iter  80 value 198.318816
iter  90 value 190.209539
iter 100 value 185.407441
final  value 185.407441 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 16: size=16; decay=0.00906
# weights:  177
initial  value 3925.328480 
iter  10 value 118.231119
iter  20 value 54.741783
iter  30 value 25.521355
iter  40 value 16.791427
iter  50 value 10.997440
iter  60 value 8.300012
iter  70 value 6.696891
iter  80 value 5.606392
iter  90 value 4.977668
iter 100 value 4.219216
final  value 4.219216 
stopped after 100 iterations
# weights:  177
initial  value 2940.944001 
iter  10 value 92.473047
iter  20 value 41.724822
iter  30 value 27.889855
iter  40 value 22.465675
iter  50 value 18.561449
iter  60 value 13.767088
iter  70 value 10.409418
iter  80 value 6.520754
iter  90 value 4.773056
iter 100 value 3.869435
final  value 3.869435 
stopped after 100 iterations
# weights:  177
initial  value 3959.943895 
iter  10 value 78.283094
iter  20 value 26.589071
iter  30 value 9.534193
iter  40 value 5.077567
iter  50 value 3.803870
iter  60 value 3.049601
iter  70 value 2.850004
iter  80 value 2.667572
iter  90 value 2.495815
iter 100 value 2.379705
final  value 2.379705 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=2.15; time: 0.0 min
[Tune-x] 17: size=3; decay=0.502
# weights:  34
initial  value 3456.580849 
iter  10 value 209.622003
iter  20 value 147.622968
iter  30 value 95.914478
iter  40 value 69.252690
iter  50 value 58.633620
iter  60 value 54.540535
iter  70 value 52.859928
iter  80 value 52.168458
iter  90 value 52.082395
iter 100 value 52.069166
final  value 52.069166 
stopped after 100 iterations
# weights:  34
initial  value 3719.438618 
iter  10 value 164.622493
iter  20 value 115.236529
iter  30 value 85.269180
iter  40 value 77.597579
iter  50 value 73.060502
iter  60 value 70.982122
iter  70 value 70.260777
iter  80 value 69.856362
iter  90 value 69.704229
iter 100 value 69.659382
final  value 69.659382 
stopped after 100 iterations
# weights:  34
initial  value 3954.622801 
iter  10 value 210.289091
iter  20 value 129.247681
iter  30 value 85.161586
iter  40 value 70.484902
iter  50 value 67.430579
iter  60 value 64.181851
iter  70 value 63.473184
iter  80 value 63.360289
iter  90 value 63.330133
iter 100 value 63.324985
final  value 63.324985 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=1.25; time: 0.0 min
[Tune-x] 18: size=10; decay=0.617
# weights:  111
initial  value 3944.797160 
iter  10 value 81.805997
iter  20 value 64.198786
iter  30 value 55.310827
iter  40 value 52.452636
iter  50 value 51.303791
iter  60 value 50.836439
iter  70 value 50.506808
iter  80 value 50.011143
iter  90 value 49.752834
iter 100 value 49.681059
final  value 49.681059 
stopped after 100 iterations
# weights:  111
initial  value 2830.662084 
iter  10 value 105.241050
iter  20 value 80.020897
iter  30 value 70.632735
iter  40 value 67.318731
iter  50 value 66.146863
iter  60 value 65.520666
iter  70 value 65.212497
iter  80 value 64.970001
iter  90 value 64.606287
iter 100 value 64.253593
final  value 64.253593 
stopped after 100 iterations
# weights:  111
initial  value 4576.029518 
iter  10 value 120.542301
iter  20 value 85.711882
iter  30 value 75.804975
iter  40 value 70.658232
iter  50 value 67.111612
iter  60 value 64.862851
iter  70 value 64.203336
iter  80 value 63.803712
iter  90 value 63.598655
iter 100 value 63.510297
final  value 63.510297 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=1.22; time: 0.0 min
[Tune-x] 19: size=19; decay=0.0268
# weights:  210
initial  value 4929.143714 
iter  10 value 88.884382
iter  20 value 47.745055
iter  30 value 35.508094
iter  40 value 28.881291
iter  50 value 21.150426
iter  60 value 17.782160
iter  70 value 15.123828
iter  80 value 13.443530
iter  90 value 11.962025
iter 100 value 10.844898
final  value 10.844898 
stopped after 100 iterations
# weights:  210
initial  value 4487.263621 
iter  10 value 140.337956
iter  20 value 80.734473
iter  30 value 51.468718
iter  40 value 33.202265
iter  50 value 22.942778
iter  60 value 19.029251
iter  70 value 16.593834
iter  80 value 15.161595
iter  90 value 13.344369
iter 100 value 11.470467
final  value 11.470467 
stopped after 100 iterations
# weights:  210
initial  value 4377.322852 
iter  10 value 127.831710
iter  20 value 88.242059
iter  30 value 43.900467
iter  40 value 24.871556
iter  50 value 15.169235
iter  60 value 11.027308
iter  70 value 9.190338
iter  80 value 8.024010
iter  90 value 7.297601
iter 100 value 6.918633
final  value 6.918633 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=1.93; time: 0.0 min
[Tune-x] 20: size=19; decay=0.0023
# weights:  210
initial  value 5804.969097 
iter  10 value 75.543965
iter  20 value 30.091181
iter  30 value 9.840185
iter  40 value 3.141176
iter  50 value 1.973148
iter  60 value 1.700443
iter  70 value 1.518780
iter  80 value 1.269462
iter  90 value 1.132270
iter 100 value 0.976786
final  value 0.976786 
stopped after 100 iterations
# weights:  210
initial  value 5214.850193 
iter  10 value 87.804003
iter  20 value 42.887007
iter  30 value 23.168383
iter  40 value 16.147673
iter  50 value 11.180670
iter  60 value 6.489236
iter  70 value 4.633329
iter  80 value 3.638985
iter  90 value 3.104060
iter 100 value 2.483634
final  value 2.483634 
stopped after 100 iterations
# weights:  210
initial  value 4732.091173 
iter  10 value 112.079050
iter  20 value 53.853816
iter  30 value 21.708094
iter  40 value 16.741639
iter  50 value 10.665696
iter  60 value 7.599810
iter  70 value 5.196057
iter  80 value 3.822837
iter  90 value 3.076360
iter 100 value 2.587082
final  value 2.587082 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=2.12; time: 0.0 min
[Tune] Result: size=2; decay=0.00124 : rmse.test.rmse=1.17
# weights:  23
initial  value 5172.785883 
iter  10 value 257.561077
iter  20 value 257.515411
iter  30 value 257.485826
iter  40 value 222.059228
iter  50 value 157.900863
iter  60 value 85.465034
iter  70 value 47.874155
iter  80 value 38.374698
iter  90 value 38.092753
iter 100 value 38.082054
final  value 38.082054 
stopped after 100 iterations
[1] "Sat Jan 13 18:52:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1006
 total number of nodes after removal of identical nodes : 570 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 494
 number of selected nodes                               : 65 
[1] "Sat Jan 13 18:53:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Sat Jan 13 18:53:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Sat Jan 13 18:53:38 2018"
[Tune] Started tuning learner regr.randomForest for parameter set:
            Type len Def  Constr Req Tunable Trafo
nodesize integer   -   1 1 to 10   -    TRUE     -
mtry     integer   -   3  1 to 9   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nodesize=10; mtry=7
[Tune-y] 1: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 2: nodesize=7; mtry=4
[Tune-y] 2: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 3: nodesize=9; mtry=2
[Tune-y] 3: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 4: nodesize=8; mtry=7
[Tune-y] 4: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 5: nodesize=5; mtry=8
[Tune-y] 5: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 6: nodesize=7; mtry=6
[Tune-y] 6: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 7: nodesize=4; mtry=6
[Tune-y] 7: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 8: nodesize=6; mtry=7
[Tune-y] 8: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 9: nodesize=9; mtry=6
[Tune-y] 9: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 10: nodesize=2; mtry=4
[Tune-y] 10: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 11: nodesize=5; mtry=3
[Tune-y] 11: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 12: nodesize=1; mtry=4
[Tune-y] 12: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 13: nodesize=7; mtry=9
[Tune-y] 13: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 14: nodesize=10; mtry=2
[Tune-y] 14: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 15: nodesize=10; mtry=9
[Tune-y] 15: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 16: nodesize=8; mtry=5
[Tune-y] 16: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 17: nodesize=2; mtry=8
[Tune-y] 17: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 18: nodesize=5; mtry=8
[Tune-y] 18: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 19: nodesize=10; mtry=6
[Tune-y] 19: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 20: nodesize=10; mtry=4
[Tune-y] 20: rmse.test.rmse=1.52; time: 0.0 min
[Tune] Result: nodesize=5; mtry=8 : rmse.test.rmse=1.51
[1] "Sat Jan 13 18:53:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Sat Jan 13 18:53:59 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=9; min.node.size=7
[Tune-y] 1: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 2: mtry=6; min.node.size=5
[Tune-y] 2: rmse.test.rmse= 1.5; time: 0.0 min
[Tune-x] 3: mtry=9; min.node.size=2
[Tune-y] 3: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 4: mtry=7; min.node.size=8
[Tune-y] 4: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 5: mtry=4; min.node.size=8
[Tune-y] 5: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 6: mtry=6; min.node.size=7
[Tune-y] 6: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 7: mtry=4; min.node.size=7
[Tune-y] 7: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 8: mtry=6; min.node.size=8
[Tune-y] 8: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 9: mtry=8; min.node.size=7
[Tune-y] 9: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 10: mtry=2; min.node.size=4
[Tune-y] 10: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 11: mtry=4; min.node.size=3
[Tune-y] 11: rmse.test.rmse= 1.5; time: 0.0 min
[Tune-x] 12: mtry=1; min.node.size=4
[Tune-y] 12: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 13: mtry=6; min.node.size=10
[Tune-y] 13: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 14: mtry=9; min.node.size=2
[Tune-y] 14: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 15: mtry=9; min.node.size=10
[Tune-y] 15: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 16: mtry=7; min.node.size=5
[Tune-y] 16: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 17: mtry=2; min.node.size=8
[Tune-y] 17: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 18: mtry=5; min.node.size=8
[Tune-y] 18: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 19: mtry=9; min.node.size=6
[Tune-y] 19: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 20: mtry=9; min.node.size=4
[Tune-y] 20: rmse.test.rmse=1.52; time: 0.0 min
[Tune] Result: mtry=6; min.node.size=5 : rmse.test.rmse= 1.5
[1] "Sat Jan 13 18:54:16 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Sat Jan 13 18:54:23 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.842; maxdepth=22; minbucket=34; minsplit=24
[Tune-y] 1: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 2: cp=0.499; maxdepth=6; minbucket=37; minsplit=39
[Tune-y] 2: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 3: cp=0.0205; maxdepth=25; minbucket=34; minsplit=33
[Tune-y] 3: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 4: cp=0.0133; maxdepth=20; minbucket=31; minsplit=39
[Tune-y] 4: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 5: cp=0.282; maxdepth=20; minbucket=13; minsplit=21
[Tune-y] 5: rmse.test.rmse=2.04; time: 0.0 min
[Tune-x] 6: cp=0.0156; maxdepth=10; minbucket=9; minsplit=21
[Tune-y] 6: rmse.test.rmse=1.91; time: 0.0 min
[Tune-x] 7: cp=0.0921; maxdepth=28; minbucket=49; minsplit=11
[Tune-y] 7: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 8: cp=0.562; maxdepth=30; minbucket=40; minsplit=27
[Tune-y] 8: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 9: cp=0.00263; maxdepth=24; minbucket=25; minsplit=41
[Tune-y] 9: rmse.test.rmse=1.98; time: 0.0 min
[Tune-x] 10: cp=0.532; maxdepth=18; minbucket=47; minsplit=23
[Tune-y] 10: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 11: cp=0.00273; maxdepth=12; minbucket=29; minsplit=24
[Tune-y] 11: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 12: cp=0.437; maxdepth=22; minbucket=29; minsplit=24
[Tune-y] 12: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 13: cp=0.738; maxdepth=21; minbucket=27; minsplit=26
[Tune-y] 13: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 14: cp=0.0868; maxdepth=3; minbucket=48; minsplit=27
[Tune-y] 14: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 15: cp=0.093; maxdepth=9; minbucket=40; minsplit=30
[Tune-y] 15: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 16: cp=0.00138; maxdepth=16; minbucket=43; minsplit=49
[Tune-y] 16: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 17: cp=0.0143; maxdepth=15; minbucket=45; minsplit=7
[Tune-y] 17: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 18: cp=0.113; maxdepth=19; minbucket=32; minsplit=39
[Tune-y] 18: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 19: cp=0.00619; maxdepth=12; minbucket=43; minsplit=20
[Tune-y] 19: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 20: cp=0.0217; maxdepth=17; minbucket=28; minsplit=17
[Tune-y] 20: rmse.test.rmse=1.97; time: 0.0 min
[Tune] Result: cp=0.0156; maxdepth=10; minbucket=9; minsplit=21 : rmse.test.rmse=1.91
[1] "Sat Jan 13 18:54:31 2018"
[Tune] Started tuning learner regr.RRF for parameter set:
           Type len Def Constr Req Tunable Trafo
mtry    integer   -   3 1 to 9   -    TRUE     -
coefReg numeric   - 0.8 0 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=9; coefReg=0.698
[Tune-y] 1: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 2: mtry=6; coefReg=0.423
[Tune-y] 2: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 3: mtry=9; coefReg=0.112
[Tune-y] 3: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 4: mtry=7; coefReg=0.76
[Tune-y] 4: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 5: mtry=4; coefReg=0.789
[Tune-y] 5: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 6: mtry=6; coefReg=0.623
[Tune-y] 6: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 7: mtry=4; coefReg=0.629
[Tune-y] 7: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 8: mtry=6; coefReg=0.754
[Tune-y] 8: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 9: mtry=8; coefReg=0.633
[Tune-y] 9: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 10: mtry=2; coefReg=0.363
[Tune-y] 10: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 11: mtry=4; coefReg=0.256
[Tune-y] 11: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 12: mtry=1; coefReg=0.349
[Tune-y] 12: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 13: mtry=6; coefReg=0.901
[Tune-y] 13: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 14: mtry=9; coefReg=0.148
[Tune-y] 14: rmse.test.rmse=1.71; time: 0.0 min
[Tune-x] 15: mtry=9; coefReg=0.989
[Tune-y] 15: rmse.test.rmse= 1.5; time: 0.0 min
[Tune-x] 16: mtry=7; coefReg=0.493
[Tune-y] 16: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 17: mtry=2; coefReg=0.783
[Tune-y] 17: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 18: mtry=5; coefReg=0.798
[Tune-y] 18: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 19: mtry=9; coefReg=0.571
[Tune-y] 19: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 20: mtry=9; coefReg=0.394
[Tune-y] 20: rmse.test.rmse=1.54; time: 0.0 min
[Tune] Result: mtry=9; coefReg=0.989 : rmse.test.rmse= 1.5
[1] "Sat Jan 13 18:54:46 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Sat Jan 13 18:54:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Sat Jan 13 18:54:59 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.480 0.371 0.286 0.220 0.170
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 6 
Runtime: 0.01700091 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.2858 
    Y 1          8.387 
    Y 2          7.668 
    Y 3          9.215 
    Y 4          9.498 
    Y 5          8.459 
[1] "Sat Jan 13 18:55:05 2018"
[Tune] Started tuning learner regr.svm for parameter set:
         Type len   Def    Constr Req Tunable Trafo
cost  numeric   -     0 -15 to 15   -    TRUE     Y
gamma numeric   - -3.17 -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cost=1.96e+04; gamma=62
[Tune-y] 1: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 2: cost=20.8; gamma=0.201
[Tune-y] 2: rmse.test.rmse=1.72; time: 0.0 min
[Tune-x] 3: cost=4.07e+03; gamma=0.000312
[Tune-y] 3: rmse.test.rmse=0.922; time: 0.0 min
[Tune-x] 4: cost=82.6; gamma=223
[Tune-y] 4: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 5: cost=0.281; gamma=407
[Tune-y] 5: rmse.test.rmse=1.96; time: 0.0 min
[Tune-x] 6: cost=16.2; gamma=13
[Tune-y] 6: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 7: cost=0.0778; gamma=14.7
[Tune-y] 7: rmse.test.rmse=1.96; time: 0.0 min
[Tune-x] 8: cost=5.36; gamma=196
[Tune-y] 8: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 9: cost=737; gamma=15.8
[Tune-y] 9: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 10: cost=0.00148; gamma=0.058
[Tune-y] 10: rmse.test.rmse=1.95; time: 0.0 min
[Tune-x] 11: cost=0.126; gamma=0.0063
[Tune-y] 11: rmse.test.rmse=1.88; time: 0.0 min
[Tune-x] 12: cost=0.000211; gamma=0.0432
[Tune-y] 12: rmse.test.rmse=1.95; time: 0.0 min
[Tune-x] 13: cost=25.6; gamma=4.17e+03
[Tune-y] 13: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 14: cost=2.08e+04; gamma=0.000658
[Tune-y] 14: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 15: cost=5.81e+03; gamma=2.62e+04
[Tune-y] 15: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 16: cost=304; gamma=0.863
[Tune-y] 16: rmse.test.rmse=1.91; time: 0.0 min
[Tune-x] 17: cost=0.000595; gamma=363
[Tune-y] 17: rmse.test.rmse=1.95; time: 0.0 min
[Tune-x] 18: cost=0.387; gamma=495
[Tune-y] 18: rmse.test.rmse=1.96; time: 0.0 min
[Tune-x] 19: cost=4.94e+03; gamma=4.41
[Tune-y] 19: rmse.test.rmse=1.96; time: 0.0 min
[Tune-x] 20: cost=8.07e+03; gamma=0.11
[Tune-y] 20: rmse.test.rmse=1.66; time: 0.0 min
[Tune] Result: cost=4.07e+03; gamma=0.000312 : rmse.test.rmse=0.922
[1] "Sat Jan 13 18:55:14 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=3.45e+03; max_depth=7; eta=0.388; gamma=4.23; colsample_bytree=0.66; min_child_weight=2.24; subsample=0.784
[Tune-y] 1: rmse.test.rmse= 1.3; time: 0.9 min
[Tune-x] 2: nrounds=950; max_depth=5; eta=0.474; gamma=6.34; colsample_bytree=0.549; min_child_weight=7.54; subsample=0.722
[Tune-y] 2: rmse.test.rmse=1.39; time: 0.1 min
[Tune-x] 3: nrounds=324; max_depth=8; eta=0.491; gamma=6.33; colsample_bytree=0.375; min_child_weight=7.26; subsample=0.55
[Tune-y] 3: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 4: nrounds=46; max_depth=1; eta=0.21; gamma=6.56; colsample_bytree=0.66; min_child_weight=19.6; subsample=0.361
[Tune-y] 4: rmse.test.rmse=2.04; time: 0.0 min
[Tune-x] 5: nrounds=2.43e+03; max_depth=10; eta=0.465; gamma=4.93; colsample_bytree=0.357; min_child_weight=15.7; subsample=0.591
[Tune-y] 5: rmse.test.rmse=1.74; time: 0.2 min
[Tune-x] 6: nrounds=1.2e+03; max_depth=10; eta=0.343; gamma=9.33; colsample_bytree=0.458; min_child_weight=2.97; subsample=0.515
[Tune-y] 6: rmse.test.rmse=1.45; time: 0.3 min
[Tune-x] 7: nrounds=255; max_depth=5; eta=0.528; gamma=6.86; colsample_bytree=0.513; min_child_weight=8.63; subsample=0.967
[Tune-y] 7: rmse.test.rmse=1.39; time: 0.0 min
[Tune-x] 8: nrounds=496; max_depth=5; eta=0.284; gamma=6.47; colsample_bytree=0.311; min_child_weight=19; subsample=0.624
[Tune-y] 8: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 9: nrounds=513; max_depth=3; eta=0.459; gamma=5.62; colsample_bytree=0.32; min_child_weight=9.96; subsample=0.884
[Tune-y] 9: rmse.test.rmse= 1.4; time: 0.1 min
[Tune-x] 10: nrounds=3.35e+03; max_depth=4; eta=0.264; gamma=8.7; colsample_bytree=0.325; min_child_weight=13.7; subsample=0.679
[Tune-y] 10: rmse.test.rmse=1.63; time: 0.3 min
[Tune-x] 11: nrounds=370; max_depth=8; eta=0.161; gamma=3.43; colsample_bytree=0.631; min_child_weight=6.78; subsample=0.585
[Tune-y] 11: rmse.test.rmse=1.39; time: 0.1 min
[Tune-x] 12: nrounds=211; max_depth=6; eta=0.162; gamma=6.24; colsample_bytree=0.676; min_child_weight=6.49; subsample=0.965
[Tune-y] 12: rmse.test.rmse=1.49; time: 0.0 min
[Tune-x] 13: nrounds=1.06e+03; max_depth=10; eta=0.318; gamma=6.08; colsample_bytree=0.61; min_child_weight=18.9; subsample=0.926
[Tune-y] 13: rmse.test.rmse=1.78; time: 0.1 min
[Tune-x] 14: nrounds=1.77e+03; max_depth=1; eta=0.00431; gamma=6.51; colsample_bytree=0.665; min_child_weight=6.91; subsample=0.428
[Tune-y] 14: rmse.test.rmse=1.58; time: 0.1 min
[Tune-x] 15: nrounds=190; max_depth=4; eta=0.346; gamma=6.13; colsample_bytree=0.602; min_child_weight=15.1; subsample=0.611
[Tune-y] 15: rmse.test.rmse=1.85; time: 0.0 min
[Tune-x] 16: nrounds=50; max_depth=2; eta=0.284; gamma=3.64; colsample_bytree=0.479; min_child_weight=0.176; subsample=0.349
[Tune-y] 16: rmse.test.rmse=1.41; time: 0.0 min
[Tune-x] 17: nrounds=56; max_depth=7; eta=0.13; gamma=7.3; colsample_bytree=0.394; min_child_weight=10.3; subsample=0.999
[Tune-y] 17: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 18: nrounds=274; max_depth=4; eta=0.194; gamma=6.17; colsample_bytree=0.588; min_child_weight=3.23; subsample=0.287
[Tune-y] 18: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 19: nrounds=65; max_depth=1; eta=0.141; gamma=5.63; colsample_bytree=0.601; min_child_weight=14.8; subsample=0.389
[Tune-y] 19: rmse.test.rmse=1.94; time: 0.0 min
[Tune-x] 20: nrounds=342; max_depth=4; eta=0.0318; gamma=7.41; colsample_bytree=0.395; min_child_weight=1.51; subsample=0.547
[Tune-y] 20: rmse.test.rmse= 1.5; time: 0.1 min
[Tune] Result: nrounds=3.45e+03; max_depth=7; eta=0.388; gamma=4.23; colsample_bytree=0.66; min_child_weight=2.24; subsample=0.784 : rmse.test.rmse= 1.3
[1] "Sat Jan 13 18:58:04 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Sat Jan 13 18:58:10 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [2,0.491537], n=(63,13)
**PRUNE** @depth 0: [2,0.491537]
**GROW** @depth 0: [5,0.52154], n=(43,33)
**PRUNE** @depth 0: [5,0.529102]
**GROW** @depth 0: [2,0.464937], n=(58,18)
**PRUNE** @depth 0: [2,0.395691]
**GROW** @depth 0: [2,0.340954], n=(43,33)
**GROW** @depth 1: [2,0.491537], n=(19,13)
**PRUNE** @depth 1: [2,0.491537]
**PRUNE** @depth 0: [2,0.340954]
**GROW** @depth 0: [2,0.491537], n=(63,13)
**PRUNE** @depth 0: [2,0.534403]
**GROW** @depth 0: [1,0.524856], n=(20,56)
**PRUNE** @depth 0: [1,0.513969]
r=1000 d=[0]; n=76
**GROW** @depth 0: [5,0.52154], n=(43,33)
**PRUNE** @depth 0: [5,0.52154]
**GROW** @depth 0: [2,0.491537], n=(63,13)
**PRUNE** @depth 0: [2,0.48736]
**GROW** @depth 0: [8,0.490015], n=(53,23)
**PRUNE** @depth 0: [8,0.480954]
**GROW** @depth 0: [1,0.617502], n=(24,52)
**PRUNE** @depth 0: [1,0.524856]
**GROW** @depth 0: [5,0.52154], n=(43,33)
**PRUNE** @depth 0: [5,0.52154]
**GROW** @depth 0: [1,0.481512], n=(16,60)
**PRUNE** @depth 0: [1,0.489934]
r=2000 d=[0]; n=76

Sampling @ nn=0 pred locs:
**GROW** @depth 0: [5,0.52154], n=(43,33)
**PRUNE** @depth 0: [5,0.52154]
**GROW** @depth 0: [4,0.540408], n=(56,20)
**PRUNE** @depth 0: [4,0.540408]
**GROW** @depth 0: [4,0.496838], n=(54,22)
**PRUNE** @depth 0: [4,0.531975]
**GROW** @depth 0: [1,0.547658], n=(21,55)
**PRUNE** @depth 0: [1,0.513969]
**GROW** @depth 0: [3,0.444066], n=(48,28)
**PRUNE** @depth 0: [3,0.455593]
**GROW** @depth 0: [1,0.432621], n=(14,62)
**PRUNE** @depth 0: [1,0.439606]
**GROW** @depth 0: [2,0.491537], n=(63,13)
**PRUNE** @depth 0: [2,0.491537]
**GROW** @depth 0: [1,0.489934], n=(17,59)
r=1000 d=[0] [0]; mh=2 n=(22,54)
**PRUNE** @depth 0: [1,0.547658]
**GROW** @depth 0: [4,0.496838], n=(54,22)
**PRUNE** @depth 0: [4,0.540408]
**GROW** @depth 0: [1,0.513969], n=(18,58)
r=2000 d=[0] [0]; mh=2 n=(18,58)
**PRUNE** @depth 0: [1,0.524856]
**GROW** @depth 0: [2,0.48736], n=(62,14)
r=3000 d=[0] [0]; mh=2 n=(59,17)
**PRUNE** @depth 0: [2,0.454386]
**GROW** @depth 0: [4,0.496838], n=(54,22)
**PRUNE** @depth 0: [4,0.540408]
**GROW** @depth 0: [1,0.637634], n=(25,51)
r=4000 d=[0] [0]; mh=2 n=(17,59)
**PRUNE** @depth 0: [1,0.489934]
**GROW** @depth 0: [2,0.534403], n=(64,12)
**PRUNE** @depth 0: [2,0.534403]
**GROW** @depth 0: [1,0.524856], n=(20,56)
**PRUNE** @depth 0: [1,0.600247]
**GROW** @depth 0: [2,0.48736], n=(62,14)
r=5000 d=[0] [0]; mh=2 n=(62,14)
Grow: 8.761%, Prune: 12.67%, Change: 72.78%, 

[1] "Sat Jan 13 18:59:21 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Sat Jan 13 18:59:27 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=76

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=76
r=2000 d=[0]; mh=1 n=76
r=3000 d=[0]; mh=1 n=76

[1] "Sat Jan 13 18:59:36 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7064135 
gamma= 10.8042 	 alpha= 2.2965 	 beta= 10.2668 
[1] "Sat Jan 13 18:59:42 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Sat Jan 13 18:59:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=76
r=2000 d=[0]; n=76

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=76
r=2000 d=[0]; mh=1 n=76
r=3000 d=[0]; mh=1 n=76
r=4000 d=[0]; mh=1 n=76
r=5000 d=[0]; mh=1 n=76
Grow: 0%, 

[1] "Sat Jan 13 18:59:55 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cforest no default is available.
[1] "Sat Jan 13 19:00:01 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Sat Jan 13 19:00:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Sat Jan 13 19:00:16 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Sat Jan 13 19:00:22 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Sat Jan 13 19:00:28 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Sat Jan 13 19:00:35 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
[Tune] Started tuning learner regr.extraTrees for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
numRandomCuts integer   -   1 1 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=3; numRandomCuts=11
[Tune-y] 1: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 2: mtry=1; numRandomCuts=9
[Tune-y] 2: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 3: mtry=5; numRandomCuts=13
[Tune-y] 3: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 4: mtry=6; numRandomCuts=18
[Tune-y] 4: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 5: mtry=4; numRandomCuts=8
[Tune-y] 5: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 6: mtry=3; numRandomCuts=17
[Tune-y] 6: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 7: mtry=3; numRandomCuts=2
[Tune-y] 7: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 8: mtry=1; numRandomCuts=21
[Tune-y] 8: rmse.test.rmse=1.66; time: 0.0 min
[Tune-x] 9: mtry=6; numRandomCuts=19
[Tune-y] 9: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 10: mtry=5; numRandomCuts=16
[Tune-y] 10: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 11: mtry=6; numRandomCuts=13
[Tune-y] 11: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 12: mtry=9; numRandomCuts=18
[Tune-y] 12: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 13: mtry=8; numRandomCuts=14
[Tune-y] 13: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 14: mtry=6; numRandomCuts=11
[Tune-y] 14: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 15: mtry=3; numRandomCuts=7
[Tune-y] 15: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 16: mtry=8; numRandomCuts=5
[Tune-y] 16: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 17: mtry=2; numRandomCuts=2
[Tune-y] 17: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 18: mtry=4; numRandomCuts=15
[Tune-y] 18: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 19: mtry=1; numRandomCuts=8
[Tune-y] 19: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 20: mtry=6; numRandomCuts=5
[Tune-y] 20: rmse.test.rmse=1.53; time: 0.0 min
[Tune] Result: mtry=3; numRandomCuts=2 : rmse.test.rmse=1.51
[1] "Sat Jan 13 19:01:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Sat Jan 13 19:01:15 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Sat Jan 13 19:01:21 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.frbs no default is available.
[1] "Sat Jan 13 19:01:30 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
In addition: Warning messages:
1: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
2: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
3: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Sat Jan 13 19:01:39 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=46; interaction.depth=5; shrinkage=0.0163; n.minobsinnode=11
[Tune-y] 1: rmse.test.rmse= 1.7; time: 0.0 min
[Tune-x] 2: n.trees=128; interaction.depth=5; shrinkage=0.398; n.minobsinnode=19
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 2: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 3: n.trees=47; interaction.depth=3; shrinkage=0.151; n.minobsinnode=19
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: n.trees=45; interaction.depth=1; shrinkage=0.0417; n.minobsinnode=22
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: n.trees=208; interaction.depth=8; shrinkage=0.305; n.minobsinnode=17
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: n.trees=156; interaction.depth=6; shrinkage=0.597; n.minobsinnode=19
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: n.trees=389; interaction.depth=6; shrinkage=0.355; n.minobsinnode=13
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 7: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 8: n.trees=30; interaction.depth=3; shrinkage=0.481; n.minobsinnode=8
[Tune-y] 8: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 9: n.trees=21; interaction.depth=1; shrinkage=0.224; n.minobsinnode=17
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: n.trees=15; interaction.depth=4; shrinkage=0.387; n.minobsinnode=8
[Tune-y] 10: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 11: n.trees=970; interaction.depth=1; shrinkage=0.0534; n.minobsinnode=9
[Tune-y] 11: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 12: n.trees=79; interaction.depth=7; shrinkage=0.327; n.minobsinnode=11
[Tune-y] 12: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 13: n.trees=18; interaction.depth=4; shrinkage=0.585; n.minobsinnode=20
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: n.trees=417; interaction.depth=4; shrinkage=0.0959; n.minobsinnode=15
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 14: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 15: n.trees=317; interaction.depth=1; shrinkage=0.214; n.minobsinnode=5
[Tune-y] 15: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 16: n.trees=438; interaction.depth=3; shrinkage=0.203; n.minobsinnode=18
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 16: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 17: n.trees=111; interaction.depth=4; shrinkage=0.216; n.minobsinnode=21
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 17: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 18: n.trees=52; interaction.depth=10; shrinkage=0.253; n.minobsinnode=6
[Tune-y] 18: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 19: n.trees=77; interaction.depth=8; shrinkage=0.467; n.minobsinnode=12
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: n.trees=66; interaction.depth=4; shrinkage=0.386; n.minobsinnode=16
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: n.trees=317; interaction.depth=1; shrinkage=0.214; n.minobsinnode=5 : rmse.test.rmse=1.45
[1] "Sat Jan 13 19:01:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Sat Jan 13 19:01:53 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.33; lambda=0.046
[Tune-y] 1: rmse.test.rmse=0.999; time: 0.0 min
[Tune-x] 2: alpha=0.0256; lambda=0.0193
[Tune-y] 2: rmse.test.rmse=   1; time: 0.0 min
[Tune-x] 3: alpha=0.554; lambda=0.0788
[Tune-y] 3: rmse.test.rmse=0.996; time: 0.0 min
[Tune-x] 4: alpha=0.663; lambda=0.564
[Tune-y] 4: rmse.test.rmse=1.33; time: 0.0 min
[Tune-x] 5: alpha=0.336; lambda=0.0144
[Tune-y] 5: rmse.test.rmse=0.999; time: 0.0 min
[Tune-x] 6: alpha=0.25; lambda=0.409
[Tune-y] 6: rmse.test.rmse=1.08; time: 0.0 min
[Tune-x] 7: alpha=0.327; lambda=0.00182
[Tune-y] 7: rmse.test.rmse=   1; time: 0.0 min
[Tune-x] 8: alpha=0.0679; lambda=1.55
[Tune-y] 8: rmse.test.rmse=1.33; time: 0.0 min
[Tune-x] 9: alpha=0.659; lambda=0.728
[Tune-y] 9: rmse.test.rmse=1.48; time: 0.0 min
[Tune-x] 10: alpha=0.508; lambda=0.231
[Tune-y] 10: rmse.test.rmse=1.03; time: 0.0 min
[Tune-x] 11: alpha=0.597; lambda=0.0953
[Tune-y] 11: rmse.test.rmse=0.996; time: 0.0 min
[Tune-x] 12: alpha=0.995; lambda=0.553
[Tune-y] 12: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 13: alpha=0.795; lambda=0.147
[Tune-y] 13: rmse.test.rmse=0.998; time: 0.0 min
[Tune-x] 14: alpha=0.591; lambda=0.0409
[Tune-y] 14: rmse.test.rmse=0.996; time: 0.0 min
[Tune-x] 15: alpha=0.238; lambda=0.011
[Tune-y] 15: rmse.test.rmse=0.999; time: 0.0 min
[Tune-x] 16: alpha=0.802; lambda=0.00481
[Tune-y] 16: rmse.test.rmse=0.999; time: 0.0 min
[Tune-x] 17: alpha=0.159; lambda=0.00166
[Tune-y] 17: rmse.test.rmse=   1; time: 0.0 min
[Tune-x] 18: alpha=0.373; lambda=0.175
[Tune-y] 18: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 19: alpha=0.0927; lambda=0.0166
[Tune-y] 19: rmse.test.rmse=   1; time: 0.0 min
[Tune-x] 20: alpha=0.644; lambda=0.00471
[Tune-y] 20: rmse.test.rmse=0.999; time: 0.0 min
[Tune] Result: alpha=0.591; lambda=0.0409 : rmse.test.rmse=0.996
[1] "Sat Jan 13 19:02:04 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:02:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===============================                                       |  44%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:02:23 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:02:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |================================                                      |  46%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:02:41 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  9.736 9.098 7.634 8.538 8.728 11.262 10.93 10.816 9.922 
  - best initial criterion value(s) :  -147.9406 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       147.94  |proj g|=       6.1177
At iterate     1  f =       145.43  |proj g|=        1.0716
At iterate     2  f =       145.21  |proj g|=        1.2122
At iterate     3  f =       144.01  |proj g|=        1.7856
At iterate     4  f =       140.19  |proj g|=        2.8831
At iterate     5  f =       139.04  |proj g|=        6.2745
At iterate     6  f =       138.16  |proj g|=        4.2064
At iterate     7  f =       136.84  |proj g|=        2.1741
At iterate     8  f =       136.15  |proj g|=        1.8245
At iterate     9  f =       135.56  |proj g|=       0.86227
At iterate    10  f =       135.35  |proj g|=        0.4981
At iterate    11  f =        135.3  |proj g|=       0.48405
At iterate    12  f =       135.16  |proj g|=        1.0381
At iterate    13  f =       134.97  |proj g|=       0.66359
At iterate    14  f =       134.92  |proj g|=         1.882
At iterate    15  f =       134.85  |proj g|=         0.469
At iterate    16  f =       134.83  |proj g|=       0.22594
At iterate    17  f =       134.79  |proj g|=       0.60102
At iterate    18  f =       134.76  |proj g|=        0.7459
At iterate    19  f =        134.7  |proj g|=       0.41807
At iterate    20  f =       134.65  |proj g|=       0.20927
At iterate    21  f =       134.61  |proj g|=        2.6285
At iterate    22  f =       134.52  |proj g|=        1.7234
At iterate    23  f =       134.11  |proj g|=         1.008
At iterate    24  f =       133.98  |proj g|=        1.1507
At iterate    25  f =       133.85  |proj g|=        1.2069
At iterate    26  f =       133.62  |proj g|=        1.1383
At iterate    27  f =       132.87  |proj g|=       0.61648
At iterate    28  f =        132.8  |proj g|=        1.0026
At iterate    29  f =       132.75  |proj g|=       0.58404
At iterate    30  f =       132.58  |proj g|=       0.30243
At iterate    31  f =        132.5  |proj g|=        2.4048
At iterate    32  f =       132.37  |proj g|=        0.7376
At iterate    33  f =       132.31  |proj g|=       0.22655
At iterate    34  f =       132.27  |proj g|=       0.55502
At iterate    35  f =       132.23  |proj g|=       0.37735
At iterate    36  f =       132.11  |proj g|=        0.4399
At iterate    37  f =       131.93  |proj g|=       0.40248
At iterate    38  f =       131.91  |proj g|=         0.736
At iterate    39  f =       131.87  |proj g|=       0.24572
At iterate    40  f =       131.85  |proj g|=       0.27713
At iterate    41  f =       131.82  |proj g|=       0.42879
At iterate    42  f =       131.74  |proj g|=       0.36297
At iterate    43  f =       131.67  |proj g|=       0.49743
At iterate    44  f =        131.6  |proj g|=        1.0954
At iterate    45  f =       131.56  |proj g|=       0.48655
At iterate    46  f =       131.54  |proj g|=       0.13127
At iterate    47  f =       131.54  |proj g|=      0.052772
At iterate    48  f =       131.53  |proj g|=       0.04977
At iterate    49  f =       131.53  |proj g|=      0.048303
At iterate    50  f =       131.53  |proj g|=       0.37418
At iterate    51  f =       131.52  |proj g|=      0.092477
At iterate    52  f =       131.52  |proj g|=       0.15869
At iterate    53  f =       131.51  |proj g|=       0.27195
At iterate    54  f =        131.5  |proj g|=      0.099317
At iterate    55  f =        131.5  |proj g|=      0.046548
At iterate    56  f =        131.5  |proj g|=      0.032437
At iterate    57  f =        131.5  |proj g|=      0.016211
At iterate    58  f =        131.5  |proj g|=      0.025641
At iterate    59  f =        131.5  |proj g|=     0.0075831
At iterate    60  f =        131.5  |proj g|=     0.0029529
At iterate    61  f =        131.5  |proj g|=     0.0044942
At iterate    62  f =        131.5  |proj g|=    0.00091198
At iterate    63  f =        131.5  |proj g|=    0.00081571

iterations 63
function evaluations 79
segments explored during Cauchy searches 63
BFGS updates skipped 0
active bounds at final generalized Cauchy point 2
norm of the final projected gradient 0.000815711
final function value 131.5

F = 131.5
final  value 131.499562 
converged
[1] "Sat Jan 13 19:02:59 2018"
[Tune] Started tuning learner regr.ksvm for parameter set:
         Type len  Def    Constr Req Tunable Trafo
C     numeric   -    0  -5 to 10   -    TRUE     Y
sigma numeric   - TRUE -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: C=25; sigma=3.97
[Tune-y] 1: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 2: C=0.0537; sigma=0.00309
[Tune-y] 2: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 3: C=2.49; sigma=17.2
[Tune-y] 3: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 4: C=0.0587; sigma=1.33
[Tune-y] 4: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 5: C=0.248; sigma=0.000952
[Tune-y] 5: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 6: C=0.936; sigma=0.000174
[Tune-y] 6: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 7: C=0.0383; sigma=1.91e+04
[Tune-y] 7: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 8: C=276; sigma=8.15e-05
[Tune-y] 8: rmse.test.rmse=1.16; time: 0.0 min
[Tune-x] 9: C=9.26; sigma=1.07
[Tune-y] 9: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 10: C=1.69; sigma=0.00114
[Tune-y] 10: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 11: C=743; sigma=0.00208
[Tune-y] 11: rmse.test.rmse=1.13; time: 0.0 min
[Tune-x] 12: C=0.229; sigma=0.15
[Tune-y] 12: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 13: C=0.311; sigma=0.000287
[Tune-y] 13: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 14: C=0.0451; sigma=0.000663
[Tune-y] 14: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 15: C=0.206; sigma=1.35
[Tune-y] 15: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 16: C=322; sigma=1.27
[Tune-y] 16: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 17: C=2.29; sigma=3.46e-05
[Tune-y] 17: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 18: C=0.386; sigma=0.283
[Tune-y] 18: rmse.test.rmse=1.66; time: 0.0 min
[Tune-x] 19: C=9.72; sigma=2.27e+04
[Tune-y] 19: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 20: C=10.3; sigma=15.7
[Tune-y] 20: rmse.test.rmse=1.74; time: 0.0 min
[Tune] Result: C=743; sigma=0.00208 : rmse.test.rmse=1.13
[1] "Sat Jan 13 19:03:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 24), d = 14.009, its = 7
i = 2 (of 24), d = 11.2639, its = 6
i = 3 (of 24), d = 8.62071, its = 6
i = 4 (of 24), d = 9.72886, its = 6
i = 5 (of 24), d = 9.63801, its = 6
i = 6 (of 24), d = 10.2976, its = 6
i = 7 (of 24), d = 12.536, its = 6
i = 8 (of 24), d = 8.42363, its = 6
i = 9 (of 24), d = 13.7466, its = 7
i = 10 (of 24), d = 10.2984, its = 6
i = 11 (of 24), d = 9.89076, its = 6
i = 12 (of 24), d = 16.1616, its = 7
i = 13 (of 24), d = 9.40002, its = 6
i = 14 (of 24), d = 10.3143, its = 6
i = 15 (of 24), d = 9.66006, its = 6
i = 16 (of 24), d = 11.6715, its = 6
i = 17 (of 24), d = 8.40033, its = 5
i = 18 (of 24), d = 8.0088, its = 5
i = 19 (of 24), d = 11.6203, its = 6
i = 20 (of 24), d = 11.4673, its = 6
i = 21 (of 24), d = 8.77592, its = 5
i = 22 (of 24), d = 9.63237, its = 6
i = 23 (of 24), d = 10.9633, its = 6
i = 24 (of 24), d = 8.2978, its = 5
[1] "Sat Jan 13 19:03:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Sat Jan 13 19:03:21 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Sat Jan 13 19:03:27 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Sat Jan 13 19:03:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Sat Jan 13 19:03:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Sat Jan 13 19:03:45 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=7; decay=0.00367
# weights:  78
initial  value 3572.755739 
iter  10 value 155.449200
iter  20 value 83.450827
iter  30 value 50.832484
iter  40 value 38.814325
iter  50 value 36.320241
iter  60 value 35.883125
iter  70 value 30.554488
iter  80 value 27.042269
iter  90 value 25.468297
iter 100 value 21.417555
final  value 21.417555 
stopped after 100 iterations
# weights:  78
initial  value 3603.329079 
iter  10 value 72.550316
iter  20 value 29.827027
iter  30 value 13.888688
iter  40 value 9.204523
iter  50 value 7.091596
iter  60 value 6.013680
iter  70 value 5.019853
iter  80 value 4.478707
iter  90 value 4.095242
iter 100 value 3.825609
final  value 3.825609 
stopped after 100 iterations
# weights:  78
initial  value 3410.690508 
iter  10 value 56.055784
iter  20 value 29.778200
iter  30 value 19.080864
iter  40 value 14.204821
iter  50 value 10.432859
iter  60 value 7.818951
iter  70 value 6.471912
iter  80 value 4.741118
iter  90 value 4.042625
iter 100 value 3.400420
final  value 3.400420 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=2.15; time: 0.0 min
[Tune-x] 2: size=1; decay=0.000972
# weights:  12
initial  value 3779.843810 
iter  10 value 159.574145
iter  20 value 125.108283
iter  30 value 87.619205
iter  40 value 78.322495
iter  50 value 63.038273
iter  60 value 46.464391
iter  70 value 36.516149
iter  80 value 33.882784
iter  90 value 32.164454
iter 100 value 32.110696
final  value 32.110696 
stopped after 100 iterations
# weights:  12
initial  value 4205.292892 
iter  10 value 136.038305
iter  20 value 89.944079
iter  30 value 87.696089
iter  40 value 87.382467
iter  50 value 86.029913
iter  60 value 84.976744
iter  70 value 84.432121
iter  80 value 80.463431
iter  90 value 80.396911
iter 100 value 80.365625
final  value 80.365625 
stopped after 100 iterations
# weights:  12
initial  value 3662.654826 
iter  10 value 121.706139
iter  20 value 67.162013
iter  30 value 62.220129
iter  40 value 62.099444
iter  50 value 62.047916
iter  60 value 62.022474
iter  70 value 62.012618
iter  80 value 61.990497
iter  90 value 61.975261
iter 100 value 61.894310
final  value 61.894310 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 3: size=12; decay=0.00838
# weights:  133
initial  value 3562.955539 
iter  10 value 65.305432
iter  20 value 37.178212
iter  30 value 16.538716
iter  40 value 10.257549
iter  50 value 8.290256
iter  60 value 6.667925
iter  70 value 5.585940
iter  80 value 4.642427
iter  90 value 4.076961
iter 100 value 3.750961
final  value 3.750961 
stopped after 100 iterations
# weights:  133
initial  value 2878.121976 
iter  10 value 109.335551
iter  20 value 68.936672
iter  30 value 40.602650
iter  40 value 33.922078
iter  50 value 28.874711
iter  60 value 26.920413
iter  70 value 23.217215
iter  80 value 19.571322
iter  90 value 14.767123
iter 100 value 10.405508
final  value 10.405508 
stopped after 100 iterations
# weights:  133
initial  value 4572.277348 
iter  10 value 76.559714
iter  20 value 37.192753
iter  30 value 18.211453
iter  40 value 10.162874
iter  50 value 8.362099
iter  60 value 6.790866
iter  70 value 6.092750
iter  80 value 5.547497
iter  90 value 4.897165
iter 100 value 4.328033
final  value 4.328033 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=2.29; time: 0.0 min
[Tune-x] 4: size=14; decay=0.171
# weights:  155
initial  value 3604.825804 
iter  10 value 107.297055
iter  20 value 78.339080
iter  30 value 54.649985
iter  40 value 43.308851
iter  50 value 35.148762
iter  60 value 28.797674
iter  70 value 26.987448
iter  80 value 25.963848
iter  90 value 24.943928
iter 100 value 24.552299
final  value 24.552299 
stopped after 100 iterations
# weights:  155
initial  value 2786.501222 
iter  10 value 82.064261
iter  20 value 46.717869
iter  30 value 38.616890
iter  40 value 35.019543
iter  50 value 32.643376
iter  60 value 30.806918
iter  70 value 29.692711
iter  80 value 29.046756
iter  90 value 28.619084
iter 100 value 28.328737
final  value 28.328737 
stopped after 100 iterations
# weights:  155
initial  value 3509.548805 
iter  10 value 70.862167
iter  20 value 41.493044
iter  30 value 34.937364
iter  40 value 31.055910
iter  50 value 28.828537
iter  60 value 27.506905
iter  70 value 26.184695
iter  80 value 25.226242
iter  90 value 24.719975
iter 100 value 24.506318
final  value 24.506318 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 5: size=7; decay=0.00062
# weights:  78
initial  value 3457.040796 
iter  10 value 67.067030
iter  20 value 29.849327
iter  30 value 13.593697
iter  40 value 6.252577
iter  50 value 3.336659
iter  60 value 2.404681
iter  70 value 1.919819
iter  80 value 1.561224
iter  90 value 1.321256
iter 100 value 1.094561
final  value 1.094561 
stopped after 100 iterations
# weights:  78
initial  value 4147.634595 
iter  10 value 96.288938
iter  20 value 52.814312
iter  30 value 34.320882
iter  40 value 23.568687
iter  50 value 15.911458
iter  60 value 11.174571
iter  70 value 8.426845
iter  80 value 7.479144
iter  90 value 6.678631
iter 100 value 6.141928
final  value 6.141928 
stopped after 100 iterations
# weights:  78
initial  value 3421.736758 
iter  10 value 142.514368
iter  20 value 63.724425
iter  30 value 49.078150
iter  40 value 46.590773
iter  50 value 41.149095
iter  60 value 40.070195
iter  70 value 39.943775
iter  80 value 39.925842
iter  90 value 39.818974
iter 100 value 25.338189
final  value 25.338189 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=1.93; time: 0.0 min
[Tune-x] 6: size=5; decay=0.105
# weights:  56
initial  value 3266.358372 
iter  10 value 147.995375
iter  20 value 99.950753
iter  30 value 75.274805
iter  40 value 43.740368
iter  50 value 33.431900
iter  60 value 24.912620
iter  70 value 22.855498
iter  80 value 21.502761
iter  90 value 20.862465
iter 100 value 20.390569
final  value 20.390569 
stopped after 100 iterations
# weights:  56
initial  value 4451.709990 
iter  10 value 143.003078
iter  20 value 94.338813
iter  30 value 68.193336
iter  40 value 53.008244
iter  50 value 44.618169
iter  60 value 39.938895
iter  70 value 37.324712
iter  80 value 32.090139
iter  90 value 28.039424
iter 100 value 26.855859
final  value 26.855859 
stopped after 100 iterations
# weights:  56
initial  value 3773.091307 
iter  10 value 180.974335
iter  20 value 110.718966
iter  30 value 69.557033
iter  40 value 45.231284
iter  50 value 29.469581
iter  60 value 25.551181
iter  70 value 24.367963
iter  80 value 23.221973
iter  90 value 22.472931
iter 100 value 22.254132
final  value 22.254132 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 7: size=7; decay=2.59e-05
# weights:  78
initial  value 4078.061376 
iter  10 value 70.598542
iter  20 value 40.095811
iter  30 value 29.567013
iter  40 value 23.283289
iter  50 value 17.688954
iter  60 value 13.307458
iter  70 value 11.407468
iter  80 value 9.447511
iter  90 value 8.815023
iter 100 value 8.625822
final  value 8.625822 
stopped after 100 iterations
# weights:  78
initial  value 5033.429519 
iter  10 value 95.718509
iter  20 value 82.847195
iter  30 value 78.262619
iter  40 value 55.186205
iter  50 value 42.838754
iter  60 value 32.558348
iter  70 value 30.822417
iter  80 value 30.493691
iter  90 value 30.408266
iter 100 value 30.398723
final  value 30.398723 
stopped after 100 iterations
# weights:  78
initial  value 2173.373022 
iter  10 value 100.542936
iter  20 value 53.923472
iter  30 value 46.514224
iter  40 value 28.728162
iter  50 value 15.338660
iter  60 value 11.234687
iter  70 value 8.791065
iter  80 value 8.031949
iter  90 value 7.856823
iter 100 value 7.830160
final  value 7.830160 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 8: size=2; decay=0.808
# weights:  23
initial  value 3151.322910 
iter  10 value 184.161601
iter  20 value 123.297657
iter  30 value 113.771254
iter  40 value 106.656915
iter  50 value 100.833117
iter  60 value 100.572860
iter  70 value 100.554084
iter  80 value 100.548618
final  value 100.548609 
converged
# weights:  23
initial  value 3505.979173 
iter  10 value 172.544551
iter  20 value 142.318873
iter  30 value 103.738163
iter  40 value 91.886443
iter  50 value 86.522419
iter  60 value 83.396210
iter  70 value 83.343056
iter  80 value 83.321923
final  value 83.320482 
converged
# weights:  23
initial  value 3166.216302 
iter  10 value 160.727370
iter  20 value 99.183492
iter  30 value 86.007068
iter  40 value 83.093583
iter  50 value 81.967164
iter  60 value 76.453069
iter  70 value 75.454729
iter  80 value 75.425309
final  value 75.425298 
converged
[Tune-y] 8: rmse.test.rmse=1.22; time: 0.0 min
[Tune-x] 9: size=14; decay=0.254
# weights:  155
initial  value 3635.037246 
iter  10 value 94.252560
iter  20 value 62.391250
iter  30 value 46.423836
iter  40 value 41.253370
iter  50 value 36.532942
iter  60 value 34.796717
iter  70 value 34.035490
iter  80 value 33.519873
iter  90 value 32.955239
iter 100 value 32.430153
final  value 32.430153 
stopped after 100 iterations
# weights:  155
initial  value 4251.763301 
iter  10 value 78.587023
iter  20 value 58.831463
iter  30 value 46.999540
iter  40 value 43.118731
iter  50 value 40.892510
iter  60 value 38.651474
iter  70 value 37.815502
iter  80 value 37.390004
iter  90 value 37.160190
iter 100 value 36.860577
final  value 36.860577 
stopped after 100 iterations
# weights:  155
initial  value 3836.328472 
iter  10 value 73.316711
iter  20 value 47.110744
iter  30 value 38.855755
iter  40 value 35.066707
iter  50 value 33.424693
iter  60 value 32.831947
iter  70 value 32.472438
iter  80 value 32.233286
iter  90 value 32.161943
iter 100 value 32.106844
final  value 32.106844 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=1.35; time: 0.0 min
[Tune-x] 10: size=11; decay=0.0435
# weights:  122
initial  value 5168.630388 
iter  10 value 52.784963
iter  20 value 31.588256
iter  30 value 22.400758
iter  40 value 19.524061
iter  50 value 15.463473
iter  60 value 11.794320
iter  70 value 10.715660
iter  80 value 10.197047
iter  90 value 9.918629
iter 100 value 9.698168
final  value 9.698168 
stopped after 100 iterations
# weights:  122
initial  value 3368.742981 
iter  10 value 68.641471
iter  20 value 37.055531
iter  30 value 30.397167
iter  40 value 26.008822
iter  50 value 20.844268
iter  60 value 18.222725
iter  70 value 16.387038
iter  80 value 15.234330
iter  90 value 13.872225
iter 100 value 12.560902
final  value 12.560902 
stopped after 100 iterations
# weights:  122
initial  value 3880.768704 
iter  10 value 67.437172
iter  20 value 37.909247
iter  30 value 26.088332
iter  40 value 18.735653
iter  50 value 14.367663
iter  60 value 11.422660
iter  70 value 9.929726
iter  80 value 9.291442
iter  90 value 8.716653
iter 100 value 8.317886
final  value 8.317886 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=1.94; time: 0.0 min
[Tune-x] 11: size=12; decay=0.0112
# weights:  133
initial  value 3283.034962 
iter  10 value 86.991849
iter  20 value 59.579076
iter  30 value 31.715865
iter  40 value 19.165369
iter  50 value 12.974409
iter  60 value 7.918108
iter  70 value 5.761380
iter  80 value 4.504407
iter  90 value 3.911408
iter 100 value 3.532587
final  value 3.532587 
stopped after 100 iterations
# weights:  133
initial  value 3905.551227 
iter  10 value 68.670957
iter  20 value 27.890958
iter  30 value 20.840185
iter  40 value 14.174804
iter  50 value 9.283770
iter  60 value 7.526537
iter  70 value 6.213106
iter  80 value 5.079986
iter  90 value 4.540057
iter 100 value 4.244700
final  value 4.244700 
stopped after 100 iterations
# weights:  133
initial  value 2659.179040 
iter  10 value 58.270592
iter  20 value 23.116212
iter  30 value 12.729233
iter  40 value 7.225651
iter  50 value 5.660211
iter  60 value 4.687222
iter  70 value 4.050974
iter  80 value 3.634538
iter  90 value 3.451300
iter 100 value 3.224854
final  value 3.224854 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=2.25; time: 0.0 min
[Tune-x] 12: size=20; decay=0.166
# weights:  221
initial  value 3892.565208 
iter  10 value 77.866360
iter  20 value 49.639392
iter  30 value 35.931449
iter  40 value 29.963159
iter  50 value 26.533578
iter  60 value 25.316438
iter  70 value 24.561872
iter  80 value 24.095809
iter  90 value 23.399231
iter 100 value 23.063025
final  value 23.063025 
stopped after 100 iterations
# weights:  221
initial  value 3208.555794 
iter  10 value 112.974096
iter  20 value 73.632531
iter  30 value 49.022932
iter  40 value 40.707179
iter  50 value 38.365190
iter  60 value 36.956106
iter  70 value 35.663022
iter  80 value 34.595803
iter  90 value 33.897360
iter 100 value 33.084945
final  value 33.084945 
stopped after 100 iterations
# weights:  221
initial  value 1990.562765 
iter  10 value 69.255015
iter  20 value 42.707357
iter  30 value 31.458152
iter  40 value 27.579363
iter  50 value 26.048732
iter  60 value 25.458271
iter  70 value 25.040121
iter  80 value 24.563655
iter  90 value 23.957242
iter 100 value 23.704627
final  value 23.704627 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=1.39; time: 0.0 min
[Tune-x] 13: size=16; decay=0.0218
# weights:  177
initial  value 4206.036700 
iter  10 value 70.526848
iter  20 value 40.336575
iter  30 value 29.143424
iter  40 value 23.591254
iter  50 value 20.753557
iter  60 value 15.076101
iter  70 value 11.681904
iter  80 value 9.632659
iter  90 value 8.396088
iter 100 value 7.613393
final  value 7.613393 
stopped after 100 iterations
# weights:  177
initial  value 3119.073038 
iter  10 value 62.029544
iter  20 value 34.450835
iter  30 value 20.097310
iter  40 value 13.628615
iter  50 value 10.506943
iter  60 value 8.291275
iter  70 value 7.266891
iter  80 value 6.569692
iter  90 value 6.179363
iter 100 value 5.741378
final  value 5.741378 
stopped after 100 iterations
# weights:  177
initial  value 3505.251477 
iter  10 value 73.771784
iter  20 value 40.103568
iter  30 value 25.022754
iter  40 value 17.727676
iter  50 value 12.969124
iter  60 value 9.799960
iter  70 value 8.836918
iter  80 value 8.006573
iter  90 value 7.398136
iter 100 value 6.730525
final  value 6.730525 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 14: size=12; decay=0.00307
# weights:  133
initial  value 3090.051618 
iter  10 value 74.647729
iter  20 value 36.636365
iter  30 value 21.491052
iter  40 value 10.584159
iter  50 value 7.805074
iter  60 value 6.200890
iter  70 value 4.143889
iter  80 value 3.168256
iter  90 value 2.295659
iter 100 value 1.974510
final  value 1.974510 
stopped after 100 iterations
# weights:  133
initial  value 4202.085268 
iter  10 value 60.419257
iter  20 value 26.437390
iter  30 value 15.495806
iter  40 value 9.762744
iter  50 value 7.076439
iter  60 value 5.010780
iter  70 value 4.150290
iter  80 value 3.303943
iter  90 value 2.622389
iter 100 value 2.033016
final  value 2.033016 
stopped after 100 iterations
# weights:  133
initial  value 2920.960750 
iter  10 value 58.946558
iter  20 value 27.164516
iter  30 value 10.966180
iter  40 value 6.469415
iter  50 value 3.445545
iter  60 value 2.297224
iter  70 value 1.843389
iter  80 value 1.610201
iter  90 value 1.433731
iter 100 value 1.304869
final  value 1.304869 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=   2; time: 0.0 min
[Tune-x] 15: size=5; decay=0.000412
# weights:  56
initial  value 3253.870642 
iter  10 value 159.605352
iter  20 value 159.603588
final  value 159.599349 
converged
# weights:  56
initial  value 4148.691780 
iter  10 value 69.043617
iter  20 value 32.733125
iter  30 value 17.817332
iter  40 value 11.437000
iter  50 value 9.692057
iter  60 value 8.987813
iter  70 value 8.071492
iter  80 value 7.388273
iter  90 value 6.799500
iter 100 value 6.501931
final  value 6.501931 
stopped after 100 iterations
# weights:  56
initial  value 4667.093950 
iter  10 value 80.811730
iter  20 value 50.746300
iter  30 value 48.185926
iter  40 value 45.198647
iter  50 value 40.740522
iter  60 value 39.853935
iter  70 value 39.796067
iter  80 value 39.786735
iter  90 value 39.738622
iter 100 value 38.549729
final  value 38.549729 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 16: size=17; decay=0.000115
# weights:  188
initial  value 2758.656579 
iter  10 value 82.852143
iter  20 value 37.098391
iter  30 value 13.859638
iter  40 value 6.388614
iter  50 value 2.193679
iter  60 value 0.394869
iter  70 value 0.150580
iter  80 value 0.130546
iter  90 value 0.118722
iter 100 value 0.101253
final  value 0.101253 
stopped after 100 iterations
# weights:  188
initial  value 4455.162751 
iter  10 value 78.507172
iter  20 value 39.714027
iter  30 value 18.756075
iter  40 value 13.567832
iter  50 value 8.900072
iter  60 value 3.759462
iter  70 value 2.418990
iter  80 value 1.530400
iter  90 value 1.197585
iter 100 value 0.785063
final  value 0.785063 
stopped after 100 iterations
# weights:  188
initial  value 3319.800457 
iter  10 value 90.603240
iter  20 value 35.550423
iter  30 value 16.700890
iter  40 value 10.968647
iter  50 value 8.166315
iter  60 value 4.521788
iter  70 value 3.617414
iter  80 value 2.674767
iter  90 value 1.538137
iter 100 value 0.616954
final  value 0.616954 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=2.53; time: 0.0 min
[Tune-x] 17: size=4; decay=2.26e-05
# weights:  45
initial  value 3619.638499 
iter  10 value 159.537431
iter  20 value 71.460473
iter  30 value 38.379167
iter  40 value 32.920073
iter  50 value 32.092516
iter  60 value 32.008154
iter  70 value 31.926108
iter  80 value 31.856181
iter  90 value 31.848362
iter 100 value 31.843571
final  value 31.843571 
stopped after 100 iterations
# weights:  45
initial  value 4445.263695 
final  value 139.355973 
converged
# weights:  45
initial  value 2792.289718 
final  value 154.378337 
converged
[Tune-y] 17: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 18: size=8; decay=0.0284
# weights:  89
initial  value 3723.761857 
iter  10 value 152.083540
iter  20 value 85.802839
iter  30 value 51.040052
iter  40 value 41.707881
iter  50 value 37.555841
iter  60 value 34.649172
iter  70 value 26.986353
iter  80 value 20.702029
iter  90 value 16.472757
iter 100 value 13.855589
final  value 13.855589 
stopped after 100 iterations
# weights:  89
initial  value 4363.001986 
iter  10 value 185.459392
iter  20 value 113.267145
iter  30 value 86.953934
iter  40 value 50.458419
iter  50 value 38.953503
iter  60 value 30.549457
iter  70 value 21.293102
iter  80 value 13.330773
iter  90 value 11.157225
iter 100 value 10.265953
final  value 10.265953 
stopped after 100 iterations
# weights:  89
initial  value 4453.807985 
iter  10 value 60.693653
iter  20 value 29.766112
iter  30 value 18.607017
iter  40 value 13.627052
iter  50 value 10.953887
iter  60 value 9.605529
iter  70 value 8.766373
iter  80 value 8.213613
iter  90 value 7.938756
iter 100 value 7.742862
final  value 7.742862 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=1.88; time: 0.0 min
[Tune-x] 19: size=2; decay=0.000772
# weights:  23
initial  value 3641.388538 
final  value 159.602640 
converged
# weights:  23
initial  value 3530.757476 
iter  10 value 89.932111
iter  20 value 66.006942
iter  30 value 53.697504
iter  40 value 32.899054
iter  50 value 29.935964
iter  60 value 28.950187
iter  70 value 27.359861
iter  80 value 25.409775
iter  90 value 24.500066
iter 100 value 24.042283
final  value 24.042283 
stopped after 100 iterations
# weights:  23
initial  value 3684.119321 
iter  10 value 147.466651
iter  20 value 105.898235
iter  30 value 54.016251
iter  40 value 49.220564
iter  50 value 48.390746
iter  60 value 48.328228
iter  70 value 48.311485
iter  80 value 48.303385
iter  90 value 48.302437
iter 100 value 47.638086
final  value 47.638086 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=1.49; time: 0.0 min
[Tune-x] 20: size=13; decay=0.000112
# weights:  144
initial  value 2380.674963 
iter  10 value 66.449785
iter  20 value 28.343723
iter  30 value 13.228606
iter  40 value 5.206664
iter  50 value 1.781079
iter  60 value 0.957880
iter  70 value 0.404172
iter  80 value 0.167537
iter  90 value 0.136818
iter 100 value 0.121021
final  value 0.121021 
stopped after 100 iterations
# weights:  144
initial  value 5912.078699 
iter  10 value 63.580874
iter  20 value 31.094669
iter  30 value 12.505621
iter  40 value 6.257124
iter  50 value 2.496268
iter  60 value 1.157171
iter  70 value 0.365310
iter  80 value 0.220391
iter  90 value 0.169644
iter 100 value 0.146502
final  value 0.146502 
stopped after 100 iterations
# weights:  144
initial  value 3333.680203 
iter  10 value 67.408979
iter  20 value 34.298042
iter  30 value 12.363500
iter  40 value 6.420513
iter  50 value 4.590941
iter  60 value 2.321705
iter  70 value 0.960145
iter  80 value 0.494503
iter  90 value 0.305219
iter 100 value 0.149273
final  value 0.149273 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=2.38; time: 0.0 min
[Tune] Result: size=2; decay=0.808 : rmse.test.rmse=1.22
# weights:  23
initial  value 6021.364537 
iter  10 value 247.475158
iter  20 value 186.373366
iter  30 value 134.511390
iter  40 value 114.584215
iter  50 value 107.078464
iter  60 value 106.212266
iter  70 value 105.859742
final  value 105.795647 
converged
[1] "Sat Jan 13 19:03:57 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1077
 total number of nodes after removal of identical nodes : 604 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 528
 number of selected nodes                               : 56 
[1] "Sat Jan 13 19:04:23 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Sat Jan 13 19:04:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Sat Jan 13 19:04:57 2018"
[Tune] Started tuning learner regr.randomForest for parameter set:
            Type len Def  Constr Req Tunable Trafo
nodesize integer   -   1 1 to 10   -    TRUE     -
mtry     integer   -   3  1 to 9   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nodesize=4; mtry=4
[Tune-y] 1: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 2: nodesize=1; mtry=3
[Tune-y] 2: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 3: nodesize=6; mtry=5
[Tune-y] 3: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 4: nodesize=7; mtry=7
[Tune-y] 4: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 5: nodesize=4; mtry=3
[Tune-y] 5: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 6: nodesize=3; mtry=7
[Tune-y] 6: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 7: nodesize=4; mtry=1
[Tune-y] 7: rmse.test.rmse=1.66; time: 0.0 min
[Tune-x] 8: nodesize=1; mtry=8
[Tune-y] 8: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 9: nodesize=7; mtry=7
[Tune-y] 9: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 10: nodesize=6; mtry=6
[Tune-y] 10: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 11: nodesize=6; mtry=5
[Tune-y] 11: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 12: nodesize=10; mtry=7
[Tune-y] 12: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 13: nodesize=8; mtry=6
[Tune-y] 13: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 14: nodesize=6; mtry=4
[Tune-y] 14: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 15: nodesize=3; mtry=3
[Tune-y] 15: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 16: nodesize=9; mtry=2
[Tune-y] 16: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 17: nodesize=2; mtry=1
[Tune-y] 17: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 18: nodesize=4; mtry=6
[Tune-y] 18: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 19: nodesize=1; mtry=3
[Tune-y] 19: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 20: nodesize=7; mtry=2
[Tune-y] 20: rmse.test.rmse= 1.6; time: 0.0 min
[Tune] Result: nodesize=7; mtry=7 : rmse.test.rmse=1.53
[1] "Sat Jan 13 19:05:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Sat Jan 13 19:05:17 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=3; min.node.size=5
[Tune-y] 1: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 2: mtry=1; min.node.size=4
[Tune-y] 2: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 3: mtry=5; min.node.size=5
[Tune-y] 3: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 4: mtry=6; min.node.size=8
[Tune-y] 4: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 5: mtry=4; min.node.size=3
[Tune-y] 5: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 6: mtry=3; min.node.size=7
[Tune-y] 6: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 7: mtry=3; min.node.size=1
[Tune-y] 7: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 8: mtry=1; min.node.size=9
[Tune-y] 8: rmse.test.rmse=1.66; time: 0.0 min
[Tune-x] 9: mtry=6; min.node.size=8
[Tune-y] 9: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 10: mtry=5; min.node.size=7
[Tune-y] 10: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 11: mtry=6; min.node.size=6
[Tune-y] 11: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 12: mtry=9; min.node.size=8
[Tune-y] 12: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 13: mtry=8; min.node.size=6
[Tune-y] 13: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 14: mtry=6; min.node.size=5
[Tune-y] 14: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 15: mtry=3; min.node.size=3
[Tune-y] 15: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 16: mtry=8; min.node.size=2
[Tune-y] 16: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 17: mtry=2; min.node.size=1
[Tune-y] 17: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 18: mtry=4; min.node.size=6
[Tune-y] 18: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 19: mtry=1; min.node.size=4
[Tune-y] 19: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 20: mtry=6; min.node.size=2
[Tune-y] 20: rmse.test.rmse=1.53; time: 0.0 min
[Tune] Result: mtry=6; min.node.size=8 : rmse.test.rmse=1.53
[1] "Sat Jan 13 19:05:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Sat Jan 13 19:05:38 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.00961; maxdepth=14; minbucket=6; minsplit=20
[Tune-y] 1: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 2: cp=0.0453; maxdepth=16; minbucket=35; minsplit=37
[Tune-y] 2: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 3: cp=0.01; maxdepth=11; minbucket=16; minsplit=35
[Tune-y] 3: rmse.test.rmse=1.78; time: 0.0 min
[Tune-x] 4: cp=0.00943; maxdepth=4; minbucket=8; minsplit=42
[Tune-y] 4: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 5: cp=0.0943; maxdepth=23; minbucket=28; minsplit=32
[Tune-y] 5: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 6: cp=0.0611; maxdepth=17; minbucket=50; minsplit=37
[Tune-y] 6: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 7: cp=0.242; maxdepth=18; minbucket=32; minsplit=24
[Tune-y] 7: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 8: cp=0.00507; maxdepth=10; minbucket=41; minsplit=13
[Tune-y] 8: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 9: cp=0.00293; maxdepth=4; minbucket=22; minsplit=31
[Tune-y] 9: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 10: cp=0.00186; maxdepth=11; minbucket=34; minsplit=13
[Tune-y] 10: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 11: cp=0.954; maxdepth=3; minbucket=9; minsplit=14
[Tune-y] 11: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 12: cp=0.0221; maxdepth=20; minbucket=30; minsplit=19
[Tune-y] 12: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 13: cp=0.0023; maxdepth=13; minbucket=49; minsplit=39
[Tune-y] 13: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 14: cp=0.268; maxdepth=12; minbucket=12; minsplit=27
[Tune-y] 14: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 15: cp=0.178; maxdepth=4; minbucket=21; minsplit=6
[Tune-y] 15: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 16: cp=0.289; maxdepth=10; minbucket=20; minsplit=34
[Tune-y] 16: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 17: cp=0.0367; maxdepth=13; minbucket=21; minsplit=40
[Tune-y] 17: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 18: cp=0.0116; maxdepth=30; minbucket=24; minsplit=7
[Tune-y] 18: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 19: cp=0.0211; maxdepth=24; minbucket=40; minsplit=22
[Tune-y] 19: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 20: cp=0.0169; maxdepth=13; minbucket=34; minsplit=31
[Tune-y] 20: rmse.test.rmse=1.77; time: 0.0 min
[Tune] Result: cp=0.00293; maxdepth=4; minbucket=22; minsplit=31 : rmse.test.rmse=1.73
[1] "Sat Jan 13 19:05:46 2018"
[Tune] Started tuning learner regr.RRF for parameter set:
           Type len Def Constr Req Tunable Trafo
mtry    integer   -   3 1 to 9   -    TRUE     -
coefReg numeric   - 0.8 0 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=3; coefReg=0.428
[Tune-y] 1: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 2: mtry=1; coefReg=0.331
[Tune-y] 2: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 3: mtry=5; coefReg=0.487
[Tune-y] 3: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 4: mtry=6; coefReg=0.706
[Tune-y] 4: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 5: mtry=4; coefReg=0.299
[Tune-y] 5: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 6: mtry=3; coefReg=0.67
[Tune-y] 6: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 7: mtry=3; coefReg=0.069
[Tune-y] 7: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 8: mtry=1; coefReg=0.818
[Tune-y] 8: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 9: mtry=6; coefReg=0.734
[Tune-y] 9: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 10: mtry=5; coefReg=0.606
[Tune-y] 10: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 11: mtry=6; coefReg=0.508
[Tune-y] 11: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 12: mtry=9; coefReg=0.704
[Tune-y] 12: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 13: mtry=8; coefReg=0.556
[Tune-y] 13: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 14: mtry=6; coefReg=0.414
[Tune-y] 14: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 15: mtry=3; coefReg=0.269
[Tune-y] 15: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 16: mtry=8; coefReg=0.177
[Tune-y] 16: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 17: mtry=2; coefReg=0.0589
[Tune-y] 17: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 18: mtry=4; coefReg=0.576
[Tune-y] 18: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 19: mtry=1; coefReg=0.315
[Tune-y] 19: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 20: mtry=6; coefReg=0.175
[Tune-y] 20: rmse.test.rmse=1.46; time: 0.0 min
[Tune] Result: mtry=6; coefReg=0.175 : rmse.test.rmse=1.46
[1] "Sat Jan 13 19:06:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Sat Jan 13 19:06:06 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Sat Jan 13 19:06:12 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.458 0.358 0.279 0.218 0.170
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 1 -----> 4 
Runtime: 0.01099992 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.2791 
    Y 1          8.149 
    Y 2          8.653 
    Y 3          8.555 
    Y 4          8.804 
    Y 5          8.287 
[1] "Sat Jan 13 19:06:19 2018"
[Tune] Started tuning learner regr.svm for parameter set:
         Type len   Def    Constr Req Tunable Trafo
cost  numeric   -     0 -15 to 15   -    TRUE     Y
gamma numeric   - -3.17 -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cost=0.0291; gamma=0.222
[Tune-y] 1: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 2: cost=5.2e-05; gamma=0.0299
[Tune-y] 2: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 3: cost=3.06; gamma=0.767
[Tune-y] 3: rmse.test.rmse=1.79; time: 0.0 min
[Tune-x] 4: cost=29.4; gamma=72
[Tune-y] 4: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 5: cost=0.0328; gamma=0.0152
[Tune-y] 5: rmse.test.rmse=1.72; time: 0.0 min
[Tune-x] 6: cost=0.00551; gamma=34.3
[Tune-y] 6: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 7: cost=0.0274; gamma=0.000128
[Tune-y] 7: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 8: cost=0.000125; gamma=742
[Tune-y] 8: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 9: cost=27.5; gamma=130
[Tune-y] 9: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 10: cost=1.19; gamma=9.14
[Tune-y] 10: rmse.test.rmse=1.76; time: 0.0 min
[Tune-x] 11: cost=7.46; gamma=1.19
[Tune-y] 11: rmse.test.rmse=1.78; time: 0.0 min
[Tune-x] 12: cost=2.92e+04; gamma=68.9
[Tune-y] 12: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 13: cost=464; gamma=3.22
[Tune-y] 13: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 14: cost=6.57; gamma=0.169
[Tune-y] 14: rmse.test.rmse=1.79; time: 0.0 min
[Tune-x] 15: cost=0.00428; gamma=0.00822
[Tune-y] 15: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 16: cost=534; gamma=0.00121
[Tune-y] 16: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 17: cost=0.000827; gamma=0.000104
[Tune-y] 17: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 18: cost=0.0709; gamma=4.82
[Tune-y] 18: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 19: cost=0.00021; gamma=0.0212
[Tune-y] 19: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 20: cost=20; gamma=0.00115
[Tune-y] 20: rmse.test.rmse= 1.1; time: 0.0 min
[Tune] Result: cost=534; gamma=0.00121 : rmse.test.rmse=1.01
[1] "Sat Jan 13 19:06:28 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=72; max_depth=5; eta=0.0163; gamma=3.31; colsample_bytree=0.521; min_child_weight=9.74; subsample=0.747
[Tune-y] 1: rmse.test.rmse=2.94; time: 0.0 min
[Tune-x] 2: nrounds=686; max_depth=4; eta=0.18; gamma=2.5; colsample_bytree=0.568; min_child_weight=6.54; subsample=0.302
[Tune-y] 2: rmse.test.rmse= 1.5; time: 0.1 min
[Tune-x] 3: nrounds=15; max_depth=9; eta=0.396; gamma=7.34; colsample_bytree=0.503; min_child_weight=12.1; subsample=0.697
[Tune-y] 3: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 4: nrounds=210; max_depth=10; eta=0.422; gamma=7.95; colsample_bytree=0.522; min_child_weight=11.8; subsample=0.561
[Tune-y] 4: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 5: nrounds=42; max_depth=3; eta=0.481; gamma=1.77; colsample_bytree=0.363; min_child_weight=1.18; subsample=0.53
[Tune-y] 5: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 6: nrounds=315; max_depth=1; eta=0.189; gamma=6.44; colsample_bytree=0.37; min_child_weight=19.9; subsample=0.262
[Tune-y] 6: rmse.test.rmse=2.03; time: 0.0 min
[Tune-x] 7: nrounds=17; max_depth=3; eta=0.271; gamma=6.36; colsample_bytree=0.518; min_child_weight=6.4; subsample=0.343
[Tune-y] 7: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 8: nrounds=91; max_depth=10; eta=0.447; gamma=8.1; colsample_bytree=0.44; min_child_weight=3.17; subsample=0.623
[Tune-y] 8: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 9: nrounds=898; max_depth=1; eta=0.214; gamma=0.41; colsample_bytree=0.628; min_child_weight=5.38; subsample=0.503
[Tune-y] 9: rmse.test.rmse=1.51; time: 0.1 min
[Tune-x] 10: nrounds=440; max_depth=6; eta=0.219; gamma=3.59; colsample_bytree=0.609; min_child_weight=7.14; subsample=0.977
[Tune-y] 10: rmse.test.rmse=1.53; time: 0.1 min
[Tune-x] 11: nrounds=124; max_depth=1; eta=0.267; gamma=7.79; colsample_bytree=0.611; min_child_weight=7.44; subsample=0.558
[Tune-y] 11: rmse.test.rmse=1.48; time: 0.0 min
[Tune-x] 12: nrounds=101; max_depth=7; eta=0.34; gamma=0.522; colsample_bytree=0.389; min_child_weight=8.42; subsample=0.728
[Tune-y] 12: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 13: nrounds=14; max_depth=6; eta=0.12; gamma=1.65; colsample_bytree=0.431; min_child_weight=1.67; subsample=0.265
[Tune-y] 13: rmse.test.rmse=2.18; time: 0.0 min
[Tune-x] 14: nrounds=3.43e+03; max_depth=9; eta=0.0293; gamma=5.47; colsample_bytree=0.501; min_child_weight=7.68; subsample=0.38
[Tune-y] 14: rmse.test.rmse=1.47; time: 0.3 min
[Tune-x] 15: nrounds=3.32e+03; max_depth=3; eta=0.116; gamma=4.09; colsample_bytree=0.388; min_child_weight=2.16; subsample=0.277
[Tune-y] 15: rmse.test.rmse=1.49; time: 0.4 min
[Tune-x] 16: nrounds=24; max_depth=2; eta=0.309; gamma=8.89; colsample_bytree=0.505; min_child_weight=8.26; subsample=0.255
[Tune-y] 16: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 17: nrounds=43; max_depth=5; eta=0.332; gamma=9.82; colsample_bytree=0.523; min_child_weight=12.7; subsample=0.417
[Tune-y] 17: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 18: nrounds=127; max_depth=7; eta=0.00779; gamma=2.67; colsample_bytree=0.366; min_child_weight=15.2; subsample=0.992
[Tune-y] 18: rmse.test.rmse=3.35; time: 0.0 min
[Tune-x] 19: nrounds=102; max_depth=10; eta=0.427; gamma=1.01; colsample_bytree=0.55; min_child_weight=16.2; subsample=0.856
[Tune-y] 19: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 20: nrounds=291; max_depth=9; eta=0.457; gamma=1.48; colsample_bytree=0.399; min_child_weight=19.4; subsample=0.911
[Tune-y] 20: rmse.test.rmse=1.73; time: 0.0 min
[Tune] Result: nrounds=3.43e+03; max_depth=9; eta=0.0293; gamma=5.47; colsample_bytree=0.501; min_child_weight=7.68; subsample=0.38 : rmse.test.rmse=1.47
[1] "Sat Jan 13 19:07:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Sat Jan 13 19:07:56 2018"
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  These variables have zero variances: V11, V12
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  These variables have zero variances: V11, V12
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [2,0.311057], n=(16,60)
**PRUNE** @depth 0: [2,0.317213]
**GROW** @depth 0: [1,0.424404], n=(14,62)
**PRUNE** @depth 0: [1,0.424404]
**GROW** @depth 0: [3,0.535237], n=(56,20)
**GROW** @depth 1: [1,0.671939], n=(38,17)
**PRUNE** @depth 1: [3,0.52764]
**PRUNE** @depth 0: [1,0.6818]
**GROW** @depth 0: [2,0.47417], n=(62,14)
**PRUNE** @depth 0: [2,0.469114]
**GROW** @depth 0: [4,0.385336], n=(21,55)
**PRUNE** @depth 0: [4,0.369876]
**GROW** @depth 0: [1,0.424404], n=(14,62)
r=1000 d=[0] [0]; n=(14,62)
**PRUNE** @depth 0: [1,0.424404]
**GROW** @depth 0: [4,0.496838], n=(51,25)
**PRUNE** @depth 0: [4,0.505973]
**GROW** @depth 0: [2,0.491537], n=(63,13)
**PRUNE** @depth 0: [2,0.534403]
**GROW** @depth 0: [5,0.473862], n=(44,32)
**PRUNE** @depth 0: [5,0.473862]
**GROW** @depth 0: [1,0.520953], n=(19,57)
**PRUNE** @depth 0: [1,0.513969]
**GROW** @depth 0: [1,0.695768], n=(60,16)
r=2000 d=[0] [0]; n=(64,12)

Sampling @ nn=0 pred locs:
**PRUNE** @depth 0: [1,0.778965]
**GROW** @depth 0: [3,0.444066], n=(48,28)
**PRUNE** @depth 0: [3,0.444066]
**GROW** @depth 0: [2,0.425148], n=(56,20)
**GROW** @depth 1: [1,0.637634], n=(17,37)
**PRUNE** @depth 1: [1,0.600247]
r=1000 d=[0] [0]; mh=2 n=(61,15)
r=2000 d=[0] [0]; mh=2 n=(59,17)
**PRUNE** @depth 0: [2,0.47417]
**GROW** @depth 0: [2,0.491537], n=(63,13)
**PRUNE** @depth 0: [2,0.534403]
**GROW** @depth 0: [1,0.413928], n=(13,63)
**PRUNE** @depth 0: [1,0.439606]
**GROW** @depth 0: [1,0.424404], n=(14,62)
**PRUNE** @depth 0: [1,0.424404]
**GROW** @depth 0: [1,0.648316], n=(25,51)
**PRUNE** @depth 0: [1,0.648316]
r=3000 d=[0]; mh=2 n=76
**GROW** @depth 0: [4,0.496838], n=(51,25)
**PRUNE** @depth 0: [4,0.496838]
**GROW** @depth 0: [4,0.496838], n=(51,25)
**PRUNE** @depth 0: [4,0.505973]
**GROW** @depth 0: [1,0.489934], n=(17,59)
**PRUNE** @depth 0: [1,0.481512]
**GROW** @depth 0: [1,0.481512], n=(16,60)
**PRUNE** @depth 0: [1,0.481512]
**GROW** @depth 0: [2,0.331941], n=(20,56)
**GROW** @depth 1: [4,0.496838], n=(39,19)
**PRUNE** @depth 1: [4,0.496838]
**PRUNE** @depth 0: [2,0.317872]
**GROW** @depth 0: [2,0.534403], n=(64,12)
**PRUNE** @depth 0: [2,0.534403]
**GROW** @depth 0: [2,0.491537], n=(63,13)
r=4000 d=[0] [0]; mh=2 n=(61,15)
**GROW** @depth 1: [1,0.640304], n=(18,40)
**PRUNE** @depth 1: [1,0.640304]
**GROW** @depth 1: [1,0.481512], n=(12,46)
**PRUNE** @depth 1: [1,0.481512]
**PRUNE** @depth 0: [2,0.469114]
**GROW** @depth 0: [1,0.403657], n=(12,64)
**PRUNE** @depth 0: [1,0.403657]
**GROW** @depth 0: [2,0.355683], n=(46,30)
**PRUNE** @depth 0: [2,0.355683]
**GROW** @depth 0: [2,0.340954], n=(44,32)
**GROW** @depth 1: [2,0.425148], n=(12,20)
**PRUNE** @depth 1: [2,0.42273]
**PRUNE** @depth 0: [2,0.318312]
r=5000 d=[0]; mh=2 n=76
Grow: 9.27%, Prune: 15%, Change: 63.76%, Swap: 27.27%

[1] "Sat Jan 13 19:09:18 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Sat Jan 13 19:09:24 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=76

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=76
r=2000 d=[0]; mh=1 n=76
r=3000 d=[0]; mh=1 n=76

[1] "Sat Jan 13 19:09:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7064135 
gamma= 14.4953 	 alpha= 1.9352 	 beta= 10.0404 
[1] "Sat Jan 13 19:09:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Sat Jan 13 19:09:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=76
r=2000 d=[0]; n=76

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=76
r=2000 d=[0]; mh=1 n=76
r=3000 d=[0]; mh=1 n=76
r=4000 d=[0]; mh=1 n=76
r=5000 d=[0]; mh=1 n=76
Grow: 0%, 

[1] "Sat Jan 13 19:09:52 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cforest no default is available.
[1] "Sat Jan 13 19:09:58 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Sat Jan 13 19:10:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Sat Jan 13 19:10:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Sat Jan 13 19:10:19 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Sat Jan 13 19:10:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Sat Jan 13 19:10:32 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
[Tune] Started tuning learner regr.extraTrees for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
numRandomCuts integer   -   1 1 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=2; numRandomCuts=25
[Tune-y] 1: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 2: mtry=7; numRandomCuts=18
[Tune-y] 2: rmse.test.rmse=1.49; time: 0.0 min
[Tune-x] 3: mtry=8; numRandomCuts=21
[Tune-y] 3: rmse.test.rmse=1.51; time: 0.1 min
[Tune-x] 4: mtry=5; numRandomCuts=18
[Tune-y] 4: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 5: mtry=6; numRandomCuts=12
[Tune-y] 5: rmse.test.rmse= 1.5; time: 0.0 min
[Tune-x] 6: mtry=5; numRandomCuts=3
[Tune-y] 6: rmse.test.rmse= 1.5; time: 0.0 min
[Tune-x] 7: mtry=8; numRandomCuts=5
[Tune-y] 7: rmse.test.rmse=1.48; time: 0.0 min
[Tune-x] 8: mtry=7; numRandomCuts=17
[Tune-y] 8: rmse.test.rmse=1.49; time: 0.0 min
[Tune-x] 9: mtry=5; numRandomCuts=24
[Tune-y] 9: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 10: mtry=2; numRandomCuts=12
[Tune-y] 10: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 11: mtry=1; numRandomCuts=9
[Tune-y] 11: rmse.test.rmse=1.72; time: 0.0 min
[Tune-x] 12: mtry=7; numRandomCuts=6
[Tune-y] 12: rmse.test.rmse=1.51; time: 0.1 min
[Tune-x] 13: mtry=3; numRandomCuts=24
[Tune-y] 13: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 14: mtry=2; numRandomCuts=7
[Tune-y] 14: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 15: mtry=5; numRandomCuts=3
[Tune-y] 15: rmse.test.rmse=1.48; time: 0.1 min
[Tune-x] 16: mtry=4; numRandomCuts=12
[Tune-y] 16: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 17: mtry=7; numRandomCuts=22
[Tune-y] 17: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 18: mtry=7; numRandomCuts=21
[Tune-y] 18: rmse.test.rmse=1.52; time: 0.0 min
[Tune-x] 19: mtry=3; numRandomCuts=19
[Tune-y] 19: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 20: mtry=1; numRandomCuts=2
[Tune-y] 20: rmse.test.rmse=1.68; time: 0.0 min
[Tune] Result: mtry=8; numRandomCuts=5 : rmse.test.rmse=1.48
[1] "Sat Jan 13 19:11:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Sat Jan 13 19:11:23 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Sat Jan 13 19:11:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.frbs no default is available.
[1] "Sat Jan 13 19:11:36 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
In addition: Warning messages:
1: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
2: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
3: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Sat Jan 13 19:11:45 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=21; interaction.depth=10; shrinkage=0.41; n.minobsinnode=19
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 1: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 2: n.trees=427; interaction.depth=9; shrinkage=0.314; n.minobsinnode=19
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 2: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 3: n.trees=201; interaction.depth=5; shrinkage=0.314; n.minobsinnode=6
[Tune-y] 3: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 4: n.trees=566; interaction.depth=2; shrinkage=0.409; n.minobsinnode=18
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: n.trees=112; interaction.depth=10; shrinkage=0.12; n.minobsinnode=14
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: n.trees=14; interaction.depth=4; shrinkage=0.413; n.minobsinnode=9
[Tune-y] 6: rmse.test.rmse= 1.7; time: 0.0 min
[Tune-x] 7: n.trees=42; interaction.depth=10; shrinkage=0.09; n.minobsinnode=10
[Tune-y] 7: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 8: n.trees=112; interaction.depth=1; shrinkage=0.208; n.minobsinnode=14
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 8: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 9: n.trees=300; interaction.depth=9; shrinkage=0.412; n.minobsinnode=21
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: n.trees=29; interaction.depth=8; shrinkage=0.0279; n.minobsinnode=6
[Tune-y] 10: rmse.test.rmse=1.81; time: 0.0 min
[Tune-x] 11: n.trees=93; interaction.depth=10; shrinkage=0.524; n.minobsinnode=12
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 11: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 12: n.trees=48; interaction.depth=3; shrinkage=0.315; n.minobsinnode=6
[Tune-y] 12: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 13: n.trees=14; interaction.depth=4; shrinkage=0.0996; n.minobsinnode=22
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: n.trees=153; interaction.depth=6; shrinkage=0.0513; n.minobsinnode=5
[Tune-y] 14: rmse.test.rmse=1.47; time: 0.0 min
[Tune-x] 15: n.trees=14; interaction.depth=5; shrinkage=0.502; n.minobsinnode=7
[Tune-y] 15: rmse.test.rmse=1.69; time: 0.0 min
[Tune-x] 16: n.trees=152; interaction.depth=7; shrinkage=0.196; n.minobsinnode=23
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 16: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 17: n.trees=12; interaction.depth=5; shrinkage=0.539; n.minobsinnode=13
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 17: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 18: n.trees=45; interaction.depth=5; shrinkage=0.189; n.minobsinnode=11
[Tune-y] 18: rmse.test.rmse=1.76; time: 0.0 min
[Tune-x] 19: n.trees=153; interaction.depth=2; shrinkage=0.0129; n.minobsinnode=17
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: n.trees=569; interaction.depth=2; shrinkage=0.111; n.minobsinnode=8
[Tune-y] 20: rmse.test.rmse=1.74; time: 0.0 min
[Tune] Result: n.trees=153; interaction.depth=6; shrinkage=0.0513; n.minobsinnode=5 : rmse.test.rmse=1.47
[1] "Sat Jan 13 19:11:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Sat Jan 13 19:11:59 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.158; lambda=6.84
[Tune-y] 1: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 2: alpha=0.682; lambda=0.54
[Tune-y] 2: rmse.test.rmse=1.44; time: 0.0 min
[Tune-x] 3: alpha=0.815; lambda=1.72
[Tune-y] 3: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 4: alpha=0.522; lambda=0.546
[Tune-y] 4: rmse.test.rmse=1.34; time: 0.0 min
[Tune-x] 5: alpha=0.652; lambda=0.0591
[Tune-y] 5: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 6: alpha=0.523; lambda=0.00203
[Tune-y] 6: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 7: alpha=0.876; lambda=0.0046
[Tune-y] 7: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 8: alpha=0.682; lambda=0.315
[Tune-y] 8: rmse.test.rmse=1.18; time: 0.0 min
[Tune-x] 9: alpha=0.525; lambda=4.56
[Tune-y] 9: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 10: alpha=0.199; lambda=0.0702
[Tune-y] 10: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 11: alpha=0.0751; lambda=0.0193
[Tune-y] 11: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 12: alpha=0.688; lambda=0.00701
[Tune-y] 12: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 13: alpha=0.311; lambda=5.58
[Tune-y] 13: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 14: alpha=0.149; lambda=0.00852
[Tune-y] 14: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 15: alpha=0.524; lambda=0.00207
[Tune-y] 15: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 16: alpha=0.345; lambda=0.0526
[Tune-y] 16: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 17: alpha=0.739; lambda=2.67
[Tune-y] 17: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 18: alpha=0.686; lambda=1.37
[Tune-y] 18: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 19: alpha=0.23; lambda=0.803
[Tune-y] 19: rmse.test.rmse=1.34; time: 0.0 min
[Tune-x] 20: alpha=0.0449; lambda=0.00161
[Tune-y] 20: rmse.test.rmse=1.01; time: 0.0 min
[Tune] Result: alpha=0.688; lambda=0.00701 : rmse.test.rmse=1.01
[1] "Sat Jan 13 19:12:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:12:20 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |====================================                                  |  52%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:12:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:12:37 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==================================                                    |  48%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:12:47 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  10.38461 11.36373 10.59997 10.13085 9.81111 12.27093 12.40865 12.31216 12.58212 
  - best initial criterion value(s) :  -141.3568 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       141.36  |proj g|=       10.161
At iterate     1  f =       139.98  |proj g|=        8.9002
At iterate     2  f =       136.84  |proj g|=        2.2869
At iterate     3  f =       135.67  |proj g|=        2.2867
At iterate     4  f =       132.64  |proj g|=        8.2385
At iterate     5  f =       130.63  |proj g|=       0.85969
At iterate     6  f =       130.47  |proj g|=       0.83441
At iterate     7  f =       129.98  |proj g|=       0.84043
At iterate     8  f =       129.67  |proj g|=        1.4271
At iterate     9  f =       129.42  |proj g|=        1.0282
At iterate    10  f =       129.09  |proj g|=       0.47082
At iterate    11  f =       128.85  |proj g|=       0.62679
At iterate    12  f =       128.41  |proj g|=        1.5011
At iterate    13  f =       128.08  |proj g|=       0.87061
At iterate    14  f =       127.96  |proj g|=        1.0528
At iterate    15  f =       127.83  |proj g|=       0.22543
At iterate    16  f =        127.8  |proj g|=        0.1881
At iterate    17  f =       127.78  |proj g|=       0.14025
At iterate    18  f =       127.75  |proj g|=      0.073766
At iterate    19  f =       127.75  |proj g|=       0.20442
At iterate    20  f =       127.75  |proj g|=      0.036912
At iterate    21  f =       127.75  |proj g|=      0.036492
At iterate    22  f =       127.74  |proj g|=      0.059244
At iterate    23  f =       127.74  |proj g|=       0.10023
At iterate    24  f =       127.73  |proj g|=       0.18192
At iterate    25  f =       127.71  |proj g|=       0.12052
At iterate    26  f =        127.7  |proj g|=      0.078328
At iterate    27  f =        127.7  |proj g|=       0.26018
At iterate    28  f =        127.7  |proj g|=      0.034748
At iterate    29  f =        127.7  |proj g|=      0.027913
At iterate    30  f =        127.7  |proj g|=      0.039459
At iterate    31  f =        127.7  |proj g|=      0.017077
At iterate    32  f =        127.7  |proj g|=      0.014089
At iterate    33  f =        127.7  |proj g|=     0.0091168
At iterate    34  f =        127.7  |proj g|=     0.0032755
At iterate    35  f =        127.7  |proj g|=     0.0018711
At iterate    36  f =        127.7  |proj g|=     0.0028246
At iterate    37  f =        127.7  |proj g|=    0.00066389
At iterate    38  f =        127.7  |proj g|=    0.00065416

iterations 38
function evaluations 43
segments explored during Cauchy searches 38
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0.000654159
final function value 127.698

F = 127.698
final  value 127.698174 
converged
[1] "Sat Jan 13 19:13:04 2018"
[Tune] Started tuning learner regr.ksvm for parameter set:
         Type len  Def    Constr Req Tunable Trafo
C     numeric   -    0  -5 to 10   -    TRUE     Y
sigma numeric   - TRUE -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: C=0.21; sigma=0.000646
[Tune-y] 1: rmse.test.rmse=1.85; time: 0.0 min
[Tune-x] 2: C=15.6; sigma=0.112
[Tune-y] 2: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 3: C=0.639; sigma=0.0114
[Tune-y] 3: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 4: C=76; sigma=435
[Tune-y] 4: rmse.test.rmse=1.86; time: 0.0 min
[Tune-x] 5: C=265; sigma=0.0901
[Tune-y] 5: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 6: C=1.18; sigma=0.000917
[Tune-y] 6: rmse.test.rmse=1.79; time: 0.0 min
[Tune-x] 7: C=521; sigma=0.00561
[Tune-y] 7: rmse.test.rmse=1.39; time: 0.0 min
[Tune-x] 8: C=1.39; sigma=15.1
[Tune-y] 8: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 9: C=6.47; sigma=0.00019
[Tune-y] 9: rmse.test.rmse=1.78; time: 0.0 min
[Tune-x] 10: C=0.933; sigma=0.00466
[Tune-y] 10: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 11: C=26.6; sigma=0.0965
[Tune-y] 11: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 12: C=0.279; sigma=0.000219
[Tune-y] 12: rmse.test.rmse=1.86; time: 0.0 min
[Tune-x] 13: C=772; sigma=0.0286
[Tune-y] 13: rmse.test.rmse=1.83; time: 0.0 min
[Tune-x] 14: C=0.185; sigma=2.86
[Tune-y] 14: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 15: C=862; sigma=3.01e+04
[Tune-y] 15: rmse.test.rmse=1.86; time: 0.0 min
[Tune-x] 16: C=0.752; sigma=2.13e+04
[Tune-y] 16: rmse.test.rmse=1.88; time: 0.0 min
[Tune-x] 17: C=82.8; sigma=0.000228
[Tune-y] 17: rmse.test.rmse=1.13; time: 0.0 min
[Tune-x] 18: C=0.0342; sigma=606
[Tune-y] 18: rmse.test.rmse=1.86; time: 0.0 min
[Tune-x] 19: C=52.4; sigma=742
[Tune-y] 19: rmse.test.rmse=1.86; time: 0.0 min
[Tune-x] 20: C=6.98; sigma=0.000165
[Tune-y] 20: rmse.test.rmse=1.79; time: 0.0 min
[Tune] Result: C=82.8; sigma=0.000228 : rmse.test.rmse=1.13
[1] "Sat Jan 13 19:13:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 24), d = 17.0696, its = 6
i = 2 (of 24), d = 19.9926, its = 7
i = 3 (of 24), d = 19.4705, its = 7
i = 4 (of 24), d = 19.3622, its = 7
i = 5 (of 24), d = 18.7788, its = 7
i = 6 (of 24), d = 18.9213, its = 6
i = 7 (of 24), d = 16.5031, its = 6
i = 8 (of 24), d = 22.8191, its = 7
i = 9 (of 24), d = 22.3889, its = 7
i = 10 (of 24), d = 20.4568, its = 7
i = 11 (of 24), d = 18.428, its = 6
i = 12 (of 24), d = 20.0588, its = 7
i = 13 (of 24), d = 22.3633, its = 7
i = 14 (of 24), d = 19.4128, its = 7
i = 15 (of 24), d = 16.2441, its = 6
i = 16 (of 24), d = 17.0813, its = 6
i = 17 (of 24), d = 19.097, its = 7
i = 18 (of 24), d = 17.8029, its = 6
i = 19 (of 24), d = 17.4077, its = 6
i = 20 (of 24), d = 19.1607, its = 7
i = 21 (of 24), d = 27.7189, its = 7
i = 22 (of 24), d = 26.1632, its = 7
i = 23 (of 24), d = 20.8685, its = 7
i = 24 (of 24), d = 17.8798, its = 7
[1] "Sat Jan 13 19:13:20 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Sat Jan 13 19:13:26 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Sat Jan 13 19:13:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Sat Jan 13 19:13:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Sat Jan 13 19:13:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Sat Jan 13 19:13:50 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=4; decay=7.86
# weights:  45
initial  value 3332.547766 
iter  10 value 889.335344
iter  20 value 329.190956
iter  30 value 281.593542
iter  40 value 260.591330
iter  50 value 257.643150
iter  60 value 257.605305
final  value 257.603833 
converged
# weights:  45
initial  value 4055.036719 
iter  10 value 1070.025200
iter  20 value 440.310366
iter  30 value 300.630854
iter  40 value 267.474483
iter  50 value 260.970105
iter  60 value 259.659231
final  value 259.621718 
converged
# weights:  45
initial  value 4213.629355 
iter  10 value 1086.013855
iter  20 value 341.121633
iter  30 value 289.673307
iter  40 value 278.070764
iter  50 value 275.932514
iter  60 value 275.877491
final  value 275.877389 
converged
[Tune-y] 1: rmse.test.rmse=1.26; time: 0.0 min
[Tune-x] 2: size=14; decay=0.16
# weights:  155
initial  value 4396.244479 
iter  10 value 42.495404
iter  20 value 27.040565
iter  30 value 22.328179
iter  40 value 19.717927
iter  50 value 18.969988
iter  60 value 18.686300
iter  70 value 18.424978
iter  80 value 18.326971
iter  90 value 18.278644
iter 100 value 18.212754
final  value 18.212754 
stopped after 100 iterations
# weights:  155
initial  value 3035.965024 
iter  10 value 36.457736
iter  20 value 24.214829
iter  30 value 19.318715
iter  40 value 16.851258
iter  50 value 16.280328
iter  60 value 16.014259
iter  70 value 15.890855
iter  80 value 15.843849
iter  90 value 15.662764
iter 100 value 15.495292
final  value 15.495292 
stopped after 100 iterations
# weights:  155
initial  value 3038.644208 
iter  10 value 32.151536
iter  20 value 19.394154
iter  30 value 16.526637
iter  40 value 15.158061
iter  50 value 14.819823
iter  60 value 14.498027
iter  70 value 14.296925
iter  80 value 14.225360
iter  90 value 14.187244
iter 100 value 14.166320
final  value 14.166320 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=1.32; time: 0.0 min
[Tune-x] 3: size=17; decay=0.944
# weights:  188
initial  value 3382.349732 
iter  10 value 79.843290
iter  20 value 57.567061
iter  30 value 52.576017
iter  40 value 51.262981
iter  50 value 50.370382
iter  60 value 50.167108
iter  70 value 50.109768
iter  80 value 50.093550
iter  90 value 50.087282
iter 100 value 50.085921
final  value 50.085921 
stopped after 100 iterations
# weights:  188
initial  value 4045.821529 
iter  10 value 104.139481
iter  20 value 58.435625
iter  30 value 49.588557
iter  40 value 47.418565
iter  50 value 46.954952
iter  60 value 46.827777
iter  70 value 46.792918
iter  80 value 46.775011
iter  90 value 46.754876
iter 100 value 46.721907
final  value 46.721907 
stopped after 100 iterations
# weights:  188
initial  value 3822.026507 
iter  10 value 62.510936
iter  20 value 48.980431
iter  30 value 46.947140
iter  40 value 46.645539
iter  50 value 46.625600
iter  60 value 46.617275
iter  70 value 46.614148
iter  80 value 46.613157
iter  90 value 46.613034
final  value 46.613026 
converged
[Tune-y] 3: rmse.test.rmse=1.04; time: 0.0 min
[Tune-x] 4: size=11; decay=0.163
# weights:  122
initial  value 2532.691762 
iter  10 value 49.128551
iter  20 value 32.441374
iter  30 value 24.713294
iter  40 value 21.730283
iter  50 value 20.394967
iter  60 value 19.837454
iter  70 value 19.504252
iter  80 value 19.388369
iter  90 value 19.205458
iter 100 value 18.869417
final  value 18.869417 
stopped after 100 iterations
# weights:  122
initial  value 2608.383481 
iter  10 value 44.205860
iter  20 value 30.630962
iter  30 value 22.147853
iter  40 value 18.588326
iter  50 value 17.163690
iter  60 value 16.765228
iter  70 value 16.545858
iter  80 value 16.467230
iter  90 value 16.377846
iter 100 value 16.240840
final  value 16.240840 
stopped after 100 iterations
# weights:  122
initial  value 3098.714598 
iter  10 value 56.288874
iter  20 value 31.996358
iter  30 value 23.772443
iter  40 value 18.362023
iter  50 value 16.986531
iter  60 value 16.184618
iter  70 value 15.728953
iter  80 value 15.497020
iter  90 value 15.369216
iter 100 value 15.276725
final  value 15.276725 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=1.26; time: 0.0 min
[Tune-x] 5: size=14; decay=0.00539
# weights:  155
initial  value 3896.355795 
iter  10 value 28.462852
iter  20 value 6.931237
iter  30 value 2.982155
iter  40 value 2.068401
iter  50 value 1.777038
iter  60 value 1.514376
iter  70 value 1.381892
iter  80 value 1.255183
iter  90 value 1.184610
iter 100 value 1.126914
final  value 1.126914 
stopped after 100 iterations
# weights:  155
initial  value 3469.399445 
iter  10 value 15.318247
iter  20 value 4.926539
iter  30 value 1.896132
iter  40 value 1.661803
iter  50 value 1.402132
iter  60 value 1.288435
iter  70 value 1.200247
iter  80 value 1.099572
iter  90 value 1.012066
iter 100 value 0.936143
final  value 0.936143 
stopped after 100 iterations
# weights:  155
initial  value 3639.018842 
iter  10 value 20.781856
iter  20 value 6.480447
iter  30 value 2.100495
iter  40 value 1.787640
iter  50 value 1.455519
iter  60 value 1.308540
iter  70 value 1.169534
iter  80 value 1.081324
iter  90 value 0.999074
iter 100 value 0.931443
final  value 0.931443 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 6: size=11; decay=3.08e-05
# weights:  122
initial  value 2974.163026 
iter  10 value 26.148852
iter  20 value 5.398041
iter  30 value 1.483781
iter  40 value 0.141894
iter  50 value 0.021181
iter  60 value 0.016797
iter  70 value 0.016677
iter  80 value 0.015142
iter  90 value 0.013738
iter 100 value 0.012861
final  value 0.012861 
stopped after 100 iterations
# weights:  122
initial  value 2129.233969 
iter  10 value 16.756915
iter  20 value 3.096160
iter  30 value 0.208040
iter  40 value 0.028010
iter  50 value 0.012404
iter  60 value 0.011960
iter  70 value 0.011742
iter  80 value 0.010581
iter  90 value 0.009855
iter 100 value 0.009475
final  value 0.009475 
stopped after 100 iterations
# weights:  122
initial  value 3726.803733 
iter  10 value 20.831864
iter  20 value 4.054597
iter  30 value 0.482083
iter  40 value 0.029537
iter  50 value 0.009920
iter  60 value 0.009725
iter  70 value 0.009427
iter  80 value 0.008841
iter  90 value 0.008209
iter 100 value 0.007755
final  value 0.007755 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=1.78; time: 0.0 min
[Tune-x] 7: size=18; decay=0.000108
# weights:  199
initial  value 3484.134309 
iter  10 value 23.291515
iter  20 value 3.133445
iter  30 value 0.202823
iter  40 value 0.030012
iter  50 value 0.029762
iter  60 value 0.028597
iter  70 value 0.025892
iter  80 value 0.023789
iter  90 value 0.022029
iter 100 value 0.021320
final  value 0.021320 
stopped after 100 iterations
# weights:  199
initial  value 3549.858897 
iter  10 value 17.665947
iter  20 value 2.634916
iter  30 value 0.103138
iter  40 value 0.034765
iter  50 value 0.034042
iter  60 value 0.033565
iter  70 value 0.028846
iter  80 value 0.025353
iter  90 value 0.024065
iter 100 value 0.022800
final  value 0.022800 
stopped after 100 iterations
# weights:  199
initial  value 3661.275502 
iter  10 value 17.714607
iter  20 value 3.095111
iter  30 value 0.103835
iter  40 value 0.029511
iter  50 value 0.028458
iter  60 value 0.025876
iter  70 value 0.022641
iter  80 value 0.020784
iter  90 value 0.019677
iter 100 value 0.018854
final  value 0.018854 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=1.48; time: 0.0 min
[Tune-x] 8: size=14; decay=0.0701
# weights:  155
initial  value 2897.529965 
iter  10 value 38.031092
iter  20 value 17.893827
iter  30 value 13.258421
iter  40 value 11.145776
iter  50 value 10.375790
iter  60 value 9.728832
iter  70 value 9.417266
iter  80 value 9.275219
iter  90 value 9.091483
iter 100 value 8.907968
final  value 8.907968 
stopped after 100 iterations
# weights:  155
initial  value 4149.269960 
iter  10 value 28.720205
iter  20 value 14.852521
iter  30 value 10.077890
iter  40 value 8.725420
iter  50 value 8.387362
iter  60 value 8.185385
iter  70 value 8.037425
iter  80 value 7.910883
iter  90 value 7.771250
iter 100 value 7.665876
final  value 7.665876 
stopped after 100 iterations
# weights:  155
initial  value 5870.539602 
iter  10 value 23.565035
iter  20 value 11.964551
iter  30 value 8.689992
iter  40 value 7.979746
iter  50 value 7.391246
iter  60 value 7.099082
iter  70 value 6.999129
iter  80 value 6.936008
iter  90 value 6.874840
iter 100 value 6.831943
final  value 6.831943 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=1.31; time: 0.0 min
[Tune-x] 9: size=11; decay=4.22
# weights:  122
initial  value 4356.353622 
iter  10 value 167.420034
iter  20 value 130.821492
iter  30 value 121.800643
iter  40 value 117.356877
iter  50 value 116.912042
iter  60 value 116.909573
final  value 116.909562 
converged
# weights:  122
initial  value 2157.858071 
iter  10 value 168.612357
iter  20 value 119.186293
iter  30 value 115.470904
iter  40 value 114.534060
iter  50 value 114.408621
iter  60 value 114.407841
final  value 114.407838 
converged
# weights:  122
initial  value 3514.308393 
iter  10 value 154.005617
iter  20 value 131.113116
iter  30 value 124.511969
iter  40 value 121.226797
iter  50 value 120.713530
iter  60 value 120.691028
final  value 120.690760 
converged
[Tune-y] 9: rmse.test.rmse=1.08; time: 0.0 min
[Tune-x] 10: size=4; decay=0.00702
# weights:  45
initial  value 3584.501911 
iter  10 value 122.198097
iter  20 value 101.766939
iter  30 value 87.569706
iter  40 value 65.525203
iter  50 value 52.024920
iter  60 value 34.078272
iter  70 value 25.678226
iter  80 value 21.119265
iter  90 value 17.489380
iter 100 value 14.072102
final  value 14.072102 
stopped after 100 iterations
# weights:  45
initial  value 3376.219834 
iter  10 value 145.013138
iter  20 value 69.870040
iter  30 value 58.837652
iter  40 value 46.858403
iter  50 value 34.925164
iter  60 value 26.438876
iter  70 value 20.430997
iter  80 value 17.586702
iter  90 value 15.073718
iter 100 value 13.658325
final  value 13.658325 
stopped after 100 iterations
# weights:  45
initial  value 3046.145206 
iter  10 value 157.182206
iter  20 value 114.162666
iter  30 value 97.142784
iter  40 value 72.800167
iter  50 value 60.676716
iter  60 value 44.732638
iter  70 value 33.983675
iter  80 value 22.614987
iter  90 value 17.615972
iter 100 value 15.432173
final  value 15.432173 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 11: size=2; decay=0.000968
# weights:  23
initial  value 3400.590578 
iter  10 value 66.975634
iter  20 value 42.083057
iter  30 value 27.603801
iter  40 value 25.434728
iter  50 value 24.908756
iter  60 value 24.696312
iter  70 value 23.947396
iter  80 value 22.689364
iter  90 value 20.832548
iter 100 value 20.536806
final  value 20.536806 
stopped after 100 iterations
# weights:  23
initial  value 2933.529324 
iter  10 value 41.621341
iter  20 value 31.635940
iter  30 value 23.689344
iter  40 value 21.440813
iter  50 value 18.421962
iter  60 value 17.622760
iter  70 value 16.967004
iter  80 value 16.742355
iter  90 value 16.735761
iter 100 value 16.731915
final  value 16.731915 
stopped after 100 iterations
# weights:  23
initial  value 3182.074516 
iter  10 value 64.621955
iter  20 value 46.043778
iter  30 value 29.413058
iter  40 value 26.984652
iter  50 value 26.870543
iter  60 value 26.808132
iter  70 value 23.968051
iter  80 value 22.831404
iter  90 value 22.729796
iter 100 value 22.706757
final  value 22.706757 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=1.38; time: 0.0 min
[Tune-x] 12: size=14; decay=0.000205
# weights:  155
initial  value 3159.334064 
iter  10 value 30.800393
iter  20 value 7.761365
iter  30 value 1.455665
iter  40 value 0.150504
iter  50 value 0.085480
iter  60 value 0.081350
iter  70 value 0.072588
iter  80 value 0.067439
iter  90 value 0.062191
iter 100 value 0.058984
final  value 0.058984 
stopped after 100 iterations
# weights:  155
initial  value 3862.223592 
iter  10 value 24.084759
iter  20 value 5.668204
iter  30 value 0.567184
iter  40 value 0.112666
iter  50 value 0.082827
iter  60 value 0.081117
iter  70 value 0.071965
iter  80 value 0.067333
iter  90 value 0.064206
iter 100 value 0.062536
final  value 0.062536 
stopped after 100 iterations
# weights:  155
initial  value 4431.580043 
iter  10 value 18.620019
iter  20 value 2.103166
iter  30 value 0.133940
iter  40 value 0.039352
iter  50 value 0.038354
iter  60 value 0.036105
iter  70 value 0.033114
iter  80 value 0.031427
iter  90 value 0.030645
iter 100 value 0.030177
final  value 0.030177 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=1.47; time: 0.0 min
[Tune-x] 13: size=7; decay=5.75
# weights:  78
initial  value 3671.523270 
iter  10 value 244.257391
iter  20 value 187.145430
iter  30 value 175.524163
iter  40 value 169.356121
iter  50 value 167.978383
iter  60 value 167.959598
final  value 167.959530 
converged
# weights:  78
initial  value 3602.396736 
iter  10 value 214.375073
iter  20 value 178.500536
iter  30 value 170.990189
iter  40 value 166.588553
iter  50 value 166.484622
final  value 166.484226 
converged
# weights:  78
initial  value 4017.000867 
iter  10 value 232.213691
iter  20 value 191.493918
iter  30 value 181.847440
iter  40 value 177.243152
iter  50 value 176.209351
iter  60 value 176.111771
final  value 176.110321 
converged
[Tune-y] 13: rmse.test.rmse=1.13; time: 0.0 min
[Tune-x] 14: size=3; decay=0.000277
# weights:  34
initial  value 2704.764606 
iter  10 value 152.888127
iter  20 value 149.314391
iter  30 value 148.008222
iter  40 value 127.017191
iter  50 value 126.546172
iter  60 value 126.253321
iter  70 value 124.964499
iter  80 value 124.720207
iter  90 value 123.435559
iter 100 value 122.138251
final  value 122.138251 
stopped after 100 iterations
# weights:  34
initial  value 3914.991944 
iter  10 value 100.491204
iter  20 value 48.669976
iter  30 value 41.469314
iter  40 value 39.832793
iter  50 value 38.788160
iter  60 value 38.048471
iter  70 value 37.978061
iter  80 value 35.975513
iter  90 value 30.098488
iter 100 value 26.343193
final  value 26.343193 
stopped after 100 iterations
# weights:  34
initial  value 4211.564544 
iter  10 value 98.729617
iter  20 value 62.179784
iter  30 value 45.795977
iter  40 value 35.802531
iter  50 value 26.386723
iter  60 value 23.537510
iter  70 value 22.096888
iter  80 value 20.048419
iter  90 value 19.262324
iter 100 value 18.289666
final  value 18.289666 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=2.39; time: 0.0 min
[Tune-x] 15: size=11; decay=3.17e-05
# weights:  122
initial  value 3847.234467 
iter  10 value 37.087489
iter  20 value 17.356509
iter  30 value 5.771739
iter  40 value 2.079580
iter  50 value 0.325779
iter  60 value 0.019279
iter  70 value 0.012019
iter  80 value 0.011927
iter  90 value 0.011296
iter 100 value 0.010423
final  value 0.010423 
stopped after 100 iterations
# weights:  122
initial  value 3683.182365 
iter  10 value 71.534490
iter  20 value 6.348163
iter  30 value 0.683681
iter  40 value 0.048321
iter  50 value 0.010524
iter  60 value 0.009526
iter  70 value 0.009428
iter  80 value 0.008911
iter  90 value 0.008415
iter 100 value 0.008046
final  value 0.008046 
stopped after 100 iterations
# weights:  122
initial  value 4309.584906 
iter  10 value 15.067543
iter  20 value 2.759104
iter  30 value 0.249826
iter  40 value 0.009708
iter  50 value 0.006793
iter  60 value 0.006695
iter  70 value 0.006584
iter  80 value 0.006218
iter  90 value 0.005963
iter 100 value 0.005787
final  value 0.005787 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=1.83; time: 0.0 min
[Tune-x] 16: size=7; decay=0.00451
# weights:  78
initial  value 3432.229803 
iter  10 value 39.848987
iter  20 value 15.357258
iter  30 value 6.683706
iter  40 value 4.303486
iter  50 value 2.608043
iter  60 value 2.004546
iter  70 value 1.618687
iter  80 value 1.441815
iter  90 value 1.271047
iter 100 value 1.178929
final  value 1.178929 
stopped after 100 iterations
# weights:  78
initial  value 4120.155547 
iter  10 value 27.779488
iter  20 value 8.478785
iter  30 value 2.509011
iter  40 value 1.721354
iter  50 value 1.447182
iter  60 value 1.244588
iter  70 value 1.148811
iter  80 value 1.071507
iter  90 value 1.022120
iter 100 value 0.961082
final  value 0.961082 
stopped after 100 iterations
# weights:  78
initial  value 3760.802287 
iter  10 value 25.939507
iter  20 value 8.600467
iter  30 value 2.800507
iter  40 value 1.732051
iter  50 value 1.283688
iter  60 value 1.116707
iter  70 value 1.027769
iter  80 value 0.940145
iter  90 value 0.866742
iter 100 value 0.820860
final  value 0.820860 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 17: size=15; decay=1.86
# weights:  166
initial  value 2800.358048 
iter  10 value 118.357163
iter  20 value 78.377212
iter  30 value 71.059339
iter  40 value 68.703614
iter  50 value 68.128101
iter  60 value 67.985814
iter  70 value 67.970400
iter  80 value 67.969286
iter  90 value 67.969215
iter  90 value 67.969214
iter  90 value 67.969214
final  value 67.969214 
converged
# weights:  166
initial  value 2897.165301 
iter  10 value 143.238134
iter  20 value 79.258818
iter  30 value 70.737425
iter  40 value 66.326390
iter  50 value 65.268229
iter  60 value 65.013566
iter  70 value 64.965388
iter  80 value 64.953267
iter  90 value 64.947317
iter 100 value 64.946748
final  value 64.946748 
stopped after 100 iterations
# weights:  166
initial  value 3833.118117 
iter  10 value 96.052581
iter  20 value 71.306927
iter  30 value 68.751998
iter  40 value 68.422356
iter  50 value 68.321868
iter  60 value 68.311548
iter  70 value 68.311066
final  value 68.311042 
converged
[Tune-y] 17: rmse.test.rmse=1.04; time: 0.0 min
[Tune-x] 18: size=14; decay=0.671
# weights:  155
initial  value 3124.040466 
iter  10 value 84.145626
iter  20 value 58.532761
iter  30 value 49.010569
iter  40 value 46.264089
iter  50 value 45.081084
iter  60 value 44.205226
iter  70 value 43.837638
iter  80 value 43.616062
iter  90 value 43.500538
iter 100 value 43.450769
final  value 43.450769 
stopped after 100 iterations
# weights:  155
initial  value 3372.007917 
iter  10 value 111.304248
iter  20 value 51.207146
iter  30 value 43.128994
iter  40 value 41.228225
iter  50 value 40.646862
iter  60 value 40.156505
iter  70 value 39.799899
iter  80 value 39.724504
iter  90 value 39.672079
iter 100 value 39.647722
final  value 39.647722 
stopped after 100 iterations
# weights:  155
initial  value 3811.634146 
iter  10 value 51.054255
iter  20 value 42.901727
iter  30 value 40.091909
iter  40 value 39.191125
iter  50 value 38.880751
iter  60 value 38.840228
iter  70 value 38.825151
iter  80 value 38.817732
iter  90 value 38.815521
iter 100 value 38.815133
final  value 38.815133 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=1.07; time: 0.0 min
[Tune-x] 19: size=5; decay=0.295
# weights:  56
initial  value 2915.861979 
iter  10 value 346.410456
iter  20 value 201.727850
iter  30 value 95.018884
iter  40 value 56.874310
iter  50 value 47.218283
iter  60 value 40.859118
iter  70 value 35.514293
iter  80 value 32.256987
iter  90 value 30.970360
iter 100 value 30.276650
final  value 30.276650 
stopped after 100 iterations
# weights:  56
initial  value 3496.041407 
iter  10 value 469.371379
iter  20 value 282.923390
iter  30 value 150.369555
iter  40 value 76.883941
iter  50 value 60.121757
iter  60 value 50.602040
iter  70 value 39.892082
iter  80 value 36.521162
iter  90 value 33.972116
iter 100 value 31.580148
final  value 31.580148 
stopped after 100 iterations
# weights:  56
initial  value 3429.798179 
iter  10 value 61.972211
iter  20 value 41.694740
iter  30 value 34.301772
iter  40 value 30.757348
iter  50 value 27.766513
iter  60 value 27.015590
iter  70 value 26.814247
iter  80 value 26.748730
iter  90 value 26.730445
iter 100 value 26.729037
final  value 26.729037 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=1.19; time: 0.0 min
[Tune-x] 20: size=1; decay=2.14e-05
# weights:  12
initial  value 3611.496842 
iter  10 value 53.786772
iter  20 value 43.149459
iter  30 value 37.061634
iter  40 value 36.150436
iter  50 value 35.120371
iter  60 value 34.961992
iter  70 value 34.932003
iter  80 value 34.904571
final  value 34.903972 
converged
# weights:  12
initial  value 3664.130155 
iter  10 value 55.181448
iter  20 value 46.835606
iter  30 value 36.498025
iter  40 value 33.473313
iter  50 value 32.264296
iter  60 value 32.208185
iter  70 value 32.171434
final  value 32.169675 
converged
# weights:  12
initial  value 4167.670956 
iter  10 value 66.077038
iter  20 value 54.225415
iter  30 value 41.076589
iter  40 value 35.312268
iter  50 value 34.497046
iter  60 value 34.490841
final  value 34.489043 
converged
[Tune-y] 20: rmse.test.rmse=1.03; time: 0.0 min
[Tune] Result: size=1; decay=2.14e-05 : rmse.test.rmse=1.03
# weights:  12
initial  value 4548.749572 
iter  10 value 254.411475
iter  20 value 250.787438
iter  30 value 250.629737
iter  40 value 234.712219
iter  50 value 233.032038
iter  60 value 232.192837
iter  70 value 231.809651
iter  80 value 231.806356
final  value 231.806308 
converged
[1] "Sat Jan 13 19:14:01 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1031
 total number of nodes after removal of identical nodes : 567 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 491
 number of selected nodes                               : 58 
[1] "Sat Jan 13 19:14:24 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Sat Jan 13 19:14:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Sat Jan 13 19:15:00 2018"
[Tune] Started tuning learner regr.randomForest for parameter set:
            Type len Def  Constr Req Tunable Trafo
nodesize integer   -   1 1 to 10   -    TRUE     -
mtry     integer   -   3  1 to 9   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nodesize=2; mtry=9
[Tune-y] 1: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 2: nodesize=7; mtry=7
[Tune-y] 2: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 3: nodesize=9; mtry=8
[Tune-y] 3: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 4: nodesize=6; mtry=7
[Tune-y] 4: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 5: nodesize=7; mtry=5
[Tune-y] 5: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 6: nodesize=6; mtry=1
[Tune-y] 6: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 7: nodesize=9; mtry=2
[Tune-y] 7: rmse.test.rmse=1.69; time: 0.0 min
[Tune-x] 8: nodesize=7; mtry=6
[Tune-y] 8: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 9: nodesize=6; mtry=9
[Tune-y] 9: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 10: nodesize=2; mtry=5
[Tune-y] 10: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 11: nodesize=1; mtry=3
[Tune-y] 11: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 12: nodesize=7; mtry=2
[Tune-y] 12: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 13: nodesize=4; mtry=9
[Tune-y] 13: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 14: nodesize=2; mtry=3
[Tune-y] 14: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 15: nodesize=6; mtry=1
[Tune-y] 15: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 16: nodesize=4; mtry=4
[Tune-y] 16: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 17: nodesize=8; mtry=8
[Tune-y] 17: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 18: nodesize=7; mtry=8
[Tune-y] 18: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 19: nodesize=3; mtry=7
[Tune-y] 19: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 20: nodesize=1; mtry=1
[Tune-y] 20: rmse.test.rmse=1.74; time: 0.0 min
[Tune] Result: nodesize=2; mtry=9 : rmse.test.rmse=1.57
[1] "Sat Jan 13 19:15:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Sat Jan 13 19:15:20 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=2; min.node.size=10
[Tune-y] 1: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 2: mtry=7; min.node.size=8
[Tune-y] 2: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 3: mtry=8; min.node.size=9
[Tune-y] 3: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 4: mtry=5; min.node.size=8
[Tune-y] 4: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 5: mtry=6; min.node.size=5
[Tune-y] 5: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 6: mtry=5; min.node.size=1
[Tune-y] 6: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 7: mtry=8; min.node.size=2
[Tune-y] 7: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 8: mtry=7; min.node.size=7
[Tune-y] 8: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 9: mtry=5; min.node.size=10
[Tune-y] 9: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 10: mtry=2; min.node.size=5
[Tune-y] 10: rmse.test.rmse=1.66; time: 0.0 min
[Tune-x] 11: mtry=1; min.node.size=4
[Tune-y] 11: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 12: mtry=7; min.node.size=3
[Tune-y] 12: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 13: mtry=3; min.node.size=10
[Tune-y] 13: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 14: mtry=2; min.node.size=3
[Tune-y] 14: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 15: mtry=5; min.node.size=1
[Tune-y] 15: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 16: mtry=4; min.node.size=5
[Tune-y] 16: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 17: mtry=7; min.node.size=9
[Tune-y] 17: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 18: mtry=7; min.node.size=9
[Tune-y] 18: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 19: mtry=3; min.node.size=8
[Tune-y] 19: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 20: mtry=1; min.node.size=1
[Tune-y] 20: rmse.test.rmse=1.75; time: 0.0 min
[Tune] Result: mtry=6; min.node.size=5 : rmse.test.rmse=1.57
[1] "Sat Jan 13 19:15:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Sat Jan 13 19:15:41 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.00291; maxdepth=30; minbucket=36; minsplit=37
[Tune-y] 1: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 2: cp=0.278; maxdepth=26; minbucket=29; minsplit=37
[Tune-y] 2: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 3: cp=0.0895; maxdepth=15; minbucket=29; minsplit=8
[Tune-y] 3: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 4: cp=0.424; maxdepth=7; minbucket=36; minsplit=34
[Tune-y] 4: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 5: cp=0.0371; maxdepth=29; minbucket=14; minsplit=26
[Tune-y] 5: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 6: cp=0.00164; maxdepth=12; minbucket=36; minsplit=15
[Tune-y] 6: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 7: cp=0.00844; maxdepth=29; minbucket=11; minsplit=16
[Tune-y] 7: rmse.test.rmse=1.86; time: 0.0 min
[Tune-x] 8: cp=0.0369; maxdepth=5; minbucket=20; minsplit=25
[Tune-y] 8: rmse.test.rmse=1.95; time: 0.0 min
[Tune-x] 9: cp=0.164; maxdepth=27; minbucket=36; minsplit=42
[Tune-y] 9: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 10: cp=0.00481; maxdepth=23; minbucket=7; minsplit=7
[Tune-y] 10: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 11: cp=0.0279; maxdepth=29; minbucket=45; minsplit=22
[Tune-y] 11: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 12: cp=0.0102; maxdepth=10; minbucket=29; minsplit=8
[Tune-y] 12: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 13: cp=0.00161; maxdepth=12; minbucket=12; minsplit=43
[Tune-y] 13: rmse.test.rmse=1.86; time: 0.0 min
[Tune-x] 14: cp=0.059; maxdepth=17; minbucket=8; minsplit=5
[Tune-y] 14: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 15: cp=0.00162; maxdepth=16; minbucket=43; minsplit=11
[Tune-y] 15: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 16: cp=0.0588; maxdepth=19; minbucket=19; minsplit=44
[Tune-y] 16: rmse.test.rmse=1.95; time: 0.0 min
[Tune-x] 17: cp=0.00133; maxdepth=14; minbucket=46; minsplit=23
[Tune-y] 17: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 18: cp=0.00931; maxdepth=16; minbucket=19; minsplit=18
[Tune-y] 18: rmse.test.rmse=1.95; time: 0.0 min
[Tune-x] 19: cp=0.0591; maxdepth=6; minbucket=5; minsplit=32
[Tune-y] 19: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 20: cp=0.428; maxdepth=6; minbucket=13; minsplit=11
[Tune-y] 20: rmse.test.rmse=1.99; time: 0.0 min
[Tune] Result: cp=0.059; maxdepth=17; minbucket=8; minsplit=5 : rmse.test.rmse=1.64
[1] "Sat Jan 13 19:15:49 2018"
[Tune] Started tuning learner regr.RRF for parameter set:
           Type len Def Constr Req Tunable Trafo
mtry    integer   -   3 1 to 9   -    TRUE     -
coefReg numeric   - 0.8 0 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=2; coefReg=0.983
[Tune-y] 1: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 2: mtry=7; coefReg=0.701
[Tune-y] 2: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 3: mtry=8; coefReg=0.829
[Tune-y] 3: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 4: mtry=5; coefReg=0.702
[Tune-y] 4: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 5: mtry=6; coefReg=0.455
[Tune-y] 5: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 6: mtry=5; coefReg=0.0814
[Tune-y] 6: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 7: mtry=8; coefReg=0.172
[Tune-y] 7: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 8: mtry=7; coefReg=0.641
[Tune-y] 8: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 9: mtry=5; coefReg=0.938
[Tune-y] 9: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 10: mtry=2; coefReg=0.474
[Tune-y] 10: rmse.test.rmse=1.66; time: 0.0 min
[Tune-x] 11: mtry=1; coefReg=0.331
[Tune-y] 11: rmse.test.rmse=1.76; time: 0.0 min
[Tune-x] 12: mtry=7; coefReg=0.219
[Tune-y] 12: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 13: mtry=3; coefReg=0.96
[Tune-y] 13: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 14: mtry=2; coefReg=0.24
[Tune-y] 14: rmse.test.rmse=1.66; time: 0.0 min
[Tune-x] 15: mtry=5; coefReg=0.0835
[Tune-y] 15: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 16: mtry=4; coefReg=0.442
[Tune-y] 16: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 17: mtry=7; coefReg=0.878
[Tune-y] 17: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 18: mtry=7; coefReg=0.804
[Tune-y] 18: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 19: mtry=3; coefReg=0.745
[Tune-y] 19: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 20: mtry=1; coefReg=0.0552
[Tune-y] 20: rmse.test.rmse=1.77; time: 0.0 min
[Tune] Result: mtry=8; coefReg=0.172 : rmse.test.rmse=1.53
[1] "Sat Jan 13 19:16:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Sat Jan 13 19:16:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Sat Jan 13 19:16:15 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.495 0.379 0.290 0.222 0.170
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 1 -----> 4 
Runtime: 0.01199985 secs 

 Values of predicted responses: 
   index             3 
   lambda         0.29 
    Y 1          7.399 
    Y 2          7.109 
    Y 3          8.082 
    Y 4          8.575 
    Y 5          8.341 
[1] "Sat Jan 13 19:16:21 2018"
[Tune] Started tuning learner regr.svm for parameter set:
         Type len   Def    Constr Req Tunable Trafo
cost  numeric   -     0 -15 to 15   -    TRUE     Y
gamma numeric   - -3.17 -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cost=0.000809; gamma=2.28e+04
[Tune-y] 1: rmse.test.rmse=1.95; time: 0.0 min
[Tune-x] 2: cost=44.2; gamma=65.1
[Tune-y] 2: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 3: cost=706; gamma=938
[Tune-y] 3: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 4: cost=1.6; gamma=66.9
[Tune-y] 4: rmse.test.rmse=   2; time: 0.0 min
[Tune-x] 5: cost=23.5; gamma=0.395
[Tune-y] 5: rmse.test.rmse=1.81; time: 0.0 min
[Tune-x] 6: cost=1.62; gamma=0.000166
[Tune-y] 6: rmse.test.rmse=1.93; time: 0.0 min
[Tune-x] 7: cost=2.5e+03; gamma=0.00109
[Tune-y] 7: rmse.test.rmse=1.11; time: 0.0 min
[Tune-x] 8: cost=43.8; gamma=18.8
[Tune-y] 8: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 9: cost=1.67; gamma=8.94e+03
[Tune-y] 9: rmse.test.rmse=   2; time: 0.0 min
[Tune-x] 10: cost=0.00189; gamma=0.587
[Tune-y] 10: rmse.test.rmse=1.95; time: 0.0 min
[Tune-x] 11: cost=0.000145; gamma=0.0297
[Tune-y] 11: rmse.test.rmse=1.95; time: 0.0 min
[Tune-x] 12: cost=50; gamma=0.00289
[Tune-y] 12: rmse.test.rmse=1.05; time: 0.0 min
[Tune-x] 13: cost=0.0197; gamma=1.42e+04
[Tune-y] 13: rmse.test.rmse=1.96; time: 0.0 min
[Tune-x] 14: cost=0.000671; gamma=0.00452
[Tune-y] 14: rmse.test.rmse=1.95; time: 0.0 min
[Tune-x] 15: cost=1.64; gamma=0.000173
[Tune-y] 15: rmse.test.rmse=1.93; time: 0.0 min
[Tune-x] 16: cost=0.0402; gamma=0.302
[Tune-y] 16: rmse.test.rmse=1.95; time: 0.0 min
[Tune-x] 17: cost=144; gamma=2.6e+03
[Tune-y] 17: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 18: cost=48.1; gamma=562
[Tune-y] 18: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 19: cost=0.00365; gamma=163
[Tune-y] 19: rmse.test.rmse=1.96; time: 0.0 min
[Tune-x] 20: cost=7.77e-05; gamma=9.62e-05
[Tune-y] 20: rmse.test.rmse=1.95; time: 0.0 min
[Tune] Result: cost=50; gamma=0.00289 : rmse.test.rmse=1.05
[1] "Sat Jan 13 19:16:30 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=26; max_depth=10; eta=0.41; gamma=7.01; colsample_bytree=0.626; min_child_weight=16.6; subsample=0.642
[Tune-y] 1: rmse.test.rmse=1.95; time: 0.0 min
[Tune-x] 2: nrounds=671; max_depth=7; eta=0.274; gamma=5.23; colsample_bytree=0.333; min_child_weight=17.5; subsample=0.379
[Tune-y] 2: rmse.test.rmse=1.91; time: 0.0 min
[Tune-x] 3: nrounds=594; max_depth=7; eta=0.315; gamma=9.38; colsample_bytree=0.379; min_child_weight=9.49; subsample=0.306
[Tune-y] 3: rmse.test.rmse=1.85; time: 0.0 min
[Tune-x] 4: nrounds=73; max_depth=7; eta=0.132; gamma=3.11; colsample_bytree=0.684; min_child_weight=2.97; subsample=0.43
[Tune-y] 4: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 5: nrounds=231; max_depth=1; eta=0.208; gamma=4.42; colsample_bytree=0.596; min_child_weight=17.6; subsample=0.765
[Tune-y] 5: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 6: nrounds=1.24e+03; max_depth=3; eta=0.447; gamma=0.449; colsample_bytree=0.322; min_child_weight=9.67; subsample=0.953
[Tune-y] 6: rmse.test.rmse=1.61; time: 0.2 min
[Tune-x] 7: nrounds=1.87e+03; max_depth=4; eta=0.204; gamma=2.73; colsample_bytree=0.509; min_child_weight=1.46; subsample=0.304
[Tune-y] 7: rmse.test.rmse=1.56; time: 0.3 min
[Tune-x] 8: nrounds=73; max_depth=2; eta=0.5; gamma=5.92; colsample_bytree=0.51; min_child_weight=1.68; subsample=0.256
[Tune-y] 8: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 9: nrounds=16; max_depth=5; eta=0.502; gamma=1.37; colsample_bytree=0.537; min_child_weight=12; subsample=0.494
[Tune-y] 9: rmse.test.rmse= 1.8; time: 0.0 min
[Tune-x] 10: nrounds=1.75e+03; max_depth=1; eta=0.241; gamma=8.99; colsample_bytree=0.46; min_child_weight=6.51; subsample=0.608
[Tune-y] 10: rmse.test.rmse=1.46; time: 0.1 min
[Tune-x] 11: nrounds=65; max_depth=3; eta=0.356; gamma=1.28; colsample_bytree=0.308; min_child_weight=12; subsample=0.908
[Tune-y] 11: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 12: nrounds=22; max_depth=2; eta=0.0889; gamma=5.98; colsample_bytree=0.458; min_child_weight=5.8; subsample=0.464
[Tune-y] 12: rmse.test.rmse=2.16; time: 0.0 min
[Tune-x] 13: nrounds=894; max_depth=8; eta=0.522; gamma=3.84; colsample_bytree=0.44; min_child_weight=3.27; subsample=0.951
[Tune-y] 13: rmse.test.rmse=1.51; time: 0.2 min
[Tune-x] 14: nrounds=45; max_depth=4; eta=0.379; gamma=5.13; colsample_bytree=0.335; min_child_weight=6.53; subsample=0.431
[Tune-y] 14: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 15: nrounds=488; max_depth=4; eta=0.127; gamma=0.947; colsample_bytree=0.689; min_child_weight=6.58; subsample=0.378
[Tune-y] 15: rmse.test.rmse= 1.6; time: 0.1 min
[Tune-x] 16: nrounds=271; max_depth=10; eta=0.598; gamma=3.06; colsample_bytree=0.692; min_child_weight=15.2; subsample=0.323
[Tune-y] 16: rmse.test.rmse=1.94; time: 0.0 min
[Tune-x] 17: nrounds=11; max_depth=9; eta=0.429; gamma=8.18; colsample_bytree=0.508; min_child_weight=1.62; subsample=0.257
[Tune-y] 17: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 18: nrounds=15; max_depth=1; eta=0.422; gamma=9.15; colsample_bytree=0.642; min_child_weight=15.5; subsample=0.547
[Tune-y] 18: rmse.test.rmse=2.01; time: 0.0 min
[Tune-x] 19: nrounds=663; max_depth=5; eta=0.086; gamma=0.474; colsample_bytree=0.499; min_child_weight=16.5; subsample=0.426
[Tune-y] 19: rmse.test.rmse=1.96; time: 0.0 min
[Tune-x] 20: nrounds=344; max_depth=9; eta=0.115; gamma=0.355; colsample_bytree=0.641; min_child_weight=12.4; subsample=0.276
[Tune-y] 20: rmse.test.rmse=1.98; time: 0.0 min
[Tune] Result: nrounds=73; max_depth=7; eta=0.132; gamma=3.11; colsample_bytree=0.684; min_child_weight=2.97; subsample=0.43 : rmse.test.rmse=1.45
[1] "Sat Jan 13 19:17:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Sat Jan 13 19:17:50 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [4,0.507078], n=(50,26)
**PRUNE** @depth 0: [4,0.507078]
**GROW** @depth 0: [4,0.507078], n=(50,26)
**PRUNE** @depth 0: [4,0.507078]
**GROW** @depth 0: [1,0.40644], n=(16,60)
**PRUNE** @depth 0: [1,0.40644]
**GROW** @depth 0: [4,0.507078], n=(50,26)
**PRUNE** @depth 0: [4,0.507078]
**GROW** @depth 0: [3,0.447283], n=(19,57)
**GROW** @depth 1: [1,0.494163], n=(19,38)
**PRUNE** @depth 1: [1,0.494163]
**PRUNE** @depth 0: [3,0.446429]
**GROW** @depth 0: [1,0.494163], n=(22,54)
**PRUNE** @depth 0: [1,0.492277]
**GROW** @depth 0: [4,0.507078], n=(50,26)
**PRUNE** @depth 0: [4,0.507078]
**GROW** @depth 0: [4,0.395068], n=(21,55)
**PRUNE** @depth 0: [4,0.395068]
r=1000 d=[0]; n=76
**GROW** @depth 0: [1,0.511815], n=(48,28)
**PRUNE** @depth 0: [1,0.511815]
**GROW** @depth 0: [4,0.507078], n=(50,26)
**PRUNE** @depth 0: [4,0.51621]
**GROW** @depth 0: [5,0.463383], n=(45,31)
**PRUNE** @depth 0: [5,0.463383]
**GROW** @depth 0: [4,0.507078], n=(50,26)
**PRUNE** @depth 0: [4,0.507078]
**GROW** @depth 0: [1,0.40644], n=(16,60)
**GROW** @depth 1: [3,0.473242], n=(18,42)
**PRUNE** @depth 1: [3,0.491575]
r=2000 d=[0] [0]; n=(16,60)

Sampling @ nn=0 pred locs:
**GROW** @depth 1: [3,0.537444], n=(41,18)
**PRUNE** @depth 1: [3,0.551251]
**PRUNE** @depth 0: [1,0.43175]
**GROW** @depth 0: [5,0.463383], n=(45,31)
**PRUNE** @depth 0: [5,0.463383]
**GROW** @depth 0: [1,0.40644], n=(16,60)
r=1000 d=[0] [0]; mh=3 n=(16,60)
**PRUNE** @depth 0: [1,0.352039]
**GROW** @depth 0: [3,0.447283], n=(19,57)
**PRUNE** @depth 0: [3,0.440147]
**GROW** @depth 0: [2,0.482421], n=(45,31)
**PRUNE** @depth 0: [2,0.47236]
**GROW** @depth 0: [4,0.498408], n=(32,44)
**PRUNE** @depth 0: [4,0.51621]
**GROW** @depth 0: [8,0.513217], n=(50,26)
**PRUNE** @depth 0: [8,0.56733]
r=2000 d=[0]; mh=3 n=76
**GROW** @depth 0: [1,0.492277], n=(21,55)
**PRUNE** @depth 0: [1,0.492277]
**GROW** @depth 0: [4,0.507078], n=(50,26)
**PRUNE** @depth 0: [4,0.507078]
**GROW** @depth 0: [9,0.552053], n=(44,32)
**PRUNE** @depth 0: [9,0.552053]
**GROW** @depth 0: [4,0.507078], n=(50,26)
**PRUNE** @depth 0: [4,0.53609]
**GROW** @depth 0: [1,0.508207], n=(24,52)
**PRUNE** @depth 0: [1,0.511815]
**GROW** @depth 0: [1,0.511815], n=(48,28)
**PRUNE** @depth 0: [1,0.511815]
**GROW** @depth 0: [7,0.578621], n=(47,29)
**PRUNE** @depth 0: [7,0.579044]
**GROW** @depth 0: [1,0.511815], n=(48,28)
**PRUNE** @depth 0: [1,0.508207]
**GROW** @depth 0: [4,0.239015], n=(12,64)
**PRUNE** @depth 0: [4,0.264603]
**GROW** @depth 0: [5,0.463383], n=(45,31)
**PRUNE** @depth 0: [5,0.463383]
r=3000 d=[0]; mh=3 n=76
**GROW** @depth 0: [1,0.532558], n=(51,25)
**PRUNE** @depth 0: [1,0.540249]
**GROW** @depth 0: [4,0.507078], n=(50,26)
**PRUNE** @depth 0: [4,0.507078]
**GROW** @depth 0: [1,0.494163], n=(22,54)
**PRUNE** @depth 0: [1,0.494163]
**GROW** @depth 0: [1,0.511815], n=(48,28)
**PRUNE** @depth 0: [1,0.523466]
**GROW** @depth 0: [3,0.447283], n=(19,57)
**PRUNE** @depth 0: [3,0.446429]
**GROW** @depth 0: [3,0.637129], n=(58,18)
**PRUNE** @depth 0: [3,0.627202]
**GROW** @depth 0: [4,0.507078], n=(50,26)
**PRUNE** @depth 0: [4,0.51621]
**GROW** @depth 0: [4,0.271768], n=(14,62)
r=4000 d=[0] [0]; mh=3 n=(13,63)
**PRUNE** @depth 0: [4,0.271768]
**GROW** @depth 0: [3,0.537444], n=(49,27)
**PRUNE** @depth 0: [3,0.537444]
**GROW** @depth 0: [7,0.559449], n=(21,55)
**PRUNE** @depth 0: [7,0.533124]
**GROW** @depth 0: [5,0.463383], n=(45,31)
**PRUNE** @depth 0: [5,0.437332]
r=5000 d=[0]; mh=3 n=76
Grow: 12.01%, Prune: 40.57%, Change: 58.94%, Swap: 36.36%

[1] "Sat Jan 13 19:19:24 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Sat Jan 13 19:19:30 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=76

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=76
r=2000 d=[0]; mh=1 n=76
r=3000 d=[0]; mh=1 n=76

[1] "Sat Jan 13 19:19:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7064135 
gamma= 13.0064 	 alpha= 2.1555 	 beta= 10.1925 
[1] "Sat Jan 13 19:19:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Sat Jan 13 19:19:51 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=76
r=2000 d=[0]; n=76

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=76
r=2000 d=[0]; mh=1 n=76
r=3000 d=[0]; mh=1 n=76
r=4000 d=[0]; mh=1 n=76
r=5000 d=[0]; mh=1 n=76
Grow: 0%, 

[1] "Sat Jan 13 19:19:58 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cforest no default is available.
[1] "Sat Jan 13 19:20:04 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Sat Jan 13 19:20:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Sat Jan 13 19:20:19 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Sat Jan 13 19:20:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Sat Jan 13 19:20:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Sat Jan 13 19:20:37 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
[Tune] Started tuning learner regr.extraTrees for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
numRandomCuts integer   -   1 1 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=8; numRandomCuts=22
[Tune-y] 1: rmse.test.rmse=1.74; time: 0.1 min
[Tune-x] 2: mtry=4; numRandomCuts=6
[Tune-y] 2: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 3: mtry=6; numRandomCuts=4
[Tune-y] 3: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 4: mtry=8; numRandomCuts=6
[Tune-y] 4: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 5: mtry=4; numRandomCuts=11
[Tune-y] 5: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 6: mtry=2; numRandomCuts=19
[Tune-y] 6: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 7: mtry=7; numRandomCuts=8
[Tune-y] 7: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 8: mtry=6; numRandomCuts=22
[Tune-y] 8: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 9: mtry=4; numRandomCuts=2
[Tune-y] 9: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 10: mtry=3; numRandomCuts=22
[Tune-y] 10: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 11: mtry=4; numRandomCuts=23
[Tune-y] 11: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 12: mtry=8; numRandomCuts=17
[Tune-y] 12: rmse.test.rmse=1.72; time: 0.1 min
[Tune-x] 13: mtry=9; numRandomCuts=18
[Tune-y] 13: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 14: mtry=3; numRandomCuts=6
[Tune-y] 14: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 15: mtry=4; numRandomCuts=21
[Tune-y] 15: rmse.test.rmse=1.59; time: 0.1 min
[Tune-x] 16: mtry=3; numRandomCuts=7
[Tune-y] 16: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 17: mtry=9; numRandomCuts=8
[Tune-y] 17: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 18: mtry=6; numRandomCuts=20
[Tune-y] 18: rmse.test.rmse=1.66; time: 0.0 min
[Tune-x] 19: mtry=4; numRandomCuts=6
[Tune-y] 19: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 20: mtry=9; numRandomCuts=23
[Tune-y] 20: rmse.test.rmse=1.79; time: 0.1 min
[Tune] Result: mtry=4; numRandomCuts=2 : rmse.test.rmse=1.53
[1] "Sat Jan 13 19:21:23 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Sat Jan 13 19:21:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Sat Jan 13 19:21:37 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.frbs no default is available.
[1] "Sat Jan 13 19:21:45 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
In addition: Warning message:
In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Sat Jan 13 19:21:55 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=459; interaction.depth=9; shrinkage=0.249; n.minobsinnode=9
[Tune-y] 1: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 2: n.trees=130; interaction.depth=2; shrinkage=0.491; n.minobsinnode=9
[Tune-y] 2: rmse.test.rmse=1.95; time: 0.0 min
[Tune-x] 3: n.trees=63; interaction.depth=5; shrinkage=0.092; n.minobsinnode=20
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: n.trees=226; interaction.depth=3; shrinkage=0.387; n.minobsinnode=22
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: n.trees=68; interaction.depth=1; shrinkage=0.158; n.minobsinnode=23
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: n.trees=61; interaction.depth=10; shrinkage=0.505; n.minobsinnode=18
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: n.trees=831; interaction.depth=7; shrinkage=0.17; n.minobsinnode=9
[Tune-y] 7: rmse.test.rmse=1.86; time: 0.0 min
[Tune-x] 8: n.trees=53; interaction.depth=9; shrinkage=0.136; n.minobsinnode=10
[Tune-y] 8: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 9: n.trees=664; interaction.depth=3; shrinkage=0.379; n.minobsinnode=21
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: n.trees=72; interaction.depth=3; shrinkage=0.565; n.minobsinnode=23
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: n.trees=177; interaction.depth=2; shrinkage=0.267; n.minobsinnode=5
[Tune-y] 11: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 12: n.trees=619; interaction.depth=1; shrinkage=0.0441; n.minobsinnode=13
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: n.trees=22; interaction.depth=9; shrinkage=0.221; n.minobsinnode=19
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: n.trees=364; interaction.depth=5; shrinkage=0.54; n.minobsinnode=15
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 14: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 15: n.trees=232; interaction.depth=10; shrinkage=0.0866; n.minobsinnode=23
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: n.trees=30; interaction.depth=7; shrinkage=0.257; n.minobsinnode=22
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 16: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 17: n.trees=90; interaction.depth=3; shrinkage=0.513; n.minobsinnode=19
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 17: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 18: n.trees=486; interaction.depth=5; shrinkage=0.488; n.minobsinnode=12
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 18: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 19: n.trees=28; interaction.depth=5; shrinkage=0.561; n.minobsinnode=14
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: n.trees=749; interaction.depth=3; shrinkage=0.586; n.minobsinnode=5
[Tune-y] 20: rmse.test.rmse=2.07; time: 0.0 min
[Tune] Result: n.trees=177; interaction.depth=2; shrinkage=0.267; n.minobsinnode=5 : rmse.test.rmse=1.57
[1] "Sat Jan 13 19:22:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Sat Jan 13 19:22:09 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.831; lambda=2.48
[Tune-y] 1: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 2: alpha=0.414; lambda=0.00737
[Tune-y] 2: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 3: alpha=0.556; lambda=0.00355
[Tune-y] 3: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 4: alpha=0.817; lambda=0.0063
[Tune-y] 4: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 5: alpha=0.401; lambda=0.0439
[Tune-y] 5: rmse.test.rmse=1.04; time: 0.0 min
[Tune-x] 6: alpha=0.152; lambda=0.646
[Tune-y] 6: rmse.test.rmse= 1.2; time: 0.0 min
[Tune-x] 7: alpha=0.677; lambda=0.0125
[Tune-y] 7: rmse.test.rmse=1.05; time: 0.0 min
[Tune-x] 8: alpha=0.645; lambda=2.07
[Tune-y] 8: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 9: alpha=0.417; lambda=0.00167
[Tune-y] 9: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 10: alpha=0.262; lambda=2.23
[Tune-y] 10: rmse.test.rmse=1.79; time: 0.0 min
[Tune-x] 11: alpha=0.394; lambda=3.53
[Tune-y] 11: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 12: alpha=0.842; lambda=0.331
[Tune-y] 12: rmse.test.rmse=1.22; time: 0.0 min
[Tune-x] 13: alpha=0.96; lambda=0.474
[Tune-y] 13: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 14: alpha=0.282; lambda=0.00603
[Tune-y] 14: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 15: alpha=0.362; lambda=1.57
[Tune-y] 15: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 16: alpha=0.225; lambda=0.00954
[Tune-y] 16: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 17: alpha=0.911; lambda=0.0127
[Tune-y] 17: rmse.test.rmse=1.05; time: 0.0 min
[Tune-x] 18: alpha=0.631; lambda=0.969
[Tune-y] 18: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 19: alpha=0.428; lambda=0.00831
[Tune-y] 19: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 20: alpha=0.942; lambda=2.94
[Tune-y] 20: rmse.test.rmse= 1.9; time: 0.0 min
[Tune] Result: alpha=0.401; lambda=0.0439 : rmse.test.rmse=1.04
[1] "Sat Jan 13 19:22:20 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:22:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===========================                                           |  38%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:22:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:22:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |================================                                      |  46%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:22:57 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  14.71383 5.00558 5.045747 8.216649 10.58054 12.30722 11.03728 10.09387 9.488676 
  - best initial criterion value(s) :  -151.7785 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       151.78  |proj g|=       5.6008
At iterate     1  f =       141.96  |proj g|=         1.564
At iterate     2  f =       141.46  |proj g|=         0.986
At iterate     3  f =       139.29  |proj g|=        4.1998
At iterate     4  f =       137.73  |proj g|=        1.4833
At iterate     5  f =       137.14  |proj g|=        1.7283
At iterate     6  f =       136.66  |proj g|=        1.2005
At iterate     7  f =       136.49  |proj g|=       0.51384
At iterate     8  f =       136.41  |proj g|=       0.44366
At iterate     9  f =       136.29  |proj g|=       0.86351
At iterate    10  f =       136.18  |proj g|=       0.46111
At iterate    11  f =       136.09  |proj g|=         0.295
At iterate    12  f =       135.87  |proj g|=       0.60083
At iterate    13  f =       135.63  |proj g|=        0.9734
At iterate    14  f =       135.52  |proj g|=       0.67238
At iterate    15  f =       135.42  |proj g|=       0.81077
At iterate    16  f =       135.38  |proj g|=        0.4733
At iterate    17  f =       135.35  |proj g|=       0.20736
At iterate    18  f =       135.34  |proj g|=       0.14485
At iterate    19  f =       135.31  |proj g|=       0.14912
At iterate    20  f =       135.27  |proj g|=       0.18618
At iterate    21  f =       135.17  |proj g|=       0.30066
At iterate    22  f =       135.15  |proj g|=       0.35456
At iterate    23  f =       135.12  |proj g|=      0.097833
At iterate    24  f =       135.11  |proj g|=       0.11978
At iterate    25  f =       135.11  |proj g|=      0.073657
At iterate    26  f =       135.11  |proj g|=      0.060722
At iterate    27  f =        135.1  |proj g|=       0.08258
At iterate    28  f =       135.09  |proj g|=       0.20925
At iterate    29  f =       135.08  |proj g|=        0.3146
At iterate    30  f =       135.06  |proj g|=       0.10881
At iterate    31  f =       135.05  |proj g|=      0.077909
At iterate    32  f =       135.05  |proj g|=      0.050514
At iterate    33  f =       135.05  |proj g|=      0.090284
At iterate    34  f =       135.05  |proj g|=      0.018552
At iterate    35  f =       135.05  |proj g|=     0.0067046
At iterate    36  f =       135.05  |proj g|=     0.0027914
At iterate    37  f =       135.05  |proj g|=     0.0042113
At iterate    38  f =       135.05  |proj g|=     0.0025396
At iterate    39  f =       135.05  |proj g|=    0.00043815
At iterate    40  f =       135.05  |proj g|=     0.0009264

iterations 40
function evaluations 49
segments explored during Cauchy searches 40
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0.000926401
final function value 135.047

F = 135.047
final  value 135.046850 
converged
[1] "Sat Jan 13 19:23:16 2018"
[Tune] Started tuning learner regr.ksvm for parameter set:
         Type len  Def    Constr Req Tunable Trafo
C     numeric   -    0  -5 to 10   -    TRUE     Y
sigma numeric   - TRUE -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: C=806; sigma=4.23e-05
[Tune-y] 1: rmse.test.rmse=1.13; time: 0.0 min
[Tune-x] 2: C=45.1; sigma=16.2
[Tune-y] 2: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 3: C=0.307; sigma=5.66e-05
[Tune-y] 3: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 4: C=0.107; sigma=812
[Tune-y] 4: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 5: C=0.0608; sigma=0.00638
[Tune-y] 5: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 6: C=172; sigma=6.16e+03
[Tune-y] 6: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 7: C=11.9; sigma=2.72e+03
[Tune-y] 7: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 8: C=121; sigma=0.00012
[Tune-y] 8: rmse.test.rmse=1.28; time: 0.0 min
[Tune-x] 9: C=0.198; sigma=45
[Tune-y] 9: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 10: C=534; sigma=1.76e+04
[Tune-y] 10: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 11: C=434; sigma=135
[Tune-y] 11: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 12: C=4.46; sigma=0.474
[Tune-y] 12: rmse.test.rmse=1.85; time: 0.0 min
[Tune-x] 13: C=1.87; sigma=6.85e+03
[Tune-y] 13: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 14: C=580; sigma=1.25
[Tune-y] 14: rmse.test.rmse=1.88; time: 0.0 min
[Tune-x] 15: C=5.16; sigma=0.922
[Tune-y] 15: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 16: C=372; sigma=0.00128
[Tune-y] 16: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 17: C=74.8; sigma=0.00267
[Tune-y] 17: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 18: C=49.7; sigma=2.07
[Tune-y] 18: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 19: C=0.704; sigma=42.2
[Tune-y] 19: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 20: C=2.92; sigma=0.186
[Tune-y] 20: rmse.test.rmse=1.76; time: 0.0 min
[Tune] Result: C=372; sigma=0.00128 : rmse.test.rmse=1.06
[1] "Sat Jan 13 19:23:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 24), d = 11.0679, its = 6
i = 2 (of 24), d = 21.4913, its = 7
i = 3 (of 24), d = 11.3848, its = 6
i = 4 (of 24), d = 11.2595, its = 6
i = 5 (of 24), d = 11.6908, its = 6
i = 6 (of 24), d = 14.7116, its = 6
i = 7 (of 24), d = 12.2188, its = 6
i = 8 (of 24), d = 13.0639, its = 6
i = 9 (of 24), d = 12.4159, its = 7
i = 10 (of 24), d = 11.7841, its = 6
i = 11 (of 24), d = 12.2789, its = 6
i = 12 (of 24), d = 12.6158, its = 6
i = 13 (of 24), d = 17.0609, its = 7
i = 14 (of 24), d = 11.5721, its = 6
i = 15 (of 24), d = 11.3202, its = 6
i = 16 (of 24), d = 12.2447, its = 6
i = 17 (of 24), d = 15.1219, its = 7
i = 18 (of 24), d = 11.4679, its = 6
i = 19 (of 24), d = 11.6536, its = 6
i = 20 (of 24), d = 15.7611, its = 7
i = 21 (of 24), d = 11.3429, its = 6
i = 22 (of 24), d = 9.45918, its = 6
i = 23 (of 24), d = 14.2774, its = 7
i = 24 (of 24), d = 12.0004, its = 6
[1] "Sat Jan 13 19:23:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Sat Jan 13 19:23:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Sat Jan 13 19:23:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Sat Jan 13 19:23:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Sat Jan 13 19:23:56 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=17; decay=1.66
# weights:  188
initial  value 4329.596738 
iter  10 value 153.930471
iter  20 value 124.608832
iter  30 value 116.009850
iter  40 value 111.939610
iter  50 value 108.180372
iter  60 value 104.054828
iter  70 value 101.610624
iter  80 value 99.618647
iter  90 value 98.757653
iter 100 value 98.403872
final  value 98.403872 
stopped after 100 iterations
# weights:  188
initial  value 3042.651087 
iter  10 value 212.043206
iter  20 value 162.775834
iter  30 value 127.712475
iter  40 value 105.897521
iter  50 value 99.580388
iter  60 value 96.491084
iter  70 value 93.685934
iter  80 value 93.087730
iter  90 value 92.782481
iter 100 value 92.601955
final  value 92.601955 
stopped after 100 iterations
# weights:  188
initial  value 3843.883493 
iter  10 value 141.193408
iter  20 value 118.073369
iter  30 value 109.967141
iter  40 value 107.448671
iter  50 value 104.821850
iter  60 value 102.046458
iter  70 value 101.101060
iter  80 value 100.477433
iter  90 value 100.208299
iter 100 value 100.098778
final  value 100.098778 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=1.36; time: 0.0 min
[Tune-x] 2: size=9; decay=0.000222
# weights:  100
initial  value 4253.437286 
iter  10 value 207.423061
iter  20 value 199.596951
iter  30 value 180.171338
iter  40 value 177.632040
iter  50 value 177.366375
iter  60 value 174.901528
iter  70 value 174.851742
iter  80 value 168.837312
iter  90 value 112.008314
iter 100 value 52.394136
final  value 52.394136 
stopped after 100 iterations
# weights:  100
initial  value 3022.614422 
iter  10 value 95.256528
iter  20 value 48.891621
iter  30 value 16.029639
iter  40 value 8.060357
iter  50 value 3.862279
iter  60 value 2.065593
iter  70 value 1.100160
iter  80 value 0.688031
iter  90 value 0.511330
iter 100 value 0.357904
final  value 0.357904 
stopped after 100 iterations
# weights:  100
initial  value 3352.120867 
iter  10 value 119.228344
iter  20 value 72.753840
iter  30 value 32.889124
iter  40 value 13.201273
iter  50 value 5.106086
iter  60 value 2.532522
iter  70 value 1.093123
iter  80 value 0.753713
iter  90 value 0.658286
iter 100 value 0.592462
final  value 0.592462 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=2.78; time: 0.0 min
[Tune-x] 3: size=12; decay=7.24e-05
# weights:  133
initial  value 4240.688736 
iter  10 value 79.173582
iter  20 value 32.063590
iter  30 value 12.139777
iter  40 value 3.360843
iter  50 value 0.672857
iter  60 value 0.142775
iter  70 value 0.062050
iter  80 value 0.048903
iter  90 value 0.046573
iter 100 value 0.042086
final  value 0.042086 
stopped after 100 iterations
# weights:  133
initial  value 4102.603241 
iter  10 value 73.641578
iter  20 value 26.829018
iter  30 value 9.829108
iter  40 value 3.828639
iter  50 value 0.709035
iter  60 value 0.293587
iter  70 value 0.079047
iter  80 value 0.062893
iter  90 value 0.059638
iter 100 value 0.052329
final  value 0.052329 
stopped after 100 iterations
# weights:  133
initial  value 4088.540890 
iter  10 value 81.122995
iter  20 value 35.031181
iter  30 value 14.893828
iter  40 value 7.059178
iter  50 value 3.559081
iter  60 value 1.689306
iter  70 value 0.651593
iter  80 value 0.331234
iter  90 value 0.197253
iter 100 value 0.136760
final  value 0.136760 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=2.37; time: 0.0 min
[Tune-x] 4: size=17; decay=0.000174
# weights:  188
initial  value 4125.289928 
iter  10 value 99.878467
iter  20 value 41.295596
iter  30 value 22.910646
iter  40 value 12.208961
iter  50 value 6.938769
iter  60 value 3.674197
iter  70 value 1.424322
iter  80 value 0.606378
iter  90 value 0.413830
iter 100 value 0.283429
final  value 0.283429 
stopped after 100 iterations
# weights:  188
initial  value 4910.065963 
iter  10 value 80.072488
iter  20 value 38.568816
iter  30 value 18.915613
iter  40 value 8.751390
iter  50 value 5.006368
iter  60 value 2.428783
iter  70 value 1.606100
iter  80 value 0.953163
iter  90 value 0.723409
iter 100 value 0.436820
final  value 0.436820 
stopped after 100 iterations
# weights:  188
initial  value 2840.504818 
iter  10 value 89.410403
iter  20 value 39.917457
iter  30 value 29.507939
iter  40 value 20.501974
iter  50 value 10.830349
iter  60 value 4.538739
iter  70 value 2.961909
iter  80 value 2.407336
iter  90 value 2.204695
iter 100 value 2.108373
final  value 2.108373 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=2.81; time: 0.0 min
[Tune-x] 5: size=9; decay=0.00342
# weights:  100
initial  value 3366.082344 
iter  10 value 103.076903
iter  20 value 46.331404
iter  30 value 23.630440
iter  40 value 10.398996
iter  50 value 6.176194
iter  60 value 4.082473
iter  70 value 3.462233
iter  80 value 3.047188
iter  90 value 2.802720
iter 100 value 2.495075
final  value 2.495075 
stopped after 100 iterations
# weights:  100
initial  value 3220.889390 
iter  10 value 82.786075
iter  20 value 38.814312
iter  30 value 17.223398
iter  40 value 12.025606
iter  50 value 11.183023
iter  60 value 10.488520
iter  70 value 9.477997
iter  80 value 5.163231
iter  90 value 3.489649
iter 100 value 2.962316
final  value 2.962316 
stopped after 100 iterations
# weights:  100
initial  value 2841.046368 
iter  10 value 82.167711
iter  20 value 34.235844
iter  30 value 21.212104
iter  40 value 13.187927
iter  50 value 10.618027
iter  60 value 6.112860
iter  70 value 4.256058
iter  80 value 3.029073
iter  90 value 2.456033
iter 100 value 2.179594
final  value 2.179594 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=2.14; time: 0.0 min
[Tune-x] 6: size=4; decay=0.211
# weights:  45
initial  value 4008.290652 
iter  10 value 114.693164
iter  20 value 78.026869
iter  30 value 60.894363
iter  40 value 52.417192
iter  50 value 43.737018
iter  60 value 37.969281
iter  70 value 37.247334
iter  80 value 36.655656
iter  90 value 36.401725
iter 100 value 36.136778
final  value 36.136778 
stopped after 100 iterations
# weights:  45
initial  value 3247.718656 
iter  10 value 98.254125
iter  20 value 69.250795
iter  30 value 50.095816
iter  40 value 45.224335
iter  50 value 44.266993
iter  60 value 43.605108
iter  70 value 42.995541
iter  80 value 41.817345
iter  90 value 41.053442
iter 100 value 40.909324
final  value 40.909324 
stopped after 100 iterations
# weights:  45
initial  value 4093.549356 
iter  10 value 203.912763
iter  20 value 105.074140
iter  30 value 75.073285
iter  40 value 58.585258
iter  50 value 51.953274
iter  60 value 49.977851
iter  70 value 47.917600
iter  80 value 46.133446
iter  90 value 44.298622
iter 100 value 43.677671
final  value 43.677671 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=1.41; time: 0.0 min
[Tune-x] 7: size=14; decay=0.0005
# weights:  155
initial  value 3368.672652 
iter  10 value 107.748488
iter  20 value 58.340689
iter  30 value 26.864804
iter  40 value 8.156823
iter  50 value 2.923587
iter  60 value 1.077181
iter  70 value 0.429143
iter  80 value 0.366305
iter  90 value 0.342337
iter 100 value 0.276934
final  value 0.276934 
stopped after 100 iterations
# weights:  155
initial  value 4347.689534 
iter  10 value 79.348445
iter  20 value 30.961020
iter  30 value 14.234873
iter  40 value 6.712961
iter  50 value 2.193976
iter  60 value 0.695730
iter  70 value 0.388829
iter  80 value 0.351530
iter  90 value 0.301665
iter 100 value 0.244513
final  value 0.244513 
stopped after 100 iterations
# weights:  155
initial  value 2638.151244 
iter  10 value 98.039394
iter  20 value 51.019763
iter  30 value 23.396504
iter  40 value 8.684278
iter  50 value 3.815207
iter  60 value 2.239620
iter  70 value 1.637633
iter  80 value 1.180808
iter  90 value 1.057841
iter 100 value 0.930542
final  value 0.930542 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=2.24; time: 0.0 min
[Tune-x] 8: size=13; decay=1.26
# weights:  144
initial  value 5544.066181 
iter  10 value 165.374333
iter  20 value 127.063713
iter  30 value 105.241367
iter  40 value 98.189863
iter  50 value 94.828605
iter  60 value 93.037334
iter  70 value 91.540850
iter  80 value 90.076242
iter  90 value 89.298542
iter 100 value 89.007955
final  value 89.007955 
stopped after 100 iterations
# weights:  144
initial  value 2829.107016 
iter  10 value 112.948771
iter  20 value 95.217666
iter  30 value 91.760596
iter  40 value 90.322690
iter  50 value 89.379686
iter  60 value 88.758513
iter  70 value 88.555102
iter  80 value 88.457095
iter  90 value 88.379750
iter 100 value 88.348445
final  value 88.348445 
stopped after 100 iterations
# weights:  144
initial  value 2954.839254 
iter  10 value 146.537601
iter  20 value 114.810707
iter  30 value 106.233491
iter  40 value 100.102010
iter  50 value 96.159194
iter  60 value 94.253143
iter  70 value 92.576547
iter  80 value 91.693977
iter  90 value 91.572669
iter 100 value 91.535137
final  value 91.535137 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=1.37; time: 0.0 min
[Tune-x] 9: size=9; decay=2.27e-05
# weights:  100
initial  value 4170.471221 
iter  10 value 129.195391
iter  20 value 51.765749
iter  30 value 31.082169
iter  40 value 16.218563
iter  50 value 11.357019
iter  60 value 9.073845
iter  70 value 6.999803
iter  80 value 5.826478
iter  90 value 5.405632
iter 100 value 5.056084
final  value 5.056084 
stopped after 100 iterations
# weights:  100
initial  value 3615.898560 
iter  10 value 82.041356
iter  20 value 35.708648
iter  30 value 14.064913
iter  40 value 6.564522
iter  50 value 3.178116
iter  60 value 1.785647
iter  70 value 0.664145
iter  80 value 0.233883
iter  90 value 0.084787
iter 100 value 0.057149
final  value 0.057149 
stopped after 100 iterations
# weights:  100
initial  value 3769.238363 
iter  10 value 80.211876
iter  20 value 23.001437
iter  30 value 9.849618
iter  40 value 1.110117
iter  50 value 0.250499
iter  60 value 0.122960
iter  70 value 0.071535
iter  80 value 0.035327
iter  90 value 0.020680
iter 100 value 0.017510
final  value 0.017510 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=2.37; time: 0.0 min
[Tune-x] 10: size=6; decay=1.41
# weights:  67
initial  value 3764.182494 
iter  10 value 198.188873
iter  20 value 131.038082
iter  30 value 114.758263
iter  40 value 109.423361
iter  50 value 105.584699
iter  60 value 102.999686
iter  70 value 102.772221
iter  80 value 102.754181
iter  90 value 102.753309
final  value 102.753284 
converged
# weights:  67
initial  value 4028.662537 
iter  10 value 253.890307
iter  20 value 134.431461
iter  30 value 123.050542
iter  40 value 117.403285
iter  50 value 112.315087
iter  60 value 106.559187
iter  70 value 100.258214
iter  80 value 98.084263
iter  90 value 97.362471
iter 100 value 97.228263
final  value 97.228263 
stopped after 100 iterations
# weights:  67
initial  value 3007.422401 
iter  10 value 258.006887
iter  20 value 130.614258
iter  30 value 114.024774
iter  40 value 108.758100
iter  50 value 107.138972
iter  60 value 106.969214
iter  70 value 106.826926
iter  80 value 106.484947
iter  90 value 106.300598
iter 100 value 106.234453
final  value 106.234453 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=1.43; time: 0.0 min
[Tune-x] 11: size=8; decay=2.86
# weights:  89
initial  value 3484.679590 
iter  10 value 241.287591
iter  20 value 184.422730
iter  30 value 150.678956
iter  40 value 136.415330
iter  50 value 134.039691
iter  60 value 133.069084
iter  70 value 132.655911
iter  80 value 132.303591
iter  90 value 132.166382
iter 100 value 132.152359
final  value 132.152359 
stopped after 100 iterations
# weights:  89
initial  value 5228.145978 
iter  10 value 146.371585
iter  20 value 132.425558
iter  30 value 127.327939
iter  40 value 123.789796
iter  50 value 122.927944
iter  60 value 122.706657
iter  70 value 122.693833
iter  80 value 122.692481
final  value 122.692451 
converged
# weights:  89
initial  value 3405.348149 
iter  10 value 431.572468
iter  20 value 230.684729
iter  30 value 167.966469
iter  40 value 148.358107
iter  50 value 140.702527
iter  60 value 137.920671
iter  70 value 136.733776
iter  80 value 136.552676
iter  90 value 136.486855
iter 100 value 136.441338
final  value 136.441338 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 12: size=17; decay=0.0759
# weights:  188
initial  value 4830.915294 
iter  10 value 138.194467
iter  20 value 84.971758
iter  30 value 47.789133
iter  40 value 30.563556
iter  50 value 20.323706
iter  60 value 17.221900
iter  70 value 15.754774
iter  80 value 14.648719
iter  90 value 13.984732
iter 100 value 13.531405
final  value 13.531405 
stopped after 100 iterations
# weights:  188
initial  value 3643.626190 
iter  10 value 103.985949
iter  20 value 58.436185
iter  30 value 35.817980
iter  40 value 29.284935
iter  50 value 25.812609
iter  60 value 23.261696
iter  70 value 21.359817
iter  80 value 20.195508
iter  90 value 18.690314
iter 100 value 16.857020
final  value 16.857020 
stopped after 100 iterations
# weights:  188
initial  value 3044.404492 
iter  10 value 90.261989
iter  20 value 51.338233
iter  30 value 36.196309
iter  40 value 29.339808
iter  50 value 25.540397
iter  60 value 22.770895
iter  70 value 20.329811
iter  80 value 18.890779
iter  90 value 17.676869
iter 100 value 16.799935
final  value 16.799935 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 13: size=20; decay=0.131
# weights:  221
initial  value 2109.038394 
iter  10 value 104.968850
iter  20 value 62.979978
iter  30 value 38.585423
iter  40 value 31.426101
iter  50 value 27.299998
iter  60 value 25.445914
iter  70 value 23.741772
iter  80 value 22.257781
iter  90 value 21.511612
iter 100 value 20.711708
final  value 20.711708 
stopped after 100 iterations
# weights:  221
initial  value 3854.247434 
iter  10 value 104.615025
iter  20 value 69.408454
iter  30 value 48.062584
iter  40 value 36.612948
iter  50 value 33.164605
iter  60 value 30.686426
iter  70 value 28.263461
iter  80 value 24.961804
iter  90 value 22.777097
iter 100 value 21.879411
final  value 21.879411 
stopped after 100 iterations
# weights:  221
initial  value 4737.914410 
iter  10 value 99.219210
iter  20 value 63.518396
iter  30 value 44.184418
iter  40 value 36.365247
iter  50 value 33.444251
iter  60 value 31.819399
iter  70 value 30.527947
iter  80 value 29.493120
iter  90 value 28.485489
iter 100 value 27.699533
final  value 27.699533 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 14: size=6; decay=0.000163
# weights:  67
initial  value 3571.013159 
iter  10 value 134.655224
iter  20 value 63.201735
iter  30 value 33.102749
iter  40 value 23.685860
iter  50 value 13.480076
iter  60 value 12.610334
iter  70 value 12.000147
iter  80 value 11.750962
iter  90 value 11.684460
iter 100 value 11.621635
final  value 11.621635 
stopped after 100 iterations
# weights:  67
initial  value 3604.744897 
iter  10 value 124.125431
iter  20 value 55.930223
iter  30 value 21.961112
iter  40 value 12.898522
iter  50 value 10.184731
iter  60 value 9.182504
iter  70 value 8.819067
iter  80 value 8.474194
iter  90 value 7.859146
iter 100 value 7.253151
final  value 7.253151 
stopped after 100 iterations
# weights:  67
initial  value 2872.171357 
iter  10 value 103.811467
iter  20 value 46.863121
iter  30 value 24.255161
iter  40 value 20.181080
iter  50 value 18.963827
iter  60 value 18.424021
iter  70 value 17.386859
iter  80 value 16.482894
iter  90 value 16.247747
iter 100 value 15.542418
final  value 15.542418 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=2.46; time: 0.0 min
[Tune-x] 15: size=8; decay=0.82
# weights:  89
initial  value 3519.509864 
iter  10 value 133.336226
iter  20 value 99.695492
iter  30 value 86.273033
iter  40 value 79.786490
iter  50 value 77.007184
iter  60 value 75.627339
iter  70 value 75.316364
iter  80 value 74.935787
iter  90 value 74.477542
iter 100 value 74.410598
final  value 74.410598 
stopped after 100 iterations
# weights:  89
initial  value 3652.983047 
iter  10 value 194.753435
iter  20 value 112.771660
iter  30 value 85.011148
iter  40 value 77.751494
iter  50 value 75.051719
iter  60 value 74.327544
iter  70 value 74.023420
iter  80 value 73.912433
iter  90 value 73.893603
iter 100 value 73.887121
final  value 73.887121 
stopped after 100 iterations
# weights:  89
initial  value 3879.319540 
iter  10 value 123.546841
iter  20 value 101.701982
iter  30 value 91.088542
iter  40 value 86.459291
iter  50 value 81.795979
iter  60 value 80.209003
iter  70 value 79.338054
iter  80 value 79.009282
iter  90 value 78.777647
iter 100 value 78.494288
final  value 78.494288 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=1.43; time: 0.0 min
[Tune-x] 16: size=5; decay=0.000329
# weights:  56
initial  value 3078.618575 
iter  10 value 92.974613
iter  20 value 58.942144
iter  30 value 32.323860
iter  40 value 21.724015
iter  50 value 14.486578
iter  60 value 11.601248
iter  70 value 10.214194
iter  80 value 9.759024
iter  90 value 9.681994
iter 100 value 9.351178
final  value 9.351178 
stopped after 100 iterations
# weights:  56
initial  value 4756.533807 
iter  10 value 128.832453
iter  20 value 125.235870
iter  30 value 117.996846
iter  40 value 116.614442
iter  50 value 116.469634
iter  60 value 116.392520
iter  70 value 105.999961
iter  80 value 63.645190
iter  90 value 47.534830
iter 100 value 25.998849
final  value 25.998849 
stopped after 100 iterations
# weights:  56
initial  value 3846.265103 
iter  10 value 99.120793
iter  20 value 77.926356
iter  30 value 33.434272
iter  40 value 16.541173
iter  50 value 11.149482
iter  60 value 8.891030
iter  70 value 7.009134
iter  80 value 6.319622
iter  90 value 6.046101
iter 100 value 5.893268
final  value 5.893268 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=1.97; time: 0.0 min
[Tune-x] 17: size=19; decay=0.00051
# weights:  210
initial  value 3873.512538 
iter  10 value 182.983969
iter  20 value 72.554705
iter  30 value 28.535003
iter  40 value 10.596426
iter  50 value 4.956857
iter  60 value 3.796919
iter  70 value 3.283698
iter  80 value 2.933890
iter  90 value 1.843824
iter 100 value 1.260199
final  value 1.260199 
stopped after 100 iterations
# weights:  210
initial  value 3374.148366 
iter  10 value 103.991303
iter  20 value 40.130612
iter  30 value 13.077627
iter  40 value 5.496413
iter  50 value 3.444625
iter  60 value 1.729111
iter  70 value 1.016186
iter  80 value 0.734569
iter  90 value 0.586267
iter 100 value 0.499246
final  value 0.499246 
stopped after 100 iterations
# weights:  210
initial  value 2627.875844 
iter  10 value 87.750569
iter  20 value 34.292322
iter  30 value 19.757080
iter  40 value 9.139882
iter  50 value 4.246965
iter  60 value 1.625955
iter  70 value 0.612256
iter  80 value 0.429772
iter  90 value 0.401518
iter 100 value 0.339903
final  value 0.339903 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=2.38; time: 0.0 min
[Tune-x] 18: size=13; decay=0.393
# weights:  144
initial  value 3541.137198 
iter  10 value 113.026155
iter  20 value 76.542296
iter  30 value 62.238895
iter  40 value 55.954120
iter  50 value 51.132362
iter  60 value 48.032981
iter  70 value 46.920052
iter  80 value 46.290264
iter  90 value 45.845419
iter 100 value 45.477336
final  value 45.477336 
stopped after 100 iterations
# weights:  144
initial  value 3962.352176 
iter  10 value 110.133750
iter  20 value 75.048662
iter  30 value 61.859670
iter  40 value 55.809736
iter  50 value 53.801816
iter  60 value 52.567244
iter  70 value 51.744975
iter  80 value 51.064558
iter  90 value 50.564663
iter 100 value 50.338964
final  value 50.338964 
stopped after 100 iterations
# weights:  144
initial  value 2958.345731 
iter  10 value 94.255794
iter  20 value 68.789263
iter  30 value 57.087321
iter  40 value 54.344502
iter  50 value 52.861042
iter  60 value 51.373765
iter  70 value 50.555585
iter  80 value 50.122488
iter  90 value 49.959329
iter 100 value 49.874744
final  value 49.874744 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=1.37; time: 0.0 min
[Tune-x] 19: size=9; decay=0.000266
# weights:  100
initial  value 4636.166497 
iter  10 value 120.643358
iter  20 value 51.308445
iter  30 value 24.887603
iter  40 value 16.937705
iter  50 value 10.377530
iter  60 value 8.260856
iter  70 value 6.251133
iter  80 value 5.257375
iter  90 value 3.432584
iter 100 value 2.563480
final  value 2.563480 
stopped after 100 iterations
# weights:  100
initial  value 3917.356667 
iter  10 value 86.495042
iter  20 value 34.027986
iter  30 value 12.710101
iter  40 value 7.063631
iter  50 value 3.952414
iter  60 value 2.844558
iter  70 value 2.220663
iter  80 value 1.789082
iter  90 value 1.294274
iter 100 value 1.026579
final  value 1.026579 
stopped after 100 iterations
# weights:  100
initial  value 3617.494946 
iter  10 value 75.114128
iter  20 value 32.195927
iter  30 value 19.443174
iter  40 value 7.476495
iter  50 value 1.466999
iter  60 value 0.787868
iter  70 value 0.609403
iter  80 value 0.497349
iter  90 value 0.449609
iter 100 value 0.398614
final  value 0.398614 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=   3; time: 0.0 min
[Tune-x] 20: size=19; decay=2.15
# weights:  210
initial  value 3630.081413 
iter  10 value 169.866883
iter  20 value 129.240706
iter  30 value 118.576880
iter  40 value 115.683727
iter  50 value 112.221423
iter  60 value 109.966649
iter  70 value 108.814223
iter  80 value 107.817725
iter  90 value 107.399697
iter 100 value 107.215043
final  value 107.215043 
stopped after 100 iterations
# weights:  210
initial  value 3209.480407 
iter  10 value 243.340330
iter  20 value 160.968525
iter  30 value 125.544239
iter  40 value 114.914706
iter  50 value 109.219141
iter  60 value 105.914015
iter  70 value 104.606128
iter  80 value 103.793143
iter  90 value 103.478945
iter 100 value 102.970709
final  value 102.970709 
stopped after 100 iterations
# weights:  210
initial  value 4228.663106 
iter  10 value 154.940316
iter  20 value 129.249474
iter  30 value 124.994053
iter  40 value 122.288601
iter  50 value 117.658387
iter  60 value 114.617163
iter  70 value 113.423247
iter  80 value 113.278227
iter  90 value 113.208749
iter 100 value 113.138282
final  value 113.138282 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=1.36; time: 0.0 min
[Tune] Result: size=19; decay=2.15 : rmse.test.rmse=1.36
# weights:  210
initial  value 7664.944548 
iter  10 value 321.906150
iter  20 value 284.392030
iter  30 value 264.938999
iter  40 value 242.233717
iter  50 value 218.095303
iter  60 value 193.755900
iter  70 value 177.813034
iter  80 value 163.664024
iter  90 value 153.262032
iter 100 value 147.705087
final  value 147.705087 
stopped after 100 iterations
[1] "Sat Jan 13 19:24:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1076
 total number of nodes after removal of identical nodes : 525 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 449
 number of selected nodes                               : 61 
[1] "Sat Jan 13 19:24:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Sat Jan 13 19:24:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Sat Jan 13 19:25:04 2018"
[Tune] Started tuning learner regr.randomForest for parameter set:
            Type len Def  Constr Req Tunable Trafo
nodesize integer   -   1 1 to 10   -    TRUE     -
mtry     integer   -   3  1 to 9   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nodesize=9; mtry=8
[Tune-y] 1: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 2: nodesize=5; mtry=3
[Tune-y] 2: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 3: nodesize=6; mtry=2
[Tune-y] 3: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 4: nodesize=9; mtry=2
[Tune-y] 4: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 5: nodesize=5; mtry=4
[Tune-y] 5: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 6: nodesize=2; mtry=7
[Tune-y] 6: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 7: nodesize=7; mtry=3
[Tune-y] 7: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 8: nodesize=7; mtry=8
[Tune-y] 8: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 9: nodesize=5; mtry=1
[Tune-y] 9: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 10: nodesize=3; mtry=8
[Tune-y] 10: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 11: nodesize=4; mtry=9
[Tune-y] 11: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 12: nodesize=9; mtry=6
[Tune-y] 12: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 13: nodesize=10; mtry=7
[Tune-y] 13: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 14: nodesize=3; mtry=2
[Tune-y] 14: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 15: nodesize=4; mtry=8
[Tune-y] 15: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 16: nodesize=3; mtry=3
[Tune-y] 16: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 17: nodesize=10; mtry=3
[Tune-y] 17: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 18: nodesize=7; mtry=7
[Tune-y] 18: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 19: nodesize=5; mtry=3
[Tune-y] 19: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 20: nodesize=10; mtry=8
[Tune-y] 20: rmse.test.rmse=1.64; time: 0.0 min
[Tune] Result: nodesize=7; mtry=8 : rmse.test.rmse= 1.6
[1] "Sat Jan 13 19:25:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Sat Jan 13 19:25:23 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=8; min.node.size=9
[Tune-y] 1: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 2: mtry=4; min.node.size=3
[Tune-y] 2: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 3: mtry=6; min.node.size=2
[Tune-y] 3: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 4: mtry=8; min.node.size=3
[Tune-y] 4: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 5: mtry=4; min.node.size=5
[Tune-y] 5: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 6: mtry=2; min.node.size=8
[Tune-y] 6: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 7: mtry=7; min.node.size=3
[Tune-y] 7: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 8: mtry=6; min.node.size=9
[Tune-y] 8: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 9: mtry=4; min.node.size=1
[Tune-y] 9: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 10: mtry=3; min.node.size=9
[Tune-y] 10: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 11: mtry=4; min.node.size=10
[Tune-y] 11: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 12: mtry=8; min.node.size=7
[Tune-y] 12: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 13: mtry=9; min.node.size=7
[Tune-y] 13: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 14: mtry=3; min.node.size=3
[Tune-y] 14: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 15: mtry=4; min.node.size=9
[Tune-y] 15: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 16: mtry=3; min.node.size=3
[Tune-y] 16: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 17: mtry=9; min.node.size=3
[Tune-y] 17: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 18: mtry=6; min.node.size=8
[Tune-y] 18: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 19: mtry=4; min.node.size=3
[Tune-y] 19: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 20: mtry=9; min.node.size=9
[Tune-y] 20: rmse.test.rmse=1.64; time: 0.0 min
[Tune] Result: mtry=8; min.node.size=3 : rmse.test.rmse=1.61
[1] "Sat Jan 13 19:25:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Sat Jan 13 19:25:45 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.31; maxdepth=27; minbucket=24; minsplit=15
[Tune-y] 1: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 2: cp=0.0462; maxdepth=7; minbucket=42; minsplit=14
[Tune-y] 2: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 3: cp=0.0157; maxdepth=14; minbucket=11; minsplit=38
[Tune-y] 3: rmse.test.rmse=2.05; time: 0.0 min
[Tune-x] 4: cp=0.106; maxdepth=10; minbucket=34; minsplit=44
[Tune-y] 4: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 5: cp=0.0176; maxdepth=4; minbucket=17; minsplit=44
[Tune-y] 5: rmse.test.rmse=2.03; time: 0.0 min
[Tune-x] 6: cp=0.0149; maxdepth=28; minbucket=43; minsplit=34
[Tune-y] 6: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 7: cp=0.757; maxdepth=22; minbucket=17; minsplit=14
[Tune-y] 7: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 8: cp=0.012; maxdepth=25; minbucket=15; minsplit=16
[Tune-y] 8: rmse.test.rmse=1.98; time: 0.0 min
[Tune-x] 9: cp=0.54; maxdepth=10; minbucket=34; minsplit=40
[Tune-y] 9: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 10: cp=0.019; maxdepth=9; minbucket=48; minsplit=45
[Tune-y] 10: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 11: cp=0.0737; maxdepth=8; minbucket=25; minsplit=7
[Tune-y] 11: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 12: cp=0.485; maxdepth=5; minbucket=8; minsplit=23
[Tune-y] 12: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 13: cp=0.00322; maxdepth=27; minbucket=21; minsplit=35
[Tune-y] 13: rmse.test.rmse=1.94; time: 0.0 min
[Tune-x] 14: cp=0.219; maxdepth=14; minbucket=46; minsplit=28
[Tune-y] 14: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 15: cp=0.111; maxdepth=28; minbucket=11; minsplit=45
[Tune-y] 15: rmse.test.rmse=2.05; time: 0.0 min
[Tune-x] 16: cp=0.00519; maxdepth=20; minbucket=24; minsplit=44
[Tune-y] 16: rmse.test.rmse=1.91; time: 0.0 min
[Tune-x] 17: cp=0.0268; maxdepth=11; minbucket=44; minsplit=35
[Tune-y] 17: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 18: cp=0.337; maxdepth=15; minbucket=42; minsplit=20
[Tune-y] 18: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 19: cp=0.00456; maxdepth=16; minbucket=47; minsplit=26
[Tune-y] 19: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 20: cp=0.648; maxdepth=9; minbucket=49; minsplit=5
[Tune-y] 20: rmse.test.rmse= 1.9; time: 0.0 min
[Tune] Result: cp=0.219; maxdepth=14; minbucket=46; minsplit=28 : rmse.test.rmse= 1.9
[1] "Sat Jan 13 19:25:53 2018"
[Tune] Started tuning learner regr.RRF for parameter set:
           Type len Def Constr Req Tunable Trafo
mtry    integer   -   3 1 to 9   -    TRUE     -
coefReg numeric   - 0.8 0 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=8; coefReg=0.87
[Tune-y] 1: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 2: mtry=4; coefReg=0.224
[Tune-y] 2: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 3: mtry=6; coefReg=0.143
[Tune-y] 3: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 4: mtry=8; coefReg=0.207
[Tune-y] 4: rmse.test.rmse=1.66; time: 0.0 min
[Tune-x] 5: mtry=4; coefReg=0.422
[Tune-y] 5: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 6: mtry=2; coefReg=0.721
[Tune-y] 6: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 7: mtry=7; coefReg=0.283
[Tune-y] 7: rmse.test.rmse=1.69; time: 0.0 min
[Tune-x] 8: mtry=6; coefReg=0.85
[Tune-y] 8: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 9: mtry=4; coefReg=0.0593
[Tune-y] 9: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 10: mtry=3; coefReg=0.858
[Tune-y] 10: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 11: mtry=4; coefReg=0.909
[Tune-y] 11: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 12: mtry=8; coefReg=0.647
[Tune-y] 12: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 13: mtry=9; coefReg=0.686
[Tune-y] 13: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 14: mtry=3; coefReg=0.202
[Tune-y] 14: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 15: mtry=4; coefReg=0.819
[Tune-y] 15: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 16: mtry=3; coefReg=0.253
[Tune-y] 16: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 17: mtry=9; coefReg=0.285
[Tune-y] 17: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 18: mtry=6; coefReg=0.766
[Tune-y] 18: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 19: mtry=4; coefReg=0.238
[Tune-y] 19: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 20: mtry=9; coefReg=0.889
[Tune-y] 20: rmse.test.rmse=1.62; time: 0.0 min
[Tune] Result: mtry=4; coefReg=0.819 : rmse.test.rmse=1.61
[1] "Sat Jan 13 19:26:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Sat Jan 13 19:26:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Sat Jan 13 19:26:19 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.456 0.357 0.279 0.218 0.170
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 1 -----> 5 
Runtime: 0.01499987 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.2786 
    Y 1          8.238 
    Y 2          7.359 
    Y 3          7.037 
    Y 4          8.542 
    Y 5           6.79 
[1] "Sat Jan 13 19:26:26 2018"
[Tune] Started tuning learner regr.svm for parameter set:
         Type len   Def    Constr Req Tunable Trafo
cost  numeric   -     0 -15 to 15   -    TRUE     Y
gamma numeric   - -3.17 -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cost=974; gamma=2.2e+03
[Tune-y] 1: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 2: cost=0.167; gamma=0.00324
[Tune-y] 2: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 3: cost=3.23; gamma=0.0006
[Tune-y] 3: rmse.test.rmse=1.78; time: 0.0 min
[Tune-x] 4: cost=733; gamma=0.00225
[Tune-y] 4: rmse.test.rmse= 1.2; time: 0.0 min
[Tune-x] 5: cost=0.128; gamma=0.199
[Tune-y] 5: rmse.test.rmse=1.85; time: 0.0 min
[Tune-x] 6: cost=0.000718; gamma=98.4
[Tune-y] 6: rmse.test.rmse=1.93; time: 0.0 min
[Tune-x] 7: cost=39.4; gamma=0.011
[Tune-y] 7: rmse.test.rmse=1.19; time: 0.0 min
[Tune-x] 8: cost=20.5; gamma=1.44e+03
[Tune-y] 8: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 9: cost=0.177; gamma=0.000105
[Tune-y] 9: rmse.test.rmse=1.93; time: 0.0 min
[Tune-x] 10: cost=0.00707; gamma=1.72e+03
[Tune-y] 10: rmse.test.rmse=1.93; time: 0.0 min
[Tune-x] 11: cost=0.109; gamma=4.97e+03
[Tune-y] 11: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 12: cost=1.23e+03; gamma=21.1
[Tune-y] 12: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 13: cost=1.42e+04; gamma=48.3
[Tune-y] 13: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 14: cost=0.0108; gamma=0.00204
[Tune-y] 14: rmse.test.rmse=1.93; time: 0.0 min
[Tune-x] 15: cost=0.0566; gamma=760
[Tune-y] 15: rmse.test.rmse=1.93; time: 0.0 min
[Tune-x] 16: cost=0.00329; gamma=0.00587
[Tune-y] 16: rmse.test.rmse=1.93; time: 0.0 min
[Tune-x] 17: cost=5.16e+03; gamma=0.0113
[Tune-y] 17: rmse.test.rmse=3.17; time: 0.0 min
[Tune-x] 18: cost=15.3; gamma=251
[Tune-y] 18: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 19: cost=0.225; gamma=0.00427
[Tune-y] 19: rmse.test.rmse=1.86; time: 0.0 min
[Tune-x] 20: cost=9.88e+03; gamma=3.25e+03
[Tune-y] 20: rmse.test.rmse= 1.9; time: 0.0 min
[Tune] Result: cost=39.4; gamma=0.011 : rmse.test.rmse=1.19
[1] "Sat Jan 13 19:26:34 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=1.45e+03; max_depth=9; eta=0.249; gamma=2.24; colsample_bytree=0.523; min_child_weight=2.87; subsample=0.863
[Tune-y] 1: rmse.test.rmse=1.56; time: 0.4 min
[Tune-x] 2: nrounds=35; max_depth=5; eta=0.254; gamma=1.52; colsample_bytree=0.588; min_child_weight=13.5; subsample=0.462
[Tune-y] 2: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 3: nrounds=477; max_depth=9; eta=0.251; gamma=0.593; colsample_bytree=0.405; min_child_weight=17.2; subsample=0.545
[Tune-y] 3: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 4: nrounds=2.32e+03; max_depth=9; eta=0.388; gamma=9.6; colsample_bytree=0.575; min_child_weight=5.64; subsample=0.402
[Tune-y] 4: rmse.test.rmse=1.72; time: 0.3 min
[Tune-x] 5: nrounds=87; max_depth=9; eta=0.136; gamma=2.53; colsample_bytree=0.664; min_child_weight=5.69; subsample=0.723
[Tune-y] 5: rmse.test.rmse=1.49; time: 0.0 min
[Tune-x] 6: nrounds=983; max_depth=5; eta=0.143; gamma=9.42; colsample_bytree=0.656; min_child_weight=12.5; subsample=0.389
[Tune-y] 6: rmse.test.rmse=1.93; time: 0.1 min
[Tune-x] 7: nrounds=142; max_depth=1; eta=0.538; gamma=0.988; colsample_bytree=0.329; min_child_weight=7.87; subsample=0.379
[Tune-y] 7: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 8: nrounds=2.08e+03; max_depth=4; eta=0.403; gamma=7.81; colsample_bytree=0.46; min_child_weight=18; subsample=0.627
[Tune-y] 8: rmse.test.rmse=1.87; time: 0.1 min
[Tune-x] 9: nrounds=597; max_depth=10; eta=0.0866; gamma=8.71; colsample_bytree=0.396; min_child_weight=12.2; subsample=0.57
[Tune-y] 9: rmse.test.rmse=1.83; time: 0.1 min
[Tune-x] 10: nrounds=1.63e+03; max_depth=5; eta=0.179; gamma=8.54; colsample_bytree=0.569; min_child_weight=16.9; subsample=0.58
[Tune-y] 10: rmse.test.rmse=1.88; time: 0.1 min
[Tune-x] 11: nrounds=1.31e+03; max_depth=4; eta=0.134; gamma=4.8; colsample_bytree=0.674; min_child_weight=9.22; subsample=0.953
[Tune-y] 11: rmse.test.rmse=1.56; time: 0.2 min
[Tune-x] 12: nrounds=38; max_depth=10; eta=0.0104; gamma=7; colsample_bytree=0.554; min_child_weight=4.39; subsample=0.272
[Tune-y] 12: rmse.test.rmse=5.67; time: 0.0 min
[Tune-x] 13: nrounds=20; max_depth=9; eta=0.0393; gamma=2.57; colsample_bytree=0.631; min_child_weight=18.4; subsample=0.679
[Tune-y] 13: rmse.test.rmse=4.04; time: 0.0 min
[Tune-x] 14: nrounds=1.95e+03; max_depth=8; eta=0.0405; gamma=1.78; colsample_bytree=0.573; min_child_weight=18.7; subsample=0.978
[Tune-y] 14: rmse.test.rmse=1.97; time: 0.2 min
[Tune-x] 15: nrounds=2.44e+03; max_depth=8; eta=0.287; gamma=4.64; colsample_bytree=0.458; min_child_weight=18.5; subsample=0.959
[Tune-y] 15: rmse.test.rmse=1.97; time: 0.2 min
[Tune-x] 16: nrounds=213; max_depth=5; eta=0.298; gamma=9.03; colsample_bytree=0.372; min_child_weight=15; subsample=0.411
[Tune-y] 16: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 17: nrounds=700; max_depth=6; eta=0.18; gamma=6.8; colsample_bytree=0.475; min_child_weight=8.38; subsample=0.545
[Tune-y] 17: rmse.test.rmse=1.75; time: 0.1 min
[Tune-x] 18: nrounds=210; max_depth=9; eta=0.422; gamma=0.862; colsample_bytree=0.42; min_child_weight=0.282; subsample=0.347
[Tune-y] 18: rmse.test.rmse= 1.9; time: 0.1 min
[Tune-x] 19: nrounds=1.73e+03; max_depth=1; eta=0.418; gamma=2.18; colsample_bytree=0.364; min_child_weight=5.32; subsample=0.592
[Tune-y] 19: rmse.test.rmse=1.65; time: 0.1 min
[Tune-x] 20: nrounds=20; max_depth=8; eta=0.252; gamma=4.15; colsample_bytree=0.577; min_child_weight=6.44; subsample=0.811
[Tune-y] 20: rmse.test.rmse=1.67; time: 0.0 min
[Tune] Result: nrounds=87; max_depth=9; eta=0.136; gamma=2.53; colsample_bytree=0.664; min_child_weight=5.69; subsample=0.723 : rmse.test.rmse=1.49
[1] "Sat Jan 13 19:28:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Sat Jan 13 19:28:50 2018"
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  No variation for for: V11, V12
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  No variation for for: V11, V12
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [1,0.600247], n=(23,53)
**PRUNE** @depth 0: [1,0.547658]
**GROW** @depth 0: [4,0.496838], n=(51,25)
**PRUNE** @depth 0: [4,0.496838]
**GROW** @depth 0: [1,0.439606], n=(16,60)
**PRUNE** @depth 0: [1,0.513969]
**GROW** @depth 0: [2,0.469114], n=(62,14)
**PRUNE** @depth 0: [2,0.47417]
**GROW** @depth 0: [1,0.481512], n=(17,59)
**PRUNE** @depth 0: [1,0.481512]
**GROW** @depth 0: [1,0.403657], n=(12,64)
**GROW** @depth 1: [4,0.531975], n=(46,16)
**PRUNE** @depth 1: [4,0.531975]
**PRUNE** @depth 0: [1,0.439606]
**GROW** @depth 0: [3,0.564318], n=(60,16)
**PRUNE** @depth 0: [3,0.629028]
r=1000 d=[0]; n=76
**GROW** @depth 0: [4,0.496838], n=(51,25)
**PRUNE** @depth 0: [4,0.496838]
**GROW** @depth 0: [1,0.513969], n=(18,58)
**PRUNE** @depth 0: [1,0.520953]
**GROW** @depth 0: [1,0.513969], n=(18,58)
**PRUNE** @depth 0: [1,0.539441]
**GROW** @depth 0: [2,0.420312], n=(57,19)
**PRUNE** @depth 0: [2,0.420312]
**GROW** @depth 0: [2,0.237635], n=(14,62)
**PRUNE** @depth 0: [2,0.244669]
**GROW** @depth 0: [1,0.671939], n=(55,21)
**PRUNE** @depth 0: [1,0.671939]
**GROW** @depth 0: [2,0.414377], n=(56,20)
**PRUNE** @depth 0: [2,0.414377]
**GROW** @depth 0: [4,0.496838], n=(51,25)
**PRUNE** @depth 0: [4,0.496838]
**GROW** @depth 0: [1,0.413928], n=(13,63)
r=2000 d=[0] [0]; n=(15,61)

Sampling @ nn=0 pred locs:
**PRUNE** @depth 0: [1,0.439606]
**GROW** @depth 0: [1,0.513969], n=(18,58)
**PRUNE** @depth 0: [1,0.439606]
**GROW** @depth 0: [3,0.381975], n=(18,58)
**PRUNE** @depth 0: [3,0.381975]
**GROW** @depth 0: [1,0.432621], n=(15,61)
**PRUNE** @depth 0: [1,0.432621]
r=1000 d=[0]; mh=2 n=76
**GROW** @depth 0: [4,0.496838], n=(51,25)
**PRUNE** @depth 0: [4,0.496838]
**GROW** @depth 0: [1,0.424404], n=(14,62)
**GROW** @depth 1: [7,0.542603], n=(12,52)
**PRUNE** @depth 1: [7,0.542603]
**PRUNE** @depth 0: [1,0.432621]
r=2000 d=[0]; mh=2 n=76
**GROW** @depth 0: [1,0.424404], n=(14,62)
**PRUNE** @depth 0: [1,0.424404]
**GROW** @depth 0: [4,0.496838], n=(51,25)
**PRUNE** @depth 0: [4,0.496838]
**GROW** @depth 0: [1,0.709326], n=(60,16)
**PRUNE** @depth 0: [1,0.706656]
**GROW** @depth 0: [4,0.496838], n=(51,25)
**PRUNE** @depth 0: [4,0.496838]
**GROW** @depth 0: [1,0.513969], n=(18,58)
**PRUNE** @depth 0: [1,0.520953]
**GROW** @depth 0: [2,0.47417], n=(63,13)
**PRUNE** @depth 0: [2,0.47417]
**GROW** @depth 0: [4,0.496838], n=(51,25)
**PRUNE** @depth 0: [4,0.525884]
**GROW** @depth 0: [4,0.653549], n=(64,12)
**PRUNE** @depth 0: [4,0.653549]
r=3000 d=[0]; mh=2 n=76
**GROW** @depth 0: [4,0.496838], n=(51,25)
**PRUNE** @depth 0: [4,0.496838]
**GROW** @depth 0: [1,0.547658], n=(21,55)
**PRUNE** @depth 0: [1,0.547658]
**GROW** @depth 0: [4,0.496838], n=(51,25)
**PRUNE** @depth 0: [4,0.496838]
**GROW** @depth 0: [3,0.517422], n=(54,22)
**PRUNE** @depth 0: [3,0.504323]
r=4000 d=[0]; mh=2 n=76
**GROW** @depth 0: [8,0.358306], n=(52,24)
**PRUNE** @depth 0: [8,0.358306]
**GROW** @depth 0: [4,0.653549], n=(64,12)
**PRUNE** @depth 0: [4,0.653549]
**GROW** @depth 0: [1,0.539441], n=(20,56)
**PRUNE** @depth 0: [1,0.481512]
**GROW** @depth 0: [1,0.413928], n=(13,63)
**PRUNE** @depth 0: [1,0.413928]
**GROW** @depth 0: [2,0.425148], n=(58,18)
**PRUNE** @depth 0: [2,0.469114]
**GROW** @depth 0: [4,0.528227], n=(53,23)
**PRUNE** @depth 0: [4,0.525884]
**GROW** @depth 0: [1,0.520953], n=(19,57)
**PRUNE** @depth 0: [1,0.513969]
r=5000 d=[0]; mh=2 n=76
Grow: 11.9%, Prune: 36.52%, Change: 55.6%, Swap: 0%

[1] "Sat Jan 13 19:30:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Sat Jan 13 19:30:45 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=76

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=76
r=2000 d=[0]; mh=1 n=76
r=3000 d=[0]; mh=1 n=76

[1] "Sat Jan 13 19:30:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7064135 
gamma= 13.2188 	 alpha= 2.215 	 beta= 9.2703 
[1] "Sat Jan 13 19:31:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Sat Jan 13 19:31:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=76
r=2000 d=[0]; n=76

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=76
r=2000 d=[0]; mh=1 n=76
r=3000 d=[0]; mh=1 n=76
r=4000 d=[0]; mh=1 n=76
r=5000 d=[0]; mh=1 n=76
Grow: 0%, 

[1] "Sat Jan 13 19:31:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cforest no default is available.
[1] "Sat Jan 13 19:31:20 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Sat Jan 13 19:31:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Sat Jan 13 19:31:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Sat Jan 13 19:31:41 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Sat Jan 13 19:31:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Sat Jan 13 19:31:53 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
[Tune] Started tuning learner regr.extraTrees for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
numRandomCuts integer   -   1 1 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=9; numRandomCuts=18
[Tune-y] 1: rmse.test.rmse= 1.9; time: 0.1 min
[Tune-x] 2: mtry=3; numRandomCuts=7
[Tune-y] 2: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 3: mtry=1; numRandomCuts=6
[Tune-y] 3: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 4: mtry=8; numRandomCuts=7
[Tune-y] 4: rmse.test.rmse=1.79; time: 0.0 min
[Tune-x] 5: mtry=3; numRandomCuts=21
[Tune-y] 5: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 6: mtry=9; numRandomCuts=24
[Tune-y] 6: rmse.test.rmse=1.95; time: 0.1 min
[Tune-x] 7: mtry=5; numRandomCuts=9
[Tune-y] 7: rmse.test.rmse=1.72; time: 0.0 min
[Tune-x] 8: mtry=3; numRandomCuts=11
[Tune-y] 8: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 9: mtry=8; numRandomCuts=14
[Tune-y] 9: rmse.test.rmse=1.86; time: 0.0 min
[Tune-x] 10: mtry=5; numRandomCuts=2
[Tune-y] 10: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 11: mtry=9; numRandomCuts=9
[Tune-y] 11: rmse.test.rmse=1.84; time: 0.0 min
[Tune-x] 12: mtry=5; numRandomCuts=24
[Tune-y] 12: rmse.test.rmse=1.78; time: 0.0 min
[Tune-x] 13: mtry=9; numRandomCuts=12
[Tune-y] 13: rmse.test.rmse=1.88; time: 0.0 min
[Tune-x] 14: mtry=8; numRandomCuts=15
[Tune-y] 14: rmse.test.rmse=1.84; time: 0.0 min
[Tune-x] 15: mtry=6; numRandomCuts=8
[Tune-y] 15: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 16: mtry=4; numRandomCuts=1
[Tune-y] 16: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 17: mtry=5; numRandomCuts=1
[Tune-y] 17: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 18: mtry=1; numRandomCuts=23
[Tune-y] 18: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 19: mtry=5; numRandomCuts=12
[Tune-y] 19: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 20: mtry=8; numRandomCuts=7
[Tune-y] 20: rmse.test.rmse=1.81; time: 0.0 min
[Tune] Result: mtry=5; numRandomCuts=1 : rmse.test.rmse=1.61
[1] "Sat Jan 13 19:32:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Sat Jan 13 19:32:37 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Sat Jan 13 19:32:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.frbs no default is available.
[1] "Sat Jan 13 19:32:50 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
In addition: Warning messages:
1: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
2: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
3: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
4: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
5: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
6: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Sat Jan 13 19:32:59 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=601; interaction.depth=7; shrinkage=0.136; n.minobsinnode=10
[Tune-y] 1: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 2: n.trees=14; interaction.depth=3; shrinkage=0.523; n.minobsinnode=10
[Tune-y] 2: rmse.test.rmse=1.71; time: 0.0 min
[Tune-x] 3: n.trees=29; interaction.depth=9; shrinkage=0.54; n.minobsinnode=24
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: n.trees=81; interaction.depth=4; shrinkage=0.196; n.minobsinnode=13
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: n.trees=537; interaction.depth=6; shrinkage=0.332; n.minobsinnode=6
[Tune-y] 5: rmse.test.rmse= 1.7; time: 0.0 min
[Tune-x] 6: n.trees=649; interaction.depth=4; shrinkage=0.296; n.minobsinnode=24
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: n.trees=840; interaction.depth=5; shrinkage=0.47; n.minobsinnode=17
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 7: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 8: n.trees=174; interaction.depth=4; shrinkage=0.214; n.minobsinnode=5
[Tune-y] 8: rmse.test.rmse=1.69; time: 0.0 min
[Tune-x] 9: n.trees=95; interaction.depth=1; shrinkage=0.0197; n.minobsinnode=24
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: n.trees=100; interaction.depth=5; shrinkage=0.479; n.minobsinnode=10
[Tune-y] 10: rmse.test.rmse=2.06; time: 0.0 min
[Tune-x] 11: n.trees=371; interaction.depth=3; shrinkage=0.578; n.minobsinnode=24
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 11: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 12: n.trees=36; interaction.depth=8; shrinkage=0.01; n.minobsinnode=17
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: n.trees=640; interaction.depth=8; shrinkage=0.538; n.minobsinnode=17
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: n.trees=14; interaction.depth=8; shrinkage=0.0574; n.minobsinnode=10
[Tune-y] 14: rmse.test.rmse=1.84; time: 0.0 min
[Tune-x] 15: n.trees=31; interaction.depth=10; shrinkage=0.447; n.minobsinnode=21
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: n.trees=11; interaction.depth=6; shrinkage=0.531; n.minobsinnode=19
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 16: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 17: n.trees=322; interaction.depth=9; shrinkage=0.175; n.minobsinnode=25
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 17: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 18: n.trees=42; interaction.depth=5; shrinkage=0.142; n.minobsinnode=13
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 18: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 19: n.trees=66; interaction.depth=4; shrinkage=0.428; n.minobsinnode=15
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: n.trees=418; interaction.depth=2; shrinkage=0.592; n.minobsinnode=21
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: n.trees=174; interaction.depth=4; shrinkage=0.214; n.minobsinnode=5 : rmse.test.rmse=1.69
[1] "Sat Jan 13 19:33:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Sat Jan 13 19:33:13 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.89; lambda=0.498
[Tune-y] 1: rmse.test.rmse=1.46; time: 0.0 min
[Tune-x] 2: alpha=0.225; lambda=0.0105
[Tune-y] 2: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 3: alpha=0.0706; lambda=0.00839
[Tune-y] 3: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 4: alpha=0.872; lambda=0.0114
[Tune-y] 4: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 5: alpha=0.229; lambda=1.61
[Tune-y] 5: rmse.test.rmse=1.57; time: 0.0 min
[Tune-x] 6: alpha=0.9; lambda=5.16
[Tune-y] 6: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 7: alpha=0.454; lambda=0.0246
[Tune-y] 7: rmse.test.rmse=1.07; time: 0.0 min
[Tune-x] 8: alpha=0.325; lambda=0.0376
[Tune-y] 8: rmse.test.rmse=1.07; time: 0.0 min
[Tune-x] 9: alpha=0.865; lambda=0.111
[Tune-y] 9: rmse.test.rmse=1.09; time: 0.0 min
[Tune-x] 10: alpha=0.552; lambda=0.0015
[Tune-y] 10: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 11: alpha=0.906; lambda=0.0209
[Tune-y] 11: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 12: alpha=0.492; lambda=5.18
[Tune-y] 12: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 13: alpha=0.962; lambda=0.0525
[Tune-y] 13: rmse.test.rmse=1.07; time: 0.0 min
[Tune-x] 14: alpha=0.782; lambda=0.21
[Tune-y] 14: rmse.test.rmse=1.13; time: 0.0 min
[Tune-x] 15: alpha=0.62; lambda=0.017
[Tune-y] 15: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 16: alpha=0.355; lambda=0.00105
[Tune-y] 16: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 17: alpha=0.488; lambda=0.00101
[Tune-y] 17: rmse.test.rmse=1.06; time: 0.0 min
[Tune-x] 18: alpha=0.0312; lambda=3.42
[Tune-y] 18: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 19: alpha=0.5; lambda=0.068
[Tune-y] 19: rmse.test.rmse=1.07; time: 0.0 min
[Tune-x] 20: alpha=0.797; lambda=0.0114
[Tune-y] 20: rmse.test.rmse=1.06; time: 0.0 min
[Tune] Result: alpha=0.488; lambda=0.00101 : rmse.test.rmse=1.06
[1] "Sat Jan 13 19:33:24 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==============                                                        |  20%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:33:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================                                |  54%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:33:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:33:51 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |====================                                                  |  28%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:34:00 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  2 1.974392 2 2 1.893274 1.897354 1.808234 1.589497 1.778136 
  - best initial criterion value(s) :  -152.167 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       152.17  |proj g|=       1.8588
At iterate     1  f =       150.66  |proj g|=        1.5146
At iterate     2  f =          148  |proj g|=        1.5036
At iterate     3  f =       145.97  |proj g|=        1.7418
At iterate     4  f =        145.3  |proj g|=        1.7289
At iterate     5  f =       144.83  |proj g|=        1.7636
At iterate     6  f =       144.14  |proj g|=        1.7412
At iterate     7  f =       143.23  |proj g|=        1.3975
At iterate     8  f =       142.64  |proj g|=        1.6669
At iterate     9  f =       140.98  |proj g|=        1.7632
At iterate    10  f =       140.62  |proj g|=        1.7585
At iterate    11  f =       139.71  |proj g|=         1.318
At iterate    12  f =       139.04  |proj g|=        1.3047
At iterate    13  f =        138.8  |proj g|=        1.7206
At iterate    14  f =       138.71  |proj g|=        1.7378
At iterate    15  f =       138.63  |proj g|=        1.7352
At iterate    16  f =       138.54  |proj g|=        1.1514
At iterate    17  f =        138.4  |proj g|=        1.1275
At iterate    18  f =       138.17  |proj g|=         1.101
At iterate    19  f =       138.04  |proj g|=        1.7253
At iterate    20  f =       137.93  |proj g|=        1.7335
At iterate    21  f =       137.89  |proj g|=        1.7363
At iterate    22  f =       137.85  |proj g|=        1.7366
At iterate    23  f =       137.55  |proj g|=        1.2139
At iterate    24  f =       137.43  |proj g|=       0.66484
At iterate    25  f =        137.4  |proj g|=       0.64464
At iterate    26  f =       137.38  |proj g|=       0.77055
At iterate    27  f =       137.36  |proj g|=       0.82714
At iterate    28  f =       137.28  |proj g|=        1.7302
At iterate    29  f =       137.25  |proj g|=        1.7231
At iterate    30  f =       137.15  |proj g|=        1.2566
At iterate    31  f =        137.1  |proj g|=        1.4404
At iterate    32  f =       137.07  |proj g|=        1.7168
At iterate    33  f =       137.03  |proj g|=       0.75589
At iterate    34  f =       136.91  |proj g|=        1.7002
At iterate    35  f =       136.89  |proj g|=        1.6974
At iterate    36  f =       136.77  |proj g|=         1.681
At iterate    37  f =       136.59  |proj g|=        1.3421
At iterate    38  f =       136.51  |proj g|=       0.96731
At iterate    39  f =       136.47  |proj g|=        1.6284
At iterate    40  f =       136.46  |proj g|=       0.40906
At iterate    41  f =       136.46  |proj g|=       0.46113
At iterate    42  f =       136.45  |proj g|=       0.32699
At iterate    43  f =       136.44  |proj g|=       0.22099
At iterate    44  f =       136.44  |proj g|=       0.19326
At iterate    45  f =       136.44  |proj g|=       0.20285
At iterate    46  f =       136.44  |proj g|=       0.91384
At iterate    47  f =       136.43  |proj g|=       0.47783
At iterate    48  f =       136.42  |proj g|=       0.52499
At iterate    49  f =        136.4  |proj g|=       0.52081
At iterate    50  f =       136.39  |proj g|=       0.17757
At iterate    51  f =       136.39  |proj g|=       0.21697
At iterate    52  f =       136.39  |proj g|=       0.17725
At iterate    53  f =       136.39  |proj g|=       0.22881
At iterate    54  f =       136.39  |proj g|=       0.47239
At iterate    55  f =       136.38  |proj g|=       0.13709
At iterate    56  f =       136.38  |proj g|=       0.10889
At iterate    57  f =       136.38  |proj g|=       0.15749
At iterate    58  f =       136.38  |proj g|=       0.16599
At iterate    59  f =       136.38  |proj g|=       0.23396
At iterate    60  f =       136.38  |proj g|=       0.22133
At iterate    61  f =       136.37  |proj g|=       0.97848
At iterate    62  f =       136.35  |proj g|=        1.6309
At iterate    63  f =       136.34  |proj g|=         1.628
At iterate    64  f =       136.33  |proj g|=        1.6265
At iterate    65  f =       136.31  |proj g|=        1.5329
At iterate    66  f =        136.3  |proj g|=        1.6926
At iterate    67  f =       136.27  |proj g|=        1.2789
At iterate    68  f =       136.27  |proj g|=       0.71978
At iterate    69  f =       136.26  |proj g|=       0.26962
At iterate    70  f =       136.26  |proj g|=       0.23416
At iterate    71  f =       136.26  |proj g|=       0.20106
At iterate    72  f =       136.26  |proj g|=       0.18702
At iterate    73  f =       136.26  |proj g|=       0.21854
At iterate    74  f =       136.25  |proj g|=       0.29966
At iterate    75  f =       136.25  |proj g|=      0.040704
At iterate    76  f =       136.25  |proj g|=       0.03384
At iterate    77  f =       136.25  |proj g|=      0.035795
At iterate    78  f =       136.25  |proj g|=      0.024751
At iterate    79  f =       136.25  |proj g|=      0.013055
At iterate    80  f =       136.25  |proj g|=      0.013835
At iterate    81  f =       136.25  |proj g|=      0.020773
At iterate    82  f =       136.25  |proj g|=      0.051823
At iterate    83  f =       136.25  |proj g|=      0.034252
At iterate    84  f =       136.25  |proj g|=      0.011135
At iterate    85  f =       136.25  |proj g|=      0.002771
At iterate    86  f =       136.25  |proj g|=     0.0028152

iterations 86
function evaluations 107
segments explored during Cauchy searches 93
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0.00281525
final function value 136.255

F = 136.255
final  value 136.254586 
converged
[1] "Sat Jan 13 19:34:18 2018"
[Tune] Started tuning learner regr.ksvm for parameter set:
         Type len  Def    Constr Req Tunable Trafo
C     numeric   -    0  -5 to 10   -    TRUE     Y
sigma numeric   - TRUE -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: C=894; sigma=358
[Tune-y] 1: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 2: C=0.251; sigma=1.18e+04
[Tune-y] 2: rmse.test.rmse=1.88; time: 0.0 min
[Tune-x] 3: C=0.925; sigma=6.96e-05
[Tune-y] 3: rmse.test.rmse=1.88; time: 0.0 min
[Tune-x] 4: C=2.24; sigma=0.00018
[Tune-y] 4: rmse.test.rmse=1.86; time: 0.0 min
[Tune-x] 5: C=2.31; sigma=2.33e+04
[Tune-y] 5: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 6: C=11.9; sigma=4.39
[Tune-y] 6: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 7: C=29; sigma=3.96e-05
[Tune-y] 7: rmse.test.rmse=1.79; time: 0.0 min
[Tune-x] 8: C=19.5; sigma=3.15e+04
[Tune-y] 8: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 9: C=285; sigma=566
[Tune-y] 9: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 10: C=0.377; sigma=0.00828
[Tune-y] 10: rmse.test.rmse=1.69; time: 0.0 min
[Tune-x] 11: C=302; sigma=0.00125
[Tune-y] 11: rmse.test.rmse=1.08; time: 0.0 min
[Tune-x] 12: C=3.73; sigma=2.01
[Tune-y] 12: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 13: C=0.911; sigma=0.165
[Tune-y] 13: rmse.test.rmse= 1.7; time: 0.0 min
[Tune-x] 14: C=0.0573; sigma=6.83e+03
[Tune-y] 14: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 15: C=2.43; sigma=344
[Tune-y] 15: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 16: C=0.38; sigma=0.0794
[Tune-y] 16: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 17: C=256; sigma=0.0776
[Tune-y] 17: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 18: C=0.422; sigma=8.7
[Tune-y] 18: rmse.test.rmse=1.88; time: 0.0 min
[Tune-x] 19: C=0.135; sigma=0.0131
[Tune-y] 19: rmse.test.rmse=1.79; time: 0.0 min
[Tune-x] 20: C=0.463; sigma=0.000174
[Tune-y] 20: rmse.test.rmse=1.88; time: 0.0 min
[Tune] Result: C=302; sigma=0.00125 : rmse.test.rmse=1.08
[1] "Sat Jan 13 19:34:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 24), d = 0.492612, its = 6
i = 2 (of 24), d = 0.496712, its = 6
i = 3 (of 24), d = 0.513291, its = 6
i = 4 (of 24), d = 0.531245, its = 6
i = 5 (of 24), d = 0.494368, its = 6
i = 6 (of 24), d = 0.510395, its = 7
i = 7 (of 24), d = 0.593564, its = 7
i = 8 (of 24), d = 0.432223, its = 6
i = 9 (of 24), d = 0.560845, its = 7
i = 10 (of 24), d = 0.502055, its = 7
i = 11 (of 24), d = 0.477971, its = 6
i = 12 (of 24), d = 0.465053, its = 6
i = 13 (of 24), d = 0.531684, its = 7
i = 14 (of 24), d = 0.481109, its = 6
i = 15 (of 24), d = 0.584604, its = 7
i = 16 (of 24), d = 0.5137, its = 7
i = 17 (of 24), d = 0.487175, its = 7
i = 18 (of 24), d = 0.764179, its = 8
i = 19 (of 24), d = 0.492157, its = 7
i = 20 (of 24), d = 0.589312, its = 7
i = 21 (of 24), d = 0.524132, its = 7
i = 22 (of 24), d = 0.498427, its = 7
i = 23 (of 24), d = 0.502276, its = 7
i = 24 (of 24), d = 0.482043, its = 6
[1] "Sat Jan 13 19:34:36 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Sat Jan 13 19:34:42 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Sat Jan 13 19:34:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Sat Jan 13 19:34:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Sat Jan 13 19:35:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Sat Jan 13 19:35:06 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=18; decay=0.142
# weights:  199
initial  value 4093.458092 
iter  10 value 60.299927
iter  20 value 46.627331
iter  30 value 45.428390
iter  40 value 45.296457
iter  50 value 45.179817
iter  60 value 45.070233
iter  70 value 44.981671
iter  80 value 44.935287
iter  90 value 44.912779
iter 100 value 44.903515
final  value 44.903515 
stopped after 100 iterations
# weights:  199
initial  value 3151.305774 
iter  10 value 114.431100
iter  20 value 62.223803
iter  30 value 58.726300
iter  40 value 58.255123
iter  50 value 58.060083
iter  60 value 57.693954
iter  70 value 57.599478
iter  80 value 57.584922
iter  90 value 57.583849
iter 100 value 57.582893
final  value 57.582893 
stopped after 100 iterations
# weights:  199
initial  value 2482.657145 
iter  10 value 76.392315
iter  20 value 56.797591
iter  30 value 55.459772
iter  40 value 55.167302
iter  50 value 55.022341
iter  60 value 54.958306
iter  70 value 54.921630
iter  80 value 54.895392
iter  90 value 54.885428
iter 100 value 54.883866
final  value 54.883866 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse= 1.1; time: 0.0 min
[Tune-x] 2: size=5; decay=0.000381
# weights:  56
initial  value 3404.120471 
iter  10 value 47.958821
iter  20 value 25.946079
iter  30 value 10.357525
iter  40 value 6.132487
iter  50 value 4.137465
iter  60 value 2.893645
iter  70 value 2.176584
iter  80 value 1.704031
iter  90 value 1.441361
iter 100 value 1.263687
final  value 1.263687 
stopped after 100 iterations
# weights:  56
initial  value 3613.829945 
iter  10 value 65.501068
iter  20 value 40.411632
iter  30 value 23.513771
iter  40 value 15.856098
iter  50 value 11.717688
iter  60 value 9.736942
iter  70 value 6.494158
iter  80 value 5.413058
iter  90 value 4.761510
iter 100 value 4.403887
final  value 4.403887 
stopped after 100 iterations
# weights:  56
initial  value 3478.196089 
iter  10 value 143.064274
iter  20 value 57.994094
iter  30 value 45.210756
iter  40 value 40.794025
iter  50 value 40.355414
iter  60 value 40.276818
iter  70 value 39.858167
iter  80 value 33.762861
iter  90 value 29.129147
iter 100 value 26.151145
final  value 26.151145 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 3: size=2; decay=0.00027
# weights:  23
initial  value 4177.097348 
iter  10 value 166.499587
final  value 166.435592 
converged
# weights:  23
initial  value 3288.313365 
iter  10 value 121.931046
iter  20 value 56.564176
iter  30 value 46.575776
iter  40 value 45.252250
iter  50 value 45.048692
iter  60 value 44.906394
iter  70 value 44.743944
iter  80 value 44.662282
iter  90 value 44.323465
iter 100 value 43.645614
final  value 43.645614 
stopped after 100 iterations
# weights:  23
initial  value 3948.282423 
iter  10 value 49.257955
iter  20 value 32.302275
iter  30 value 24.405158
iter  40 value 21.095215
iter  50 value 21.029827
iter  60 value 21.019458
iter  70 value 21.009439
iter  80 value 21.003390
iter  90 value 21.001265
iter 100 value 20.998925
final  value 20.998925 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse= 1.6; time: 0.0 min
[Tune-x] 4: size=18; decay=0.00043
# weights:  199
initial  value 4462.537008 
iter  10 value 166.419841
iter  20 value 72.619548
iter  30 value 29.779983
iter  40 value 20.557437
iter  50 value 16.535359
iter  60 value 15.332276
iter  70 value 13.181443
iter  80 value 11.480966
iter  90 value 8.752311
iter 100 value 6.332790
final  value 6.332790 
stopped after 100 iterations
# weights:  199
initial  value 3533.986301 
iter  10 value 79.538185
iter  20 value 49.302943
iter  30 value 46.838376
iter  40 value 45.722232
iter  50 value 45.531807
iter  60 value 45.360618
iter  70 value 44.428466
iter  80 value 40.951295
iter  90 value 28.525753
iter 100 value 20.175713
final  value 20.175713 
stopped after 100 iterations
# weights:  199
initial  value 2571.866386 
iter  10 value 57.268365
iter  20 value 35.385749
iter  30 value 19.875653
iter  40 value 12.387141
iter  50 value 8.881814
iter  60 value 6.083142
iter  70 value 3.459820
iter  80 value 1.779015
iter  90 value 1.437679
iter 100 value 1.137594
final  value 1.137594 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=2.16; time: 0.0 min
[Tune-x] 5: size=5; decay=0.859
# weights:  56
initial  value 4622.715543 
iter  10 value 243.541673
iter  20 value 154.873798
iter  30 value 136.072975
iter  40 value 118.017684
iter  50 value 112.189885
iter  60 value 107.530743
iter  70 value 106.235409
iter  80 value 105.114221
iter  90 value 104.031501
iter 100 value 103.487218
final  value 103.487218 
stopped after 100 iterations
# weights:  56
initial  value 3241.123935 
iter  10 value 318.128730
iter  20 value 168.811271
iter  30 value 134.194675
iter  40 value 121.293735
iter  50 value 114.713469
iter  60 value 112.494095
iter  70 value 111.861850
iter  80 value 110.346974
iter  90 value 109.721298
iter 100 value 108.190373
final  value 108.190373 
stopped after 100 iterations
# weights:  56
initial  value 4502.763363 
iter  10 value 265.438335
iter  20 value 162.405593
iter  30 value 142.022883
iter  40 value 125.425607
iter  50 value 121.228029
iter  60 value 119.093332
iter  70 value 117.176068
iter  80 value 115.787845
iter  90 value 115.272265
iter 100 value 114.386678
final  value 114.386678 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse= 1.3; time: 0.0 min
[Tune-x] 6: size=19; decay=5.11
# weights:  210
initial  value 3527.202711 
iter  10 value 211.120046
iter  20 value 197.818483
iter  30 value 194.112074
iter  40 value 193.648363
iter  50 value 193.645413
final  value 193.645404 
converged
# weights:  210
initial  value 2390.023814 
iter  10 value 229.239258
iter  20 value 203.444349
iter  30 value 197.584886
iter  40 value 196.245177
iter  50 value 196.137328
iter  60 value 196.122830
iter  70 value 196.120076
final  value 196.120032 
converged
# weights:  210
initial  value 4592.351433 
iter  10 value 238.221255
iter  20 value 221.161075
iter  30 value 211.483503
iter  40 value 210.641129
iter  50 value 210.180878
iter  60 value 210.106179
final  value 210.104957 
converged
[Tune-y] 6: rmse.test.rmse=1.76; time: 0.0 min
[Tune-x] 7: size=10; decay=0.0014
# weights:  111
initial  value 3048.172297 
iter  10 value 36.611323
iter  20 value 22.626131
iter  30 value 11.485075
iter  40 value 6.047997
iter  50 value 4.096236
iter  60 value 3.260398
iter  70 value 2.747989
iter  80 value 2.389806
iter  90 value 2.060035
iter 100 value 1.912681
final  value 1.912681 
stopped after 100 iterations
# weights:  111
initial  value 3015.272748 
iter  10 value 53.289147
iter  20 value 39.898201
iter  30 value 23.838010
iter  40 value 17.376104
iter  50 value 11.886381
iter  60 value 8.486708
iter  70 value 5.880621
iter  80 value 4.725913
iter  90 value 3.749224
iter 100 value 3.175734
final  value 3.175734 
stopped after 100 iterations
# weights:  111
initial  value 4301.302786 
iter  10 value 135.140447
iter  20 value 73.103295
iter  30 value 33.320594
iter  40 value 25.804552
iter  50 value 16.986264
iter  60 value 12.284187
iter  70 value 9.507828
iter  80 value 7.192938
iter  90 value 5.787483
iter 100 value 4.822452
final  value 4.822452 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=2.85; time: 0.0 min
[Tune-x] 8: size=7; decay=0.0027
# weights:  78
initial  value 3319.787330 
iter  10 value 158.625699
iter  20 value 49.270017
iter  30 value 32.544335
iter  40 value 26.074500
iter  50 value 21.974581
iter  60 value 19.802999
iter  70 value 18.843886
iter  80 value 17.616191
iter  90 value 13.781737
iter 100 value 11.767924
final  value 11.767924 
stopped after 100 iterations
# weights:  78
initial  value 3637.380675 
iter  10 value 171.512256
iter  20 value 49.836308
iter  30 value 47.283765
iter  40 value 45.812029
iter  50 value 41.287157
iter  60 value 37.583758
iter  70 value 32.245618
iter  80 value 21.466361
iter  90 value 17.141511
iter 100 value 12.774615
final  value 12.774615 
stopped after 100 iterations
# weights:  78
initial  value 4310.079159 
iter  10 value 164.332392
iter  20 value 62.803932
iter  30 value 49.799228
iter  40 value 43.527958
iter  50 value 40.346607
iter  60 value 33.615951
iter  70 value 25.415697
iter  80 value 15.895634
iter  90 value 11.799780
iter 100 value 9.522915
final  value 9.522915 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 9: size=18; decay=0.0142
# weights:  199
initial  value 3278.032668 
iter  10 value 38.704454
iter  20 value 28.353809
iter  30 value 20.993091
iter  40 value 16.699848
iter  50 value 14.341830
iter  60 value 13.399194
iter  70 value 12.539309
iter  80 value 12.011957
iter  90 value 11.609359
iter 100 value 11.371878
final  value 11.371878 
stopped after 100 iterations
# weights:  199
initial  value 3440.443592 
iter  10 value 48.508798
iter  20 value 40.977509
iter  30 value 30.359932
iter  40 value 26.417197
iter  50 value 23.904609
iter  60 value 20.988883
iter  70 value 19.115762
iter  80 value 18.212383
iter  90 value 17.472766
iter 100 value 16.928704
final  value 16.928704 
stopped after 100 iterations
# weights:  199
initial  value 4457.564372 
iter  10 value 53.179706
iter  20 value 38.467166
iter  30 value 31.209554
iter  40 value 24.754735
iter  50 value 19.780818
iter  60 value 17.320425
iter  70 value 16.084196
iter  80 value 15.533309
iter  90 value 15.059915
iter 100 value 14.838474
final  value 14.838474 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=2.03; time: 0.0 min
[Tune-x] 10: size=12; decay=1.93e-05
# weights:  133
initial  value 3257.031661 
iter  10 value 41.399800
iter  20 value 22.438662
iter  30 value 11.645118
iter  40 value 6.023757
iter  50 value 2.031690
iter  60 value 0.582202
iter  70 value 0.247322
iter  80 value 0.123900
iter  90 value 0.065701
iter 100 value 0.058170
final  value 0.058170 
stopped after 100 iterations
# weights:  133
initial  value 2560.555691 
iter  10 value 48.485030
iter  20 value 32.361200
iter  30 value 14.681916
iter  40 value 9.647921
iter  50 value 5.856188
iter  60 value 3.897454
iter  70 value 2.643401
iter  80 value 1.296103
iter  90 value 0.733553
iter 100 value 0.369938
final  value 0.369938 
stopped after 100 iterations
# weights:  133
initial  value 4989.402628 
iter  10 value 55.422650
iter  20 value 33.476210
iter  30 value 21.057332
iter  40 value 13.398246
iter  50 value 7.445513
iter  60 value 4.023727
iter  70 value 1.991851
iter  80 value 1.255813
iter  90 value 0.970153
iter 100 value 0.778631
final  value 0.778631 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=   3; time: 0.0 min
[Tune-x] 11: size=19; decay=0.00109
# weights:  210
initial  value 3647.384846 
iter  10 value 36.115306
iter  20 value 26.611545
iter  30 value 16.334832
iter  40 value 9.789439
iter  50 value 6.559636
iter  60 value 4.478703
iter  70 value 2.723676
iter  80 value 2.189158
iter  90 value 1.870959
iter 100 value 1.652768
final  value 1.652768 
stopped after 100 iterations
# weights:  210
initial  value 1991.584303 
iter  10 value 68.467185
iter  20 value 43.786041
iter  30 value 26.292322
iter  40 value 15.902725
iter  50 value 11.781828
iter  60 value 8.356317
iter  70 value 4.495349
iter  80 value 3.258079
iter  90 value 2.735065
iter 100 value 2.369808
final  value 2.369808 
stopped after 100 iterations
# weights:  210
initial  value 4451.672033 
iter  10 value 51.295976
iter  20 value 36.598099
iter  30 value 21.121747
iter  40 value 11.344234
iter  50 value 7.409306
iter  60 value 4.830844
iter  70 value 3.529016
iter  80 value 2.634696
iter  90 value 2.313050
iter 100 value 2.041762
final  value 2.041762 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse= 2.5; time: 0.0 min
[Tune-x] 12: size=10; decay=5.13
# weights:  111
initial  value 3895.227718 
iter  10 value 218.995544
iter  20 value 212.867420
iter  30 value 212.355307
iter  40 value 212.227072
iter  50 value 212.203308
final  value 212.203277 
converged
# weights:  111
initial  value 4057.954089 
iter  10 value 241.983645
iter  20 value 213.858384
iter  30 value 212.723004
iter  40 value 212.557647
iter  50 value 212.515779
final  value 212.515695 
converged
# weights:  111
initial  value 3596.815797 
iter  10 value 232.407199
iter  20 value 228.935734
iter  30 value 227.790071
iter  40 value 227.155991
iter  50 value 227.053148
iter  60 value 227.051973
iter  60 value 227.051971
iter  60 value 227.051971
final  value 227.051971 
converged
[Tune-y] 12: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 13: size=20; decay=0.00449
# weights:  221
initial  value 3928.240664 
iter  10 value 41.912553
iter  20 value 25.075868
iter  30 value 14.119029
iter  40 value 10.217031
iter  50 value 7.813445
iter  60 value 6.623175
iter  70 value 5.836634
iter  80 value 5.287105
iter  90 value 5.045294
iter 100 value 4.763005
final  value 4.763005 
stopped after 100 iterations
# weights:  221
initial  value 3070.872686 
iter  10 value 47.422012
iter  20 value 39.047941
iter  30 value 25.060289
iter  40 value 17.421260
iter  50 value 13.072942
iter  60 value 9.259197
iter  70 value 7.921342
iter  80 value 7.287379
iter  90 value 6.950754
iter 100 value 6.740176
final  value 6.740176 
stopped after 100 iterations
# weights:  221
initial  value 3371.524422 
iter  10 value 47.750164
iter  20 value 37.341665
iter  30 value 24.394898
iter  40 value 17.171514
iter  50 value 12.166886
iter  60 value 9.425292
iter  70 value 7.761876
iter  80 value 7.089541
iter  90 value 6.625662
iter 100 value 6.314165
final  value 6.314165 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=2.22; time: 0.0 min
[Tune-x] 14: size=16; decay=0.0378
# weights:  177
initial  value 2913.122564 
iter  10 value 43.172100
iter  20 value 34.776496
iter  30 value 29.840272
iter  40 value 26.822345
iter  50 value 25.027157
iter  60 value 23.802333
iter  70 value 22.885949
iter  80 value 22.444472
iter  90 value 22.250164
iter 100 value 22.173059
final  value 22.173059 
stopped after 100 iterations
# weights:  177
initial  value 3924.537062 
iter  10 value 53.672779
iter  20 value 46.735304
iter  30 value 40.021835
iter  40 value 36.354204
iter  50 value 34.582348
iter  60 value 33.353056
iter  70 value 32.935330
iter  80 value 32.830915
iter  90 value 32.795519
iter 100 value 32.773255
final  value 32.773255 
stopped after 100 iterations
# weights:  177
initial  value 4291.941814 
iter  10 value 74.954320
iter  20 value 46.963120
iter  30 value 40.439294
iter  40 value 36.736915
iter  50 value 34.217855
iter  60 value 33.103797
iter  70 value 32.833191
iter  80 value 32.663011
iter  90 value 32.227493
iter 100 value 30.828181
final  value 30.828181 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 15: size=13; decay=0.000796
# weights:  144
initial  value 3280.915859 
iter  10 value 44.180701
iter  20 value 24.394600
iter  30 value 11.549990
iter  40 value 5.486103
iter  50 value 3.674493
iter  60 value 2.448722
iter  70 value 1.981241
iter  80 value 1.704435
iter  90 value 1.545000
iter 100 value 1.466974
final  value 1.466974 
stopped after 100 iterations
# weights:  144
initial  value 2953.395393 
iter  10 value 55.533876
iter  20 value 44.707695
iter  30 value 28.372463
iter  40 value 16.840474
iter  50 value 9.423454
iter  60 value 5.482240
iter  70 value 3.216720
iter  80 value 2.316212
iter  90 value 1.906277
iter 100 value 1.714860
final  value 1.714860 
stopped after 100 iterations
# weights:  144
initial  value 3336.719274 
iter  10 value 54.084604
iter  20 value 36.301203
iter  30 value 22.459534
iter  40 value 12.184354
iter  50 value 7.906468
iter  60 value 5.111911
iter  70 value 3.756389
iter  80 value 2.704334
iter  90 value 2.110535
iter 100 value 1.739697
final  value 1.739697 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=2.45; time: 0.0 min
[Tune-x] 16: size=8; decay=1.11e-05
# weights:  89
initial  value 3959.534422 
iter  10 value 55.537929
iter  20 value 24.034413
iter  30 value 10.066600
iter  40 value 5.573846
iter  50 value 3.265983
iter  60 value 2.193735
iter  70 value 1.343175
iter  80 value 0.852536
iter  90 value 0.520572
iter 100 value 0.318732
final  value 0.318732 
stopped after 100 iterations
# weights:  89
initial  value 4418.968782 
iter  10 value 59.303977
iter  20 value 40.677199
iter  30 value 24.437051
iter  40 value 14.174786
iter  50 value 10.173727
iter  60 value 8.558370
iter  70 value 6.143809
iter  80 value 4.767647
iter  90 value 3.736104
iter 100 value 2.975574
final  value 2.975574 
stopped after 100 iterations
# weights:  89
initial  value 4143.033212 
iter  10 value 59.393276
iter  20 value 37.979117
iter  30 value 28.790166
iter  40 value 15.991506
iter  50 value 8.973751
iter  60 value 4.065751
iter  70 value 1.809193
iter  80 value 0.923642
iter  90 value 0.414000
iter 100 value 0.184853
final  value 0.184853 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=3.67; time: 0.0 min
[Tune-x] 17: size=10; decay=1.05e-05
# weights:  111
initial  value 3720.701576 
iter  10 value 64.797872
iter  20 value 24.352221
iter  30 value 11.225933
iter  40 value 4.592740
iter  50 value 1.209145
iter  60 value 0.435431
iter  70 value 0.127513
iter  80 value 0.049985
iter  90 value 0.036666
iter 100 value 0.033949
final  value 0.033949 
stopped after 100 iterations
# weights:  111
initial  value 3577.577461 
iter  10 value 98.648973
iter  20 value 45.331362
iter  30 value 29.485982
iter  40 value 17.859726
iter  50 value 11.226879
iter  60 value 5.985576
iter  70 value 4.009271
iter  80 value 2.492521
iter  90 value 0.994613
iter 100 value 0.491907
final  value 0.491907 
stopped after 100 iterations
# weights:  111
initial  value 4138.444462 
iter  10 value 75.200650
iter  20 value 40.188314
iter  30 value 23.484974
iter  40 value 13.107373
iter  50 value 8.624921
iter  60 value 5.305411
iter  70 value 2.741537
iter  80 value 1.840261
iter  90 value 1.573865
iter 100 value 1.411235
final  value 1.411235 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=3.43; time: 0.0 min
[Tune-x] 18: size=1; decay=2.71
# weights:  12
initial  value 3566.333639 
iter  10 value 268.823696
iter  20 value 253.625625
final  value 253.581455 
converged
# weights:  12
initial  value 3369.788292 
iter  10 value 250.801094
iter  20 value 242.418397
final  value 242.412944 
converged
# weights:  12
initial  value 3372.304120 
iter  10 value 264.403418
iter  20 value 262.561108
final  value 262.560514 
converged
[Tune-y] 18: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 19: size=11; decay=0.00668
# weights:  122
initial  value 3864.694455 
iter  10 value 52.410060
iter  20 value 26.264297
iter  30 value 19.314116
iter  40 value 14.637686
iter  50 value 11.605193
iter  60 value 10.033979
iter  70 value 9.096338
iter  80 value 8.403243
iter  90 value 7.873454
iter 100 value 7.490489
final  value 7.490489 
stopped after 100 iterations
# weights:  122
initial  value 4894.794608 
iter  10 value 181.846928
iter  20 value 153.708082
iter  30 value 56.575595
iter  40 value 45.100227
iter  50 value 38.909382
iter  60 value 36.641048
iter  70 value 32.597077
iter  80 value 26.944746
iter  90 value 21.553934
iter 100 value 18.987789
final  value 18.987789 
stopped after 100 iterations
# weights:  122
initial  value 3863.767179 
iter  10 value 85.985112
iter  20 value 38.761600
iter  30 value 27.569088
iter  40 value 22.280746
iter  50 value 18.795231
iter  60 value 16.508209
iter  70 value 15.422713
iter  80 value 14.744277
iter  90 value 14.075143
iter 100 value 13.256901
final  value 13.256901 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=2.08; time: 0.0 min
[Tune-x] 20: size=16; decay=0.000435
# weights:  177
initial  value 3255.767163 
iter  10 value 37.748266
iter  20 value 22.319517
iter  30 value 12.427333
iter  40 value 7.380588
iter  50 value 4.820338
iter  60 value 2.610559
iter  70 value 1.616433
iter  80 value 1.120683
iter  90 value 0.944686
iter 100 value 0.867415
final  value 0.867415 
stopped after 100 iterations
# weights:  177
initial  value 3105.252762 
iter  10 value 176.551618
iter  20 value 81.401387
iter  30 value 43.133672
iter  40 value 29.745044
iter  50 value 24.849173
iter  60 value 16.092439
iter  70 value 10.118211
iter  80 value 6.902664
iter  90 value 4.045637
iter 100 value 2.395653
final  value 2.395653 
stopped after 100 iterations
# weights:  177
initial  value 3329.928448 
iter  10 value 54.601922
iter  20 value 36.982780
iter  30 value 24.245343
iter  40 value 14.551182
iter  50 value 6.932792
iter  60 value 2.514619
iter  70 value 1.309366
iter  80 value 1.038666
iter  90 value 0.925325
iter 100 value 0.798002
final  value 0.798002 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse= 2.3; time: 0.0 min
[Tune] Result: size=18; decay=0.142 : rmse.test.rmse= 1.1
# weights:  199
initial  value 6853.898931 
iter  10 value 112.282019
iter  20 value 82.509345
iter  30 value 78.923267
iter  40 value 77.525373
iter  50 value 77.160166
iter  60 value 76.813935
iter  70 value 76.557557
iter  80 value 76.322587
iter  90 value 76.170944
iter 100 value 76.101829
final  value 76.101829 
stopped after 100 iterations
[1] "Sat Jan 13 19:35:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 544 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 468
 number of selected nodes                               : 62 
[1] "Sat Jan 13 19:35:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Sat Jan 13 19:35:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Sat Jan 13 19:36:14 2018"
[Tune] Started tuning learner regr.randomForest for parameter set:
            Type len Def  Constr Req Tunable Trafo
nodesize integer   -   1 1 to 10   -    TRUE     -
mtry     integer   -   3  1 to 9   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nodesize=9; mtry=7
[Tune-y] 1: rmse.test.rmse=1.69; time: 0.0 min
[Tune-x] 2: nodesize=3; mtry=3
[Tune-y] 2: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 3: nodesize=1; mtry=3
[Tune-y] 3: rmse.test.rmse=1.66; time: 0.0 min
[Tune-x] 4: nodesize=9; mtry=3
[Tune-y] 4: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 5: nodesize=3; mtry=8
[Tune-y] 5: rmse.test.rmse=1.69; time: 0.0 min
[Tune-x] 6: nodesize=10; mtry=9
[Tune-y] 6: rmse.test.rmse=1.71; time: 0.0 min
[Tune-x] 7: nodesize=5; mtry=4
[Tune-y] 7: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 8: nodesize=4; mtry=4
[Tune-y] 8: rmse.test.rmse=1.66; time: 0.0 min
[Tune-x] 9: nodesize=9; mtry=5
[Tune-y] 9: rmse.test.rmse=1.69; time: 0.0 min
[Tune-x] 10: nodesize=6; mtry=1
[Tune-y] 10: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 11: nodesize=10; mtry=4
[Tune-y] 11: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 12: nodesize=5; mtry=9
[Tune-y] 12: rmse.test.rmse=1.71; time: 0.0 min
[Tune-x] 13: nodesize=10; mtry=4
[Tune-y] 13: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 14: nodesize=8; mtry=6
[Tune-y] 14: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 15: nodesize=7; mtry=3
[Tune-y] 15: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 16: nodesize=4; mtry=1
[Tune-y] 16: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 17: nodesize=5; mtry=1
[Tune-y] 17: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 18: nodesize=1; mtry=9
[Tune-y] 18: rmse.test.rmse=1.71; time: 0.0 min
[Tune-x] 19: nodesize=6; mtry=5
[Tune-y] 19: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 20: nodesize=8; mtry=3
[Tune-y] 20: rmse.test.rmse=1.66; time: 0.0 min
[Tune] Result: nodesize=5; mtry=4 : rmse.test.rmse=1.65
[1] "Sat Jan 13 19:36:27 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Sat Jan 13 19:36:33 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=9; min.node.size=7
[Tune-y] 1: rmse.test.rmse= 1.7; time: 0.0 min
[Tune-x] 2: mtry=3; min.node.size=3
[Tune-y] 2: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 3: mtry=1; min.node.size=3
[Tune-y] 3: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 4: mtry=8; min.node.size=3
[Tune-y] 4: rmse.test.rmse=1.71; time: 0.0 min
[Tune-x] 5: mtry=3; min.node.size=9
[Tune-y] 5: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 6: mtry=9; min.node.size=10
[Tune-y] 6: rmse.test.rmse=1.71; time: 0.0 min
[Tune-x] 7: mtry=5; min.node.size=4
[Tune-y] 7: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 8: mtry=3; min.node.size=5
[Tune-y] 8: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 9: mtry=8; min.node.size=6
[Tune-y] 9: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 10: mtry=5; min.node.size=1
[Tune-y] 10: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 11: mtry=9; min.node.size=4
[Tune-y] 11: rmse.test.rmse=1.69; time: 0.0 min
[Tune-x] 12: mtry=5; min.node.size=10
[Tune-y] 12: rmse.test.rmse=1.69; time: 0.0 min
[Tune-x] 13: mtry=9; min.node.size=5
[Tune-y] 13: rmse.test.rmse=1.69; time: 0.0 min
[Tune-x] 14: mtry=8; min.node.size=6
[Tune-y] 14: rmse.test.rmse=1.69; time: 0.0 min
[Tune-x] 15: mtry=6; min.node.size=4
[Tune-y] 15: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 16: mtry=4; min.node.size=1
[Tune-y] 16: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 17: mtry=5; min.node.size=1
[Tune-y] 17: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 18: mtry=1; min.node.size=10
[Tune-y] 18: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 19: mtry=5; min.node.size=5
[Tune-y] 19: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 20: mtry=8; min.node.size=3
[Tune-y] 20: rmse.test.rmse=1.69; time: 0.0 min
[Tune] Result: mtry=5; min.node.size=4 : rmse.test.rmse=1.67
[1] "Sat Jan 13 19:36:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Sat Jan 13 19:36:55 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.465; maxdepth=22; minbucket=15; minsplit=17
[Tune-y] 1: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 2: cp=0.00159; maxdepth=9; minbucket=45; minsplit=17
[Tune-y] 2: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 3: cp=0.00479; maxdepth=26; minbucket=46; minsplit=48
[Tune-y] 3: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 4: cp=0.0228; maxdepth=13; minbucket=19; minsplit=23
[Tune-y] 4: rmse.test.rmse=1.88; time: 0.0 min
[Tune-x] 5: cp=0.392; maxdepth=17; minbucket=30; minsplit=7
[Tune-y] 5: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 6: cp=0.522; maxdepth=12; minbucket=27; minsplit=48
[Tune-y] 6: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 7: cp=0.769; maxdepth=15; minbucket=40; minsplit=32
[Tune-y] 7: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 8: cp=0.0717; maxdepth=11; minbucket=21; minsplit=5
[Tune-y] 8: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 9: cp=0.0288; maxdepth=3; minbucket=6; minsplit=46
[Tune-y] 9: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 10: cp=0.0313; maxdepth=16; minbucket=41; minsplit=17
[Tune-y] 10: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 11: cp=0.225; maxdepth=11; minbucket=49; minsplit=48
[Tune-y] 11: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 12: cp=0.00669; maxdepth=25; minbucket=5; minsplit=31
[Tune-y] 12: rmse.test.rmse=2.09; time: 0.0 min
[Tune-x] 13: cp=0.511; maxdepth=22; minbucket=46; minsplit=31
[Tune-y] 13: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 14: cp=0.00169; maxdepth=25; minbucket=9; minsplit=17
[Tune-y] 14: rmse.test.rmse=1.95; time: 0.0 min
[Tune-x] 15: cp=0.00534; maxdepth=29; minbucket=39; minsplit=40
[Tune-y] 15: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 16: cp=0.0012; maxdepth=19; minbucket=45; minsplit=36
[Tune-y] 16: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 17: cp=0.181; maxdepth=25; minbucket=18; minsplit=49
[Tune-y] 17: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 18: cp=0.00837; maxdepth=15; minbucket=15; minsplit=23
[Tune-y] 18: rmse.test.rmse=1.85; time: 0.0 min
[Tune-x] 19: cp=0.0167; maxdepth=13; minbucket=37; minsplit=28
[Tune-y] 19: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 20: cp=0.269; maxdepth=8; minbucket=50; minsplit=41
[Tune-y] 20: rmse.test.rmse=1.92; time: 0.0 min
[Tune] Result: cp=0.00837; maxdepth=15; minbucket=15; minsplit=23 : rmse.test.rmse=1.85
[1] "Sat Jan 13 19:37:03 2018"
[Tune] Started tuning learner regr.RRF for parameter set:
           Type len Def Constr Req Tunable Trafo
mtry    integer   -   3 1 to 9   -    TRUE     -
coefReg numeric   - 0.8 0 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=9; coefReg=0.692
[Tune-y] 1: rmse.test.rmse=1.69; time: 0.0 min
[Tune-x] 2: mtry=3; coefReg=0.264
[Tune-y] 2: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 3: mtry=1; coefReg=0.239
[Tune-y] 3: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 4: mtry=8; coefReg=0.272
[Tune-y] 4: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 5: mtry=3; coefReg=0.822
[Tune-y] 5: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 6: mtry=9; coefReg=0.951
[Tune-y] 6: rmse.test.rmse= 1.7; time: 0.0 min
[Tune-x] 7: mtry=5; coefReg=0.358
[Tune-y] 7: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 8: mtry=3; coefReg=0.405
[Tune-y] 8: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 9: mtry=8; coefReg=0.525
[Tune-y] 9: rmse.test.rmse= 1.7; time: 0.0 min
[Tune-x] 10: mtry=5; coefReg=0.0477
[Tune-y] 10: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 11: mtry=9; coefReg=0.34
[Tune-y] 11: rmse.test.rmse=1.71; time: 0.0 min
[Tune-x] 12: mtry=5; coefReg=0.952
[Tune-y] 12: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 13: mtry=9; coefReg=0.442
[Tune-y] 13: rmse.test.rmse= 1.7; time: 0.0 min
[Tune-x] 14: mtry=8; coefReg=0.596
[Tune-y] 14: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 15: mtry=6; coefReg=0.317
[Tune-y] 15: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 16: mtry=4; coefReg=0.00778
[Tune-y] 16: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 17: mtry=5; coefReg=0.0033
[Tune-y] 17: rmse.test.rmse=1.64; time: 0.0 min
[Tune-x] 18: mtry=1; coefReg=0.906
[Tune-y] 18: rmse.test.rmse=1.74; time: 0.0 min
[Tune-x] 19: mtry=5; coefReg=0.471
[Tune-y] 19: rmse.test.rmse=1.67; time: 0.0 min
[Tune-x] 20: mtry=8; coefReg=0.273
[Tune-y] 20: rmse.test.rmse=1.73; time: 0.0 min
[Tune] Result: mtry=5; coefReg=0.0033 : rmse.test.rmse=1.64
[1] "Sat Jan 13 19:37:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Sat Jan 13 19:37:23 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Sat Jan 13 19:37:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.471 0.365 0.283 0.219 0.170
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 1 -----> 4 
Runtime: 0.01200104 secs 

 Values of predicted responses: 
   index             3 
   lambda        0.283 
    Y 1          8.499 
    Y 2          7.513 
    Y 3          9.026 
    Y 4          8.823 
    Y 5           8.38 
[1] "Sat Jan 13 19:37:36 2018"
[Tune] Started tuning learner regr.svm for parameter set:
         Type len   Def    Constr Req Tunable Trafo
cost  numeric   -     0 -15 to 15   -    TRUE     Y
gamma numeric   - -3.17 -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cost=3.3e+03; gamma=54
[Tune-y] 1: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 2: cost=0.00326; gamma=0.00732
[Tune-y] 2: rmse.test.rmse=1.91; time: 0.0 min
[Tune-x] 3: cost=0.000132; gamma=0.00436
[Tune-y] 3: rmse.test.rmse=1.91; time: 0.0 min
[Tune-x] 4: cost=2.3e+03; gamma=0.00878
[Tune-y] 4: rmse.test.rmse=2.72; time: 0.0 min
[Tune-x] 5: cost=0.0036; gamma=814
[Tune-y] 5: rmse.test.rmse=1.91; time: 0.0 min
[Tune-x] 6: cost=4.11e+03; gamma=1.19e+04
[Tune-y] 6: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 7: cost=0.388; gamma=0.052
[Tune-y] 7: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 8: cost=0.0264; gamma=0.14
[Tune-y] 8: rmse.test.rmse=1.89; time: 0.0 min
[Tune-x] 9: cost=1.97e+03; gamma=1.7
[Tune-y] 9: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 10: cost=2.97; gamma=8.24e-05
[Tune-y] 10: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 11: cost=4.65e+03; gamma=0.0357
[Tune-y] 11: rmse.test.rmse=2.12; time: 0.0 min
[Tune-x] 12: cost=0.856; gamma=1.2e+04
[Tune-y] 12: rmse.test.rmse=1.91; time: 0.0 min
[Tune-x] 13: cost=1.49e+04; gamma=0.3
[Tune-y] 13: rmse.test.rmse=1.78; time: 0.0 min
[Tune-x] 14: cost=353; gamma=7.39
[Tune-y] 14: rmse.test.rmse=1.92; time: 0.0 min
[Tune-x] 15: cost=12.1; gamma=0.0222
[Tune-y] 15: rmse.test.rmse=1.33; time: 0.0 min
[Tune-x] 16: cost=0.0492; gamma=3.59e-05
[Tune-y] 16: rmse.test.rmse=1.91; time: 0.0 min
[Tune-x] 17: cost=0.782; gamma=3.27e-05
[Tune-y] 17: rmse.test.rmse=1.91; time: 0.0 min
[Tune-x] 18: cost=5.84e-05; gamma=4.6e+03
[Tune-y] 18: rmse.test.rmse=1.91; time: 0.0 min
[Tune-x] 19: cost=1.01; gamma=0.545
[Tune-y] 19: rmse.test.rmse=1.85; time: 0.0 min
[Tune-x] 20: cost=485; gamma=0.00893
[Tune-y] 20: rmse.test.rmse=1.96; time: 0.0 min
[Tune] Result: cost=12.1; gamma=0.0222 : rmse.test.rmse=1.33
[1] "Sat Jan 13 19:37:45 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=2.06e+03; max_depth=7; eta=0.136; gamma=2.64; colsample_bytree=0.328; min_child_weight=4.77; subsample=0.904
[Tune-y] 1: rmse.test.rmse=1.68; time: 0.4 min
[Tune-x] 2: nrounds=51; max_depth=3; eta=0.494; gamma=9; colsample_bytree=0.681; min_child_weight=9.09; subsample=0.518
[Tune-y] 2: rmse.test.rmse=1.84; time: 0.0 min
[Tune-x] 3: nrounds=70; max_depth=5; eta=0.519; gamma=5.25; colsample_bytree=0.521; min_child_weight=0.955; subsample=0.93
[Tune-y] 3: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 4: nrounds=77; max_depth=5; eta=0.571; gamma=9.62; colsample_bytree=0.477; min_child_weight=15.6; subsample=0.697
[Tune-y] 4: rmse.test.rmse=1.91; time: 0.0 min
[Tune-x] 5: nrounds=410; max_depth=4; eta=0.214; gamma=0.0778; colsample_bytree=0.495; min_child_weight=0.066; subsample=0.273
[Tune-y] 5: rmse.test.rmse=1.81; time: 0.1 min
[Tune-x] 6: nrounds=2.27e+03; max_depth=6; eta=0.283; gamma=7.97; colsample_bytree=0.409; min_child_weight=15.7; subsample=0.466
[Tune-y] 6: rmse.test.rmse= 1.9; time: 0.1 min
[Tune-x] 7: nrounds=3.2e+03; max_depth=10; eta=0.167; gamma=7.96; colsample_bytree=0.306; min_child_weight=11.6; subsample=0.927
[Tune-y] 7: rmse.test.rmse=1.67; time: 0.4 min
[Tune-x] 8: nrounds=707; max_depth=9; eta=0.352; gamma=0.791; colsample_bytree=0.615; min_child_weight=1.88; subsample=0.458
[Tune-y] 8: rmse.test.rmse=1.81; time: 0.2 min
[Tune-x] 9: nrounds=43; max_depth=10; eta=0.447; gamma=7.63; colsample_bytree=0.312; min_child_weight=11.5; subsample=0.914
[Tune-y] 9: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 10: nrounds=585; max_depth=8; eta=0.483; gamma=2.9; colsample_bytree=0.684; min_child_weight=6.2; subsample=0.573
[Tune-y] 10: rmse.test.rmse=1.74; time: 0.1 min
[Tune-x] 11: nrounds=41; max_depth=5; eta=0.247; gamma=3.76; colsample_bytree=0.585; min_child_weight=10.2; subsample=0.858
[Tune-y] 11: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 12: nrounds=31; max_depth=10; eta=0.47; gamma=2; colsample_bytree=0.68; min_child_weight=6.52; subsample=0.28
[Tune-y] 12: rmse.test.rmse= 1.9; time: 0.0 min
[Tune-x] 13: nrounds=117; max_depth=1; eta=0.249; gamma=9.84; colsample_bytree=0.529; min_child_weight=11.4; subsample=0.743
[Tune-y] 13: rmse.test.rmse=1.69; time: 0.0 min
[Tune-x] 14: nrounds=11; max_depth=7; eta=0.599; gamma=8.77; colsample_bytree=0.622; min_child_weight=4.79; subsample=0.452
[Tune-y] 14: rmse.test.rmse=1.75; time: 0.0 min
[Tune-x] 15: nrounds=1.98e+03; max_depth=2; eta=0.277; gamma=5.34; colsample_bytree=0.43; min_child_weight=8.27; subsample=0.294
[Tune-y] 15: rmse.test.rmse=1.81; time: 0.1 min
[Tune-x] 16: nrounds=2.55e+03; max_depth=5; eta=0.469; gamma=2.4; colsample_bytree=0.451; min_child_weight=17.3; subsample=0.533
[Tune-y] 16: rmse.test.rmse=1.93; time: 0.2 min
[Tune-x] 17: nrounds=45; max_depth=7; eta=0.0855; gamma=2.91; colsample_bytree=0.404; min_child_weight=1.67; subsample=0.866
[Tune-y] 17: rmse.test.rmse= 1.7; time: 0.0 min
[Tune-x] 18: nrounds=641; max_depth=8; eta=0.165; gamma=6.05; colsample_bytree=0.462; min_child_weight=12.6; subsample=0.42
[Tune-y] 18: rmse.test.rmse=1.86; time: 0.0 min
[Tune-x] 19: nrounds=255; max_depth=5; eta=0.157; gamma=8.08; colsample_bytree=0.378; min_child_weight=10; subsample=0.288
[Tune-y] 19: rmse.test.rmse=1.94; time: 0.0 min
[Tune-x] 20: nrounds=501; max_depth=2; eta=0.52; gamma=6.86; colsample_bytree=0.603; min_child_weight=5.33; subsample=0.569
[Tune-y] 20: rmse.test.rmse=1.71; time: 0.1 min
[Tune] Result: nrounds=41; max_depth=5; eta=0.247; gamma=3.76; colsample_bytree=0.585; min_child_weight=10.2; subsample=0.858 : rmse.test.rmse=1.59
[1] "Sat Jan 13 19:39:41 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Sat Jan 13 19:39:47 2018"
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  No variation for for: V11, V12
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  No variation for for: V11, V12
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [2,0.663265], n=(50,26)
**PRUNE** @depth 0: [2,0.663265]
**GROW** @depth 0: [4,0.540404], n=(51,25)
**PRUNE** @depth 0: [4,0.414141]
**GROW** @depth 0: [2,0.816327], n=(64,12)
r=1000 d=[0] [0]; n=(60,16)
**GROW** @depth 1: [1,0.292929], n=(21,39)
**PRUNE** @depth 1: [1,0.282828]
**GROW** @depth 1: [3,0.510101], n=(32,21)
**PRUNE** @depth 1: [3,0.510101]
r=2000 d=[0] [0]; n=(53,23)

Sampling @ nn=0 pred locs:
**GROW** @depth 1: [1,0.353535], n=(25,39)
r=1000 d=[0] [0] [0]; mh=3 n=(24,40,12)
**PRUNE** @depth 1: [1,0.323232]
**GROW** @depth 1: [1,0.333333], n=(25,37)
**PRUNE** @depth 1: [1,0.353535]
**PRUNE** @depth 0: [2,0.77551]
**GROW** @depth 0: [2,0.673469], n=(51,25)
r=2000 d=[0] [0]; mh=3 n=(56,20)
**GROW** @depth 1: [5,0.646465], n=(33,21)
**PRUNE** @depth 1: [5,0.636364]
**PRUNE** @depth 0: [2,0.704082]
**GROW** @depth 0: [1,0.242424], n=(21,55)
**PRUNE** @depth 0: [1,0.272727]
**GROW** @depth 0: [3,0.510101], n=(45,31)
**PRUNE** @depth 0: [3,0.510101]
**GROW** @depth 0: [3,0.510101], n=(45,31)
**PRUNE** @depth 0: [3,0.510101]
**GROW** @depth 0: [3,0.808081], n=(60,16)
r=3000 d=[0] [0]; mh=3 n=(59,17)
r=4000 d=[0] [0]; mh=3 n=(60,16)
**PRUNE** @depth 0: [3,0.848485]
**GROW** @depth 0: [6,0.181818], n=(12,64)
**GROW** @depth 1: [2,0.77551], n=(50,14)
**PRUNE** @depth 1: [2,0.765306]
**PRUNE** @depth 0: [6,0.181818]
**GROW** @depth 0: [2,0.423469], n=(45,31)
**PRUNE** @depth 0: [2,0.607143]
**GROW** @depth 0: [2,0.714286], n=(54,22)
r=5000 d=[0] [0]; mh=3 n=(62,14)
Grow: 4.658%, Prune: 5.993%, Change: 64.61%, Swap: 8.333%

[1] "Sat Jan 13 19:43:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Sat Jan 13 19:43:56 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=76

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=76
r=2000 d=[0]; mh=1 n=76
r=3000 d=[0]; mh=1 n=76

[1] "Sat Jan 13 19:44:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7064135 
gamma= 13.5027 	 alpha= 3.3009 	 beta= 6.4088 
[1] "Sat Jan 13 19:44:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Sat Jan 13 19:44:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=76
r=2000 d=[0]; n=76

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=76
r=2000 d=[0]; mh=1 n=76
r=3000 d=[0]; mh=1 n=76
r=4000 d=[0]; mh=1 n=76
r=5000 d=[0]; mh=1 n=76
Grow: 0%, 

[1] "Sat Jan 13 19:44:24 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cforest no default is available.
[1] "Sat Jan 13 19:44:30 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Sat Jan 13 19:44:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Sat Jan 13 19:44:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Sat Jan 13 19:44:51 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Sat Jan 13 19:44:57 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Sat Jan 13 19:45:05 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
[Tune] Started tuning learner regr.extraTrees for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
numRandomCuts integer   -   1 1 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=9; numRandomCuts=9
[Tune-y] 1: rmse.test.rmse= 1.4; time: 0.0 min
[Tune-x] 2: mtry=6; numRandomCuts=3
[Tune-y] 2: rmse.test.rmse=1.35; time: 0.0 min
[Tune-x] 3: mtry=1; numRandomCuts=1
[Tune-y] 3: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 4: mtry=4; numRandomCuts=14
[Tune-y] 4: rmse.test.rmse=1.38; time: 0.0 min
[Tune-x] 5: mtry=6; numRandomCuts=16
[Tune-y] 5: rmse.test.rmse=1.39; time: 0.0 min
[Tune-x] 6: mtry=7; numRandomCuts=15
[Tune-y] 6: rmse.test.rmse= 1.4; time: 0.0 min
[Tune-x] 7: mtry=1; numRandomCuts=5
[Tune-y] 7: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 8: mtry=2; numRandomCuts=25
[Tune-y] 8: rmse.test.rmse=1.38; time: 0.0 min
[Tune-x] 9: mtry=6; numRandomCuts=24
[Tune-y] 9: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 10: mtry=3; numRandomCuts=6
[Tune-y] 10: rmse.test.rmse=1.37; time: 0.0 min
[Tune-x] 11: mtry=9; numRandomCuts=2
[Tune-y] 11: rmse.test.rmse=1.36; time: 0.0 min
[Tune-x] 12: mtry=2; numRandomCuts=2
[Tune-y] 12: rmse.test.rmse=1.44; time: 0.0 min
[Tune-x] 13: mtry=8; numRandomCuts=7
[Tune-y] 13: rmse.test.rmse=1.38; time: 0.0 min
[Tune-x] 14: mtry=2; numRandomCuts=7
[Tune-y] 14: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 15: mtry=1; numRandomCuts=19
[Tune-y] 15: rmse.test.rmse=1.47; time: 0.0 min
[Tune-x] 16: mtry=9; numRandomCuts=8
[Tune-y] 16: rmse.test.rmse= 1.4; time: 0.0 min
[Tune-x] 17: mtry=9; numRandomCuts=16
[Tune-y] 17: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 18: mtry=7; numRandomCuts=5
[Tune-y] 18: rmse.test.rmse=1.35; time: 0.0 min
[Tune-x] 19: mtry=4; numRandomCuts=9
[Tune-y] 19: rmse.test.rmse=1.36; time: 0.0 min
[Tune-x] 20: mtry=8; numRandomCuts=20
[Tune-y] 20: rmse.test.rmse=1.42; time: 0.0 min
[Tune] Result: mtry=6; numRandomCuts=3 : rmse.test.rmse=1.35
[1] "Sat Jan 13 19:45:36 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Sat Jan 13 19:45:42 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Sat Jan 13 19:45:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.frbs no default is available.
[1] "Sat Jan 13 19:45:55 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
In addition: Warning messages:
1: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
2: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
3: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
4: In validate.params(object, newdata) :
  There are your newdata which are out of the specified range
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Sat Jan 13 19:46:04 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=870; interaction.depth=4; shrinkage=0.395; n.minobsinnode=6
[Tune-y] 1: rmse.test.rmse=1.59; time: 0.0 min
[Tune-x] 2: n.trees=14; interaction.depth=1; shrinkage=0.222; n.minobsinnode=16
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 2: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 3: n.trees=150; interaction.depth=7; shrinkage=0.462; n.minobsinnode=16
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: n.trees=14; interaction.depth=2; shrinkage=0.0762; n.minobsinnode=25
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: n.trees=133; interaction.depth=10; shrinkage=0.193; n.minobsinnode=9
[Tune-y] 5: rmse.test.rmse=1.44; time: 0.0 min
[Tune-x] 6: n.trees=624; interaction.depth=1; shrinkage=0.106; n.minobsinnode=6
[Tune-y] 6: rmse.test.rmse=1.48; time: 0.0 min
[Tune-x] 7: n.trees=379; interaction.depth=3; shrinkage=0.0711; n.minobsinnode=10
[Tune-y] 7: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 8: n.trees=14; interaction.depth=8; shrinkage=0.591; n.minobsinnode=11
[Tune-y] 8: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 9: n.trees=652; interaction.depth=7; shrinkage=0.458; n.minobsinnode=8
[Tune-y] 9: rmse.test.rmse=1.73; time: 0.0 min
[Tune-x] 10: n.trees=60; interaction.depth=4; shrinkage=0.512; n.minobsinnode=21
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: n.trees=51; interaction.depth=1; shrinkage=0.186; n.minobsinnode=21
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 11: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 12: n.trees=530; interaction.depth=7; shrinkage=0.587; n.minobsinnode=14
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: n.trees=28; interaction.depth=4; shrinkage=0.174; n.minobsinnode=8
[Tune-y] 13: rmse.test.rmse=1.39; time: 0.0 min
[Tune-x] 14: n.trees=132; interaction.depth=7; shrinkage=0.382; n.minobsinnode=5
[Tune-y] 14: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 15: n.trees=230; interaction.depth=5; shrinkage=0.118; n.minobsinnode=22
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: n.trees=45; interaction.depth=2; shrinkage=0.454; n.minobsinnode=15
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 16: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 17: n.trees=383; interaction.depth=1; shrinkage=0.0612; n.minobsinnode=20
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 17: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 18: n.trees=35; interaction.depth=8; shrinkage=0.539; n.minobsinnode=10
[Tune-y] 18: rmse.test.rmse=1.49; time: 0.0 min
[Tune-x] 19: n.trees=24; interaction.depth=9; shrinkage=0.568; n.minobsinnode=21
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: n.trees=16; interaction.depth=4; shrinkage=0.444; n.minobsinnode=15
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.gbm: Error in gbm.fit(x, y, offset = offset, distribution = distribution, w = w,  : 
  The dataset size is too small or subsampling rate is too large: nTrain*bag.fraction <= n.minobsinnode

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: n.trees=28; interaction.depth=4; shrinkage=0.174; n.minobsinnode=8 : rmse.test.rmse=1.39
[1] "Sat Jan 13 19:46:12 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Sat Jan 13 19:46:18 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.97; lambda=0.0244
[Tune-y] 1: rmse.test.rmse=1.22; time: 0.0 min
[Tune-x] 2: alpha=0.657; lambda=0.00205
[Tune-y] 2: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 3: alpha=0.0737; lambda=0.00121
[Tune-y] 3: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 4: alpha=0.369; lambda=0.13
[Tune-y] 4: rmse.test.rmse=1.22; time: 0.0 min
[Tune-x] 5: alpha=0.588; lambda=0.278
[Tune-y] 5: rmse.test.rmse= 1.3; time: 0.0 min
[Tune-x] 6: alpha=0.769; lambda=0.162
[Tune-y] 6: rmse.test.rmse=1.25; time: 0.0 min
[Tune-x] 7: alpha=0.0706; lambda=0.00467
[Tune-y] 7: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 8: alpha=0.125; lambda=5.71
[Tune-y] 8: rmse.test.rmse=1.81; time: 0.0 min
[Tune-x] 9: alpha=0.562; lambda=4.7
[Tune-y] 9: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 10: alpha=0.32; lambda=0.00649
[Tune-y] 10: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 11: alpha=0.898; lambda=0.00162
[Tune-y] 11: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 12: alpha=0.176; lambda=0.00178
[Tune-y] 12: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 13: alpha=0.789; lambda=0.0105
[Tune-y] 13: rmse.test.rmse=1.23; time: 0.0 min
[Tune-x] 14: alpha=0.117; lambda=0.0109
[Tune-y] 14: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 15: alpha=0.0686; lambda=0.668
[Tune-y] 15: rmse.test.rmse=1.32; time: 0.0 min
[Tune-x] 16: alpha=0.985; lambda=0.0163
[Tune-y] 16: rmse.test.rmse=1.23; time: 0.0 min
[Tune-x] 17: alpha=0.907; lambda=0.301
[Tune-y] 17: rmse.test.rmse=1.37; time: 0.0 min
[Tune-x] 18: alpha=0.763; lambda=0.00514
[Tune-y] 18: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 19: alpha=0.39; lambda=0.0245
[Tune-y] 19: rmse.test.rmse=1.23; time: 0.0 min
[Tune-x] 20: alpha=0.853; lambda=1.3
[Tune-y] 20: rmse.test.rmse=1.82; time: 0.0 min
[Tune] Result: alpha=0.369; lambda=0.13 : rmse.test.rmse=1.22
[1] "Sat Jan 13 19:46:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:46:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===========================                                           |  38%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:46:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:46:57 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=============================                                         |  42%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Sat Jan 13 19:47:06 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  2 1.979798 2 2 2 2 1.959596 2 1.959596 
  - best initial criterion value(s) :  -144.7468 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       144.75  |proj g|=       1.8306
At iterate     1  f =       144.65  |proj g|=        1.1498
At iterate     2  f =       144.51  |proj g|=        1.1437
At iterate     3  f =       144.16  |proj g|=        1.8318
At iterate     4  f =       143.66  |proj g|=        1.8474
At iterate     5  f =       142.96  |proj g|=        1.8563
At iterate     6  f =       142.34  |proj g|=        1.8505
At iterate     7  f =       142.03  |proj g|=       0.63098
At iterate     8  f =       141.99  |proj g|=        1.0682
At iterate     9  f =       141.92  |proj g|=        1.8395
At iterate    10  f =       141.88  |proj g|=        0.6705
At iterate    11  f =       141.84  |proj g|=       0.69885
At iterate    12  f =       141.81  |proj g|=       0.66521
At iterate    13  f =       141.77  |proj g|=       0.26913
At iterate    14  f =       141.77  |proj g|=       0.26314
At iterate    15  f =       141.76  |proj g|=       0.21883
At iterate    16  f =       141.74  |proj g|=       0.17197
At iterate    17  f =       141.74  |proj g|=        1.8263
At iterate    18  f =       141.73  |proj g|=       0.33941
At iterate    19  f =       141.73  |proj g|=       0.33645
At iterate    20  f =       141.72  |proj g|=       0.34959
At iterate    21  f =       141.72  |proj g|=       0.29145
At iterate    22  f =       141.71  |proj g|=       0.17923
At iterate    23  f =       141.71  |proj g|=       0.16928
At iterate    24  f =       141.71  |proj g|=       0.15443
At iterate    25  f =       141.71  |proj g|=       0.17766
At iterate    26  f =       141.71  |proj g|=      0.097571
At iterate    27  f =       141.71  |proj g|=      0.091987
At iterate    28  f =       141.71  |proj g|=       0.15506
At iterate    29  f =       141.71  |proj g|=      0.045716
At iterate    30  f =       141.71  |proj g|=      0.040816
At iterate    31  f =       141.71  |proj g|=       0.18093
At iterate    32  f =       141.71  |proj g|=      0.015004
At iterate    33  f =       141.71  |proj g|=      0.013083
At iterate    34  f =       141.71  |proj g|=      0.012066
At iterate    35  f =       141.71  |proj g|=     0.0098402
At iterate    36  f =       141.71  |proj g|=      0.015046
At iterate    37  f =       141.71  |proj g|=      0.001871
At iterate    38  f =       141.71  |proj g|=     0.0056844

iterations 38
function evaluations 46
segments explored during Cauchy searches 44
BFGS updates skipped 0
active bounds at final generalized Cauchy point 2
norm of the final projected gradient 0.00568442
final function value 141.71

F = 141.71
final  value 141.709999 
converged
[1] "Sat Jan 13 19:47:24 2018"
[Tune] Started tuning learner regr.ksvm for parameter set:
         Type len  Def    Constr Req Tunable Trafo
C     numeric   -    0  -5 to 10   -    TRUE     Y
sigma numeric   - TRUE -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: C=67.9; sigma=1.24
[Tune-y] 1: rmse.test.rmse=1.79; time: 0.0 min
[Tune-x] 2: C=3.74; sigma=0.0434
[Tune-y] 2: rmse.test.rmse=1.44; time: 0.0 min
[Tune-x] 3: C=12.7; sigma=0.00995
[Tune-y] 3: rmse.test.rmse= 1.3; time: 0.0 min
[Tune-x] 4: C=38.2; sigma=1.47
[Tune-y] 4: rmse.test.rmse=1.79; time: 0.0 min
[Tune-x] 5: C=0.558; sigma=288
[Tune-y] 5: rmse.test.rmse= 1.8; time: 0.0 min
[Tune-x] 6: C=0.165; sigma=0.0707
[Tune-y] 6: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 7: C=0.0346; sigma=1.08e+03
[Tune-y] 7: rmse.test.rmse= 1.8; time: 0.0 min
[Tune-x] 8: C=76.2; sigma=0.0129
[Tune-y] 8: rmse.test.rmse=1.55; time: 0.0 min
[Tune-x] 9: C=431; sigma=0.611
[Tune-y] 9: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 10: C=38.9; sigma=8.34e-05
[Tune-y] 10: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 11: C=0.681; sigma=53.1
[Tune-y] 11: rmse.test.rmse= 1.8; time: 0.0 min
[Tune-x] 12: C=0.422; sigma=0.56
[Tune-y] 12: rmse.test.rmse=1.79; time: 0.0 min
[Tune-x] 13: C=711; sigma=3.76e+03
[Tune-y] 13: rmse.test.rmse=1.79; time: 0.0 min
[Tune-x] 14: C=0.888; sigma=1.06e+03
[Tune-y] 14: rmse.test.rmse= 1.8; time: 0.0 min
[Tune-x] 15: C=255; sigma=0.103
[Tune-y] 15: rmse.test.rmse= 1.7; time: 0.0 min
[Tune-x] 16: C=1.55; sigma=11.1
[Tune-y] 16: rmse.test.rmse=1.79; time: 0.0 min
[Tune-x] 17: C=18.8; sigma=0.0302
[Tune-y] 17: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 18: C=1.05; sigma=0.0541
[Tune-y] 18: rmse.test.rmse=1.41; time: 0.0 min
[Tune-x] 19: C=364; sigma=53.3
[Tune-y] 19: rmse.test.rmse=1.79; time: 0.0 min
[Tune-x] 20: C=23.6; sigma=0.0739
[Tune-y] 20: rmse.test.rmse=1.71; time: 0.0 min
[Tune] Result: C=12.7; sigma=0.00995 : rmse.test.rmse= 1.3
[1] "Sat Jan 13 19:47:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 24), d = 2.35068, its = 7
i = 2 (of 24), d = 2.93247, its = 7
i = 3 (of 24), d = 2.01728, its = 7
i = 4 (of 24), d = 1.53121, its = 6
i = 5 (of 24), d = 1.28401, its = 6
i = 6 (of 24), d = 1.79856, its = 7
i = 7 (of 24), d = 2.3367, its = 7
i = 8 (of 24), d = 1.53498, its = 6
i = 9 (of 24), d = 1.34932, its = 6
i = 10 (of 24), d = 2.31884, its = 7
i = 11 (of 24), d = 1.97304, its = 7
i = 12 (of 24), d = 2.36175, its = 7
i = 13 (of 24), d = 1.95607, its = 6
i = 14 (of 24), d = 1.85422, its = 6
i = 15 (of 24), d = 1.63347, its = 6
i = 16 (of 24), d = 1.95408, its = 7
i = 17 (of 24), d = 1.55952, its = 6
i = 18 (of 24), d = 1.75914, its = 7
i = 19 (of 24), d = 2.44079, its = 7
i = 20 (of 24), d = 1.53869, its = 6
i = 21 (of 24), d = 1.78582, its = 6
i = 22 (of 24), d = 1.50495, its = 6
i = 23 (of 24), d = 2.303, its = 7
i = 24 (of 24), d = 1.55638, its = 6
[1] "Sat Jan 13 19:47:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Sat Jan 13 19:47:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Sat Jan 13 19:47:51 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Sat Jan 13 19:47:57 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Sat Jan 13 19:48:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Sat Jan 13 19:48:09 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=20; decay=0.00139
# weights:  221
initial  value 2241.530248 
iter  10 value 59.374446
iter  20 value 36.132344
iter  30 value 11.499458
iter  40 value 5.213598
iter  50 value 3.165017
iter  60 value 2.212300
iter  70 value 1.816352
iter  80 value 1.518365
iter  90 value 1.407620
iter 100 value 1.356835
final  value 1.356835 
stopped after 100 iterations
# weights:  221
initial  value 4251.143480 
iter  10 value 51.348634
iter  20 value 25.374847
iter  30 value 13.379118
iter  40 value 4.830719
iter  50 value 2.548653
iter  60 value 1.967665
iter  70 value 1.547501
iter  80 value 1.353513
iter  90 value 1.231080
iter 100 value 1.192790
final  value 1.192790 
stopped after 100 iterations
# weights:  221
initial  value 3639.820246 
iter  10 value 191.875233
iter  20 value 139.521118
iter  30 value 51.126179
iter  40 value 38.664529
iter  50 value 32.958830
iter  60 value 29.630516
iter  70 value 20.502467
iter  80 value 9.894468
iter  90 value 5.582909
iter 100 value 4.099923
final  value 4.099923 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=2.43; time: 0.0 min
[Tune-x] 2: size=14; decay=3.13e-05
# weights:  155
initial  value 3384.409616 
iter  10 value 60.602742
iter  20 value 34.891465
iter  30 value 10.611144
iter  40 value 1.881944
iter  50 value 0.488647
iter  60 value 0.100897
iter  70 value 0.067878
iter  80 value 0.066336
iter  90 value 0.058558
iter 100 value 0.047824
final  value 0.047824 
stopped after 100 iterations
# weights:  155
initial  value 3578.355180 
iter  10 value 50.449654
iter  20 value 32.770033
iter  30 value 12.358207
iter  40 value 1.548843
iter  50 value 0.127172
iter  60 value 0.051131
iter  70 value 0.049930
iter  80 value 0.042450
iter  90 value 0.034858
iter 100 value 0.031231
final  value 0.031231 
stopped after 100 iterations
# weights:  155
initial  value 3966.542588 
iter  10 value 82.004040
iter  20 value 38.792686
iter  30 value 16.978142
iter  40 value 2.988854
iter  50 value 1.517499
iter  60 value 0.211533
iter  70 value 0.068703
iter  80 value 0.060841
iter  90 value 0.060018
iter 100 value 0.053452
final  value 0.053452 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=2.06; time: 0.0 min
[Tune-x] 3: size=2; decay=1.39e-05
# weights:  23
initial  value 3404.486499 
iter  10 value 66.089477
iter  20 value 45.054871
iter  30 value 42.525511
iter  40 value 39.502627
iter  50 value 39.279597
iter  60 value 39.168892
iter  70 value 39.112052
iter  80 value 39.088573
iter  90 value 39.087026
final  value 39.081894 
converged
# weights:  23
initial  value 3994.825914 
iter  10 value 62.152586
iter  20 value 45.522720
iter  30 value 33.550814
iter  40 value 32.191312
iter  50 value 30.884446
iter  60 value 30.572487
iter  70 value 29.798446
iter  80 value 27.439596
iter  90 value 25.848885
iter 100 value 25.499474
final  value 25.499474 
stopped after 100 iterations
# weights:  23
initial  value 3395.745368 
iter  10 value 60.368028
iter  20 value 50.898558
iter  30 value 49.028750
iter  40 value 49.005147
iter  50 value 49.004911
iter  60 value 49.004132
iter  70 value 49.002857
iter  80 value 49.001499
iter  90 value 47.691953
iter 100 value 38.221711
final  value 38.221711 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=1.94; time: 0.0 min
[Tune-x] 4: size=8; decay=0.0182
# weights:  89
initial  value 3160.536920 
iter  10 value 68.348802
iter  20 value 39.124260
iter  30 value 27.020011
iter  40 value 22.539569
iter  50 value 18.650524
iter  60 value 17.125402
iter  70 value 15.411257
iter  80 value 14.230810
iter  90 value 12.919297
iter 100 value 12.168694
final  value 12.168694 
stopped after 100 iterations
# weights:  89
initial  value 3444.948735 
iter  10 value 82.898967
iter  20 value 56.322726
iter  30 value 39.012135
iter  40 value 29.150071
iter  50 value 25.290277
iter  60 value 21.448020
iter  70 value 16.648039
iter  80 value 14.894578
iter  90 value 14.373198
iter 100 value 14.004464
final  value 14.004464 
stopped after 100 iterations
# weights:  89
initial  value 3208.975265 
iter  10 value 144.383937
iter  20 value 80.324652
iter  30 value 59.351441
iter  40 value 44.362777
iter  50 value 34.603794
iter  60 value 29.753099
iter  70 value 25.392943
iter  80 value 21.970252
iter  90 value 19.832373
iter 100 value 17.780310
final  value 17.780310 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=1.86; time: 0.0 min
[Tune-x] 5: size=12; decay=0.0579
# weights:  133
initial  value 4391.413012 
iter  10 value 66.890457
iter  20 value 49.162761
iter  30 value 41.890168
iter  40 value 37.734756
iter  50 value 35.476656
iter  60 value 33.431300
iter  70 value 32.100301
iter  80 value 31.307349
iter  90 value 30.911713
iter 100 value 30.587116
final  value 30.587116 
stopped after 100 iterations
# weights:  133
initial  value 4413.572955 
iter  10 value 67.636282
iter  20 value 46.597005
iter  30 value 36.999400
iter  40 value 34.212154
iter  50 value 31.860764
iter  60 value 29.772669
iter  70 value 27.674332
iter  80 value 26.649265
iter  90 value 25.934298
iter 100 value 25.497303
final  value 25.497303 
stopped after 100 iterations
# weights:  133
initial  value 3486.598720 
iter  10 value 94.673935
iter  20 value 52.905302
iter  30 value 40.554236
iter  40 value 33.961743
iter  50 value 32.511019
iter  60 value 31.878577
iter  70 value 31.158481
iter  80 value 30.397527
iter  90 value 29.815634
iter 100 value 29.527089
final  value 29.527089 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 6: size=16; decay=0.0253
# weights:  177
initial  value 2915.679561 
iter  10 value 60.804475
iter  20 value 41.102922
iter  30 value 27.289865
iter  40 value 21.656024
iter  50 value 18.790017
iter  60 value 17.470521
iter  70 value 16.895143
iter  80 value 16.427877
iter  90 value 15.958595
iter 100 value 15.621807
final  value 15.621807 
stopped after 100 iterations
# weights:  177
initial  value 2684.977164 
iter  10 value 53.845631
iter  20 value 31.775652
iter  30 value 21.101662
iter  40 value 16.709853
iter  50 value 15.322602
iter  60 value 14.467752
iter  70 value 13.951763
iter  80 value 13.678256
iter  90 value 13.468402
iter 100 value 13.321918
final  value 13.321918 
stopped after 100 iterations
# weights:  177
initial  value 3563.370919 
iter  10 value 59.439643
iter  20 value 39.508665
iter  30 value 27.773391
iter  40 value 22.470360
iter  50 value 17.217076
iter  60 value 15.847712
iter  70 value 15.251380
iter  80 value 14.935243
iter  90 value 14.651306
iter 100 value 14.484255
final  value 14.484255 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse= 1.7; time: 0.0 min
[Tune-x] 7: size=2; decay=0.00011
# weights:  23
initial  value 2506.289543 
iter  10 value 149.678216
iter  20 value 77.049439
iter  30 value 56.021661
iter  40 value 48.392647
iter  50 value 40.178766
iter  60 value 31.310452
iter  70 value 29.984165
iter  80 value 29.554564
iter  90 value 29.218452
iter 100 value 28.950073
final  value 28.950073 
stopped after 100 iterations
# weights:  23
initial  value 3957.934752 
iter  10 value 52.429989
iter  20 value 48.150165
iter  30 value 47.375157
iter  40 value 47.178513
iter  50 value 47.126947
iter  60 value 47.121593
iter  70 value 47.086910
iter  80 value 47.072146
iter  90 value 47.068948
iter 100 value 47.068604
final  value 47.068604 
stopped after 100 iterations
# weights:  23
initial  value 3843.073729 
iter  10 value 189.651323
iter  20 value 169.264402
iter  30 value 73.078569
iter  40 value 70.565323
iter  50 value 62.735480
iter  60 value 59.567728
iter  70 value 59.357009
iter  80 value 59.143887
iter  90 value 58.046481
iter 100 value 50.350239
final  value 50.350239 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=1.71; time: 0.0 min
[Tune-x] 8: size=3; decay=5.97
# weights:  34
initial  value 3063.545773 
iter  10 value 597.493776
iter  20 value 337.414893
iter  30 value 276.864079
iter  40 value 265.165765
iter  50 value 262.760820
final  value 262.736876 
converged
# weights:  34
initial  value 3253.811127 
iter  10 value 422.299723
iter  20 value 297.578533
iter  30 value 269.375774
iter  40 value 263.521926
iter  50 value 263.463937
final  value 263.463780 
converged
# weights:  34
initial  value 3252.200078 
iter  10 value 548.844275
iter  20 value 342.122062
iter  30 value 309.266536
iter  40 value 300.563081
iter  50 value 299.686256
final  value 299.680061 
converged
[Tune-y] 8: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 9: size=12; decay=4.42
# weights:  133
initial  value 3322.251225 
iter  10 value 201.821199
iter  20 value 175.247779
iter  30 value 170.835433
iter  40 value 170.702080
iter  50 value 170.689790
iter  60 value 170.685710
final  value 170.685646 
converged
# weights:  133
initial  value 3785.170364 
iter  10 value 215.400024
iter  20 value 172.904545
iter  30 value 166.815545
iter  40 value 166.223112
iter  50 value 165.907762
iter  60 value 165.814110
iter  70 value 165.800816
final  value 165.800733 
converged
# weights:  133
initial  value 2522.255758 
iter  10 value 205.766060
iter  20 value 197.983279
iter  30 value 197.507259
iter  40 value 197.238071
iter  50 value 197.227231
iter  50 value 197.227229
iter  50 value 197.227229
final  value 197.227229 
converged
[Tune-y] 9: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 10: size=7; decay=0.000182
# weights:  78
initial  value 3212.490424 
iter  10 value 101.573104
iter  20 value 52.436877
iter  30 value 43.016626
iter  40 value 31.863187
iter  50 value 24.699793
iter  60 value 23.702296
iter  70 value 23.439558
iter  80 value 23.190160
iter  90 value 22.767198
iter 100 value 22.700061
final  value 22.700061 
stopped after 100 iterations
# weights:  78
initial  value 2746.497223 
iter  10 value 59.566451
iter  20 value 32.343347
iter  30 value 13.205378
iter  40 value 5.897513
iter  50 value 2.639923
iter  60 value 1.181147
iter  70 value 0.966370
iter  80 value 0.755157
iter  90 value 0.673498
iter 100 value 0.622557
final  value 0.622557 
stopped after 100 iterations
# weights:  78
initial  value 4250.198768 
iter  10 value 189.920160
iter  20 value 189.900538
iter  30 value 189.894166
iter  40 value 140.609861
iter  50 value 68.808276
iter  60 value 58.547302
iter  70 value 45.037156
iter  80 value 31.794925
iter  90 value 25.179448
iter 100 value 21.913952
final  value 21.913952 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=2.03; time: 0.0 min
[Tune-x] 11: size=18; decay=2.17e-05
# weights:  199
initial  value 2850.243572 
iter  10 value 58.459443
iter  20 value 35.324579
iter  30 value 12.156468
iter  40 value 1.598416
iter  50 value 0.236592
iter  60 value 0.070311
iter  70 value 0.059742
iter  80 value 0.055486
iter  90 value 0.047607
iter 100 value 0.043645
final  value 0.043645 
stopped after 100 iterations
# weights:  199
initial  value 3645.736291 
iter  10 value 61.009042
iter  20 value 26.357737
iter  30 value 8.559801
iter  40 value 1.225734
iter  50 value 0.094333
iter  60 value 0.041395
iter  70 value 0.040541
iter  80 value 0.035622
iter  90 value 0.030230
iter 100 value 0.027279
final  value 0.027279 
stopped after 100 iterations
# weights:  199
initial  value 4846.543483 
iter  10 value 51.423023
iter  20 value 32.275199
iter  30 value 12.444532
iter  40 value 2.752105
iter  50 value 0.347043
iter  60 value 0.067861
iter  70 value 0.058948
iter  80 value 0.052339
iter  90 value 0.042037
iter 100 value 0.036445
final  value 0.036445 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=2.48; time: 0.0 min
[Tune-x] 12: size=4; decay=2.51e-05
# weights:  45
initial  value 4501.358176 
iter  10 value 149.687150
final  value 149.687058 
converged
# weights:  45
initial  value 3568.334594 
iter  10 value 120.118748
iter  20 value 54.299911
iter  30 value 35.772146
iter  40 value 21.024159
iter  50 value 14.738604
iter  60 value 12.332037
iter  70 value 10.583778
iter  80 value 9.775739
iter  90 value 9.530271
iter 100 value 9.481424
final  value 9.481424 
stopped after 100 iterations
# weights:  45
initial  value 3378.839672 
iter  10 value 79.071099
iter  20 value 49.864284
iter  30 value 48.838709
iter  40 value 48.054888
iter  50 value 43.204151
iter  60 value 30.926931
iter  70 value 30.282761
iter  80 value 29.825489
iter  90 value 22.538089
iter 100 value 21.105689
final  value 21.105689 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=2.17; time: 0.0 min
[Tune-x] 13: size=16; decay=0.000381
# weights:  177
initial  value 2490.288210 
iter  10 value 56.648186
iter  20 value 30.633309
iter  30 value 7.705235
iter  40 value 2.231042
iter  50 value 0.929703
iter  60 value 0.734311
iter  70 value 0.534778
iter  80 value 0.443674
iter  90 value 0.407018
iter 100 value 0.387571
final  value 0.387571 
stopped after 100 iterations
# weights:  177
initial  value 2726.720118 
iter  10 value 54.146141
iter  20 value 26.176596
iter  30 value 13.127622
iter  40 value 4.193595
iter  50 value 1.547053
iter  60 value 0.940437
iter  70 value 0.777963
iter  80 value 0.595137
iter  90 value 0.506540
iter 100 value 0.453097
final  value 0.453097 
stopped after 100 iterations
# weights:  177
initial  value 4039.373759 
iter  10 value 57.146121
iter  20 value 29.774785
iter  30 value 12.343146
iter  40 value 4.223058
iter  50 value 1.265169
iter  60 value 0.798812
iter  70 value 0.569121
iter  80 value 0.466290
iter  90 value 0.408396
iter 100 value 0.377249
final  value 0.377249 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=2.16; time: 0.0 min
[Tune-x] 14: size=3; decay=0.000402
# weights:  34
initial  value 3136.865157 
iter  10 value 149.983179
iter  20 value 137.568347
iter  30 value 75.636790
iter  40 value 60.829693
iter  50 value 59.673339
iter  60 value 59.447295
iter  70 value 58.531499
iter  80 value 42.590542
iter  90 value 27.590682
iter 100 value 22.238883
final  value 22.238883 
stopped after 100 iterations
# weights:  34
initial  value 3983.144879 
iter  10 value 146.264493
iter  20 value 146.218045
iter  30 value 140.908526
iter  40 value 114.581583
iter  50 value 57.617835
iter  60 value 54.725810
iter  70 value 53.279208
iter  80 value 52.539444
iter  90 value 52.275345
iter 100 value 52.177786
final  value 52.177786 
stopped after 100 iterations
# weights:  34
initial  value 3929.776627 
iter  10 value 158.953351
iter  20 value 83.380008
iter  30 value 50.429081
iter  40 value 49.029836
iter  50 value 48.776378
iter  60 value 48.548860
iter  70 value 47.170272
iter  80 value 45.906294
iter  90 value 45.594569
iter 100 value 45.519351
final  value 45.519351 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=1.81; time: 0.0 min
[Tune-x] 15: size=2; decay=0.222
# weights:  23
initial  value 3345.788405 
iter  10 value 102.189045
iter  20 value 84.826646
iter  30 value 78.592308
iter  40 value 77.518977
iter  50 value 77.503552
iter  60 value 77.500792
final  value 77.500506 
converged
# weights:  23
initial  value 4209.957298 
iter  10 value 184.752642
iter  20 value 93.415714
iter  30 value 75.917715
iter  40 value 73.811347
iter  50 value 71.617267
iter  60 value 71.069529
iter  70 value 70.668331
iter  80 value 70.275894
iter  90 value 70.206984
final  value 70.206969 
converged
# weights:  23
initial  value 3799.061094 
iter  10 value 203.780799
iter  20 value 103.398051
iter  30 value 82.214574
iter  40 value 76.120767
iter  50 value 75.583553
iter  60 value 75.400047
iter  70 value 73.741467
iter  80 value 73.627059
final  value 73.627054 
converged
[Tune-y] 15: rmse.test.rmse=1.39; time: 0.0 min
[Tune-x] 16: size=20; decay=0.00075
# weights:  221
initial  value 3236.923650 
iter  10 value 58.246301
iter  20 value 36.061078
iter  30 value 15.050682
iter  40 value 5.370496
iter  50 value 2.410273
iter  60 value 1.453755
iter  70 value 1.118441
iter  80 value 0.875947
iter  90 value 0.772055
iter 100 value 0.713858
final  value 0.713858 
stopped after 100 iterations
# weights:  221
initial  value 5712.055938 
iter  10 value 52.606673
iter  20 value 28.134719
iter  30 value 14.053806
iter  40 value 5.090647
iter  50 value 1.867130
iter  60 value 1.414312
iter  70 value 1.050074
iter  80 value 0.865678
iter  90 value 0.772486
iter 100 value 0.693326
final  value 0.693326 
stopped after 100 iterations
# weights:  221
initial  value 5025.937599 
iter  10 value 51.851191
iter  20 value 30.379400
iter  30 value 10.478080
iter  40 value 2.180985
iter  50 value 1.239177
iter  60 value 1.002214
iter  70 value 0.799019
iter  80 value 0.697416
iter  90 value 0.648683
iter 100 value 0.611670
final  value 0.611670 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=1.99; time: 0.0 min
[Tune-x] 17: size=19; decay=0.0655
# weights:  210
initial  value 3374.996548 
iter  10 value 68.889663
iter  20 value 52.995273
iter  30 value 45.259930
iter  40 value 40.312841
iter  50 value 37.078804
iter  60 value 34.757997
iter  70 value 33.530675
iter  80 value 32.861495
iter  90 value 32.591318
iter 100 value 32.400553
final  value 32.400553 
stopped after 100 iterations
# weights:  210
initial  value 3837.582838 
iter  10 value 59.213473
iter  20 value 46.549517
iter  30 value 35.779280
iter  40 value 30.674225
iter  50 value 29.059373
iter  60 value 28.524351
iter  70 value 28.264950
iter  80 value 27.943985
iter  90 value 27.816769
iter 100 value 27.715187
final  value 27.715187 
stopped after 100 iterations
# weights:  210
initial  value 2845.759511 
iter  10 value 64.640225
iter  20 value 49.745261
iter  30 value 41.129541
iter  40 value 36.233679
iter  50 value 34.620466
iter  60 value 33.784987
iter  70 value 33.330748
iter  80 value 32.655694
iter  90 value 32.335739
iter 100 value 32.144323
final  value 32.144323 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse= 1.7; time: 0.0 min
[Tune-x] 18: size=16; decay=0.000127
# weights:  177
initial  value 2697.983227 
iter  10 value 50.766234
iter  20 value 23.583465
iter  30 value 4.046461
iter  40 value 0.563389
iter  50 value 0.228698
iter  60 value 0.199510
iter  70 value 0.183230
iter  80 value 0.155848
iter  90 value 0.140958
iter 100 value 0.130995
final  value 0.130995 
stopped after 100 iterations
# weights:  177
initial  value 4494.152544 
iter  10 value 57.971885
iter  20 value 27.281576
iter  30 value 10.802022
iter  40 value 1.683006
iter  50 value 0.233802
iter  60 value 0.178216
iter  70 value 0.160174
iter  80 value 0.128450
iter  90 value 0.114591
iter 100 value 0.108011
final  value 0.108011 
stopped after 100 iterations
# weights:  177
initial  value 4936.157428 
iter  10 value 47.721047
iter  20 value 26.246929
iter  30 value 6.307332
iter  40 value 1.592423
iter  50 value 0.261130
iter  60 value 0.190318
iter  70 value 0.174179
iter  80 value 0.144982
iter  90 value 0.131940
iter 100 value 0.122786
final  value 0.122786 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=2.08; time: 0.0 min
[Tune-x] 19: size=8; decay=0.0014
# weights:  89
initial  value 3393.256155 
iter  10 value 63.669329
iter  20 value 36.807181
iter  30 value 22.953293
iter  40 value 14.745824
iter  50 value 8.690491
iter  60 value 6.492618
iter  70 value 4.845179
iter  80 value 3.799539
iter  90 value 3.169002
iter 100 value 2.629767
final  value 2.629767 
stopped after 100 iterations
# weights:  89
initial  value 3798.504738 
iter  10 value 55.237869
iter  20 value 27.297685
iter  30 value 12.158981
iter  40 value 5.051793
iter  50 value 3.158478
iter  60 value 2.449654
iter  70 value 1.939289
iter  80 value 1.743320
iter  90 value 1.592254
iter 100 value 1.409811
final  value 1.409811 
stopped after 100 iterations
# weights:  89
initial  value 3309.658147 
iter  10 value 58.735760
iter  20 value 35.369143
iter  30 value 10.573875
iter  40 value 5.702764
iter  50 value 2.673480
iter  60 value 1.810379
iter  70 value 1.459849
iter  80 value 1.323904
iter  90 value 1.216479
iter 100 value 1.145780
final  value 1.145780 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=2.52; time: 0.0 min
[Tune-x] 20: size=18; decay=0.619
# weights:  199
initial  value 2128.729279 
iter  10 value 104.654817
iter  20 value 87.245413
iter  30 value 86.221828
iter  40 value 86.068769
iter  50 value 85.944063
iter  60 value 85.860748
iter  70 value 85.771925
iter  80 value 85.719996
iter  90 value 85.693408
iter 100 value 85.688268
final  value 85.688268 
stopped after 100 iterations
# weights:  199
initial  value 3277.812068 
iter  10 value 83.811017
iter  20 value 79.268568
iter  30 value 78.669012
iter  40 value 78.480370
iter  50 value 78.337368
iter  60 value 78.210501
iter  70 value 78.165050
iter  80 value 78.154045
iter  90 value 78.132586
iter 100 value 78.131859
final  value 78.131859 
stopped after 100 iterations
# weights:  199
initial  value 4222.911957 
iter  10 value 98.599266
iter  20 value 89.343755
iter  30 value 88.364298
iter  40 value 88.104276
iter  50 value 87.838330
iter  60 value 87.721187
iter  70 value 87.645063
iter  80 value 87.615463
iter  90 value 87.613842
iter 100 value 87.612551
final  value 87.612551 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=1.29; time: 0.0 min
[Tune] Result: size=18; decay=0.619 : rmse.test.rmse=1.29
# weights:  199
initial  value 5117.210751 
iter  10 value 242.706881
iter  20 value 186.363129
iter  30 value 140.514306
iter  40 value 133.091898
iter  50 value 124.715177
iter  60 value 121.150078
iter  70 value 120.215799
iter  80 value 119.676009
iter  90 value 119.465252
iter 100 value 119.208644
final  value 119.208644 
stopped after 100 iterations
[1] "Sat Jan 13 19:48:20 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 511 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 435
 number of selected nodes                               : 58 
[1] "Sat Jan 13 19:48:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Sat Jan 13 19:48:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Sat Jan 13 19:49:14 2018"
[Tune] Started tuning learner regr.randomForest for parameter set:
            Type len Def  Constr Req Tunable Trafo
nodesize integer   -   1 1 to 10   -    TRUE     -
mtry     integer   -   3  1 to 9   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nodesize=10; mtry=4
[Tune-y] 1: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 2: nodesize=7; mtry=1
[Tune-y] 2: rmse.test.rmse= 1.5; time: 0.0 min
[Tune-x] 3: nodesize=1; mtry=1
[Tune-y] 3: rmse.test.rmse= 1.5; time: 0.0 min
[Tune-x] 4: nodesize=4; mtry=5
[Tune-y] 4: rmse.test.rmse=1.38; time: 0.0 min
[Tune-x] 5: nodesize=6; mtry=6
[Tune-y] 5: rmse.test.rmse= 1.4; time: 0.0 min
[Tune-x] 6: nodesize=8; mtry=6
[Tune-y] 6: rmse.test.rmse=1.41; time: 0.0 min
[Tune-x] 7: nodesize=1; mtry=2
[Tune-y] 7: rmse.test.rmse=1.43; time: 0.0 min
[Tune-x] 8: nodesize=2; mtry=9
[Tune-y] 8: rmse.test.rmse=1.41; time: 0.0 min
[Tune-x] 9: nodesize=6; mtry=9
[Tune-y] 9: rmse.test.rmse=1.44; time: 0.0 min
[Tune-x] 10: nodesize=4; mtry=2
[Tune-y] 10: rmse.test.rmse=1.41; time: 0.0 min
[Tune-x] 11: nodesize=9; mtry=1
[Tune-y] 11: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 12: nodesize=2; mtry=1
[Tune-y] 12: rmse.test.rmse=1.48; time: 0.0 min
[Tune-x] 13: nodesize=8; mtry=3
[Tune-y] 13: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 14: nodesize=2; mtry=3
[Tune-y] 14: rmse.test.rmse=1.39; time: 0.0 min
[Tune-x] 15: nodesize=1; mtry=7
[Tune-y] 15: rmse.test.rmse= 1.4; time: 0.0 min
[Tune-x] 16: nodesize=10; mtry=3
[Tune-y] 16: rmse.test.rmse=1.41; time: 0.0 min
[Tune-x] 17: nodesize=10; mtry=6
[Tune-y] 17: rmse.test.rmse=1.43; time: 0.0 min
[Tune-x] 18: nodesize=8; mtry=2
[Tune-y] 18: rmse.test.rmse=1.44; time: 0.0 min
[Tune-x] 19: nodesize=4; mtry=4
[Tune-y] 19: rmse.test.rmse=1.38; time: 0.0 min
[Tune-x] 20: nodesize=9; mtry=8
[Tune-y] 20: rmse.test.rmse=1.42; time: 0.0 min
[Tune] Result: nodesize=4; mtry=5 : rmse.test.rmse=1.38
[1] "Sat Jan 13 19:49:27 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Sat Jan 13 19:49:33 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=9; min.node.size=4
[Tune-y] 1: rmse.test.rmse=1.44; time: 0.0 min
[Tune-x] 2: mtry=6; min.node.size=1
[Tune-y] 2: rmse.test.rmse=1.41; time: 0.0 min
[Tune-x] 3: mtry=1; min.node.size=1
[Tune-y] 3: rmse.test.rmse= 1.5; time: 0.0 min
[Tune-x] 4: mtry=4; min.node.size=6
[Tune-y] 4: rmse.test.rmse=1.39; time: 0.0 min
[Tune-x] 5: mtry=6; min.node.size=7
[Tune-y] 5: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 6: mtry=7; min.node.size=6
[Tune-y] 6: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 7: mtry=1; min.node.size=2
[Tune-y] 7: rmse.test.rmse=1.49; time: 0.0 min
[Tune-x] 8: mtry=2; min.node.size=10
[Tune-y] 8: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 9: mtry=6; min.node.size=10
[Tune-y] 9: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 10: mtry=3; min.node.size=3
[Tune-y] 10: rmse.test.rmse= 1.4; time: 0.0 min
[Tune-x] 11: mtry=9; min.node.size=1
[Tune-y] 11: rmse.test.rmse=1.44; time: 0.0 min
[Tune-x] 12: mtry=2; min.node.size=1
[Tune-y] 12: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 13: mtry=8; min.node.size=3
[Tune-y] 13: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 14: mtry=2; min.node.size=3
[Tune-y] 14: rmse.test.rmse=1.44; time: 0.0 min
[Tune-x] 15: mtry=1; min.node.size=8
[Tune-y] 15: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 16: mtry=9; min.node.size=4
[Tune-y] 16: rmse.test.rmse=1.43; time: 0.0 min
[Tune-x] 17: mtry=9; min.node.size=7
[Tune-y] 17: rmse.test.rmse=1.43; time: 0.0 min
[Tune-x] 18: mtry=7; min.node.size=2
[Tune-y] 18: rmse.test.rmse=1.41; time: 0.0 min
[Tune-x] 19: mtry=4; min.node.size=4
[Tune-y] 19: rmse.test.rmse=1.39; time: 0.0 min
[Tune-x] 20: mtry=8; min.node.size=8
[Tune-y] 20: rmse.test.rmse=1.42; time: 0.0 min
[Tune] Result: mtry=4; min.node.size=6 : rmse.test.rmse=1.39
[1] "Sat Jan 13 19:49:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Sat Jan 13 19:49:55 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.811; maxdepth=12; minbucket=35; minsplit=8
[Tune-y] 1: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 2: cp=0.00163; maxdepth=3; minbucket=21; minsplit=29
[Tune-y] 2: rmse.test.rmse=1.86; time: 0.0 min
[Tune-x] 3: cp=0.0577; maxdepth=20; minbucket=40; minsplit=31
[Tune-y] 3: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 4: cp=0.00159; maxdepth=7; minbucket=10; minsplit=49
[Tune-y] 4: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 5: cp=0.0479; maxdepth=29; minbucket=19; minsplit=14
[Tune-y] 5: rmse.test.rmse=1.83; time: 0.0 min
[Tune-x] 6: cp=0.492; maxdepth=4; minbucket=13; minsplit=8
[Tune-y] 6: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 7: cp=0.232; maxdepth=10; minbucket=10; minsplit=17
[Tune-y] 7: rmse.test.rmse=1.85; time: 0.0 min
[Tune-x] 8: cp=0.00157; maxdepth=23; minbucket=50; minsplit=19
[Tune-y] 8: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 9: cp=0.525; maxdepth=20; minbucket=40; minsplit=13
[Tune-y] 9: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 10: cp=0.0146; maxdepth=13; minbucket=44; minsplit=41
[Tune-y] 10: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 11: cp=0.0113; maxdepth=5; minbucket=19; minsplit=41
[Tune-y] 11: rmse.test.rmse=1.83; time: 0.0 min
[Tune-x] 12: cp=0.384; maxdepth=22; minbucket=49; minsplit=25
[Tune-y] 12: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 13: cp=0.00462; maxdepth=11; minbucket=18; minsplit=12
[Tune-y] 13: rmse.test.rmse=1.85; time: 0.0 min
[Tune-x] 14: cp=0.0474; maxdepth=20; minbucket=34; minsplit=6
[Tune-y] 14: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 15: cp=0.109; maxdepth=14; minbucket=13; minsplit=44
[Tune-y] 15: rmse.test.rmse=1.77; time: 0.0 min
[Tune-x] 16: cp=0.00926; maxdepth=7; minbucket=39; minsplit=27
[Tune-y] 16: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 17: cp=0.236; maxdepth=5; minbucket=9; minsplit=37
[Tune-y] 17: rmse.test.rmse=1.85; time: 0.0 min
[Tune-x] 18: cp=0.00641; maxdepth=22; minbucket=46; minsplit=16
[Tune-y] 18: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 19: cp=0.00369; maxdepth=26; minbucket=48; minsplit=41
[Tune-y] 19: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 20: cp=0.00206; maxdepth=13; minbucket=38; minsplit=28
[Tune-y] 20: rmse.test.rmse=1.82; time: 0.0 min
[Tune] Result: cp=0.109; maxdepth=14; minbucket=13; minsplit=44 : rmse.test.rmse=1.77
[1] "Sat Jan 13 19:50:03 2018"
[Tune] Started tuning learner regr.RRF for parameter set:
           Type len Def Constr Req Tunable Trafo
mtry    integer   -   3 1 to 9   -    TRUE     -
coefReg numeric   - 0.8 0 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=9; coefReg=0.357
[Tune-y] 1: rmse.test.rmse= 1.4; time: 0.0 min
[Tune-x] 2: mtry=6; coefReg=0.0825
[Tune-y] 2: rmse.test.rmse=1.44; time: 0.0 min
[Tune-x] 3: mtry=1; coefReg=0.0239
[Tune-y] 3: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 4: mtry=4; coefReg=0.543
[Tune-y] 4: rmse.test.rmse=1.39; time: 0.0 min
[Tune-x] 5: mtry=6; coefReg=0.627
[Tune-y] 5: rmse.test.rmse=1.41; time: 0.0 min
[Tune-x] 6: mtry=7; coefReg=0.567
[Tune-y] 6: rmse.test.rmse=1.41; time: 0.0 min
[Tune-x] 7: mtry=1; coefReg=0.174
[Tune-y] 7: rmse.test.rmse=1.49; time: 0.0 min
[Tune-x] 8: mtry=2; coefReg=0.963
[Tune-y] 8: rmse.test.rmse=1.41; time: 0.0 min
[Tune-x] 9: mtry=6; coefReg=0.941
[Tune-y] 9: rmse.test.rmse=1.39; time: 0.0 min
[Tune-x] 10: mtry=3; coefReg=0.21
[Tune-y] 10: rmse.test.rmse= 1.4; time: 0.0 min
[Tune-x] 11: mtry=9; coefReg=0.0561
[Tune-y] 11: rmse.test.rmse= 1.5; time: 0.0 min
[Tune-x] 12: mtry=2; coefReg=0.0667
[Tune-y] 12: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 13: mtry=8; coefReg=0.264
[Tune-y] 13: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 14: mtry=2; coefReg=0.267
[Tune-y] 14: rmse.test.rmse=1.43; time: 0.0 min
[Tune-x] 15: mtry=1; coefReg=0.724
[Tune-y] 15: rmse.test.rmse=1.48; time: 0.0 min
[Tune-x] 16: mtry=9; coefReg=0.312
[Tune-y] 16: rmse.test.rmse=1.41; time: 0.0 min
[Tune-x] 17: mtry=9; coefReg=0.636
[Tune-y] 17: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 18: mtry=7; coefReg=0.184
[Tune-y] 18: rmse.test.rmse=1.34; time: 0.0 min
[Tune-x] 19: mtry=4; coefReg=0.358
[Tune-y] 19: rmse.test.rmse=1.39; time: 0.0 min
[Tune-x] 20: mtry=8; coefReg=0.799
[Tune-y] 20: rmse.test.rmse=1.41; time: 0.0 min
[Tune] Result: mtry=7; coefReg=0.184 : rmse.test.rmse=1.34
[1] "Sat Jan 13 19:50:18 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Sat Jan 13 19:50:24 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Sat Jan 13 19:50:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.418 0.334 0.267 0.213 0.170
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 1 -----> 4 
Runtime: 0.01000094 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.2667 
    Y 1          8.523 
    Y 2          9.065 
    Y 3          7.331 
    Y 4          8.546 
    Y 5          7.516 
[1] "Sat Jan 13 19:50:37 2018"
[Tune] Started tuning learner regr.svm for parameter set:
         Type len   Def    Constr Req Tunable Trafo
cost  numeric   -     0 -15 to 15   -    TRUE     Y
gamma numeric   - -3.17 -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cost=1.75e+04; gamma=0.0511
[Tune-y] 1: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 2: cost=26.2; gamma=0.00017
[Tune-y] 2: rmse.test.rmse=1.62; time: 0.0 min
[Tune-x] 3: cost=0.000141; gamma=5.01e-05
[Tune-y] 3: rmse.test.rmse=1.81; time: 0.0 min
[Tune-x] 4: cost=0.0659; gamma=2.46
[Tune-y] 4: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 5: cost=6.28; gamma=14.1
[Tune-y] 5: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 6: cost=271; gamma=4.04
[Tune-y] 6: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 7: cost=0.000132; gamma=0.00113
[Tune-y] 7: rmse.test.rmse=1.81; time: 0.0 min
[Tune-x] 8: cost=0.000415; gamma=1.51e+04
[Tune-y] 8: rmse.test.rmse=1.81; time: 0.0 min
[Tune-x] 9: cost=3.6; gamma=9.59e+03
[Tune-y] 9: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 10: cost=0.0236; gamma=0.00241
[Tune-y] 10: rmse.test.rmse=1.81; time: 0.0 min
[Tune-x] 11: cost=3.9e+03; gamma=9.79e-05
[Tune-y] 11: rmse.test.rmse=1.28; time: 0.0 min
[Tune-x] 12: cost=0.00118; gamma=0.000122
[Tune-y] 12: rmse.test.rmse=1.81; time: 0.0 min
[Tune-x] 13: cost=409; gamma=0.00732
[Tune-y] 13: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 14: cost=0.000347; gamma=0.00792
[Tune-y] 14: rmse.test.rmse=1.81; time: 0.0 min
[Tune-x] 15: cost=0.000127; gamma=106
[Tune-y] 15: rmse.test.rmse=1.81; time: 0.0 min
[Tune-x] 16: cost=2.38e+04; gamma=0.0203
[Tune-y] 16: rmse.test.rmse=1.96; time: 0.0 min
[Tune-x] 17: cost=4.74e+03; gamma=16.9
[Tune-y] 17: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 18: cost=237; gamma=0.00141
[Tune-y] 18: rmse.test.rmse=1.33; time: 0.0 min
[Tune-x] 19: cost=0.101; gamma=0.0519
[Tune-y] 19: rmse.test.rmse= 1.7; time: 0.0 min
[Tune-x] 20: cost=1.53e+03; gamma=497
[Tune-y] 20: rmse.test.rmse=1.82; time: 0.0 min
[Tune] Result: cost=3.9e+03; gamma=9.79e-05 : rmse.test.rmse=1.28
[1] "Sat Jan 13 19:50:45 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=3.34e+03; max_depth=4; eta=0.395; gamma=0.825; colsample_bytree=0.329; min_child_weight=0.477; subsample=0.527
[Tune-y] 1: rmse.test.rmse=1.53; time: 0.6 min
[Tune-x] 2: nrounds=259; max_depth=6; eta=0.377; gamma=7.69; colsample_bytree=0.527; min_child_weight=1.41; subsample=0.38
[Tune-y] 2: rmse.test.rmse=1.31; time: 0.1 min
[Tune-x] 3: nrounds=21; max_depth=10; eta=0.337; gamma=9.41; colsample_bytree=0.428; min_child_weight=4.2; subsample=0.923
[Tune-y] 3: rmse.test.rmse=1.51; time: 0.0 min
[Tune-x] 4: nrounds=14; max_depth=2; eta=0.0409; gamma=7.89; colsample_bytree=0.405; min_child_weight=2.34; subsample=0.45
[Tune-y] 4: rmse.test.rmse=4.78; time: 0.0 min
[Tune-x] 5: nrounds=15; max_depth=8; eta=0.591; gamma=3.12; colsample_bytree=0.663; min_child_weight=12.7; subsample=0.822
[Tune-y] 5: rmse.test.rmse= 1.4; time: 0.0 min
[Tune-x] 6: nrounds=30; max_depth=4; eta=0.215; gamma=8.53; colsample_bytree=0.619; min_child_weight=7.05; subsample=0.31
[Tune-y] 6: rmse.test.rmse=1.71; time: 0.0 min
[Tune-x] 7: nrounds=64; max_depth=9; eta=0.517; gamma=6.92; colsample_bytree=0.691; min_child_weight=8.75; subsample=0.418
[Tune-y] 7: rmse.test.rmse=1.56; time: 0.0 min
[Tune-x] 8: nrounds=67; max_depth=3; eta=0.0986; gamma=5.6; colsample_bytree=0.552; min_child_weight=12.7; subsample=0.279
[Tune-y] 8: rmse.test.rmse=1.85; time: 0.0 min
[Tune-x] 9: nrounds=590; max_depth=5; eta=0.118; gamma=8.49; colsample_bytree=0.43; min_child_weight=3.23; subsample=0.817
[Tune-y] 9: rmse.test.rmse=1.41; time: 0.1 min
[Tune-x] 10: nrounds=180; max_depth=8; eta=0.0514; gamma=1; colsample_bytree=0.587; min_child_weight=5.43; subsample=0.784
[Tune-y] 10: rmse.test.rmse=1.38; time: 0.0 min
[Tune-x] 11: nrounds=2.16e+03; max_depth=3; eta=0.116; gamma=8.22; colsample_bytree=0.679; min_child_weight=15.7; subsample=0.331
[Tune-y] 11: rmse.test.rmse=1.83; time: 0.1 min
[Tune-x] 12: nrounds=90; max_depth=8; eta=0.307; gamma=4.6; colsample_bytree=0.44; min_child_weight=11.6; subsample=0.459
[Tune-y] 12: rmse.test.rmse=1.63; time: 0.0 min
[Tune-x] 13: nrounds=601; max_depth=6; eta=0.167; gamma=7.72; colsample_bytree=0.364; min_child_weight=7.45; subsample=0.257
[Tune-y] 13: rmse.test.rmse=1.58; time: 0.0 min
[Tune-x] 14: nrounds=1.5e+03; max_depth=8; eta=0.175; gamma=9.17; colsample_bytree=0.491; min_child_weight=13.7; subsample=0.286
[Tune-y] 14: rmse.test.rmse=1.81; time: 0.1 min
[Tune-x] 15: nrounds=59; max_depth=7; eta=0.151; gamma=4.72; colsample_bytree=0.686; min_child_weight=17.9; subsample=0.491
[Tune-y] 15: rmse.test.rmse=1.82; time: 0.0 min
[Tune-x] 16: nrounds=1.49e+03; max_depth=9; eta=0.235; gamma=3.75; colsample_bytree=0.546; min_child_weight=12.3; subsample=0.499
[Tune-y] 16: rmse.test.rmse=1.48; time: 0.1 min
[Tune-x] 17: nrounds=76; max_depth=4; eta=0.54; gamma=6.91; colsample_bytree=0.555; min_child_weight=7.49; subsample=0.947
[Tune-y] 17: rmse.test.rmse= 1.5; time: 0.0 min
[Tune-x] 18: nrounds=12; max_depth=4; eta=0.344; gamma=7.93; colsample_bytree=0.653; min_child_weight=8.95; subsample=0.471
[Tune-y] 18: rmse.test.rmse=1.65; time: 0.0 min
[Tune-x] 19: nrounds=327; max_depth=2; eta=0.426; gamma=3.98; colsample_bytree=0.667; min_child_weight=13.4; subsample=0.344
[Tune-y] 19: rmse.test.rmse=1.87; time: 0.0 min
[Tune-x] 20: nrounds=14; max_depth=4; eta=0.0966; gamma=4; colsample_bytree=0.37; min_child_weight=17.7; subsample=0.552
[Tune-y] 20: rmse.test.rmse=2.69; time: 0.0 min
[Tune] Result: nrounds=259; max_depth=6; eta=0.377; gamma=7.69; colsample_bytree=0.527; min_child_weight=1.41; subsample=0.38 : rmse.test.rmse=1.31
[1] "Sat Jan 13 19:52:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Sat Jan 13 19:52:14 2018"
[Tune] Started tuning learner regr.ksvm for parameter set:
         Type len  Def    Constr Req Tunable Trafo
C     numeric   -    0  -5 to 10   -    TRUE     Y
sigma numeric   - TRUE -15 to 15   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: C=5.31; sigma=0.371
[Tune-y] 1: rmse.test.rmse=1.27; time: 93.0 min
[Tune-x] 2: C=16.3; sigma=4.71e-05
[Tune-y] 2: rmse.test.rmse=2.38; time: 6.5 min
[Tune-x] 3: C=29.6; sigma=0.503
[Tune-y] 3: rmse.test.rmse=1.23; time: 38.0 min
[Tune-x] 4: C=13.7; sigma=202
[Tune-y] 4: rmse.test.rmse=3.51; time: 10.6 min
[Tune-x] 5: C=6.35; sigma=9.25e-05
[Tune-y] 5: rmse.test.rmse=2.37; time: 8.0 min
[Tune-x] 6: C=548; sigma=0.54
[Tune-y] 6: rmse.test.rmse=1.23; time: 31.3 min
[Tune-x] 7: C=29.8; sigma=15.5
[Tune-y] 7: rmse.test.rmse=3.51; time: 20.2 min
[Tune-x] 8: C=803; sigma=0.136

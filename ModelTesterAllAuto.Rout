
R version 3.4.1 (2017-06-30) -- "Single Candle"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> options(repos=structure(c(CRAN="https://rweb.crmda.ku.edu/cran/")))
> ## capture messages and errors to a file.https://rweb.crmda.ku.edu/cran/
> #zz <- file("all.Rout", open="wt")https://cran.cnr.berkeley.edu
> #sink(zz, type="message") edit for rebaseless
> #chek for R package updates
> #try(log("a")) ## test --no-edit
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> task.subject<-"14th20hp3cv"
> pc.mlr<-c("ACE")#"ALTA","HOPPER"
> which.computer<-Sys.info()[['nodename']]
> out.file<-paste("out",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,".csv",sep="")
> importance.file<-paste("importance",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,sep="")
> 
> base.folder<-getwd()
> cpout.folder<-paste(base.folder,"/",which.computer,sep = "")
> setwd(cpout.folder)
> 
> if(length(which(list.files() == out.file))<1) write.table( "0.01,0.01,100,100,100,Wed Aug 02 16:37:25 2017,dummy,8,1,basic latent features,ignore,none,asis,1.12784979099243,random,333,53,adaptive_cv,16,5,2,2,19,0.0107744822639878,FALSE,,,,,,,,,," ,file =,out.file,  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,".csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,".csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,"mlr.csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,"mlr.csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> 
> cv.iters=3
> tuneLength=20
> tuneLength2=8
> normings=c("YeoJohnson","ICA", "centernscale","expoTrans","range01","asis","quantile")#,"centernscale"
> 
> gensTTesto<-c(56,53,4,12,13,14,15,20,45,54,55, 44,3,1,52)#,  51,c(4)#c(1:40)#c(5,10,11,13,14,15,16,17,18,19,20,21,24,28,38,39,40)
> gensTTest<-vector()
> write.table( t(gensTTesto),file = "initial tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
> try({
+   gensTTest<-t(read.csv("tasks to test.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   gensTTest<-as.vector(gensTTest)
+ })
> if(!exists("gensTTest")) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> gensTTesto<-c(gensTTesto[length(gensTTesto):1])
> if(length(gensTTest)<1) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> 
> 
> ########packages install check######
> 
> #list.of.packages <- c("caret","caretEnsemble","mlr","MLmetrics","tgp")
> #list.of.packages <- c("gower","dimRed","DEoptimR","caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> #new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> #if(length(new.packages)) install.packages(new.packages, dep = TRUE)
> 
> 
> #install.packages("mlr", dependencies = c("Depends", "Suggests"))
> #install.packages("caret", dependencies = c("Depends", "Suggests"))
> #install.packages("caret",repos = "http://cran.r-project.org",dependencies = c("Depends", "Imports", "Suggests"))
> #install.packages("SuperLearner", dependencies = c("Depends", "Suggests"))
> #install.packages("rattle", dependencies = c("Depends", "Suggests"))
> 
> # Load libraries
> #library(mlbench)
> 
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> #library(caretEnsemble)
> library(MLmetrics)

Attaching package: 'MLmetrics'

The following object is masked from 'package:caret':

    RMSE

The following object is masked from 'package:base':

    Recall

> 
> ########error no repeat#########
> 
> 
> try({
+   before.last.alg<-as.matrix(read.csv("beforelast algorithm.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.alg<-as.matrix(read.csv("last algorithm tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   #write.table(paste(date(), last.alg,.Platform$OS.type,.Platform$r_arch,which.computer,sep=" "),file = "algos after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.alg==before.last.alg){print("algorithm may be broken")}
+   write.table(last.alg,file = "beforelast algorithm.csv",  quote = F, row.names = F,col.names = F)
+ })
[1] "algorithm may be broken"
> try({
+   before.last.tsk<-as.matrix(read.csv("beforelast task.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.tsk<-as.matrix(read.csv("last task tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   write.table(paste(date(),last.alg, last.tsk,cv.iters,tuneLength,.Platform$OS.type,.Platform$r_arch,which.computer,sep=","),file = "test after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.tsk==before.last.tsk){print("task may be broken")}
+   write.table(last.tsk,file = "beforelast task.csv",  quote = F, row.names = F,col.names = F)
+ })
[1] "task may be broken"
> bad.models=c("spaccceeee")
> previous.fails<-(read.csv("test after which reset.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
> previous.fails<-previous.fails[previous.fails[,8]==which.computer,]
> lgf<-length(previous.fails[,2])
> for(lt in 2:lgf)  {
+   if(previous.fails[lt,2]==previous.fails[lt-1,2])  {
+     bad.models=union(bad.models,c(paste(previous.fails[lt,2])))  }}
> 
> #######not to redo a test function#####
> check.redundant<-function(df=df.previous.calcs,norming="asis",trans.y=1,withextra="missing",missingdata="leaveempty",datasource="mean" ,column.to.predict=200,allmodel="ctree")
+ {
+   for(intern in 1:length(df[,1])){
+     if((any(df[intern,] == norming, na.rm=T))&&
+        (any(df[intern,] == withextra, na.rm=T))&&
+        (any(df[intern,] == missingdata, na.rm=T))&&
+        (any(df[intern,] == datasource, na.rm=T))&&
+        (any(df[intern,] == column.to.predict, na.rm=T))&&
+        (any(df[intern,] == allmodel, na.rm=T))&&
+        (  (df[intern,9] == trans.y)))
+     {return(TRUE)}
+   }
+   return(FALSE)
+ }
> #####caret init#####
> best.ranged <- c("avNNet", "nnet", "pcaNNet", "glm.nb")
> best.asis <- c("svmLinear3", "relaxo", "superpc", "xgbTree")
> best.cns <- c("gam", "bam", "svmLinear2", "msaenet", "BstLm", "gbm") 
> 
> cv6hp5 <- c( "BstLm", "qrnn")#earth
> cv3hp32 <- c("Rborist", "pcaNNet", "SBC")
> cv7x5hp32 <- c("gbm", "krlsPoly", "kknn", "xgbLinear","RRF", "cubist", "rlm" )
> cv6hp5.avoid <- c("pcaNNet")
> cv3hp32.avoid <- c("glm.nb", "gamboost", "ctree2","glmboost", "leapSeq","ctree","svmLinear2")
> cv7x5hp32.avoid <- c("SBC","bagearthgcv","gcvearth","lmStepAIC","glmStepAIC","bridge","lm","glm","bayesglm","blassoAveraged","treebag","rpart1SE")
> 
> allmodels <- c("avNNet", "bagEarth", "bagEarthGCV",
+                "bayesglm", "bdk", "blackboost", "Boruta", "brnn", "BstLm" ,
+                "bstTree", "cforest", "ctree", "ctree2", "cubist", "DENFIS",
+                "dnn", "earth", "elm", "enet",   "evtree",
+                "extraTrees",  "gamLoess",  "gaussprLinear", "gaussprPoly", "gaussprRadial",
+                "gcvEarth","glm", "glmboost",  "icr", "kernelpls",
+                "kknn", "knn",  "krlsRadial", "lars" , "lasso",
+                "leapBackward", "leapForward", "leapSeq", "lm", "M5", "M5Rules",
+                "mlpWeightDecay", "neuralnet" , "partDSA",
+                "pcaNNet", "pcr", "penalized", "pls", "plsRglm", "ppr",
+                "qrf" , "ranger",  "rf")
> allmodels <- c("rlm", "rpart", "rpart2",
+                "RRF", "RRFglobal",  "simpls",
+                "svmLinear", "svmPoly", "svmRadial", "svmRadialCost",
+                "widekernelpls",  "xgbLinear",
+                "xgbTree")
> allmodels <- c("avNNet","BstLm","bstTree","cforest","ctree","ctree2",
+                "cubist","earth","enet","evtree","glmboost",
+                "icr","kernelpls","kknn","lasso","pcaNNet",
+                "pcr","pls","qrf","ranger","rf")
> 
> allmodels <- c("kknn", "cubist", "avNNet", "xgbLinear", "RRF", "pcaNNet","earth","nnet","gbm","enet","lasso","BstLm",
+                "foba", "leapBackward", "gcvEarth", "SBC","glm.nb","gamboost","ctree2","relaxo", 
+                "bartMachine","extraTrees","bam","gam","randomGLM")
> #allmodels <- c("bam")
> #allmodels <- c("rf")"rqlasso",, "xyf" "rvmPoly", "rvmRadial",    "spls", "superpc" ,   "treebag",  "svmLinear2",  "SBC",
> #allmodels <- c("bartMachine", "xgbLinear", "pcaNNet","svmLinear","glmnet","cforest","cubist","rf","ranger")"glmnet",
> #wow rfRules is really slow "rfRules","WM", takes 50min
> # brak everythig "rbfDDA","ridge","rqnc",
> # use "rf" to test all
> library(caret)
> allmodels <- unique(modelLookup()[modelLookup()$forReg,c(1)])
> #allmodels <-c("avNNet", "nnet", "pcaNNet",  "glm.nb", "gam" ,
> #              "bam","msaenet", "svmLinear2","svmLinear3",
> #              "relaxo",  "superpc", "xgbTree", "BstLm")
> #allmodels<- c("svmLinear","svmPoly","svmRadial")
> #library(doParallel); cl <- makeCluster(detectCores()); registerDoParallel(cl)
> #allmodels<-c("bartMachine","extraTrees")#,"randomGLM"
> 
> 
> adaptControl <- trainControl(method = "adaptive_cv",
+                              number = 7, repeats = 5,
+                              adaptive = list(min = 4, alpha = 0.05,
+                                              method = "gls", complete = FALSE),
+                              search = "random")
> adaptControl <-trainControl(method = "cv", number = cv.iters,  search = "random")
> simpleControl <- trainControl(method = "cv",
+                               number = cv.iters,
+                               search = "random")
> 
> 
> #########MLR init######
> #R.utils::gcDLLs()
> #list.of.packages <- c("ParamHelpers","devtools","mlrMBO","RJSONIO","plot3D","plotly")
> #install.packages("mlrMBO", dependencies = c("Depends", "Suggests"))
> list.of.packages <- c("caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> if(length(new.packages)) install.packages(new.packages, dep = TRUE)
Installing packages into 'C:/Users/irina grishina/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Warning message:
packages ' RWeka', 'rPython', 'rrf' are not available (for R version 3.4.1) 
> 
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> tuneLengthMLR<-tuneLength
> mlr.iters<-cv.iters
> #######data read process start#####
> seed.var =222+round(runif(1,min=0,max=100))
> column.to.predict=1
> print(date());
[1] "Wed Feb 28 15:32:09 2018"
> 
> setwd(base.folder)
> if(!exists("gen.count")){gen.count=56}
> gens.names<-as.matrix(read.table("gens names.csv", sep = ",",header = FALSE,row.names=1,fill=TRUE, quote="",dec="."))
> count.toy.data.passed<-1
> for(gend.data in gensTTest){
+   count.toy.data.passed<-count.toy.data.passed+1
+   setwd(base.folder)
+   data.source<-as.matrix(read.csv(paste("Generats/",gens.names[gend.data],".csv", sep = ""), sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   datasource<-gens.names[gend.data,1]
+   setwd(cpout.folder)
+   missingdatas=c("ignore")
+   for(missingdata in missingdatas){
+     withextras=c("none")
+     for(withextra in withextras){
+       ################data wrestling###############
+       
+       dependant.selection=complete.cases(data.source[,column.to.predict])
+       df.previous.calcs=as.data.frame(read.csv(file=out.file, header = FALSE, sep = ",", quote = "",
+                                                dec = ".", fill = TRUE, comment.char = ""))
+       unimportant.computations<-vector(mode = "logical",length=length(df.previous.calcs[,1])  )
+       for(intern in 1:length(df.previous.calcs[,1])){
+         if((any(df.previous.calcs[intern,] == withextra, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == missingdata, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == datasource, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == column.to.predict, na.rm=T)))
+         {unimportant.computations[intern]<-T}}
+       
+       df.previous.calcs<-df.previous.calcs[unimportant.computations,]
+       
+       #data.source=data.frame( data.source[,column.to.predict],data.source[,1:2], data.source[,4:(column.to.predict-1)], data.source[,(column.to.predict+1):length( data.source[1,])])
+       
+         for(norming in normings) {
+         for(trans.y in 1:2) {
+           df.toprocess=data.source
+           y.untransformed<-df.toprocess[,1]
+           
+           if(norming=="centernscale"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("center", "scale"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="range01"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="expoTrans"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("expoTrans"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="YeoJohnson"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("YeoJohnson"))#"center", "scale",
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           if((norming=="asis")&&(trans.y==2)){next}
+           
+           
+           ################preprocess###########
+           df.toprocess=data.frame(df.toprocess[dependant.selection,])
+           y.untransformed=y.untransformed[dependant.selection]
+           if(norming=="quantile"){
+             for(Clol in trans.y:length(data.source[1,])){
+               df.toprocess[,Clol]<- (rank(df.toprocess[,Clol],na.last = "keep",ties.method = "average")-1) }
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           loess.model<-loess(y.untransformed~ df.toprocess[,1],span = 0.21, degree = 1)
+   
+           #df.toprocess = data.frame(df.toprocess,)
+           nzv <- nearZeroVar(df.toprocess[,])#, saveMetrics= TRUE
+           #nzv[nzv$nzv,][1:10,]
+           if(length(nzv)>1){
+             df.toprocess = (df.toprocess[, -nzv])}
+           df.toprocess = signif(df.toprocess,digits = 3)
+           
+           seed.var =222+round(runif(1,min=0,max=100))
+           set.seed(seed.var)
+           inTrain <- createDataPartition(y = df.toprocess[,1],
+                                          p = .75,
+                                          list = FALSE)
+           training <- df.toprocess[ inTrain,]
+           testing  <- df.toprocess[-inTrain,]
+           write.table(df.toprocess,file = "sanity check 1.csv",  quote = F, row.names = F,col.names = F)
+           
+           ###########for all models#################
+           setwd(base.folder)
+           if(max(which.computer==pc.mlr)>0)
+             source("MLR part.R")
+           else
+             source("Caret part.R")
+           
+          setwd(cpout.folder)
+           if(norming == normings[length(normings)]){
+             if(count.toy.data.passed>length(gensTTest)){gensTTest<-c(gensTTesto)}
+             write.table( t(gensTTest[count.toy.data.passed:length(gensTTest)]),file = "tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
+             
+             }
+           
+         }
+       }
+     }
+   }
+   
+ }
Loading required package: pls

Attaching package: 'pls'

The following object is masked from 'package:caret':

    R2

The following object is masked from 'package:stats':

    loadings

Partial Least Squares 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results:

  RMSE        Rsquared 
  0.08282632  0.9967928

Tuning parameter 'ncomp' was held constant at a value of 1
[1] "Wed Feb 28 15:32:43 2018"
Loading required package: spikeslab
Loading required package: lars
Loaded lars 1.2

Loading required package: randomForest
randomForest 4.6-12
Type rfNews() to see new features/changes/bug fixes.

Attaching package: 'randomForest'

The following object is masked from 'package:ggplot2':

    margin

Loading required package: parallel

 spikeslab 1.1.5 
 
 Type spikeslab.news() to see new features, changes, and bug fixes. 
 

Loading required package: plyr
Spike and Slab Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  vars  RMSE        Rsquared   Selected
  1     1.41592676  0.5620641          
  2     0.01144068  0.9999406  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was vars = 2.
[1] "Wed Feb 28 15:32:49 2018"
Loading required package: spls
Sparse Partial Least Squares (SPLS) Regression and
Classification (version 2.2-1)


Attaching package: 'spls'

The following object is masked from 'package:caret':

    splsda

Sparse Partial Least Squares 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  kappa        eta         K  RMSE        Rsquared   Selected
  0.007255503  0.67749892  2  0.01144071  0.9999406          
  0.008178812  0.84236134  1  0.01263983  0.9999270          
  0.031912900  0.73987056  1  0.01263983  0.9999270          
  0.059674022  0.98634917  1  0.96909655  0.5620641          
  0.060663107  0.84032897  1  0.01263983  0.9999270          
  0.065617367  0.82276286  2  0.01144071  0.9999406  *       
  0.067622546  0.84590722  1  0.01263983  0.9999270          
  0.128530828  0.50480420  1  0.01263983  0.9999270          
  0.138834747  0.20633166  1  0.01263983  0.9999270          
  0.146772003  0.20176901  1  0.01263983  0.9999270          
  0.212057777  0.51721534  2  0.01144071  0.9999406          
  0.240844158  0.73565573  1  0.01263983  0.9999270          
  0.250222347  0.33155950  1  0.01263983  0.9999270          
  0.275187086  0.01602648  2  0.01144071  0.9999406          
  0.293789700  0.60352954  2  0.01144071  0.9999406          
  0.342542854  0.48168796  1  0.01263983  0.9999270          
  0.382992246  0.21947972  1  0.01263983  0.9999270          
  0.392687838  0.47704334  1  0.01263983  0.9999270          
  0.418226220  0.98454018  1  0.96909655  0.5620641          
  0.483847799  0.17806528  2  0.01144071  0.9999406          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were K = 2, eta = 0.8227629 and kappa
 = 0.06561737.
[1] "Wed Feb 28 15:32:51 2018"
Loading required package: superpc
Loading required package: survival

Attaching package: 'survival'

The following object is masked from 'package:caret':

    cluster

Supervised Principal Component Analysis 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  threshold   n.components  RMSE        Rsquared   Selected
  0.01451101  3             0.02444512  0.9999406  *       
  0.01635762  3             0.02444512  0.9999406          
  0.06382580  3             0.02444512  0.9999406          
  0.11934804  3             0.02444512  0.9999406          
  0.12132621  3             0.02444512  0.9999406          
  0.13123473  3             0.02444512  0.9999406          
  0.13524509  3             0.02444512  0.9999406          
  0.25706166  2             0.02444512  0.9999406          
  0.27766949  1             0.52191387  0.8728724          
  0.29354401  1             0.52191387  0.8728724          
  0.42411555  2             0.02444512  0.9999406          
  0.48168832  3             0.02444512  0.9999406          
  0.50044469  1             0.52191387  0.8728724          
  0.55037417  1             0.52191387  0.8728724          
  0.58757940  2             0.02444512  0.9999406          
  0.68508571  2             0.02444512  0.9999406          
  0.76598449  1             0.52191387  0.8728724          
  0.78537568  2             0.02444512  0.9999406          
  0.83645244  3             0.02444512  0.9999406          
  0.96769560  1             0.52191387  0.8728724          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were threshold = 0.01451101
 and n.components = 3.
[1] "Wed Feb 28 15:32:54 2018"
Loading required package: kernlab

Attaching package: 'kernlab'

The following object is masked from 'package:ggplot2':

    alpha

Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Wed Feb 28 15:32:56 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "YeoJohnson"               "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "svmBoundrangeString"     
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Wed Feb 28 15:32:57 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "YeoJohnson"               "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "svmExpoString"           
Support Vector Machines with Linear Kernel 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  C             RMSE        Rsquared   Selected
    0.03633906  0.07146961  0.9999398          
    0.03704350  0.07152482  0.9999400          
    0.06068093  0.07147181  0.9999379          
    0.10808368  0.06917354  0.9999150          
    0.11032971  0.06908085  0.9999195          
    0.12230209  0.06907087  0.9999150  *       
    0.12750948  0.06907669  0.9999157          
    0.45247966  0.07247765  0.9998963          
    0.56059952  0.07247765  0.9998963          
    0.66120067  0.07247765  0.9998963          
    2.56993758  0.07247765  0.9998963          
    4.67616146  0.07247765  0.9998963          
    5.68306973  0.07247765  0.9998963          
    9.55074044  0.07247765  0.9998963          
   14.06162094  0.07247765  0.9998963          
   38.75433012  0.07247765  0.9998963          
   89.86997995  0.07247765  0.9998963          
  109.94476883  0.07247765  0.9998963          
  186.98601832  0.07247765  0.9998963          
  731.86511892  0.07247765  0.9998963          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was C = 0.1223021.
[1] "Wed Feb 28 15:33:00 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Loading required package: e1071
Support Vector Machines with Linear Kernel 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  cost          RMSE        Rsquared   Selected
    0.03633906  0.07146961  0.9999398          
    0.03704350  0.07152482  0.9999400          
    0.06068093  0.07147181  0.9999379          
    0.10808368  0.06917354  0.9999150          
    0.11032971  0.06908085  0.9999195          
    0.12230209  0.06907087  0.9999150  *       
    0.12750948  0.06907669  0.9999157          
    0.45247966  0.07247765  0.9998963          
    0.56059952  0.07247765  0.9998963          
    0.66120067  0.07247765  0.9998963          
    2.56993758  0.07247765  0.9998963          
    4.67616146  0.07247765  0.9998963          
    5.68306973  0.07247765  0.9998963          
    9.55074044  0.07247765  0.9998963          
   14.06162094  0.07247765  0.9998963          
   38.75433012  0.07247765  0.9998963          
   89.86997995  0.07247765  0.9998963          
  109.94476883  0.07247765  0.9998963          
  186.98601832  0.07247765  0.9998963          
  731.86511892  0.07247765  0.9998963          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was cost = 0.1223021.
[1] "Wed Feb 28 15:33:03 2018"
Loading required package: LiblineaR
L2 Regularized Support Vector Machine (dual) with Linear Kernel 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  cost          Loss  RMSE        Rsquared   Selected
  1.194167e-03  L2    0.95728424  0.9987758          
  1.225132e-03  L2    0.94427966  0.9989088          
  2.365754e-03  L2    0.47996552  0.9997321          
  5.107953e-03  L2    0.10428660  0.9999272          
  5.249968e-03  L2    0.10469764  0.9999174          
  6.022986e-03  L2    0.09212297  0.9999274          
  6.367319e-03  L2    0.08653885  0.9999124          
  3.446398e-02  L2    0.05676832  0.9999412          
  4.586031e-02  L1    0.09833405  0.9995282          
  5.714932e-02  L1    0.09185829  0.9994779          
  3.492449e-01  L2    0.05749940  0.9999274          
  7.758052e-01  L2    0.05497110  0.9999314  *       
  1.006184e+00  L1    0.06695707  0.9988698          
  2.010401e+00  L1    0.06632690  0.9988664          
  3.367290e+00  L2    0.05542080  0.9999233          
  1.301149e+01  L1    0.06590978  0.9988604          
  3.993799e+01  L1    0.06585878  0.9988596          
  5.225559e+01  L1    0.06585298  0.9988595          
  1.060829e+02  L2    0.05609579  0.9999285          
  6.543469e+02  L1    0.06583567  0.9988593          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were cost = 0.7758052 and Loss = L2.
[1] "Wed Feb 28 15:33:06 2018"
Support Vector Machines with Polynomial Kernel 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  degree  scale         C             RMSE        Rsquared   Selected
  1       1.173771e-04    1.04041165  1.38878962  0.9998105          
  1       1.240996e-04    5.17394154  1.07045265  0.9997747          
  1       4.742226e-03    2.13178535  0.08368510  0.9999393          
  1       3.903391e-02  199.98330693  0.07264553  0.9998961          
  1       8.357493e-02    0.90691274  0.06971432  0.9999293          
  1       2.298744e-01   65.64622310  0.07249249  0.9998964          
  1       2.848453e-01    0.26852150  0.06966090  0.9999286          
  1       2.919999e-01    0.05437496  0.07616011  0.9999353          
  1       3.049155e-01    2.22940480  0.07261214  0.9998961          
  1       1.693037e+00    0.73306514  0.07253549  0.9998967          
  2       1.216065e-05   44.93004999  0.80012339  0.9998137          
  2       5.722778e-04    0.06398820  1.41881665  0.9997978          
  2       5.517903e-03   20.12722573  0.06317030  0.9999350          
  2       1.582443e-02   91.19606139  0.05819401  0.9998832  *       
  2       7.938402e-02    0.08322075  0.07689492  0.9999792          
  3       8.788796e-05  123.55196468  0.07230139  0.9999397          
  3       1.457030e-04    1.07798592  1.17468907  0.9997647          
  3       3.379256e-03    1.09558124  0.08348939  0.9999487          
  3       3.576369e-03    0.27844065  0.17450292  0.9999403          
  3       1.656064e+00    3.60857396  0.09249680  0.9982147          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were degree = 2, scale = 0.01582443 and C
 = 91.19606.
[1] "Wed Feb 28 15:33:09 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Support Vector Machines with Radial Basis Function Kernel 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  sigma        C             RMSE        Rsquared   Selected
   0.01160185    0.12446302  0.23849854  0.9989669          
   0.01819868  973.72298375  0.08454209  0.9984871  *       
   0.01998248  113.05170282  0.08484504  0.9984679          
   0.02578474   28.79022210  0.08619073  0.9983756          
   0.03040905    0.82095574  0.10430004  0.9953522          
   0.04944290    4.65913108  0.09535566  0.9972713          
   0.08214541    0.07478430  0.26358034  0.9745990          
   0.09073397    0.80505185  0.12256989  0.9933417          
   0.31405740    0.10876951  0.32647024  0.9538890          
   0.33973209    4.32883809  0.11961071  0.9938749          
   0.79267204    0.34892679  0.28763144  0.9680233          
   1.09338734    0.14352144  0.45099370  0.9275705          
   1.23420224    0.44570201  0.31934376  0.9615644          
   2.19415883   22.31230070  0.30286081  0.9628497          
   2.30613835  260.99134221  0.31000415  0.9610595          
   6.85717505    0.35135707  0.66088049  0.8306176          
   9.86576243  106.31521082  0.52963538  0.8870222          
  12.31432555    0.28914385  0.85844669  0.7110809          
  13.30584915    0.09123236  1.11347911  0.5146005          
  13.60895421    0.27459030  0.89592565  0.6839944          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were sigma = 0.01819868 and C = 973.723.
[1] "Wed Feb 28 15:33:12 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  C             RMSE       Rsquared   Selected
    0.03633906  0.7395157  0.8074030          
    0.03704350  0.7836959  0.7804055          
    0.06068093  0.6437966  0.8548283          
    0.10808368  0.5309924  0.8995116          
    0.11032971  0.5473798  0.8924871          
    0.12230209  0.5204380  0.9033230          
    0.12750948  0.4964894  0.9126004          
    0.45247966  0.3030983  0.9644834          
    0.56059952  0.2710281  0.9701889          
    0.66120067  0.3005167  0.9661560          
    2.56993758  0.2109759  0.9830796  *       
    4.67616146  0.2223591  0.9803342          
    5.68306973  0.2340098  0.9781779          
    9.55074044  0.2410979  0.9764918          
   14.06162094  0.2540326  0.9727885          
   38.75433012  0.2455353  0.9757761          
   89.86997995  0.2244521  0.9790447          
  109.94476883  0.2261723  0.9791252          
  186.98601832  0.2495653  0.9746503          
  731.86511892  0.2148660  0.9817384          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was C = 2.569938.
[1] "Wed Feb 28 15:33:15 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  sigma        C             RMSE        Rsquared   Selected
   0.01160185    0.12446302  0.23849854  0.9989669          
   0.01819868  973.72298375  0.08454209  0.9984871  *       
   0.01998248  113.05170282  0.08484504  0.9984679          
   0.02578474   28.79022210  0.08619073  0.9983756          
   0.03040905    0.82095574  0.10430004  0.9953522          
   0.04944290    4.65913108  0.09535566  0.9972713          
   0.08214541    0.07478430  0.26358034  0.9745990          
   0.09073397    0.80505185  0.12256989  0.9933417          
   0.31405740    0.10876951  0.32647024  0.9538890          
   0.33973209    4.32883809  0.11961071  0.9938749          
   0.79267204    0.34892679  0.28763144  0.9680233          
   1.09338734    0.14352144  0.45099370  0.9275705          
   1.23420224    0.44570201  0.31934376  0.9615644          
   2.19415883   22.31230070  0.30286081  0.9628497          
   2.30613835  260.99134221  0.31000415  0.9610595          
   6.85717505    0.35135707  0.66088049  0.8306176          
   9.86576243  106.31521082  0.52963538  0.8870222          
  12.31432555    0.28914385  0.85844669  0.7110809          
  13.30584915    0.09123236  1.11347911  0.5146005          
  13.60895421    0.27459030  0.89592565  0.6839944          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were sigma = 0.01819868 and C = 973.723.
[1] "Wed Feb 28 15:33:18 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Wed Feb 28 15:33:20 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "YeoJohnson"               "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "svmSpectrumString"       
Loading required package: ipred
Bagged CART 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results:

  RMSE       Rsquared 
  0.4098552  0.9318749

[1] "Wed Feb 28 15:33:23 2018"
Loading required package: rpart
Partial Least Squares 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results:

  RMSE        Rsquared 
  0.08282632  0.9967928

Tuning parameter 'ncomp' was held constant at a value of 1
[1] "Wed Feb 28 15:33:30 2018"
Loading required package: frbs
Wang and Mendel Fuzzy Rules 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  num.labels  type.mf    RMSE       Rsquared   Selected
   2          TRIANGLE   1.2304047  0.4947484          
   3          TRIANGLE   1.3081760  0.4170248          
   4          TRIANGLE   0.6591133  0.8138804          
   6          TRAPEZOID  0.4656623  0.9422404          
   7          GAUSSIAN   0.3771258  0.9369299          
  10          TRAPEZOID  0.3444507  0.9649843          
  11          GAUSSIAN   0.2379725  0.9751658          
  11          TRIANGLE   0.3147052  0.9547693          
  12          GAUSSIAN   0.2345473  0.9752477          
  13          TRAPEZOID  0.3402143  0.9572004          
  15          TRAPEZOID  0.4007252  0.9297116          
  16          GAUSSIAN   0.2017559  0.9815239          
  16          TRAPEZOID  0.3788850  0.9404690          
  17          TRIANGLE   0.3906811  0.9287316          
  20          GAUSSIAN   0.1690700  0.9870005  *       

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were num.labels = 20 and type.mf = GAUSSIAN.
[1] "Wed Feb 28 15:35:54 2018"
Loading required package: xgboost
eXtreme Gradient Boosting 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  lambda        alpha         nrounds  eta         RMSE       Rsquared 
  1.181830e-05  2.440589e-02  85       0.65112877  0.1618705  0.9887524
  1.207225e-05  1.628571e-01   6       2.27236241  0.3075663  0.9798380
  2.085110e-05  5.004409e-02  33       1.11608236  0.1600124  0.9889353
  3.951302e-05  8.545665e-01  31       2.05212445  0.1877427  0.9840810
  4.042324e-05  1.590907e-01  21       0.72618724  0.1768085  0.9862065
  4.530787e-05  1.299614e-01  74       0.87751718  0.1614387  0.9884891
  4.744882e-05  1.696431e-01  42       0.47190148  0.1694494  0.9872588
  1.928894e-04  3.342112e-03  41       0.75629283  0.1700174  0.9873660
  2.445386e-04  1.075619e-04  50       0.41920808  0.1699029  0.9873903
  2.935757e-04  1.020575e-04  34       0.97365807  0.1704938  0.9873057
  1.320012e-03  3.855464e-03  63       0.86853833  0.1675970  0.9877144
  2.561189e-03  4.767368e-02  10       0.01818764  0.1882584  0.9870959
  3.178509e-03  4.547760e-04   7       0.82107289  0.2463387  0.9845541
  5.647690e-03  1.202631e-05  70       0.84817048  0.1655421  0.9880851
  8.667563e-03  1.041472e-02  77       2.77510629  0.1652786  0.9882715
  2.663352e-02  2.561178e-03  22       0.52811924  0.1714930  0.9871724
  6.759623e-02  1.251407e-04  35       1.42137600  0.1693476  0.9876784
  8.450422e-02  2.427821e-03  35       1.10417955  0.1719046  0.9872546
  1.521466e-01  8.369526e-01  46       1.63335997  0.1836809  0.9847910
  6.894107e-01  7.768307e-05  80       1.31960771  0.1646567  0.9879800
  Selected
          
          
  *       
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 33, lambda =
 2.08511e-05, alpha = 0.05004409 and eta = 1.116082.
[1] "Wed Feb 28 15:36:06 2018"
eXtreme Gradient Boosting 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  eta         max_depth  gamma       colsample_bytree  min_child_weight
  0.03291018   9         7.57454136  0.5214813          9              
  0.04228906   4         2.73690963  0.4961101          6              
  0.05742928   8         0.06062547  0.3140025          0              
  0.12491751   9         2.42062412  0.5544964          0              
  0.12700731   5         1.76039746  0.3601712         17              
  0.18277707  10         6.84041482  0.6561833          6              
  0.19503749   8         3.72027454  0.5565982          2              
  0.20294903   3         3.24552690  0.5123897         16              
  0.20499298   3         4.73791999  0.4567804         16              
  0.20592575   5         3.68059849  0.4971717          6              
  0.24427635   6         2.52097611  0.5306055          0              
  0.24685590   9         1.57300492  0.6574144         20              
  0.27460039  10         5.44453324  0.5409947         11              
  0.29535913   3         1.39736026  0.5509788         10              
  0.37362099   6         2.89512777  0.4501132         19              
  0.41988504   1         2.82723493  0.4589130         16              
  0.44172983   9         2.92505726  0.3737611          9              
  0.46066859   7         9.25035428  0.6057234          3              
  0.47816237   2         4.39869236  0.3901790         18              
  0.50590652   7         2.17042925  0.6677189          2              
  subsample  nrounds  RMSE       Rsquared   Selected
  0.4041324   17      1.1685606  0.9332084          
  0.4831966  501            NaN        NaN          
  0.8479422  482            NaN        NaN          
  0.4168563  122      0.2688930  0.9781351          
  0.8706329  686            NaN        NaN          
  0.9023369  120      0.3014297  0.9790654          
  0.9803359   64      0.2781652  0.9797235          
  0.7279396  294      0.2482427  0.9818569          
  0.5214505  766            NaN        NaN          
  0.3822459  786            NaN        NaN          
  0.4641288  258      0.2514656  0.9788392          
  0.9481535  136      0.2294479  0.9810420  *       
  0.3122063  837      0.3220899  0.9642075          
  0.3371397  278      0.2306092  0.9763438          
  0.8302152  425            NaN        NaN          
  0.3790764  551            NaN        NaN          
  0.8105079  132            NaN        NaN          
  0.6968413  588      0.3416694  0.9535703          
  0.4078663  968            NaN        NaN          
  0.5777972   15      0.3922189  0.9209244          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 136, max_depth = 9, eta
 = 0.2468559, gamma = 1.573005, colsample_bytree = 0.6574144,
 min_child_weight = 20 and subsample = 0.9481535.
[1] "Wed Feb 28 15:36:31 2018"
Loading required package: kohonen
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :24    NA's   :24   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Wed Feb 28 15:36:43 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "YeoJohnson"               "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "xyf"                     
Loading required package: nnet
Fitting Repeat 1 

# weights:  21
initial  value 1174.031236 
iter  10 value 1063.824227
iter  20 value 696.261515
iter  30 value 688.335544
iter  40 value 687.695975
iter  50 value 687.561439
iter  60 value 687.514795
iter  70 value 687.398232
iter  80 value 687.386475
iter  90 value 687.385948
final  value 687.385914 
converged
Fitting Repeat 2 

# weights:  21
initial  value 1128.765580 
iter  10 value 1063.184460
iter  20 value 694.470531
iter  30 value 688.739192
iter  40 value 688.129034
iter  50 value 687.607625
iter  60 value 687.482032
iter  70 value 687.396254
iter  80 value 687.387914
iter  90 value 687.385983
final  value 687.385880 
converged
Fitting Repeat 3 

# weights:  21
initial  value 1260.651036 
iter  10 value 1063.759151
iter  20 value 689.753466
iter  30 value 687.572434
iter  40 value 687.362627
iter  50 value 687.288253
iter  60 value 687.255464
iter  70 value 687.245141
final  value 687.244932 
converged
Fitting Repeat 4 

# weights:  21
initial  value 1269.461821 
iter  10 value 694.859627
iter  20 value 688.666723
iter  30 value 687.557017
iter  40 value 687.426014
iter  50 value 687.384101
iter  60 value 687.284642
iter  70 value 687.276934
final  value 687.276802 
converged
Fitting Repeat 5 

# weights:  21
initial  value 1218.592985 
iter  10 value 1063.591216
iter  20 value 798.354190
iter  30 value 703.352135
iter  40 value 690.048617
iter  50 value 688.464031
iter  60 value 688.202624
iter  70 value 687.525060
iter  80 value 687.389298
iter  90 value 687.386972
iter 100 value 687.386338
final  value 687.386338 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  69
initial  value 1338.561490 
iter  10 value 801.120763
iter  20 value 754.002383
iter  30 value 749.771735
iter  40 value 749.264139
iter  50 value 749.220065
iter  60 value 749.203484
iter  70 value 749.190659
final  value 749.190560 
converged
Fitting Repeat 2 

# weights:  69
initial  value 1311.805772 
iter  10 value 868.078063
iter  20 value 849.228241
iter  30 value 848.795182
iter  40 value 848.782038
iter  50 value 848.773935
final  value 848.773783 
converged
Fitting Repeat 3 

# weights:  69
initial  value 1182.949592 
iter  10 value 937.340972
iter  20 value 857.623372
iter  30 value 851.514604
iter  40 value 850.926053
iter  50 value 850.759233
iter  60 value 850.644907
iter  70 value 850.600072
iter  80 value 850.592366
final  value 850.592155 
converged
Fitting Repeat 4 

# weights:  69
initial  value 1347.233891 
iter  10 value 800.092939
iter  20 value 751.830055
iter  30 value 747.294367
iter  40 value 747.026927
iter  50 value 746.839264
iter  60 value 746.806593
final  value 746.805896 
converged
Fitting Repeat 5 

# weights:  69
initial  value 1143.892752 
iter  10 value 843.373133
iter  20 value 800.210263
iter  30 value 796.877476
iter  40 value 796.711199
iter  50 value 796.678186
iter  60 value 796.673035
iter  70 value 796.668587
final  value 796.667569 
converged
Fitting Repeat 1 

# weights:  25
initial  value 1106.471543 
iter  10 value 717.779630
iter  20 value 637.337122
iter  30 value 632.219333
iter  40 value 631.658528
iter  50 value 631.475539
iter  60 value 631.425729
iter  70 value 631.380395
iter  80 value 631.379727
final  value 631.379708 
converged
Fitting Repeat 2 

# weights:  25
initial  value 1108.823604 
iter  10 value 1046.349068
iter  20 value 715.208912
iter  30 value 654.204162
iter  40 value 646.898612
iter  50 value 646.541235
iter  60 value 646.480136
iter  70 value 646.476854
final  value 646.476664 
converged
Fitting Repeat 3 

# weights:  25
initial  value 1252.061795 
iter  10 value 751.133279
iter  20 value 691.332660
iter  30 value 686.397369
iter  40 value 685.993359
iter  50 value 685.952396
iter  60 value 685.939651
iter  70 value 685.934685
iter  80 value 685.932883
iter  80 value 685.932878
iter  80 value 685.932878
final  value 685.932878 
converged
Fitting Repeat 4 

# weights:  25
initial  value 1224.352084 
iter  10 value 782.862657
iter  20 value 710.592972
iter  30 value 702.583666
iter  40 value 701.294012
iter  50 value 700.974461
iter  60 value 700.845634
iter  70 value 700.814376
final  value 700.813793 
converged
Fitting Repeat 5 

# weights:  25
initial  value 1140.247383 
iter  10 value 685.065638
iter  20 value 683.063208
iter  30 value 682.745650
iter  40 value 682.647994
iter  50 value 682.627758
iter  60 value 682.616394
iter  70 value 682.611307
final  value 682.611211 
converged
Fitting Repeat 1 

# weights:  41
initial  value 1150.489347 
iter  10 value 1048.116049
iter  20 value 1046.944735
iter  30 value 705.762813
iter  40 value 703.388447
iter  50 value 703.044314
iter  60 value 702.643780
iter  70 value 702.584905
iter  80 value 702.568348
iter  90 value 702.559739
iter 100 value 702.553490
final  value 702.553490 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  41
initial  value 1168.016952 
iter  10 value 956.589834
iter  20 value 954.406644
iter  30 value 954.349218
iter  40 value 623.089525
iter  50 value 621.144292
iter  60 value 613.928180
iter  70 value 613.124880
iter  80 value 612.716300
iter  90 value 612.256951
iter 100 value 611.942230
final  value 611.942230 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  41
initial  value 1113.771587 
iter  10 value 978.954144
iter  20 value 976.362735
iter  30 value 650.108322
iter  40 value 647.251771
iter  50 value 644.862480
iter  60 value 641.837490
iter  70 value 632.837524
iter  80 value 629.000499
iter  90 value 626.964200
iter 100 value 626.383501
final  value 626.383501 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  41
initial  value 1180.819129 
iter  10 value 1007.336421
iter  20 value 1005.128102
iter  30 value 670.834060
iter  40 value 670.275217
iter  50 value 666.916911
iter  60 value 663.566510
iter  70 value 660.169295
iter  80 value 659.277494
iter  90 value 658.675335
iter 100 value 658.285996
final  value 658.285996 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  41
initial  value 1144.944211 
iter  10 value 694.446808
iter  20 value 691.606957
iter  30 value 689.324313
iter  40 value 687.455304
iter  50 value 686.976004
iter  60 value 686.919923
iter  70 value 686.858649
iter  80 value 686.819512
iter  90 value 686.804764
iter 100 value 686.800345
final  value 686.800345 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  17
initial  value 1213.643826 
iter  10 value 1116.065443
iter  20 value 1116.043122
iter  30 value 1116.042986
iter  40 value 1116.042838
iter  50 value 1116.042675
iter  60 value 1116.042493
iter  70 value 1116.042290
iter  80 value 1116.042063
iter  90 value 1116.041804
iter 100 value 1116.041505
final  value 1116.041505 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  17
initial  value 1126.605695 
iter  10 value 1080.518687
final  value 1080.509355 
converged
Fitting Repeat 3 

# weights:  17
initial  value 1075.704583 
iter  10 value 972.445355
iter  20 value 972.406659
iter  30 value 972.406264
iter  40 value 972.405693
iter  50 value 972.404836
iter  60 value 972.403481
iter  70 value 972.401227
iter  80 value 972.397179
iter  90 value 972.388765
iter 100 value 972.363406
final  value 972.363406 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  17
initial  value 1342.288196 
iter  10 value 1105.435353
final  value 1105.400639 
converged
Fitting Repeat 5 

# weights:  17
initial  value 1198.513270 
iter  10 value 1017.646362
iter  20 value 1017.562050
iter  30 value 1017.556068
iter  40 value 1017.525257
iter  50 value 661.387899
iter  60 value 656.391864
iter  70 value 656.191416
iter  80 value 656.179787
final  value 656.179229 
converged
Fitting Repeat 1 

# weights:  21
initial  value 1088.213832 
iter  10 value 1063.318366
iter  20 value 697.216517
iter  30 value 691.516998
iter  40 value 690.892104
iter  50 value 690.482694
iter  60 value 690.394828
final  value 690.388753 
converged
Fitting Repeat 2 

# weights:  21
initial  value 1077.461007 
iter  10 value 704.268710
iter  20 value 690.051244
iter  30 value 689.358136
iter  40 value 689.178594
iter  50 value 689.155297
final  value 689.154757 
converged
Fitting Repeat 3 

# weights:  21
initial  value 1151.337926 
iter  10 value 1064.620461
iter  20 value 740.299697
iter  30 value 693.269549
iter  40 value 690.305256
iter  50 value 689.552773
iter  60 value 689.329124
iter  70 value 689.155162
final  value 689.154758 
converged
Fitting Repeat 4 

# weights:  21
initial  value 1149.607175 
iter  10 value 1038.871522
iter  20 value 697.565204
iter  30 value 691.197952
iter  40 value 690.435099
iter  50 value 690.396210
iter  60 value 690.388788
final  value 690.388755 
converged
Fitting Repeat 5 

# weights:  21
initial  value 1271.377904 
iter  10 value 1066.745281
iter  20 value 701.130078
iter  30 value 690.651060
iter  40 value 690.483388
iter  50 value 690.390720
iter  60 value 690.388776
iter  60 value 690.388771
iter  60 value 690.388771
final  value 690.388771 
converged
Fitting Repeat 1 

# weights:  73
initial  value 1165.462587 
iter  10 value 991.369077
iter  20 value 723.323472
iter  30 value 692.728750
iter  40 value 688.208302
iter  50 value 687.845018
iter  60 value 687.746066
iter  70 value 687.718697
iter  80 value 687.692478
iter  90 value 687.681087
iter 100 value 687.679042
final  value 687.679042 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  73
initial  value 1073.375686 
iter  10 value 689.105894
iter  20 value 687.877909
iter  30 value 687.740299
iter  40 value 687.700973
iter  50 value 687.688011
iter  60 value 687.677726
iter  70 value 687.675752
final  value 687.675418 
converged
Fitting Repeat 3 

# weights:  73
initial  value 1140.608512 
iter  10 value 1041.740942
iter  20 value 691.544820
iter  30 value 688.159682
iter  40 value 687.918197
iter  50 value 687.830398
iter  60 value 687.771874
iter  70 value 687.738360
iter  80 value 687.723269
iter  90 value 687.704188
iter 100 value 687.695539
final  value 687.695539 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  73
initial  value 1246.499927 
iter  10 value 1072.320737
iter  20 value 736.460267
iter  30 value 694.388219
iter  40 value 688.051701
iter  50 value 687.772541
iter  60 value 687.717876
iter  70 value 687.711369
iter  80 value 687.706337
iter  90 value 687.704190
iter 100 value 687.702905
final  value 687.702905 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  73
initial  value 1154.287071 
iter  10 value 1066.851255
iter  20 value 710.888316
iter  30 value 691.409000
iter  40 value 687.950431
iter  50 value 687.738464
iter  60 value 687.686469
iter  70 value 687.678851
iter  80 value 687.677511
iter  90 value 687.676595
iter 100 value 687.675655
final  value 687.675655 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  49
initial  value 1195.458904 
iter  10 value 985.758947
iter  20 value 980.179415
iter  30 value 680.088035
iter  40 value 678.749426
iter  50 value 671.623310
iter  60 value 668.427672
iter  70 value 666.426560
iter  80 value 665.851375
iter  90 value 665.623223
iter 100 value 665.485215
final  value 665.485215 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  49
initial  value 1253.176185 
iter  10 value 1050.458662
iter  20 value 1046.481049
iter  30 value 684.851993
iter  40 value 677.396251
iter  50 value 676.468480
iter  60 value 676.230554
iter  70 value 676.136134
iter  80 value 676.081039
iter  90 value 676.052157
iter 100 value 676.037119
final  value 676.037119 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  49
initial  value 1301.565240 
iter  10 value 1080.834406
iter  20 value 1077.981743
iter  30 value 712.021592
iter  40 value 706.219609
iter  50 value 703.636409
iter  60 value 701.122704
iter  70 value 700.698402
iter  80 value 700.256988
iter  90 value 700.106322
iter 100 value 700.032081
final  value 700.032081 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  49
initial  value 1295.559915 
iter  10 value 1083.142508
iter  20 value 1081.050579
iter  30 value 815.676665
iter  40 value 727.526947
iter  50 value 716.936498
iter  60 value 711.782886
iter  70 value 710.341809
iter  80 value 710.033431
iter  90 value 709.856461
iter 100 value 709.783708
final  value 709.783708 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  49
initial  value 1414.529994 
iter  10 value 1125.480112
iter  20 value 1122.180387
iter  30 value 789.825351
iter  40 value 764.807995
iter  50 value 760.508789
iter  60 value 750.045193
iter  70 value 749.499726
iter  80 value 749.405947
iter  90 value 749.339556
iter 100 value 749.320640
final  value 749.320640 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  13
initial  value 1174.687147 
iter  10 value 1061.763041
iter  20 value 1061.709447
iter  30 value 695.665037
iter  40 value 693.837258
iter  50 value 693.653978
iter  60 value 693.291111
iter  70 value 689.374872
iter  80 value 685.559348
iter  90 value 685.365036
iter 100 value 685.024515
final  value 685.024515 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  13
initial  value 1238.549693 
iter  10 value 1061.788818
iter  20 value 1061.703919
iter  30 value 1061.674829
iter  40 value 812.449681
iter  50 value 685.373766
iter  60 value 684.857423
iter  70 value 684.832407
iter  80 value 684.831137
iter  90 value 684.823730
iter 100 value 684.822915
final  value 684.822915 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  13
initial  value 1164.153890 
iter  10 value 1061.750240
iter  20 value 1061.702332
iter  30 value 1061.702154
iter  40 value 1061.701946
iter  50 value 1061.701702
iter  60 value 1061.701410
iter  70 value 1061.701054
iter  80 value 1061.700612
iter  90 value 1061.700050
iter 100 value 1061.699313
final  value 1061.699313 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  13
initial  value 1249.784212 
iter  10 value 1061.773920
final  value 1061.705288 
converged
Fitting Repeat 5 

# weights:  13
initial  value 1283.205939 
iter  10 value 1061.763237
final  value 1061.702617 
converged
Fitting Repeat 1 

# weights:  61
initial  value 1117.819323 
iter  10 value 1063.435381
iter  20 value 693.097803
iter  30 value 688.561996
iter  40 value 687.416642
iter  50 value 686.797596
iter  60 value 686.601426
iter  70 value 686.511049
iter  80 value 686.491752
iter  90 value 686.484627
iter 100 value 686.477574
final  value 686.477574 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  61
initial  value 1193.842899 
iter  10 value 690.210052
iter  20 value 687.011601
iter  30 value 686.646382
iter  40 value 686.571615
iter  50 value 686.525927
iter  60 value 686.509840
iter  70 value 686.499023
iter  80 value 686.495484
iter  90 value 686.493594
iter 100 value 686.492736
final  value 686.492736 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  61
initial  value 1250.667276 
iter  10 value 719.084930
iter  20 value 690.919239
iter  30 value 687.970182
iter  40 value 687.128725
iter  50 value 686.862343
iter  60 value 686.690501
iter  70 value 686.593431
iter  80 value 686.541194
iter  90 value 686.522042
iter 100 value 686.503212
final  value 686.503212 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  61
initial  value 1148.363202 
iter  10 value 1061.951746
iter  20 value 704.425186
iter  30 value 691.271499
iter  40 value 687.450557
iter  50 value 686.756260
iter  60 value 686.553310
iter  70 value 686.492612
iter  80 value 686.475705
iter  90 value 686.469254
iter 100 value 686.468572
final  value 686.468572 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  61
initial  value 1432.878927 
iter  10 value 1063.759925
iter  20 value 697.779426
iter  30 value 689.803139
iter  40 value 686.837777
iter  50 value 686.567984
iter  60 value 686.497284
iter  70 value 686.481049
iter  80 value 686.474858
iter  90 value 686.471457
iter 100 value 686.469002
final  value 686.469002 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  25
initial  value 1092.734076 
iter  10 value 832.340315
iter  20 value 794.341367
iter  30 value 792.206185
iter  40 value 790.404001
final  value 790.396231 
converged
Fitting Repeat 2 

# weights:  25
initial  value 1136.889175 
iter  10 value 819.495677
iter  20 value 796.647668
iter  30 value 795.879356
final  value 795.875672 
converged
Fitting Repeat 3 

# weights:  25
initial  value 1128.745999 
iter  10 value 828.944056
iter  20 value 819.799280
iter  30 value 819.236724
iter  40 value 819.234006
final  value 819.233996 
converged
Fitting Repeat 4 

# weights:  25
initial  value 1248.877269 
iter  10 value 848.791259
iter  20 value 791.468190
iter  30 value 790.407858
final  value 790.396240 
converged
Fitting Repeat 5 

# weights:  25
initial  value 1074.755982 
iter  10 value 798.113491
iter  20 value 790.559963
iter  30 value 790.400984
final  value 790.396230 
converged
Fitting Repeat 1 

# weights:  33
initial  value 1248.798832 
iter  10 value 711.812816
iter  20 value 699.785505
iter  30 value 696.702229
iter  40 value 696.117677
iter  50 value 695.326065
iter  60 value 695.073733
iter  70 value 695.066356
final  value 695.066233 
converged
Fitting Repeat 2 

# weights:  33
initial  value 1180.797382 
iter  10 value 702.302385
iter  20 value 695.005887
iter  30 value 694.744304
iter  40 value 694.721604
final  value 694.720985 
converged
Fitting Repeat 3 

# weights:  33
initial  value 1151.582085 
iter  10 value 705.210003
iter  20 value 695.270582
iter  30 value 695.017258
iter  40 value 694.967751
iter  50 value 694.956340
iter  60 value 694.954870
final  value 694.954847 
converged
Fitting Repeat 4 

# weights:  33
initial  value 1072.091851 
iter  10 value 699.549300
iter  20 value 695.127024
iter  30 value 694.740956
iter  40 value 694.739728
final  value 694.739643 
converged
Fitting Repeat 5 

# weights:  33
initial  value 1197.730132 
iter  10 value 734.702932
iter  20 value 697.310368
iter  30 value 696.160424
iter  40 value 695.589560
iter  50 value 695.160470
iter  60 value 695.087023
iter  70 value 695.066295
final  value 695.066199 
converged
Fitting Repeat 1 

# weights:  61
initial  value 955.904425 
iter  10 value 633.239510
iter  20 value 618.588869
iter  30 value 614.790764
iter  40 value 612.731581
iter  50 value 612.524399
iter  60 value 612.292384
iter  70 value 612.076082
iter  80 value 611.953770
iter  90 value 611.911058
iter 100 value 611.884609
final  value 611.884609 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  61
initial  value 1510.747969 
iter  10 value 1157.799505
iter  20 value 1156.284813
iter  30 value 907.775130
iter  40 value 764.418731
iter  50 value 761.929611
iter  60 value 756.473911
iter  70 value 756.286688
iter  80 value 756.225410
iter  90 value 756.181312
iter 100 value 756.132430
final  value 756.132430 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  61
initial  value 958.990588 
iter  10 value 931.139053
iter  20 value 547.836841
iter  30 value 547.251382
iter  40 value 546.910236
iter  50 value 546.624822
iter  60 value 546.544523
iter  70 value 546.498217
iter  80 value 546.478160
iter  90 value 546.463002
iter 100 value 546.454858
final  value 546.454858 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  61
initial  value 1486.995585 
iter  10 value 1107.910533
iter  20 value 1106.381371
iter  30 value 1106.281339
iter  40 value 759.644951
iter  50 value 756.721527
iter  60 value 756.646135
iter  70 value 756.535048
iter  80 value 756.485143
iter  90 value 756.455442
iter 100 value 756.419065
final  value 756.419065 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  61
initial  value 1065.743201 
iter  10 value 1043.313694
iter  20 value 1043.201434
iter  30 value 702.154729
iter  40 value 697.623240
iter  50 value 696.349783
iter  60 value 693.217391
iter  70 value 690.697900
iter  80 value 688.205046
iter  90 value 687.515386
iter 100 value 686.429192
final  value 686.429192 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  37
initial  value 1243.841603 
iter  10 value 870.437915
iter  20 value 730.322396
iter  30 value 717.805092
iter  40 value 714.876913
iter  50 value 714.094626
iter  60 value 713.694587
iter  70 value 713.641979
iter  80 value 713.610252
iter  90 value 713.600175
final  value 713.600020 
converged
Fitting Repeat 2 

# weights:  37
initial  value 1226.913298 
iter  10 value 769.749319
iter  20 value 651.938036
iter  30 value 650.657913
iter  40 value 650.497751
iter  50 value 650.490374
iter  60 value 650.489841
final  value 650.489789 
converged
Fitting Repeat 3 

# weights:  37
initial  value 1242.913802 
iter  10 value 857.794138
iter  20 value 643.599845
iter  30 value 637.398485
iter  40 value 636.372660
iter  50 value 636.173300
iter  60 value 636.098826
iter  70 value 636.090630
iter  80 value 636.082970
iter  90 value 636.066638
final  value 636.066263 
converged
Fitting Repeat 4 

# weights:  37
initial  value 1159.846974 
iter  10 value 927.273851
iter  20 value 688.930740
iter  30 value 679.681417
iter  40 value 679.131311
iter  50 value 679.065820
iter  60 value 679.044874
iter  70 value 679.041846
final  value 679.041786 
converged
Fitting Repeat 5 

# weights:  37
initial  value 1215.445256 
iter  10 value 888.914692
iter  20 value 650.260840
iter  30 value 646.678305
iter  40 value 645.920126
iter  50 value 645.425120
iter  60 value 645.352678
iter  70 value 645.350596
iter  80 value 645.350233
final  value 645.350182 
converged
Fitting Repeat 1 

# weights:  41
initial  value 1125.146385 
iter  10 value 1062.307187
iter  20 value 709.386029
iter  30 value 691.188154
iter  40 value 687.892002
iter  50 value 687.060656
iter  60 value 686.657637
iter  70 value 686.433150
iter  80 value 686.292117
iter  90 value 686.241589
iter 100 value 686.223520
final  value 686.223520 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  41
initial  value 1189.655223 
iter  10 value 1062.681593
iter  20 value 726.613651
iter  30 value 699.540634
iter  40 value 689.021552
iter  50 value 687.316762
iter  60 value 686.550399
iter  70 value 686.347349
iter  80 value 686.282498
iter  90 value 686.235992
iter 100 value 686.197139
final  value 686.197139 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  41
initial  value 1123.731121 
iter  10 value 1062.682607
iter  20 value 706.072784
iter  30 value 693.668565
iter  40 value 689.485286
iter  50 value 687.503023
iter  60 value 686.530616
iter  70 value 686.330563
iter  80 value 686.266243
iter  90 value 686.223660
iter 100 value 686.205344
final  value 686.205344 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  41
initial  value 1044.371324 
iter  10 value 790.948962
iter  20 value 723.300104
iter  30 value 698.546715
iter  40 value 688.522621
iter  50 value 686.642899
iter  60 value 686.330598
iter  70 value 686.251908
iter  80 value 686.207548
iter  90 value 686.192689
iter 100 value 686.181073
final  value 686.181073 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  41
initial  value 1409.167069 
iter  10 value 1061.845039
iter  20 value 686.989761
iter  30 value 686.408084
iter  40 value 686.292767
iter  50 value 686.275457
iter  60 value 686.244657
iter  70 value 686.212829
iter  80 value 686.195909
iter  90 value 686.187384
iter 100 value 686.180801
final  value 686.180801 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  81
initial  value 1119.993558 
iter  10 value 981.260521
iter  20 value 976.729105
iter  30 value 630.229970
iter  40 value 625.752269
iter  50 value 619.553767
iter  60 value 619.165911
iter  70 value 618.943405
iter  80 value 618.730820
iter  90 value 618.630219
iter 100 value 618.570784
final  value 618.570784 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  81
initial  value 1074.900740 
iter  10 value 1013.280778
iter  20 value 1004.304199
iter  30 value 607.048922
iter  40 value 603.055129
iter  50 value 597.486263
iter  60 value 596.325584
iter  70 value 596.182849
iter  80 value 596.042874
iter  90 value 595.968836
iter 100 value 595.930146
final  value 595.930146 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  81
initial  value 1214.845135 
iter  10 value 1015.930242
iter  20 value 1011.200158
iter  30 value 683.298373
iter  40 value 680.884024
iter  50 value 680.694493
iter  60 value 680.619571
iter  70 value 680.585044
iter  80 value 680.554872
iter  90 value 680.510849
iter 100 value 680.485437
final  value 680.485437 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  81
initial  value 1189.305244 
iter  10 value 950.393637
iter  20 value 944.786963
iter  30 value 944.556125
iter  40 value 676.811492
iter  50 value 658.740644
iter  60 value 640.108556
iter  70 value 638.990448
iter  80 value 638.850543
iter  90 value 638.726318
iter 100 value 638.689685
final  value 638.689685 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  81
initial  value 1140.330711 
iter  10 value 1096.311118
iter  20 value 1095.855082
iter  30 value 1095.850877
iter  40 value 1095.840739
iter  50 value 1094.937388
iter  60 value 715.293434
iter  70 value 713.064971
iter  80 value 712.923693
iter  90 value 712.749499
iter 100 value 712.719005
final  value 712.719005 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  61
initial  value 1333.911230 
iter  10 value 1083.860402
iter  20 value 1081.461255
iter  30 value 1081.419614
iter  40 value 773.894050
iter  50 value 768.127748
iter  60 value 767.895180
iter  70 value 767.622770
iter  80 value 767.581158
iter  90 value 767.550787
iter 100 value 767.515517
final  value 767.515517 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  61
initial  value 1174.198233 
iter  10 value 1013.311134
iter  20 value 1011.374134
iter  30 value 722.029646
iter  40 value 685.039370
iter  50 value 684.541381
iter  60 value 681.734108
iter  70 value 679.573688
iter  80 value 677.408375
iter  90 value 675.325195
iter 100 value 673.110677
final  value 673.110677 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  61
initial  value 1098.306771 
iter  10 value 1050.590991
iter  20 value 957.438341
iter  30 value 715.709198
iter  40 value 673.942390
iter  50 value 670.877269
iter  60 value 668.515957
iter  70 value 663.723800
iter  80 value 663.051113
iter  90 value 662.399426
iter 100 value 661.996730
final  value 661.996730 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  61
initial  value 1160.855745 
iter  10 value 986.561348
iter  20 value 984.695795
iter  30 value 772.715281
iter  40 value 629.187756
iter  50 value 628.875571
iter  60 value 628.702941
iter  70 value 628.630511
iter  80 value 628.551839
iter  90 value 628.465825
iter 100 value 628.428291
final  value 628.428291 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  61
initial  value 1040.262804 
iter  10 value 946.111019
iter  20 value 944.921652
iter  30 value 628.394781
iter  40 value 604.707961
iter  50 value 602.493661
iter  60 value 601.884885
iter  70 value 601.808775
iter  80 value 601.560020
iter  90 value 601.386525
iter 100 value 601.323708
final  value 601.323708 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  65
initial  value 1350.446878 
iter  10 value 1153.720110
iter  20 value 1153.271035
iter  30 value 739.963430
iter  40 value 728.941345
iter  50 value 728.790035
iter  60 value 728.684087
iter  70 value 726.782233
iter  80 value 726.371771
iter  90 value 725.893529
iter 100 value 723.058644
final  value 723.058644 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  65
initial  value 1041.168981 
iter  10 value 969.186073
iter  20 value 969.047915
iter  30 value 969.047754
iter  40 value 969.047552
iter  50 value 969.047291
iter  60 value 969.046933
iter  70 value 969.046402
iter  80 value 969.045515
iter  90 value 969.043696
iter 100 value 969.037833
final  value 969.037833 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  65
initial  value 1135.994484 
iter  10 value 1095.158801
iter  20 value 1094.910034
iter  30 value 1094.770244
iter  40 value 708.824414
iter  50 value 708.635695
iter  60 value 708.384536
iter  70 value 707.865591
iter  80 value 707.832443
iter  90 value 707.816761
iter 100 value 707.797263
final  value 707.797263 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  65
initial  value 1130.366453 
iter  10 value 951.789044
iter  20 value 951.300651
iter  30 value 951.228213
iter  40 value 653.209052
iter  50 value 650.977819
iter  60 value 644.721870
iter  70 value 643.736572
iter  80 value 643.705476
iter  90 value 643.684532
iter 100 value 643.670343
final  value 643.670343 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  65
initial  value 1163.218818 
iter  10 value 1024.834304
iter  20 value 1024.527691
final  value 1024.526322 
converged
Fitting Repeat 1 

# weights:  13
initial  value 1076.319791 
iter  10 value 779.244488
iter  20 value 666.378079
iter  30 value 665.712404
final  value 665.712229 
converged
Fitting Repeat 2 

# weights:  13
initial  value 1100.607426 
iter  10 value 715.015419
iter  20 value 709.868781
final  value 709.836598 
converged
Fitting Repeat 3 

# weights:  13
initial  value 1117.106175 
iter  10 value 782.716595
iter  20 value 759.231956
iter  30 value 755.866980
iter  40 value 755.758507
final  value 755.758456 
converged
Fitting Repeat 4 

# weights:  13
initial  value 1145.339246 
iter  10 value 744.099585
iter  20 value 742.749984
final  value 742.734071 
converged
Fitting Repeat 5 

# weights:  13
initial  value 1271.281054 
iter  10 value 872.994720
iter  20 value 788.227004
iter  30 value 786.288369
final  value 786.286666 
converged
Fitting Repeat 1 

# weights:  9
initial  value 1182.528881 
iter  10 value 838.805657
iter  20 value 785.366063
final  value 785.252608 
converged
Fitting Repeat 2 

# weights:  9
initial  value 1292.532873 
iter  10 value 846.363664
iter  20 value 785.270842
final  value 785.252600 
converged
Fitting Repeat 3 

# weights:  9
initial  value 1153.737745 
iter  10 value 798.416966
iter  20 value 785.253437
final  value 785.252600 
converged
Fitting Repeat 4 

# weights:  9
initial  value 1145.776626 
iter  10 value 830.392045
iter  20 value 785.312862
iter  30 value 785.252625
final  value 785.252599 
converged
Fitting Repeat 5 

# weights:  9
initial  value 1194.038946 
iter  10 value 855.836777
iter  20 value 785.271315
final  value 785.252600 
converged
Fitting Repeat 1 

# weights:  21
initial  value 1163.940720 
iter  10 value 997.746106
iter  20 value 650.970197
iter  30 value 639.446169
iter  40 value 638.341805
iter  50 value 638.129953
iter  60 value 638.068651
iter  70 value 638.043130
final  value 638.042633 
converged
Fitting Repeat 2 

# weights:  21
initial  value 1133.508758 
iter  10 value 996.842465
iter  20 value 657.966999
iter  30 value 640.365704
iter  40 value 639.265309
iter  50 value 639.096182
iter  60 value 638.928631
iter  70 value 638.823107
iter  80 value 638.802732
iter  90 value 638.801694
final  value 638.801634 
converged
Fitting Repeat 3 

# weights:  21
initial  value 1070.938718 
iter  10 value 996.477113
iter  20 value 678.063830
iter  30 value 640.567021
iter  40 value 638.767523
iter  50 value 638.236511
iter  60 value 638.066730
iter  70 value 638.047127
iter  80 value 638.043956
iter  90 value 638.042843
final  value 638.042634 
converged
Fitting Repeat 4 

# weights:  21
initial  value 1086.807957 
iter  10 value 996.826418
iter  20 value 665.416995
iter  30 value 643.269508
iter  40 value 640.267278
iter  50 value 639.605843
iter  60 value 639.227101
iter  70 value 638.852822
iter  80 value 638.805763
iter  90 value 638.802799
iter 100 value 638.801753
final  value 638.801753 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  21
initial  value 1122.402333 
iter  10 value 677.639744
iter  20 value 645.813150
iter  30 value 638.965540
iter  40 value 638.184062
iter  50 value 638.057798
iter  60 value 638.048968
iter  70 value 638.042692
final  value 638.042655 
converged
Fitting Repeat 1 

# weights:  69
initial  value 1468.560750 
iter  10 value 848.954541
iter  20 value 839.593093
iter  30 value 839.414666
iter  40 value 839.412062
iter  40 value 839.412056
iter  40 value 839.412054
final  value 839.412054 
converged
Fitting Repeat 2 

# weights:  69
initial  value 1225.919100 
iter  10 value 945.356662
iter  20 value 841.652808
iter  30 value 817.116593
iter  40 value 815.532299
iter  50 value 815.377850
iter  60 value 815.186608
iter  70 value 814.919443
iter  80 value 814.780338
final  value 814.772349 
converged
Fitting Repeat 3 

# weights:  69
initial  value 1449.289700 
iter  10 value 816.103892
iter  20 value 813.711321
iter  30 value 813.162818
iter  40 value 813.143442
final  value 813.143375 
converged
Fitting Repeat 4 

# weights:  69
initial  value 1254.415464 
iter  10 value 901.267095
iter  20 value 766.568732
iter  30 value 745.677993
iter  40 value 742.518962
iter  50 value 742.192153
iter  60 value 742.107875
iter  70 value 741.945081
iter  80 value 741.860220
final  value 741.858637 
converged
Fitting Repeat 5 

# weights:  69
initial  value 1400.182997 
iter  10 value 954.666936
iter  20 value 833.984942
iter  30 value 816.662832
iter  40 value 815.969582
iter  50 value 815.858796
iter  60 value 815.258305
iter  70 value 814.854773
iter  80 value 814.745386
final  value 814.745031 
converged
Fitting Repeat 1 

# weights:  25
initial  value 1322.474748 
iter  10 value 715.793496
iter  20 value 706.775513
iter  30 value 705.795591
iter  40 value 705.588403
iter  50 value 705.540692
iter  60 value 705.499381
iter  70 value 705.488820
final  value 705.488623 
converged
Fitting Repeat 2 

# weights:  25
initial  value 1042.074080 
iter  10 value 941.912227
iter  20 value 680.092886
iter  30 value 576.748026
iter  40 value 574.352793
iter  50 value 573.246927
iter  60 value 572.748393
iter  70 value 572.526887
iter  80 value 572.456560
iter  90 value 572.445368
final  value 572.444478 
converged
Fitting Repeat 3 

# weights:  25
initial  value 1116.103465 
iter  10 value 617.320255
iter  20 value 583.966057
iter  30 value 583.276750
iter  40 value 583.196367
iter  50 value 583.189016
iter  60 value 583.180338
iter  70 value 583.152608
iter  80 value 583.151851
iter  80 value 583.151845
iter  80 value 583.151845
final  value 583.151845 
converged
Fitting Repeat 4 

# weights:  25
initial  value 1001.621211 
iter  10 value 685.074280
iter  20 value 677.043307
iter  30 value 675.790497
iter  40 value 675.637497
iter  50 value 675.512542
iter  60 value 675.418744
iter  70 value 675.401416
final  value 675.401263 
converged
Fitting Repeat 5 

# weights:  25
initial  value 1344.178442 
iter  10 value 1055.398906
iter  20 value 722.542524
iter  30 value 700.504481
iter  40 value 698.539909
iter  50 value 698.214451
iter  60 value 698.129819
iter  70 value 698.063586
iter  80 value 698.027093
iter  90 value 698.025844
iter  90 value 698.025839
iter  90 value 698.025839
final  value 698.025839 
converged
Fitting Repeat 1 

# weights:  41
initial  value 1013.953163 
iter  10 value 961.922623
iter  20 value 596.143139
iter  30 value 594.691659
iter  40 value 594.423748
iter  50 value 594.334362
iter  60 value 594.311140
iter  70 value 594.292985
iter  80 value 594.285169
iter  90 value 594.276759
iter 100 value 594.273135
final  value 594.273135 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  41
initial  value 1125.929450 
iter  10 value 924.007961
iter  20 value 921.578307
iter  30 value 921.501607
iter  40 value 589.241569
iter  50 value 586.478880
iter  60 value 585.770737
iter  70 value 585.617644
iter  80 value 585.586815
iter  90 value 585.567188
iter 100 value 585.554523
final  value 585.554523 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  41
initial  value 1067.517311 
iter  10 value 964.210499
iter  20 value 963.154462
iter  30 value 677.124170
iter  40 value 604.584108
iter  50 value 603.037920
iter  60 value 602.880966
iter  70 value 602.859103
iter  80 value 602.848845
iter  90 value 602.843990
iter 100 value 602.840891
final  value 602.840891 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  41
initial  value 1110.072712 
iter  10 value 970.003252
iter  20 value 967.824508
iter  30 value 966.762185
iter  40 value 600.885304
iter  50 value 598.735023
iter  60 value 592.901642
iter  70 value 591.769484
iter  80 value 590.774934
iter  90 value 589.789740
iter 100 value 589.544674
final  value 589.544674 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  41
initial  value 1005.724059 
iter  10 value 956.512915
iter  20 value 762.892049
iter  30 value 612.657464
iter  40 value 612.151915
iter  50 value 611.858962
iter  60 value 611.687005
iter  70 value 611.620629
iter  80 value 611.587847
iter  90 value 611.574380
iter 100 value 611.560062
final  value 611.560062 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  17
initial  value 1014.643764 
iter  10 value 917.453078
final  value 917.445879 
converged
Fitting Repeat 2 

# weights:  17
initial  value 1134.273913 
iter  10 value 1009.224550
final  value 1009.181463 
converged
Fitting Repeat 3 

# weights:  17
initial  value 1120.454210 
iter  10 value 1056.361917
iter  20 value 1056.317158
iter  30 value 1056.268114
iter  40 value 727.862721
iter  50 value 714.740876
iter  60 value 714.260682
iter  70 value 714.223118
iter  80 value 714.201830
iter  90 value 714.191672
iter 100 value 714.187621
final  value 714.187621 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  17
initial  value 1046.410411 
iter  10 value 1009.687291
iter  20 value 1009.677611
iter  30 value 1009.677038
iter  40 value 1009.676324
iter  50 value 1009.675411
iter  60 value 1009.674197
iter  70 value 1009.672505
iter  80 value 1009.669977
iter  90 value 1009.665783
iter 100 value 1009.657467
final  value 1009.657467 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  17
initial  value 1091.313921 
iter  10 value 977.474330
iter  20 value 977.436232
iter  30 value 977.434963
iter  40 value 977.432825
iter  50 value 977.428688
iter  60 value 977.418096
iter  70 value 977.364577
iter  80 value 660.126731
iter  90 value 642.924147
iter 100 value 642.387948
final  value 642.387948 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  21
initial  value 1137.503262 
iter  10 value 653.985630
iter  20 value 641.615520
iter  30 value 640.161222
iter  40 value 639.824180
iter  50 value 639.768117
iter  60 value 639.744595
final  value 639.744565 
converged
Fitting Repeat 2 

# weights:  21
initial  value 979.469170 
iter  10 value 660.621328
iter  20 value 640.897745
iter  30 value 639.716904
iter  40 value 639.684131
iter  50 value 639.681365
final  value 639.681253 
converged
Fitting Repeat 3 

# weights:  21
initial  value 1074.356403 
iter  10 value 997.612215
iter  20 value 653.004929
iter  30 value 640.992276
iter  40 value 640.271302
iter  50 value 639.969726
final  value 639.953137 
converged
Fitting Repeat 4 

# weights:  21
initial  value 1012.591553 
iter  10 value 656.811208
iter  20 value 641.386922
iter  30 value 640.215571
iter  40 value 639.887063
iter  50 value 639.696070
final  value 639.681256 
converged
Fitting Repeat 5 

# weights:  21
initial  value 1164.668592 
iter  10 value 644.096712
iter  20 value 639.955055
iter  30 value 639.742554
iter  40 value 639.711510
iter  50 value 639.683772
final  value 639.681249 
converged
Fitting Repeat 1 

# weights:  73
initial  value 1069.880546 
iter  10 value 999.894305
iter  20 value 652.259442
iter  30 value 639.827197
iter  40 value 638.882300
iter  50 value 638.717651
iter  60 value 638.612844
iter  70 value 638.532619
iter  80 value 638.500156
iter  90 value 638.484180
iter 100 value 638.474473
final  value 638.474473 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  73
initial  value 1059.615288 
iter  10 value 988.342891
iter  20 value 674.270769
iter  30 value 642.815344
iter  40 value 639.509384
iter  50 value 638.901376
iter  60 value 638.772375
iter  70 value 638.640207
iter  80 value 638.546513
iter  90 value 638.518165
iter 100 value 638.509778
final  value 638.509778 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  73
initial  value 1377.297086 
iter  10 value 643.118585
iter  20 value 639.159240
iter  30 value 638.752328
iter  40 value 638.584874
iter  50 value 638.518262
iter  60 value 638.484893
iter  70 value 638.477089
iter  80 value 638.476064
iter  90 value 638.475743
iter 100 value 638.475549
final  value 638.475549 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  73
initial  value 1229.507004 
iter  10 value 644.633378
iter  20 value 639.882640
iter  30 value 638.796866
iter  40 value 638.507170
iter  50 value 638.483080
iter  60 value 638.478080
iter  70 value 638.476111
iter  80 value 638.474802
iter  90 value 638.474372
final  value 638.474257 
converged
Fitting Repeat 5 

# weights:  73
initial  value 1089.027657 
iter  10 value 718.351176
iter  20 value 642.055840
iter  30 value 639.007256
iter  40 value 638.621708
iter  50 value 638.521394
iter  60 value 638.493617
iter  70 value 638.483458
iter  80 value 638.480481
iter  90 value 638.476823
iter 100 value 638.473407
final  value 638.473407 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  49
initial  value 1153.763900 
iter  10 value 1106.871121
iter  20 value 1106.665657
iter  30 value 1106.639312
iter  40 value 718.995058
iter  50 value 713.595027
iter  60 value 713.260245
iter  70 value 713.131361
iter  80 value 713.093040
iter  90 value 713.063707
iter 100 value 713.052212
final  value 713.052212 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  49
initial  value 1268.379014 
iter  10 value 1084.906900
iter  20 value 1081.768796
iter  30 value 1013.904983
iter  40 value 726.064238
iter  50 value 725.243033
iter  60 value 721.393820
iter  70 value 713.997069
iter  80 value 712.223216
iter  90 value 711.238238
iter 100 value 710.522436
final  value 710.522436 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  49
initial  value 1154.494781 
iter  10 value 968.775753
iter  20 value 964.944675
iter  30 value 777.667181
iter  40 value 606.749388
iter  50 value 605.537050
iter  60 value 605.463400
iter  70 value 605.360587
iter  80 value 605.328382
iter  90 value 605.273223
iter 100 value 605.255902
final  value 605.255902 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  49
initial  value 1334.432345 
iter  10 value 1093.662186
iter  20 value 1089.682714
iter  30 value 916.840262
iter  40 value 713.768924
iter  50 value 711.478981
iter  60 value 709.176628
iter  70 value 706.487489
iter  80 value 704.954909
iter  90 value 704.404714
iter 100 value 703.840510
final  value 703.840510 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  49
initial  value 1234.851889 
iter  10 value 1060.775096
iter  20 value 1058.181596
iter  30 value 690.393227
iter  40 value 681.790731
iter  50 value 677.419375
iter  60 value 665.545021
iter  70 value 664.578480
iter  80 value 664.232656
iter  90 value 664.033246
iter 100 value 663.936893
final  value 663.936893 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  13
initial  value 1074.271270 
iter  10 value 995.380124
iter  20 value 995.342015
iter  30 value 642.583831
iter  40 value 637.124189
iter  50 value 635.645361
iter  60 value 635.610150
iter  70 value 635.603666
iter  80 value 635.597054
final  value 635.595990 
converged
Fitting Repeat 2 

# weights:  13
initial  value 1173.075001 
iter  10 value 995.400798
iter  20 value 995.341485
iter  30 value 995.339949
iter  40 value 995.337593
iter  50 value 995.333437
iter  60 value 995.324326
iter  70 value 995.292194
iter  80 value 992.324633
iter  90 value 636.033802
iter 100 value 635.807984
final  value 635.807984 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  13
initial  value 1164.441080 
iter  10 value 995.405577
final  value 995.347804 
converged
Fitting Repeat 4 

# weights:  13
initial  value 1185.549860 
iter  10 value 995.406605
iter  20 value 995.005990
iter  30 value 643.793499
iter  40 value 639.934564
iter  50 value 636.032410
iter  60 value 635.693885
iter  70 value 635.679939
iter  80 value 635.660342
iter  90 value 635.656251
iter 100 value 635.651028
final  value 635.651028 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  13
initial  value 1067.884169 
iter  10 value 995.393542
iter  20 value 995.346741
iter  30 value 995.346597
iter  40 value 995.346440
iter  50 value 995.346268
iter  60 value 995.346082
iter  70 value 995.345877
iter  80 value 995.345650
iter  90 value 995.345396
iter 100 value 995.345112
final  value 995.345112 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  61
initial  value 1055.887911 
iter  10 value 749.511723
iter  20 value 638.449986
iter  30 value 637.648470
iter  40 value 637.413214
iter  50 value 637.369406
iter  60 value 637.354197
iter  70 value 637.327742
iter  80 value 637.301755
iter  90 value 637.298490
iter 100 value 637.297515
final  value 637.297515 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  61
initial  value 1122.628274 
iter  10 value 998.521134
iter  20 value 647.637502
iter  30 value 641.857564
iter  40 value 638.819071
iter  50 value 637.818377
iter  60 value 637.432893
iter  70 value 637.304287
iter  80 value 637.271405
iter  90 value 637.264767
iter 100 value 637.262099
final  value 637.262099 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  61
initial  value 1007.823836 
iter  10 value 996.800831
iter  20 value 653.222939
iter  30 value 643.585643
iter  40 value 638.203068
iter  50 value 637.545740
iter  60 value 637.402046
iter  70 value 637.338316
iter  80 value 637.311574
iter  90 value 637.281884
iter 100 value 637.273240
final  value 637.273240 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  61
initial  value 1156.996178 
iter  10 value 978.291827
iter  20 value 637.715740
iter  30 value 637.400030
iter  40 value 637.319515
iter  50 value 637.302858
iter  60 value 637.291690
iter  70 value 637.281597
iter  80 value 637.269430
iter  90 value 637.268159
iter 100 value 637.267389
final  value 637.267389 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  61
initial  value 1017.368547 
iter  10 value 997.040097
iter  20 value 692.143680
iter  30 value 648.000241
iter  40 value 639.706482
iter  50 value 638.073844
iter  60 value 637.517815
iter  70 value 637.375285
iter  80 value 637.313207
iter  90 value 637.283641
iter 100 value 637.274869
final  value 637.274869 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  25
initial  value 1025.859066 
iter  10 value 750.888409
iter  20 value 741.710474
iter  30 value 741.612565
final  value 741.610343 
converged
Fitting Repeat 2 

# weights:  25
initial  value 1099.643254 
iter  10 value 749.609435
iter  20 value 741.708789
iter  30 value 741.610499
final  value 741.610348 
converged
Fitting Repeat 3 

# weights:  25
initial  value 1050.043184 
iter  10 value 748.795171
iter  20 value 745.158627
iter  30 value 744.591119
iter  40 value 741.610343
iter  40 value 741.610343
iter  40 value 741.610343
final  value 741.610343 
converged
Fitting Repeat 4 

# weights:  25
initial  value 1232.506175 
iter  10 value 751.089184
iter  20 value 742.014307
iter  30 value 741.614838
final  value 741.610343 
converged
Fitting Repeat 5 

# weights:  25
initial  value 1087.118934 
iter  10 value 785.434294
iter  20 value 748.008694
iter  30 value 742.254039
iter  40 value 741.610435
final  value 741.610343 
converged
Fitting Repeat 1 

# weights:  33
initial  value 1272.773541 
iter  10 value 854.461140
iter  20 value 649.121992
iter  30 value 646.220631
iter  40 value 646.028908
iter  50 value 645.744727
iter  60 value 645.574210
iter  70 value 645.561648
iter  80 value 645.555061
final  value 645.554784 
converged
Fitting Repeat 2 

# weights:  33
initial  value 1262.584546 
iter  10 value 815.877903
iter  20 value 650.054184
iter  30 value 646.966498
iter  40 value 646.877259
iter  50 value 646.865812
iter  60 value 646.857799
iter  70 value 646.855442
final  value 646.855419 
converged
Fitting Repeat 3 

# weights:  33
initial  value 1269.255213 
iter  10 value 707.588437
iter  20 value 648.116377
iter  30 value 645.853404
iter  40 value 645.592332
iter  50 value 645.561608
iter  60 value 645.555031
final  value 645.554787 
converged
Fitting Repeat 4 

# weights:  33
initial  value 1271.998641 
iter  10 value 926.381515
iter  20 value 661.967602
iter  30 value 647.144354
iter  40 value 646.299639
iter  50 value 645.706883
iter  60 value 645.610662
iter  70 value 645.571634
final  value 645.554792 
converged
Fitting Repeat 5 

# weights:  33
initial  value 1006.429630 
iter  10 value 652.515388
iter  20 value 646.949609
iter  30 value 646.858972
iter  40 value 646.855428
final  value 646.855419 
converged
Fitting Repeat 1 

# weights:  61
initial  value 1179.841125 
iter  10 value 988.847074
iter  20 value 987.437897
iter  30 value 987.421188
iter  40 value 987.315666
iter  50 value 653.855911
iter  60 value 653.682150
iter  70 value 648.423941
iter  80 value 644.257572
iter  90 value 643.949825
iter 100 value 643.723132
final  value 643.723132 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  61
initial  value 1165.901483 
iter  10 value 976.291178
iter  20 value 974.217746
iter  30 value 972.936365
iter  40 value 647.300048
iter  50 value 646.710357
iter  60 value 646.323268
iter  70 value 646.004717
iter  80 value 645.964316
iter  90 value 645.945632
iter 100 value 645.927117
final  value 645.927117 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  61
initial  value 1246.812107 
iter  10 value 1041.644306
iter  20 value 1040.116197
iter  30 value 669.469717
iter  40 value 638.083870
iter  50 value 635.109847
iter  60 value 634.956646
iter  70 value 634.891486
iter  80 value 634.858695
iter  90 value 634.841662
iter 100 value 634.809658
final  value 634.809658 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  61
initial  value 1265.616630 
iter  10 value 1017.953883
iter  20 value 1016.010958
iter  30 value 662.185825
iter  40 value 655.377855
iter  50 value 654.266581
iter  60 value 654.084703
iter  70 value 653.900588
iter  80 value 653.672082
iter  90 value 653.542731
iter 100 value 653.492253
final  value 653.492253 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  61
initial  value 1230.573655 
iter  10 value 1015.090386
iter  20 value 1012.064016
iter  30 value 1012.002941
iter  40 value 674.775229
iter  50 value 667.345102
iter  60 value 666.986254
iter  70 value 666.940198
iter  80 value 666.906014
iter  90 value 666.852922
iter 100 value 666.838172
final  value 666.838172 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  37
initial  value 1272.320178 
iter  10 value 749.475021
iter  20 value 721.928184
iter  30 value 721.770402
iter  40 value 721.767049
final  value 721.766333 
converged
Fitting Repeat 2 

# weights:  37
initial  value 996.472953 
iter  10 value 657.705154
iter  20 value 656.257694
iter  30 value 656.237970
final  value 656.237960 
converged
Fitting Repeat 3 

# weights:  37
initial  value 1235.746743 
iter  10 value 900.375722
iter  20 value 643.519099
iter  30 value 641.637094
iter  40 value 641.447887
iter  50 value 641.418240
final  value 641.417270 
converged
Fitting Repeat 4 

# weights:  37
initial  value 1109.402971 
iter  10 value 1020.448421
iter  20 value 673.200223
iter  30 value 666.180026
iter  40 value 666.024074
final  value 666.023834 
converged
Fitting Repeat 5 

# weights:  37
initial  value 1096.091862 
iter  10 value 761.001876
iter  20 value 675.429228
iter  30 value 674.737552
iter  40 value 674.640193
iter  50 value 674.601866
iter  60 value 674.597909
final  value 674.597491 
converged
Fitting Repeat 1 

# weights:  41
initial  value 1321.362201 
iter  10 value 702.728153
iter  20 value 637.751818
iter  30 value 637.095847
iter  40 value 637.031749
iter  50 value 637.006596
iter  60 value 636.986554
iter  70 value 636.972435
iter  80 value 636.957559
iter  90 value 636.952806
final  value 636.952111 
converged
Fitting Repeat 2 

# weights:  41
initial  value 1041.664501 
iter  10 value 996.264962
iter  20 value 772.159525
iter  30 value 643.772323
iter  40 value 639.054201
iter  50 value 637.498672
iter  60 value 637.196743
iter  70 value 637.077188
iter  80 value 636.999689
iter  90 value 636.971721
iter 100 value 636.964229
final  value 636.964229 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  41
initial  value 1207.130200 
iter  10 value 996.286092
iter  20 value 983.204073
iter  30 value 680.848312
iter  40 value 643.982201
iter  50 value 639.045831
iter  60 value 637.521164
iter  70 value 637.154692
iter  80 value 637.024404
iter  90 value 636.977827
iter 100 value 636.959944
final  value 636.959944 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  41
initial  value 1097.585735 
iter  10 value 996.232829
iter  20 value 784.515388
iter  30 value 643.853670
iter  40 value 639.880815
iter  50 value 638.014497
iter  60 value 637.422637
iter  70 value 637.065063
iter  80 value 637.017835
iter  90 value 637.001916
iter 100 value 636.988414
final  value 636.988414 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  41
initial  value 1030.976212 
iter  10 value 996.132142
iter  20 value 763.461421
iter  30 value 670.934890
iter  40 value 639.970889
iter  50 value 637.861938
iter  60 value 637.232974
iter  70 value 637.066208
iter  80 value 637.000333
iter  90 value 636.963386
iter 100 value 636.954882
final  value 636.954882 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  81
initial  value 1004.761333 
iter  10 value 981.795558
iter  20 value 687.045383
iter  30 value 599.359492
iter  40 value 595.492209
iter  50 value 591.614314
iter  60 value 589.798923
iter  70 value 587.651874
iter  80 value 586.097890
iter  90 value 585.114876
iter 100 value 584.118410
final  value 584.118410 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  81
initial  value 1259.820744 
iter  10 value 1001.472342
iter  20 value 996.790039
iter  30 value 725.812598
iter  40 value 645.346754
iter  50 value 642.328877
iter  60 value 635.797156
iter  70 value 633.735851
iter  80 value 633.460995
iter  90 value 633.214447
iter 100 value 633.036694
final  value 633.036694 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  81
initial  value 1006.148600 
iter  10 value 989.759356
iter  20 value 752.743212
iter  30 value 608.694284
iter  40 value 607.456590
iter  50 value 607.311185
iter  60 value 606.947991
iter  70 value 606.814007
iter  80 value 606.771935
iter  90 value 606.747717
iter 100 value 606.740335
final  value 606.740335 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  81
initial  value 1057.898351 
iter  10 value 915.146184
iter  20 value 911.230320
iter  30 value 911.157041
iter  40 value 634.376437
iter  50 value 591.083077
iter  60 value 584.855778
iter  70 value 572.596498
iter  80 value 571.805895
iter  90 value 571.689671
iter 100 value 571.629886
final  value 571.629886 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  81
initial  value 1026.549005 
iter  10 value 969.147809
iter  20 value 967.507038
iter  30 value 640.800237
iter  40 value 639.192817
iter  50 value 634.387831
iter  60 value 633.042079
iter  70 value 632.821415
iter  80 value 632.581583
iter  90 value 632.475833
iter 100 value 632.396353
final  value 632.396353 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  61
initial  value 952.102703 
iter  10 value 870.349050
iter  20 value 869.230411
iter  30 value 869.095453
iter  40 value 579.321277
iter  50 value 577.391197
iter  60 value 577.031855
iter  70 value 576.837423
iter  80 value 576.798916
iter  90 value 576.785553
iter 100 value 576.778245
final  value 576.778245 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  61
initial  value 1103.573393 
iter  10 value 1059.327894
iter  20 value 1058.363597
iter  30 value 683.719328
iter  40 value 682.757348
iter  50 value 678.846451
iter  60 value 673.989586
iter  70 value 673.647569
iter  80 value 673.343413
iter  90 value 673.197021
iter 100 value 673.122566
final  value 673.122566 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  61
initial  value 1175.742185 
iter  10 value 1006.289052
iter  20 value 1004.546680
iter  30 value 1004.517345
iter  40 value 666.409985
iter  50 value 630.945634
iter  60 value 625.176784
iter  70 value 624.058845
iter  80 value 623.965189
iter  90 value 623.919300
iter 100 value 623.893388
final  value 623.893388 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  61
initial  value 1348.913012 
iter  10 value 1071.220974
iter  20 value 1070.494674
iter  30 value 663.257668
iter  40 value 657.163519
iter  50 value 656.849980
iter  60 value 656.749917
iter  70 value 656.660493
iter  80 value 656.630503
iter  90 value 656.615291
iter 100 value 656.603179
final  value 656.603179 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  61
initial  value 955.498346 
iter  10 value 877.107384
iter  20 value 876.149610
iter  30 value 876.148855
iter  40 value 876.147803
iter  50 value 876.146103
iter  60 value 876.142432
iter  70 value 876.125238
iter  80 value 555.053537
iter  90 value 536.936144
iter 100 value 536.682373
final  value 536.682373 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  65
initial  value 1254.667209 
iter  10 value 1001.827913
iter  20 value 1001.567901
iter  30 value 683.341450
iter  40 value 612.991863
iter  50 value 609.405291
iter  60 value 609.223524
iter  70 value 609.137055
iter  80 value 609.095802
iter  90 value 609.037311
iter 100 value 608.981690
final  value 608.981690 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  65
initial  value 1090.684436 
iter  10 value 966.059671
final  value 965.781927 
converged
Fitting Repeat 3 

# weights:  65
initial  value 1156.498518 
iter  10 value 964.088333
iter  20 value 963.389656
iter  30 value 963.384294
iter  40 value 963.383617
iter  50 value 963.382603
iter  60 value 963.380722
iter  70 value 963.375615
iter  80 value 963.324610
iter  90 value 633.414660
iter 100 value 625.319678
final  value 625.319678 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  65
initial  value 1020.515933 
iter  10 value 640.712639
iter  20 value 639.157873
iter  30 value 638.591744
iter  40 value 637.815881
iter  50 value 635.978437
iter  60 value 632.472798
iter  70 value 630.167428
iter  80 value 628.720657
iter  90 value 628.313385
iter 100 value 628.279834
final  value 628.279834 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  65
initial  value 1496.226314 
iter  10 value 1068.809463
iter  20 value 1068.451252
iter  20 value 1068.451251
iter  20 value 1068.451243
final  value 1068.451243 
converged
Fitting Repeat 1 

# weights:  13
initial  value 1159.890512 
iter  10 value 731.683890
iter  20 value 663.033174
iter  30 value 656.667183
iter  40 value 655.995922
final  value 655.993682 
converged
Fitting Repeat 2 

# weights:  13
initial  value 1165.683807 
iter  10 value 732.775464
iter  20 value 652.952602
iter  30 value 652.561838
final  value 652.561400 
converged
Fitting Repeat 3 

# weights:  13
initial  value 1288.245253 
iter  10 value 834.813551
iter  20 value 765.737324
iter  30 value 754.952316
iter  40 value 754.623154
final  value 754.620779 
converged
Fitting Repeat 4 

# weights:  13
initial  value 1045.168673 
iter  10 value 619.537592
iter  20 value 601.940514
final  value 601.913060 
converged
Fitting Repeat 5 

# weights:  13
initial  value 1023.515703 
iter  10 value 660.062803
iter  20 value 633.967583
iter  30 value 625.306346
iter  40 value 625.292359
iter  40 value 625.292358
iter  40 value 625.292358
final  value 625.292358 
converged
Fitting Repeat 1 

# weights:  9
initial  value 1250.563156 
iter  10 value 866.005701
iter  20 value 727.264178
iter  30 value 723.384916
final  value 723.379875 
converged
Fitting Repeat 2 

# weights:  9
initial  value 1074.926141 
iter  10 value 895.497032
iter  20 value 734.071801
iter  30 value 723.510317
final  value 723.379875 
converged
Fitting Repeat 3 

# weights:  9
initial  value 1193.780100 
iter  10 value 740.121777
iter  20 value 735.058027
final  value 735.058018 
converged
Fitting Repeat 4 

# weights:  9
initial  value 1101.160195 
iter  10 value 780.961022
iter  20 value 723.433643
final  value 723.379875 
converged
Fitting Repeat 5 

# weights:  9
initial  value 1110.736387 
iter  10 value 745.984958
iter  20 value 735.058419
final  value 735.058018 
converged
Fitting Repeat 1 

# weights:  21
initial  value 1130.498803 
iter  10 value 688.603176
iter  20 value 676.678637
iter  30 value 674.680892
iter  40 value 674.508572
iter  50 value 674.471110
iter  60 value 674.444673
iter  70 value 674.444015
iter  70 value 674.444010
iter  70 value 674.444010
final  value 674.444010 
converged
Fitting Repeat 2 

# weights:  21
initial  value 1181.654226 
iter  10 value 717.142057
iter  20 value 677.160329
iter  30 value 674.745780
iter  40 value 674.503206
iter  50 value 674.455208
iter  60 value 674.444308
final  value 674.444007 
converged
Fitting Repeat 3 

# weights:  21
initial  value 1138.206862 
iter  10 value 1040.468501
iter  20 value 822.996235
iter  30 value 683.438960
iter  40 value 677.105884
iter  50 value 675.874402
iter  60 value 675.262449
iter  70 value 674.645954
iter  80 value 674.537874
iter  90 value 674.468051
iter 100 value 674.454908
final  value 674.454908 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  21
initial  value 1181.494807 
iter  10 value 1040.513192
iter  20 value 703.020043
iter  30 value 675.803187
iter  40 value 674.981763
iter  50 value 674.748769
iter  60 value 674.674659
iter  70 value 674.590296
iter  80 value 674.587110
final  value 674.586880 
converged
Fitting Repeat 5 

# weights:  21
initial  value 1144.194403 
iter  10 value 1040.576545
iter  20 value 695.752114
iter  30 value 678.937925
iter  40 value 675.535947
iter  50 value 674.957143
iter  60 value 674.782428
iter  70 value 674.594352
iter  80 value 674.586926
final  value 674.586886 
converged
Fitting Repeat 1 

# weights:  69
initial  value 1204.997915 
iter  10 value 818.285481
iter  20 value 813.047517
iter  30 value 812.436449
iter  40 value 812.351088
final  value 812.347573 
converged
Fitting Repeat 2 

# weights:  69
initial  value 1260.837405 
iter  10 value 831.712814
iter  20 value 823.663318
iter  30 value 823.340498
iter  40 value 823.230886
final  value 823.228192 
converged
Fitting Repeat 3 

# weights:  69
initial  value 1369.655598 
iter  10 value 1094.210740
iter  20 value 879.763845
iter  30 value 853.362921
iter  40 value 852.144976
iter  50 value 851.951251
iter  60 value 851.889082
iter  70 value 851.841394
iter  80 value 851.825554
iter  80 value 851.825547
iter  80 value 851.825547
final  value 851.825547 
converged
Fitting Repeat 4 

# weights:  69
initial  value 1098.749984 
iter  10 value 835.759652
iter  20 value 812.696780
iter  30 value 812.142676
iter  40 value 812.128577
final  value 812.126441 
converged
Fitting Repeat 5 

# weights:  69
initial  value 1108.381689 
iter  10 value 798.345660
iter  20 value 775.380402
iter  30 value 773.560873
iter  40 value 773.390993
iter  50 value 773.361854
final  value 773.361529 
converged
Fitting Repeat 1 

# weights:  25
initial  value 1132.934304 
iter  10 value 1076.992303
iter  20 value 724.478470
iter  30 value 709.932240
iter  40 value 708.485564
iter  50 value 707.717025
iter  60 value 707.515639
iter  70 value 707.394916
iter  80 value 707.342368
iter  90 value 707.341136
final  value 707.341071 
converged
Fitting Repeat 2 

# weights:  25
initial  value 1250.942339 
iter  10 value 758.929743
iter  20 value 733.788120
iter  30 value 732.582035
iter  40 value 732.469677
iter  50 value 732.425044
iter  60 value 732.371619
iter  70 value 732.318259
iter  80 value 732.314785
iter  90 value 732.314095
final  value 732.314037 
converged
Fitting Repeat 3 

# weights:  25
initial  value 1219.253120 
iter  10 value 1062.414630
iter  20 value 726.058117
iter  30 value 702.208638
iter  40 value 699.670275
iter  50 value 699.256027
iter  60 value 698.654701
iter  70 value 698.340291
iter  80 value 698.143150
iter  90 value 698.081759
iter 100 value 698.074339
final  value 698.074339 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  25
initial  value 1221.956730 
iter  10 value 1073.235403
iter  20 value 691.090660
iter  30 value 689.052052
iter  40 value 688.673509
iter  50 value 688.475943
iter  60 value 688.444244
iter  70 value 688.430666
iter  80 value 688.425255
final  value 688.425140 
converged
Fitting Repeat 5 

# weights:  25
initial  value 1200.290547 
iter  10 value 1142.799139
iter  20 value 775.507889
iter  30 value 771.486039
iter  40 value 771.106039
iter  50 value 770.839858
iter  60 value 770.661086
iter  70 value 770.548586
iter  80 value 770.366806
iter  90 value 770.318830
iter 100 value 770.314136
final  value 770.314136 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  41
initial  value 1377.236818 
iter  10 value 1109.513594
iter  20 value 1106.546764
iter  30 value 836.529391
iter  40 value 771.447742
iter  50 value 766.759853
iter  60 value 761.996867
iter  70 value 761.093502
iter  80 value 760.866943
iter  90 value 760.663136
iter 100 value 760.545786
final  value 760.545786 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  41
initial  value 1264.467792 
iter  10 value 972.868637
iter  20 value 970.239099
iter  30 value 639.294681
iter  40 value 636.982753
iter  50 value 636.633337
iter  60 value 636.368501
iter  70 value 636.266244
iter  80 value 636.237568
iter  90 value 636.209499
iter 100 value 636.199428
final  value 636.199428 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  41
initial  value 1171.633443 
iter  10 value 1104.722277
iter  20 value 732.264359
iter  30 value 690.806718
iter  40 value 690.576212
iter  50 value 690.489521
iter  60 value 690.462448
iter  70 value 690.440164
iter  80 value 690.429258
iter  90 value 690.420477
iter 100 value 690.415586
final  value 690.415586 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  41
initial  value 985.709063 
iter  10 value 626.294504
iter  20 value 614.817212
iter  30 value 612.426204
iter  40 value 612.122150
iter  50 value 612.067666
iter  60 value 612.010024
iter  70 value 611.996663
iter  80 value 611.984139
iter  90 value 611.979334
iter 100 value 611.974108
final  value 611.974108 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  41
initial  value 955.971935 
iter  10 value 602.016916
iter  20 value 599.699726
iter  30 value 591.917950
iter  40 value 590.337411
iter  50 value 589.227848
iter  60 value 588.298186
iter  70 value 588.170662
iter  80 value 588.090318
iter  90 value 588.036711
iter 100 value 587.996455
final  value 587.996455 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  17
initial  value 1110.057768 
iter  10 value 1073.891555
final  value 1073.878416 
converged
Fitting Repeat 2 

# weights:  17
initial  value 1236.028898 
iter  10 value 1123.494860
final  value 1123.471929 
converged
Fitting Repeat 3 

# weights:  17
initial  value 1240.568900 
iter  10 value 1104.892487
iter  20 value 1104.862914
iter  30 value 1104.862632
iter  40 value 1104.862281
iter  50 value 1104.861830
iter  60 value 1104.861231
iter  70 value 1104.860390
iter  80 value 1104.859120
iter  90 value 1104.856967
iter 100 value 1104.852488
final  value 1104.852488 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  17
initial  value 1250.324669 
iter  10 value 1143.916301
iter  20 value 1143.883046
iter  30 value 1143.882782
iter  40 value 1143.882454
iter  50 value 1143.882032
iter  60 value 1143.881466
iter  70 value 1143.880661
iter  80 value 1143.879418
iter  90 value 1143.877228
iter 100 value 1143.872323
final  value 1143.872323 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  17
initial  value 1287.293990 
iter  10 value 1174.183935
final  value 1174.170350 
converged
Fitting Repeat 1 

# weights:  21
initial  value 1132.508391 
iter  10 value 1043.533077
iter  20 value 680.448704
iter  30 value 676.544486
iter  40 value 676.484600
iter  50 value 676.379639
iter  60 value 676.365689
final  value 676.362995 
converged
Fitting Repeat 2 

# weights:  21
initial  value 1124.368279 
iter  10 value 1042.822718
iter  20 value 713.116732
iter  30 value 680.753589
iter  40 value 677.922753
iter  50 value 677.673773
iter  60 value 677.603710
final  value 677.599144 
converged
Fitting Repeat 3 

# weights:  21
initial  value 1301.741498 
iter  10 value 1042.929422
iter  20 value 683.106449
iter  30 value 677.331522
iter  40 value 676.607274
iter  50 value 676.388513
iter  60 value 676.366127
final  value 676.362995 
converged
Fitting Repeat 4 

# weights:  21
initial  value 1094.197897 
iter  10 value 1041.053828
iter  20 value 690.262793
iter  30 value 676.969903
iter  40 value 676.383904
iter  50 value 676.363826
final  value 676.363101 
converged
Fitting Repeat 5 

# weights:  21
initial  value 1121.991925 
iter  10 value 1042.450605
iter  20 value 709.128881
iter  30 value 679.764112
iter  40 value 677.802933
iter  50 value 677.621648
iter  60 value 677.599192
final  value 677.599147 
converged
Fitting Repeat 1 

# weights:  73
initial  value 1097.376648 
iter  10 value 1037.175037
iter  20 value 704.383797
iter  30 value 680.403343
iter  40 value 675.903942
iter  50 value 675.369117
iter  60 value 675.159189
iter  70 value 675.032887
iter  80 value 674.962979
iter  90 value 674.940189
iter 100 value 674.919957
final  value 674.919957 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  73
initial  value 1061.721781 
iter  10 value 962.933991
iter  20 value 700.585433
iter  30 value 676.452296
iter  40 value 675.140091
iter  50 value 674.962707
iter  60 value 674.910527
iter  70 value 674.888352
iter  80 value 674.882623
iter  90 value 674.877281
iter 100 value 674.875020
final  value 674.875020 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  73
initial  value 1274.825942 
iter  10 value 1048.551464
iter  20 value 678.146852
iter  30 value 675.168359
iter  40 value 675.004408
iter  50 value 674.921653
iter  60 value 674.912118
iter  70 value 674.905915
iter  80 value 674.901546
iter  90 value 674.899629
iter 100 value 674.899364
final  value 674.899364 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  73
initial  value 1131.780592 
iter  10 value 1038.699433
iter  20 value 688.781037
iter  30 value 676.919047
iter  40 value 675.679957
iter  50 value 675.323218
iter  60 value 675.028518
iter  70 value 674.934484
iter  80 value 674.902615
iter  90 value 674.888596
iter 100 value 674.880480
final  value 674.880480 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  73
initial  value 1104.610277 
iter  10 value 1037.298580
iter  20 value 720.570209
iter  30 value 677.698746
iter  40 value 675.034849
iter  50 value 674.971604
iter  60 value 674.923065
iter  70 value 674.901797
iter  80 value 674.894006
iter  90 value 674.886581
iter 100 value 674.881731
final  value 674.881731 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  49
initial  value 1091.056670 
iter  10 value 990.804303
iter  20 value 989.456742
iter  30 value 989.449356
iter  40 value 989.392526
iter  50 value 654.254242
iter  60 value 649.238991
iter  70 value 643.048668
iter  80 value 641.605337
iter  90 value 640.890667
iter 100 value 640.358492
final  value 640.358492 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  49
initial  value 1062.527579 
iter  10 value 1028.034710
iter  20 value 1027.362553
iter  30 value 1027.315716
iter  40 value 633.549027
iter  50 value 633.005116
iter  60 value 632.860490
iter  70 value 632.610474
iter  80 value 632.492831
iter  90 value 632.455117
iter 100 value 632.438995
final  value 632.438995 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  49
initial  value 1184.280525 
iter  10 value 1034.286328
iter  20 value 1030.622203
iter  30 value 1030.537145
iter  40 value 647.233967
iter  50 value 642.518718
iter  60 value 642.243837
iter  70 value 641.909263
iter  80 value 641.797474
iter  90 value 641.746040
iter 100 value 641.707391
final  value 641.707391 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  49
initial  value 1118.245981 
iter  10 value 1041.120312
iter  20 value 1039.420797
iter  30 value 680.470961
iter  40 value 665.836623
iter  50 value 665.245677
iter  60 value 665.011109
iter  70 value 664.946570
iter  80 value 664.906675
iter  90 value 664.895289
iter 100 value 664.878046
final  value 664.878046 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  49
initial  value 1484.823875 
iter  10 value 1078.886464
iter  20 value 1075.518118
iter  30 value 900.920569
iter  40 value 761.160732
iter  50 value 760.785333
iter  60 value 760.521255
iter  70 value 760.439853
iter  80 value 760.368258
iter  90 value 760.341472
iter 100 value 760.328775
final  value 760.328775 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  13
initial  value 1125.791282 
iter  10 value 1039.155589
final  value 1039.126686 
converged
Fitting Repeat 2 

# weights:  13
initial  value 1125.628184 
iter  10 value 1039.170333
final  value 1039.122306 
converged
Fitting Repeat 3 

# weights:  13
initial  value 1312.068650 
iter  10 value 1039.192540
iter  20 value 1039.125270
iter  30 value 1039.124160
iter  40 value 1039.109039
iter  50 value 1039.097035
iter  60 value 1039.040714
iter  70 value 778.280145
iter  80 value 673.112440
iter  90 value 672.105187
iter 100 value 672.092385
final  value 672.092385 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  13
initial  value 1195.801804 
iter  10 value 1039.175902
final  value 1039.122468 
converged
Fitting Repeat 5 

# weights:  13
initial  value 1228.480829 
iter  10 value 1039.176794
final  value 1039.127325 
converged
Fitting Repeat 1 

# weights:  61
initial  value 1117.864511 
iter  10 value 1041.264888
iter  20 value 684.225549
iter  30 value 677.650547
iter  40 value 675.340317
iter  50 value 674.248327
iter  60 value 673.843793
iter  70 value 673.722614
iter  80 value 673.678903
iter  90 value 673.670440
iter 100 value 673.666598
final  value 673.666598 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  61
initial  value 1256.177564 
iter  10 value 1041.235095
iter  20 value 771.035828
iter  30 value 674.452596
iter  40 value 673.952967
iter  50 value 673.802330
iter  60 value 673.749501
iter  70 value 673.726410
iter  80 value 673.714554
iter  90 value 673.707096
iter 100 value 673.703366
final  value 673.703366 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  61
initial  value 1114.587443 
iter  10 value 1041.076409
iter  20 value 682.685785
iter  30 value 676.829831
iter  40 value 675.025429
iter  50 value 674.068608
iter  60 value 673.814861
iter  70 value 673.712211
iter  80 value 673.688412
iter  90 value 673.681198
iter 100 value 673.676488
final  value 673.676488 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  61
initial  value 1387.480464 
iter  10 value 1040.830458
iter  20 value 676.499265
iter  30 value 674.109088
iter  40 value 673.741776
iter  50 value 673.688998
iter  60 value 673.677739
iter  70 value 673.668958
iter  80 value 673.663254
iter  90 value 673.661864
iter 100 value 673.661222
final  value 673.661222 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  61
initial  value 1014.584968 
iter  10 value 696.864396
iter  20 value 676.582568
iter  30 value 674.581251
iter  40 value 674.173068
iter  50 value 673.930431
iter  60 value 673.831824
iter  70 value 673.739503
iter  80 value 673.709053
iter  90 value 673.701653
iter 100 value 673.697059
final  value 673.697059 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  25
initial  value 1248.911570 
iter  10 value 806.823391
iter  20 value 784.684974
iter  30 value 779.160925
iter  40 value 777.783009
final  value 777.782892 
converged
Fitting Repeat 2 

# weights:  25
initial  value 1178.604084 
iter  10 value 803.754922
iter  20 value 779.395955
iter  30 value 777.789954
final  value 777.782892 
converged
Fitting Repeat 3 

# weights:  25
initial  value 1231.509836 
iter  10 value 781.885453
iter  20 value 777.680193
iter  30 value 777.635145
final  value 777.634876 
converged
Fitting Repeat 4 

# weights:  25
initial  value 1152.682511 
iter  10 value 803.796233
iter  20 value 780.417963
iter  30 value 778.153992
iter  40 value 777.783047
final  value 777.782892 
converged
Fitting Repeat 5 

# weights:  25
initial  value 1252.662234 
iter  10 value 974.947280
iter  20 value 794.113244
iter  30 value 779.402778
iter  40 value 777.920840
iter  50 value 777.809160
iter  60 value 777.782962
final  value 777.782894 
converged
Fitting Repeat 1 

# weights:  33
initial  value 1269.384714 
iter  10 value 871.200808
iter  20 value 683.626425
iter  30 value 682.239353
iter  40 value 681.974532
final  value 681.968709 
converged
Fitting Repeat 2 

# weights:  33
initial  value 1131.662816 
iter  10 value 709.097636
iter  20 value 684.906037
iter  30 value 682.469074
iter  40 value 682.314328
iter  50 value 682.297383
final  value 682.297292 
converged
Fitting Repeat 3 

# weights:  33
initial  value 1199.060410 
iter  10 value 864.133870
iter  20 value 688.711974
iter  30 value 683.330486
iter  40 value 682.062298
iter  50 value 681.996077
iter  60 value 681.971764
iter  70 value 681.968756
final  value 681.968707 
converged
Fitting Repeat 4 

# weights:  33
initial  value 1237.831658 
iter  10 value 902.072136
iter  20 value 695.162122
iter  30 value 684.976652
iter  40 value 683.652294
iter  50 value 682.560463
iter  60 value 682.182415
iter  70 value 682.035614
iter  80 value 681.987237
final  value 681.968706 
converged
Fitting Repeat 5 

# weights:  33
initial  value 1055.751399 
iter  10 value 685.161798
iter  20 value 683.358882
iter  30 value 683.282175
iter  40 value 683.278648
iter  40 value 683.278641
iter  40 value 683.278641
final  value 683.278641 
converged
Fitting Repeat 1 

# weights:  61
initial  value 1264.228568 
iter  10 value 1137.244334
iter  20 value 1136.016448
iter  30 value 763.592284
iter  40 value 762.942734
iter  50 value 762.592594
iter  60 value 762.399913
iter  70 value 762.285936
iter  80 value 762.254924
iter  90 value 762.238323
iter 100 value 762.228017
final  value 762.228017 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  61
initial  value 1126.438247 
iter  10 value 722.341176
iter  20 value 717.846053
iter  30 value 717.358339
iter  40 value 717.177994
iter  50 value 717.117059
iter  60 value 717.063092
iter  70 value 717.035857
iter  80 value 717.028444
iter  90 value 717.021342
iter 100 value 717.017870
final  value 717.017870 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  61
initial  value 1189.006941 
iter  10 value 1012.879469
iter  20 value 1010.179492
iter  30 value 1010.025156
iter  40 value 652.838265
iter  50 value 652.069348
iter  60 value 644.792374
iter  70 value 642.596862
iter  80 value 640.238353
iter  90 value 639.361846
iter 100 value 638.918884
final  value 638.918884 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  61
initial  value 922.923012 
iter  10 value 615.571071
iter  20 value 602.708513
iter  30 value 602.366293
iter  40 value 602.235394
iter  50 value 601.989629
iter  60 value 601.871910
iter  70 value 601.758488
iter  80 value 601.716508
iter  90 value 601.691088
iter 100 value 601.669039
final  value 601.669039 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  61
initial  value 1545.678000 
iter  10 value 1017.600031
iter  20 value 1017.336885
iter  30 value 1017.320089
iter  40 value 1017.243905
iter  50 value 705.908114
iter  60 value 697.900267
iter  70 value 694.183684
iter  80 value 690.471627
iter  90 value 689.668468
iter 100 value 689.288266
final  value 689.288266 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  37
initial  value 1048.987555 
iter  10 value 857.918067
iter  20 value 647.129221
iter  30 value 636.782489
iter  40 value 633.153655
iter  50 value 631.999106
iter  60 value 631.793509
iter  70 value 631.762310
iter  80 value 631.747870
iter  90 value 631.700629
final  value 631.695015 
converged
Fitting Repeat 2 

# weights:  37
initial  value 1288.323157 
iter  10 value 624.012745
iter  20 value 597.841787
iter  30 value 597.364504
iter  40 value 597.355989
final  value 597.354987 
converged
Fitting Repeat 3 

# weights:  37
initial  value 930.967482 
iter  10 value 757.013982
iter  20 value 556.816638
iter  30 value 539.381685
iter  40 value 538.096407
iter  50 value 537.503733
iter  60 value 537.413945
iter  70 value 537.368270
iter  80 value 537.358937
iter  90 value 537.334660
final  value 537.334104 
converged
Fitting Repeat 4 

# weights:  37
initial  value 1248.397208 
iter  10 value 728.328040
iter  20 value 707.089911
iter  30 value 706.342234
iter  40 value 706.292164
final  value 706.280825 
converged
Fitting Repeat 5 

# weights:  37
initial  value 1363.658357 
iter  10 value 667.070257
iter  20 value 616.567174
iter  30 value 615.327503
iter  40 value 614.899856
iter  50 value 614.812162
iter  60 value 614.785925
iter  70 value 614.783415
iter  70 value 614.783411
iter  70 value 614.783409
final  value 614.783409 
converged
Fitting Repeat 1 

# weights:  41
initial  value 1259.101161 
iter  10 value 1040.325909
iter  20 value 722.908515
iter  30 value 680.869979
iter  40 value 674.755685
iter  50 value 673.872417
iter  60 value 673.672899
iter  70 value 673.575246
iter  80 value 673.520620
iter  90 value 673.461370
iter 100 value 673.426120
final  value 673.426120 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  41
initial  value 1203.501570 
iter  10 value 674.743985
iter  20 value 673.772975
iter  30 value 673.489368
iter  40 value 673.445422
iter  50 value 673.425657
iter  60 value 673.406063
iter  70 value 673.393789
iter  80 value 673.384659
iter  90 value 673.378187
iter 100 value 673.375801
final  value 673.375801 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  41
initial  value 1154.045857 
iter  10 value 1040.143546
iter  20 value 709.136771
iter  30 value 685.305942
iter  40 value 677.556199
iter  50 value 674.336298
iter  60 value 673.619924
iter  70 value 673.459327
iter  80 value 673.421373
iter  90 value 673.398549
iter 100 value 673.384492
final  value 673.384492 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  41
initial  value 1081.188143 
iter  10 value 1040.146513
iter  20 value 700.115943
iter  30 value 684.159745
iter  40 value 675.526979
iter  50 value 674.073076
iter  60 value 673.582701
iter  70 value 673.458657
iter  80 value 673.414966
iter  90 value 673.404954
iter 100 value 673.396935
final  value 673.396935 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  41
initial  value 1143.296292 
iter  10 value 1040.288625
iter  20 value 712.634768
iter  30 value 682.235900
iter  40 value 675.657730
iter  50 value 674.169667
iter  60 value 673.648620
iter  70 value 673.517829
iter  80 value 673.457092
iter  90 value 673.421867
iter 100 value 673.404330
final  value 673.404330 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  81
initial  value 1078.772107 
iter  10 value 1000.214841
iter  20 value 998.540109
iter  30 value 657.156290
iter  40 value 646.920639
iter  50 value 640.240354
iter  60 value 639.965906
iter  70 value 639.778226
iter  80 value 639.677663
iter  90 value 639.620404
iter 100 value 639.597508
final  value 639.597508 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  81
initial  value 1260.933329 
iter  10 value 1080.607517
iter  20 value 1075.119222
iter  30 value 1072.048537
iter  40 value 723.475829
iter  50 value 722.480943
iter  60 value 716.682326
iter  70 value 712.822767
iter  80 value 711.491394
iter  90 value 711.050617
iter 100 value 710.510836
final  value 710.510836 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  81
initial  value 1335.842732 
iter  10 value 1000.587066
iter  20 value 997.193492
iter  30 value 997.154721
iter  40 value 661.652913
iter  50 value 652.905462
iter  60 value 652.727799
iter  70 value 652.359014
iter  80 value 652.302601
iter  90 value 652.269559
iter 100 value 652.245155
final  value 652.245155 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  81
initial  value 1267.971448 
iter  10 value 1074.137690
iter  20 value 1069.967005
iter  30 value 1069.913017
iter  40 value 740.580690
iter  50 value 703.512047
iter  60 value 699.698277
iter  70 value 695.345223
iter  80 value 693.793733
iter  90 value 691.040300
iter 100 value 689.520942
final  value 689.520942 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  81
initial  value 1109.988136 
iter  10 value 717.868658
iter  20 value 709.987426
iter  30 value 708.173381
iter  40 value 707.678642
iter  50 value 707.400137
iter  60 value 707.266605
iter  70 value 707.187254
iter  80 value 707.133191
iter  90 value 707.102962
iter 100 value 707.075165
final  value 707.075165 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  61
initial  value 1414.250523 
iter  10 value 1115.224881
iter  20 value 1114.009249
iter  30 value 1113.977033
iter  40 value 812.946185
iter  50 value 760.838614
iter  60 value 751.687211
iter  70 value 751.464585
iter  80 value 751.235238
iter  90 value 751.077758
iter 100 value 750.954354
final  value 750.954354 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  61
initial  value 1130.818203 
iter  10 value 1063.014857
iter  20 value 1062.240245
iter  30 value 628.377224
iter  40 value 626.875429
iter  50 value 626.787858
iter  60 value 626.603504
iter  70 value 626.272117
iter  80 value 626.230849
iter  90 value 626.203235
iter 100 value 626.182581
final  value 626.182581 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  61
initial  value 1118.019749 
iter  10 value 994.071618
iter  20 value 992.751659
iter  30 value 992.718142
iter  40 value 628.547164
iter  50 value 627.635922
iter  60 value 627.342700
iter  70 value 627.248328
iter  80 value 627.223654
iter  90 value 627.208222
iter 100 value 627.182962
final  value 627.182962 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  61
initial  value 1241.760872 
iter  10 value 1048.415381
iter  20 value 1046.989302
iter  30 value 1046.837961
iter  40 value 668.413565
iter  50 value 665.138954
iter  60 value 664.825432
iter  70 value 664.270159
iter  80 value 661.557840
iter  90 value 659.432095
iter 100 value 656.515117
final  value 656.515117 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  61
initial  value 1310.290371 
iter  10 value 1166.103544
iter  20 value 1164.951289
iter  30 value 822.927834
iter  40 value 817.180677
iter  50 value 817.055392
iter  60 value 816.968581
iter  70 value 816.925892
iter  80 value 816.892756
iter  90 value 816.871077
iter 100 value 816.850687
final  value 816.850687 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  65
initial  value 980.250441 
iter  10 value 611.168521
iter  20 value 610.762148
iter  30 value 610.131424
iter  40 value 610.011162
iter  50 value 609.999026
iter  60 value 609.991415
iter  70 value 609.982373
iter  80 value 609.974105
iter  90 value 609.971074
iter 100 value 609.966240
final  value 609.966240 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  65
initial  value 1277.393908 
iter  10 value 1031.828614
iter  20 value 1031.167476
iter  30 value 709.585082
iter  40 value 701.638943
iter  50 value 698.155625
iter  60 value 698.075145
iter  70 value 698.044833
iter  80 value 698.010632
iter  90 value 697.984953
iter 100 value 697.962061
final  value 697.962061 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  65
initial  value 1226.938738 
iter  10 value 1102.432512
iter  20 value 1101.973750
iter  20 value 1101.973746
final  value 1101.973746 
converged
Fitting Repeat 4 

# weights:  65
initial  value 1021.758069 
iter  10 value 988.991300
iter  20 value 988.851493
iter  30 value 988.840971
iter  40 value 782.306569
iter  50 value 607.105302
iter  60 value 606.630737
iter  70 value 606.554607
iter  80 value 606.544551
iter  90 value 606.534131
iter 100 value 606.523097
final  value 606.523097 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  65
initial  value 1023.750626 
iter  10 value 996.892568
iter  20 value 996.536844
iter  30 value 648.560432
iter  40 value 642.823240
iter  50 value 637.344699
iter  60 value 637.270218
iter  70 value 637.239976
iter  80 value 637.228747
iter  90 value 637.177404
iter 100 value 637.141255
final  value 637.141255 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  13
initial  value 1239.032022 
iter  10 value 922.031317
iter  20 value 750.058719
iter  30 value 748.213543
final  value 748.213501 
converged
Fitting Repeat 2 

# weights:  13
initial  value 1110.137395 
iter  10 value 700.296339
iter  20 value 630.644766
iter  30 value 630.164069
final  value 630.163930 
converged
Fitting Repeat 3 

# weights:  13
initial  value 1250.328655 
iter  10 value 823.143033
iter  20 value 778.590971
iter  30 value 773.820792
final  value 773.818997 
converged
Fitting Repeat 4 

# weights:  13
initial  value 1225.544931 
iter  10 value 808.229169
iter  20 value 791.585798
final  value 791.331924 
converged
Fitting Repeat 5 

# weights:  13
initial  value 1270.368015 
iter  10 value 851.310300
iter  20 value 757.325348
iter  30 value 751.498413
iter  40 value 751.486997
iter  40 value 751.486992
iter  40 value 751.486991
final  value 751.486991 
converged
Fitting Repeat 1 

# weights:  9
initial  value 1249.749861 
iter  10 value 778.631941
iter  20 value 772.086424
final  value 772.086300 
converged
Fitting Repeat 2 

# weights:  9
initial  value 1115.112362 
iter  10 value 799.984809
iter  20 value 760.239986
iter  30 value 760.136251
final  value 760.136229 
converged
Fitting Repeat 3 

# weights:  9
initial  value 1190.655342 
iter  10 value 984.148639
iter  20 value 771.331443
iter  30 value 760.181469
final  value 760.136223 
converged
Fitting Repeat 4 

# weights:  9
initial  value 1181.204111 
iter  10 value 871.439787
iter  20 value 783.324066
iter  30 value 763.589980
final  value 760.136223 
converged
Fitting Repeat 5 

# weights:  9
initial  value 1206.096034 
iter  10 value 787.948659
iter  20 value 772.086311
final  value 772.086300 
converged
Fitting Repeat 1 

# weights:  61
initial  value 2008.362457 
iter  10 value 1401.964991
iter  20 value 1399.252389
iter  30 value 1399.231002
iter  40 value 1399.228061
iter  50 value 1399.216647
iter  60 value 1182.089812
iter  70 value 928.526196
iter  80 value 917.052318
iter  90 value 914.281137
iter 100 value 909.091004
final  value 909.091004 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  61
initial  value 1964.675877 
iter  10 value 1588.229474
iter  20 value 1584.301725
iter  30 value 1580.789659
iter  40 value 1028.543007
iter  50 value 1019.849542
iter  60 value 1018.570441
iter  70 value 1018.270459
iter  80 value 1018.050909
iter  90 value 1017.956696
iter 100 value 1017.887849
final  value 1017.887849 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  61
initial  value 1979.652217 
iter  10 value 1686.858993
iter  20 value 1682.186574
iter  30 value 1098.440898
iter  40 value 1093.176379
iter  50 value 1091.649964
iter  60 value 1091.222002
iter  70 value 1090.966946
iter  80 value 1090.900851
iter  90 value 1090.806762
iter 100 value 1090.744360
final  value 1090.744360 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  61
initial  value 2207.741050 
iter  10 value 1588.631111
iter  20 value 1588.150595
iter  30 value 1588.130349
iter  40 value 1588.109765
iter  50 value 1162.278983
iter  60 value 1022.402712
iter  70 value 1013.604309
iter  80 value 1012.219336
iter  90 value 1011.184444
iter 100 value 1010.829183
final  value 1010.829183 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  61
initial  value 1489.604011 
iter  10 value 1028.590779
iter  20 value 994.934419
iter  30 value 987.387438
iter  40 value 984.327650
iter  50 value 980.498716
iter  60 value 974.255449
iter  70 value 957.694113
iter  80 value 954.680858
iter  90 value 952.728235
iter 100 value 951.096555
final  value 951.096555 
stopped after 100 iterations
Model Averaged Neural Network 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  size  decay         bag    RMSE      Rsquared   Selected
   2    1.444316e+00  FALSE  1.177912  0.7910760          
   3    4.205678e-05  FALSE  1.301310  0.7137761          
   3    5.931464e-01   TRUE  1.161306  0.7606306          
   4    2.297096e-05   TRUE  1.366040  0.7003546          
   5    1.257469e-02  FALSE  1.151567  0.7046221          
   5    3.103773e-02  FALSE  1.151846  0.7087579          
   6    1.316455e-02   TRUE  1.151505  0.7048849          
   6    2.580930e+00  FALSE  1.179694  0.8291819          
   8    1.205263e-01  FALSE  1.152427  0.7260482          
   9    1.386895e-01   TRUE  1.152541  0.7289942          
  10    5.675367e-04   TRUE  1.150496  0.7147384          
  10    4.689059e-03  FALSE  1.150879  0.7127079          
  12    7.582850e-04   TRUE  1.150511  0.7149192          
  15    2.944146e-04   TRUE  1.150656  0.7125084          
  15    3.252118e-04   TRUE  1.150493  0.7133757  *       
  15    8.210370e-03  FALSE  1.151323  0.7057902          
  16    8.704734e-05   TRUE  1.224484  0.7083891          
  17    4.899928e+00   TRUE  1.194097  0.8681372          
  18    2.469658e-02  FALSE  1.151374  0.7106053          
  20    6.245483e-04   TRUE  1.150535  0.7138967          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 15, decay = 0.0003252118
 and bag = TRUE.
[1] "Wed Feb 28 15:37:22 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: executing %dopar% sequentially: no parallel backend registered 
2: In eval(xpr, envir = envir) :
  model fit failed for Fold1: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

3: In eval(xpr, envir = envir) :
  model fit failed for Fold2: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

4: In eval(xpr, envir = envir) :
  model fit failed for Fold3: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

5: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  model fit failed for Fold1: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

2: In eval(xpr, envir = envir) :
  model fit failed for Fold2: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

3: In eval(xpr, envir = envir) :
  model fit failed for Fold3: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                   "failed"                  
 [3] "Wed Feb 28 15:37:24 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "YeoJohnson"               "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "bag"                     
Loading required package: earth
Loading required package: plotmo
Loading required package: plotrix
Loading required package: TeachingDemos
Bagged MARS 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  degree  nprune  RMSE         Rsquared   Selected
  1       2       0.800369232  0.8416941          
  1       3       0.290756444  0.9666618          
  1       4       0.081483966  0.9971126          
  1       5       0.003886989  0.9999928  *       
  2       2       0.820778899  0.8089951          
  2       3       0.311976962  0.9617887          
  2       5       0.003924825  0.9999927          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nprune = 5 and degree = 1.
[1] "Wed Feb 28 15:37:39 2018"
Bagged MARS using gCV Pruning 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results:

  RMSE         Rsquared 
  0.003911377  0.9999927

Tuning parameter 'degree' was held constant at a value of 1
[1] "Wed Feb 28 15:37:45 2018"
Loading required package: mgcv
Loading required package: nlme
This is mgcv 1.8-17. For overview type 'help("mgcv-package")'.

Attaching package: 'mgcv'

The following object is masked from 'package:nnet':

    multinom

Generalized Additive Model using Splines 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  select  method  RMSE         Rsquared   Selected
  FALSE   GCV.Cp  0.003190185  0.9999952          
  FALSE   ML      0.003180595  0.9999952  *       
  FALSE   REML    0.003180841  0.9999952          
   TRUE   GCV.Cp  0.003194596  0.9999952          
   TRUE   ML      0.003180848  0.9999952          
   TRUE   REML    0.003180834  0.9999952          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were select = FALSE and method = ML.
[1] "Wed Feb 28 15:38:24 2018"
Loading required package: bartMachine
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Failed with error:  'package 'rJava' could not be loaded'
In addition: Warning message:
In max(c(NaN, NaN), na.rm = TRUE) :
  no non-missing arguments to max; returning -Inf
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package 'rJava' could not be loaded
Loading required package: bartMachine
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Failed with error:  'package 'rJava' could not be loaded'
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package 'rJava' could not be loaded
Loading required package: bartMachine
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Failed with error:  'package 'rJava' could not be loaded'
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package 'rJava' could not be loaded
 [1] "failed"                   "failed"                  
 [3] "Wed Feb 28 15:38:26 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "YeoJohnson"               "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "bartMachine"             
Loading required package: arm
Loading required package: MASS
Loading required package: Matrix
Loading required package: lme4

Attaching package: 'lme4'

The following object is masked from 'package:nlme':

    lmList


arm (Version 1.9-3, built: 2016-11-21)

Working directory is C:/Users/irina grishina/Desktop/generated data test/LAPTOP-1SBQTC5I


Attaching package: 'arm'

The following object is masked from 'package:plotrix':

    rescale

The following objects are masked from 'package:pls':

    coefplot, corrplot

Bayesian Generalized Linear Model 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results:

  RMSE         Rsquared 
  0.007902547  0.9999698

[1] "Wed Feb 28 15:38:29 2018"
Loading required package: monomvn
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
The Bayesian lasso 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  sparsity    RMSE         Rsquared   Selected
  0.07238307  0.008579707  0.9999697          
  0.10340567  0.008579707  0.9999697          
  0.11870339  0.008579707  0.9999697          
  0.15190523  0.008579707  0.9999697          
  0.22530082  0.008579707  0.9999697          
  0.24019490  0.008579707  0.9999697          
  0.26801212  0.008579707  0.9999697          
  0.29245749  0.008579707  0.9999697          
  0.36535981  0.008579707  0.9999697          
  0.44413967  0.008579707  0.9999697          
  0.49330484  0.008579707  0.9999697          
  0.49431225  0.008579707  0.9999697          
  0.55162130  0.008579707  0.9999697          
  0.70078533  0.008579707  0.9999697          
  0.71573529  0.008579707  0.9999697          
  0.74279587  0.008579707  0.9999697          
  0.76493775  0.008579707  0.9999697          
  0.82209933  0.008579707  0.9999697          
  0.85745459  0.008579707  0.9999697          
  0.96934448  0.008579707  0.9999697  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was sparsity = 0.9693445.
[1] "Wed Feb 28 15:38:33 2018"
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
Bayesian Ridge Regression (Model Averaged) 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results:

  RMSE         Rsquared 
  0.008483408  0.9999692

[1] "Wed Feb 28 15:38:36 2018"
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
Bayesian Ridge Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results:

  RMSE         Rsquared 
  0.008169224  0.9999695

[1] "Wed Feb 28 15:38:38 2018"
Loading required package: bst
Loading required package: gbm
Loading required package: splines
Loaded gbm 2.1.3
Boosted Linear Model 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  nu          mstop  RMSE        Rsquared   Selected
  0.03705772   76    0.32357079  0.9999319          
  0.06327962   60    0.18760695  0.9999697          
  0.09481892  382    0.01412232  0.9999699          
  0.14765111  350    0.01412232  0.9999699          
  0.15196461  371    0.01412232  0.9999699          
  0.17610706  247    0.01412232  0.9999699          
  0.18025734  484    0.01412232  0.9999699          
  0.18866994  276    0.01412232  0.9999699          
  0.26766339  247    0.01412232  0.9999699          
  0.29195055  358    0.01412232  0.9999699          
  0.31043313  113    0.01412232  0.9999699          
  0.31242071  134    0.01412232  0.9999699          
  0.33969824  428    0.01412232  0.9999699  *       
  0.34960701  120    0.01412232  0.9999699          
  0.40842799  183    0.01412232  0.9999699          
  0.41451401  222    0.01412232  0.9999699          
  0.47752066   52    0.01412232  0.9999699          
  0.51610627   37    0.01412233  0.9999699          
  0.54127566  146    0.01412232  0.9999699          
  0.56907060  411    0.01412232  0.9999699          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mstop = 428 and nu = 0.3396982.
[1] "Wed Feb 28 15:39:17 2018"
Loading required package: party
Loading required package: grid
Loading required package: mvtnorm
Loading required package: modeltools
Loading required package: stats4

Attaching package: 'modeltools'

The following object is masked from 'package:lme4':

    refit

The following object is masked from 'package:kernlab':

    prior

The following object is masked from 'package:plyr':

    empty

Loading required package: strucchange
Loading required package: zoo

Attaching package: 'zoo'

The following objects are masked from 'package:base':

    as.Date, as.Date.numeric

Loading required package: sandwich
Conditional Inference Random Forest 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   Selected
  1     0.6148388  0.9270130          
  2     0.2797059  0.9666658  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 2.
[1] "Wed Feb 28 15:39:36 2018"
Conditional Inference Tree 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  mincriterion  RMSE       Rsquared   Selected
  0.07238307    0.3770851  0.9312553          
  0.10340567    0.3770851  0.9312553          
  0.11870339    0.3770851  0.9312553          
  0.15190523    0.3770851  0.9312553          
  0.22530082    0.3770851  0.9312553          
  0.24019490    0.3770851  0.9312553          
  0.26801212    0.3770851  0.9312553          
  0.29245749    0.3770851  0.9312553          
  0.36535981    0.3770851  0.9312553          
  0.44413967    0.3770851  0.9312553          
  0.49330484    0.3770851  0.9312553          
  0.49431225    0.3770851  0.9312553          
  0.55162130    0.3770851  0.9312553          
  0.70078533    0.3770851  0.9312553          
  0.71573529    0.3770851  0.9312553          
  0.74279587    0.3770851  0.9312553          
  0.76493775    0.3770851  0.9312553          
  0.82209933    0.3770851  0.9312553          
  0.85745459    0.3770851  0.9312553  *       
  0.96934448    0.3782459  0.9308616          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mincriterion = 0.8574546.
[1] "Wed Feb 28 15:39:40 2018"
Conditional Inference Tree 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  maxdepth  mincriterion  RMSE       Rsquared   Selected
   2        0.10397266    0.8634269  0.6394518          
   2        0.79552698    0.8634269  0.6394518          
   2        0.85994369    0.8634269  0.6394518          
   3        0.06019653    0.7023796  0.7608196          
   4        0.51658285    0.5452755  0.8554404          
   4        0.58198165    0.5452755  0.8554404          
   5        0.51990101    0.4401274  0.9062749          
   5        0.90196271    0.4401274  0.9062749          
   6        0.68018028    0.3968980  0.9238262          
   7        0.69034059    0.3783444  0.9308012          
   8        0.29233232    0.3770851  0.9312553  *       
   8        0.44518095    0.3770851  0.9312553          
   9        0.31330541    0.3770851  0.9312553          
  11        0.24482656    0.3770851  0.9312553          
  11        0.48572712    0.3770851  0.9312553          
  12        0.15662592    0.3770851  0.9312553          
  12        0.25202772    0.3770851  0.9312553          
  13        0.56543946    0.3770851  0.9312553          
  13        0.94836495    0.3770851  0.9312553          
  15        0.29926101    0.3770851  0.9312553          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were maxdepth = 8 and mincriterion
 = 0.2923323.
[1] "Wed Feb 28 15:39:43 2018"
Loading required package: Cubist
Cubist 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  committees  neighbors  RMSE         Rsquared   Selected
   7          1          0.003867988  0.9999928          
  11          1          0.003874756  0.9999928          
  16          7          0.003512533  0.9999941          
  25          7          0.003491630  0.9999942          
  26          7          0.003488201  0.9999942          
  30          4          0.003528220  0.9999940          
  30          9          0.003460543  0.9999943          
  32          5          0.003495943  0.9999941          
  45          4          0.003507602  0.9999941          
  49          7          0.003453785  0.9999943          
  52          2          0.003666106  0.9999935          
  57          8          0.003444111  0.9999943  *       
  59          2          0.003669146  0.9999935          
  69          3          0.003575433  0.9999939          
  70          4          0.003518154  0.9999941          
  80          1          0.003877495  0.9999928          
  86          0          0.003544761  0.9999942          
  91          2          0.003663926  0.9999935          
  95          8          0.003446928  0.9999943          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were committees = 57 and neighbors = 8.
[1] "Wed Feb 28 15:40:22 2018"
Loading required package: deepnet
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
Stacked AutoEncoder Deep Neural Network 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  layer1  layer2  layer3  hidden_dropout  visible_dropout  RMSE      Rsquared 
   3      17      10      0.027757978     0.010313174      1.453886  0.7403917
   3      18      19      0.090773621     0.036429030      1.442650  0.7264155
   4       3       5      0.024125379     0.029166366      1.467206  0.2787424
   4       3      11      0.074727741     0.040652711      1.545022  0.8944803
   6      11      17      0.009170249     0.012058837      1.673239  0.7609285
   6      13      12      0.029411054     0.075929961      1.476115  0.2108030
   7      11      10      0.078554640     0.087680781      2.047740  0.6437467
   7      19      11      0.018234408     0.029187520      1.476300  0.5179515
   8      14      14      0.007078732     0.064394198      1.462154  0.5427137
  10      15       2      0.024600247     0.007471774      1.477846  0.3184573
  11       7       3      0.073854654     0.006696088      1.447270  0.4449872
  11      10      15      0.032653422     0.095937011      1.484736  0.3661934
  12       7       9      0.062332709     0.022465589      1.535811  0.7165814
  15       6       8      0.021004125     0.032730312      1.461477  0.5286476
  15      11      14      0.091440833     0.047007709      2.010024  0.4466091
  16       4       5      0.048708861     0.089561793      2.413558  0.5485017
  16       6       9      0.099537287     0.038901888      1.780510  0.2641813
  17      20      10      0.045836481     0.041686257      1.577335  0.5173298
  18      12      14      0.035641808     0.056387720      1.454755  0.1885693
  20       7       2      0.087948211     0.030927093      1.543677  0.2582263
  Selected
          
  *       
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were layer1 = 3, layer2 = 18, layer3 =
 19, hidden_dropout = 0.09077362 and visible_dropout = 0.03642903.
[1] "Wed Feb 28 15:40:28 2018"
Multivariate Adaptive Regression Spline 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  degree  nprune  RMSE         Rsquared   Selected
  1       2       1.011311346  0.5012194          
  1       3       0.498086805  0.8777842          
  1       4       0.177723321  0.9757733          
  1       5       0.004750095  0.9999893  *       
  2       2       1.011311346  0.5012194          
  2       3       0.498086805  0.8777842          
  2       5       0.004750095  0.9999893          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nprune = 5 and degree = 1.
[1] "Wed Feb 28 15:40:31 2018"
Loading required package: elmNN
Extreme Learning Machine 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  nhid  actfun   RMSE         Rsquared   Selected
   2    tansig   1.073657944  0.5051535          
   3    sin      0.615705881  0.8166189          
   5    purelin  0.007902547  0.9999698          
   6    purelin  0.007902547  0.9999698          
   6    tansig   0.300861797  0.9554580          
   7    purelin  0.007902547  0.9999698          
   9    purelin  0.007902547  0.9999698  *       
  10    radbas   0.435407711  0.8675529          
  11    radbas   0.383076318  0.9046146          
  14    radbas   0.286112113  0.9592100          
  14    sin      0.051161642  0.9983789          
  15    radbas   0.512696028  0.8732591          
  15    sin      0.044784225  0.9989114          
  16    tansig   0.091602324  0.9955686          
  17    purelin  0.007902547  0.9999698          
  19    radbas   0.241300799  0.9680802          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nhid = 9 and actfun = purelin.
[1] "Wed Feb 28 15:40:33 2018"
Loading required package: elasticnet
Elasticnet 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  lambda        fraction    RMSE        Rsquared   Selected
  2.718307e-05  0.85994369  0.20110375  0.9999623          
  4.172862e-05  0.79552698  0.29348772  0.9999495          
  5.154901e-05  0.10397266  1.28573756  0.9750042          
  8.155139e-05  0.06019653  1.34855204  0.9246119          
  2.248045e-04  0.51658285  0.69368660  0.9996744          
  2.761655e-04  0.58198165  0.59984869  0.9997975          
  4.055764e-04  0.51990101  0.68892050  0.9996823          
  5.685189e-04  0.90196271  0.14084963  0.9999668          
  1.556535e-03  0.68018028  0.45890859  0.9998977          
  4.622086e-03  0.69034059  0.44421770  0.9999046          
  9.116519e-03  0.44518095  0.79592736  0.9994434          
  9.244289e-03  0.29233232  1.01531770  0.9979604          
  2.040459e-02  0.31330541  0.98503054  0.9983300          
  1.602183e-01  0.24482656  1.08179469  0.9968213          
  1.969753e-01  0.48572712  0.73363669  0.9996162          
  2.862696e-01  0.25202772  1.07024211  0.9971277          
  3.887107e-01  0.15662592  1.20778787  0.9907115          
  8.562409e-01  0.94836495  0.05073192  0.9999698  *       
  1.395493e+00  0.56543946  0.60539106  0.9998247          
  6.547370e+00  0.29926101  0.99094883  0.9985385          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were fraction = 0.9483649 and lambda
 = 0.8562409.
[1] "Wed Feb 28 15:40:36 2018"
Loading required package: extraTrees
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Failed with error:  'package 'rJava' could not be loaded'
In addition: There were 48 warnings (use warnings() to see them)
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package 'rJava' could not be loaded
Loading required package: extraTrees
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Failed with error:  'package 'rJava' could not be loaded'
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package 'rJava' could not be loaded
Loading required package: extraTrees
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Failed with error:  'package 'rJava' could not be loaded'
note: only 1 unique complexity parameters in default grid. Truncating the grid to 1 .

Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package 'rJava' could not be loaded
 [1] "failed"                   "failed"                  
 [3] "Wed Feb 28 15:40:38 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "YeoJohnson"               "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "extraTrees"              
Loading required package: foba
Ridge Regression with Variable Selection 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  lambda        k  RMSE         Rsquared   Selected
  2.718307e-05  2  0.007902662  0.9999698  *       
  4.172862e-05  2  0.007902797  0.9999698          
  5.154901e-05  1  0.979085350  0.5343335          
  8.155139e-05  1  0.979085380  0.5343335          
  2.248045e-04  2  0.007908849  0.9999698          
  2.761655e-04  2  0.007911994  0.9999698          
  4.055764e-04  2  0.007922719  0.9999698          
  5.685189e-04  2  0.007941898  0.9999698          
  1.556535e-03  2  0.008189669  0.9999698          
  4.622086e-03  2  0.010137938  0.9999698          
  9.116519e-03  1  0.979139310  0.5343335          
  9.244289e-03  1  0.979140703  0.5343335          
  2.040459e-02  1  0.979327523  0.5343335          
  1.602183e-01  1  0.989830928  0.5343335          
  1.969753e-01  1  0.994282767  0.5343335          
  2.862696e-01  1  1.006643871  0.5343335          
  3.887107e-01  1  1.022280558  0.5343335          
  8.562409e-01  2  0.648629756  0.9999685          
  1.395493e+00  2  0.823146369  0.9999680          
  6.547370e+00  1  1.337241710  0.5343335          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were k = 2 and lambda = 2.718307e-05.
[1] "Wed Feb 28 15:40:40 2018"
Generalized Additive Model using Splines 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  select  method  RMSE         Rsquared   Selected
  FALSE   GCV.Cp  0.003202595  0.9999951          
  FALSE   ML      0.003180594  0.9999952  *       
   TRUE   GCV.Cp  0.003189018  0.9999952          
   TRUE   ML      0.003180848  0.9999952          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were select = FALSE and method = ML.
[1] "Wed Feb 28 15:40:44 2018"
Loading required package: gam
Loading required package: foreach
Loaded gam 1.14-4


Attaching package: 'gam'

The following objects are masked from 'package:mgcv':

    gam, gam.control, gam.fit, plot.gam, predict.gam, s, summary.gam

Generalized Additive Model using Splines 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  df         RMSE         Rsquared   Selected
  0.3619154  0.007902415  0.9999698          
  0.5170283  0.007902415  0.9999698          
  0.5935170  0.007902415  0.9999698          
  0.7595261  0.007902415  0.9999698          
  1.1265041  0.007249576  0.9999746          
  1.2009745  0.006886749  0.9999771          
  1.3400606  0.006258020  0.9999811          
  1.4622874  0.005757158  0.9999840          
  1.8267990  0.004587820  0.9999899          
  2.2206984  0.003836780  0.9999930          
  2.4665242  0.003575882  0.9999939          
  2.4715613  0.003571766  0.9999939          
  2.7581065  0.003396231  0.9999945          
  3.5039267  0.003230236  0.9999951          
  3.5786765  0.003223886  0.9999951          
  3.7139794  0.003214369  0.9999951          
  3.8246888  0.003208235  0.9999951          
  4.1104966  0.003197175  0.9999952          
  4.2872730  0.003192597  0.9999952          
  4.8467224  0.003184201  0.9999952  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was df = 4.846722.
[1] "Wed Feb 28 15:40:49 2018"
Error in .local(object, ...) : test vector does not match model !
In addition: Warning messages:
1: In max(c(NaN, NaN), na.rm = TRUE) :
  no non-missing arguments to max; returning -Inf
2: In max(c(NaN, NaN), na.rm = TRUE) :
  no non-missing arguments to max; returning -Inf
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Wed Feb 28 15:41:11 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "YeoJohnson"               "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "gaussprLinear"           
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Wed Feb 28 15:44:23 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "YeoJohnson"               "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "gaussprPoly"             
Loading required package: h2o

----------------------------------------------------------------------

Your next step is to start H2O:
    > h2o.init()

For H2O package documentation, ask for help:
    > ??h2o

After starting H2O, you can use the Web UI at http://localhost:54321
For more information visit http://docs.h2o.ai

----------------------------------------------------------------------


Attaching package: 'h2o'

The following objects are masked from 'package:stats':

    cor, sd, var

The following objects are masked from 'package:base':

    %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,
    colnames<-, ifelse, is.character, is.factor, is.numeric, log,
    log10, log1p, log2, round, signif, trunc

Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Wed Feb 28 15:45:49 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "YeoJohnson"               "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "gbm_h2o"                 
Multivariate Adaptive Regression Splines 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results:

  RMSE         Rsquared 
  0.004750095  0.9999893

Tuning parameter 'degree' was held constant at a value of 1
[1] "Wed Feb 28 15:45:51 2018"
Generalized Linear Model 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results:

  RMSE         Rsquared 
  0.007902547  0.9999698

[1] "Wed Feb 28 15:45:52 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

2: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

3: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

4: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

5: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

6: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

7: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

8: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

9: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

2: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

3: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

4: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

5: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

6: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

7: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=log Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

8: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=sqrt Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

9: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=identity Error in eval(family$initialize) : 
  negative values not allowed for the 'Poisson' family

10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Wed Feb 28 15:45:56 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "YeoJohnson"               "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "glm.nb"                  
Loading required package: mboost
Loading required package: stabs

Attaching package: 'stabs'

The following object is masked from 'package:modeltools':

    parameters

This is mboost 2.8-1. See 'package?mboost' and 'news(package  = "mboost")'
for a complete list of changes.


Attaching package: 'mboost'

The following object is masked from 'package:party':

    varimp

The following object is masked from 'package:ipred':

    cv

The following object is masked from 'package:MLmetrics':

    AUC

The following object is masked from 'package:ggplot2':

    %+%

Boosted Generalized Linear Model 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  mstop  prune  RMSE         Rsquared   Selected
   73    no     0.027630636  0.9999687          
  104    no     0.009305893  0.9999698          
  119    yes    0.008205018  0.9999698          
  152    yes    0.007913141  0.9999698          
  226    no     0.007902613  0.9999698          
  241    no     0.007902568  0.9999698          
  269    no     0.007902552  0.9999698          
  293    no     0.007902548  0.9999698          
  366    no     0.007902547  0.9999698          
  445    no     0.007902547  0.9999698          
  494    yes    0.007902543  0.9999698  *       
  495    yes    0.007902543  0.9999698          
  552    yes    0.007902543  0.9999698          
  701    yes    0.007902543  0.9999698          
  716    yes    0.007902543  0.9999698          
  743    yes    0.007902543  0.9999698          
  765    yes    0.007902543  0.9999698          
  823    no     0.007902547  0.9999698          
  858    no     0.007902547  0.9999698          
  970    yes    0.007902543  0.9999698          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mstop = 494 and prune = yes.
[1] "Wed Feb 28 15:46:04 2018"
Loading required package: glmnet
Loaded glmnet 2.0-10

glmnet 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  alpha       lambda       RMSE        Rsquared   Selected
  0.07238307  2.264618349  0.93315729  0.9999666          
  0.10340567  1.267381411  0.72354272  0.9999679          
  0.11870339  0.002492216  0.04409170  0.9999698          
  0.15190523  0.001679858  0.04224215  0.9999698          
  0.22530082  0.102633747  0.10345715  0.9999699          
  0.24019490  0.185021072  0.18023332  0.9999697          
  0.26801212  0.105748799  0.10854827  0.9999699          
  0.29245749  3.306982129  1.39785780  0.9607085          
  0.36535981  0.448240120  0.42091285  0.9999621          
  0.44413967  0.491215381  0.47818797  0.9999520          
  0.49330484  0.053934503  0.06261304  0.9999698          
  0.49431225  0.013605372  0.04173426  0.9999699  *       
  0.55162130  0.016435628  0.04195409  0.9999699          
  0.70078533  0.008867513  0.04204414  0.9999699          
  0.71573529  0.077721232  0.09739700  0.9999693          
  0.74279587  0.009461997  0.04411053  0.9999698          
  0.76493775  0.004005330  0.04320779  0.9999698          
  0.82209933  5.023679214  1.43431437        NaN          
  0.85745459  0.159398835  0.20865424  0.9999637          
  0.96934448  0.014481884  0.04401812  0.9999698          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were alpha = 0.4943123 and lambda
 = 0.01360537.
[1] "Wed Feb 28 15:46:08 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Wed Feb 28 15:47:31 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "YeoJohnson"               "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "glmnet_h2o"              
Start:  AIC=-3420.68
.outcome ~ V2 + V3

       Df Deviance     AIC
<none>        0.03 -3420.7
- V2    1   484.34  1412.6
- V3    1   519.02  1447.4
Start:  AIC=-3432.87
.outcome ~ V2 + V3

       Df Deviance     AIC
<none>        0.03 -3432.9
- V2    1   489.68  1418.1
- V3    1   495.57  1424.1
Start:  AIC=-3432.06
.outcome ~ V2 + V3

       Df Deviance     AIC
<none>        0.03 -3432.1
- V2    1   464.36  1388.0
- V3    1   518.52  1443.1
Start:  AIC=-5145.15
.outcome ~ V2 + V3

       Df Deviance     AIC
<none>        0.05 -5145.1
- V2    1   719.53  2106.9
- V3    1   766.94  2154.9
Generalized Linear Model with Stepwise Feature Selection 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results:

  RMSE         Rsquared 
  0.007902547  0.9999698

[1] "Wed Feb 28 15:47:33 2018"
Loading required package: fastICA
Independent Component Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  n.comp  RMSE         Rsquared   Selected
  1       0.024676197  0.9996122          
  2       0.007902547  0.9999698  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was n.comp = 2.
[1] "Wed Feb 28 15:47:36 2018"
Partial Least Squares 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  ncomp  RMSE         Rsquared   Selected
  1      0.042998986  0.9987594          
  2      0.007902547  0.9999698  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 2.
[1] "Wed Feb 28 15:47:38 2018"
k-Nearest Neighbors 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  k    RMSE       Rsquared   Selected
   19  0.2280661  0.9851530  *       
   26  0.2637698  0.9824251          
   30  0.2807251  0.9811738          
   38  0.3108912  0.9793680          
   57  0.3824176  0.9733646          
   61  0.3954217  0.9725628          
   68  0.4190526  0.9703406          
   74  0.4371405  0.9694138          
   92  0.4911850  0.9650351          
  112  0.5468824  0.9604667          
  124  0.5788484  0.9581411          
  138  0.6144278  0.9551175          
  176  0.7020934  0.9482833          
  179  0.7086864  0.9479371          
  186  0.7235747  0.9465610          
  192  0.7357190  0.9454266          
  206  0.7643709  0.9427840          
  215  0.7827619  0.9410464          
  243  0.8391523  0.9357135          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was k = 19.
[1] "Wed Feb 28 15:47:42 2018"
Loading required package: KRLS
## KRLS Package for Kernel-based Regularized Least Squares.

## See Hainmueller and Hazlett (2014) for details.

Polynomial Kernel Regularized Least Squares 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 502, 502, 500 
Resampling results across tuning parameters:

  lambda        degree  RMSE          Rsquared      Selected
  2.300993e-05  3         793.435719  0.0105539530          
  3.288731e-05  3         555.113934  0.0105612048          
  3.922085e-05  1       13232.706113  0.0009508284          
  5.748124e-05  1        9028.990240  0.0009513350          
  1.338148e-04  2         182.038886  0.2052677262          
  1.588454e-04  2         153.416561  0.2052391661          
  2.188067e-04  2         111.484946  0.2051707543          
  2.899262e-04  3          62.917926  0.0107520119          
  6.711182e-04  3          27.167323  0.0110424137          
  1.662258e-03  3          10.996935  0.0118388722          
  2.927684e-03  2           8.781313  0.2020869228          
  2.961838e-03  1         175.212547  0.0010338554          
  5.729365e-03  1          90.574060  0.0011160845          
  3.190999e-02  1          16.294340  0.0020671242          
  3.790325e-02  2           1.651410  0.1634493242          
  5.175822e-02  1          10.088613  0.0029958376          
  6.678651e-02  1           7.855850  0.0038173128          
  1.289724e-01  3           1.347291  0.3355201991  *       
  1.937639e-01  2           1.398137  0.1410229312          
  7.026231e-01  1           1.470385  0.1146859140          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were lambda = 0.1289724 and degree = 3.
[1] "Wed Feb 28 15:48:48 2018"

 Average Marginal Effects:
 
       V2        V3 
0.9997405 1.0000774 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9937597 0.996642
50% 1.0006919 1.000775
75% 1.0068101 1.004472

 Average Marginal Effects:
 
       V2        V3 
0.9787227 0.9779489 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9857483 0.9828378
50% 1.0026552 1.0021479
75% 1.0119809 1.0147680

 Average Marginal Effects:
 
       V2        V3 
0.9996597 1.0000152 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9940939 0.9968307
50% 1.0012323 1.0011359
75% 1.0069019 1.0047344

 Average Marginal Effects:
 
       V2        V3 
0.9974871 0.9982281 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9947654 0.9953591
50% 1.0004157 1.0012727
75% 1.0034278 1.0034968

 Average Marginal Effects:
 
      V2       V3 
0.999531 1.000101 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9967449 0.9983921
50% 1.0001282 1.0006397
75% 1.0027900 1.0023397

 Average Marginal Effects:
 
       V2        V3 
0.9996782 0.9998441 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9925145 0.9961978
50% 0.9987324 0.9986698
75% 1.0076179 1.0043294

 Average Marginal Effects:
 
       V2        V3 
0.9817079 0.9846845 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9543882 0.9578236
50% 1.0087938 1.0153895
75% 1.0355981 1.0381468

 Average Marginal Effects:
 
       V2        V3 
0.9962256 0.9970360 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9931196 0.993602
50% 0.9994456 1.000611
75% 1.0027241 1.003231

 Average Marginal Effects:
 
       V2        V3 
0.9997151 1.0001963 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9953896 0.997626
50% 1.0006152 1.001003
75% 1.0047338 1.003556

 Average Marginal Effects:
 
       V2        V3 
0.9924283 0.9941711 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9792911 0.9792931
50% 1.0057675 1.0100374
75% 1.0200575 1.0211413

 Average Marginal Effects:
 
       V2        V3 
0.9965744 0.9969724 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9922457 0.9962190
50% 1.0004905 0.9990343
75% 1.0071627 1.0041195

 Average Marginal Effects:
 
       V2        V3 
0.9992874 0.9988771 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9924244 0.9941523
50% 0.9977342 0.9985936
75% 1.0066027 1.0048300

 Average Marginal Effects:
 
       V2        V3 
0.9534065 0.9540535 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9519167 0.9525331
50% 0.9550586 0.9559181
75% 0.9566175 0.9572461

 Average Marginal Effects:
 
       V2        V3 
0.9989366 0.9982238 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9912390 0.9923082
50% 0.9986376 0.9985973
75% 1.0066397 1.0076927

 Average Marginal Effects:
 
       V2        V3 
0.9976334 0.9987825 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9905089 0.990084
50% 1.0057294 1.008761
75% 1.0145515 1.015295

 Average Marginal Effects:
 
       V2        V3 
0.7005576 0.7003245 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.6987358 0.6986706
50% 0.7024295 0.7024638
75% 0.7041934 0.7039519

 Average Marginal Effects:
 
       V2        V3 
0.9686810 0.9693877 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9672946 0.9679152
50% 0.9701483 0.9711534
75% 0.9717502 0.9724215

 Average Marginal Effects:
 
       V2        V3 
0.8696855 0.8699141 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.8690911 0.8693709
50% 0.8703070 0.8706345
75% 0.8708982 0.8711246

 Average Marginal Effects:
 
       V2        V3 
0.9995681 0.9997674 

 Quartiles of Marginal Effects:
 
          V2        V3
25% 0.991815 0.9958561
50% 1.000636 1.0010814
75% 1.007941 1.0045068

 Average Marginal Effects:
 
       V2        V3 
0.9987044 0.9989920 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9921677 0.9964658
50% 1.0000948 0.9994933
75% 1.0079427 1.0046289

 Average Marginal Effects:
 
      V2       V3 
0.999990 1.000277 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9932618 0.9962389
50% 1.0005702 0.9998483
75% 1.0075965 1.0042206

 Average Marginal Effects:
 
       V2        V3 
0.9748236 0.9788303 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9788089 0.9818584
50% 0.9984050 0.9966618
75% 1.0122622 1.0138624

 Average Marginal Effects:
 
       V2        V3 
0.9999266 1.0002849 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9935841 0.9965098
50% 1.0010573 1.0001261
75% 1.0076119 1.0041906

 Average Marginal Effects:
 
       V2        V3 
0.9977115 0.9988851 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9951397 0.9968374
50% 1.0003349 1.0017281
75% 1.0033823 1.0039300

 Average Marginal Effects:
 
      V2       V3 
0.999579 1.000238 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9967377 0.9985607
50% 1.0001885 1.0007456
75% 1.0028642 1.0023942

 Average Marginal Effects:
 
       V2        V3 
0.9998976 1.0000072 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9925403 0.9957443
50% 0.9988973 0.9986157
75% 1.0073187 1.0042330

 Average Marginal Effects:
 
       V2        V3 
0.9835251 0.9903614 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9598412 0.9710614
50% 1.0093058 1.0177472
75% 1.0349395 1.0398892

 Average Marginal Effects:
 
       V2        V3 
0.9964318 0.9977789 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9932963 0.9952628
50% 0.9994809 1.0011216
75% 1.0027852 1.0036620

 Average Marginal Effects:
 
      V2       V3 
0.999869 1.000399 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9955015 0.9979698
50% 1.0007076 1.0011800
75% 1.0049285 1.0034749

 Average Marginal Effects:
 
       V2        V3 
0.9935978 0.9974477 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9800235 0.987392
50% 1.0061529 1.012144
75% 1.0200691 1.023044

 Average Marginal Effects:
 
       V2        V3 
0.9960642 0.9970359 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9928056 0.9955035
50% 1.0003648 0.9995299
75% 1.0075141 1.0046245

 Average Marginal Effects:
 
       V2        V3 
0.9994426 0.9994846 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9923824 0.9944775
50% 0.9974651 0.9981280
75% 1.0082453 1.0054347

 Average Marginal Effects:
 
       V2        V3 
0.9514242 0.9526324 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9501394 0.9514414
50% 0.9529353 0.9544578
75% 0.9544838 0.9557369

 Average Marginal Effects:
 
       V2        V3 
0.9990959 0.9990554 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9909632 0.9923353
50% 0.9978690 0.9977753
75% 1.0084901 1.0076644

 Average Marginal Effects:
 
       V2        V3 
0.9985023 1.0008666 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9897609 0.9945663
50% 1.0055609 1.0085871
75% 1.0153251 1.0155399

 Average Marginal Effects:
 
       V2        V3 
0.6910212 0.6917885 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.6897179 0.6904680
50% 0.6928015 0.6938016
75% 0.6944702 0.6952397

 Average Marginal Effects:
 
      V2       V3 
0.967347 0.968557 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9660715 0.9673646
50% 0.9687553 0.9703308
75% 0.9702520 0.9715184

 Average Marginal Effects:
 
       V2        V3 
0.8644042 0.8652815 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.8639451 0.8648428
50% 0.8649967 0.8659685
75% 0.8655598 0.8664506

 Average Marginal Effects:
 
       V2        V3 
0.9994588 0.9998430 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9917725 0.9957931
50% 1.0012452 1.0006874
75% 1.0067575 1.0044717

 Average Marginal Effects:
 
       V2        V3 
0.9988937 0.9991844 

 Quartiles of Marginal Effects:
 
          V2        V3
25% 0.992590 0.9955920
50% 1.000947 0.9997454
75% 1.007818 1.0044612

 Average Marginal Effects:
 
      V2       V3 
1.000037 1.000340 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9937066 0.9964999
50% 1.0000435 0.9999997
75% 1.0075942 1.0044230

 Average Marginal Effects:
 
       V2        V3 
0.9733035 0.9721987 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9730206 0.9811907
50% 0.9988482 0.9962495
75% 1.0214730 1.0121023

 Average Marginal Effects:
 
       V2        V3 
0.9999505 1.0003556 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9938985 0.9967985
50% 1.0006198 1.0004184
75% 1.0074071 1.0045108

 Average Marginal Effects:
 
       V2        V3 
0.9980918 0.9990857 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9957824 0.9978289
50% 1.0010491 1.0020892
75% 1.0039635 1.0040535

 Average Marginal Effects:
 
      V2       V3 
0.999984 1.000208 

 Quartiles of Marginal Effects:
 
          V2       V3
25% 0.997324 0.998673
50% 1.000407 1.000804
75% 1.003268 1.002530

 Average Marginal Effects:
 
       V2        V3 
0.9999793 1.0000755 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9931108 0.9956568
50% 0.9984603 0.9989881
75% 1.0071438 1.0046694

 Average Marginal Effects:
 
       V2        V3 
0.9833775 0.9924479 

 Quartiles of Marginal Effects:
 
           V2       V3
25% 0.9615931 0.970580
50% 1.0081585 1.022416
75% 1.0347507 1.043661

 Average Marginal Effects:
 
       V2        V3 
0.9968849 0.9980452 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9944828 0.9962115
50% 1.0000450 1.0016131
75% 1.0032927 1.0038838

 Average Marginal Effects:
 
      V2       V3 
1.000111 1.000421 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9958502 0.9982742
50% 1.0006633 1.0012856
75% 1.0052509 1.0036937

 Average Marginal Effects:
 
       V2        V3 
0.9936613 0.9985059 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9846014 0.9876007
50% 1.0067272 1.0141559
75% 1.0202786 1.0248239

 Average Marginal Effects:
 
       V2        V3 
0.9956967 0.9968037 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9933487 0.9956858
50% 0.9990181 1.0000733
75% 1.0070212 1.0045541

 Average Marginal Effects:
 
       V2        V3 
0.9996599 0.9993276 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9931434 0.9936233
50% 0.9970563 0.9982762
75% 1.0075047 1.0065726

 Average Marginal Effects:
 
       V2        V3 
0.9539033 0.9541692 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9526257 0.9530282
50% 0.9554560 0.9561024
75% 0.9570074 0.9573874

 Average Marginal Effects:
 
       V2        V3 
0.9993547 0.9988971 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9921559 0.9914786
50% 0.9976197 0.9982966
75% 1.0079967 1.0093565

 Average Marginal Effects:
 
       V2        V3 
0.9983815 1.0013932 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9922322 0.9959012
50% 1.0066504 1.0098417
75% 1.0151373 1.0157106

 Average Marginal Effects:
 
       V2        V3 
0.7002586 0.6992362 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.6987717 0.6978132
50% 0.7021024 0.7014558
75% 0.7037917 0.7029230

 Average Marginal Effects:
 
       V2        V3 
0.9692304 0.9695811 

 Quartiles of Marginal Effects:
 
           V2        V3
25% 0.9681485 0.9684801
50% 0.9707191 0.9714693
75% 0.9722198 0.9726200

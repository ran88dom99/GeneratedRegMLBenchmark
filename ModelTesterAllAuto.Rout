
R version 3.4.3 (2017-11-30) -- "Kite-Eating Tree"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> options(repos=structure(c(CRAN="https://rweb.crmda.ku.edu/cran/")))
> ## capture messages and errors to a file.https://rweb.crmda.ku.edu/cran/
> #zz <- file("all.Rout", open="wt")https://cran.cnr.berkeley.edu
> #sink(zz, type="message") edit for rebaseless
> #chek for R package updates
> #try(log("a")) ## test --no-edit
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> which.computer<-Sys.info()[['nodename']]
> task.subject<-"14th20hp3cv"
> out.file<-paste("out",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,".csv",sep="")
> importance.file<-paste("importance",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,sep="")
> 
> base.folder<-getwd()
> cpout.folder<-paste(base.folder,"/",which.computer,sep = "")
> setwd(cpout.folder)
> 
> if(length(which(list.files() == out.file))<1) write.table( "0.01,0.01,100,100,100,Wed Aug 02 16:37:25 2017,dummy,8,1,basic latent features,ignore,none,asis,1.12784979099243,random,333,53,adaptive_cv,16,5,2,2,19,0.0107744822639878,FALSE,,,,,,,,,," ,file =,out.file,  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,".csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,".csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,"mlr.csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,"mlr.csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> 
> cv.iters=3
> tuneLength=20
> tuneLength2=8
> normings=c("YeoJohnson","ICA", "centernscale","expoTrans","range01","asis","quantile")#,"centernscale"
> 
> gensTTesto<-c(56,53,4,12,13,14,15,20,45,54,55, 44,52,1,3)#,  51,c(4)#c(1:40)#c(5,10,11,13,14,15,16,17,18,19,20,21,24,28,38,39,40)
> write.table( t(gensTTesto),file = "initial tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
> try({
+   gensTTest<-(read.csv("tasks to test.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+ })
> if(length(gensTTest)<1) gensTTest<-gensTTesto#inversion[length(inversion):1]
> 
> ########packages install check######
> 
> #list.of.packages <- c("caret","caretEnsemble","mlr","MLmetrics","tgp")
> #list.of.packages <- c("gower","dimRed","DEoptimR","caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> #new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> #if(length(new.packages)) install.packages(new.packages, dep = TRUE)
> 
> 
> #install.packages("mlr", dependencies = c("Depends", "Suggests"))
> #install.packages("caret", dependencies = c("Depends", "Suggests"))
> #install.packages("caret",repos = "http://cran.r-project.org",dependencies = c("Depends", "Imports", "Suggests"))
> #install.packages("SuperLearner", dependencies = c("Depends", "Suggests"))
> #install.packages("rattle", dependencies = c("Depends", "Suggests"))
> 
> # Load libraries
> #library(mlbench)
> 
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> #library(caretEnsemble)
> library(MLmetrics)

Attaching package: 'MLmetrics'

The following objects are masked from 'package:caret':

    MAE, RMSE

The following object is masked from 'package:base':

    Recall

> 
> ########error no repeat#########
> 
> 
> try({
+   before.last.alg<-as.matrix(read.csv("beforelast algorithm.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.alg<-as.matrix(read.csv("last algorithm tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   #write.table(paste(date(), last.alg,.Platform$OS.type,.Platform$r_arch,which.computer,sep=" "),file = "algos after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.alg==before.last.alg){print("algorithm may be broken")}
+   write.table(last.alg,file = "beforelast algorithm.csv",  quote = F, row.names = F,col.names = F)
+ })
> try({
+   before.last.tsk<-as.matrix(read.csv("beforelast task.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.tsk<-as.matrix(read.csv("last task tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   write.table(paste(date(),last.alg, last.tsk,cv.iters,tuneLength,.Platform$OS.type,.Platform$r_arch,which.computer,sep=","),file = "test after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.tsk==before.last.tsk){print("task may be broken")}
+   write.table(last.tsk,file = "beforelast task.csv",  quote = F, row.names = F,col.names = F)
+ })
[1] "task may be broken"
> bad.models=c("spaccceeee")
> previous.fails<-(read.csv("test after which reset.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
> previous.fails<-previous.fails[previous.fails[,8]==which.computer,]
> lgf<-length(previous.fails[,2])
> for(lt in 2:lgf)  {
+   if(previous.fails[lt,2]==previous.fails[lt-1,2])  {
+     bad.models=union(bad.models,c(paste(previous.fails[lt,2])))  }}
> 
> #######not to redo a test function#####
> check.redundant<-function(df=df.previous.calcs,norming="asis",trans.y=1,withextra="missing",missingdata="leaveempty",datasource="mean" ,column.to.predict=200,allmodel="ctree")
+ {
+   for(intern in 1:length(df[,1])){
+     if((any(df[intern,] == norming, na.rm=T))&&
+        (any(df[intern,] == withextra, na.rm=T))&&
+        (any(df[intern,] == missingdata, na.rm=T))&&
+        (any(df[intern,] == datasource, na.rm=T))&&
+        (any(df[intern,] == column.to.predict, na.rm=T))&&
+        (any(df[intern,] == allmodel, na.rm=T))&&
+        (  (df[intern,9] == trans.y)))
+     {return(TRUE)}
+   }
+   return(FALSE)
+ }
> #####caret init#####
> best.ranged <- c("avNNet", "nnet", "pcaNNet", "glm.nb")
> best.asis <- c("svmLinear3", "relaxo", "superpc", "xgbTree")
> best.cns <- c("gam", "bam", "svmLinear2", "msaenet", "BstLm", "gbm") 
> 
> cv6hp5 <- c( "BstLm", "qrnn")#earth
> cv3hp32 <- c("Rborist", "pcaNNet", "SBC")
> cv7x5hp32 <- c("gbm", "krlsPoly", "kknn", "xgbLinear","RRF", "cubist", "rlm" )
> cv6hp5.avoid <- c("pcaNNet")
> cv3hp32.avoid <- c("glm.nb", "gamboost", "ctree2","glmboost", "leapSeq","ctree","svmLinear2")
> cv7x5hp32.avoid <- c("SBC","bagearthgcv","gcvearth","lmStepAIC","glmStepAIC","bridge","lm","glm","bayesglm","blassoAveraged","treebag","rpart1SE")
> 
> allmodels <- c("avNNet", "bagEarth", "bagEarthGCV",
+                "bayesglm", "bdk", "blackboost", "Boruta", "brnn", "BstLm" ,
+                "bstTree", "cforest", "ctree", "ctree2", "cubist", "DENFIS",
+                "dnn", "earth", "elm", "enet",   "evtree",
+                "extraTrees",  "gamLoess",  "gaussprLinear", "gaussprPoly", "gaussprRadial",
+                "gcvEarth","glm", "glmboost",  "icr", "kernelpls",
+                "kknn", "knn",  "krlsRadial", "lars" , "lasso",
+                "leapBackward", "leapForward", "leapSeq", "lm", "M5", "M5Rules",
+                "mlpWeightDecay", "neuralnet" , "partDSA",
+                "pcaNNet", "pcr", "penalized", "pls", "plsRglm", "ppr",
+                "qrf" , "ranger",  "rf")
> allmodels <- c("rlm", "rpart", "rpart2",
+                "RRF", "RRFglobal",  "simpls",
+                "svmLinear", "svmPoly", "svmRadial", "svmRadialCost",
+                "widekernelpls",  "xgbLinear",
+                "xgbTree")
> allmodels <- c("avNNet","BstLm","bstTree","cforest","ctree","ctree2",
+                "cubist","earth","enet","evtree","glmboost",
+                "icr","kernelpls","kknn","lasso","pcaNNet",
+                "pcr","pls","qrf","ranger","rf")
> 
> allmodels <- c("kknn", "cubist", "avNNet", "xgbLinear", "RRF", "pcaNNet","earth","nnet","gbm","enet","lasso","BstLm",
+                "foba", "leapBackward", "gcvEarth", "SBC","glm.nb","gamboost","ctree2","relaxo", 
+                "bartMachine","extraTrees","bam","gam","randomGLM")
> #allmodels <- c("bam")
> #allmodels <- c("rf")"rqlasso",, "xyf" "rvmPoly", "rvmRadial",    "spls", "superpc" ,   "treebag",  "svmLinear2",  "SBC",
> #allmodels <- c("bartMachine", "xgbLinear", "pcaNNet","svmLinear","glmnet","cforest","cubist","rf","ranger")"glmnet",
> #wow rfRules is really slow "rfRules","WM", takes 50min
> # brak everythig "rbfDDA","ridge","rqnc",
> # use "rf" to test all
> library(caret)
> allmodels <- unique(modelLookup()[modelLookup()$forReg,c(1)])
> #allmodels <-c("avNNet", "nnet", "pcaNNet",  "glm.nb", "gam" ,
> #              "bam","msaenet", "svmLinear2","svmLinear3",
> #              "relaxo",  "superpc", "xgbTree", "BstLm")
> #allmodels<- c("svmLinear","svmPoly","svmRadial")
> #library(doParallel); cl <- makeCluster(detectCores()); registerDoParallel(cl)
> #allmodels<-c("bartMachine","extraTrees")#,"randomGLM"
> 
> 
> adaptControl <- trainControl(method = "adaptive_cv",
+                              number = 7, repeats = 5,
+                              adaptive = list(min = 4, alpha = 0.05,
+                                              method = "gls", complete = FALSE),
+                              search = "random")
> adaptControl <-trainControl(method = "cv", number = cv.iters,  search = "random")
> simpleControl <- trainControl(method = "cv",
+                               number = cv.iters,
+                               search = "random")
> 
> 
> #########MLR init######
> #R.utils::gcDLLs()
> #list.of.packages <- c("ParamHelpers","devtools","mlrMBO","RJSONIO","plot3D","plotly")
> #install.packages("mlrMBO", dependencies = c("Depends", "Suggests"))
> list.of.packages <- c("caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> if(length(new.packages)) install.packages(new.packages, dep = TRUE)
Warning message:
packages 'logicFS', ' RWeka', 'rPython', 'rrf' are not available (for R version 3.4.3) 
> 
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> tuneLengthMLR<-tuneLength
> mlr.iters<-cv.iters
> #######data read process start#####
> seed.var =222+round(runif(1,min=0,max=100))
> column.to.predict=1
> print(date());
[1] "Sat Feb 03 17:09:09 2018"
> 
> setwd(base.folder)
> if(!exists("gen.count")){gen.count=56}
> gens.names<-as.matrix(read.table("gens names.csv", sep = ",",header = FALSE,row.names=1,fill=TRUE, quote="",dec="."))
> for(gend.data in gensTTest){
+   setwd(base.folder)
+   data.source<-as.matrix(read.csv(paste("Generats/",gens.names[gend.data],".csv", sep = ""), sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   datasource<-gens.names[gend.data,1]
+   setwd(cpout.folder)
+   missingdatas=c("ignore")
+   for(missingdata in missingdatas){
+     withextras=c("none")
+     for(withextra in withextras){
+       ################data wrestling###############
+       
+       dependant.selection=complete.cases(data.source[,column.to.predict])
+       df.previous.calcs=as.data.frame(read.csv(file=out.file, header = FALSE, sep = ",", quote = "",
+                                                dec = ".", fill = TRUE, comment.char = ""))
+       unimportant.computations<-vector(mode = "logical",length=length(df.previous.calcs[,1])  )
+       for(intern in 1:length(df.previous.calcs[,1])){
+         if((any(df.previous.calcs[intern,] == withextra, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == missingdata, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == datasource, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == column.to.predict, na.rm=T)))
+         {unimportant.computations[intern]<-T}}
+       
+       df.previous.calcs<-df.previous.calcs[unimportant.computations,]
+       
+       
+       
+       #data.source=data.frame( data.source[,column.to.predict],data.source[,1:2], data.source[,4:(column.to.predict-1)], data.source[,(column.to.predict+1):length( data.source[1,])])
+       
+       
+         for(norming in normings) {
+         for(trans.y in 1:2) {
+           df.toprocess=data.source
+           y.untransformed<-df.toprocess[,1]
+           
+           if(norming=="centernscale"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("center", "scale"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="range01"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="expoTrans"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("expoTrans"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="YeoJohnson"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("YeoJohnson"))#"center", "scale",
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           if((norming=="asis")&&(trans.y==2)){next}
+           
+           
+           ################preprocess###########
+           df.toprocess=data.frame(df.toprocess[dependant.selection,])
+           y.untransformed=y.untransformed[dependant.selection]
+           if(norming=="quantile"){
+             for(Clol in trans.y:length(data.source[1,])){
+               df.toprocess[,Clol]<- (rank(df.toprocess[,Clol],na.last = "keep",ties.method = "average")-1) }
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           
+           loess.model<-loess(y.untransformed~ df.toprocess[,1],span = 0.21, degree = 1)
+           
+           
+           #df.toprocess = data.frame(df.toprocess,)
+           nzv <- nearZeroVar(df.toprocess[,])#, saveMetrics= TRUE
+           #nzv[nzv$nzv,][1:10,]
+           if(length(nzv)>1){
+             df.toprocess = (df.toprocess[, -nzv])}
+           
+           seed.var =222+round(runif(1,min=0,max=100))
+           set.seed(seed.var)
+           inTrain <- createDataPartition(y = df.toprocess[,1],
+                                          p = .75,
+                                          list = FALSE)
+           training <- df.toprocess[ inTrain,]
+           testing  <- df.toprocess[-inTrain,]
+           write.table(df.toprocess,file = "sanity check 1.csv",  quote = F, row.names = F,col.names = F)
+           
+           
+           
+           ###########for all models#################
+           setwd(base.folder)
+           if(max(which.computer==c("ALTA","HOPPER"))>0)
+             source("MLR part.R")
+           else
+             source("Caret part.R")
+           
+          setwd(cpout.folder)
+           if(norming == normings[length(normings)]){
+             write.table( gensTTest[-1],file = "tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)}
+           
+         }
+       }
+     }
+   }
+   
+ }

Attaching package: 'mlr'

The following object is masked from 'package:caret':

    train

[Tune] Started tuning learner regr.randomForest for parameter set:
            Type len Def  Constr Req Tunable Trafo
nodesize integer   -   1 1 to 10   -    TRUE     -
mtry     integer   -   3 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nodesize=5; mtry=9
[Tune-y] 1: rmse.test.rmse=1.08; time: 61.6 min
[Tune-x] 2: nodesize=6; mtry=5
[Tune-y] 2: rmse.test.rmse=1.05; time: 39.0 min
[Tune-x] 3: nodesize=7; mtry=8
[Tune-y] 3: rmse.test.rmse=1.06; time: 45.1 min
[Tune-x] 4: nodesize=4; mtry=4
[Tune-y] 4: rmse.test.rmse=1.04; time: 39.2 min
[Tune-x] 5: nodesize=10; mtry=2
[Tune-y] 5: rmse.test.rmse=1.41; time: 8.3 min
[Tune-x] 6: nodesize=6; mtry=3
[Tune-y] 6: rmse.test.rmse=1.06; time: 21.6 min
[Tune-x] 7: nodesize=1; mtry=5
[Tune-y] 7: rmse.test.rmse=1.08; time: 88.9 min
[Tune-x] 8: nodesize=2; mtry=6
[Tune-y] 8: rmse.test.rmse=1.09; time: 82.0 min
[Tune-x] 9: nodesize=9; mtry=9
[Tune-y] 9: rmse.test.rmse=1.06; time: 40.0 min
[Tune-x] 10: nodesize=5; mtry=4
[Tune-y] 10: rmse.test.rmse=1.04; time: 33.7 min
[Tune-x] 11: nodesize=1; mtry=9
[Tune-y] 11: rmse.test.rmse=1.12; time: 161.3 min
[Tune-x] 12: nodesize=2; mtry=7
[Tune-y] 12: rmse.test.rmse= 1.1; time: 99.6 min
[Tune-x] 13: nodesize=9; mtry=4
[Tune-y] 13: rmse.test.rmse=1.03; time: 22.0 min
[Tune-x] 14: nodesize=4; mtry=5
[Tune-y] 14: rmse.test.rmse=1.06; time: 56.7 min
[Tune-x] 15: nodesize=8; mtry=2
[Tune-y] 15: rmse.test.rmse= 1.4; time: 8.6 min
[Tune-x] 16: nodesize=10; mtry=3
[Tune-y] 16: rmse.test.rmse=1.06; time: 14.5 min
[Tune-x] 17: nodesize=10; mtry=7
[Tune-y] 17: rmse.test.rmse=1.05; time: 28.4 min
[Tune-x] 18: nodesize=7; mtry=8
[Tune-y] 18: rmse.test.rmse=1.06; time: 42.6 min
[Tune-x] 19: nodesize=1; mtry=7
[Tune-y] 19: rmse.test.rmse=1.11; time: 120.8 min
[Tune-x] 20: nodesize=8; mtry=3
[Tune-y] 20: rmse.test.rmse=1.06; time: 17.6 min
[Tune] Result: nodesize=9; mtry=4 : rmse.test.rmse=1.03
[1] "Sun Feb 04 12:52:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
In addition: Warning message:
replacing previous import 'BBmisc::isFALSE' by 'backports::isFALSE' when loading 'mlr' 
[1] "Sun Feb 04 12:59:13 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3 1 to 10   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=5; min.node.size=9
[Tune-y] 1: rmse.test.rmse=1.04; time: 3.1 min
[Tune-x] 2: mtry=6; min.node.size=5
[Tune-y] 2: rmse.test.rmse=1.06; time: 3.9 min
[Tune-x] 3: mtry=7; min.node.size=8
[Tune-y] 3: rmse.test.rmse=1.05; time: 3.7 min
[Tune-x] 4: mtry=4; min.node.size=4
[Tune-y] 4: rmse.test.rmse=1.04; time: 2.9 min
[Tune-x] 5: mtry=10; min.node.size=2
[Tune-y] 5: rmse.test.rmse=1.12; time: 6.6 min
[Tune-x] 6: mtry=6; min.node.size=3
[Tune-y] 6: rmse.test.rmse=1.08; time: 7.1 min
[Tune-x] 7: mtry=1; min.node.size=5
[Tune-y] 7: rmse.test.rmse=2.83; time: 0.8 min
[Tune-x] 8: mtry=2; min.node.size=6
[Tune-y] 8: rmse.test.rmse=1.39; time: 1.7 min
[Tune-x] 9: mtry=9; min.node.size=9
[Tune-y] 9: rmse.test.rmse=1.06; time: 5.3 min
[Tune-x] 10: mtry=5; min.node.size=4
[Tune-y] 10: rmse.test.rmse=1.06; time: 4.6 min
[Tune-x] 11: mtry=1; min.node.size=9
[Tune-y] 11: rmse.test.rmse=2.87; time: 0.8 min
[Tune-x] 12: mtry=2; min.node.size=7
[Tune-y] 12: rmse.test.rmse=1.39; time: 1.9 min
[Tune-x] 13: mtry=9; min.node.size=4
[Tune-y] 13: rmse.test.rmse=1.09; time: 6.1 min
[Tune-x] 14: mtry=4; min.node.size=5
[Tune-y] 14: rmse.test.rmse=1.04; time: 3.0 min
[Tune-x] 15: mtry=8; min.node.size=2
[Tune-y] 15: rmse.test.rmse=1.11; time: 10.4 min
[Tune-x] 16: mtry=10; min.node.size=3
[Tune-y] 16: rmse.test.rmse=1.11; time: 8.1 min
[Tune-x] 17: mtry=10; min.node.size=7
[Tune-y] 17: rmse.test.rmse=1.07; time: 6.3 min
[Tune-x] 18: mtry=7; min.node.size=8
[Tune-y] 18: rmse.test.rmse=1.05; time: 4.5 min
[Tune-x] 19: mtry=1; min.node.size=7
[Tune-y] 19: rmse.test.rmse=2.84; time: 1.0 min
[Tune-x] 20: mtry=8; min.node.size=3
[Tune-y] 20: rmse.test.rmse= 1.1; time: 9.6 min
[Tune] Result: mtry=5; min.node.size=9 : rmse.test.rmse=1.04
[1] "Sun Feb 04 14:33:42 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.rknn: Error in knn.reg(train = data[, fset], test = newdata[, fset], y = y,  : 
  too many ties in knn

[1] "Sun Feb 04 14:34:20 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.0301; maxdepth=27; minbucket=30; minsplit=24
[Tune-y] 1: rmse.test.rmse= 2.1; time: 0.0 min
[Tune-x] 2: cp=0.12; maxdepth=23; minbucket=22; minsplit=20
[Tune-y] 2: rmse.test.rmse=3.12; time: 0.0 min
[Tune-x] 3: cp=0.613; maxdepth=5; minbucket=28; minsplit=15
[Tune-y] 3: rmse.test.rmse=4.39; time: 0.0 min
[Tune-x] 4: cp=0.00105; maxdepth=16; minbucket=11; minsplit=28
[Tune-y] 4: rmse.test.rmse=1.08; time: 0.0 min
[Tune-x] 5: cp=0.289; maxdepth=28; minbucket=27; minsplit=23
[Tune-y] 5: rmse.test.rmse=3.23; time: 0.0 min
[Tune-x] 6: cp=0.00148; maxdepth=26; minbucket=12; minsplit=36
[Tune-y] 6: rmse.test.rmse=1.15; time: 0.2 min
[Tune-x] 7: cp=0.285; maxdepth=13; minbucket=22; minsplit=23
[Tune-y] 7: rmse.test.rmse=3.23; time: 0.1 min
[Tune-x] 8: cp=0.196; maxdepth=7; minbucket=49; minsplit=17
[Tune-y] 8: rmse.test.rmse=3.23; time: 0.2 min
[Tune-x] 9: cp=0.626; maxdepth=22; minbucket=34; minsplit=39
[Tune-y] 9: rmse.test.rmse=4.39; time: 0.2 min
[Tune-x] 10: cp=0.00118; maxdepth=21; minbucket=38; minsplit=14
[Tune-y] 10: rmse.test.rmse=1.08; time: 0.1 min
[Tune-x] 11: cp=0.002; maxdepth=4; minbucket=30; minsplit=37
[Tune-y] 11: rmse.test.rmse=1.46; time: 0.0 min
[Tune-x] 12: cp=0.0659; maxdepth=28; minbucket=41; minsplit=13
[Tune-y] 12: rmse.test.rmse=2.43; time: 0.0 min
[Tune-x] 13: cp=0.00643; maxdepth=26; minbucket=20; minsplit=10
[Tune-y] 13: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 14: cp=0.00271; maxdepth=8; minbucket=9; minsplit=49
[Tune-y] 14: rmse.test.rmse= 1.3; time: 0.1 min
[Tune-x] 15: cp=0.191; maxdepth=25; minbucket=12; minsplit=49
[Tune-y] 15: rmse.test.rmse=3.23; time: 0.0 min
[Tune-x] 16: cp=0.00133; maxdepth=5; minbucket=36; minsplit=41
[Tune-y] 16: rmse.test.rmse=1.19; time: 0.0 min
[Tune-x] 17: cp=0.00858; maxdepth=29; minbucket=6; minsplit=33
[Tune-y] 17: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 18: cp=0.00503; maxdepth=27; minbucket=46; minsplit=19
[Tune-y] 18: rmse.test.rmse= 1.3; time: 0.0 min
[Tune-x] 19: cp=0.00853; maxdepth=28; minbucket=30; minsplit=10
[Tune-y] 19: rmse.test.rmse=1.54; time: 0.0 min
[Tune-x] 20: cp=0.0258; maxdepth=5; minbucket=37; minsplit=46
[Tune-y] 20: rmse.test.rmse= 2.1; time: 0.0 min
[Tune] Result: cp=0.00105; maxdepth=16; minbucket=11; minsplit=28 : rmse.test.rmse=1.08
[1] "Sun Feb 04 14:35:36 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Sun Feb 04 14:35:41 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.rvm: Error : cannot allocate vector of size 7.0 Gb

[1] "Sun Feb 04 14:36:11 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.67600 0.22800 0.07660 0.02580 0.00868
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 1 -----> 8 
Runtime: 13.91293 mins 

 Values of predicted responses: 
   index             3 
   lambda       0.0766 
    Y 1         0.2187 
    Y 2          3.461 
    Y 3         -1.292 
    Y 4          2.564 
    Y 5           -3.3 
[1] "Sun Feb 04 14:50:11 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=193; max_depth=9; eta=0.337; gamma=4.21; colsample_bytree=0.578; min_child_weight=14.6; subsample=0.535
[Tune-y] 1: rmse.test.rmse=1.04; time: 0.3 min
[Tune-x] 2: nrounds=79; max_depth=10; eta=0.0651; gamma=5.06; colsample_bytree=0.39; min_child_weight=0.2; subsample=0.611
[Tune-y] 2: rmse.test.rmse=1.98; time: 0.1 min
[Tune-x] 3: nrounds=22; max_depth=6; eta=0.493; gamma=8.99; colsample_bytree=0.494; min_child_weight=7.96; subsample=0.295
[Tune-y] 3: rmse.test.rmse= 1.4; time: 0.0 min
[Tune-x] 4: nrounds=1.48e+03; max_depth=2; eta=0.417; gamma=8.19; colsample_bytree=0.455; min_child_weight=7.59; subsample=0.553
[Tune-y] 4: rmse.test.rmse=   1; time: 0.5 min
[Tune-x] 5: nrounds=977; max_depth=2; eta=0.584; gamma=2.72; colsample_bytree=0.673; min_child_weight=13.9; subsample=0.735
[Tune-y] 5: rmse.test.rmse=   1; time: 0.4 min
[Tune-x] 6: nrounds=897; max_depth=1; eta=0.397; gamma=7.37; colsample_bytree=0.38; min_child_weight=2.08; subsample=0.296
[Tune-y] 6: rmse.test.rmse=2.38; time: 0.2 min
[Tune-x] 7: nrounds=267; max_depth=8; eta=0.365; gamma=8.95; colsample_bytree=0.616; min_child_weight=3.52; subsample=0.454
[Tune-y] 7: rmse.test.rmse=1.03; time: 0.3 min
[Tune-x] 8: nrounds=1.62e+03; max_depth=4; eta=0.0773; gamma=1.47; colsample_bytree=0.381; min_child_weight=1.85; subsample=0.971
[Tune-y] 8: rmse.test.rmse=   1; time: 0.7 min
[Tune-x] 9: nrounds=958; max_depth=9; eta=0.0994; gamma=9.57; colsample_bytree=0.318; min_child_weight=1.83; subsample=0.758
[Tune-y] 9: rmse.test.rmse=1.01; time: 0.6 min
[Tune-x] 10: nrounds=1.09e+03; max_depth=4; eta=0.569; gamma=0.236; colsample_bytree=0.549; min_child_weight=4.73; subsample=0.918
[Tune-y] 10: rmse.test.rmse=1.03; time: 0.6 min
[Tune-x] 11: nrounds=2.17e+03; max_depth=4; eta=0.188; gamma=9.04; colsample_bytree=0.525; min_child_weight=2.55; subsample=0.604
[Tune-y] 11: rmse.test.rmse=1.01; time: 1.2 min
[Tune-x] 12: nrounds=18; max_depth=7; eta=0.542; gamma=1.57; colsample_bytree=0.683; min_child_weight=4.74; subsample=0.31
[Tune-y] 12: rmse.test.rmse=1.11; time: 0.0 min
[Tune-x] 13: nrounds=14; max_depth=8; eta=0.023; gamma=5.2; colsample_bytree=0.313; min_child_weight=19.1; subsample=0.695
[Tune-y] 13: rmse.test.rmse= 4.2; time: 0.0 min
[Tune-x] 14: nrounds=439; max_depth=2; eta=0.182; gamma=8.15; colsample_bytree=0.53; min_child_weight=3.68; subsample=0.481
[Tune-y] 14: rmse.test.rmse=   1; time: 0.2 min
[Tune-x] 15: nrounds=742; max_depth=7; eta=0.473; gamma=7.45; colsample_bytree=0.309; min_child_weight=5; subsample=0.614
[Tune-y] 15: rmse.test.rmse=1.01; time: 0.5 min
[Tune-x] 16: nrounds=46; max_depth=7; eta=0.593; gamma=7.52; colsample_bytree=0.522; min_child_weight=13.2; subsample=0.612
[Tune-y] 16: rmse.test.rmse=1.03; time: 0.0 min
[Tune-x] 17: nrounds=24; max_depth=6; eta=0.425; gamma=2.5; colsample_bytree=0.368; min_child_weight=8.11; subsample=0.739
[Tune-y] 17: rmse.test.rmse=1.61; time: 0.0 min
[Tune-x] 18: nrounds=41; max_depth=3; eta=0.577; gamma=8.19; colsample_bytree=0.546; min_child_weight=12.4; subsample=0.504
[Tune-y] 18: rmse.test.rmse=1.02; time: 0.0 min
[Tune-x] 19: nrounds=429; max_depth=6; eta=0.0548; gamma=4.37; colsample_bytree=0.53; min_child_weight=18.6; subsample=0.424
[Tune-y] 19: rmse.test.rmse=1.01; time: 0.3 min
[Tune-x] 20: nrounds=141; max_depth=10; eta=0.328; gamma=2.12; colsample_bytree=0.64; min_child_weight=3.31; subsample=0.694
[Tune-y] 20: rmse.test.rmse=1.07; time: 0.2 min
[Tune] Result: nrounds=977; max_depth=2; eta=0.584; gamma=2.72; colsample_bytree=0.673; min_child_weight=13.9; subsample=0.735 : rmse.test.rmse=   1
[1] "Sun Feb 04 14:56:35 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Sun Feb 04 14:56:41 2018"
Error : cannot allocate vector of size 405.0 Mb
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

burn in:
**GROW** @depth 0: [7,0.499252], n=(20456,10120)
**GROW** @depth 1: [4,0.497622], n=(6780,3436)
**GROW** @depth 2: [4,0], n=(3314,3446)
**GROW** @depth 3: [3,0.499656], n=(4443,2217)
**GROW** @depth 2: [2,0.502511], n=(4589,2191)
**PRUNE** @depth 3: [4,0]
**GROW** @depth 3: [5,0.49867], n=(3010,1495)
**GROW** @depth 3: [7,0.499252], n=(5908,2995)
**GROW** @depth 3: [3,0], n=(2308,2299)
**GROW** @depth 1: [7,0.499252], n=(6926,3360)
**GROW** @depth 3: [2,0.502511], n=(2213,2263)
**GROW** @depth 2: [3,0.499656], n=(4555,2295)
**GROW** @depth 4: [6,0.4999], n=(1479,734)
**GROW** @depth 4: [1,0], n=(2251,2228)
**GROW** @depth 4: [3,0.499656], n=(1971,1012)
**GROW** @depth 5: [1,0], n=(719,766)
**GROW** @depth 5: [6,0.4999], n=(989,518)
**GROW** @depth 2: [6,0.4999], n=(2237,1192)
**GROW** @depth 4: [3,0.499656], n=(792,756)
**GROW** @depth 3: [4,0], n=(1147,1161)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(1147,1161,792,756,989,518,1508,1971,1012,1501,2971,719,766,2251,2228,2237,1192,2300,4557)
**GROW** @depth 4: [6,0.4999], n=(766,381)
**PRUNE** @depth 4: [7,0.499252]
**GROW** @depth 3: [2,0], n=(741,1559)
**GROW** @depth 4: [6,0.4999], n=(1019,513)
**GROW** @depth 5: [4,0], n=(1119,1109)
**GROW** @depth 5: [5,0.49867], n=(557,235)
**GROW** @depth 3: [3,0.499656], n=(2271,2312)
**PRUNE** @depth 5: [6,0.4999]
**PRUNE** @depth 4: [4,0]
**GROW** @depth 5: [6,0.4999], n=(994,542)
**PRUNE** @depth 5: [3,0.499656]
**GROW** @depth 5: [6,0], n=(784,738)
**GROW** @depth 5: [5,0.49867], n=(1534,723)
**GROW** @depth 5: [2,0.502511], n=(386,361)
**GROW** @depth 6: [5,0.49867], n=(679,315)
**GROW** @depth 4: [6,0.4999], n=(1957,985)
**GROW** @depth 4: [5,0], n=(254,500)
**GROW** @depth 5: [4,0], n=(1142,1097)
**GROW** @depth 6: [7,0.499252], n=(714,343)
**PRUNE** @depth 5: [7,0.499252]
**GROW** @depth 5: [3,0], n=(484,522)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(1015,714,343,484,522,824,705,505,244,1515,1957,985,997,535,1534,723,1142,1097,2214,1093,1142,2275,1145,788,736,165,79,515,2271,2312)

Sampling @ nn=0 pred locs:
**GROW** @depth 3: [3,0.499656], n=(1516,759)
**GROW** @depth 5: [1,0], n=(396,428)
**GROW** @depth 3: [1,0], n=(560,585)
**GROW** @depth 3: [6,0], n=(739,1573)
**GROW** @depth 6: [1,0], n=(366,370)
**GROW** @depth 4: [5,0.49867], n=(1048,525)
**GROW** @depth 5: [7,0.499252], n=(467,238)
**GROW** @depth 5: [7,0.499252], n=(694,359)
**GROW** @depth 6: [1,0], n=(216,251)
**GROW** @depth 6: [5,0], n=(781,753)
**CPRUNE** @depth 1: var=2, val=0->0.502511, n=(6864,3422)
**PRUNE** @depth 6: [7,0]
**GROW** @depth 6: [1,0], n=(129,122)
**GROW** @depth 5: [6,0.4999], n=(1498,716)
**GROW** @depth 6: [6,0], n=(377,367)
**GROW** @depth 7: [5,0], n=(350,340)
**GROW** @depth 6: [6,0.4999], n=(244,152)
**PRUNE** @depth 6: [6,0]
**GROW** @depth 5: [6,0], n=(489,500)
**GROW** @depth 7: [6,0.4999], n=(159,57)
**GROW** @depth 4: [1,0], n=(387,353)
**PRUNE** @depth 4: [6,0.4999]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(377,367,653,335,340,484,522,244,152,428,159,83,102,361,489,500,502,773,1957,985,479,355,698,781,753,723,1142,1097,1498,716,1093,1142,387,353,1511,2258,2355,366,192,557,1171,554,200,382)
**GROW** @depth 5: [1,0], n=(221,258)
**CPRUNE** @depth 2: var=3, val=0.499656->0, n=(1115,2307)
**GROW** @depth 6: [6,0.4999], n=(346,352)
**GROW** @depth 4: [2,0], n=(1169,1186)
**PRUNE** @depth 4: [5,0]
**GROW** @depth 6: [7,0.499252], n=(148,73)
**GROW** @depth 6: [7,0.499252], n=(483,240)
**GROW** @depth 7: [1,0], n=(244,249)
**GROW** @depth 5: [6,0.4999], n=(126,66)
**GROW** @depth 8: [3,0], n=(136,113)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=10 n=(244,136,113,525,522,192,340,484,522,127,269,428,159,83,102,361,489,500,502,773,991,482,295,155,524,989,364,674,781,753,483,240,1142,1097,768,1446,1093,1142,387,750,1114,2258,1169,1186,366,126,66,557,1129,1178)
**GROW** @depth 7: [5,0.49867], n=(82,73)
**PRUNE** @depth 4: [3,0]
**GROW** @depth 6: [7,0.499252], n=(334,168)
**GROW** @depth 5: [2,0.502511], n=(267,215)
**GROW** @depth 6: [4,0], n=(179,182)
**PRUNE** @depth 6: [7,0.499252]
**GROW** @depth 4: [6,0.4999], n=(371,183)
**GROW** @depth 7: [7,0.499252], n=(171,73)
**GROW** @depth 6: [5,0], n=(237,247)
**PRUNE** @depth 7: [7,0]
**PRUNE** @depth 6: [6,0]
**GROW** @depth 5: [3,0.499656], n=(809,360)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=10 n=(244,127,139,522,1040,237,247,522,171,73,152,428,159,83,102,179,182,489,500,502,773,991,267,215,157,147,146,500,1013,364,674,781,753,483,240,1142,1097,768,1446,1093,1142,1137,1114,2258,809,360,1186,725,282,126,1153,371,183,582)
**GROW** @depth 6: [1,0], n=(273,249)
**GROW** @depth 5: [5,0.49867], n=(112,71)
**GROW** @depth 7: [7,0.499252], n=(160,87)
**PRUNE** @depth 7: [7,0.499252]
**GROW** @depth 6: [5,0.49867], n=(922,524)
**PRUNE** @depth 8: [3,0]
**GROW** @depth 5: [6,0.4999], n=(476,249)
**GROW** @depth 8: [7,0.499252], n=(174,92)
**GROW** @depth 6: [3,0.499656], n=(405,360)
**GROW** @depth 6: [2,0.502511], n=(168,198)
**GROW** @depth 5: [3,0.499656], n=(756,430)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=10 n=(244,174,92,522,1040,237,247,273,249,171,73,152,428,159,83,102,179,182,489,500,502,773,476,515,482,229,148,73,500,1013,672,168,198,781,753,248,475,1142,1097,768,922,524,1093,1142,1137,1114,2258,404,405,360,756,430,242,483,282,126,1153,359,66,129,582)
**GROW** @depth 6: [1,0], n=(247,268)
**GROW** @depth 5: [5,0], n=(244,258)
**GROW** @depth 6: [7,0.499252], n=(319,164)
**GROW** @depth 4: [6,0.4999], n=(522,251)
**GROW** @depth 4: [3,0], n=(557,596)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=10 n=(244,94,172,522,1040,273,232,252,249,171,73,152,428,159,83,102,179,182,489,500,244,258,265,508,476,247,268,482,229,151,84,486,497,533,532,489,781,753,483,240,1142,1097,1443,505,266,1093,1142,1137,1114,2258,404,405,360,756,430,242,319,164,282,126,557,596,359,66,129,582)
Grow: 23.51%, Prune: 4.651%, Change: 7.089%, Swap: 17.65%

Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.bcart: Error : cannot allocate vector of size 792.5 Mb

[1] "Sun Feb 04 15:29:24 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Sun Feb 04 15:29:29 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

burn in:
r=1000 d=[0]; n=30576

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=30576
r=2000 d=[0]; mh=1 n=30576
r=3000 d=[0]; mh=1 n=30576

Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.blm: Error : cannot allocate vector of size 792.5 Mb

[1] "Sun Feb 04 15:31:40 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Number of parameters (weights and biases) to estimate: 24 
Nguyen-Widrow method
Scaling factor= 0.7000159 
gamma= 23.5672 	 alpha= 0.076 	 beta= 73.4735 
[1] "Sun Feb 04 15:33:36 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Sun Feb 04 15:33:44 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

burn in:
r=1000 d=[0]; n=30576
r=2000 d=[0]; n=30576

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=30576
r=2000 d=[0]; mh=1 n=30576
r=3000 d=[0]; mh=1 n=30576
r=4000 d=[0]; mh=1 n=30576
r=5000 d=[0]; mh=1 n=30576
Grow: 0%, 

Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.btlm: Error : cannot allocate vector of size 792.5 Mb

[1] "Sun Feb 04 15:44:42 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Sun Feb 04 15:44:54 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Sun Feb 04 15:45:08 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Sun Feb 04 15:45:21 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Sun Feb 04 15:45:28 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Loading required package: MASS
[1] "Sun Feb 04 15:45:34 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
[Tune] Started tuning learner regr.extraTrees for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3 1 to 10   -    TRUE     -
numRandomCuts integer   -   1 1 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=3; numRandomCuts=16
Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: GC overhead limit exceeded

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: GC overhead limit exceeded

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: GC overhead limit exceeded

[Tune-y] 1: rmse.test.rmse=  NA; time: 13.0 min
[Tune-x] 2: mtry=8; numRandomCuts=12
Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: GC overhead limit exceeded

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: Java heap space

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: Java heap space

[Tune-y] 2: rmse.test.rmse=  NA; time: 28.4 min
[Tune-x] 3: mtry=7; numRandomCuts=18
Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: Java heap space

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: GC overhead limit exceeded

Warning in predict.WrappedModel(m, task, subset = test.i) :
  Could not predict with learner regr.extraTrees: Error in .jcall(et$jobject, "[D", "getValues", toJavaMatrix2D(newdata)) : 
  java.lang.OutOfMemoryError: Java heap space

[Tune-y] 3: rmse.test.rmse=  NA; time: 35.2 min
[Tune-x] 4: mtry=7; numRandomCuts=12

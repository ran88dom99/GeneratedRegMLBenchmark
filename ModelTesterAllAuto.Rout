
R version 3.4.1 (2017-06-30) -- "Single Candle"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> options(repos=structure(c(CRAN="https://rweb.crmda.ku.edu/cran/")))
> ## capture messages and errors to a file.https://rweb.crmda.ku.edu/cran/
> #zz <- file("all.Rout", open="wt")https://cran.cnr.berkeley.edu
> #sink(zz, type="message") edit for rebaseless
> #chek for R package updates
> #try(log("a")) ## test --no-edit
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> which.computer<-Sys.info()[['nodename']]
> task.subject<-"14th20hp3cv"
> out.file<-paste("out",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,".csv",sep="")
> importance.file<-paste("importance",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,sep="")
> 
> base.folder<-getwd()
> cpout.folder<-paste(base.folder,"/",which.computer,sep = "")
> setwd(cpout.folder)
> 
> if(length(which(list.files() == out.file))<1) write.table( "0.01,0.01,100,100,100,Wed Aug 02 16:37:25 2017,dummy,8,1,basic latent features,ignore,none,asis,1.12784979099243,random,333,53,adaptive_cv,16,5,2,2,19,0.0107744822639878,FALSE,,,,,,,,,," ,file =,out.file,  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,".csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,".csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,"mlr.csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,"mlr.csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> 
> cv.iters=3
> tuneLength=20
> tuneLength2=8
> normings=c("YeoJohnson","ICA", "centernscale","expoTrans","range01","asis","quantile")#,"centernscale"
> 
> gensTTesto<-c(56,53,4,12,13,14,15,20,45,54,55, 44,52,1,3)#,  51,c(4)#c(1:40)#c(5,10,11,13,14,15,16,17,18,19,20,21,24,28,38,39,40)
> gensTTest<-vector()
> write.table( t(gensTTesto),file = "initial tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
> try({
+   gensTTest<-(read.csv("tasks to test.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+ })
> if(!exists("gensTTest")) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> if(length(gensTTest)<1) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> 
> 
> ########packages install check######
> 
> #list.of.packages <- c("caret","caretEnsemble","mlr","MLmetrics","tgp")
> #list.of.packages <- c("gower","dimRed","DEoptimR","caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> #new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> #if(length(new.packages)) install.packages(new.packages, dep = TRUE)
> 
> 
> #install.packages("mlr", dependencies = c("Depends", "Suggests"))
> #install.packages("caret", dependencies = c("Depends", "Suggests"))
> #install.packages("caret",repos = "http://cran.r-project.org",dependencies = c("Depends", "Imports", "Suggests"))
> #install.packages("SuperLearner", dependencies = c("Depends", "Suggests"))
> #install.packages("rattle", dependencies = c("Depends", "Suggests"))
> 
> # Load libraries
> #library(mlbench)
> 
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> #library(caretEnsemble)
> library(MLmetrics)

Attaching package: 'MLmetrics'

The following object is masked from 'package:caret':

    RMSE

The following object is masked from 'package:base':

    Recall

> 
> ########error no repeat#########
> 
> 
> try({
+   before.last.alg<-as.matrix(read.csv("beforelast algorithm.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.alg<-as.matrix(read.csv("last algorithm tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   #write.table(paste(date(), last.alg,.Platform$OS.type,.Platform$r_arch,which.computer,sep=" "),file = "algos after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.alg==before.last.alg){print("algorithm may be broken")}
+   write.table(last.alg,file = "beforelast algorithm.csv",  quote = F, row.names = F,col.names = F)
+ })
[1] "algorithm may be broken"
> try({
+   before.last.tsk<-as.matrix(read.csv("beforelast task.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.tsk<-as.matrix(read.csv("last task tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   write.table(paste(date(),last.alg, last.tsk,cv.iters,tuneLength,.Platform$OS.type,.Platform$r_arch,which.computer,sep=","),file = "test after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.tsk==before.last.tsk){print("task may be broken")}
+   write.table(last.tsk,file = "beforelast task.csv",  quote = F, row.names = F,col.names = F)
+ })
[1] "task may be broken"
> bad.models=c("spaccceeee")
> previous.fails<-(read.csv("test after which reset.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
> previous.fails<-previous.fails[previous.fails[,8]==which.computer,]
> lgf<-length(previous.fails[,2])
> for(lt in 2:lgf)  {
+   if(previous.fails[lt,2]==previous.fails[lt-1,2])  {
+     bad.models=union(bad.models,c(paste(previous.fails[lt,2])))  }}
> 
> #######not to redo a test function#####
> check.redundant<-function(df=df.previous.calcs,norming="asis",trans.y=1,withextra="missing",missingdata="leaveempty",datasource="mean" ,column.to.predict=200,allmodel="ctree")
+ {
+   for(intern in 1:length(df[,1])){
+     if((any(df[intern,] == norming, na.rm=T))&&
+        (any(df[intern,] == withextra, na.rm=T))&&
+        (any(df[intern,] == missingdata, na.rm=T))&&
+        (any(df[intern,] == datasource, na.rm=T))&&
+        (any(df[intern,] == column.to.predict, na.rm=T))&&
+        (any(df[intern,] == allmodel, na.rm=T))&&
+        (  (df[intern,9] == trans.y)))
+     {return(TRUE)}
+   }
+   return(FALSE)
+ }
> #####caret init#####
> best.ranged <- c("avNNet", "nnet", "pcaNNet", "glm.nb")
> best.asis <- c("svmLinear3", "relaxo", "superpc", "xgbTree")
> best.cns <- c("gam", "bam", "svmLinear2", "msaenet", "BstLm", "gbm") 
> 
> cv6hp5 <- c( "BstLm", "qrnn")#earth
> cv3hp32 <- c("Rborist", "pcaNNet", "SBC")
> cv7x5hp32 <- c("gbm", "krlsPoly", "kknn", "xgbLinear","RRF", "cubist", "rlm" )
> cv6hp5.avoid <- c("pcaNNet")
> cv3hp32.avoid <- c("glm.nb", "gamboost", "ctree2","glmboost", "leapSeq","ctree","svmLinear2")
> cv7x5hp32.avoid <- c("SBC","bagearthgcv","gcvearth","lmStepAIC","glmStepAIC","bridge","lm","glm","bayesglm","blassoAveraged","treebag","rpart1SE")
> 
> allmodels <- c("avNNet", "bagEarth", "bagEarthGCV",
+                "bayesglm", "bdk", "blackboost", "Boruta", "brnn", "BstLm" ,
+                "bstTree", "cforest", "ctree", "ctree2", "cubist", "DENFIS",
+                "dnn", "earth", "elm", "enet",   "evtree",
+                "extraTrees",  "gamLoess",  "gaussprLinear", "gaussprPoly", "gaussprRadial",
+                "gcvEarth","glm", "glmboost",  "icr", "kernelpls",
+                "kknn", "knn",  "krlsRadial", "lars" , "lasso",
+                "leapBackward", "leapForward", "leapSeq", "lm", "M5", "M5Rules",
+                "mlpWeightDecay", "neuralnet" , "partDSA",
+                "pcaNNet", "pcr", "penalized", "pls", "plsRglm", "ppr",
+                "qrf" , "ranger",  "rf")
> allmodels <- c("rlm", "rpart", "rpart2",
+                "RRF", "RRFglobal",  "simpls",
+                "svmLinear", "svmPoly", "svmRadial", "svmRadialCost",
+                "widekernelpls",  "xgbLinear",
+                "xgbTree")
> allmodels <- c("avNNet","BstLm","bstTree","cforest","ctree","ctree2",
+                "cubist","earth","enet","evtree","glmboost",
+                "icr","kernelpls","kknn","lasso","pcaNNet",
+                "pcr","pls","qrf","ranger","rf")
> 
> allmodels <- c("kknn", "cubist", "avNNet", "xgbLinear", "RRF", "pcaNNet","earth","nnet","gbm","enet","lasso","BstLm",
+                "foba", "leapBackward", "gcvEarth", "SBC","glm.nb","gamboost","ctree2","relaxo", 
+                "bartMachine","extraTrees","bam","gam","randomGLM")
> #allmodels <- c("bam")
> #allmodels <- c("rf")"rqlasso",, "xyf" "rvmPoly", "rvmRadial",    "spls", "superpc" ,   "treebag",  "svmLinear2",  "SBC",
> #allmodels <- c("bartMachine", "xgbLinear", "pcaNNet","svmLinear","glmnet","cforest","cubist","rf","ranger")"glmnet",
> #wow rfRules is really slow "rfRules","WM", takes 50min
> # brak everythig "rbfDDA","ridge","rqnc",
> # use "rf" to test all
> library(caret)
> allmodels <- unique(modelLookup()[modelLookup()$forReg,c(1)])
> #allmodels <-c("avNNet", "nnet", "pcaNNet",  "glm.nb", "gam" ,
> #              "bam","msaenet", "svmLinear2","svmLinear3",
> #              "relaxo",  "superpc", "xgbTree", "BstLm")
> #allmodels<- c("svmLinear","svmPoly","svmRadial")
> #library(doParallel); cl <- makeCluster(detectCores()); registerDoParallel(cl)
> #allmodels<-c("bartMachine","extraTrees")#,"randomGLM"
> 
> 
> adaptControl <- trainControl(method = "adaptive_cv",
+                              number = 7, repeats = 5,
+                              adaptive = list(min = 4, alpha = 0.05,
+                                              method = "gls", complete = FALSE),
+                              search = "random")
> adaptControl <-trainControl(method = "cv", number = cv.iters,  search = "random")
> simpleControl <- trainControl(method = "cv",
+                               number = cv.iters,
+                               search = "random")
> 
> 
> #########MLR init######
> #R.utils::gcDLLs()
> #list.of.packages <- c("ParamHelpers","devtools","mlrMBO","RJSONIO","plot3D","plotly")
> #install.packages("mlrMBO", dependencies = c("Depends", "Suggests"))
> list.of.packages <- c("caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> if(length(new.packages)) install.packages(new.packages, dep = TRUE)
Installing packages into 'C:/Users/irina grishina/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Warning message:
packages ' RWeka', 'rPython', 'rrf' are not available (for R version 3.4.1) 
> 
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> tuneLengthMLR<-tuneLength
> mlr.iters<-cv.iters
> #######data read process start#####
> seed.var =222+round(runif(1,min=0,max=100))
> column.to.predict=1
> print(date());
[1] "Fri Feb 16 15:30:29 2018"
> 
> setwd(base.folder)
> if(!exists("gen.count")){gen.count=56}
> gens.names<-as.matrix(read.table("gens names.csv", sep = ",",header = FALSE,row.names=1,fill=TRUE, quote="",dec="."))
> count.toy.data.passed<-0
> for(gend.data in gensTTest){
+   count.toy.data.passed<-count.toy.data.passed+1
+   setwd(base.folder)
+   data.source<-as.matrix(read.csv(paste("Generats/",gens.names[gend.data],".csv", sep = ""), sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   datasource<-gens.names[gend.data,1]
+   setwd(cpout.folder)
+   missingdatas=c("ignore")
+   for(missingdata in missingdatas){
+     withextras=c("none")
+     for(withextra in withextras){
+       ################data wrestling###############
+       
+       dependant.selection=complete.cases(data.source[,column.to.predict])
+       df.previous.calcs=as.data.frame(read.csv(file=out.file, header = FALSE, sep = ",", quote = "",
+                                                dec = ".", fill = TRUE, comment.char = ""))
+       unimportant.computations<-vector(mode = "logical",length=length(df.previous.calcs[,1])  )
+       for(intern in 1:length(df.previous.calcs[,1])){
+         if((any(df.previous.calcs[intern,] == withextra, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == missingdata, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == datasource, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == column.to.predict, na.rm=T)))
+         {unimportant.computations[intern]<-T}}
+       
+       df.previous.calcs<-df.previous.calcs[unimportant.computations,]
+       
+       #data.source=data.frame( data.source[,column.to.predict],data.source[,1:2], data.source[,4:(column.to.predict-1)], data.source[,(column.to.predict+1):length( data.source[1,])])
+       
+         for(norming in normings) {
+         for(trans.y in 1:2) {
+           df.toprocess=data.source
+           y.untransformed<-df.toprocess[,1]
+           
+           if(norming=="centernscale"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("center", "scale"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="range01"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="expoTrans"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("expoTrans"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="YeoJohnson"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("YeoJohnson"))#"center", "scale",
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           if((norming=="asis")&&(trans.y==2)){next}
+           
+           
+           ################preprocess###########
+           df.toprocess=data.frame(df.toprocess[dependant.selection,])
+           y.untransformed=y.untransformed[dependant.selection]
+           if(norming=="quantile"){
+             for(Clol in trans.y:length(data.source[1,])){
+               df.toprocess[,Clol]<- (rank(df.toprocess[,Clol],na.last = "keep",ties.method = "average")-1) }
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           loess.model<-loess(y.untransformed~ df.toprocess[,1],span = 0.21, degree = 1)
+   
+           #df.toprocess = data.frame(df.toprocess,)
+           nzv <- nearZeroVar(df.toprocess[,])#, saveMetrics= TRUE
+           #nzv[nzv$nzv,][1:10,]
+           if(length(nzv)>1){
+             df.toprocess = (df.toprocess[, -nzv])}
+           df.toprocess = signif(df.toprocess,digits = 3)
+           
+           seed.var =222+round(runif(1,min=0,max=100))
+           set.seed(seed.var)
+           inTrain <- createDataPartition(y = df.toprocess[,1],
+                                          p = .75,
+                                          list = FALSE)
+           training <- df.toprocess[ inTrain,]
+           testing  <- df.toprocess[-inTrain,]
+           write.table(df.toprocess,file = "sanity check 1.csv",  quote = F, row.names = F,col.names = F)
+           
+           ###########for all models#################
+           setwd(base.folder)
+           if(max(which.computer==c("ALTA","HOPPER"))>0)
+             source("MLR part.R")
+           else
+             source("Caret part.R")
+           
+          setwd(cpout.folder)
+           if(norming == normings[length(normings)]){
+             write.table( t(gensTTest[-count.toy.data.passed]),file = "tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)}
+           
+         }
+       }
+     }
+   }
+   
+ }
Loading required package: h2o

----------------------------------------------------------------------

Your next step is to start H2O:
    > h2o.init()

For H2O package documentation, ask for help:
    > ??h2o

After starting H2O, you can use the Web UI at http://localhost:54321
For more information visit http://docs.h2o.ai

----------------------------------------------------------------------


Attaching package: 'h2o'

The following objects are masked from 'package:stats':

    cor, sd, var

The following objects are masked from 'package:base':

    %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,
    colnames<-, ifelse, is.character, is.factor, is.numeric, log,
    log10, log1p, log2, round, signif, trunc

Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
[1] "failed"                   "failed"                  
[3] "Fri Feb 16 15:35:13 2018" "OpenML 2dplanes"         
[5] "ignore"                   "none"                    
[7] "YeoJohnson"               "gbm_h2o"                 
Loading required package: earth
Loading required package: plotmo
Loading required package: plotrix
Loading required package: TeachingDemos
Multivariate Adaptive Regression Splines 

30578 samples
   10 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 20385, 20386, 20385 
Resampling results:

  RMSE      Rsquared 
  2.387414  0.7034645

Tuning parameter 'degree' was held constant at a value of 1
[1] "Fri Feb 16 15:35:22 2018"
Generalized Linear Model 

30578 samples
   10 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 20385, 20386, 20385 
Resampling results:

  RMSE      Rsquared 
  2.387193  0.7035205

[1] "Fri Feb 16 15:35:24 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

2: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

3: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

4: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

5: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

6: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

7: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

8: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

9: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

2: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

3: In eval(xpr, envir = envir) :
  model fit failed for Fold1: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

4: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

5: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

6: In eval(xpr, envir = envir) :
  model fit failed for Fold2: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

7: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=log Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

8: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=sqrt Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

9: In eval(xpr, envir = envir) :
  model fit failed for Fold3: link=identity Error in glm.nb(formula = .outcome ~ ., data = structure(list(V2 = c(1,  : 
  could not find function "glm.nb"

10: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :3     NA's   :3    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
[1] "failed"                   "failed"                  
[3] "Fri Feb 16 15:37:06 2018" "OpenML 2dplanes"         
[5] "ignore"                   "none"                    
[7] "YeoJohnson"               "glm.nb"                  
Loading required package: plyr
Loading required package: mboost
Loading required package: parallel
Loading required package: stabs
This is mboost 2.8-1. See 'package?mboost' and 'news(package  = "mboost")'
for a complete list of changes.


Attaching package: 'mboost'

The following object is masked from 'package:MLmetrics':

    AUC

The following object is masked from 'package:ggplot2':

    %+%

Timing stopped at: 1.72 1.58 18.83
Error : cannot allocate vector of size 7.0 Gb
Error in confusionMatrix.default(p[, 1], p[, 2]) : 
  The data must contain some levels that overlap the reference.
Boosted Generalized Linear Model 

30578 samples
   10 predictor

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 30578, 30578, 30578, 30578, 30578, 30578, ... 
Resampling results across tuning parameters:

  mstop  RMSE      Rsquared   Selected
   50    2.649958  0.6641619          
  100    2.447710  0.6964485          
  150    2.400329  0.7017723  *       

Tuning parameter 'prune' was held constant at a value of no
RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mstop = 150 and prune = no.
[1] "Fri Feb 16 17:44:23 2018"
Loading required package: glmnet
Loading required package: Matrix
Loading required package: foreach
Loaded glmnet 2.0-10

glmnet 

30578 samples
   10 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 20385, 20386, 20385 
Resampling results across tuning parameters:

  alpha       lambda       RMSE      Rsquared   Selected
  0.01645968  3.040599640  2.850244  0.7031595          
  0.04677441  0.002838353  2.387293  0.7035268          
  0.17055393  0.037773798  2.387448  0.7035381          
  0.18754930  0.466766137  2.435534  0.7025009          
  0.24733937  0.009948263  2.387226  0.7035377          
  0.34182773  0.016624574  2.387214  0.7035388          
  0.34933733  0.001919369  2.387212  0.7035388          
  0.37195975  0.014375812  2.387204  0.7035388          
  0.42853949  0.001252185  2.387206  0.7035386          
  0.47260151  0.015507693  2.387225  0.7035380          
  0.54704845  0.002312852  2.387200  0.7035382          
  0.58282449  0.164138625  2.405551  0.7022945          
  0.60336554  2.108758627  3.582552  0.4571805          
  0.61891148  0.001252640  2.387191  0.7035383  *       
  0.72168405  0.001401914  2.387195  0.7035378          
  0.73648557  1.102434418  3.113921  0.5753177          
  0.76381006  0.006100012  2.387191  0.7035378          
  0.83909508  0.358631401  2.526031  0.6876282          
  0.94036432  0.014882656  2.387378  0.7035230          
  0.99476209  2.984870174  4.383926  0.4530712          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were alpha = 0.6189115 and lambda
 = 0.00125264.
[1] "Fri Feb 16 17:44:36 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
[1] "failed"                   "failed"                  
[3] "Fri Feb 16 17:49:26 2018" "OpenML 2dplanes"         
[5] "ignore"                   "none"                    
[7] "YeoJohnson"               "glmnet_h2o"              
Loading required package: MASS
Start:  AIC=93226.86
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Deviance    AIC
- V10   1   115476  93225
- V9    1   115478  93225
- V11   1   115480  93226
<none>      115475  93227
- V5    1   119204  93873
- V8    1   119389  93904
- V7    1   129801  95609
- V4    1   130445  95710
- V6    1   144564  97805
- V3    1   148450  98346
- V2    1   293809 112262

Step:  AIC=93225.02
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V11

       Df Deviance    AIC
- V9    1   115479  93224
- V11   1   115481  93224
<none>      115476  93225
- V5    1   119205  93871
- V8    1   119389  93902
- V7    1   129801  95607
- V4    1   130449  95708
- V6    1   144574  97804
- V3    1   148453  98344
- V2    1   293829 112261

Step:  AIC=93223.65
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V11

       Df Deviance    AIC
- V11   1   115485  93223
<none>      115479  93224
- V5    1   119209  93870
- V8    1   119392  93901
- V7    1   129805  95605
- V4    1   130450  95707
- V6    1   144581  97803
- V3    1   148461  98343
- V2    1   293870 112262

Step:  AIC=93222.6
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8

       Df Deviance    AIC
<none>      115485  93223
- V5    1   119214  93869
- V8    1   119396  93900
- V7    1   129812  95605
- V4    1   130455  95705
- V6    1   144592  97803
- V3    1   148467  98342
- V2    1   293873 112260
Start:  AIC=93438.77
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Deviance    AIC
- V9    1   116661  93437
- V10   1   116672  93439
<none>      116661  93439
- V11   1   116690  93442
- V5    1   120178  94042
- V8    1   120291  94062
- V7    1   130958  95794
- V4    1   131062  95810
- V6    1   147424  98208
- V3    1   147790  98258
- V2    1   294810 112336

Step:  AIC=93436.82
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V10 + V11

       Df Deviance    AIC
- V10   1   116672  93437
<none>      116661  93437
- V11   1   116690  93440
- V5    1   120179  94040
- V8    1   120292  94060
- V7    1   130959  95792
- V4    1   131064  95808
- V6    1   147424  98206
- V3    1   147790  98257
- V2    1   294822 112335

Step:  AIC=93436.76
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V11

       Df Deviance    AIC
<none>      116672  93437
- V11   1   116701  93440
- V5    1   120189  94040
- V8    1   120301  94059
- V7    1   130966  95791
- V4    1   131091  95810
- V6    1   147447  98207
- V3    1   147801  98256
- V2    1   294850 112335
Start:  AIC=93313.9
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Deviance    AIC
- V10   1   115970  93312
- V9    1   115972  93313
<none>      115969  93314
- V11   1   116017  93320
- V8    1   119425  93910
- V5    1   119681  93954
- V7    1   130079  95652
- V4    1   130189  95670
- V6    1   145564  97945
- V3    1   148220  98314
- V2    1   293786 112260

Step:  AIC=93312.03
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V11

       Df Deviance    AIC
- V9    1   115973  93311
<none>      115970  93312
- V11   1   116018  93319
- V8    1   119425  93909
- V5    1   119682  93952
- V7    1   130079  95651
- V4    1   130190  95668
- V6    1   145564  97943
- V3    1   148220  98312
- V2    1   293796 112259

Step:  AIC=93310.66
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V11

       Df Deviance    AIC
<none>      115973  93311
- V11   1   116022  93317
- V8    1   119427  93907
- V5    1   119686  93951
- V7    1   130081  95649
- V4    1   130193  95666
- V6    1   145573  97943
- V3    1   148222  98310
- V2    1   293815 112258
Start:  AIC=139984
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Deviance    AIC
- V9    1   174088 139982
- V10   1   174088 139982
<none>      174086 139984
- V11   1   174123 139988
- V5    1   179565 140930
- V8    1   179584 140933
- V7    1   195455 143522
- V4    1   195886 143590
- V6    1   218815 146974
- V3    1   222256 147452
- V2    1   441274 168423

Step:  AIC=139982.4
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V10 + V11

       Df Deviance    AIC
- V10   1   174090 139981
<none>      174088 139982
- V11   1   174124 139987
- V5    1   179568 140928
- V8    1   179585 140931
- V7    1   195456 143520
- V4    1   195886 143588
- V6    1   218821 146973
- V3    1   222259 147450
- V2    1   441309 168423

Step:  AIC=139980.7
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V11

       Df Deviance    AIC
<none>      174090 139981
- V11   1   174126 139985
- V5    1   179569 140926
- V8    1   179586 140929
- V7    1   195457 143519
- V4    1   195898 143587
- V6    1   218831 146973
- V3    1   222262 147448
- V2    1   441336 168423
Generalized Linear Model with Stepwise Feature Selection 

30578 samples
   10 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 20385, 20386, 20385 
Resampling results:

  RMSE      Rsquared 
  2.387138  0.7035332

[1] "Fri Feb 16 17:49:39 2018"
Loading required package: fastICA
Independent Component Regression 

30578 samples
   10 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 20385, 20386, 20385 
Resampling results across tuning parameters:

  n.comp  RMSE      Rsquared   Selected
   1      4.128832  0.1120807          
   2      3.970973  0.1767888          
   3      3.771560  0.2570223          
   4      3.718619  0.2767949          
   5      3.554283  0.3402047          
   6      3.467444  0.3699320          
   7      3.114942  0.4937333          
   8      3.062817  0.5106258          
   9      2.932242  0.5523307          
  10      2.387193  0.7035205  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was n.comp = 10.
[1] "Fri Feb 16 17:49:54 2018"
Loading required package: pls

Attaching package: 'pls'

The following object is masked from 'package:caret':

    R2

The following object is masked from 'package:stats':

    loadings

Partial Least Squares 

30578 samples
   10 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 20385, 20386, 20385 
Resampling results across tuning parameters:

  ncomp  RMSE      Rsquared   Selected
   1     2.478782  0.6803637          
   2     2.387193  0.7035208          
   3     2.387193  0.7035204          
   4     2.387193  0.7035205          
   5     2.387193  0.7035205          
   6     2.387193  0.7035205          
   7     2.387193  0.7035205  *       
   8     2.387193  0.7035205          
   9     2.387193  0.7035205          
  10     2.387193  0.7035205          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 7.
[1] "Fri Feb 16 17:49:57 2018"
Loading required package: kknn

Attaching package: 'kknn'

The following object is masked from 'package:caret':

    contr.dummy


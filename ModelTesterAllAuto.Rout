
R version 3.4.1 (2017-06-30) -- "Single Candle"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> options(repos=structure(c(CRAN="https://rweb.crmda.ku.edu/cran/")))
> ## capture messages and errors to a file.https://rweb.crmda.ku.edu/cran/
> #zz <- file("all.Rout", open="wt")https://cran.cnr.berkeley.edu
> #sink(zz, type="message") edit for rebaseless
> #chek for R package updates
> #try(log("a")) ## test --no-edit
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> task.subject<-"14th20hp3cv"
> pc.mlr<-c("ACE")#"ALTA","HOPPER"
> which.computer<-Sys.info()[['nodename']]
> out.file<-paste("out",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,".csv",sep="")
> importance.file<-paste("importance",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,sep="")
> 
> base.folder<-getwd()
> cpout.folder<-paste(base.folder,"/",which.computer,sep = "")
> setwd(cpout.folder)
> 
> if(length(which(list.files() == out.file))<1) write.table( "0.01,0.01,100,100,100,Wed Aug 02 16:37:25 2017,dummy,8,1,basic latent features,ignore,none,asis,1.12784979099243,random,333,53,adaptive_cv,16,5,2,2,19,0.0107744822639878,FALSE,,,,,,,,,," ,file =,out.file,  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,".csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,".csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,"mlr.csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,"mlr.csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> 
> cv.iters=3
> tuneLength=20
> tuneLength2=8
> normings=c("YeoJohnson","ICA", "centernscale","expoTrans","range01","asis","quantile")#,"centernscale"
> 
> gensTTesto<-c(56,53,4,12,13,14,15,20,45,54,55, 44,3,1,52)#,  51,c(4)#c(1:40)#c(5,10,11,13,14,15,16,17,18,19,20,21,24,28,38,39,40)
> gensTTest<-vector()
> write.table( t(gensTTesto),file = "initial tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
> try({
+   gensTTest<-t(read.csv("tasks to test.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   gensTTest<-as.vector(gensTTest)
+ })
> if(!exists("gensTTest")) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> gensTTesto<-c(gensTTesto[length(gensTTesto):1])
> if(length(gensTTest)<1) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> 
> 
> ########packages install check######
> 
> #list.of.packages <- c("caret","caretEnsemble","mlr","MLmetrics","tgp")
> #list.of.packages <- c("gower","dimRed","DEoptimR","caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> #new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> #if(length(new.packages)) install.packages(new.packages, dep = TRUE)
> 
> 
> #install.packages("mlr", dependencies = c("Depends", "Suggests"))
> #install.packages("caret", dependencies = c("Depends", "Suggests"))
> #install.packages("caret",repos = "http://cran.r-project.org",dependencies = c("Depends", "Imports", "Suggests"))
> #install.packages("SuperLearner", dependencies = c("Depends", "Suggests"))
> #install.packages("rattle", dependencies = c("Depends", "Suggests"))
> 
> # Load libraries
> #library(mlbench)
> 
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> #library(caretEnsemble)
> library(MLmetrics)

Attaching package: 'MLmetrics'

The following object is masked from 'package:caret':

    RMSE

The following object is masked from 'package:base':

    Recall

> 
> ########error no repeat#########
> 
> 
> try({
+   before.last.alg<-as.matrix(read.csv("beforelast algorithm.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.alg<-as.matrix(read.csv("last algorithm tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   #write.table(paste(date(), last.alg,.Platform$OS.type,.Platform$r_arch,which.computer,sep=" "),file = "algos after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.alg==before.last.alg){print("algorithm may be broken")}
+   write.table(last.alg,file = "beforelast algorithm.csv",  quote = F, row.names = F,col.names = F)
+ })
> try({
+   before.last.tsk<-as.matrix(read.csv("beforelast task.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.tsk<-as.matrix(read.csv("last task tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   write.table(paste(date(),last.alg, last.tsk,cv.iters,tuneLength,.Platform$OS.type,.Platform$r_arch,which.computer,sep=","),file = "test after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.tsk==before.last.tsk){print("task may be broken")}
+   write.table(last.tsk,file = "beforelast task.csv",  quote = F, row.names = F,col.names = F)
+ })
[1] "task may be broken"
> bad.models=c("spaccceeee")
> previous.fails<-(read.csv("test after which reset.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
> previous.fails<-previous.fails[previous.fails[,8]==which.computer,]
> lgf<-length(previous.fails[,2])
> for(lt in 2:lgf)  {
+   if(previous.fails[lt,2]==previous.fails[lt-1,2])  {
+     bad.models=union(bad.models,c(paste(previous.fails[lt,2])))  }}
> 
> #######not to redo a test function#####
> check.redundant<-function(df=df.previous.calcs,norming="asis",trans.y=1,withextra="missing",missingdata="leaveempty",datasource="mean" ,column.to.predict=200,allmodel="ctree")
+ {
+   for(intern in 1:length(df[,1])){
+     if((any(df[intern,] == norming, na.rm=T))&&
+        (any(df[intern,] == withextra, na.rm=T))&&
+        (any(df[intern,] == missingdata, na.rm=T))&&
+        (any(df[intern,] == datasource, na.rm=T))&&
+        (any(df[intern,] == column.to.predict, na.rm=T))&&
+        (any(df[intern,] == allmodel, na.rm=T))&&
+        (  (df[intern,9] == trans.y)))
+     {return(TRUE)}
+   }
+   return(FALSE)
+ }
> #####caret init#####
> best.ranged <- c("avNNet", "nnet", "pcaNNet", "glm.nb")
> best.asis <- c("svmLinear3", "relaxo", "superpc", "xgbTree")
> best.cns <- c("gam", "bam", "svmLinear2", "msaenet", "BstLm", "gbm") 
> 
> cv6hp5 <- c( "BstLm", "qrnn")#earth
> cv3hp32 <- c("Rborist", "pcaNNet", "SBC")
> cv7x5hp32 <- c("gbm", "krlsPoly", "kknn", "xgbLinear","RRF", "cubist", "rlm" )
> cv6hp5.avoid <- c("pcaNNet")
> cv3hp32.avoid <- c("glm.nb", "gamboost", "ctree2","glmboost", "leapSeq","ctree","svmLinear2")
> cv7x5hp32.avoid <- c("SBC","bagearthgcv","gcvearth","lmStepAIC","glmStepAIC","bridge","lm","glm","bayesglm","blassoAveraged","treebag","rpart1SE")
> 
> allmodels <- c("avNNet", "bagEarth", "bagEarthGCV",
+                "bayesglm", "bdk", "blackboost", "Boruta", "brnn", "BstLm" ,
+                "bstTree", "cforest", "ctree", "ctree2", "cubist", "DENFIS",
+                "dnn", "earth", "elm", "enet",   "evtree",
+                "extraTrees",  "gamLoess",  "gaussprLinear", "gaussprPoly", "gaussprRadial",
+                "gcvEarth","glm", "glmboost",  "icr", "kernelpls",
+                "kknn", "knn",  "krlsRadial", "lars" , "lasso",
+                "leapBackward", "leapForward", "leapSeq", "lm", "M5", "M5Rules",
+                "mlpWeightDecay", "neuralnet" , "partDSA",
+                "pcaNNet", "pcr", "penalized", "pls", "plsRglm", "ppr",
+                "qrf" , "ranger",  "rf")
> allmodels <- c("rlm", "rpart", "rpart2",
+                "RRF", "RRFglobal",  "simpls",
+                "svmLinear", "svmPoly", "svmRadial", "svmRadialCost",
+                "widekernelpls",  "xgbLinear",
+                "xgbTree")
> allmodels <- c("avNNet","BstLm","bstTree","cforest","ctree","ctree2",
+                "cubist","earth","enet","evtree","glmboost",
+                "icr","kernelpls","kknn","lasso","pcaNNet",
+                "pcr","pls","qrf","ranger","rf")
> 
> allmodels <- c("kknn", "cubist", "avNNet", "xgbLinear", "RRF", "pcaNNet","earth","nnet","gbm","enet","lasso","BstLm",
+                "foba", "leapBackward", "gcvEarth", "SBC","glm.nb","gamboost","ctree2","relaxo", 
+                "bartMachine","extraTrees","bam","gam","randomGLM")
> #allmodels <- c("bam")
> #allmodels <- c("rf")"rqlasso",, "xyf" "rvmPoly", "rvmRadial",    "spls", "superpc" ,   "treebag",  "svmLinear2",  "SBC",
> #allmodels <- c("bartMachine", "xgbLinear", "pcaNNet","svmLinear","glmnet","cforest","cubist","rf","ranger")"glmnet",
> #wow rfRules is really slow "rfRules","WM", takes 50min
> # brak everythig "rbfDDA","ridge","rqnc",
> # use "rf" to test all
> library(caret)
> allmodels <- unique(modelLookup()[modelLookup()$forReg,c(1)])
> #allmodels <-c("avNNet", "nnet", "pcaNNet",  "glm.nb", "gam" ,
> #              "bam","msaenet", "svmLinear2","svmLinear3",
> #              "relaxo",  "superpc", "xgbTree", "BstLm")
> #allmodels<- c("svmLinear","svmPoly","svmRadial")
> #library(doParallel); cl <- makeCluster(detectCores()); registerDoParallel(cl)
> #allmodels<-c("bartMachine","extraTrees")#,"randomGLM"
> 
> 
> adaptControl <- trainControl(method = "adaptive_cv",
+                              number = 7, repeats = 5,
+                              adaptive = list(min = 4, alpha = 0.05,
+                                              method = "gls", complete = FALSE),
+                              search = "random")
> adaptControl <-trainControl(method = "cv", number = cv.iters,  search = "random")
> simpleControl <- trainControl(method = "cv",
+                               number = cv.iters,
+                               search = "random")
> 
> 
> #########MLR init######
> #R.utils::gcDLLs()
> #list.of.packages <- c("ParamHelpers","devtools","mlrMBO","RJSONIO","plot3D","plotly")
> #install.packages("mlrMBO", dependencies = c("Depends", "Suggests"))
> list.of.packages <- c("caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> if(length(new.packages)) install.packages(new.packages, dep = TRUE)
Installing packages into 'C:/Users/irina grishina/Documents/R/win-library/3.4'
(as 'lib' is unspecified)
Warning message:
packages ' RWeka', 'rPython', 'rrf' are not available (for R version 3.4.1) 
> 
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> tuneLengthMLR<-tuneLength
> mlr.iters<-cv.iters
> #######data read process start#####
> seed.var =222+round(runif(1,min=0,max=100))
> column.to.predict=1
> print(date());
[1] "Thu Mar 08 15:31:11 2018"
> 
> setwd(base.folder)
> if(!exists("gen.count")){gen.count=56}
> gens.names<-as.matrix(read.table("gens names.csv", sep = ",",header = FALSE,row.names=1,fill=TRUE, quote="",dec="."))
> count.toy.data.passed<-1
> for(gend.data in gensTTest){
+   count.toy.data.passed<-count.toy.data.passed+1
+   setwd(base.folder)
+   data.source<-as.matrix(read.csv(paste("Generats/",gens.names[gend.data],".csv", sep = ""), sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   datasource<-gens.names[gend.data,1]
+   setwd(cpout.folder)
+   missingdatas=c("ignore")
+   for(missingdata in missingdatas){
+     withextras=c("none")
+     for(withextra in withextras){
+       ################data wrestling###############
+       
+       dependant.selection=complete.cases(data.source[,column.to.predict])
+       df.previous.calcs=as.data.frame(read.csv(file=out.file, header = FALSE, sep = ",", quote = "",
+                                                dec = ".", fill = TRUE, comment.char = ""))
+       unimportant.computations<-vector(mode = "logical",length=length(df.previous.calcs[,1])  )
+       for(intern in 1:length(df.previous.calcs[,1])){
+         if((any(df.previous.calcs[intern,] == withextra, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == missingdata, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == datasource, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == column.to.predict, na.rm=T)))
+         {unimportant.computations[intern]<-T}}
+       
+       df.previous.calcs<-df.previous.calcs[unimportant.computations,]
+       
+       #data.source=data.frame( data.source[,column.to.predict],data.source[,1:2], data.source[,4:(column.to.predict-1)], data.source[,(column.to.predict+1):length( data.source[1,])])
+       
+         for(norming in normings) {
+         for(trans.y in 1:2) {
+           df.toprocess=data.source
+           y.untransformed<-df.toprocess[,1]
+           
+           if(norming=="centernscale"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("center", "scale"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="range01"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="expoTrans"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("expoTrans"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="YeoJohnson"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("YeoJohnson"))#"center", "scale",
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           if((norming=="asis")&&(trans.y==2)){next}
+           
+           
+           ################preprocess###########
+           df.toprocess=data.frame(df.toprocess[dependant.selection,])
+           y.untransformed=y.untransformed[dependant.selection]
+           if(norming=="quantile"){
+             for(Clol in trans.y:length(data.source[1,])){
+               df.toprocess[,Clol]<- (rank(df.toprocess[,Clol],na.last = "keep",ties.method = "average")-1) }
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           loess.model<-loess(y.untransformed~ df.toprocess[,1],span = 0.21, degree = 1)
+   
+           #df.toprocess = data.frame(df.toprocess,)
+           nzv <- nearZeroVar(df.toprocess[,])#, saveMetrics= TRUE
+           #nzv[nzv$nzv,][1:10,]
+           if(length(nzv)>1){
+             df.toprocess = (df.toprocess[, -nzv])}
+           df.toprocess = signif(df.toprocess,digits = 3)
+           
+           seed.var =222+round(runif(1,min=0,max=100))
+           set.seed(seed.var)
+           inTrain <- createDataPartition(y = df.toprocess[,1],
+                                          p = .75,
+                                          list = FALSE)
+           training <- df.toprocess[ inTrain,]
+           testing  <- df.toprocess[-inTrain,]
+           write.table(df.toprocess,file = "sanity check 1.csv",  quote = F, row.names = F,col.names = F)
+           
+           ###########for all models#################
+           setwd(base.folder)
+           if(max(which.computer==pc.mlr)>0)
+             source("MLR part.R")
+           else
+             source("Caret part.R")
+           
+          setwd(cpout.folder)
+           if(norming == normings[length(normings)]){
+             if(count.toy.data.passed>length(gensTTest)){gensTTest<-c(gensTTesto)}
+             write.table( t(gensTTest[count.toy.data.passed:length(gensTTest)]),file = "tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
+             
+             }
+           
+         }
+       }
+     }
+   }
+   
+ }
Loading required package: xgboost
Loading required package: plyr
eXtreme Gradient Boosting 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 502, 501 
Resampling results across tuning parameters:

  eta        max_depth  gamma      colsample_bytree  min_child_weight
  0.1137893   1         1.8129173  0.6174136         15              
  0.1224664   9         2.7782532  0.5325234          1              
  0.1397118   9         4.1801708  0.6988656         13              
  0.2010365   7         6.9191772  0.6441186          8              
  0.2357213  10         8.5350574  0.6501885          6              
  0.2888254   8         6.7952903  0.5940949         18              
  0.3192601   1         1.2585187  0.5324968         11              
  0.3567132   2         3.3121120  0.4284870          6              
  0.3574545   7         5.8308348  0.5637329         10              
  0.3769873   5         7.3690401  0.4489470          5              
  0.4093020   6         3.9811810  0.4097948         20              
  0.4169098   1         7.4021134  0.5856684          5              
  0.4256185   5         5.0962360  0.3228905          2              
  0.4412333  10         4.1736524  0.5836279         13              
  0.4777539   4         2.0139590  0.4099067         15              
  0.4828612   4         6.2128434  0.3209048          6              
  0.4842672  10         2.2384458  0.5329332         19              
  0.5193142   3         2.4337632  0.4883380         11              
  0.5544496   7         6.9371447  0.6489635         12              
  0.5841435   9         0.8031147  0.3307850         19              
  subsample  nrounds  RMSE       Rsquared   Selected
  0.7774791   25      0.6712143  0.9162827          
  0.9489639  325      0.2603393  0.9813798          
  0.8880027  806      0.2587242  0.9832124          
  0.6104074  287      0.3113326  0.9755437          
  0.6487352  768      0.3304872  0.9691467          
  0.4485083  394      0.3435407  0.9599188          
  0.8660803  538      0.2312846  0.9765037  *       
  0.8292799  418            NaN        NaN          
  0.7823058  100      0.2867342  0.9756634          
  0.7152047  657            NaN        NaN          
  0.8495388  619            NaN        NaN          
  0.9686070  928      0.3352962  0.9540688          
  0.9139400  313            NaN        NaN          
  0.6253759  458      0.2836211  0.9667584          
  0.6994838  686            NaN        NaN          
  0.7567600  843            NaN        NaN          
  0.9523637  758      0.2494852  0.9731873          
  0.7448873  482            NaN        NaN          
  0.5324365  509      0.3263769  0.9532620          
  0.7172899  485            NaN        NaN          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nrounds = 538, max_depth = 1, eta
 = 0.3192601, gamma = 1.258519, colsample_bytree = 0.5324968,
 min_child_weight = 11 and subsample = 0.8660803.
[1] "Thu Mar 08 15:34:56 2018"
Loading required package: kohonen
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :24    NA's   :24   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Thu Mar 08 15:35:10 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "YeoJohnson"               "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "xyf"                     
Loading required package: nnet
Fitting Repeat 1 

# weights:  25
initial  value 1281.284624 
iter  10 value 1160.512835
iter  20 value 762.437230
iter  30 value 743.639752
iter  40 value 741.826383
iter  50 value 741.452647
iter  60 value 741.251505
iter  70 value 741.054610
iter  80 value 741.040598
final  value 741.040497 
converged
Fitting Repeat 2 

# weights:  25
initial  value 1051.512926 
iter  10 value 1025.137283
iter  20 value 651.065563
iter  30 value 645.498325
iter  40 value 643.903241
iter  50 value 643.615320
iter  60 value 642.959576
iter  70 value 642.406518
iter  80 value 642.207033
iter  90 value 642.202616
iter 100 value 642.201681
final  value 642.201681 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  25
initial  value 1393.118228 
iter  10 value 1114.076391
iter  20 value 730.838917
iter  30 value 720.093364
iter  40 value 718.628509
iter  50 value 718.522632
iter  60 value 718.504461
iter  70 value 718.496562
iter  80 value 718.485002
iter  90 value 718.484530
iter  90 value 718.484525
iter  90 value 718.484525
final  value 718.484525 
converged
Fitting Repeat 4 

# weights:  25
initial  value 1344.673338 
iter  10 value 1134.402950
iter  20 value 762.951022
iter  30 value 727.421876
iter  40 value 725.290192
iter  50 value 724.976996
iter  60 value 724.844629
iter  70 value 724.818356
iter  80 value 724.788243
iter  90 value 724.786474
final  value 724.786450 
converged
Fitting Repeat 5 

# weights:  25
initial  value 1254.267942 
iter  10 value 674.337300
iter  20 value 672.927539
iter  30 value 672.628093
iter  40 value 672.375713
iter  50 value 672.311867
final  value 672.310418 
converged
Fitting Repeat 1 

# weights:  49
initial  value 1078.177523 
iter  10 value 1052.169103
iter  20 value 1052.114039
iter  30 value 1051.977408
iter  40 value 666.412131
iter  50 value 665.852303
iter  60 value 665.704804
iter  70 value 665.688935
iter  80 value 665.686322
iter  90 value 665.683639
iter 100 value 665.679879
final  value 665.679879 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  49
initial  value 1115.161994 
iter  10 value 1052.166167
final  value 1052.127322 
converged
Fitting Repeat 3 

# weights:  49
initial  value 1211.060441 
iter  10 value 1052.166116
final  value 1052.126889 
converged
Fitting Repeat 4 

# weights:  49
initial  value 1141.551951 
iter  10 value 1052.180026
final  value 1052.126611 
converged
Fitting Repeat 5 

# weights:  49
initial  value 1333.313985 
iter  10 value 1052.185372
iter  20 value 1052.126615
iter  20 value 1052.126611
iter  20 value 1052.126611
final  value 1052.126611 
converged
Fitting Repeat 1 

# weights:  25
initial  value 1070.578047 
iter  10 value 1052.113709
iter  20 value 669.887162
iter  30 value 666.830915
iter  40 value 666.258268
iter  50 value 666.151383
iter  60 value 666.023179
iter  70 value 665.970440
iter  80 value 665.959291
iter  90 value 665.941555
iter 100 value 665.884114
final  value 665.884114 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  25
initial  value 1172.061759 
iter  10 value 1052.202495
final  value 1052.130604 
converged
Fitting Repeat 3 

# weights:  25
initial  value 1104.940831 
iter  10 value 1052.173291
final  value 1052.129841 
converged
Fitting Repeat 4 

# weights:  25
initial  value 1095.869993 
iter  10 value 1052.184315
iter  20 value 1052.127475
iter  30 value 1052.127302
iter  40 value 1052.127094
iter  50 value 1052.126835
iter  60 value 1052.126506
iter  70 value 1052.126068
iter  80 value 1052.125451
iter  90 value 1052.124509
iter 100 value 1052.122874
final  value 1052.122874 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  25
initial  value 1095.992231 
iter  10 value 1052.144474
final  value 1052.132474 
converged
Fitting Repeat 1 

# weights:  41
initial  value 1121.659270 
iter  10 value 1052.210332
iter  20 value 1052.127220
iter  30 value 1052.126245
iter  40 value 1052.126047
iter  50 value 1052.125797
iter  60 value 1052.125469
iter  70 value 1052.125011
iter  80 value 1052.124319
iter  90 value 1052.123143
iter 100 value 1052.120707
final  value 1052.120707 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  41
initial  value 1231.505870 
iter  10 value 1052.221007
final  value 1052.127733 
converged
Fitting Repeat 3 

# weights:  41
initial  value 1050.029488 
iter  10 value 731.936829
iter  20 value 673.741408
iter  30 value 670.860848
iter  40 value 668.165604
iter  50 value 667.571373
iter  60 value 666.813801
iter  70 value 666.036572
iter  80 value 665.830185
iter  90 value 665.801303
iter 100 value 665.797988
final  value 665.797988 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  41
initial  value 1184.468634 
iter  10 value 1052.260138
iter  20 value 1052.127843
final  value 1052.127822 
converged
Fitting Repeat 5 

# weights:  41
initial  value 1177.934648 
iter  10 value 1052.249526
iter  20 value 1052.128232
iter  20 value 1052.128230
iter  20 value 1052.128228
final  value 1052.128228 
converged
Fitting Repeat 1 

# weights:  29
initial  value 1086.316472 
iter  10 value 1052.341404
iter  20 value 1052.050466
iter  30 value 666.728204
iter  40 value 666.139062
iter  50 value 665.908912
iter  60 value 665.880320
iter  70 value 665.873910
iter  80 value 665.867877
iter  90 value 665.865924
iter 100 value 665.862498
final  value 665.862498 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  29
initial  value 1113.583046 
iter  10 value 1052.404499
iter  20 value 1052.054615
iter  30 value 668.680794
iter  40 value 667.120356
iter  50 value 666.310923
iter  60 value 665.921514
iter  70 value 665.904406
iter  80 value 665.890408
iter  90 value 665.879938
iter 100 value 665.872137
final  value 665.872137 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  29
initial  value 1203.141613 
iter  10 value 1052.865228
iter  20 value 1052.158431
iter  30 value 1052.145608
iter  40 value 1052.142721
iter  50 value 1052.136240
iter  60 value 1052.102732
iter  70 value 670.790871
iter  80 value 666.697164
iter  90 value 666.538844
iter 100 value 666.141882
final  value 666.141882 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  29
initial  value 1235.470488 
iter  10 value 1053.035058
iter  20 value 1052.149843
iter  30 value 1052.124532
iter  40 value 1049.256989
iter  50 value 667.179825
iter  60 value 666.097628
iter  70 value 666.026990
iter  80 value 665.978296
iter  90 value 665.916052
iter 100 value 665.904930
final  value 665.904930 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  29
initial  value 1298.999505 
iter  10 value 1053.154072
iter  20 value 1052.161321
iter  30 value 683.165742
iter  40 value 681.978878
iter  50 value 672.725608
iter  60 value 671.409921
iter  70 value 668.595772
iter  80 value 667.923337
iter  90 value 666.962407
iter 100 value 666.574494
final  value 666.574494 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  33
initial  value 1199.802690 
iter  10 value 1055.447283
iter  20 value 1052.175493
iter  30 value 682.524312
iter  40 value 679.284353
iter  50 value 674.816116
iter  60 value 670.952507
iter  70 value 668.809292
iter  80 value 667.659746
iter  90 value 666.818324
iter 100 value 666.363731
final  value 666.363731 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  33
initial  value 1142.927488 
iter  10 value 1053.055341
iter  20 value 1052.151869
iter  30 value 668.700548
iter  40 value 666.466673
iter  50 value 666.222168
iter  60 value 666.134340
iter  70 value 666.085907
iter  80 value 666.074690
iter  90 value 666.071303
iter 100 value 666.067793
final  value 666.067793 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  33
initial  value 1182.868121 
iter  10 value 1054.039687
iter  20 value 1052.209180
iter  30 value 679.251316
iter  40 value 670.602462
iter  50 value 668.473169
iter  60 value 667.024159
iter  70 value 666.649837
iter  80 value 666.367068
iter  90 value 666.285571
iter 100 value 666.162030
final  value 666.162030 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  33
initial  value 1273.858471 
iter  10 value 1056.030559
iter  20 value 1052.199978
iter  30 value 1052.161879
iter  40 value 1052.061399
iter  50 value 669.928051
iter  60 value 666.652998
iter  70 value 666.303017
iter  80 value 666.196073
iter  90 value 666.121056
iter 100 value 666.091040
final  value 666.091040 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  33
initial  value 1139.123923 
iter  10 value 1054.191063
iter  20 value 1052.161408
iter  30 value 695.480543
iter  40 value 667.194025
iter  50 value 666.205484
iter  60 value 666.174015
iter  70 value 666.151220
iter  80 value 666.115740
iter  90 value 666.104820
iter 100 value 666.093398
final  value 666.093398 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  25
initial  value 1255.125881 
iter  10 value 1004.673377
iter  20 value 1002.156974
iter  30 value 624.127464
iter  40 value 617.256223
iter  50 value 615.244282
iter  60 value 611.493251
iter  70 value 610.179096
iter  80 value 609.723832
iter  90 value 609.463050
iter 100 value 609.174405
final  value 609.174405 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  25
initial  value 1288.104785 
iter  10 value 1081.755760
iter  20 value 1078.862509
iter  30 value 771.027642
iter  40 value 762.648629
iter  50 value 754.769065
iter  60 value 751.397334
iter  70 value 750.393655
iter  80 value 750.101652
iter  90 value 749.940366
iter 100 value 749.819524
final  value 749.819524 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  25
initial  value 1222.189922 
iter  10 value 1134.697681
iter  20 value 1133.581397
iter  30 value 1133.302853
iter  40 value 734.597716
iter  50 value 732.102556
iter  60 value 720.459094
iter  70 value 717.048303
iter  80 value 716.338200
iter  90 value 715.754990
iter 100 value 715.650117
final  value 715.650117 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  25
initial  value 1246.303469 
iter  10 value 1041.268732
iter  20 value 1038.434346
iter  30 value 689.562601
iter  40 value 687.692800
iter  50 value 669.401201
iter  60 value 665.967524
iter  70 value 665.400177
iter  80 value 665.210998
iter  90 value 665.087090
iter 100 value 665.021180
final  value 665.021180 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  25
initial  value 1162.963333 
iter  10 value 1107.905551
iter  20 value 706.335397
iter  30 value 699.057117
iter  40 value 696.479075
iter  50 value 693.924773
iter  60 value 691.012528
iter  70 value 685.736266
iter  80 value 684.524341
iter  90 value 683.714162
iter 100 value 683.502035
final  value 683.502035 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  5
initial  value 1184.741678 
iter  10 value 809.956323
final  value 809.164755 
converged
Fitting Repeat 2 

# weights:  5
initial  value 1265.971267 
iter  10 value 800.286765
final  value 788.734090 
converged
Fitting Repeat 3 

# weights:  5
initial  value 1173.736710 
iter  10 value 833.266568
iter  20 value 825.463793
final  value 825.463763 
converged
Fitting Repeat 4 

# weights:  5
initial  value 1184.752972 
iter  10 value 812.020004
final  value 779.281276 
converged
Fitting Repeat 5 

# weights:  5
initial  value 1247.626354 
iter  10 value 863.046894
iter  20 value 796.321401
iter  20 value 796.321395
iter  20 value 796.321395
final  value 796.321395 
converged
Fitting Repeat 1 

# weights:  49
initial  value 1325.441162 
iter  10 value 1052.488577
iter  20 value 1052.132312
iter  30 value 1052.118426
iter  40 value 1051.723442
iter  50 value 667.238333
iter  60 value 666.108666
iter  70 value 665.891110
iter  80 value 665.766332
iter  90 value 665.741899
iter 100 value 665.731355
final  value 665.731355 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  49
initial  value 1341.110736 
iter  10 value 1052.361979
iter  20 value 1052.136589
iter  30 value 1052.103805
iter  40 value 741.701708
iter  50 value 676.243709
iter  60 value 675.822571
iter  70 value 675.468515
iter  80 value 668.896424
iter  90 value 666.481633
iter 100 value 666.281650
final  value 666.281650 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  49
initial  value 1165.390737 
iter  10 value 1052.477968
iter  20 value 1052.132195
iter  30 value 1052.119663
iter  40 value 1052.044119
iter  50 value 673.900283
iter  60 value 666.814631
iter  70 value 665.829133
iter  80 value 665.811899
iter  90 value 665.801363
iter 100 value 665.787758
final  value 665.787758 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  49
initial  value 1129.248048 
iter  10 value 1052.386266
iter  20 value 1052.130151
iter  30 value 1052.129610
iter  40 value 1052.128814
iter  50 value 1052.127503
iter  60 value 1052.124887
iter  70 value 1052.116871
iter  80 value 1051.794235
iter  90 value 666.605924
iter 100 value 666.113351
final  value 666.113351 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  49
initial  value 1105.852315 
iter  10 value 1052.496278
final  value 1052.131101 
converged
Fitting Repeat 1 

# weights:  65
initial  value 1161.357163 
iter  10 value 759.580791
iter  20 value 714.636244
iter  30 value 711.545690
iter  40 value 711.073648
iter  50 value 711.067000
final  value 711.066980 
converged
Fitting Repeat 2 

# weights:  65
initial  value 1125.450945 
iter  10 value 713.415652
iter  20 value 711.193697
iter  30 value 711.060111
iter  40 value 711.056423
final  value 711.056052 
converged
Fitting Repeat 3 

# weights:  65
initial  value 1147.306921 
iter  10 value 714.504878
iter  20 value 711.146354
iter  30 value 711.056145
iter  30 value 711.056142
final  value 711.056042 
converged
Fitting Repeat 4 

# weights:  65
initial  value 1078.137706 
iter  10 value 712.923845
iter  20 value 710.992057
iter  30 value 710.965188
final  value 710.964659 
converged
Fitting Repeat 5 

# weights:  65
initial  value 1076.895523 
iter  10 value 732.074070
iter  20 value 711.601655
iter  30 value 711.397659
iter  40 value 711.361868
iter  50 value 711.356321
final  value 711.356300 
converged
Fitting Repeat 1 

# weights:  41
initial  value 1108.880518 
iter  10 value 689.172928
iter  20 value 668.577876
iter  30 value 667.988563
iter  40 value 667.776487
iter  50 value 667.719533
iter  60 value 667.692113
iter  70 value 667.679691
iter  80 value 667.674231
iter  90 value 667.673048
iter 100 value 667.672092
final  value 667.672092 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  41
initial  value 1343.901072 
iter  10 value 681.236417
iter  20 value 669.858754
iter  30 value 668.047001
iter  40 value 667.796751
iter  50 value 667.759326
iter  60 value 667.738612
iter  70 value 667.719890
iter  80 value 667.710237
iter  90 value 667.707237
iter 100 value 667.706060
final  value 667.706060 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  41
initial  value 1119.752320 
iter  10 value 1054.428680
iter  20 value 677.090722
iter  30 value 670.589173
iter  40 value 668.358309
iter  50 value 667.920908
iter  60 value 667.796147
iter  70 value 667.732811
iter  80 value 667.722066
iter  90 value 667.714679
iter 100 value 667.683584
final  value 667.683584 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  41
initial  value 1078.300652 
iter  10 value 791.254658
iter  20 value 668.770866
iter  30 value 668.033072
iter  40 value 667.842457
iter  50 value 667.746736
iter  60 value 667.723034
iter  70 value 667.716675
iter  80 value 667.713474
iter  90 value 667.710143
iter 100 value 667.706823
final  value 667.706823 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  41
initial  value 1155.623525 
iter  10 value 1054.346852
iter  20 value 673.590063
iter  30 value 668.618439
iter  40 value 668.218767
iter  50 value 668.104335
iter  60 value 667.882557
iter  70 value 667.849419
iter  80 value 667.824839
iter  90 value 667.802873
iter 100 value 667.794941
final  value 667.794941 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  57
initial  value 1218.867390 
iter  10 value 1053.580638
iter  20 value 1052.143707
iter  30 value 709.530166
iter  40 value 674.768438
iter  50 value 669.280412
iter  60 value 668.391862
iter  70 value 667.468191
iter  80 value 667.044650
iter  90 value 666.472930
iter 100 value 666.230264
final  value 666.230264 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  57
initial  value 1173.826634 
iter  10 value 1053.293621
iter  20 value 1052.128513
iter  30 value 1052.069437
iter  40 value 676.660489
iter  50 value 675.606827
iter  60 value 668.183511
iter  70 value 667.227398
iter  80 value 667.009388
iter  90 value 666.693960
iter 100 value 666.327441
final  value 666.327441 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  57
initial  value 1297.331998 
iter  10 value 1053.783335
iter  20 value 1052.144464
iter  30 value 687.980455
iter  40 value 669.041843
iter  50 value 666.213732
iter  60 value 666.068707
iter  70 value 666.002413
iter  80 value 665.978549
iter  90 value 665.955279
iter 100 value 665.933909
final  value 665.933909 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  57
initial  value 1383.670434 
iter  10 value 1053.165289
iter  20 value 1052.144225
iter  30 value 764.175134
iter  40 value 681.138173
iter  50 value 676.560807
iter  60 value 670.084319
iter  70 value 667.496599
iter  80 value 666.879733
iter  90 value 666.474962
iter 100 value 666.185273
final  value 666.185273 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  57
initial  value 1282.534712 
iter  10 value 1053.782532
iter  20 value 1052.144148
iter  30 value 688.441043
iter  40 value 666.594259
iter  50 value 666.031492
iter  60 value 665.956835
iter  70 value 665.901071
iter  80 value 665.877547
iter  90 value 665.865425
iter 100 value 665.855016
final  value 665.855016 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  77
initial  value 1437.811078 
iter  10 value 722.088046
iter  20 value 671.993763
iter  30 value 669.369835
iter  40 value 669.185809
iter  50 value 669.138109
iter  60 value 669.125872
iter  70 value 669.118236
iter  80 value 669.113496
iter  90 value 669.112631
final  value 669.112561 
converged
Fitting Repeat 2 

# weights:  77
initial  value 1431.172282 
iter  10 value 690.555096
iter  20 value 671.520471
iter  30 value 669.246148
iter  40 value 669.157911
iter  50 value 669.136294
iter  60 value 669.121873
iter  70 value 669.115169
iter  80 value 669.114649
iter  90 value 669.114435
final  value 669.114325 
converged
Fitting Repeat 3 

# weights:  77
initial  value 1090.143936 
iter  10 value 706.093266
iter  20 value 672.633803
iter  30 value 669.580187
iter  40 value 669.193516
iter  50 value 669.164079
iter  60 value 669.156541
iter  70 value 669.154259
iter  80 value 669.153057
iter  90 value 669.152428
iter 100 value 669.151322
final  value 669.151322 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  77
initial  value 1034.632363 
iter  10 value 701.294981
iter  20 value 671.195384
iter  30 value 669.792930
iter  40 value 669.567677
iter  50 value 669.447135
iter  60 value 669.332487
iter  70 value 669.275896
iter  80 value 669.198317
iter  90 value 669.155614
iter 100 value 669.137112
final  value 669.137112 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  77
initial  value 1306.856492 
iter  10 value 691.532915
iter  20 value 673.594853
iter  30 value 669.458482
iter  40 value 669.195032
iter  50 value 669.128432
iter  60 value 669.121068
iter  70 value 669.118481
iter  80 value 669.115171
iter  90 value 669.113278
iter 100 value 669.112751
final  value 669.112751 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  81
initial  value 1229.532244 
iter  10 value 1055.173497
iter  20 value 1054.188938
iter  30 value 763.432081
iter  40 value 738.754625
iter  50 value 718.350020
iter  60 value 718.200374
iter  70 value 717.668794
iter  80 value 698.551365
iter  90 value 693.643557
iter 100 value 691.253478
final  value 691.253478 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  81
initial  value 1291.657895 
iter  10 value 1095.011912
iter  20 value 1094.060651
iter  30 value 1094.056253
iter  40 value 1094.056042
iter  50 value 1094.055792
iter  60 value 1094.055484
iter  70 value 1094.055086
iter  80 value 1094.054529
iter  90 value 1094.053674
iter 100 value 1094.052100
final  value 1094.052100 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  81
initial  value 1210.509417 
iter  10 value 796.991308
iter  20 value 796.733409
iter  30 value 796.283579
iter  40 value 796.211008
iter  50 value 796.195172
iter  60 value 796.182191
iter  70 value 796.173827
iter  80 value 796.170867
iter  90 value 796.167588
iter 100 value 796.164281
final  value 796.164281 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  81
initial  value 1400.599139 
iter  10 value 1106.043960
iter  20 value 1105.662414
iter  30 value 1105.574516
iter  40 value 661.874398
iter  50 value 660.860654
iter  60 value 660.641075
iter  70 value 660.600307
iter  80 value 660.564661
iter  90 value 660.533933
iter 100 value 660.508465
final  value 660.508465 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  81
initial  value 1149.106002 
iter  10 value 1094.340813
iter  20 value 1093.246609
iter  30 value 1093.236110
iter  40 value 1093.232849
iter  50 value 1093.213720
iter  60 value 687.754868
iter  70 value 685.444551
iter  80 value 685.243437
iter  90 value 684.918878
iter 100 value 684.844815
final  value 684.844815 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  29
initial  value 1242.567231 
iter  10 value 1053.158594
iter  20 value 1052.143483
iter  30 value 678.349656
iter  40 value 677.123185
iter  50 value 670.195515
iter  60 value 668.974175
iter  70 value 668.143091
iter  80 value 667.430893
iter  90 value 666.835348
iter 100 value 666.525900
final  value 666.525900 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  29
initial  value 1268.796084 
iter  10 value 1052.794848
iter  20 value 1052.168219
iter  30 value 1052.102497
iter  40 value 689.821724
iter  50 value 670.628426
iter  60 value 666.470996
iter  70 value 666.311478
iter  80 value 666.241852
iter  90 value 666.065441
iter 100 value 665.957631
final  value 665.957631 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  29
initial  value 1184.811586 
iter  10 value 1052.795762
iter  20 value 1052.146119
iter  30 value 672.835829
iter  40 value 666.906822
iter  50 value 666.396149
iter  60 value 666.306546
iter  70 value 666.086268
iter  80 value 665.946540
iter  90 value 665.901883
iter 100 value 665.886720
final  value 665.886720 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  29
initial  value 1210.436620 
iter  10 value 1052.972347
iter  20 value 1052.145351
iter  30 value 1052.143951
iter  40 value 1052.141775
iter  50 value 1052.137662
iter  60 value 1052.126192
iter  70 value 1051.994070
iter  80 value 666.757108
iter  90 value 666.194245
iter 100 value 665.884737
final  value 665.884737 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  29
initial  value 1335.645059 
iter  10 value 1052.819312
iter  20 value 1052.152292
iter  30 value 689.249033
iter  40 value 675.501884
iter  50 value 669.574613
iter  60 value 667.256461
iter  70 value 666.653797
iter  80 value 666.432081
iter  90 value 666.098465
iter 100 value 665.961088
final  value 665.961088 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  17
initial  value 1164.981227 
iter  10 value 1052.475762
iter  20 value 1051.852520
iter  30 value 667.995226
iter  40 value 666.092746
iter  50 value 665.914330
iter  60 value 665.893865
iter  70 value 665.884367
iter  80 value 665.874502
iter  90 value 665.871990
iter 100 value 665.871477
final  value 665.871477 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  17
initial  value 1193.208037 
iter  10 value 1053.002698
iter  20 value 1052.158020
iter  30 value 1052.155622
iter  40 value 1052.151427
iter  50 value 1052.138628
iter  60 value 1049.477215
iter  70 value 667.885980
iter  80 value 666.014742
iter  90 value 665.903766
iter 100 value 665.887417
final  value 665.887417 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  17
initial  value 1181.792981 
iter  10 value 1052.994314
iter  20 value 787.390723
iter  30 value 667.538409
iter  40 value 666.464554
iter  50 value 666.214985
iter  60 value 666.077810
iter  70 value 665.996141
iter  80 value 665.935950
iter  90 value 665.916042
iter 100 value 665.910108
final  value 665.910108 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  17
initial  value 1162.018721 
iter  10 value 1052.693063
iter  20 value 1052.167074
iter  30 value 1052.145432
iter  40 value 1052.093768
iter  50 value 674.038013
iter  60 value 666.515547
iter  70 value 666.027362
iter  80 value 665.998451
iter  90 value 665.951110
iter 100 value 665.927554
final  value 665.927554 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  17
initial  value 1110.422648 
iter  10 value 1052.276565
iter  20 value 1052.118816
iter  30 value 734.229506
iter  40 value 666.921102
iter  50 value 666.504840
iter  60 value 666.365021
iter  70 value 666.088842
iter  80 value 665.980783
iter  90 value 665.949032
iter 100 value 665.876649
final  value 665.876649 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  49
initial  value 1276.023537 
iter  10 value 856.429154
iter  20 value 850.159776
iter  30 value 849.579261
iter  40 value 849.559306
final  value 849.559135 
converged
Fitting Repeat 2 

# weights:  49
initial  value 1186.439603 
iter  10 value 935.387982
iter  20 value 863.272001
iter  30 value 851.881425
iter  40 value 850.321224
iter  50 value 850.138169
iter  60 value 850.117628
final  value 850.115603 
converged
Fitting Repeat 3 

# weights:  49
initial  value 1199.589399 
iter  10 value 920.838339
iter  20 value 856.841090
iter  30 value 850.297371
iter  40 value 849.671786
iter  50 value 849.583450
iter  60 value 849.559195
final  value 849.559134 
converged
Fitting Repeat 4 

# weights:  49
initial  value 1333.786218 
iter  10 value 879.176759
iter  20 value 853.856658
iter  30 value 852.850154
iter  40 value 852.837946
final  value 852.837929 
converged
Fitting Repeat 5 

# weights:  49
initial  value 1399.299980 
iter  10 value 860.146319
iter  20 value 850.324746
iter  30 value 849.606921
iter  40 value 849.559262
final  value 849.559129 
converged
Fitting Repeat 1 

# weights:  17
initial  value 1307.912644 
iter  10 value 723.383415
iter  20 value 672.651685
iter  30 value 671.106345
iter  40 value 670.559992
iter  50 value 670.222602
iter  60 value 670.220149
final  value 670.220135 
converged
Fitting Repeat 2 

# weights:  17
initial  value 1232.717876 
iter  10 value 775.435220
iter  20 value 681.801677
iter  30 value 672.285540
iter  40 value 670.387136
iter  50 value 670.222118
final  value 670.220137 
converged
Fitting Repeat 3 

# weights:  17
initial  value 1134.470296 
iter  10 value 741.360409
iter  20 value 674.308049
iter  30 value 670.461531
iter  40 value 670.244582
iter  50 value 670.220486
final  value 670.220135 
converged
Fitting Repeat 4 

# weights:  17
initial  value 1290.168126 
iter  10 value 711.986739
iter  20 value 672.617465
iter  30 value 671.069980
iter  40 value 670.571551
iter  50 value 670.408047
final  value 670.406519 
converged
Fitting Repeat 5 

# weights:  17
initial  value 1166.939029 
iter  10 value 1056.321162
iter  20 value 678.496232
iter  30 value 672.516376
iter  40 value 671.818486
iter  50 value 671.767293
iter  60 value 671.749814
final  value 671.749599 
converged
Fitting Repeat 1 

# weights:  9
initial  value 1260.359059 
iter  10 value 1063.155479
iter  20 value 1059.168728
iter  30 value 1059.142696
iter  40 value 1058.979070
iter  50 value 714.009306
iter  60 value 701.630324
iter  70 value 700.572567
iter  80 value 700.482162
iter  90 value 700.467075
final  value 700.466576 
converged
Fitting Repeat 2 

# weights:  9
initial  value 1122.272819 
iter  10 value 1063.506637
iter  20 value 1062.396295
iter  30 value 672.050357
iter  40 value 636.772923
iter  50 value 631.552134
iter  60 value 629.446757
iter  70 value 629.070767
iter  80 value 629.050105
final  value 629.049055 
converged
Fitting Repeat 3 

# weights:  9
initial  value 1294.668921 
iter  10 value 1182.297415
iter  20 value 1179.517234
iter  30 value 784.806380
iter  40 value 782.822302
iter  50 value 782.712982
iter  60 value 782.704237
iter  60 value 782.704235
iter  60 value 782.704235
final  value 782.704235 
converged
Fitting Repeat 4 

# weights:  9
initial  value 1188.361115 
iter  10 value 1101.635338
iter  20 value 1029.297154
iter  30 value 760.916679
iter  40 value 752.157870
iter  50 value 740.312291
iter  60 value 731.073365
iter  70 value 723.398755
iter  80 value 722.369836
iter  90 value 722.298670
iter 100 value 722.296138
final  value 722.296138 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  9
initial  value 1246.816782 
iter  10 value 1083.409770
iter  20 value 1081.273767
iter  30 value 722.021506
iter  40 value 684.764736
iter  50 value 681.442297
iter  60 value 680.543338
iter  70 value 680.422177
iter  80 value 680.412172
final  value 680.412090 
converged
Fitting Repeat 1 

# weights:  61
initial  value 1060.266741 
iter  10 value 686.991266
iter  20 value 677.613530
iter  30 value 677.278665
iter  40 value 677.227555
iter  50 value 677.221343
iter  60 value 677.220665
final  value 677.220626 
converged
Fitting Repeat 2 

# weights:  61
initial  value 1194.097209 
iter  10 value 762.526145
iter  20 value 682.699685
iter  30 value 677.475244
iter  40 value 677.284071
iter  50 value 677.221731
iter  60 value 677.220957
iter  70 value 677.220612
final  value 677.220577 
converged
Fitting Repeat 3 

# weights:  61
initial  value 1108.069979 
iter  10 value 703.714611
iter  20 value 678.548908
iter  30 value 677.459640
iter  40 value 677.291136
iter  50 value 677.246046
iter  60 value 677.221438
iter  70 value 677.220667
final  value 677.220571 
converged
Fitting Repeat 4 

# weights:  61
initial  value 1293.775797 
iter  10 value 877.395064
iter  20 value 699.306057
iter  30 value 677.868441
iter  40 value 677.289087
iter  50 value 677.251040
iter  60 value 677.231549
iter  70 value 677.219106
iter  80 value 677.215894
final  value 677.215856 
converged
Fitting Repeat 5 

# weights:  61
initial  value 1250.750264 
iter  10 value 873.727768
iter  20 value 685.807984
iter  30 value 678.300459
iter  40 value 677.538455
iter  50 value 677.394920
iter  60 value 677.331370
iter  70 value 677.303811
iter  80 value 677.299521
iter  90 value 677.298868
final  value 677.298660 
converged
Fitting Repeat 1 

# weights:  25
initial  value 1254.189652 
iter  10 value 718.609217
iter  20 value 679.349499
iter  30 value 676.844811
iter  40 value 676.513010
iter  50 value 676.317108
iter  60 value 676.226462
iter  70 value 676.169896
final  value 676.169080 
converged
Fitting Repeat 2 

# weights:  25
initial  value 1015.918720 
iter  10 value 955.314135
iter  20 value 677.471083
iter  30 value 609.233467
iter  40 value 608.611563
iter  50 value 607.907886
iter  60 value 607.574534
iter  70 value 607.504911
iter  80 value 607.445852
iter  90 value 607.437515
iter 100 value 607.437094
final  value 607.437094 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  25
initial  value 1018.176697 
iter  10 value 610.891241
iter  20 value 589.282795
iter  30 value 586.403815
iter  40 value 586.090871
iter  50 value 585.965328
iter  60 value 585.924554
iter  70 value 585.910421
iter  80 value 585.909615
final  value 585.909591 
converged
Fitting Repeat 4 

# weights:  25
initial  value 946.271885 
iter  10 value 613.786984
iter  20 value 592.366915
iter  30 value 589.588695
iter  40 value 589.401433
iter  50 value 589.329613
iter  60 value 589.294882
iter  70 value 589.257207
iter  80 value 589.252126
final  value 589.252064 
converged
Fitting Repeat 5 

# weights:  25
initial  value 1303.214542 
iter  10 value 718.867452
iter  20 value 703.182134
iter  30 value 699.005417
iter  40 value 698.282439
iter  50 value 698.043175
iter  60 value 697.961771
iter  70 value 697.934113
iter  80 value 697.933660
iter  80 value 697.933653
iter  80 value 697.933653
final  value 697.933653 
converged
Fitting Repeat 1 

# weights:  49
initial  value 1148.397133 
iter  10 value 1020.381072
iter  20 value 1020.331537
iter  20 value 1020.331537
iter  20 value 1020.331536
final  value 1020.331536 
converged
Fitting Repeat 2 

# weights:  49
initial  value 1022.862189 
iter  10 value 1020.343023
iter  20 value 1020.318083
iter  30 value 637.849293
iter  40 value 632.329343
iter  50 value 631.555780
iter  60 value 631.210135
iter  70 value 631.137435
iter  80 value 631.130572
iter  90 value 631.125404
iter 100 value 631.120022
final  value 631.120022 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  49
initial  value 1036.867587 
iter  10 value 1020.343666
iter  20 value 1020.331529
iter  30 value 1020.330235
iter  40 value 1020.327526
iter  50 value 1020.318082
iter  60 value 642.655580
iter  70 value 631.842010
iter  80 value 631.240322
iter  90 value 631.155594
iter 100 value 631.133793
final  value 631.133793 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  49
initial  value 1144.784273 
iter  10 value 1020.380461
iter  20 value 1020.331002
iter  30 value 1020.329556
iter  40 value 1020.327056
iter  50 value 1020.319800
iter  60 value 1018.407404
iter  70 value 632.599393
iter  80 value 632.051362
iter  90 value 631.897704
iter 100 value 631.391990
final  value 631.391990 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  49
initial  value 1052.565935 
iter  10 value 1020.339108
iter  20 value 1020.338498
iter  30 value 1020.337369
iter  40 value 1020.334693
iter  50 value 1020.322756
iter  60 value 647.264830
iter  70 value 631.584910
iter  80 value 631.211268
iter  90 value 631.194458
iter 100 value 631.186882
final  value 631.186882 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  25
initial  value 1216.416730 
iter  10 value 1020.436346
iter  20 value 1020.334070
iter  20 value 1020.334066
iter  20 value 1020.334063
final  value 1020.334063 
converged
Fitting Repeat 2 

# weights:  25
initial  value 1061.739675 
iter  10 value 1020.383150
iter  20 value 1020.331645
iter  30 value 1020.331020
iter  40 value 1020.329962
iter  50 value 1020.327789
iter  60 value 1020.321068
iter  70 value 1020.191285
iter  80 value 634.799646
iter  90 value 631.627095
iter 100 value 631.169583
final  value 631.169583 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  25
initial  value 1235.515219 
iter  10 value 1020.446094
iter  20 value 1020.334033
iter  20 value 1020.334024
iter  20 value 1020.334022
final  value 1020.334022 
converged
Fitting Repeat 4 

# weights:  25
initial  value 1214.379855 
iter  10 value 1020.427080
final  value 1020.333818 
converged
Fitting Repeat 5 

# weights:  25
initial  value 1175.652355 
iter  10 value 1020.385114
final  value 1020.333292 
converged
Fitting Repeat 1 

# weights:  41
initial  value 1126.144095 
iter  10 value 1020.390752
final  value 1020.335118 
converged
Fitting Repeat 2 

# weights:  41
initial  value 1361.748379 
iter  10 value 1020.379305
final  value 1020.333244 
converged
Fitting Repeat 3 

# weights:  41
initial  value 1034.554714 
iter  10 value 1020.367207
final  value 1020.333377 
converged
Fitting Repeat 4 

# weights:  41
initial  value 1088.390846 
iter  10 value 1020.367012
iter  20 value 645.573699
iter  30 value 631.457072
iter  40 value 631.203522
iter  50 value 631.158460
iter  60 value 631.147413
iter  70 value 631.142946
iter  80 value 631.140405
iter  90 value 631.139820
iter 100 value 631.138557
final  value 631.138557 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  41
initial  value 1136.591160 
iter  10 value 1020.412353
final  value 1020.331722 
converged
Fitting Repeat 1 

# weights:  29
initial  value 1150.579089 
iter  10 value 1021.256034
iter  20 value 1020.362845
iter  30 value 1020.327111
iter  40 value 1014.364971
iter  50 value 636.743368
iter  60 value 631.803630
iter  70 value 631.407166
iter  80 value 631.370298
iter  90 value 631.337666
iter 100 value 631.318315
final  value 631.318315 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  29
initial  value 1263.479917 
iter  10 value 1021.131759
iter  20 value 1020.363269
iter  30 value 1020.347612
iter  40 value 1020.345821
iter  50 value 1020.342036
iter  60 value 1020.325687
iter  70 value 654.898513
iter  80 value 632.072953
iter  90 value 631.552308
iter 100 value 631.483692
final  value 631.483692 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  29
initial  value 1068.106318 
iter  10 value 1020.724723
iter  20 value 1020.343452
iter  30 value 1019.964542
iter  40 value 631.994844
iter  50 value 631.352416
iter  60 value 631.339383
iter  70 value 631.322360
iter  80 value 631.317184
iter  90 value 631.310161
iter 100 value 631.305325
final  value 631.305325 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  29
initial  value 1209.413827 
iter  10 value 1021.320406
iter  20 value 1020.360355
iter  30 value 642.736310
iter  40 value 632.518945
iter  50 value 632.059192
iter  60 value 631.693479
iter  70 value 631.553475
iter  80 value 631.475647
iter  90 value 631.392107
iter 100 value 631.369550
final  value 631.369550 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  29
initial  value 1012.568848 
iter  10 value 640.823248
iter  20 value 634.767885
iter  30 value 631.991019
iter  40 value 631.774707
iter  50 value 631.480071
iter  60 value 631.387457
iter  70 value 631.360504
iter  80 value 631.340785
iter  90 value 631.308850
iter 100 value 631.300359
final  value 631.300359 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  33
initial  value 1223.122679 
iter  10 value 1024.061782
iter  20 value 1020.398686
iter  30 value 877.177465
iter  40 value 650.887472
iter  50 value 647.038414
iter  60 value 638.255860
iter  70 value 636.070598
iter  80 value 634.414429
iter  90 value 632.511605
iter 100 value 631.921041
final  value 631.921041 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  33
initial  value 1211.150557 
iter  10 value 1024.004497
iter  20 value 1020.377710
iter  30 value 636.661631
iter  40 value 632.538645
iter  50 value 631.906580
iter  60 value 631.755868
iter  70 value 631.675915
iter  80 value 631.632352
iter  90 value 631.619972
iter 100 value 631.607381
final  value 631.607381 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  33
initial  value 1065.386108 
iter  10 value 1021.062355
iter  20 value 1020.371172
iter  30 value 636.161917
iter  40 value 631.715300
iter  50 value 631.636832
iter  60 value 631.557635
iter  70 value 631.539678
iter  80 value 631.531528
iter  90 value 631.524008
iter 100 value 631.519322
final  value 631.519322 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  33
initial  value 1126.040032 
iter  10 value 1023.802810
iter  20 value 1020.377629
iter  30 value 658.286153
iter  40 value 637.943323
iter  50 value 633.813484
iter  60 value 632.550343
iter  70 value 631.868566
iter  80 value 631.724026
iter  90 value 631.666217
iter 100 value 631.632705
final  value 631.632705 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  33
initial  value 1311.676006 
iter  10 value 1022.138772
iter  20 value 1020.423228
iter  30 value 652.186535
iter  40 value 646.922880
iter  50 value 642.616731
iter  60 value 638.567722
iter  70 value 635.871072
iter  80 value 633.936753
iter  90 value 632.969207
iter 100 value 632.095555
final  value 632.095555 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  25
initial  value 1106.487494 
iter  10 value 983.155532
iter  20 value 981.862034
iter  30 value 664.215098
iter  40 value 661.904421
iter  50 value 656.813762
iter  60 value 654.920687
iter  70 value 653.449709
iter  80 value 649.565696
iter  90 value 647.205502
iter 100 value 645.923711
final  value 645.923711 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  25
initial  value 1184.015617 
iter  10 value 1152.984731
iter  20 value 1151.839020
iter  30 value 700.730838
iter  40 value 699.445618
iter  50 value 699.227745
iter  60 value 699.177312
iter  70 value 699.166905
iter  80 value 699.161863
iter  90 value 699.157549
iter 100 value 699.155754
final  value 699.155754 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  25
initial  value 1104.774631 
iter  10 value 700.889521
iter  20 value 692.503012
iter  30 value 684.535563
iter  40 value 679.778645
iter  50 value 679.325433
iter  60 value 679.263256
iter  70 value 679.235910
iter  80 value 679.214331
iter  90 value 679.202287
iter 100 value 679.195684
final  value 679.195684 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  25
initial  value 1297.375083 
iter  10 value 1192.388701
iter  20 value 1185.356451
iter  30 value 764.094070
iter  40 value 762.342555
iter  50 value 751.670284
iter  60 value 750.087450
iter  70 value 749.915642
iter  80 value 749.777213
iter  90 value 749.734760
iter 100 value 749.718795
final  value 749.718795 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  25
initial  value 1160.630727 
iter  10 value 986.290937
iter  20 value 984.100365
iter  30 value 983.120670
iter  40 value 646.946792
iter  50 value 636.492881
iter  60 value 634.297183
iter  70 value 633.889240
iter  80 value 633.764071
iter  90 value 633.727895
iter 100 value 633.678528
final  value 633.678528 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  5
initial  value 1048.798085 
iter  10 value 717.268416
iter  20 value 707.656948
iter  20 value 707.656944
iter  20 value 707.656944
final  value 707.656944 
converged
Fitting Repeat 2 

# weights:  5
initial  value 1146.208683 
iter  10 value 830.968957
iter  20 value 701.631099
final  value 701.601577 
converged
Fitting Repeat 3 

# weights:  5
initial  value 1096.288845 
iter  10 value 722.368552
final  value 714.036753 
converged
Fitting Repeat 4 

# weights:  5
initial  value 1161.995498 
iter  10 value 728.354324
final  value 722.053449 
converged
Fitting Repeat 5 

# weights:  5
initial  value 1223.472435 
iter  10 value 758.874949
iter  20 value 753.034620
final  value 753.030920 
converged
Fitting Repeat 1 

# weights:  49
initial  value 1286.457438 
iter  10 value 1020.575524
iter  20 value 1020.341555
iter  30 value 667.273726
iter  40 value 631.575498
iter  50 value 631.294159
iter  60 value 631.225149
iter  70 value 631.218003
iter  80 value 631.212736
iter  90 value 631.205971
iter 100 value 631.201978
final  value 631.201978 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  49
initial  value 1228.213474 
iter  10 value 1020.682082
iter  20 value 1020.337376
final  value 1020.336059 
converged
Fitting Repeat 3 

# weights:  49
initial  value 1151.519252 
iter  10 value 1020.824642
iter  20 value 1020.336489
iter  20 value 1020.336488
iter  20 value 1020.336479
final  value 1020.336479 
converged
Fitting Repeat 4 

# weights:  49
initial  value 1091.312150 
iter  10 value 1020.466974
final  value 1020.338764 
converged
Fitting Repeat 5 

# weights:  49
initial  value 1208.734735 
iter  10 value 1020.826513
iter  20 value 1020.335976
iter  30 value 803.914718
iter  40 value 634.674499
iter  50 value 632.734238
iter  60 value 632.399099
iter  70 value 632.131156
iter  80 value 631.977720
iter  90 value 631.883739
iter 100 value 631.673803
final  value 631.673803 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  65
initial  value 1058.648525 
iter  10 value 684.996158
iter  20 value 676.869503
iter  30 value 676.520209
iter  40 value 676.475769
iter  50 value 676.459763
final  value 676.459630 
converged
Fitting Repeat 2 

# weights:  65
initial  value 1299.220887 
iter  10 value 835.482816
iter  20 value 690.219419
iter  30 value 682.219728
iter  40 value 678.360514
iter  50 value 677.564671
iter  60 value 677.549645
final  value 677.549092 
converged
Fitting Repeat 3 

# weights:  65
initial  value 1210.999036 
iter  10 value 714.625893
iter  20 value 677.972231
iter  30 value 677.571454
iter  40 value 677.550802
final  value 677.549089 
converged
Fitting Repeat 4 

# weights:  65
initial  value 1252.628204 
iter  10 value 842.336194
iter  20 value 703.185048
iter  30 value 689.282366
iter  40 value 681.756354
iter  50 value 677.356511
iter  60 value 676.630177
iter  70 value 676.553566
iter  80 value 676.549921
final  value 676.549909 
converged
Fitting Repeat 5 

# weights:  65
initial  value 1197.437293 
iter  10 value 681.314841
iter  20 value 676.769052
iter  30 value 676.552190
iter  40 value 676.549948
final  value 676.549908 
converged
Fitting Repeat 1 

# weights:  41
initial  value 1208.614525 
iter  10 value 1022.482608
iter  20 value 675.639934
iter  30 value 643.786510
iter  40 value 635.895282
iter  50 value 633.889164
iter  60 value 633.509690
iter  70 value 633.333191
iter  80 value 633.238259
iter  90 value 633.205147
iter 100 value 633.191991
final  value 633.191991 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  41
initial  value 1102.846222 
iter  10 value 1022.661827
iter  20 value 650.713028
iter  30 value 634.737125
iter  40 value 633.576509
iter  50 value 633.336031
iter  60 value 633.233815
iter  70 value 633.206235
iter  80 value 633.194941
iter  90 value 633.185378
iter 100 value 633.175786
final  value 633.175786 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  41
initial  value 1244.369608 
iter  10 value 658.732062
iter  20 value 638.273730
iter  30 value 633.833363
iter  40 value 633.379853
iter  50 value 633.289951
iter  60 value 633.253905
iter  70 value 633.238186
iter  80 value 633.224670
iter  90 value 633.215435
iter 100 value 633.205273
final  value 633.205273 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  41
initial  value 1229.110154 
iter  10 value 1022.818908
iter  20 value 655.503723
iter  30 value 635.812575
iter  40 value 634.177068
iter  50 value 633.652539
iter  60 value 633.394268
iter  70 value 633.339022
iter  80 value 633.327913
iter  90 value 633.320820
iter 100 value 633.312590
final  value 633.312590 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  41
initial  value 1135.716647 
iter  10 value 1022.780555
iter  20 value 651.253676
iter  30 value 640.611410
iter  40 value 634.928106
iter  50 value 633.631591
iter  60 value 633.275261
iter  70 value 633.218900
iter  80 value 633.185089
iter  90 value 633.180651
iter 100 value 633.178274
final  value 633.178274 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  57
initial  value 1129.662253 
iter  10 value 1021.175296
iter  20 value 1015.110879
iter  30 value 669.480511
iter  40 value 664.274426
iter  50 value 648.579673
iter  60 value 647.386903
iter  70 value 644.623967
iter  80 value 633.750906
iter  90 value 632.284232
iter 100 value 632.092201
final  value 632.092201 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  57
initial  value 1073.216986 
iter  10 value 1021.018146
iter  20 value 1020.337105
iter  30 value 1020.296288
iter  40 value 633.292038
iter  50 value 631.549974
iter  60 value 631.371049
iter  70 value 631.339174
iter  80 value 631.322606
iter  90 value 631.304436
iter 100 value 631.299081
final  value 631.299081 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  57
initial  value 1135.866584 
iter  10 value 1021.550993
iter  20 value 1020.349231
iter  30 value 1020.204304
iter  40 value 645.771030
iter  50 value 643.615809
iter  60 value 642.008032
iter  70 value 635.875902
iter  80 value 633.618277
iter  90 value 633.041686
iter 100 value 632.678981
final  value 632.678981 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  57
initial  value 1275.694259 
iter  10 value 1021.516084
iter  20 value 1020.355583
iter  30 value 1020.335452
iter  40 value 1020.275701
iter  50 value 634.575261
iter  60 value 632.068653
iter  70 value 631.781908
iter  80 value 631.471836
iter  90 value 631.342865
iter 100 value 631.323598
final  value 631.323598 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  57
initial  value 1056.005714 
iter  10 value 1021.197394
iter  20 value 725.110100
iter  30 value 637.215936
iter  40 value 632.715892
iter  50 value 632.412968
iter  60 value 632.082140
iter  70 value 631.548499
iter  80 value 631.466645
iter  90 value 631.415397
iter 100 value 631.384209
final  value 631.384209 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  77
initial  value 1235.889668 
iter  10 value 639.393395
iter  20 value 634.827943
iter  30 value 634.650419
iter  40 value 634.617847
iter  50 value 634.604568
iter  60 value 634.603002
iter  70 value 634.602487
iter  80 value 634.601889
iter  90 value 634.601675
final  value 634.601642 
converged
Fitting Repeat 2 

# weights:  77
initial  value 1276.275518 
iter  10 value 669.593240
iter  20 value 646.383311
iter  30 value 635.884556
iter  40 value 634.764948
iter  50 value 634.633009
iter  60 value 634.614505
iter  70 value 634.612409
iter  80 value 634.611904
iter  90 value 634.611507
iter 100 value 634.611322
final  value 634.611322 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  77
initial  value 1158.255584 
iter  10 value 1017.991182
iter  20 value 668.939029
iter  30 value 640.023792
iter  40 value 635.741229
iter  50 value 635.063693
iter  60 value 634.845374
iter  70 value 634.751107
iter  80 value 634.718471
iter  90 value 634.650491
iter 100 value 634.630167
final  value 634.630167 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  77
initial  value 1120.767219 
iter  10 value 962.795056
iter  20 value 658.780327
iter  30 value 637.747741
iter  40 value 635.187757
iter  50 value 634.758410
iter  60 value 634.672372
iter  70 value 634.653850
iter  80 value 634.630209
iter  90 value 634.619835
iter 100 value 634.616819
final  value 634.616819 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  77
initial  value 1304.107615 
iter  10 value 1026.973158
iter  20 value 713.288880
iter  30 value 635.655840
iter  40 value 634.793279
iter  50 value 634.657867
iter  60 value 634.645279
iter  70 value 634.643698
iter  80 value 634.642663
iter  90 value 634.641999
final  value 634.641898 
converged
Fitting Repeat 1 

# weights:  81
initial  value 1051.303170 
iter  10 value 950.033628
iter  20 value 949.389131
iter  30 value 634.657958
iter  40 value 611.529635
iter  50 value 611.391475
iter  60 value 611.359610
iter  70 value 611.330296
iter  80 value 610.992661
iter  90 value 610.722212
iter 100 value 610.669466
final  value 610.669466 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  81
initial  value 1255.086682 
iter  10 value 997.918287
iter  20 value 997.169130
final  value 997.167467 
converged
Fitting Repeat 3 

# weights:  81
initial  value 1181.294870 
iter  10 value 1060.145405
iter  20 value 1059.137525
iter  30 value 679.169682
iter  40 value 659.095047
iter  50 value 658.947536
iter  60 value 658.477366
iter  70 value 658.292442
iter  80 value 658.271876
iter  90 value 658.245050
iter 100 value 658.239797
final  value 658.239797 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  81
initial  value 1367.094488 
iter  10 value 1072.683156
iter  20 value 1071.631968
iter  30 value 1002.582464
iter  40 value 716.042741
iter  50 value 712.727629
iter  60 value 707.462177
iter  70 value 706.597622
iter  80 value 706.532361
iter  90 value 706.445108
iter 100 value 706.340141
final  value 706.340141 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  81
initial  value 966.231133 
iter  10 value 633.144185
iter  20 value 629.628127
iter  30 value 626.209173
iter  40 value 624.557712
iter  50 value 623.767290
iter  60 value 623.512446
iter  70 value 623.403434
iter  80 value 623.344759
iter  90 value 623.281158
iter 100 value 623.187927
final  value 623.187927 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  29
initial  value 995.653183 
iter  10 value 674.029490
iter  20 value 656.850101
iter  30 value 640.901406
iter  40 value 634.540068
iter  50 value 631.822111
iter  60 value 631.696216
iter  70 value 631.468462
iter  80 value 631.361900
iter  90 value 631.318638
iter 100 value 631.311428
final  value 631.311428 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  29
initial  value 1039.000439 
iter  10 value 1020.752619
iter  20 value 1020.338862
iter  30 value 1020.238104
iter  40 value 647.668841
iter  50 value 646.848457
iter  60 value 644.284789
iter  70 value 636.110590
iter  80 value 635.190837
iter  90 value 633.134301
iter 100 value 632.402910
final  value 632.402910 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  29
initial  value 1245.864228 
iter  10 value 1021.411962
iter  20 value 1020.355075
iter  30 value 648.785631
iter  40 value 634.031828
iter  50 value 631.957823
iter  60 value 631.850006
iter  70 value 631.712818
iter  80 value 631.467612
iter  90 value 631.323389
iter 100 value 631.315173
final  value 631.315173 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  29
initial  value 1232.233647 
iter  10 value 1021.120251
iter  20 value 1020.365834
iter  30 value 1020.346761
iter  40 value 1020.344638
iter  50 value 1020.340529
iter  60 value 1020.327830
iter  70 value 1019.803993
iter  80 value 643.865611
iter  90 value 641.621824
iter 100 value 634.532348
final  value 634.532348 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  29
initial  value 1099.328155 
iter  10 value 1020.674380
iter  20 value 1020.361064
iter  30 value 1020.357635
iter  40 value 1020.348172
iter  50 value 1020.250956
iter  60 value 640.660644
iter  70 value 637.337186
iter  80 value 632.001408
iter  90 value 631.881177
iter 100 value 631.494977
final  value 631.494977 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  17
initial  value 1126.908035 
iter  10 value 1020.890360
iter  20 value 1020.361585
iter  30 value 750.308518
iter  40 value 635.426146
iter  50 value 631.922544
iter  60 value 631.571880
iter  70 value 631.462045
iter  80 value 631.408663
iter  90 value 631.364138
iter 100 value 631.348421
final  value 631.348421 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  17
initial  value 1070.012631 
iter  10 value 1020.772523
iter  20 value 1020.351923
iter  30 value 1020.286265
iter  40 value 633.186057
iter  50 value 631.400720
iter  60 value 631.326501
final  value 631.309199 
converged
Fitting Repeat 3 

# weights:  17
initial  value 1142.552950 
iter  10 value 1020.626057
iter  20 value 1019.913571
iter  30 value 632.797455
iter  40 value 631.399118
iter  50 value 631.316840
iter  60 value 631.313606
iter  70 value 631.305223
iter  80 value 631.300195
iter  90 value 631.297995
final  value 631.297313 
converged
Fitting Repeat 4 

# weights:  17
initial  value 1101.106419 
iter  10 value 1021.374063
iter  20 value 796.770209
iter  30 value 644.037003
iter  40 value 636.840345
iter  50 value 634.408728
iter  60 value 633.693994
iter  70 value 631.637323
iter  80 value 631.595552
iter  90 value 631.454739
iter 100 value 631.379485
final  value 631.379485 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  17
initial  value 1019.881026 
iter  10 value 635.169737
iter  20 value 631.521422
iter  30 value 631.383766
iter  40 value 631.372553
iter  50 value 631.324606
iter  60 value 631.303835
iter  70 value 631.296935
iter  80 value 631.295353
iter  90 value 631.293508
iter 100 value 631.291030
final  value 631.291030 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  49
initial  value 1354.797097 
iter  10 value 897.526766
iter  20 value 823.903505
iter  30 value 816.890033
iter  40 value 815.884171
iter  50 value 815.735351
iter  60 value 815.694106
final  value 815.693822 
converged
Fitting Repeat 2 

# weights:  49
initial  value 1302.792365 
iter  10 value 831.035355
iter  20 value 816.510289
iter  30 value 815.830487
iter  40 value 815.699299
iter  50 value 815.693857
final  value 815.693815 
converged
Fitting Repeat 3 

# weights:  49
initial  value 1125.218299 
iter  10 value 857.581054
iter  20 value 818.562520
iter  30 value 815.195012
iter  40 value 815.002008
iter  50 value 814.960411
final  value 814.960056 
converged
Fitting Repeat 4 

# weights:  49
initial  value 1218.133630 
iter  10 value 899.372531
iter  20 value 829.589015
iter  30 value 817.549287
iter  40 value 815.857125
iter  50 value 815.350900
iter  60 value 814.961608
final  value 814.960058 
converged
Fitting Repeat 5 

# weights:  49
initial  value 1226.185783 
iter  10 value 843.219966
iter  20 value 815.990353
iter  30 value 815.697094
final  value 815.693837 
converged
Fitting Repeat 1 

# weights:  17
initial  value 1184.917571 
iter  10 value 658.794389
iter  20 value 640.769352
iter  30 value 636.292639
iter  40 value 635.934107
iter  50 value 635.906569
final  value 635.906385 
converged
Fitting Repeat 2 

# weights:  17
initial  value 1239.831500 
iter  10 value 654.211861
iter  20 value 637.506082
iter  30 value 636.334709
iter  40 value 635.808123
iter  50 value 635.720838
final  value 635.720541 
converged
Fitting Repeat 3 

# weights:  17
initial  value 1042.443920 
iter  10 value 918.263739
iter  20 value 640.118353
iter  30 value 635.832663
iter  40 value 635.729223
iter  50 value 635.721028
final  value 635.720538 
converged
Fitting Repeat 4 

# weights:  17
initial  value 1158.302935 
iter  10 value 666.933713
iter  20 value 639.921055
iter  30 value 636.722303
iter  40 value 636.094151
iter  50 value 635.725234
iter  60 value 635.720587
final  value 635.720545 
converged
Fitting Repeat 5 

# weights:  17
initial  value 1051.531562 
iter  10 value 710.445056
iter  20 value 638.326804
iter  30 value 635.980709
iter  40 value 635.908730
iter  50 value 635.906384
iter  50 value 635.906383
iter  50 value 635.906383
final  value 635.906383 
converged
Fitting Repeat 1 

# weights:  9
initial  value 1168.533174 
iter  10 value 986.689370
iter  20 value 978.792449
iter  30 value 626.243059
iter  40 value 621.087625
iter  50 value 615.141675
iter  60 value 614.932549
iter  70 value 614.801468
iter  80 value 614.464415
iter  90 value 614.093530
iter 100 value 613.791937
final  value 613.791937 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  9
initial  value 990.080871 
iter  10 value 923.459663
iter  20 value 921.730944
iter  30 value 917.260409
iter  40 value 550.179717
iter  50 value 550.049516
iter  60 value 550.036401
final  value 550.035550 
converged
Fitting Repeat 3 

# weights:  9
initial  value 1178.764711 
iter  10 value 1035.153613
iter  20 value 807.821201
iter  30 value 685.554244
iter  40 value 673.473682
iter  50 value 670.297221
iter  60 value 665.713465
iter  70 value 665.066451
iter  80 value 664.766841
iter  90 value 664.483666
iter 100 value 664.426831
final  value 664.426831 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  9
initial  value 1162.269991 
iter  10 value 1042.772703
iter  20 value 836.701703
iter  30 value 634.861139
iter  40 value 628.054410
iter  50 value 627.281210
iter  60 value 626.610818
iter  70 value 626.576723
iter  80 value 626.569768
final  value 626.569660 
converged
Fitting Repeat 5 

# weights:  9
initial  value 1148.569394 
iter  10 value 1019.428256
iter  20 value 1017.494331
iter  30 value 1016.500527
iter  40 value 684.376195
iter  50 value 683.141308
iter  60 value 683.108718
iter  70 value 683.102866
final  value 683.102633 
converged
Fitting Repeat 1 

# weights:  61
initial  value 1026.753721 
iter  10 value 647.754390
iter  20 value 643.081916
iter  30 value 642.877943
iter  40 value 642.783238
iter  50 value 642.756985
iter  60 value 642.751706
final  value 642.751671 
converged
Fitting Repeat 2 

# weights:  61
initial  value 1124.888079 
iter  10 value 974.877273
iter  20 value 679.484729
iter  30 value 645.107244
iter  40 value 642.989592
iter  50 value 642.857198
iter  60 value 642.721405
iter  70 value 642.702597
iter  80 value 642.698670
iter  90 value 642.698093
final  value 642.697992 
converged
Fitting Repeat 3 

# weights:  61
initial  value 1224.390156 
iter  10 value 760.751758
iter  20 value 645.572089
iter  30 value 642.862262
iter  40 value 642.758974
iter  50 value 642.704643
iter  60 value 642.695082
iter  70 value 642.693197
final  value 642.693180 
converged
Fitting Repeat 4 

# weights:  61
initial  value 926.275795 
iter  10 value 649.152729
iter  20 value 643.058444
iter  30 value 642.803171
iter  40 value 642.722475
iter  50 value 642.703580
iter  60 value 642.698630
final  value 642.697976 
converged
Fitting Repeat 5 

# weights:  61
initial  value 1011.087406 
iter  10 value 651.020435
iter  20 value 643.379661
iter  30 value 642.823070
iter  40 value 642.778180
iter  50 value 642.776095
final  value 642.776083 
converged
Fitting Repeat 1 

# weights:  25
initial  value 1243.959782 
iter  10 value 772.751086
iter  20 value 772.392657
iter  30 value 772.360816
iter  40 value 772.351061
iter  50 value 772.347753
final  value 772.347398 
converged
Fitting Repeat 2 

# weights:  25
initial  value 1111.820357 
iter  10 value 694.462931
iter  20 value 693.938229
iter  30 value 693.848663
iter  40 value 693.758378
iter  50 value 693.743092
final  value 693.742732 
converged
Fitting Repeat 3 

# weights:  25
initial  value 1488.902643 
iter  10 value 1176.923997
iter  20 value 991.117026
iter  30 value 824.580316
iter  40 value 811.476580
iter  50 value 810.049837
iter  60 value 809.718380
iter  70 value 809.602919
iter  80 value 809.585822
iter  90 value 809.569195
final  value 809.568251 
converged
Fitting Repeat 4 

# weights:  25
initial  value 1339.195875 
iter  10 value 754.245792
iter  20 value 723.683850
iter  30 value 723.194962
iter  40 value 722.987621
iter  50 value 722.853655
iter  60 value 722.797737
iter  70 value 722.782669
final  value 722.782084 
converged
Fitting Repeat 5 

# weights:  25
initial  value 1230.875405 
iter  10 value 1082.649326
iter  20 value 797.895073
iter  30 value 752.507056
iter  40 value 748.427249
iter  50 value 747.725164
iter  60 value 747.522514
iter  70 value 747.418548
iter  80 value 747.304770
iter  90 value 747.296150
final  value 747.295761 
converged
Fitting Repeat 1 

# weights:  49
initial  value 1266.183995 
iter  10 value 1087.351607
iter  20 value 1087.258265
final  value 1087.258031 
converged
Fitting Repeat 2 

# weights:  49
initial  value 1325.904562 
iter  10 value 1087.341157
iter  20 value 1087.257970
final  value 1087.257920 
converged
Fitting Repeat 3 

# weights:  49
initial  value 1317.599690 
iter  10 value 1087.333034
iter  20 value 1087.257985
final  value 1087.257959 
converged
Fitting Repeat 4 

# weights:  49
initial  value 1301.555013 
iter  10 value 1087.324656
iter  20 value 1087.258172
final  value 1087.258106 
converged
Fitting Repeat 5 

# weights:  49
initial  value 1269.250897 
iter  10 value 1087.325152
final  value 1087.258837 
converged
Fitting Repeat 1 

# weights:  25
initial  value 1170.601653 
iter  10 value 1087.330479
iter  20 value 1087.173268
iter  30 value 716.002210
iter  40 value 713.733973
iter  50 value 713.270502
iter  60 value 713.237408
iter  70 value 713.231989
iter  80 value 713.227554
iter  90 value 713.222816
iter 100 value 713.217366
final  value 713.217366 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  25
initial  value 1326.780716 
iter  10 value 1087.399002
iter  20 value 1087.260836
iter  20 value 1087.260833
iter  20 value 1087.260829
final  value 1087.260829 
converged
Fitting Repeat 3 

# weights:  25
initial  value 1356.672017 
iter  10 value 1087.357225
final  value 1087.261437 
converged
Fitting Repeat 4 

# weights:  25
initial  value 1274.814872 
iter  10 value 1087.379254
final  value 1087.259886 
converged
Fitting Repeat 5 

# weights:  25
initial  value 1145.575429 
iter  10 value 1087.268619
iter  20 value 1087.256393
iter  30 value 1087.254286
iter  40 value 1087.250742
iter  50 value 1087.243475
iter  60 value 1087.220618
iter  70 value 1086.815096
iter  80 value 717.580435
iter  90 value 713.292209
iter 100 value 713.229594
final  value 713.229594 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  41
initial  value 1188.886929 
iter  10 value 1087.380071
final  value 1087.258739 
converged
Fitting Repeat 2 

# weights:  41
initial  value 1140.905535 
iter  10 value 1087.283557
iter  20 value 807.029828
iter  30 value 715.058793
iter  40 value 713.829774
iter  50 value 713.480885
iter  60 value 713.268827
iter  70 value 713.238227
iter  80 value 713.222332
iter  90 value 713.213679
iter 100 value 713.208453
final  value 713.208453 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  41
initial  value 1224.771791 
iter  10 value 1087.346897
final  value 1087.260766 
converged
Fitting Repeat 4 

# weights:  41
initial  value 1157.936972 
iter  10 value 1087.325505
final  value 1087.261105 
converged
Fitting Repeat 5 

# weights:  41
initial  value 1245.566676 
iter  10 value 1087.354134
final  value 1087.259649 
converged
Fitting Repeat 1 

# weights:  29
initial  value 1338.481805 
iter  10 value 1088.477581
iter  20 value 1087.288044
iter  30 value 1087.186589
iter  40 value 716.130325
iter  50 value 713.846037
iter  60 value 713.567665
iter  70 value 713.538013
iter  80 value 713.512990
iter  90 value 713.473217
iter 100 value 713.445319
final  value 713.445319 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  29
initial  value 1297.903083 
iter  10 value 1087.929313
iter  20 value 901.555895
iter  30 value 731.139812
iter  40 value 730.020405
iter  50 value 729.582251
iter  60 value 725.599073
iter  70 value 719.441711
iter  80 value 717.315283
iter  90 value 715.429229
iter 100 value 714.923874
final  value 714.923874 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  29
initial  value 1246.434826 
iter  10 value 1088.024229
iter  20 value 1087.305683
iter  30 value 717.218698
iter  40 value 714.184577
iter  50 value 713.889992
iter  60 value 713.698860
iter  70 value 713.580196
iter  80 value 713.468845
iter  90 value 713.444381
iter 100 value 713.434363
final  value 713.434363 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  29
initial  value 1211.800094 
iter  10 value 1088.120846
iter  20 value 1087.277309
iter  30 value 1087.270149
iter  40 value 1087.236392
iter  50 value 801.564884
iter  60 value 725.740150
iter  70 value 723.038279
iter  80 value 715.616458
iter  90 value 715.138661
iter 100 value 714.783685
final  value 714.783685 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  29
initial  value 1151.057318 
iter  10 value 1088.076920
iter  20 value 1087.285629
iter  30 value 1087.282218
iter  40 value 1087.274162
iter  50 value 1087.227695
iter  60 value 735.324599
iter  70 value 731.143966
iter  80 value 725.855548
iter  90 value 721.470567
iter 100 value 716.401585
final  value 716.401585 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  33
initial  value 1336.326905 
iter  10 value 1090.122022
iter  20 value 1087.317059
iter  30 value 1087.067129
iter  40 value 727.002234
iter  50 value 717.527371
iter  60 value 716.001673
iter  70 value 714.777538
iter  80 value 714.148459
iter  90 value 713.903099
iter 100 value 713.755963
final  value 713.755963 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  33
initial  value 1203.567802 
iter  10 value 1090.486515
iter  20 value 1087.297594
iter  30 value 719.682613
iter  40 value 716.163878
iter  50 value 714.995804
iter  60 value 714.148893
iter  70 value 714.007994
iter  80 value 713.958756
iter  90 value 713.870502
iter 100 value 713.781152
final  value 713.781152 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  33
initial  value 1429.071518 
iter  10 value 1089.616374
iter  20 value 1087.320884
iter  30 value 1052.142925
iter  40 value 724.955914
iter  50 value 716.456640
iter  60 value 714.587243
iter  70 value 713.911701
iter  80 value 713.760276
iter  90 value 713.716442
iter 100 value 713.679075
final  value 713.679075 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  33
initial  value 1166.277532 
iter  10 value 1088.872498
iter  20 value 1087.319245
iter  30 value 1087.308657
iter  40 value 1087.218710
iter  50 value 720.863606
iter  60 value 715.308261
iter  70 value 714.056015
iter  80 value 713.794398
iter  90 value 713.714042
iter 100 value 713.651498
final  value 713.651498 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  33
initial  value 1090.854482 
iter  10 value 1087.297126
iter  20 value 745.739631
iter  30 value 737.280438
iter  40 value 725.477597
iter  50 value 717.882095
iter  60 value 715.979109
iter  70 value 714.694093
iter  80 value 714.071314
iter  90 value 713.932740
iter 100 value 713.759534
final  value 713.759534 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  25
initial  value 1218.874243 
iter  10 value 1127.979794
iter  20 value 1126.024865
iter  30 value 731.600512
iter  40 value 717.095515
iter  50 value 716.621463
iter  60 value 716.592714
iter  70 value 716.573867
iter  80 value 716.560591
iter  90 value 716.541711
iter 100 value 716.528065
final  value 716.528065 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  25
initial  value 1351.128301 
iter  10 value 1166.009065
iter  20 value 1162.723633
iter  30 value 852.442974
iter  40 value 836.577858
iter  50 value 827.684761
iter  60 value 822.257398
iter  70 value 818.592848
iter  80 value 818.066785
iter  90 value 817.614829
iter 100 value 817.239251
final  value 817.239251 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  25
initial  value 1351.069210 
iter  10 value 1118.853592
iter  20 value 1116.913346
iter  30 value 1116.783625
iter  40 value 729.177831
iter  50 value 727.241208
iter  60 value 727.201665
iter  70 value 727.181815
iter  80 value 727.173614
iter  90 value 727.165853
iter 100 value 727.158083
final  value 727.158083 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  25
initial  value 1333.850433 
iter  10 value 1170.949284
iter  20 value 1167.641871
iter  30 value 863.489611
iter  40 value 774.835423
iter  50 value 773.561340
iter  60 value 773.186590
iter  70 value 772.982832
iter  80 value 772.947048
iter  90 value 772.924694
iter 100 value 772.913795
final  value 772.913795 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  25
initial  value 1203.861303 
iter  10 value 986.342711
iter  20 value 983.411237
iter  30 value 652.859000
iter  40 value 643.257604
iter  50 value 633.768654
iter  60 value 629.960659
iter  70 value 628.121654
iter  80 value 627.558890
iter  90 value 627.384534
iter 100 value 627.304802
final  value 627.304802 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  5
initial  value 1121.559596 
iter  10 value 774.325753
iter  20 value 760.080537
iter  20 value 760.080537
iter  20 value 760.080537
final  value 760.080537 
converged
Fitting Repeat 2 

# weights:  5
initial  value 1346.079985 
iter  10 value 841.800031
final  value 826.734791 
converged
Fitting Repeat 3 

# weights:  5
initial  value 1156.077324 
iter  10 value 718.544880
final  value 699.827586 
converged
Fitting Repeat 4 

# weights:  5
initial  value 1168.153067 
iter  10 value 779.585877
iter  20 value 743.916764
final  value 743.913082 
converged
Fitting Repeat 5 

# weights:  5
initial  value 1057.341270 
iter  10 value 757.742011
iter  20 value 751.187094
final  value 751.187085 
converged
Fitting Repeat 1 

# weights:  49
initial  value 1223.049706 
iter  10 value 1087.710217
iter  20 value 1087.260759
iter  30 value 1087.260295
iter  40 value 1087.259589
iter  50 value 1087.258366
iter  60 value 1087.255684
iter  70 value 1087.245346
iter  80 value 851.135201
iter  90 value 713.500426
iter 100 value 713.309027
final  value 713.309027 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  49
initial  value 1258.041020 
iter  10 value 1087.899458
iter  20 value 1087.264042
iter  30 value 1087.260654
iter  40 value 1087.260437
iter  50 value 1087.260150
iter  60 value 1087.259752
iter  70 value 1087.259156
iter  80 value 1087.258148
iter  90 value 1087.256086
iter 100 value 1087.249575
final  value 1087.249575 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  49
initial  value 1220.422410 
iter  10 value 1087.687389
iter  20 value 1087.264546
iter  30 value 1087.260484
iter  40 value 1087.258641
iter  50 value 1087.252529
iter  60 value 1086.794324
iter  70 value 721.429715
iter  80 value 713.824559
iter  90 value 713.637146
iter 100 value 713.597487
final  value 713.597487 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  49
initial  value 1211.582524 
iter  10 value 1087.642900
iter  20 value 1087.258652
iter  30 value 1087.249831
iter  40 value 1086.894973
iter  50 value 713.920320
iter  60 value 713.343851
iter  70 value 713.306783
iter  80 value 713.293746
iter  90 value 713.280664
iter 100 value 713.274110
final  value 713.274110 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  49
initial  value 1187.153852 
iter  10 value 1087.542677
iter  20 value 1087.263103
iter  30 value 749.200598
iter  40 value 714.271186
iter  50 value 714.038614
iter  60 value 713.937132
iter  70 value 713.397435
iter  80 value 713.324309
iter  90 value 713.302016
iter 100 value 713.286278
final  value 713.286278 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  65
initial  value 1328.160462 
iter  10 value 987.986182
iter  20 value 834.857709
iter  30 value 767.588058
iter  40 value 761.317608
iter  50 value 759.942782
iter  60 value 759.692834
iter  70 value 759.635929
iter  80 value 759.630670
iter  90 value 759.630057
iter 100 value 759.629261
final  value 759.629261 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  65
initial  value 1104.545330 
iter  10 value 764.847206
iter  20 value 758.784611
iter  30 value 758.647184
iter  40 value 758.645490
final  value 758.644730 
converged
Fitting Repeat 3 

# weights:  65
initial  value 1326.377433 
iter  10 value 869.722540
iter  20 value 763.423478
iter  30 value 759.023347
iter  40 value 758.967004
final  value 758.966711 
converged
Fitting Repeat 4 

# weights:  65
initial  value 1159.336611 
iter  10 value 761.515025
iter  20 value 759.242156
iter  30 value 758.673004
iter  40 value 758.658752
final  value 758.658397 
converged
Fitting Repeat 5 

# weights:  65
initial  value 1414.800907 
iter  10 value 891.637273
iter  20 value 762.007406
iter  30 value 760.106186
iter  40 value 759.280018
iter  50 value 759.030948
iter  60 value 758.972412
iter  70 value 758.967484
final  value 758.966775 
converged
Fitting Repeat 1 

# weights:  41
initial  value 1360.647442 
iter  10 value 1090.679797
iter  20 value 721.674784
iter  30 value 716.538826
iter  40 value 715.581037
iter  50 value 715.306690
iter  60 value 715.235504
iter  70 value 715.205891
iter  80 value 715.201978
iter  90 value 715.200284
iter 100 value 715.199812
final  value 715.199812 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  41
initial  value 1211.881613 
iter  10 value 1089.439252
iter  20 value 727.014998
iter  30 value 719.069705
iter  40 value 715.750444
iter  50 value 715.370991
iter  60 value 715.264065
iter  70 value 715.225319
iter  80 value 715.212625
iter  90 value 715.207463
iter 100 value 715.202692
final  value 715.202692 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  41
initial  value 1080.964755 
iter  10 value 722.862798
iter  20 value 717.365524
iter  30 value 715.847494
iter  40 value 715.552241
iter  50 value 715.381354
iter  60 value 715.285204
iter  70 value 715.249420
iter  80 value 715.241186
iter  90 value 715.233555
iter 100 value 715.232268
final  value 715.232268 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  41
initial  value 1192.400583 
iter  10 value 1089.935541
iter  20 value 731.994569
iter  30 value 717.624207
iter  40 value 716.026080
iter  50 value 715.614027
iter  60 value 715.420345
iter  70 value 715.342800
iter  80 value 715.328306
iter  90 value 715.319535
iter 100 value 715.316698
final  value 715.316698 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  41
initial  value 1215.162055 
iter  10 value 1089.901710
iter  20 value 808.635642
iter  30 value 721.699734
iter  40 value 717.031392
iter  50 value 715.547606
iter  60 value 715.343662
iter  70 value 715.269402
iter  80 value 715.234952
iter  90 value 715.222173
iter 100 value 715.214782
final  value 715.214782 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  57
initial  value 1210.352582 
iter  10 value 1089.294921
iter  20 value 1087.280131
iter  30 value 792.815288
iter  40 value 718.613845
iter  50 value 713.879289
iter  60 value 713.557725
iter  70 value 713.422505
iter  80 value 713.413676
iter  90 value 713.399787
iter 100 value 713.386981
final  value 713.386981 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  57
initial  value 1197.786711 
iter  10 value 1088.608582
iter  20 value 1087.274598
iter  30 value 724.970631
iter  40 value 722.985734
iter  50 value 721.231341
iter  60 value 714.429347
iter  70 value 714.164407
iter  80 value 713.874785
iter  90 value 713.643929
iter 100 value 713.551464
final  value 713.551464 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  57
initial  value 1144.183513 
iter  10 value 1088.638359
iter  20 value 1087.226547
iter  30 value 714.584195
iter  40 value 713.601129
iter  50 value 713.456878
iter  60 value 713.413326
iter  70 value 713.399043
iter  80 value 713.388999
iter  90 value 713.379846
iter 100 value 713.375700
final  value 713.375700 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  57
initial  value 1195.752771 
iter  10 value 1088.336584
iter  20 value 1087.262251
iter  30 value 1087.177818
iter  40 value 720.671807
iter  50 value 717.860581
iter  60 value 713.830018
iter  70 value 713.679865
iter  80 value 713.609543
iter  90 value 713.519538
iter 100 value 713.494046
final  value 713.494046 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  57
initial  value 1214.819882 
iter  10 value 1088.897070
iter  20 value 1087.273644
iter  30 value 1087.272803
iter  40 value 1087.271558
iter  50 value 1087.269313
iter  60 value 1087.263059
iter  70 value 1087.148767
iter  80 value 714.399153
iter  90 value 714.107100
iter 100 value 714.002215
final  value 714.002215 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  77
initial  value 1269.663989 
iter  10 value 1077.229627
iter  20 value 726.163364
iter  30 value 717.885775
iter  40 value 716.998830
iter  50 value 716.847681
iter  60 value 716.758245
iter  70 value 716.736482
iter  80 value 716.713839
iter  90 value 716.688445
iter 100 value 716.680792
final  value 716.680792 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  77
initial  value 1211.414136 
iter  10 value 1018.038892
iter  20 value 730.385229
iter  30 value 717.585811
iter  40 value 716.851292
iter  50 value 716.733803
iter  60 value 716.696819
iter  70 value 716.685106
iter  80 value 716.674859
iter  90 value 716.671663
iter 100 value 716.671213
final  value 716.671213 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  77
initial  value 1089.657519 
iter  10 value 717.196520
iter  20 value 716.790841
iter  30 value 716.698027
iter  40 value 716.672926
iter  50 value 716.669311
final  value 716.669167 
converged
Fitting Repeat 4 

# weights:  77
initial  value 1492.703138 
iter  10 value 779.642066
iter  20 value 731.520952
iter  30 value 717.564363
iter  40 value 716.808888
iter  50 value 716.755430
iter  60 value 716.713934
iter  70 value 716.688474
iter  80 value 716.683353
iter  90 value 716.679970
iter 100 value 716.678385
final  value 716.678385 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  77
initial  value 1303.759698 
iter  10 value 724.262327
iter  20 value 716.965352
iter  30 value 716.723021
iter  40 value 716.675276
iter  50 value 716.669802
iter  60 value 716.668576
iter  70 value 716.667622
iter  80 value 716.667191
final  value 716.667174 
converged
Fitting Repeat 1 

# weights:  81
initial  value 1277.522555 
iter  10 value 1086.878899
iter  20 value 1086.023920
iter  30 value 1086.003592
iter  40 value 785.592347
iter  50 value 707.926277
iter  60 value 706.889655
iter  70 value 706.360633
iter  80 value 704.516620
iter  90 value 700.984080
iter 100 value 699.694172
final  value 699.694172 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  81
initial  value 1169.265303 
iter  10 value 1078.211605
iter  20 value 1077.431041
iter  30 value 1077.403026
iter  40 value 781.712582
iter  50 value 771.581343
iter  60 value 770.892931
iter  70 value 767.959179
iter  80 value 765.486078
iter  90 value 762.264642
iter 100 value 761.146104
final  value 761.146104 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  81
initial  value 1235.252331 
iter  10 value 1074.563184
iter  20 value 1073.422611
iter  30 value 1073.408882
iter  40 value 1073.395661
iter  50 value 687.907027
iter  60 value 670.557228
iter  70 value 670.128735
iter  80 value 669.905261
iter  90 value 669.822785
iter 100 value 669.806275
final  value 669.806275 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  81
initial  value 1490.131154 
iter  10 value 1080.202487
iter  20 value 1079.702387
final  value 1079.701135 
converged
Fitting Repeat 5 

# weights:  81
initial  value 1463.958665 
iter  10 value 1100.603151
iter  20 value 1099.886519
iter  30 value 1099.784808
iter  40 value 726.988544
iter  50 value 725.496229
iter  60 value 716.993648
iter  70 value 715.733527
iter  80 value 715.603251
iter  90 value 715.560676
iter 100 value 715.503737
final  value 715.503737 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  29
initial  value 1157.432800 
iter  10 value 1087.750938
iter  20 value 1085.340078
iter  30 value 725.416515
iter  40 value 722.310983
iter  50 value 714.674735
iter  60 value 713.881629
iter  70 value 713.686305
iter  80 value 713.644688
iter  90 value 713.542666
iter 100 value 713.510275
final  value 713.510275 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  29
initial  value 1415.170482 
iter  10 value 1087.996358
iter  20 value 1087.284090
iter  30 value 725.163910
iter  40 value 714.359461
iter  50 value 713.849502
iter  60 value 713.494815
iter  70 value 713.429721
iter  80 value 713.389718
iter  90 value 713.382823
iter 100 value 713.367014
final  value 713.367014 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  29
initial  value 1345.191384 
iter  10 value 1088.301818
iter  20 value 1087.279162
iter  30 value 715.548123
iter  40 value 714.123750
iter  50 value 713.815176
iter  60 value 713.560865
iter  70 value 713.440916
iter  80 value 713.373965
iter  90 value 713.364095
iter 100 value 713.356546
final  value 713.356546 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  29
initial  value 1205.460473 
iter  10 value 1087.864656
iter  20 value 1039.958667
iter  30 value 724.205276
iter  40 value 721.738968
iter  50 value 715.229247
iter  60 value 714.694385
iter  70 value 714.199536
iter  80 value 713.965501
iter  90 value 713.829377
iter 100 value 713.620034
final  value 713.620034 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  29
initial  value 1251.832071 
iter  10 value 1088.425325
iter  20 value 1087.290424
iter  30 value 1087.271349
iter  40 value 1087.263876
iter  50 value 1087.145018
iter  60 value 741.073510
iter  70 value 735.621150
iter  80 value 734.379520
iter  90 value 734.007435
iter 100 value 728.012791
final  value 728.012791 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  17
initial  value 1175.647496 
iter  10 value 1087.768880
iter  20 value 1084.633331
iter  30 value 715.903444
iter  40 value 714.054882
iter  50 value 713.827215
iter  60 value 713.637750
iter  70 value 713.577070
iter  80 value 713.503760
iter  90 value 713.428611
iter 100 value 713.393604
final  value 713.393604 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  17
initial  value 1162.308335 
iter  10 value 1087.533257
iter  20 value 1087.350399
iter  30 value 1087.345455
iter  40 value 1087.339154
iter  50 value 1087.330400
iter  60 value 1087.316430
iter  70 value 1087.287692
iter  80 value 1087.173850
iter  90 value 716.098055
iter 100 value 713.655518
final  value 713.655518 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  17
initial  value 1268.527769 
iter  10 value 1088.163375
iter  20 value 1087.311678
iter  30 value 1087.238675
iter  40 value 791.951228
iter  50 value 714.528202
iter  60 value 713.447476
iter  70 value 713.381502
iter  80 value 713.374189
iter  90 value 713.371548
iter 100 value 713.367350
final  value 713.367350 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  17
initial  value 1264.048942 
iter  10 value 1088.256044
iter  20 value 1087.287802
iter  30 value 713.985275
iter  40 value 713.523662
iter  50 value 713.465751
iter  60 value 713.452664
iter  70 value 713.420162
iter  80 value 713.396716
iter  90 value 713.381618
iter 100 value 713.380009
final  value 713.380009 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  17
initial  value 1180.304090 
iter  10 value 1087.811794
iter  20 value 1087.297916
iter  30 value 1087.199427
iter  40 value 716.629382
iter  50 value 713.921502
iter  60 value 713.484332
iter  70 value 713.451745
iter  80 value 713.440423
iter  90 value 713.427241
iter 100 value 713.416062
final  value 713.416062 
stopped after 100 iterations
Fitting Repeat 1 

# weights:  49
initial  value 1245.734214 
iter  10 value 982.123524
iter  20 value 907.782691
iter  30 value 896.921901
iter  40 value 895.577234
iter  50 value 895.529622
iter  60 value 895.528246
final  value 895.528231 
converged
Fitting Repeat 2 

# weights:  49
initial  value 1485.164999 
iter  10 value 916.144274
iter  20 value 897.517894
iter  30 value 896.361695
iter  40 value 896.245133
iter  50 value 896.242116
iter  50 value 896.242113
iter  50 value 896.242111
final  value 896.242111 
converged
Fitting Repeat 3 

# weights:  49
initial  value 1198.613577 
iter  10 value 917.827791
iter  20 value 896.978399
iter  30 value 896.296412
iter  40 value 896.252410
iter  50 value 896.242384
final  value 896.242111 
converged
Fitting Repeat 4 

# weights:  49
initial  value 1587.130665 
iter  10 value 1011.751938
iter  20 value 908.691821
iter  30 value 896.981572
iter  40 value 896.265712
iter  50 value 896.243297
final  value 896.242126 
converged
Fitting Repeat 5 

# weights:  49
initial  value 1337.764557 
iter  10 value 942.222926
iter  20 value 902.494776
iter  30 value 896.756086
iter  40 value 896.366658
iter  50 value 896.292758
iter  60 value 896.242780
final  value 896.242110 
converged
Fitting Repeat 1 

# weights:  17
initial  value 1216.007466 
iter  10 value 761.224190
iter  20 value 718.643143
iter  30 value 717.962015
iter  40 value 717.929903
iter  50 value 717.922707
final  value 717.922680 
converged
Fitting Repeat 2 

# weights:  17
initial  value 1148.572074 
iter  10 value 1064.977399
iter  20 value 750.555603
iter  30 value 728.745763
iter  40 value 719.935938
iter  50 value 719.337551
iter  60 value 719.252777
final  value 719.252538 
converged
Fitting Repeat 3 

# weights:  17
initial  value 1273.123386 
iter  10 value 1092.293036
iter  20 value 786.411343
iter  30 value 734.143893
iter  40 value 720.276324
iter  50 value 718.265218
iter  60 value 717.914454
iter  70 value 717.760453
iter  80 value 717.741804
final  value 717.741712 
converged
Fitting Repeat 4 

# weights:  17
initial  value 1378.204381 
iter  10 value 1090.476799
iter  20 value 744.491235
iter  30 value 721.883052
iter  40 value 718.277072
iter  50 value 718.040493
iter  60 value 717.923146
final  value 717.922685 
converged
Fitting Repeat 5 

# weights:  17
initial  value 1216.464359 
iter  10 value 1089.852884
iter  20 value 741.787373
iter  30 value 719.819392
iter  40 value 719.305388
iter  50 value 719.265005
iter  60 value 719.252581
final  value 719.252538 
converged
Fitting Repeat 1 

# weights:  9
initial  value 1337.332748 
iter  10 value 1097.506368
iter  20 value 1092.829072
iter  30 value 1092.618567
iter  40 value 754.253125
iter  50 value 751.184787
iter  60 value 750.915202
iter  70 value 750.880707
iter  80 value 750.879422
final  value 750.879304 
converged
Fitting Repeat 2 

# weights:  9
initial  value 1296.733516 
iter  10 value 1181.304261
iter  20 value 1114.688978
iter  30 value 792.810885
iter  40 value 789.607215
iter  50 value 789.123703
iter  60 value 789.059357
final  value 789.058464 
converged
Fitting Repeat 3 

# weights:  9
initial  value 1098.306114 
iter  10 value 647.657836
iter  20 value 636.993885
iter  30 value 634.899827
iter  40 value 634.506907
iter  50 value 634.471613
iter  50 value 634.471606
final  value 634.471606 
converged
Fitting Repeat 4 

# weights:  9
initial  value 1117.135190 
iter  10 value 1022.989511
iter  20 value 793.449459
iter  30 value 670.638771
iter  40 value 667.032527
iter  50 value 666.577657
iter  60 value 666.476450
iter  70 value 666.469198
final  value 666.468802 
converged
Fitting Repeat 5 

# weights:  9
initial  value 1022.127213 
iter  10 value 638.521376
iter  20 value 616.487951
iter  30 value 612.898937
iter  40 value 611.103052
iter  50 value 610.910083
iter  60 value 610.792441
iter  70 value 610.790875
final  value 610.790678 
converged
Fitting Repeat 1 

# weights:  61
initial  value 1339.517470 
iter  10 value 866.546886
iter  20 value 726.880185
iter  30 value 725.221031
iter  40 value 724.948359
iter  50 value 724.841011
iter  60 value 724.830388
iter  70 value 724.829481
iter  80 value 724.829245
final  value 724.829191 
converged
Fitting Repeat 2 

# weights:  61
initial  value 1129.209177 
iter  10 value 992.843400
iter  20 value 731.300437
iter  30 value 725.125991
iter  40 value 724.853201
iter  50 value 724.835474
iter  60 value 724.830742
iter  70 value 724.829543
final  value 724.829147 
converged
Fitting Repeat 3 

# weights:  61
initial  value 1187.991844 
iter  10 value 884.766297
iter  20 value 737.080278
iter  30 value 727.741615
iter  40 value 725.942854
iter  50 value 725.112222
iter  60 value 724.910365
iter  70 value 724.863742
iter  80 value 724.846883
iter  90 value 724.841086
iter 100 value 724.830960
final  value 724.830960 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  61
initial  value 1498.744141 
iter  10 value 760.997932
iter  20 value 725.424503
iter  30 value 724.928924
iter  40 value 724.906262
iter  50 value 724.904727
final  value 724.904463 
converged
Fitting Repeat 5 

# weights:  61
initial  value 1164.556421 
iter  10 value 895.018988
iter  20 value 727.482300
iter  30 value 725.285642
iter  40 value 725.094263
iter  50 value 725.082799
iter  60 value 725.080073
final  value 725.079995 
converged
Fitting Repeat 1 

# weights:  17
initial  value 1756.021624 
iter  10 value 1581.019592
iter  20 value 1405.059004
iter  30 value 1005.990664
iter  40 value 1005.481457
iter  50 value 1005.359437
iter  60 value 1005.330803
iter  70 value 1005.273751
iter  80 value 1005.238557
iter  90 value 1005.205337
iter 100 value 1005.199741
final  value 1005.199741 
stopped after 100 iterations
Fitting Repeat 2 

# weights:  17
initial  value 2133.375174 
iter  10 value 1580.806258
iter  20 value 1579.934348
iter  30 value 1047.848793
iter  40 value 1030.078068
iter  50 value 1021.416611
iter  60 value 1020.085574
iter  70 value 1017.962965
iter  80 value 1008.098809
iter  90 value 1006.678273
iter 100 value 1005.639095
final  value 1005.639095 
stopped after 100 iterations
Fitting Repeat 3 

# weights:  17
initial  value 1640.421881 
iter  10 value 1580.277392
iter  20 value 1579.877433
iter  30 value 1579.599284
iter  40 value 1012.229808
iter  50 value 1007.424119
iter  60 value 1005.941461
iter  70 value 1005.800921
iter  80 value 1005.516736
iter  90 value 1005.383924
iter 100 value 1005.327217
final  value 1005.327217 
stopped after 100 iterations
Fitting Repeat 4 

# weights:  17
initial  value 1768.540404 
iter  10 value 1580.750644
iter  20 value 1579.672780
iter  30 value 1007.639641
iter  40 value 1005.469325
iter  50 value 1005.284829
iter  60 value 1005.267471
iter  70 value 1005.238166
iter  80 value 1005.221181
iter  90 value 1005.199435
iter 100 value 1005.194670
final  value 1005.194670 
stopped after 100 iterations
Fitting Repeat 5 

# weights:  17
initial  value 1777.072045 
iter  10 value 1580.954006
iter  20 value 1579.900975
iter  30 value 1579.899497
iter  40 value 1579.897729
iter  50 value 1579.895495
iter  60 value 1579.892423
iter  70 value 1579.887711
iter  80 value 1579.878969
iter  90 value 1579.855143
iter 100 value 1579.490585
final  value 1579.490585 
stopped after 100 iterations
Model Averaged Neural Network 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  size  decay         bag    RMSE      Rsquared   Selected
   1    1.091338e+00   TRUE  1.186754  0.7813948          
   2    2.012419e-03   TRUE  1.154675  0.7282013          
   4    3.446612e-04  FALSE  1.154552  0.7276476  *       
   4    3.398849e-02  FALSE  1.155965  0.7210209          
   6    3.423899e-05  FALSE  1.352695  0.7269310          
   6    9.406485e-04   TRUE  1.154652  0.7274458          
   6    1.333959e-02   TRUE  1.155561  0.7181646          
   7    2.929976e-04  FALSE  1.154700  0.7240791          
   7    3.317635e-04  FALSE  1.154565  0.7281559          
   8    8.943877e-04  FALSE  1.154591  0.7273634          
  10    2.757336e-05  FALSE  1.376897  0.7278560          
  10    1.206222e-02  FALSE  1.155435  0.7190656          
  12    1.542898e-05  FALSE  1.347837  0.5444817          
  12    9.073764e-05  FALSE  1.232771  0.7265754          
  12    6.514492e+00  FALSE  1.211403  0.8956058          
  14    3.072390e-04  FALSE  1.154586  0.7267540          
  15    1.615178e-01  FALSE  1.156539  0.7471000          
  16    9.212890e-01  FALSE  1.164036  0.7974784          
  19    3.405043e-02  FALSE  1.155449  0.7261742          
  20    1.624731e-04   TRUE  1.198253  0.7244062          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were size = 4, decay = 0.0003446612 and
 bag = FALSE.
[1] "Thu Mar 08 15:35:42 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: executing %dopar% sequentially: no parallel backend registered 
2: In eval(xpr, envir = envir) :
  model fit failed for Fold1: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

3: In eval(xpr, envir = envir) :
  model fit failed for Fold2: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

4: In eval(xpr, envir = envir) :
  model fit failed for Fold3: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

5: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: In eval(xpr, envir = envir) :
  model fit failed for Fold1: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

2: In eval(xpr, envir = envir) :
  model fit failed for Fold2: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

3: In eval(xpr, envir = envir) :
  model fit failed for Fold3: vars=2 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions

4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared  
 Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                   "failed"                  
 [3] "Thu Mar 08 15:35:44 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "bag"                     
Loading required package: earth
Loading required package: plotmo
Loading required package: plotrix
Loading required package: TeachingDemos
Bagged MARS 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  degree  nprune  RMSE         Rsquared   Selected
  1       2       0.820605041  0.7901181          
  1       3       0.166773473  0.9905433          
  1       4       0.060995775  0.9984080          
  1       5       0.003205589  0.9999952  *       
  2       2       0.839196938  0.7630662          
  2       3       0.184507199  0.9875204          
  2       4       0.059384758  0.9983694          
  2       5       0.003214044  0.9999952          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nprune = 5 and degree = 1.
[1] "Thu Mar 08 15:35:57 2018"
Bagged MARS using gCV Pruning 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results:

  RMSE         Rsquared 
  0.003201727  0.9999952

Tuning parameter 'degree' was held constant at a value of 1
[1] "Thu Mar 08 15:36:02 2018"
Loading required package: mgcv
Loading required package: nlme
This is mgcv 1.8-17. For overview type 'help("mgcv-package")'.

Attaching package: 'mgcv'

The following object is masked from 'package:nnet':

    multinom

Generalized Additive Model using Splines 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  select  method  RMSE         Rsquared   Selected
  FALSE   GCV.Cp  0.003195717  0.9999953          
  FALSE   ML      0.003198818  0.9999953          
  FALSE   REML    0.003195244  0.9999953          
   TRUE   GCV.Cp  0.003204935  0.9999952          
   TRUE   ML      0.003195242  0.9999953  *       
   TRUE   REML    0.003195250  0.9999953          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were select = TRUE and method = ML.
[1] "Thu Mar 08 15:36:34 2018"
Loading required package: bartMachine
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Failed with error:  'package 'rJava' could not be loaded'
In addition: Warning message:
In max(c(NaN, NaN), na.rm = TRUE) :
  no non-missing arguments to max; returning -Inf
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package 'rJava' could not be loaded
Loading required package: bartMachine
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Failed with error:  'package 'rJava' could not be loaded'
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package 'rJava' could not be loaded
Loading required package: bartMachine
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Failed with error:  'package 'rJava' could not be loaded'
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package 'rJava' could not be loaded
 [1] "failed"                   "failed"                  
 [3] "Thu Mar 08 15:36:35 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "bartMachine"             
Loading required package: arm
Loading required package: MASS
Loading required package: Matrix
Loading required package: lme4

Attaching package: 'lme4'

The following object is masked from 'package:nlme':

    lmList


arm (Version 1.9-3, built: 2016-11-21)

Working directory is C:/Users/irina grishina/Desktop/generated data test/LAPTOP-1SBQTC5I


Attaching package: 'arm'

The following object is masked from 'package:plotrix':

    rescale

Bayesian Generalized Linear Model 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results:

  RMSE         Rsquared 
  0.003198821  0.9999953

[1] "Thu Mar 08 15:36:38 2018"
Loading required package: monomvn
Loading required package: pls

Attaching package: 'pls'

The following objects are masked from 'package:arm':

    coefplot, corrplot

The following object is masked from 'package:caret':

    R2

The following object is masked from 'package:stats':

    loadings

Loading required package: lars
Loaded lars 1.2

t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
The Bayesian lasso 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  sparsity    RMSE         Rsquared   Selected
  0.02270368  0.004207776  0.9999952          
  0.07566496  0.004207776  0.9999952          
  0.16751808  0.004207776  0.9999952          
  0.18527058  0.004207776  0.9999952          
  0.25925992  0.004207776  0.9999952          
  0.26823929  0.004207776  0.9999952          
  0.29050742  0.004207776  0.9999952          
  0.31815992  0.004207776  0.9999952          
  0.32024113  0.004207776  0.9999952          
  0.38033831  0.004207776  0.9999952          
  0.45232675  0.004207776  0.9999952          
  0.46536954  0.004207776  0.9999952          
  0.55109471  0.004207776  0.9999952          
  0.58705981  0.004207776  0.9999952          
  0.59729490  0.004207776  0.9999952          
  0.66659629  0.004207776  0.9999952          
  0.70981269  0.004207776  0.9999952          
  0.76799555  0.004207776  0.9999952          
  0.94191140  0.004207776  0.9999952          
  0.95769455  0.004207776  0.9999952  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was sparsity = 0.9576946.
[1] "Thu Mar 08 15:36:41 2018"
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
Bayesian Ridge Regression (Model Averaged) 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results:

  RMSE         Rsquared 
  0.004649052  0.9999952

[1] "Thu Mar 08 15:36:43 2018"
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
t=100, m=2
t=200, m=2
t=300, m=2
t=400, m=2
t=500, m=2
t=600, m=2
t=700, m=2
t=800, m=2
t=900, m=2
Bayesian Ridge Regression 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results:

  RMSE         Rsquared 
  0.003901903  0.9999952

[1] "Thu Mar 08 15:36:44 2018"
Loading required package: bst
Loading required package: gbm
Loading required package: survival

Attaching package: 'survival'

The following object is masked from 'package:caret':

    cluster

Loading required package: splines
Loading required package: parallel
Loaded gbm 2.1.3
Boosted Linear Model 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  nu          mstop  RMSE        Rsquared   Selected
  0.01980235  293    0.07835570  0.9999932          
  0.04497556  226    0.02750495  0.9999949          
  0.05436301  130    0.04541705  0.9999935          
  0.09661912  275    0.02616684  0.9999950          
  0.12187634  478    0.02616680  0.9999950          
  0.14744193  160    0.02616692  0.9999950          
  0.14949972  333    0.02616680  0.9999950          
  0.15282940  159    0.02616688  0.9999950          
  0.15448301   93    0.02619150  0.9999950          
  0.19582733  190    0.02616680  0.9999950          
  0.19801383  145    0.02616681  0.9999950          
  0.23098789   38    0.02809386  0.9999950          
  0.30862914  233    0.02616680  0.9999950          
  0.31299340  134    0.02616680  0.9999950          
  0.35354463   84    0.02616680  0.9999950          
  0.35362357  471    0.02616680  0.9999950          
  0.42112066  355    0.02616680  0.9999950  *       
  0.49661219  384    0.02616680  0.9999950          
  0.50395625   12    0.03369166  0.9999949          
  0.58141907  299    0.02616680  0.9999950          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were mstop = 355 and nu = 0.4211207.
[1] "Thu Mar 08 15:37:18 2018"
Loading required package: party
Loading required package: grid
Loading required package: mvtnorm
Loading required package: modeltools
Loading required package: stats4

Attaching package: 'modeltools'

The following object is masked from 'package:lme4':

    refit

The following object is masked from 'package:plyr':

    empty

Loading required package: strucchange
Loading required package: zoo

Attaching package: 'zoo'

The following objects are masked from 'package:base':

    as.Date, as.Date.numeric

Loading required package: sandwich
Conditional Inference Random Forest 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   Selected
  1     0.6053392  0.9325713          
  2     0.2769093  0.9682157  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 2.
[1] "Thu Mar 08 15:37:35 2018"
Conditional Inference Tree 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  mincriterion  RMSE       Rsquared   Selected
  0.02270368    0.3874101  0.9311698          
  0.07566496    0.3874101  0.9311698          
  0.16751808    0.3874101  0.9311698          
  0.18527058    0.3874101  0.9311698          
  0.25925992    0.3874101  0.9311698          
  0.26823929    0.3874101  0.9311698          
  0.29050742    0.3874101  0.9311698          
  0.31815992    0.3874101  0.9311698          
  0.32024113    0.3874101  0.9311698          
  0.38033831    0.3874101  0.9311698          
  0.45232675    0.3874101  0.9311698          
  0.46536954    0.3874101  0.9311698          
  0.55109471    0.3874101  0.9311698          
  0.58705981    0.3874101  0.9311698          
  0.59729490    0.3874101  0.9311698          
  0.66659629    0.3874101  0.9311698          
  0.70981269    0.3874101  0.9311698          
  0.76799555    0.3874101  0.9311698          
  0.94191140    0.3874101  0.9311698          
  0.95769455    0.3874101  0.9311698  *       

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was mincriterion = 0.9576946.
[1] "Thu Mar 08 15:37:39 2018"
Conditional Inference Tree 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  maxdepth  mincriterion  RMSE       Rsquared   Selected
   1        0.83965985    1.1915096  0.3272499          
   2        0.38395307    0.9009297  0.6177638          
   3        0.25623208    0.7264731  0.7504510          
   3        0.58855530    0.7264731  0.7504510          
   4        0.08908682    0.5377844  0.8645575          
   5        0.24447734    0.4443790  0.9080238          
   5        0.25347144    0.4443790  0.9080238          
   5        0.32890456    0.4443790  0.9080238          
   5        0.52085710    0.4443790  0.9080238          
   6        0.32525431    0.4032957  0.9247953          
   7        0.07341495    0.3919089  0.9294485          
   7        0.51357118    0.3919089  0.9294485          
   9        0.03138956    0.3874101  0.9311698  *       
   9        0.15963125    0.3874101  0.9311698          
   9        0.96898009    0.3880599  0.9309526          
  10        0.24791273    0.3874101  0.9311698          
  11        0.70137005    0.3874101  0.9311698          
  12        0.82739931    0.3874101  0.9311698          
  15        0.20179689    0.3874101  0.9311698          
  15        0.58868710    0.3874101  0.9311698          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were maxdepth = 9 and mincriterion
 = 0.03138956.
[1] "Thu Mar 08 15:37:41 2018"
Loading required package: Cubist
Cubist 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  committees  neighbors  RMSE         Rsquared   Selected
   4          5          0.003332444  0.9999948          
   8          4          0.003363773  0.9999947          
   9          2          0.003547431  0.9999941          
  16          5          0.003332444  0.9999948          
  21          9          0.003288327  0.9999950          
  25          3          0.003426222  0.9999945          
  25          6          0.003314424  0.9999949          
  26          1          0.003854473  0.9999931          
  26          3          0.003426222  0.9999945          
  33          2          0.003547431  0.9999941          
  33          3          0.003426222  0.9999945          
  39          0          0.003178788  0.9999953  *       
  52          4          0.003363773  0.9999947          
  53          2          0.003547431  0.9999941          
  59          1          0.003854473  0.9999931          
  59          9          0.003288327  0.9999950          
  71          7          0.003300640  0.9999949          
  83          7          0.003300640  0.9999949          
  84          0          0.003178788  0.9999953          
  97          5          0.003332444  0.9999948          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were committees = 39 and neighbors = 0.
[1] "Thu Mar 08 15:37:53 2018"
Loading required package: deepnet
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
Stacked AutoEncoder Deep Neural Network 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  layer1  layer2  layer3  hidden_dropout  visible_dropout  RMSE      Rsquared  
   2      17       4      0.074795580     0.080945606      1.528472  0.18565309
   3       9       4      0.044943731     0.080394827      1.574491  0.12212067
   5       6      18      0.034301018     0.048554752      1.689412  0.27806599
   5      13      15      0.094278499     0.006562337      1.526076  0.71424140
   6       3      12      0.008435309     0.097746408      1.859705  0.57100764
   7       8       2      0.050663236     0.086765024      1.906020  0.58573841
   7      11       8      0.045667480     0.026391938      1.594992  0.42305687
   8       6      13      0.090055376     0.007896460      1.461350  0.71383038
   8       6      15      0.052563686     0.074003295      1.834493  0.32380599
   9       8      15      0.030380091     0.053794911      1.599557  0.68442992
  10       3      11      0.032684393     0.006695017      1.672094  0.94148662
  10      11      14      0.027425546     0.071905116      1.584103  0.38529329
  12       5      15      0.078010092     0.082095739      1.476570  0.48977115
  13       2      14      0.021515863     0.060797787      1.483882  0.59604659
  13      20      11      0.032978274     0.038505580      1.976610  0.07447965
  14       6      19      0.011066210     0.085197941      1.693806  0.17777204
  15      15      18      0.046962787     0.020038243      2.159571  0.25334717
  16      17      19      0.050891219     0.013894312      1.754661  0.32729797
  19      13      14      0.087426870     0.075987925      1.632404  0.30766348
  20       5       8      0.060919651     0.007868061      2.048908  0.58398822
  Selected
          
          
          
          
          
          
          
  *       
          
          
          
          
          
          
          
          
          
          
          
          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were layer1 = 8, layer2 = 6, layer3 =
 13, hidden_dropout = 0.09005538 and visible_dropout = 0.00789646.
[1] "Thu Mar 08 15:37:58 2018"
Multivariate Adaptive Regression Spline 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  degree  nprune  RMSE         Rsquared   Selected
  1       2       1.012375902  0.5149165          
  1       3       0.389720960  0.9215702          
  1       4       0.105469451  0.9909369          
  1       5       0.003204598  0.9999952  *       
  2       2       1.012375902  0.5149165          
  2       3       0.389720960  0.9215702          
  2       4       0.105469451  0.9909369          
  2       5       0.003204598  0.9999952          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nprune = 5 and degree = 1.
[1] "Thu Mar 08 15:38:00 2018"
Loading required package: elmNN
Extreme Learning Machine 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  nhid  actfun   RMSE         Rsquared   Selected
   1    tansig   1.381303492  0.3159737          
   2    radbas   1.328782732  0.1958592          
   4    purelin  0.003198821  0.9999953          
   4    radbas   0.927262990  0.6013421          
   5    sin      0.339189779  0.9430666          
   6    purelin  0.003198821  0.9999953          
   6    radbas   0.723676122  0.7548658          
   7    radbas   0.811745853  0.6340827          
   7    sin      0.252120684  0.9697301          
   8    radbas   0.822347702  0.6454567          
   9    purelin  0.003198821  0.9999953  *       
   9    sin      0.120076871  0.9920299          
  11    sin      0.055876775  0.9978540          
  12    sin      0.048574185  0.9985206          
  12    tansig   0.130409683  0.9826884          
  13    sin      0.063227624  0.9958474          
  14    purelin  0.003198821  0.9999953          
  15    tansig   0.083707796  0.9956098          
  18    purelin  0.003198821  0.9999953          
  19    sin      0.014553632  0.9998680          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were nhid = 9 and actfun = purelin.
[1] "Thu Mar 08 15:38:02 2018"
Loading required package: elasticnet
Elasticnet 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  lambda        fraction    RMSE        Rsquared   Selected
  1.368428e-05  0.83965985  0.23257833  0.9999784          
  2.844394e-05  0.38395307  0.89351596  0.9988137          
  1.011832e-04  0.58855530  0.59676359  0.9997706          
  1.293074e-04  0.25623208  1.07875973  0.9961403          
  3.593847e-04  0.08908682  1.32118506  0.9549785          
  4.068513e-04  0.52085710  0.69494602  0.9996064          
  5.534068e-04  0.32890456  0.97335143  0.9980862          
  8.108854e-04  0.25347144  1.08275768  0.9960274          
  8.345393e-04  0.24447734  1.09580266  0.9956288          
  1.914387e-03  0.32525431  0.97863032  0.9980222          
  5.175598e-03  0.07341495  1.34390326  0.9337559          
  6.197510e-03  0.51357118  0.70540999  0.9995834          
  2.025668e-02  0.15963125  1.21875849  0.9874792          
  3.329345e-02  0.03138956  1.40477835  0.7403176          
  3.835035e-02  0.96898009  0.04385153  0.9999948  *       
  9.990282e-02  0.24791273  1.09003694  0.9958187          
  1.814998e-01  0.70137005  0.42935343  0.9999145          
  4.054836e-01  0.82739931  0.24192922  0.9999772          
  4.481964e+00  0.58868710  0.57929712  0.9997976          
  5.574005e+00  0.20179689  1.15156803  0.9933282          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were fraction = 0.9689801 and lambda
 = 0.03835035.
[1] "Thu Mar 08 15:38:04 2018"
Loading required package: extraTrees
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Failed with error:  'package 'rJava' could not be loaded'
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package 'rJava' could not be loaded
Loading required package: extraTrees
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Failed with error:  'package 'rJava' could not be loaded'
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package 'rJava' could not be loaded
Loading required package: extraTrees
Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Failed with error:  'package 'rJava' could not be loaded'
note: only 1 unique complexity parameters in default grid. Truncating the grid to 1 .

Loading required package: rJava
Error: package or namespace load failed for 'rJava':
 .onLoad failed in loadNamespace() for 'rJava', details:
  call: fun(libname, pkgname)
  error: JAVA_HOME cannot be determined from the Registry
Error : package 'rJava' could not be loaded
 [1] "failed"                   "failed"                  
 [3] "Thu Mar 08 15:38:05 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "extraTrees"              
Loading required package: foba
Ridge Regression with Variable Selection 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  lambda        k  RMSE         Rsquared   Selected
  1.368428e-05  2  0.003198814  0.9999953  *       
  2.844394e-05  1  0.991918426  0.5331043          
  1.011832e-04  2  0.003201474  0.9999953          
  1.293074e-04  1  0.991918271  0.5331043          
  3.593847e-04  1  0.991917960  0.5331043          
  4.068513e-04  2  0.003247188  0.9999953          
  5.534068e-04  1  0.991917744  0.5331043          
  8.108854e-04  1  0.991917523  0.5331043          
  8.345393e-04  1  0.991917506  0.5331043          
  1.914387e-03  1  0.991917420  0.5331043          
  5.175598e-03  1  0.991925055  0.5331043          
  6.197510e-03  2  0.009272873  0.9999953          
  2.025668e-02  1  0.992108772  0.5331043          
  3.329345e-02  1  0.992452312  0.5331043          
  3.835035e-02  2  0.052409606  0.9999952          
  9.990282e-02  1  0.996422779  0.5331043          
  1.814998e-01  2  0.218272007  0.9999952          
  4.054836e-01  2  0.411340591  0.9999952          
  4.481964e+00  2  1.182099410  0.9999950          
  5.574005e+00  1  1.338039731  0.5331043          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were k = 2 and lambda = 1.368428e-05.
[1] "Thu Mar 08 15:38:07 2018"
Generalized Additive Model using Splines 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  select  method  RMSE         Rsquared   Selected
  FALSE   GCV.Cp  0.003195083  0.9999953  *       
  FALSE   ML      0.003198818  0.9999953          
   TRUE   GCV.Cp  0.003213034  0.9999952          
   TRUE   ML      0.003195238  0.9999953          

RMSE was used to select the optimal model using  the smallest value.
The final values used for the model were select = FALSE and method = GCV.Cp.
[1] "Thu Mar 08 15:38:10 2018"
Loading required package: gam
Loading required package: foreach
Loaded gam 1.14-4


Attaching package: 'gam'

The following objects are masked from 'package:mgcv':

    gam, gam.control, gam.fit, plot.gam, predict.gam, s, summary.gam

Generalized Additive Model using Splines 

752 samples
  2 predictor

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 501, 501, 502 
Resampling results across tuning parameters:

  df         RMSE         Rsquared   Selected
  0.1135184  0.003198821  0.9999953          
  0.3783248  0.003198821  0.9999953          
  0.8375904  0.003198821  0.9999953          
  0.9263529  0.003198821  0.9999953          
  1.2962996  0.003195435  0.9999953          
  1.3411964  0.003195052  0.9999953          
  1.4525371  0.003194231  0.9999953          
  1.5907996  0.003193461  0.9999953          
  1.6012056  0.003193412  0.9999953          
  1.9016915  0.003192565  0.9999953          
  2.2616338  0.003192555  0.9999953  *       
  2.3268477  0.003192626  0.9999953          
  2.7554735  0.003193375  0.9999953          
  2.9352990  0.003193755  0.9999953          
  2.9864745  0.003193865  0.9999953          
  3.3329814  0.003194608  0.9999953          
  3.5490635  0.003195076  0.9999953          
  3.8399778  0.003195722  0.9999953          
  4.7095570  0.003198021  0.9999953          
  4.7884728  0.003198272  0.9999953          

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was df = 2.261634.
[1] "Thu Mar 08 15:38:14 2018"
Loading required package: kernlab

Attaching package: 'kernlab'

The following object is masked from 'package:modeltools':

    prior

The following object is masked from 'package:ggplot2':

    alpha

Error in .local(object, ...) : test vector does not match model !
In addition: Warning message:
In max(c(NaN, NaN), na.rm = TRUE) :
  no non-missing arguments to max; returning -Inf
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Thu Mar 08 15:38:35 2018" "basic sum C1 + C2"       
 [5] "ignore"                   "none"                    
 [7] "ICA"                      "LAPTOP-1SBQTC5I"         
 [9] "14th20hp3cv"              "gaussprLinear"           
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
